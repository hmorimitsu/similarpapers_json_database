{
  "cvpr2022_wmf_ontheexploitationofdeepfakemodelrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "On the Exploitation of Deepfake Model Recognition",
    "authors": [
      "Luca Guarnera",
      "Oliver Giudice",
      "Matthias Nie\u00dfner",
      "Sebastiano Battiato"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Guarnera_On_the_Exploitation_of_Deepfake_Model_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Guarnera_On_the_Exploitation_of_Deepfake_Model_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite recent advances in Generative Adversarial Networks (GANs), with special focus to the Deepfake phenomenon there is no a clear understanding neither in terms of explainability nor of recognition of the involved models. In particular, the recognition of a specific GAN model that generated the deepfake image compared to many other possible models created by the same generative architecture (e.g. StyleGAN) is a task not yet completely addressed in the state-of-the-art. In this work, a robust processing pipeline to evaluate the possibility to point-out analytic fingerprints for Deepfake model recognition is presented. After exploiting the latent space of 50 slightly different models through an in-depth analysis on the generated images, a proper encoder was trained to discriminate among these models obtaining a classification accuracy of over 96%. Once demonstrated the possibility to discriminate extremely similar images, a dedicated metric exploiting the insights discovered in the latent space was introduced. By achieving a final accuracy of more than 94% for the Model Recognition task on images generated by models not employed in the training phase, this study takes an important step in countering the Deepfake phenomenon introducing a sort of signature in some sense similar to those employed in the multimedia forensics field (e.g. for camera source identification task, image ballistics task, etc).",
    "code_link": "https://github.com/NVlabs/stylegan2"
  },
  "cvpr2022_wmf_detectingreal-timedeep-fakevideosusingactiveillumination": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "Detecting Real-Time Deep-Fake Videos Using Active Illumination",
    "authors": [
      "Candice R. Gerstner",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Gerstner_Detecting_Real-Time_Deep-Fake_Videos_Using_Active_Illumination_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Gerstner_Detecting_Real-Time_Deep-Fake_Videos_Using_Active_Illumination_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "While many have grown suspicious of viral images and videos found online, there is a general sense that we can and should trust that the person on the other end of our video-conferencing call is who it purports to be. The real-time creation of sophisticated deep fakes, however, is making it more difficult to trust even live video calls. Detecting deep fakes in real time introduces new challenges as compared to off-line forensic analyses. We describe a technique for detecting, in real time, deep-fake videos transmitted over a live video-conferencing application. This technique leverages the fact that a video call typically places a user in front of a light source (the computer display) which can be manipulated to induce a controlled change in the appearance of the user's face. Deviations of the expected change in appearance over time can be measured in real time and used to verify the authenticity of a video-call participant.",
    "code_link": ""
  },
  "cvpr2022_wmf_coreconsistentrepresentationlearningforfaceforgerydetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "CORE: COnsistent REpresentation Learning for Face Forgery Detection",
    "authors": [
      "Yunsheng Ni",
      "Depu Meng",
      "Changqian Yu",
      "Chengbin Quan",
      "Dongchun Ren",
      "Youjian Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Ni_CORE_COnsistent_REpresentation_Learning_for_Face_Forgery_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Ni_CORE_COnsistent_REpresentation_Learning_for_Face_Forgery_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Face manipulation techniques develop rapidly and arouse widespread public concerns. Despite that vanilla convolutional neural networks achieve acceptable performance, they suffer from the overfitting issue. To relieve this issue, there is a trend to introduce some erasing-based augmentations. We find that these methods indeed attempt to implicitly induce more consistent representations for different augmentations via assigning the same label for different augmented images. However, due to the lack of explicit regularization, the consistency between different representations is less satisfactory. Therefore, we constrain the consistency of different representations explicitly and propose a simple yet effective framework, COnsistent REpresentation Learning (CORE). Specifically, we first capture the different representations with different augmentations, then regularize the cosine distance of the representations to enhance the consistency. Extensive experiments (in-dataset and cross-dataset) demonstrate that CORE performs favorably against state-of-the-art face forgery detection methods.",
    "code_link": "https://github.com/niyunsheng/CORE"
  },
  "cvpr2022_wmf_seetheseamslocalizeddetectionofseamcarvingbasedimageforgeryinsatelliteimagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "SeeTheSeams: Localized Detection of Seam Carving Based Image Forgery in Satellite Imagery",
    "authors": [
      "Chandrakanth Gudavalli",
      "Erik Rosten",
      "Lakshmanan Nataraj",
      "Shivkumar Chandrasekaran",
      "B. S. Manjunath"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Gudavalli_SeeTheSeams_Localized_Detection_of_Seam_Carving_Based_Image_Forgery_in_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Gudavalli_SeeTheSeams_Localized_Detection_of_Seam_Carving_Based_Image_Forgery_in_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Seam carving is a popular technique for content aware image retargeting. It can be used to deliberately manipulate images, for example, change the GPS locations of a building or displace/remove roads in a satellite image. This paper proposes a novel approach for detecting and localizing seams in such images. While there are methods to detect seam carving based manipulations, this is the first time that robust localization and detection of seam carving forgery is made possible. We also propose a seam localization score (SLS) metric to evaluate the effectiveness of localization. The proposed method is evaluated extensively on a large collection of images from different sources, demonstrating a high level of detection and localization performance across these datasets. The code and datasets curated during this work will be released to the public.",
    "code_link": "https://github.com/Mayachitra-Inc/SeeTheSeams"
  },
  "cvpr2022_wmf_onimprovingcross-datasetgeneralizationofdeepfakedetectors": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "On Improving Cross-Dataset Generalization of Deepfake Detectors",
    "authors": [
      "Aakash Varma Nadimpalli",
      "Ajita Rattani"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Nadimpalli_On_Improving_Cross-Dataset_Generalization_of_Deepfake_Detectors_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Nadimpalli_On_Improving_Cross-Dataset_Generalization_of_Deepfake_Detectors_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Facial manipulation by deep fake has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deep fake detection methods have been proposed recently. Most of them model deep fake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deep fake detection with the Area under the Curve (AUC) as high as 0.99. However, the performance of these methods degrades significantly when evaluated across datasets. In this paper, we formulate deep fake detection as a hybrid combination of supervised and reinforcement learning (RL) to improve its cross-dataset generalization performance. The proposed method chooses the top-k augmentations for each test sample by an RL agent in an image-specific manner. The classification scores, obtained using CNN, of all the augmentations of each test image are averaged together for final real or fake classification. Through extensive experimental validation, we demonstrate the superiority of our method over existing published research in cross-dataset generalization of deep fake detectors, thus obtaining state-of-the-art performance.",
    "code_link": ""
  },
  "cvpr2022_wmf_ariaadversariallyrobustimageattributionforcontentprovenance": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "ARIA: Adversarially Robust Image Attribution for Content Provenance",
    "authors": [
      "Maksym Andriushchenko",
      "Xiaoyang Rebecca Li",
      "Geoffrey Oxholm",
      "Thomas Gittings",
      "Tu Bui",
      "Nicolas Flammarion",
      "John Collomosse"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Andriushchenko_ARIA_Adversarially_Robust_Image_Attribution_for_Content_Provenance_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Andriushchenko_ARIA_Adversarially_Robust_Image_Attribution_for_Content_Provenance_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image attribution -- matching an image back to a trusted source -- is an emerging tool in the fight against online misinformation. Deep visual fingerprinting models have recently been explored for this purpose. However, they are not robust to tiny input perturbations known as adversarial examples. First we illustrate how to generate valid adversarial images that can easily cause incorrect image attribution. Then we describe an approach to prevent imperceptible adversarial attacks on deep visual fingerprinting models, via robust contrastive learning. The proposed training procedure leverages training on Linf-bounded adversarial examples, it is conceptually simple and incurs only a small computational overhead. The resulting models are substantially more robust, are accurate even on unperturbed images, and perform well even over a database with millions of images. In particular, we achieve 91.6% standard and 85.1% adversarial recall under Linf-bounded perturbations on manipulated images compared to 80.1% and 0.0% from prior work. We also show that robustness generalizes to other types of imperceptible perturbations unseen during training. Finally, we show how to train an adversarially robust image comparator model for detecting editorial changes in matched images.",
    "code_link": ""
  },
  "cvpr2022_wmf_issyntheticvoicedetectionresearchgoingintotherightdirection?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "Is Synthetic Voice Detection Research Going Into the Right Direction?",
    "authors": [
      "Stefano Borz\u00ec",
      "Oliver Giudice",
      "Filippo Stanco",
      "Dario Allegra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Borzi_Is_Synthetic_Voice_Detection_Research_Going_Into_the_Right_Direction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Borzi_Is_Synthetic_Voice_Detection_Research_Going_Into_the_Right_Direction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Machine Learning, and in general Artificial Intelligence approaches, brought a great advance in each and every field of Computer Science increasing accuracy levels of predictors in any known problem. Indeed, this evolution enabled the construction of effective frameworks and solutions able to be used in investigative and forensics scenarios for detection of fakes and, in general, manipulations in multimedia contents. On the other hand, can we trust these systems? Is research activity going in the right direction? Are we just taking the low-hanging fruit without taking into account many real-case-in-the-wild situations? The purpose of this paper is to raise an alert to the research community in the specific context of synthetic voice detection, where data available for training is not big enough to give sufficient trust in the techniques available in the literature. To this aim, an exploratory investigation of the most common voice spoofing dataset was carried out and it was surprisingly easy to build simple classifiers without any Deep Learning techniques. Simple considerations on bitrate were sufficient to achieve an effective detection performance.",
    "code_link": ""
  },
  "cvpr2022_wmf_sislself-supervisedimagesignaturelearningforsplicingdetection&localization": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "SISL:Self-Supervised Image Signature Learning for Splicing Detection & Localization",
    "authors": [
      "Susmit Agrawal",
      "Prabhat Kumar",
      "Siddharth Seth",
      "Toufiq Parag",
      "Maneesh Singh",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Agrawal_SISLSelf-Supervised_Image_Signature_Learning_for_Splicing_Detection__Localization_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Agrawal_SISLSelf-Supervised_Image_Signature_Learning_for_Splicing_Detection__Localization_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent algorithms for image manipulation detection almost exclusively use deep network models. These approaches require either dense pixelwise groundtruth masks, camera ids, or image metadata to train the networks. On one hand, constructing a training set to represent the countless tampering possibilities is impractical. On the other hand, social media platforms or commercial applications are often constrained to remove camera ids as well as metadata from images. A self-supervised algorithm for training manipulation detection models without dense groundtruth or camera/image metadata would be extremely useful for many forensics applications. In this paper, we propose a self-supervised approach for training splicing detection/localization models from frequency transform of images. To identify the spliced regions, our deep network learns a representation to capture an image-specific signature by enforcing (image) self consistency. We experimentally demonstrate that our proposed model can yield similar or better performances as compared to multiple existing methods on standard datasets without relying on labels or metadata.",
    "code_link": ""
  },
  "cvpr2022_wmf_thereliabilityofforensicbody-shapeidentification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "The Reliability of Forensic Body-Shape Identification",
    "authors": [
      "Neerja Thakkar",
      "Georgios Pavlakos",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Thakkar_The_Reliability_of_Forensic_Body-Shape_Identification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Thakkar_The_Reliability_of_Forensic_Body-Shape_Identification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Photo-based forensic identification can be critical in the prosecution of, and defense against, criminal charges. Identification techniques range from the specific biometric-based to the more generic, based on height, weight, gender, and race. Although fairly basic, accurate height and weight estimation remains challenging due to physiological factors, concealing clothing, body pose, and the scale ambiguity inherent to the photographic process. We describe an extension to 3D body-pose estimation that more accurately estimates body shape across a broader range of body sizes. We evaluate the reliability of this technique in making metric estimates of height and weight, and in making non-metric categorization of people based on a scale-agnostic measure of body shape. Although this approach improves on previous efforts, we find that accurate body-shape identification from a single, reference-free image remains challenging.",
    "code_link": ""
  },
  "cvpr2022_wmf_gca-netutilizinggatedcontextattentionforimprovingimageforgerylocalizationanddetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Media Forensics",
    "title": "GCA-Net: Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection",
    "authors": [
      "Sowmen Das",
      "Md. Saiful Islam",
      "Md. Ruhul Amin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/html/Das_GCA-Net_Utilizing_Gated_Context_Attention_for_Improving_Image_Forgery_Localization_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Das_GCA-Net_Utilizing_Gated_Context_Attention_for_Improving_Image_Forgery_Localization_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Forensic analysis of manipulated pixels requires the identification of various hidden and subtle features from images. Conventional image recognition models generally fail at this task because they are biased and more attentive towards the dominant local and spatial features. In this paper, we propose a novel Gated Context Attention Network (GCA-Net) that utilizes non-local attention in conjunction with a gating mechanism in order to capture the finer image discrepancies and better identify forged regions. The proposed framework uses high dimensional embeddings to filter and aggregate the relevant context from coarse feature maps at various stages of the decoding process. This improves the network's understanding of global differences and reduces false-positive localizations. Our evaluation on standard image forensic benchmarks shows that GCA-Net can both compete against and improve over state-of-the-art networks by an average of 4.7% AUC. Additional ablation studies also demonstrate the method's robustness against attributions and resilience to false-positive predictions.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_strengtheningthetransferabilityofadversarialexamplesusingadvancedlookingaheadandself-cutmix": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Strengthening the Transferability of Adversarial Examples Using Advanced Looking Ahead and Self-CutMix",
    "authors": [
      "Donggon Jang",
      "Sanghyeok Son",
      "Dae-Shik Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Jang_Strengthening_the_Transferability_of_Adversarial_Examples_Using_Advanced_Looking_Ahead_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Jang_Strengthening_the_Transferability_of_Adversarial_Examples_Using_Advanced_Looking_Ahead_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep neural networks (DNNs) are vulnerable to adversarial examples generated by adding malicious noise imperceptible to a human. The adversarial examples successfully fool the models under the white-box setting, but the performance of attacks under the black-box setting degrades significantly, which is known as the low transferability problem. Various methods have been proposed to improve transferability, yet they are not effective against adversarial training and defense models. In this paper, we introduce two new methods termed Lookahead Iterative Fast Gradient Sign Method (LI-FGSM) and Self-CutMix (SCM) to address the above issues. LI-FGSM updates adversarial perturbations with the accumulated gradient obtained by looking ahead. A previous gradient-based attack is used for looking ahead during N steps to explore the optimal direction at each iteration. It allows the optimization process to escape the sub-optimal region and stabilize the update directions. SCM leverages the modified CutMix, which copies a patch from the original image and pastes it back at random positions of the same image, to preserve the internal information. SCM makes it possible to generate more transferable adversarial examples while alleviating the overfitting to the surrogate model employed. Our two methods are easily incorporated with the previous iterative gradient-based attacks. Extensive experiments on ImageNet show that our approach acquires state-of-the-art attack success rates not only against normally trained models but also against adversarial training and defense models.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_adversarialmachinelearningattacksagainstvideoanomalydetectionsystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Adversarial Machine Learning Attacks Against Video Anomaly Detection Systems",
    "authors": [
      "Furkan Mumcu",
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Mumcu_Adversarial_Machine_Learning_Attacks_Against_Video_Anomaly_Detection_Systems_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Mumcu_Adversarial_Machine_Learning_Attacks_Against_Video_Anomaly_Detection_Systems_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Anomaly detection in videos is an important computer vision problem with various applications including automated video surveillance. Although adversarial attacks on image understanding models have been heavily investigated, there is not much work on adversarial machine learning targeting video understanding models and no previous work which focuses on video anomaly detection. To this end, we investigate an adversarial machine learning attack against video anomaly detection systems, that can be implemented via an easy-to-perform cyber-attack. Since surveillance cameras are usually connected to the server running the anomaly detection model through a wireless network, they are prone to cyber-attacks targeting the wireless connection. We demonstrate how Wi-Fi deauthentication attack, a notoriously easy-to-perform and effective denial-of-service (DoS) attack, can be utilized to generate adversarial data for video anomaly detection systems. Specifically, we apply several effects caused by the Wi-Fi deauthentication attack on video quality (e.g., slow down, freeze, fast forward, low resolution) to the popular benchmark datasets for video anomaly detection. Our experiments with several state-of-the-art anomaly detection models show that the attackers can significantly undermine the reliability of video anomaly detection systems by causing frequent false alarms and hiding physical anomalies from the surveillance system.",
    "code_link": "https://github.com/bitbrute/evillimiter"
  },
  "cvpr2022_artofrobust_theriskandopportunityofadversarialexampleinmilitaryfield": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "The Risk and Opportunity of Adversarial Example in Military Field",
    "authors": [
      "Yuwei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Chen_The_Risk_and_Opportunity_of_Adversarial_Example_in_Military_Field_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Chen_The_Risk_and_Opportunity_of_Adversarial_Example_in_Military_Field_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Artificial intelligence technology is increasingly widely used in the military field, and various countries have carried out a number of research and experiments, aiming to use artificial intelligence technology to shorten the closing time of their own kill chains, and obtain an advantage in the future battlefield, so as to increase the probability of victory in the battle. However, due to the vulnerability of deep learning models before adversarial examples, all systems or modules using artificial intelligence algorithms are at risk of being attacked, thereby delaying or hindering the closure of the opponent's kill chain and increasing the probability of combat victory from another aspect. Based on such risks, this paper proposes a conceptual scheme of military deception by attacking the AI modules of the combat units through adversarial examples, and proposes the challenges and prospects of the current technology. To the best of our knowledge, we are the first to analyze the impact of adversarial examples in the entire process of military operations, that is, the impact of each step and activity in the entire kill chain, and simulate the actual application of adversarial examples in combat through the wargame simulation platform. Ultimately, we found that when AI technology is really widely used in the military field, adversarial examples will have a subversive impact on several activities in several steps in the kill chain, which will directly lead to the interruption of the entire kill chain. This will lead to the failure of combat troops to successfully complete combat missions in accordance with the established objectives.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_patpseudo-adversarialtrainingfordetectingadversarialvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "PAT: Pseudo-Adversarial Training for Detecting Adversarial Videos",
    "authors": [
      "Nupur Thakur",
      "Baoxin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Thakur_PAT_Pseudo-Adversarial_Training_for_Detecting_Adversarial_Videos_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Thakur_PAT_Pseudo-Adversarial_Training_for_Detecting_Adversarial_Videos_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Extensive research has demonstrated that deep neural networks (DNNs) are prone to adversarial attacks. Although various defense mechanisms have been proposed for image classification networks, fewer approaches exist for video-based models used in security-sensitive applications like surveillance. In this paper, we propose a novel yet simple algorithm called Pseudo-Adversarial Training (PAT), to detect the adversarial frames in a video without requiring knowledge of the attack. Our approach generates 'transition frames' that capture critical deviation from the original frames and eliminate the components insignificant to the detection task. To avoid the necessity of knowing the attack model, we produce 'pseudo perturbations' to train our detection network. Adversarial video detection is then achieved through the use of the detected frames. Experimental results on UCF-101 and 20BN-Jester datasets show that PAT can detect the adversarial video frames and videos with a high detection rate. We also unveil the potential reasons for the effectiveness of the transition frames and pseudo perturbations through extensive experiments.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_adversarialrobustnessthroughthelensofconvolutionalfilters": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Adversarial Robustness Through the Lens of Convolutional Filters",
    "authors": [
      "Paul Gavrikov",
      "Janis Keuper"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Gavrikov_Adversarial_Robustness_Through_the_Lens_of_Convolutional_Filters_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Gavrikov_Adversarial_Robustness_Through_the_Lens_of_Convolutional_Filters_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep learning models are intrinsically sensitive to distribution shifts in the input data. In particular, small, barely perceivable perturbations to the input data can force models to make wrong predictions with high confidence. An common defense mechanism is regularization through adversarial training which injects worst-case perturbations back into training to strengthen the decision boundaries, and to reduce overfitting. In this context, we perform an investigation of 3x3 convolution filters that form in adversarially-trained models. Filters are extracted from 71 public models of the Linf-RobustBench CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from models built on the same architectures but trained without robust regularization. We observe that adversarially-robust models appear to form more diverse, less sparse, and more orthogonal convolution filters than their normal counterparts. The largest differences between robust and normal models are found in the deepest layers, and the very first convolution layer, which consistently and predominantly forms filters that can partially eliminate perturbations, irrespective of the architecture. Data & Project website: https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens",
    "code_link": ""
  },
  "cvpr2022_artofrobust_towardscomprehensivetestingontherobustnessofcooperativemulti-agentreinforcementlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Towards Comprehensive Testing on the Robustness of Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Jun Guo",
      "Yonghong Chen",
      "Yihang Hao",
      "Zixin Yin",
      "Yin Yu",
      "Simin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Guo_Towards_Comprehensive_Testing_on_the_Robustness_of_Cooperative_Multi-Agent_Reinforcement_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Guo_Towards_Comprehensive_Testing_on_the_Robustness_of_Cooperative_Multi-Agent_Reinforcement_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "While deep neural networks (DNNs) have strengthened the performance of cooperative multi-agent reinforcement learning (c-MARL), the agent policy can be easily perturbed by adversarial examples. Considering the safety critical applications of c-MARL, such as traffic management, power management and unmanned aerial vehicle control, it is crucial to test the robustness of c-MARL algorithm before it was deployed in reality. Existing adversarial attacks for MARL could be used for testing, but is limited to one robustness aspects (e.g., reward, state, action), while c-MARL model could be attacked from any aspect. To overcome the challenge, we propose MARLSafe, the first robustness testing framework for c-MARL algorithms. First, motivated by Markov Decision Process (MDP), MARLSafe consider the robustness of c-MARL algorithms comprehensively from three aspects, namely state robustness, action robustness and reward robustness. Any c-MARL algorithm must simultaneously satisfy these robustness aspects to be considered secure. Second, due to the scarceness of c-MARL attack, we propose several c-MARL attack from multi-aspect as testing algorithms for c-MARL robustness. Experiments on SMAC environment reveals that all state-of-the-art c-MARL algorithm are of low robustness in all aspect.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_exploringrobustnessconnectionbetweenartificialandnaturaladversarialexamples": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Exploring Robustness Connection Between Artificial and Natural Adversarial Examples",
    "authors": [
      "Akshay Agarwal",
      "Nalini Ratha",
      "Mayank Vatsa",
      "Richa Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Agarwal_Exploring_Robustness_Connection_Between_Artificial_and_Natural_Adversarial_Examples_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Agarwal_Exploring_Robustness_Connection_Between_Artificial_and_Natural_Adversarial_Examples_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Although recent deep neural network algorithm has shown tremendous success in several computer vision tasks, their vulnerability against minute adversarial perturbations has raised a serious concern. In the early days of crafting these adversarial examples, artificial noises are optimized through the network and added in the images to decrease the confidence of the classifiers against the true class. However, recent efforts are showcasing the presence of natural adversarial examples which can also be effectively used to fool the deep neural networks with high confidence. In this paper, for the first time, we have raised the question that whether there is any robustness connection between artificial and natural adversarial examples. The possible robustness connection between natural and artificial adversarial examples is studied in the form that whether an adversarial example detector trained on artificial examples can detect the natural adversarial examples. We have analyzed several deep neural networks for the possible detection of artificial and natural adversarial examples in seen and unseen settings to set up a robust connection. The extensive experimental results reveal several interesting insights to defend the deep classifiers whether vulnerable against natural or artificially perturbed examples. We believe these findings can pave a way for the development of unified resiliency because defense against one attack is not sufficient for real-world use cases.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_auglydataaugmentationsforadversarialrobustness": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "AugLy: Data Augmentations for Adversarial Robustness",
    "authors": [
      "Zo\u00eb Papakipos",
      "Joanna Bitton"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Papakipos_AugLy_Data_Augmentations_for_Adversarial_Robustness_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Papakipos_AugLy_Data_Augmentations_for_Adversarial_Robustness_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We introduce AugLy, a data augmentation library with a focus on adversarial robustness. AugLy provides a wide array of augmentations for multiple modalities (audio, image, text, & video). These augmentations were inspired by those that real users perform on social media platforms, some of which were not already supported by existing data augmentation libraries. AugLy can be used for any purpose where data augmentations are useful, but it is particularly well-suited for evaluating robustness and systematically generating adversarial attacks. In this paper we present how AugLy works, benchmark it against existing libraries, and use it to evaluate the robustness of various state-of-the-art models to showcase AugLy's utility. We found that models trained using a wider variety of augmentations were indeed more robust to AugLy augmentations, which validates the hypothesis that training on augmented data improves robustness against adversarial attacks.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_roddaself-supervisedapproachforrobustout-of-distributiondetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection",
    "authors": [
      "Umar Khalid",
      "Ashkan Esmaeili",
      "Nazmul Karim",
      "Nazanin Rahnavard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Khalid_RODD_A_Self-Supervised_Approach_for_Robust_Out-of-Distribution_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Khalid_RODD_A_Self-Supervised_Approach_for_Robust_Out-of-Distribution_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent studies have started to address the concern of detecting and rejecting the out-of-distribution (OOD) samples as a major challenge in the safe deployment of deep learning (DL) models. It is desired that the DL model should only be confident about the in-distribution (ID) data which re-inforces the driving principle of the OOD detection. In this paper, we propose a simple yet effective generalized OOD detection method that is independent of out-of-distribution datasets. Our approach relies on self-supervised feature learning of the training samples, where the embeddings lie on a compact low-dimensional space. Motivated by the recent studies that show self-supervised adversarial contrastive learning helps robustify the model, we empirically show that a pre-trained model with self-supervised contrastive learning yields a better model for uni-dimensional feature learning in the latent space. The method proposed in this work referred to as RODD, out-performs SOTA detection performance on an extensive suite of benchmark datasets on OOD detection tasks. On the CIFAR-100 benchmarks, RODD achieves a 26.97 % lower false-positive rate (FPR@95) compared to the current SOTA method.",
    "code_link": "https://github.com/UmarKhalidcs/RODD"
  },
  "cvpr2022_artofrobust_robustnessandadaptationtohiddenfactorsofvariation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Robustness and Adaptation to Hidden Factors of Variation",
    "authors": [
      "William Paul",
      "Philippe Burlina"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Paul_Robustness_and_Adaptation_to_Hidden_Factors_of_Variation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Paul_Robustness_and_Adaptation_to_Hidden_Factors_of_Variation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We tackle here a specific, still not widely addressed aspect, of AI robustness, which consists of seeking invariance / insensitivity of model performance to hidden factors of variations in the data. Towards this end, we employ a two step strategy that a) does unsupervised discovery, via generative models, of sensitive factors that cause models to under-perform, and b) intervenes models to make their performance invariant to these sensitive factors' influence. We consider 3 separate interventions for robustness, including: data augmentation, semantic consistency, and adversarial alignment. We evaluate our method using metrics that measure trade offs between invariance (insensitivity) and overall performance (utility) and show the benefits of our method for 3 settings (unsupervised, semi-supervised and generalization).",
    "code_link": ""
  },
  "cvpr2022_artofrobust_poisonsthatarelearnedfasteraremoreeffective": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Poisons That Are Learned Faster Are More Effective",
    "authors": [
      "Pedro Sandoval-Segura",
      "Vasu Singla",
      "Liam Fowl",
      "Jonas Geiping",
      "Micah Goldblum",
      "David Jacobs",
      "Tom Goldstein"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Sandoval-Segura_Poisons_That_Are_Learned_Faster_Are_More_Effective_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Sandoval-Segura_Poisons_That_Are_Learned_Faster_Are_More_Effective_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Imperceptible poisoning attacks on entire datasets have recently been touted as methods for protecting data privacy. However, among a number of defenses preventing the practical use of these techniques, early-stopping stands out as a simple, yet effective defense. To gauge poisons' vulnerability to early-stopping, we benchmark error-minimizing, error-maximizing, and synthetic poisons in terms of peak test accuracy over 100 epochs and make a number of surprising observations. First, we find that poisons that reach a low training loss faster have lower peak test accuracy. Second, we find that a current state-of-the-art error-maximizing poison is 7 times less effective when poison training is stopped at epoch 8. Third, we find that stronger, more transferable adversarial attacks do not make stronger poisons. We advocate for evaluating poisons in terms of peak test accuracy.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_privacyleakageofadversarialtrainingmodelsinfederatedlearningsystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Privacy Leakage of Adversarial Training Models in Federated Learning Systems",
    "authors": [
      "Jingyang Zhang",
      "Yiran Chen",
      "Hai Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Zhang_Privacy_Leakage_of_Adversarial_Training_Models_in_Federated_Learning_Systems_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Zhang_Privacy_Leakage_of_Adversarial_Training_Models_in_Federated_Learning_Systems_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Adversarial Training (AT) is crucial for obtaining deep neural networks that are robust to adversarial attacks, yet recent works found that it could also make models more vulnerable to privacy attacks. In this work, we further reveal this unsettling property of AT by designing a novel privacy attack that is practically applicable to the privacy-sensitive Federated Learning (FL) systems. Using our method, the attacker can exploit AT models in the FL system to accurately reconstruct users' private training images even when the training batch size is large. Code is available at https://github.com/zjysteven/PrivayAttack_AT_FL.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_anempiricalstudyofdata-freequantizationstuningrobustness": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "An Empirical Study of Data-Free Quantization's Tuning Robustness",
    "authors": [
      "Hong Chen",
      "Yuxuan Wen",
      "Yifu Ding",
      "Zhen Yang",
      "Yufei Guo",
      "Haotong Qin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Chen_An_Empirical_Study_of_Data-Free_Quantizations_Tuning_Robustness_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Chen_An_Empirical_Study_of_Data-Free_Quantizations_Tuning_Robustness_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep convolutional neural networks are now performing increasingly superior in various fields, while the network parameters are getting massive as the advanced neural networks tend to be deeper. Among various model compression methods, quantization is one of the most potent approaches to compress neural networks by compacting model weights and activations to lower bit-width. The data-free quantization method is also proposed, which is specialized for some privacy and security scenarios and enables quantization without access to real data. In this work, we find that the tuning robustness of existing data-free quantization is flawed, progressing an empirical study and determining some hyperparameter settings that can converge the model stably in the data-free quantization process. Our study aims to evaluate the overall tuning robustness of the current data-free quantization system, which is existing methods are significantly affected by parameter fluctuations in tuning. We also expect data-free quantification methods with tuning robustness to appear in the future.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_corrganinputtransformationtechniqueagainstnaturalcorruptions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "CorrGAN: Input Transformation Technique Against Natural Corruptions",
    "authors": [
      "Mirazul Haque",
      "Christof J. Budnik",
      "Wei Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Haque_CorrGAN_Input_Transformation_Technique_Against_Natural_Corruptions_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Haque_CorrGAN_Input_Transformation_Technique_Against_Natural_Corruptions_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Because of the increasing accuracy of Deep Neural Networks (DNNs) on different tasks, a lot of real times systems are utilizing DNNs. These DNNs are vulnerable to adversarial perturbations and corruptions. Specifically, natural corruptions like fog, blur, contrast etc can affect the prediction of DNN in an autonomous vehicle. In real time, these corruptions are needed to be detected and also the corrupted inputs are needed to be de-noised to be predicted correctly. In this work, we propose CorrGAN approach, which can generate benign input when a corrupted input is provided. In this framework, we train Generative Adversarial Network (GAN) with novel intermediate output-based loss function. The GAN can denoise the corrupted input and generate benign input. Through experimentation, we show that up to 75.2% of the corrupted misclassified inputs can be classified correctly by DNN using CorrGAN.",
    "code_link": ""
  },
  "cvpr2022_artofrobust_generalizingadversarialexplanationswithgrad-cam": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ArtOfRobust",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - The Art of Robustness: Devil and Angel in Adversarial Machine Learning",
    "title": "Generalizing Adversarial Explanations With Grad-CAM",
    "authors": [
      "Tanmay Chakraborty",
      "Utkarsh Trehan",
      "Khawla Mallat",
      "Jean-Luc Dugelay"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Chakraborty_Generalizing_Adversarial_Explanations_With_Grad-CAM_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/papers/Chakraborty_Generalizing_Adversarial_Explanations_With_Grad-CAM_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Gradient-weighted Class Activation Mapping (Grad- CAM), is an example-based explanation method that provides a gradient activation heat map as an explanation for Convolution Neural Network (CNN) models. The drawback of this method is that it cannot be used to generalize CNN behaviour. In this paper, we present a novel method that extends Grad-CAM from example-based explanations to a method for explaining global model behaviour. This is achieved by introducing two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in Dissimilarity (VID), for model generalization. These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set. For our experiment, we study adversarial attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM). We then compute the metrics MOD and VID for the automatic face recognition (AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks. The proposed method can be used to understand adversarial attacks and explain the behaviour of black box CNN models for image analysis.",
    "code_link": ""
  },
  "cvpr2022_pbvs_variationalautoencodersforgeneratinghyperspectralimaginghoneyadulterationdata": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Variational Autoencoders for Generating Hyperspectral Imaging Honey Adulteration Data",
    "authors": [
      "Tessa Phillips",
      "Waleed Abdulla"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Phillips_Variational_Autoencoders_for_Generating_Hyperspectral_Imaging_Honey_Adulteration_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Phillips_Variational_Autoencoders_for_Generating_Hyperspectral_Imaging_Honey_Adulteration_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Honey fraud and adulteration are an increasing concern globally. Hyperspectral imaging and machine learning can detect adulterated honey within a known set of honey, where we have captured data at different sugar concentrations. Previous work in this area has used a minimal number of honey types, as sample preparation and data capture is a time-consuming process. This paper develops a new approach using variational autoencoders (VAEs) for generating adulterated honey data for unseen honey types. The results show that the binary adulteration detector can achieve on average 81.3% accuracy on unseen honey types by adding the generated data to the existing training data. Without including the generated data while training, the classifier can only achieve 44% on unseen honey types.",
    "code_link": ""
  },
  "cvpr2022_pbvs_multipleobjectdetectionandtrackinginthethermalspectrum": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Multiple Object Detection and Tracking in the Thermal Spectrum",
    "authors": [
      "Wassim A. El Ahmar",
      "Dhanvin Kolhatkar",
      "Farzan Erlik Nowruzi",
      "Hamzah AlGhamdi",
      "Jonathan Hou",
      "Robert Laganiere"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Ahmar_Multiple_Object_Detection_and_Tracking_in_the_Thermal_Spectrum_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Ahmar_Multiple_Object_Detection_and_Tracking_in_the_Thermal_Spectrum_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multiple Object Tracking (MOT) is an integral part of machine vision research. Most tracking-by-detection based MOT solutions utilize video streams from RGB cameras for their operation. However, for real-world applications, it is necessary to utilize sensors that operate in different spectrums to accommodate for varying lighting conditions. Since object detection is the first step of the tracking pipeline in tracking-by-detection approaches, we compare the performance of state-of-the-art object detectors when trained on color images to their performance when trained on thermal images. We introduce a new dataset for multiple object tracking with thermal images and corresponding RGB images and show that state-of-the-art trackers perform better on thermal images, especially in poor lighting conditions. Finally, we propose the use of a dynamic cut-off threshold for tracking-by-detection approaches that factors the size of a predicted box to enhance the tracker association. Our dataset and source code are publicly available at https://github.com/wassimea/thermalMOT.",
    "code_link": "https://github.com/wassimea/thermalMOT"
  },
  "cvpr2022_pbvs_deepneuralnetworkwithwalsh-hadamardtransformlayerforemberdetectionduringawildfire": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Deep Neural Network With Walsh-Hadamard Transform Layer for Ember Detection During a Wildfire",
    "authors": [
      "Hongyi Pan",
      "Diaa Badawi",
      "Chang Chen",
      "Adam Watts",
      "Erdem Koyuncu",
      "Ahmet Enis Cetin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Pan_Deep_Neural_Network_With_Walsh-Hadamard_Transform_Layer_for_Ember_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Pan_Deep_Neural_Network_With_Walsh-Hadamard_Transform_Layer_for_Ember_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this article, we describe an ember detection method in infrared (IR) video. Embers, also called firebrands, can act as wildfire super-spreaders. We develop a novel neural network with a Walsh-Hadamard Transform (WHT) layer to process the IR video. The WHT layer is used to process the temporal dimension of the video data to model the high-frequency activity due to ember movements. We insert the WHT layer to ResNet-18 and obtained higher accuracy compared to the standard single slice ResNet-18 and the ResNet-18 processing the entire video block. We also repeat the experiments on ResNet-34, but we found that ResNet-18 is sufficient for this task. Therefore, we choose the ResNet-18 with the WHT layer as the proposed model.",
    "code_link": ""
  },
  "cvpr2022_pbvs_unsupervisedanomalydetectionfromtime-of-flightdepthimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Unsupervised Anomaly Detection From Time-of-Flight Depth Images",
    "authors": [
      "Pascal Schneider",
      "Jason Rambach",
      "Bruno Mirbach",
      "Didier Stricker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Schneider_Unsupervised_Anomaly_Detection_From_Time-of-Flight_Depth_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Schneider_Unsupervised_Anomaly_Detection_From_Time-of-Flight_Depth_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Video anomaly detection (VAD) addresses the problem of automatically finding anomalous events in video data. The primary data modalities on which current VAD systems work on are monochrome or RGB images. Using depth data in this context instead is still hardly explored in spite of depth images being a popular choice in many other computer vision research areas and the increasing availability of inexpensive depth camera hardware. We evaluate the application of existing autoencoder-based methods on depth video and propose how the advantages of using depth data can be leveraged by integration into the loss function. Training is done unsupervised using normal sequences without need for any additional annotations. We show that depth allows easy extraction of auxiliary information for scene analysis in the form of a foreground mask and demonstrate its beneficial effect on the anomaly detection performance through evaluation on a large public dataset, for which we are also the first ones to present results on.",
    "code_link": ""
  },
  "cvpr2022_pbvs_semi-supervisedhyperspectralobjectdetectionchallengeresults-pbvs2022": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Semi-Supervised Hyperspectral Object Detection Challenge Results - PBVS 2022",
    "authors": [
      "Aneesh Rangnekar",
      "Zachary Mulhollan",
      "Anthony Vodacek",
      "Matthew Hoffman",
      "Angel D. Sappa",
      "Erik Blasch",
      "Jun Yu",
      "Liwen Zhang",
      "Shenshen Du",
      "Hao Chang",
      "Keda Lu",
      "Zhong Zhang",
      "Fang Gao",
      "Ye Yu",
      "Feng Shuang",
      "Lei Wang",
      "Qiang Ling",
      "Pranjay Shyam",
      "Kuk-Jin Yoon",
      "Kyung-Soo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Rangnekar_Semi-Supervised_Hyperspectral_Object_Detection_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Rangnekar_Semi-Supervised_Hyperspectral_Object_Detection_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper summarizes the top contributions to the first semi-supervised hyperspectral object detection (SSHOD) challenge, which was organized as a part of the Perception Beyond the Visible Spectrum (PBVS) 2022 workshop at the Computer Vision and Pattern Recognition (CVPR) conference. The SSHOD challenge is a first-of-its-kind hyperspectral dataset with temporally contiguous frames collected from a university rooftop observing a 4-way vehicle intersection over a period of three days. The dataset contains a total of 2890 frames, captured at an average resolution of 1600 x 192 pixels, with 51 hyperspectral bands from 400nm to 900nm. SSHOD challenge uses 989 images as the training set, 605 images as validation set and 1296 images as the evaluation (test) set. Each set was acquired on a different day to maximize the variance in weather conditions. Labels are provided for 10% of the annotated data, hence formulating a semi-supervised learning task for the participants which is evaluated in terms of average precision over the entire set of classes, as well as individual moving object classes: namely vehicle, bus and bike. The challenge received participation registration from 38 individuals, with 8 participating in the validation phase and 3 participating in the test phase. This paper describes the dataset acquisition, with challenge formulation, proposed methods and qualitative and quantitative results.",
    "code_link": ""
  },
  "cvpr2022_pbvs_gaf-naugramianangularfieldencodedneighborhoodattentionu-netforpixel-wisehyperspectralimageclassification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "GAF-NAU: Gramian Angular Field Encoded Neighborhood Attention U-Net for Pixel-Wise Hyperspectral Image Classification",
    "authors": [
      "Sidike Paheding",
      "Abel A. Reyes",
      "Anush Kasaragod",
      "Thomas Oommen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Paheding_GAF-NAU_Gramian_Angular_Field_Encoded_Neighborhood_Attention_U-Net_for_Pixel-Wise_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Paheding_GAF-NAU_Gramian_Angular_Field_Encoded_Neighborhood_Attention_U-Net_for_Pixel-Wise_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Hyperspectral image (HSI) classification is the most vibrant area of research in the hyperspectral community due to the rich spectral information contained in HSI can greatly aid in identifying objects of interest. However, inherent non-linearity between materials and the corresponding spectral profiles brings two major challenges in HSI classification: interclass similarity and intraclass variability. Many advanced deep learning methods have attempted to address these issues from the perspective of a region/patch-based approach, instead of a pixel-based alternate. However, the patch-based approaches hypothesize that neighborhood pixels of a target pixel in a fixed spatial window belong to the same class. And this assumption is not always true. To address this problem, we herein propose a new deep learning architecture, namely Gramian Angular Field encoded Neighborhood Attention U-Net (GAF-NAU), for pixel-based HSI classification. The proposed method does not require regions or patches centered around a raw target pixel to perform 2D-CNN based classification, instead, our approach transforms 1D pixel vector in HSI into 2D angular feature space using Gramian Angular Field (GAF) and then embed it to a new neighborhood attention network to suppress irrelevant angular feature while emphasizing on pertinent features useful for HSI classification task. Evaluation results on three publicly available HSI datasets demonstrate the superior performance of the proposed model.",
    "code_link": ""
  },
  "cvpr2022_pbvs_hsi-guidedintrinsicimagedecompositionforoutdoorscenes": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "HSI-Guided Intrinsic Image Decomposition for Outdoor Scenes",
    "authors": [
      "Fan Zhang",
      "Shaodi You",
      "Yu Li",
      "Ying Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Zhang_HSI-Guided_Intrinsic_Image_Decomposition_for_Outdoor_Scenes_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Zhang_HSI-Guided_Intrinsic_Image_Decomposition_for_Outdoor_Scenes_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Intrinisic image decomposition (IID) aims to recover the reflectance and shading components from images and is the prerequisite to many downstream computer vision applications, such as image editing and image relighting. Due to the inherent difficulty in acquiring ground truth reflectance and shading, existing datasets are either synthetic indoor scenes or objects using graphics rendering (e.g., CGIntrinsics and ShapeNet etc.) or real photos with very sparse manual annotation (e.g., IIW and SAW etc.). Accompanied with the complex nature of outdoor scenes, most IID methods focus on the decomposition of indoor environment. There is still a long way to go before we can handle IID of outdoor scenes. In this paper, we take the attempt to perform intrinsic image decomposition for outdoor scenes when RGB image is not the only thing we can get from the enviroment. With the observation of prior work where nir-infrared (NIR) images are transparent to a range of colourants/dyes, we propose to extend it to more spectra by collecting hyperspectral imaging (HSI) data which are well aligned with RGB images and to perform IID with both of them. We also apply existing mainstream IID methods for comparison to examine current progress and challenges at the road towards IID outdoors. We still make some improvements and find problems when performing IID for outdoor scenes, even though we do not handle it perfectly. The data we collect will be made publicly available for further potential investigation.",
    "code_link": ""
  },
  "cvpr2022_pbvs_augmentationofatmosphericturbulenceeffectsonthermaladaptedobjectdetectionmodels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Augmentation of Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models",
    "authors": [
      "Engin Uzun",
      "Ahmet An\u0131l Dursun",
      "Erdem Akag\u00fcnd\u00fcz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Uzun_Augmentation_of_Atmospheric_Turbulence_Effects_on_Thermal_Adapted_Object_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Uzun_Augmentation_of_Atmospheric_Turbulence_Effects_on_Thermal_Adapted_Object_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Atmospheric turbulence has a degrading effect on the image quality of long-range observation systems. As a result of various elements such as temperature, wind velocity, humidity, etc., turbulence is characterized by random fluctuations in the refractive index of the atmosphere. It is a phenomenon that may occur in various imaging spectra such as the visible or the infrared bands. In this paper, we analyze the effects of atmospheric turbulence on object detection performance in thermal imagery. We use a geometric turbulence model to simulate turbulence effects on a medium-scale thermal image set, namely \"FLIR ADAS v2\". We apply thermal domain adaptation to state-of-the-art object detectors and propose a data augmentation strategy to increase the performance of object detectors which utilizes turbulent images in different severity levels as training data. Our results show that the proposed data augmentation strategy yields an increase in performance for both turbulent and non-turbulent thermal test images.",
    "code_link": ""
  },
  "cvpr2022_pbvs_semanticsegmentationforthermalimagesacomparativesurvey": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Semantic Segmentation for Thermal Images: A Comparative Survey",
    "authors": [
      "Z\u00fclfiye K\u00fct\u00fck",
      "G\u00f6rkem Algan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Kutuk_Semantic_Segmentation_for_Thermal_Images_A_Comparative_Survey_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Kutuk_Semantic_Segmentation_for_Thermal_Images_A_Comparative_Survey_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Semantic segmentation is a challenging task since it requires excessively more low-level spatial information of the image compared to other computer vision problems. The accuracy of pixel-level classification can be affected by many factors, such as imaging limitations and the ambiguity of object boundaries in an image. Conventional methods exploit three-channel RGB images captured in the visible spectrum with deep neural networks (DNN). Thermal images can significantly contribute during the segmentation since thermal imaging cameras are capable of capturing details despite the weather and illumination conditions. Using infrared spectrum in semantic segmentation has many real-world use cases, such as autonomous driving, medical imaging, agriculture, defense industry, etc. Due to this wide range of use cases, designing accurate semantic segmentation algorithms with the help of infrared spectrum is an important challenge. One approach is to use both visible and infrared spectrum images as inputs. These methods can accomplish higher accuracy due to enriched input information, with the cost of extra effort for the alignment and processing of multiple inputs. Another approach is to use only thermal images, enabling less hardware cost for smaller use cases. Even though there are multiple surveys on semantic segmentation methods, the literature lacks a comprehensive survey centered explicitly around semantic segmentation using infrared spectrum. This work aims to fill this gap by presenting algorithms in the literature and categorizing them by their input images.",
    "code_link": ""
  },
  "cvpr2022_pbvs_atwo-stageshake-shakenetworkforlong-tailedrecognitionofsaraerialviewobjects": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Two-Stage Shake-Shake Network for Long-Tailed Recognition of SAR Aerial View Objects",
    "authors": [
      "Gongzhe Li",
      "Linpeng Pan",
      "Linwei Qiu",
      "Zhiwen Tan",
      "Fengying Xie",
      "Haopeng Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Li_A_Two-Stage_Shake-Shake_Network_for_Long-Tailed_Recognition_of_SAR_Aerial_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Li_A_Two-Stage_Shake-Shake_Network_for_Long-Tailed_Recognition_of_SAR_Aerial_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Synthetic Aperture Radar (SAR) has received more attention due to its complementary superiority on capturing significant information in the remote sensing area. However, for an Aerial View Object Classification (AVOC) task, SAR images still suffer from the long-tailed distribution of the aerial view objects. This disparity dampens the performance of classification methods, especially for the data-sensitive deep learning models. In this paper, we propose a two-stage shake-shake network to tackle the long-tailed learning problem. Specifically, it decouples the learning procedure into the representation learning stage and the classification learning stage. Moreover, we apply the test time augmentation (TTA) and a post-processing approach (CAN) to improve the accuracy. In the PBVS 2022 Multi-modal Aerial View Object Classification Challenge Track 1, our method achieves 21.82% and 27.97% accuracy in the development phase and testing phase respectively, which achieves the top-tier among all the participants.",
    "code_link": "https://github.com/LinpengPan/PBVS2022-Multimodal-AVOC-Challenge-Track1"
  },
  "cvpr2022_pbvs_multi-modalaerialviewobjectclassificationchallengeresults-pbvs2022": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Multi-Modal Aerial View Object Classification Challenge Results - PBVS 2022",
    "authors": [
      "Spencer Low",
      "Oliver Nina",
      "Angel D. Sappa",
      "Erik Blasch",
      "Nathan Inkawhich"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Low_Multi-Modal_Aerial_View_Object_Classification_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Low_Multi-Modal_Aerial_View_Object_Classification_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper details the results and main findings of the second iteration of the Multi-modal Aerial View Object Classification (MAVOC) challenge. The primary goal of both MAVOC challenges is to inspire research into methods for building recognition models that utilize both synthetic aperture radar (SAR) and electro-optical (EO) imagery. Teams are encouraged to develop multi-modal approaches that incorporate complementary information from both domains. While the 2021 challenge showed a proof of concept that both modalities could be used together, the 2022 challenge focuses on the detailed multi-modal methods. The 2022 challenge uses the same UNIfied COincident Optical and Radar for recognitioN (UNICORN) dataset and competition format that was used in 2021. Specifically, the challenge focuses on two tasks, (1) SAR classification and (2) SAR + EO classification. The bulk of this document is dedicated to discussing the top performing methods and describing their performance on our blind test set. Notably, all of the top ten teams outperform a Resnet-18 baseline. For SAR classification, the top team showed a 129% improvement over baseline and an 8% average improvement from the 2021 winner. The top team for SAR + EO classification shows a 165% improvement with a 32% average improvement over 2021.",
    "code_link": "https://github.com/rwightman/pytorch-image-models"
  },
  "cvpr2022_pbvs_depthwiseconvolutionforcompactobjectdetectorinnighttimeimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Depthwise Convolution for Compact Object Detector in Nighttime Images",
    "authors": [
      "Heena Patel",
      "Kalpesh Prajapati",
      "Anjali Sarvaiya",
      "Kishor Upla",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Patel_Depthwise_Convolution_for_Compact_Object_Detector_in_Nighttime_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Patel_Depthwise_Convolution_for_Compact_Object_Detector_in_Nighttime_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite thermal imaging primarily used for night-time surveillance, uniform temperature of object and background makes it difficult to acquire details in the scene being observed and thereby object detection. Further, motion objects exhibit noisy features, and thermal images collected over long distances degrade the spatial resolution of the acquired objects. We present a computationally efficient object detection approach using Depthwise Deep Convolutional Neural Network (DDCNN) for detecting and classifying objects for night-time images under low resolution. The Depthwise Convolution (DC) employed in the proposed object detection algorithm minimises the network's computational complexity resulting in lowest number of training parameters (i.e., 3M) as compared to the other existing stateof-the-art methods such as FRCNN (52M), SSD (24M) and YOLO-v3 (61M) parameters. Further, through introducing novel Tversky and Intersection over Union (IoU) loss functions into the compact architectural design, we improves night-time object detection accuracy. The validity of the proposed model is assessed on numerous datasets such as FLIR, KAIST, MS, and our internal datasets having multiple objects. The experimental results using proposed method indicate both quantitative and qualitative improvements over the recent state-of-the-art methods for nighttime imaging. The proposed approach achieves a mean Average Precision (mAP) of 52.39% and a highest individual object detection accuracy of 72.70% accuracy for cars in nigh-time situations suggesting applications in real-time use cases.",
    "code_link": ""
  },
  "cvpr2022_pbvs_pseudo-labelgenerationandvariousdataaugmentationforsemi-supervisedhyperspectralobjectdetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Pseudo-Label Generation and Various Data Augmentation for Semi-Supervised Hyperspectral Object Detection",
    "authors": [
      "Jun Yu",
      "Liwen Zhang",
      "Shenshen Du",
      "Hao Chang",
      "Keda Lu",
      "Zhong Zhang",
      "Ye Yu",
      "Lei Wang",
      "Qiang Ling"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Yu_Pseudo-Label_Generation_and_Various_Data_Augmentation_for_Semi-Supervised_Hyperspectral_Object_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Yu_Pseudo-Label_Generation_and_Various_Data_Augmentation_for_Semi-Supervised_Hyperspectral_Object_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Semi-supervised learning is a highly researched problem, but existing semi-supervised object detection frameworks are based on RGB images, and existing pre-trained models cannot be used for hyperspectral images. To overcome these difficulties, this paper first select fewer but suitable data augmentation methods to improve the accuracy of the supervised model based on the labeled training set, which is suitable for the characteristics of hyperspectral images. Next, in order to make full use of the unlabeled training set, we generate pseudo-labels with the model trained in the first stage and mix the obtained pseudo-labels with the labeled training set. Then, a large number of strong data augmentation methods are added to make the final model better. We achieve the SOTA, with an AP of 26.35, on the Semi-Supervised Hyperspectral Object Detection Challenge (SSHODC) in the CVPR 2022 Perception Beyond the Visible Spectrum Workshop, and win the first place in this Challenge.",
    "code_link": ""
  },
  "cvpr2022_pbvs_aquaganrestorationofunderwaterimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "AquaGAN: Restoration of Underwater Images",
    "authors": [
      "Chaitra Desai",
      "Badduri Sai Sudheer Reddy",
      "Ramesh Ashok Tabib",
      "Ujwala Patil",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Desai_AquaGAN_Restoration_of_Underwater_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Desai_AquaGAN_Restoration_of_Underwater_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we propose a generative model to restore degraded underwater images considering attenuation coefficients as clue and name it as AquaGAN. Computing the attenuation coefficients as given in revised image formation model demands in-situ measurements. However, in-situ measurements in underwater scenario is infeasible. Towards this, we propose to estimate the attenuation coefficients using learning based methods and use these parameters as clue for restoration of degraded underwater images. Restoration of true colors in underwater scenario is challenging as intensity of light changes with distance. Preserving true colors during restoration by minimizing single objective function may affect the quality of restored image. Towards this, we propose to combine different objective functions for restoration of true colors. We demonstrate the results of restoration on benchmark dataset and compare the results of proposed methodology with state-of-the-art methods both qualitatively and quantitatively.",
    "code_link": ""
  },
  "cvpr2022_pbvs_amultiviewdepth-basedmotioncapturebenchmarkdatasetforhumanmotiondenoisingandenhancementresearch": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Multiview Depth-Based Motion Capture Benchmark Dataset for Human Motion Denoising and Enhancement Research",
    "authors": [
      "Nate Lannan",
      "Le Zhou",
      "Guoliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Lannan_A_Multiview_Depth-Based_Motion_Capture_Benchmark_Dataset_for_Human_Motion_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Lannan_A_Multiview_Depth-Based_Motion_Capture_Benchmark_Dataset_for_Human_Motion_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The field of human motion enhancement is a rapidly expanding field of study in which depth-based motion capture (D-Mocap) is improved to generate a more accurate counterpart for demanding high precision real-world applications. The D-Mocap that is initially generated relies on commercially available SDKs or open source tools to produce the initial skeletal sequence which works best in an ideal front-facing camera setup. This in turn creates a challenging initialization for human motion enhancement when the camera is not positioned in the ideal forward facing position. Currently there are no multiview D-Mocap datasets which have corresponding time-synced and skeleton-matched optical motion capture (Mocap) reference data for view-invariant motion enhancement. We develop a multiview D-Mocap dataset extended from the popular and comprehensive Berkeley MHAD dataset. In addition, we analyze the performance of the D-Mocap data generated through a series of open source tools, highlighting the difficulty and the need to produce robust results in a rear-facing camera setup due to a 21.4% increase in average joint position error over front-facing data. Finally, we analyze the results of some recent human motion enhancement algorithms with regard to a front-facing camera setup versus a rear-facing one.",
    "code_link": ""
  },
  "cvpr2022_pbvs_fromlesstomorespectralsplittingandaggregationnetworkforhyperspectralfacesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "From Less to More: Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution",
    "authors": [
      "Junjun Jiang",
      "Chenyang Wang",
      "Xianming Liu",
      "Kui Jiang",
      "Jiayi Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Jiang_From_Less_to_More_Spectral_Splitting_and_Aggregation_Network_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Jiang_From_Less_to_More_Spectral_Splitting_and_Aggregation_Network_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "High-resolution (HR) hyperspectral face image plays an important role in face related computer vision tasks under uncontrolled conditions, such as low-light environment and spoofing attacks. However, the dense spectral bands of hyperspectral face images come at the cost of limited amount of photons reached a narrow spectral window on average, which greatly reduces the spatial resolution of hyperspectral face images. In this paper, we investigate how to adapt the deep learning techniques to hyperspectral face image super-resolution (HFSR), especially when the training samples are very limited. Benefiting from the amount of spectral bands, in which each band can be seen as an image, we present a spectral splitting and aggregation network (SSANet) for HFSR with limited training samples. In the shallow layers, we split the hyperspectral image into different spectral groups. Then, we gradually aggregate the neighbor bands at deeper layers to exploit spectral correlations. By this spectral splitting and aggregation strategy (SSAS), we can divide the original hyperspectral image into multiple samples (from less to more) to support the efficient training of the network and effectively exploit the spectral correlations among spectrum. To cope with the challenge of small training sample size (S3) problem, we propose to expand the training samples by a self-representation model and symmetry-induced augmentation. Experiments show that SSANet can well model the joint correlations of spatial and spectral information. By expanding the training samples, SSANet can effectively alleviate the S3 problem. %The comparison results demonstrate that our proposed method can outperform the state-of-the-arts.",
    "code_link": ""
  },
  "cvpr2022_pbvs_cross-modalimagesynthesiswithindual-energyx-raysecurityimagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Cross-Modal Image Synthesis Within Dual-Energy X-Ray Security Imagery",
    "authors": [
      "Brian K. S. Isaac-Medina",
      "Neelanjan Bhowmik",
      "Chris G. Willcocks",
      "Toby P. Breckon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Isaac-Medina_Cross-Modal_Image_Synthesis_Within_Dual-Energy_X-Ray_Security_Imagery_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Isaac-Medina_Cross-Modal_Image_Synthesis_Within_Dual-Energy_X-Ray_Security_Imagery_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Dual-energy X-ray scanners are used for aviation security screening given their capability to discriminate materials inside passenger baggage. To facilitate manual operator inspection, a pseudo-colouring is assigned to the effective composition of the material. Recently, paired image to image translation models based on conditional Generative Adversarial Networks (cGAN) have shown to be effective for image colourisation. In this work, we investigate the use of such a model to translate from the raw X-ray energy responses (high, low, effective-Z) to the pseudo-coloured images and vice versa. Specifically, given N X-ray modalities, we train a cGAN conditioned in N-m domains to generate the remaining m representations. Our method achieves a mean squared error (MSE) of 16.5 and a structural similarity index (SSIM) of 0.9815 when using the raw modalities to generate the pseudo-colour representation. Additionally, raw X-ray high energy, low energy and effective-Z projections were generated given the pseudo-colour image with minimum MSE of 2.57, 5.63 and 1.43, and maximum SSIM of 0.9953, 0.9901 and 0.9921. Furthermore, we assess the quality of our synthesised pseudo-colour reconstructions by measuring the performance of two object detection models originally trained on real X-ray pseudo-colour images over our generated pseudo-colour images. Interestingly, our generated pseudo-colour images obtain marginally improved detection performance than the corresponding real X-ray pseudo-colour images, showing that meaningful representations are synthesized and that these reconstructions are applicable for differing aviation security tasks.",
    "code_link": ""
  },
  "cvpr2022_pbvs_thermalimagesuper-resolutionchallengeresults-pbvs2022": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Thermal Image Super-Resolution Challenge Results - PBVS 2022",
    "authors": [
      "Rafael E. Rivadeneira",
      "Angel D. Sappa",
      "Boris X. Vintimilla",
      "Jin Kim",
      "Dogun Kim",
      "Zhihao Li",
      "Yingchun Jian",
      "Bo Yan",
      "Leilei Cao",
      "Fengliang Qi",
      "Hongbin Wang",
      "Rongyuan Wu",
      "Lingchen Sun",
      "Yongqiang Zhao",
      "Lin Li",
      "Kai Wang",
      "Yicheng Wang",
      "Xuanming Zhang",
      "Huiyuan Wei",
      "Chonghua Lv",
      "Qigong Sun",
      "Xiaolin Tian",
      "Zhuang Jia",
      "Jiakui Hu",
      "Chenyang Wang",
      "Zhiwei Zhong",
      "Xianming Liu",
      "Junjun Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents results from the third Thermal Image Super-Resolution (TISR) challenge organized in the Perception Beyond the Visible Spectrum (PBVS) 2022 workshop. The challenge uses the same thermal image dataset as the first two challenges, with 951 training images and 50 validation images at each resolution. A set of 20 images was kept aside for testing. The evaluation tasks were to measure the PSNR and SSIM between the SR image and the ground truth (HR thermal noisy image downsampled by four), and also to measure the PSNR and SSIM between the SR image and the semi-registered HR image (acquired with another camera). The results outperformed those from last year's challenge, improving both evaluation metrics. This year, almost 100 teams participants registered for the challenge, showing the community's interest in this hot topic.",
    "code_link": ""
  },
  "cvpr2022_pbvs_tmvnetusingtransformersformulti-viewvoxel-based3dreconstruction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "TMVNet: Using Transformers for Multi-View Voxel-Based 3D Reconstruction",
    "authors": [
      "Kebin Peng",
      "Rifatul Islam",
      "John Quarles",
      "Kevin Desai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Peng_TMVNet_Using_Transformers_for_Multi-View_Voxel-Based_3D_Reconstruction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Peng_TMVNet_Using_Transformers_for_Multi-View_Voxel-Based_3D_Reconstruction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Previous research in multi-view 3D reconstruction had used different convolution neural network (CNN) architectures to obtain a 3D voxel representation. Even though CNN works well, they have limitations in exploiting the long-range dependencies in sequence transduction tasks such as multi-view 3D reconstruction. In this paper, we propose TMVNet -- a two-layer transformer encoder that can better use long-range dependencies information. In contrast to using a 2D CNN decoder by the previous approaches, our model uses a 3D CNN encoder to capture the relations between the voxels in the 3D space. Also, our proposed 3D feature fusion network aggregates 3D position feature from CNN and long-range dependencies feature from transformer together. The proposed TMVNet is trained and tested on the ShapeNet dataset. Comparison against ten state-of-the-art multi-view 3D reconstruction methods and the reported quantitative and qualitative results showcase the superiority of our method.",
    "code_link": ""
  },
  "cvpr2022_pbvs_cippsrnetacamerainternalparametersperceptionnetworkbasedcontrastivelearningforthermalimagesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "CIPPSRNet: A Camera Internal Parameters Perception Network Based Contrastive Learning for Thermal Image Super-Resolution",
    "authors": [
      "Kai Wang",
      "Qigong Sun",
      "Yicheng Wang",
      "Huiyuan Wei",
      "Chonghua Lv",
      "Xiaolin Tian",
      "Xu Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Wang_CIPPSRNet_A_Camera_Internal_Parameters_Perception_Network_Based_Contrastive_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Wang_CIPPSRNet_A_Camera_Internal_Parameters_Perception_Network_Based_Contrastive_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Thermal Image Super-Resolution (TISR) is a technique for converting Low-Resolution (LR) thermal images to High-Resolution (HR) thermal images. This technique has recently become a research hotspot due to its ability to reduce sensor costs and improve visual perception. However, current research does not provide an effective solution for multi-sensor data training, possibly driven by pixel mismatch and simple degradation setting issues. In this paper, we proposed a Camera Internal Parameters Perception Network (CIPPSRNet) for LR thermal image enhancement. The camera internal parameters (CIP) were explicitly modeled as a feature representation, the LR features were transformed into the intermediate domain containing the internal parameters information by perceiving CIP representation. The mapping between the intermediate domain and the spatial domain of the HR features was learned by CIPPSRNet. In addition, we introduced contrastive learning to optimize the pretrained Camera Internal Parameters Representation Network and the feature encoders. Our proposed network is capable of achieving a more efficient transformation from the LR to the HR domains. Additionally, the use of contrastive learning can improve the network's adaptability to misalignment data with insufficient pixel matching and its robustness. Experiments on PBVS2022 TISR Dataset show that our network has achieved state-of-the-art performance for the Thermal SR task.",
    "code_link": ""
  },
  "cvpr2022_pbvs_lidarpositioningforindoorprecisionnavigation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Lidar Positioning for Indoor Precision Navigation",
    "authors": [
      "Max Holmberg",
      "Oskar Karlsson",
      "Michael Tulldahl"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Holmberg_Lidar_Positioning_for_Indoor_Precision_Navigation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Holmberg_Lidar_Positioning_for_Indoor_Precision_Navigation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Lidar based simultaneous localization and mapping methods can be adapted for deployment on small autonomous vehicles operating in unmapped indoor environments. For this purpose, we propose a method which combines inertial data, low-drift lidar odometry, planar primitives, and loop closing in a graph-based structure. The accuracy of our method is experimentally evaluated, using a high-resolution lidar, and compared to the state-of-the-art methods LIO-SAM and Cartographer. We specifically address the lateral positioning accuracy when passing through narrow openings, where high accuracy is a prerequisite for safe operation of autonomous vehicles. The test cases include doorways, slightly wider reference passages, and a larger corridor environment. We observe a reduced lateral accuracy for all three methods when passing through the narrow openings compared to operation in larger spaces. Compared to state-of-the-art, our method shows better results in the narrow passages, and comparable results in the other environments with reasonably low usage of CPU and memory resources.",
    "code_link": ""
  },
  "cvpr2022_pbvs_3drrdbsuperresolutionofmultipleremotesensingimagesusing3dresidualinresidualdenseblocks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "3DRRDB: Super Resolution of Multiple Remote Sensing Images Using 3D Residual in Residual Dense Blocks",
    "authors": [
      "Mohamed Ramzy Ibrahim",
      "Robert Benavente",
      "Felipe Lumbreras",
      "Daniel Ponsa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Ibrahim_3DRRDB_Super_Resolution_of_Multiple_Remote_Sensing_Images_Using_3D_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Ibrahim_3DRRDB_Super_Resolution_of_Multiple_Remote_Sensing_Images_Using_3D_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The rapid advancement of Deep Convolutional Neural Networks helped in solving many remote sensing problems, especially the problems of super-resolution. However, most state-of-the-art methods focus more on Single Image Super-Resolution neglecting Multi-Image Super-Resolution. In this work, a new proposed 3D Residual in Residual Dense Blocks model (3DRRDB) focuses on remote sensing Multi-Image Super-Resolution for two different single spectral bands. The proposed 3DRRDB model explores the idea of 3D convolution layers in deeply connected Dense Blocks and the effect of local and global residual connections with residual scaling in Multi-Image Super-Resolution. The model tested on the Proba-V challenge dataset shows a significant improvement above the current state-of-the-art models scoring a Corrected Peak Signal to Noise Ratio (cPSNR) of 48.79 dB and 50.83 dB for Near Infrared (NIR) and RED Bands respectively. Moreover, the proposed 3DRRDB model scores a Corrected Structural Similarity Index Measure (cSSIM) of 0.9865 and 0.9909 for NIR and RED bands respectively.",
    "code_link": ""
  },
  "cvpr2022_pbvs_actaractor-drivenposeembeddingsforvideoactionrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "ActAR: Actor-Driven Pose Embeddings for Video Action Recognition",
    "authors": [
      "Soufiane Lamghari",
      "Guillaume-Alexandre Bilodeau",
      "Nicolas Saunier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Lamghari_ActAR_Actor-Driven_Pose_Embeddings_for_Video_Action_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Lamghari_ActAR_Actor-Driven_Pose_Embeddings_for_Video_Action_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human action recognition (HAR) in videos is one of the core tasks of video understanding. Based on video sequences, the goal is to recognize actions performed by humans. While HAR has received much attention in the visible spectrum, action recognition in infrared videos is little studied. Accurate recognition of human actions in the infrared domain is a highly challenging task because of the redundant and indistinguishable texture features present in the sequence. Furthermore, in some cases, challenges arise from the irrelevant information induced by the presence of multiple active persons not contributing to the actual action of interest. Therefore, most existing methods consider a standard paradigm that does not take into account these challenges, which is in some part due to the ambiguous definition of the recognition task in some cases. In this paper, we propose a new method that simultaneously learns to recognize efficiently human actions in the infrared spectrum, while automatically identifying the key-actors performing the action without using any prior knowledge or explicit annotations. Our method is composed of three stages. In the first stage, optical flow-based key-actor identification is performed. Then for each key-actor, we estimate key-poses that will guide the frame selection process. A scale-invariant encoding process along with embedded pose filtering are performed in order to enhance the quality of action representations. Experimental results on InfAR dataset show that our proposed model achieves promising recognition performance and learns useful action representations.",
    "code_link": ""
  },
  "cvpr2022_pbvs_lostincompressiontheimpactoflossyimagecompressiononvariablesizeobjectdetectionwithininfraredimagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Perception Beyond the Visible Spectrum",
    "title": "Lost in Compression: The Impact of Lossy Image Compression on Variable Size Object Detection Within Infrared Imagery",
    "authors": [
      "Neelanjan Bhowmik",
      "Jack W. Barker",
      "Yona Falinie A. Gaus",
      "Toby P. Breckon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Bhowmik_Lost_in_Compression_The_Impact_of_Lossy_Image_Compression_on_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Bhowmik_Lost_in_Compression_The_Impact_of_Lossy_Image_Compression_on_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Lossy image compression strategies allow for more efficient storage and transmission of data by encoding data to a reduced form. This is essential enable training with larger datasets on less storage-equipped environments. However, such compression can cause severe decline in performance of deep Convolution Neural Network (CNN) architectures even when mild compression is applied and the resulting compressed imagery is visually identical. In this work, we apply the lossy JPEG compression method with six discrete levels of increasing compression95, 75, 50, 15, 10, 5to infrared band (thermal) imagery. Our study quantitatively evaluates the affect that increasing levels of lossy compression has upon the performance of characteristically diverse object detection architectures (Cascade-RCNN, FSAF and Deformable DETR) with respect to varying sizes of objects present in the dataset. When training and evaluating on uncompressed data as a baseline, we achieve maximal mean Average Precision (mAP) of 0.823 with Cascade RCNN across the FLIR dataset, outperforming prior work. The impact of the lossy compression is more extreme at higher compression levels (15, 10, 5) across all three CNN architectures. However, re-training models on lossy compressed imagery notably ameliorated performances for all three CNN models with an average increment of 76% (at higher compression level 5). Additionally, we demonstrate the relative sensitivity of differing object areastiny, small, medium, largewith respect to the compression level. We show that tiny and small objects are more sensitive to compression than medium and large objects. Overall, Cascade R-CNN attains the maximal mAP across most of the object area categories.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022imageinpaintingchallengereport": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Image Inpainting Challenge: Report",
    "authors": [
      "Andr\u00e9s Romero",
      "Angela Castillo",
      "Jose Abril-Nova",
      "Radu Timofte",
      "Ritwik Das",
      "Sanchit Hira",
      "Zhihong Pan",
      "Min Zhang",
      "Baopu Li",
      "Dongliang He",
      "Tianwei Lin",
      "Fu Li",
      "Chengyue Wu",
      "Xianming Liu",
      "Xinying Wang",
      "Yi Yu",
      "Jie Yang",
      "Rengang Li",
      "Yaqian Zhao",
      "Zhenhua Guo",
      "Baoyu Fan",
      "Xiaochuan Li",
      "Runze Zhang",
      "Zeyu Lu",
      "Junqin Huang",
      "Gang Wu",
      "Junjun Jiang",
      "Jiayin Cai",
      "Changlin Li",
      "Xin Tao",
      "Yu-Wing Tai",
      "Xiaoqiang Zhou",
      "Huaibo Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Romero_NTIRE_2022_Image_Inpainting_Challenge_Report_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Romero_NTIRE_2022_Image_Inpainting_Challenge_Report_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image Inpainting has recently become an important research problem due to the rise of generative image synthesis models. While many solutions have been proposed for this problem, it is challenging to establish a testbed due to the different possible types of inpainting masks, e.g., completion mask, expand mask, thick brushes mask, etc. Most inpainting solutions shine on object removal or texture synthesis, while semantic generation is still difficult to achieve. To address these issues, we introduce the first general Image Inpainting Challenge. The target is to develop solutions that can achieve a robust performance across different and challenging masks while generating compelling semantic images. The proposed challenge consists of two tracks: unsupervised image inpainting and semantically-guided image inpainting. For Track 1, the participants were provided with four datasets: FFHQ, Places, ImageNet, and WikiArt, and trained their models to perform a mask-agnostic image inpainting solution. For Track 2, FFHQ and Places only. This report gathers the description and discussion of all solutions that participated in the final stage of the challenge.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022challengeonhighdynamicrangeimagingmethodsandresults": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on High Dynamic Range Imaging: Methods and Results",
    "authors": [
      "Eduardo P\u00e9rez-Pellitero",
      "Sibi Catley-Chandar",
      "Richard Shaw",
      "Ale\u0161 Leonardis",
      "Radu Timofte",
      "Zexin Zhang",
      "Cen Liu",
      "Yunbo Peng",
      "Yue Lin",
      "Gaocheng Yu",
      "Jin Zhang",
      "Zhe Ma",
      "Hongbin Wang",
      "Xiangyu Chen",
      "Xintao Wang",
      "Haiwei Wu",
      "Lin Liu",
      "Chao Dong",
      "Jiantao Zhou",
      "Qingsen Yan",
      "Song Zhang",
      "Weiye Chen",
      "Yuhang Liu",
      "Zhen Zhang",
      "Yanning Zhang",
      "Javen Qinfeng Shi",
      "Dong Gong",
      "Dan Zhu",
      "Mengdi Sun",
      "Guannan Chen",
      "Yang Hu",
      "Haowei Li",
      "Baozhu Zou",
      "Zhen Liu",
      "Wenjie Lin",
      "Ting Jiang",
      "Chengzhi Jiang",
      "Xinpeng Li",
      "Mingyan Han",
      "Haoqiang Fan",
      "Jian Sun",
      "Shuaicheng Liu",
      "Juan Mar\u00edn-Vega",
      "Michael Sloth",
      "Peter Schneider-Kamp",
      "Richard R\u00f6ttger",
      "Chunyang Li",
      "Long Bao",
      "Gang He",
      "Ziyao Xu",
      "Li Xu",
      "Gen Zhan",
      "Ming Sun",
      "Xing Wen",
      "Junlin Li",
      "Jinjing Li",
      "Jinjing Li",
      "Chenghua Li",
      "Chenghua Li",
      "Ruipeng Gang",
      "Ruipeng Gang",
      "Fangya Li",
      "Fangya Li",
      "Chenming Liu",
      "Chenming Liu",
      "Shuang Feng",
      "Fei Lei",
      "Rui Liu",
      "Junxiang Ruan",
      "Tianhong Dai",
      "Wei Li",
      "Zhan Lu",
      "Hengyan Liu",
      "Peian Huang",
      "Guangyu Ren",
      "Yonglin Luo",
      "Chang Liu",
      "Qiang Tu",
      "Sai Ma",
      "Yizhen Cao",
      "Steven Tel",
      "Barthelemy Heyrman",
      "Dominique Ginhac",
      "Chul Lee",
      "Gahyeon Kim",
      "Seonghyun Park",
      "An Gia Vien",
      "Truong Thanh Nhat Mai",
      "Howoon Yoon",
      "Tu Vo",
      "Alexander Holston",
      "Sheir Zaheer",
      "Chan Y. Park"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Perez-Pellitero_NTIRE_2022_Challenge_on_High_Dynamic_Range_Imaging_Methods_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Perez-Pellitero_NTIRE_2022_Challenge_on_High_Dynamic_Range_Imaging_Methods_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reviews the challenge on constrained high dynamic range (HDR) imaging that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2022. This manuscript focuses on the competition set-up, datasets, the proposed methods and their results. The challenge aims at estimating an HDR image from multiple respective low dynamic range (LDR) observations, which might suffer from under- or over-exposed regions and different sources of noise. The challenge is composed of two tracks with an emphasis on fidelity and complexity constraints: In Track 1, participants are asked to optimize objective fidelity scores while imposing a low-complexity constraint (i.e. solutions can not exceed a given number of operations). In Track 2, participants are asked to minimize the complexity of their solutions while imposing a constraint on fidelity scores (i.e. solutions are required to obtain a higher fidelity score than the prescribed baseline). Both tracks use the same data and metrics: Fidelity is measured by means of PSNR with respect to a ground-truth HDR image (computed both directly and with a canonical tonemapping operation), while complexity metrics include the number of operations (measured in GMACs) and runtime (in seconds).",
    "code_link": ""
  },
  "cvpr2022_ntire_blueprintseparableresidualnetworkforefficientimagesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Blueprint Separable Residual Network for Efficient Image Super-Resolution",
    "authors": [
      "Zheyuan Li",
      "Yingqi Liu",
      "Xiangyu Chen",
      "Haoming Cai",
      "Jinjin Gu",
      "Yu Qiao",
      "Chao Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Li_Blueprint_Separable_Residual_Network_for_Efficient_Image_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Li_Blueprint_Separable_Residual_Network_for_Efficient_Image_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent advances in single image super-resolution (SISR) have achieved extraordinary performance, but the computational cost is too heavy to apply in edge devices. To alleviate this problem, many novel and effective solutions have been proposed. Convolutional neural network (CNN) with the attention mechanism has attracted increasing attention due to its efficiency and effectiveness. However, there is still redundancy in the convolution operation. In this paper, we propose Blueprint Separable Residual Network (BSRN) containing two efficient designs. One is the usage of blueprint separable convolution (BSConv), which takes place of the redundant convolution operation. The other is to enhance the model ability by introducing more effective attention modules. The experimental results show that BSRN achieves state-of-the-art performance among existing efficient SR methods. Moreover, a smaller variant of our model BSRN-S won the first place in model complexity track of NTIRE 2022 Efficient SR Challenge. The code is available at https://github.com/xiaom233/BSRN.",
    "code_link": "https://github.com/xiaom233/BSRN"
  },
  "cvpr2022_ntire_ntire2022challengeonstereoimagesuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on Stereo Image Super-Resolution: Methods and Results",
    "authors": [
      "Longguang Wang",
      "Yulan Guo",
      "Yingqian Wang",
      "Juncheng Li",
      "Shuhang Gu",
      "Radu Timofte",
      "Liangyu Chen",
      "Xiaojie Chu",
      "Wenqing Yu",
      "Kai Jin",
      "Zeqiang Wei",
      "Sha Guo",
      "Angulia Yang",
      "Xiuzhuang Zhou",
      "Guodong Guo",
      "Bin Dai",
      "Feiyue Peng",
      "Huaxin Xiao",
      "Shen Yan",
      "Yuxiang Liu",
      "Hanxiao Cai",
      "Pu Cao",
      "Yang Nie",
      "Lu Yang",
      "Qing Song",
      "Xiaotao Hu",
      "Jun Xu",
      "Mai Xu",
      "Junpeng Jing",
      "Xin Deng",
      "Qunliang Xing",
      "Minglang Qiao",
      "Zhenyu Guan",
      "Wenlong Guo",
      "Chenxu Peng",
      "Zan Chen",
      "Junyang Chen",
      "Hao Li",
      "Junbin Chen",
      "Weijie Li",
      "Zhijing Yang",
      "Gen Li",
      "Aijin Li",
      "Lei Sun",
      "Dafeng Zhang",
      "Shizhuo Liu",
      "Jiangtao Zhang",
      "Yanyun Qu",
      "Hao-Hsiang Yang",
      "Zhi-Kai Huang",
      "Wei-Ting Chen",
      "Hua-En Chang",
      "Sy-Yen Kuo",
      "Qiaohui Liang",
      "Jianxin Lin",
      "Yijun Wang",
      "Lianying Yin",
      "Rongju Zhang",
      "Wei Zhao",
      "Peng Xiao",
      "Rongjian Xu",
      "Zhilu Zhang",
      "Wangmeng Zuo",
      "Hansheng Guo",
      "Guangwei Gao",
      "Tieyong Zeng",
      "Huicheng Pi",
      "Shunli Zhang",
      "Joohyeok Kim",
      "HyeonA Kim",
      "Eunpil Park",
      "Jae-Young Sim",
      "Jucai Zhai",
      "Pengcheng Zeng",
      "Yang Liu",
      "Chihao Ma",
      "Yulin Huang",
      "Junying Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wang_NTIRE_2022_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wang_NTIRE_2022_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we summarize the 1st NTIRE challenge on stereo image super-resolution (restoration of rich details in a pair of low-resolution stereo images) with a focus on new solutions and results. This challenge has 1 track aiming at the stereo image super-resolution problem under a standard bicubic degradation. In total, 238 participants were successfully registered, and 21 teams competed in the final testing phase. Among those participants, 20 teams successfully submitted results with PSNR (RGB) scores better than the baseline. This challenge establishes a new benchmark for stereo image SR.",
    "code_link": ""
  },
  "cvpr2022_ntire_renderingnighttimeimageviacascadedcolorandbrightnesscompensation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Rendering Nighttime Image via Cascaded Color and Brightness Compensation",
    "authors": [
      "Zhihao Li",
      "Si Yi",
      "Zhan Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Li_Rendering_Nighttime_Image_via_Cascaded_Color_and_Brightness_Compensation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Li_Rendering_Nighttime_Image_via_Cascaded_Color_and_Brightness_Compensation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image signal processing (ISP) is crucial for camera imaging, and neural networks (NN) solutions are extensively deployed for daytime scenes. The lack of sufficient nighttime image dataset and insights on nighttime illumination characteristics poses a great challenge for high-quality rendering using existing NN ISPs. To tackle it, we first built a high-resolution nighttime RAW-RGB (NR2R) dataset with illumination and tone mapping annotated by expert professionals. Meanwhile, to best capture the characteristics of nighttime illumination light sources, we develop the, a two-stage NN ISP to cascade the compensation of color and brightness attributes. Experiments show that our method has better visual quality compared to traditional ISP pipeline, and is ranked at the second place in the NTIRE 2022 Night Photography Rendering Challenge for two tracks by respective People's and Professional Photographer's choices.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022challengeonperceptualimagequalityassessment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on Perceptual Image Quality Assessment",
    "authors": [
      "Jinjin Gu",
      "Haoming Cai",
      "Chao Dong",
      "Jimmy S. Ren",
      "Radu Timofte",
      "Yuan Gong",
      "Shanshan Lao",
      "Shuwei Shi",
      "Jiahao Wang",
      "Sidi Yang",
      "Tianhe Wu",
      "Weihao Xia",
      "Yujiu Yang",
      "Mingdeng Cao",
      "Cong Heng",
      "Lingzhi Fu",
      "Rongyu Zhang",
      "Yusheng Zhang",
      "Hao Wang",
      "Hongjian Song",
      "Jing Wang",
      "Haotian Fan",
      "Xiaoxia Hou",
      "Ming Sun",
      "Mading Li",
      "Kai Zhao",
      "Kun Yuan",
      "Zishang Kong",
      "Mingda Wu",
      "Chuanchuan Zheng",
      "Marcos V. Conde",
      "Maxime Burchi",
      "Longtao Feng",
      "Tao Zhang",
      "Yang Li",
      "Jingwen Xu",
      "Haiqiang Wang",
      "Yiting Liao",
      "Junlin Li",
      "Kele Xu",
      "Tao Sun",
      "Yunsheng Xiong",
      "Abhisek Keshari",
      "Komal",
      "Sadbhawana Thakur",
      "Vinit Jakhetiya",
      "Badri N Subudhi",
      "Hao-Hsiang Yang",
      "Hua-En Chang",
      "Zhi-Kai Huang",
      "Wei-Ting Chen",
      "Sy-Yen Kuo",
      "Saikat Dutta",
      "Sourya Dipta Das",
      "Nisarg A. Shah",
      "Anil Kumar Tiwari"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Gu_NTIRE_2022_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Gu_NTIRE_2022_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reports on the NTIRE 2022 challenge on perceptual image quality assessment (IQA), held in conjunction with the New Trends in Image Restoration and Enhancement workshop (NTIRE) workshop at CVPR 2022. This challenge is held to address the emerging challenge of IQA by perceptual image processing algorithms. The output images of these algorithms have completely different characteristics from traditional distortions and are included in the PIPAL dataset used in this challenge. This challenge is divided into two tracks, a full-reference IQA track similar to the previous NTIRE IQA challenge and a new track that focuses on the no-reference IQA methods. The challenge has 192 and 179 registered participants for two tracks. In the final testing stage, 7 and 8 participating teams submitted their models and fact sheets. Almost all of them have achieved better results than existing IQA methods, and the winning method can demonstrate state-of-the-art performance.",
    "code_link": ""
  },
  "cvpr2022_ntire_mst++multi-stagespectral-wisetransformerforefficientspectralreconstruction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "MST++: Multi-Stage Spectral-Wise Transformer for Efficient Spectral Reconstruction",
    "authors": [
      "Yuanhao Cai",
      "Jing Lin",
      "Zudi Lin",
      "Haoqian Wang",
      "Yulun Zhang",
      "Hanspeter Pfister",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Cai_MST_Multi-Stage_Spectral-Wise_Transformer_for_Efficient_Spectral_Reconstruction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Cai_MST_Multi-Stage_Spectral-Wise_Transformer_for_Efficient_Spectral_Reconstruction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Existing leading methods for spectral reconstruction (SR) focus on designing deeper or wider convolutional neural networks (CNNs) to learn the end-to-end mapping from the RGB image to its hyperspectral image (HSI). These CNN-based methods achieve impressive restoration performance while showing limitations in capturing the long-range dependencies and self-similarity prior. To cope with this problem, we propose a novel Transformer-based method, Multi-stage Spectral-wise Transformer (MST++), for efficient spectral reconstruction. In particular, we employ Spectral-wise Multi-head Self-attention (S-MSA) that is based on the HSI spatially sparse while spectrally self-similar nature to compose the basic unit, Spectral-wise Attention Block (SAB). Then SABs build up Single-stage Spectral-wise Transformer (SST) that exploits a U-shaped structure to extract multi-resolution contextual information. Finally, our MST++, cascaded by several SSTs, progressively improves the reconstruction quality from coarse to fine. Comprehensive experiments show that our MST++ significantly outperforms other state-of-the-art methods. In the NTIRE 2022 Spectral Reconstruction Challenge, our approach won the First place. Code and pre-trained models are publicly available at https://github.com/caiyuanhao1998/MST-plus-plus.",
    "code_link": ""
  },
  "cvpr2022_ntire_vfhqahigh-qualitydatasetandbenchmarkforvideofacesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution",
    "authors": [
      "Liangbin Xie",
      "Xintao Wang",
      "Honglun Zhang",
      "Chao Dong",
      "Ying Shan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Xie_VFHQ_A_High-Quality_Dataset_and_Benchmark_for_Video_Face_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Xie_VFHQ_A_High-Quality_Dataset_and_Benchmark_for_Video_Face_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Most of the existing video face super-resolution (VFSR) methods are trained and evaluated on VoxCeleb1, which is designed specifically for speaker identification and the frames in this dataset are of low quality. As a consequence, the VFSR models trained on this dataset can not output visual-pleasing results. In this paper, we develop an automatic and scalable pipeline to collect a high-quality video face dataset (VFHQ), which contains over 16, 000 high-fidelity clips of diverse interview scenarios. To verify the necessity of VFHQ, we further conduct experiments and demonstrate that VFSR models trained on our VFHQ dataset can generate results with sharper edges and finer textures than those trained on VoxCeleb1. In addition, we show that the temporal information plays a pivotal role in eliminating video consistency issues as well as further improving visual performance. Based on VFHQ, by analyzing the benchmarking study of several state-of-the-art algorithms under bicubic and blind settings.",
    "code_link": ""
  },
  "cvpr2022_ntire_imdeceptiongroupedinformationdistillingsuper-resolutionnetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "IMDeception: Grouped Information Distilling Super-Resolution Network",
    "authors": [
      "Mustafa Ayazo\u011flu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Ayazoglu_IMDeception_Grouped_Information_Distilling_Super-Resolution_Network_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Ayazoglu_IMDeception_Grouped_Information_Distilling_Super-Resolution_Network_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Single-Image-Super-Resolution (SISR) is a classical computer vision problem that has benefited from the recent advancements in deep learning methods, especially the advancements of convolutional neural networks (CNN). Although state-of-the-art methods improve the performance of SISR on several datasets, direct application of these networks for practical use is still an issue due to heavy computational load. For this purpose, recently, researchers have focused on more efficient and high-performing network structures. Information multi-distilling network (IMDN) is one of the highly efficient SISR networks with high performance and low computational load. IMDN achieves this efficiency with various mechanisms such as Intermediate Information Collection (IIC), working in a global setting, Progressive Refinement Module (PRM), and Contrast Aware Channel Attention (CCA), employed in a local setting. These mechanisms, however, do not equally contribute to the efficiency and performance of IMDN. In this work, we propose the Global Progressive Refinement Module (GPRM) as a less parameter-demanding alternative to the IIC module for feature aggregation. To further decrease the number of parameters and floating point operations per second (FLOPS), we also propose Grouped Information Distilling Blocks (GIDB). Using the proposed structures, we design an efficient SISR network called IMDeception. Experiments reveal that the proposed network performs on par with state-of-the-art models despite having a limited number of parameters and FLOPS. Furthermore, using grouped convolutions as a building block of GIDB increases room for further optimization during deployment. To show its potential, the proposed model was deployed on NVIDIA Jetson Xavier AGX and it has been shown that it can run in real-time on this edge device.",
    "code_link": ""
  },
  "cvpr2022_ntire_bsrtimprovingburstsuper-resolutionwithswintransformerandflow-guideddeformablealignment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "BSRT: Improving Burst Super-Resolution With Swin Transformer and Flow-Guided Deformable Alignment",
    "authors": [
      "Ziwei Luo",
      "Youwei Li",
      "Shen Cheng",
      "Lei Yu",
      "Qi Wu",
      "Zhihong Wen",
      "Haoqiang Fan",
      "Jian Sun",
      "Shuaicheng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Luo_BSRT_Improving_Burst_Super-Resolution_With_Swin_Transformer_and_Flow-Guided_Deformable_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Luo_BSRT_Improving_Burst_Super-Resolution_With_Swin_Transformer_and_Flow-Guided_Deformable_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This work addresses the Burst Super-Resolution (BurstSR) task using a new architecture, which requires restoring a high-quality image from a sequence of noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can significantly improve the capability of extracting inter-frame information and reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin Transformer Blocks and Groups as our main backbone. More specifically, we combine optical flows and deformable convolutions, hence our BSRT can handle misalignment and aggregate the potential texture information in multi-frames more efficiently. In addition, our Transformer-based structure can capture long-range dependency to further improve the performance. The evaluation on both synthetic and real-world tracks demonstrates that our approach achieves a new state-of-the-art in BurstSR task. Further, our BSRT wins the championship in the NTIRE2022 Burst Super-Resolution Challenge.",
    "code_link": "https://github.com/Algolzw/BSRT"
  },
  "cvpr2022_ntire_edge-enhancedfeaturedistillationnetworkforefficientsuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Edge-Enhanced Feature Distillation Network for Efficient Super-Resolution",
    "authors": [
      "Yan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wang_Edge-Enhanced_Feature_Distillation_Network_for_Efficient_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wang_Edge-Enhanced_Feature_Distillation_Network_for_Efficient_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the recently massive development in convolution neural networks, numerous lightweight CNN-based image super-resolution methods have been proposed for practical deployments on edge devices. However, most existing methods focus on one specific aspect: network or loss design, which leads to the difficulty of minimizing the model size. To address the issue, we conclude block devising, architecture searching, and loss design to obtain a more efficient SR structure. In this paper, we proposed an edge-enhanced feature distillation network, named EFDN, to preserve the high-frequency information under constrained resources. In detail, we build an edge-enhanced convolution block based on the existing reparameterization methods. Meanwhile, we propose edge-enhanced gradient loss to calibrate the reparameterized path training. Experimental results show that our edge-enhanced strategies preserve the edge and significantly improve the final restoration quality. Code is available at https://github.com/icandle/EFDN.",
    "code_link": "https://github.com/DawnHH/DLSR-PyTorch"
  },
  "cvpr2022_ntire_ntire2022burstsuper-resolutionchallenge": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Burst Super-Resolution Challenge",
    "authors": [
      "Goutam Bhat",
      "Martin Danelljan",
      "Radu Timofte",
      "Yizhen Cao",
      "Yuntian Cao",
      "Meiya Chen",
      "Xihao Chen",
      "Shen Cheng",
      "Akshay Dudhane",
      "Haoqiang Fan",
      "Ruipeng Gang",
      "Jian Gao",
      "Yan Gu",
      "Jie Huang",
      "Liufeng Huang",
      "Youngsu Jo",
      "Sukju Kang",
      "Salman Khan",
      "Fahad Shahbaz Khan",
      "Yuki Kondo",
      "Chenghua Li",
      "Fangya Li",
      "Jinjing Li",
      "Youwei Li",
      "Zechao Li",
      "Chenming Liu",
      "Shuaicheng Liu",
      "Zikun Liu",
      "Zhuoming Liu",
      "Ziwei Luo",
      "Zhengxiong Luo",
      "Nancy Mehta",
      "Subrahmanyam Murala",
      "Yoonchan Nam",
      "Chihiro Nakatani",
      "Pavel Ostyakov",
      "Jinshan Pan",
      "Ge Song",
      "Jian Sun",
      "Long Sun",
      "Jinhui Tang",
      "Norimichi Ukita",
      "Zhihong Wen",
      "Qi Wu",
      "Xiaohe Wu",
      "Zeyu Xiao",
      "Zhiwei Xiong",
      "Rongjian Xu",
      "Ruikang Xu",
      "Youliang Yan",
      "Jialin Yang",
      "Wentao Yang",
      "Zhongbao Yang",
      "Fuma Yasue",
      "Mingde Yao",
      "Lei Yu",
      "Cong Zhang",
      "Syed Waqas Zamir",
      "Jianxing Zhang",
      "Shuohao Zhang",
      "Zhilu Zhang",
      "Qian Zheng",
      "Gaofeng Zhou",
      "Magauiya Zhussip",
      "Xueyi Zou",
      "Wangmeng Zuo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Bhat_NTIRE_2022_Burst_Super-Resolution_Challenge_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Bhat_NTIRE_2022_Burst_Super-Resolution_Challenge_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Burst super-resolution has received increased attention in recent years due to its applications in mobile photography. By merging information from multiple shifted images of a scene, burst super-resolution aims to recover details which otherwise cannot be obtained using a simple input image. This paper reviews the NTIRE 2022 challenge on burst super-resolution. In the challenge, the participants were tasked with generating a clean RGB image with 4xhigher resolution, given a RAW noisy burst as input. That is, the methods need to perform joint denoising, demosaicking, and super-resolution. The challenge consisted of 2 tracks. Track 1 employed synthetic data, where pixel-accurate high-resolution ground truths are available. Track 2 on the other hand used real-world bursts captured from a handheld camera, along with approximately aligned reference images captured using a DSLR. 14 teams participated in the final testing phase. The top performing methods establish a new state-of-the-art on the burst super-resolution task.",
    "code_link": ""
  },
  "cvpr2022_ntire_efficientimagesuper-resolutionwithcollapsiblelinearblocks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Efficient Image Super-Resolution With Collapsible Linear Blocks",
    "authors": [
      "Li Wang",
      "Dong Li",
      "Lu Tian",
      "Yi Shan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wang_Efficient_Image_Super-Resolution_With_Collapsible_Linear_Blocks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wang_Efficient_Image_Super-Resolution_With_Collapsible_Linear_Blocks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we propose a simple but effective architecture for fast and accurate single image super-resolution. Unlike other compact image super-resolution methods based on hand-crafted designs, we first apply coarse-grained pruning for network acceleration, and then introduce collapsible linear blocks to recover the representative ability of the pruned network. Specifically, each collapsible linear block has a multi-branch topology during training, and can be equivalently replaced with a single convolution in the inference stage. Such decoupling of the training-time and inference-time architecture is implemented via a structural re-parameterization technique, leading to improved representation without introducing extra computation costs. Additionally, we adopt a two-stage training mechanism with progressively larger patch sizes to facilitate the optimization procedure. We evaluate the proposed method on the NTIRE 2022 Efficient Image Super-Resolution Challenge and achieve a good trade-off between latency and accuracy. Particularly, under the condition of limited inference time (<= 49.42ms) and parameter amount (<= 0.894M), our solution obtains the best fidelity results in terms of PSNR, i.e., 29.05dB and 28.75dB on the DIV2K validation and test sets, respectively.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022challengeonnightphotographyrendering": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on Night Photography Rendering",
    "authors": [
      "Egor Ershov",
      "Alex Savchik",
      "Denis Shepelev",
      "Nikola Bani\u0107",
      "Michael S. Brown",
      "Radu Timofte",
      "Karlo Ko\u0161\u010devi\u0107",
      "Michael Freeman",
      "Vasily Tesalin",
      "Dmitry Bocharov",
      "Illya Semenkov",
      "Marko Suba\u0161ic",
      "Sven Lon\u010dari\u0107",
      "Arseniy Terekhin",
      "Shuai Liu",
      "Chaoyu Feng",
      "Hao Wang",
      "Ran Zhu",
      "Yongqiang Li",
      "Lei Lei",
      "Zhihao Li",
      "Si Yi",
      "Ling-Hao Han",
      "Ruiqi Wu",
      "Xin Jin",
      "Chunle Guo",
      "Furkan Kinli",
      "Sami Mente\u015f",
      "Bari\u015f \u00d6zcan",
      "Furkan K\u0131ra\u00e7",
      "Simone Zini",
      "Claudio Rota",
      "Marco Buzzelli",
      "Simone Bianco",
      "Raimondo Schettini",
      "Wei Li",
      "Yipeng Ma",
      "Tao Wang",
      "Ruikang Xu",
      "Fenglong Song",
      "Wei-Ting Chen",
      "Hao-Hsiang Yang",
      "Zhi-Kai Huang",
      "Hua-En Chang",
      "Sy-Yen Kuo",
      "Zhexin Liang",
      "Shangchen Zhou",
      "Ruicheng Feng",
      "Chongyi Li",
      "Xiangyu Chen",
      "Binbin Song",
      "Shile Zhang",
      "Lin Liu",
      "Zhendong Wang",
      "Dohoon Ryu",
      "Hyokyoung Bae",
      "Taesung Kwon",
      "Chaitra Desai",
      "Nikhil Akalwadi",
      "Amogh Joshi",
      "Chinmayee Mandi",
      "Sampada Malagi",
      "Akash Uppin",
      "Sai Sudheer Reddy",
      "Ramesh Ashok Tabib",
      "Ujwala Patil",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Ershov_NTIRE_2022_Challenge_on_Night_Photography_Rendering_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Ershov_NTIRE_2022_Challenge_on_Night_Photography_Rendering_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reviews the NTIRE 2022 challenge on night photography rendering. The challenge solicited solutions that processed RAW camera images captured in night scenes to produce a photo-finished output image encoded in the standard RGB (sRGB) space. Given the subjective nature of this task, the proposed solutions were evaluated based on the mean opinions of viewers asked to judge the visual appearance of the results. Michael Freeman, a world-renowned photographer, further ranked the solutions with the highest mean opinion scores. A total of 13 teams competed in the final phase of the challenge. The proposed methods provided by the participating teams represent state-of-the-art performance in nighttime photography. Results from the various teams can be found here: https://nightimaging.org/",
    "code_link": ""
  },
  "cvpr2022_ntire_fast-n-squeezetowardsreal-timespectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Fast-N-Squeeze: Towards Real-Time Spectral Reconstruction From RGB Images",
    "authors": [
      "Mirko Agarla",
      "Simone Bianco",
      "Marco Buzzelli",
      "Luigi Celona",
      "Raimondo Schettini"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Agarla_Fast-N-Squeeze_Towards_Real-Time_Spectral_Reconstruction_From_RGB_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Agarla_Fast-N-Squeeze_Towards_Real-Time_Spectral_Reconstruction_From_RGB_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present an efficient method for the reconstruction of multispectral information from RGB images, as part of the NTIRE 2022 Spectral Reconstruction Challenge. Given an input image, our method determines a global RGB-to-spectral linear transformation matrix, based on a search through optimal matrices from training images that share low-level features with the input. The resulting spectral signatures are then adjusted by a global scaling factor, determined through a lightweight SqueezeNet-inspired neural network. By combining the efficiency of linear transformation matrices with the data-driven effectiveness of convolutional neural networks, we are able to achieve superior performance than winners of the previous editions of the challenge.",
    "code_link": ""
  },
  "cvpr2022_ntire_imagequalityassessmentwithgradientsiamesenetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Image Quality Assessment With Gradient Siamese Network",
    "authors": [
      "Heng Cong",
      "Lingzhi Fu",
      "Rongyu Zhang",
      "Yusheng Zhang",
      "Hao Wang",
      "Jiarong He",
      "Jin Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Cong_Image_Quality_Assessment_With_Gradient_Siamese_Network_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Cong_Image_Quality_Assessment_With_Gradient_Siamese_Network_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this work, we introduce Gradient Siamese Network (GSN) for image quality assessment. The proposed method is skilled in capturing the gradient features between distorted images and reference images in full-reference image quality assessment(IQA) task. We utilize Central Differential Convolution to obtain both semantic features and detail difference hidden in image pair. Furthermore, spatial attention guides the network to concentrate on regions related to image detail. For the low-level, mid-level and high-level features extracted by the network, we innovatively design a multi-level fusion method to improve the efficiency of feature utilization. In addition to the common mean square error supervision, we further consider the relative distance among batch samples and successfully apply KL divergence loss to the image quality assessment task. We experimented the proposed algorithm GSN on several publicly available datasets and proved its superior performance. Our network won the second place in NTIRE 2022 Perceptual Image Quality Assessment Challenge track 1 Full-Reference.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022challengeonefficientsuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on Efficient Super-Resolution: Methods and Results",
    "authors": [
      "Yawei Li",
      "Kai Zhang",
      "Radu Timofte",
      "Luc Van Gool",
      "Fangyuan Kong",
      "Mingxi Li",
      "Songwei Liu",
      "Zongcai Du",
      "Ding Liu",
      "Chenhui Zhou",
      "Jingyi Chen",
      "Qingrui Han",
      "Zheyuan Li",
      "Yingqi Liu",
      "Xiangyu Chen",
      "Haoming Cai",
      "Yu Qiao",
      "Chao Dong",
      "Long Sun",
      "Jinshan Pan",
      "Yi Zhu",
      "Zhikai Zong",
      "Xiaoxiao Liu",
      "Zheng Hui",
      "Tao Yang",
      "Peiran Ren",
      "Xuansong Xie",
      "Xian-Sheng Hua",
      "Yanbo Wang",
      "Xiaozhong Ji",
      "Chuming Lin",
      "Donghao Luo",
      "Ying Tai",
      "Chengjie Wang",
      "Zhizhong Zhang",
      "Yuan Xie",
      "Shen Cheng",
      "Ziwei Luo",
      "Lei Yu",
      "Zhihong Wen",
      "Qi Wu",
      "Youwei Li",
      "Haoqiang Fan",
      "Jian Sun",
      "Shuaicheng Liu",
      "Yuanfei Huang",
      "Meiguang Jin",
      "Hua Huang",
      "Jing Liu",
      "Xinjian Zhang",
      "Yan Wang",
      "Lingshun Long",
      "Gen Li",
      "Yuanfan Zhang",
      "Zuowei Cao",
      "Lei Sun",
      "Panaetov Alexander",
      "Yucong Wang",
      "Minjie Cai",
      "Li Wang",
      "Lu Tian",
      "Zheyuan Wang",
      "Hongbing Ma",
      "Jie Liu",
      "Chao Chen",
      "Yidong Cai",
      "Jie Tang",
      "Gangshan Wu",
      "Weiran Wang",
      "Shirui Huang",
      "Honglei Lu",
      "Huan Liu",
      "Keyan Wang",
      "Jun Chen",
      "Shi Chen",
      "Yuchun Miao",
      "Zimo Huang",
      "Lefei Zhang",
      "Mustafa Ayazo\u011flu",
      "Wei Xiong",
      "Chengyi Xiong",
      "Fei Wang",
      "Hao Li",
      "Ruimian Wen",
      "Zhijing Yang",
      "Wenbin Zou",
      "Weixin Zheng",
      "Tian Ye",
      "Yuncheng Zhang",
      "Xiangzhen Kong",
      "Aditya Arora",
      "Syed Waqas Zamir",
      "Salman Khan",
      "Munawar Hayat",
      "Fahad Shahbaz Khan",
      "Dandan Gao",
      "Dengwen Zhou",
      "Dengwen Zhou",
      "Qian Ning",
      "Jingzhu Tang",
      "Han Huang",
      "Yufei Wang",
      "Zhangheng Peng",
      "Haobo Li",
      "Wenxue Guan",
      "Shenghua Gong",
      "Xin Li",
      "Jun Liu",
      "Wanjun Wang",
      "Kun Zeng",
      "Hanjiang Lin",
      "Xinyu Chen",
      "Jinsheng Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Li_NTIRE_2022_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Li_NTIRE_2022_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reviews the NTIRE 2022 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The task of the challenge was to super-resolve an input image with a magnification factor of 4 based on pairs of low and corresponding high resolution images. The aim was to design a network for single image super-resolution that achieved improvement of efficiency measured according to several metrics including runtime, parameters, FLOPs, activations, and memory consumption while at least maintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the baseline for efficiency measurement. The challenge had 3 tracks including the main track (runtime), sub-track one (model complexity), and sub-track two (overall performance). In the main track, the practical runtime performance of the submissions was evaluated. The rank of the teams were determined directly by the absolute value of the average runtime on the validation set and test set. In sub-track one, the number of parameters and FLOPs were considered. And the individual rankings of the two metrics were summed up to determine a final ranking in this track. In sub-track two, all of the five metrics mentioned in the description of the challenge including runtime, parameter count, FLOPs, activations, and memory consumption were considered. Similar to sub-track one, the rankings of five metrics were summed up to determine a final ranking. The challenge had 303 registered participants, and 43 teams made valid submissions. They gauge the state-of-the-art in efficient single image super-resolution.",
    "code_link": ""
  },
  "cvpr2022_ntire_exposurecorrectionmodeltoenhanceimagequality": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Exposure Correction Model To Enhance Image Quality",
    "authors": [
      "F. Irem Eyiokur",
      "Dogucan Yaman",
      "Hazim Kemal Ekenel",
      "Alexander Waibel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Eyiokur_Exposure_Correction_Model_To_Enhance_Image_Quality_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Eyiokur_Exposure_Correction_Model_To_Enhance_Image_Quality_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Exposure errors in an image cause a degradation in the contrast and low visibility in the content. In this paper, we address this problem and propose an end-to-end exposure correction model in order to handle both under- and overexposure errors with a single model. Our model contains an image encoder, consecutive residual blocks, and image decoder to synthesize the corrected image. We utilize perceptual loss, feature matching loss, and multi-scale discriminator to increase the quality of the generated image as well as to make the training more stable. The experimental results indicate the effectiveness of proposed model. We achieve the state-of-the-art result on a large-scale exposure dataset. Besides, we investigate the effect of exposure setting of the image on the portrait matting task. We find that under- and overexposed images cause severe degradation in the performance of the portrait matting models. We show that after applying exposure correction with the proposed model, the portrait matting quality increases significantly. https://github.com/yamand16/ExposureCorrection",
    "code_link": "https://github.com/yamand16/ExposureCorrection"
  },
  "cvpr2022_ntire_glamajointspatialandfrequencylossforgeneralimageinpainting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "GLaMa: Joint Spatial and Frequency Loss for General Image Inpainting",
    "authors": [
      "Zeyu Lu",
      "Junjun Jiang",
      "Junqin Huang",
      "Gang Wu",
      "Xianming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Lu_GLaMa_Joint_Spatial_and_Frequency_Loss_for_General_Image_Inpainting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Lu_GLaMa_Joint_Spatial_and_Frequency_Loss_for_General_Image_Inpainting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The purpose of image inpainting is to recover scratches and damaged areas using context information from remaining parts. In recent years, with the development of convolutional neural networks (CNNs), image inpainting task has made great breakthroughs. However, most of the work consider insufficient types of mask, and their performance will drop dramatically when encountering unseen masks. To combat these challenges, we propose a simple yet general method to solve this problem based on the LaMa image inpainting framework, dubbed GLaMa. Our proposed GLaMa can better capture different types of missing information by using more types of masks. By incorporating more degraded images in the training phase, we can expect to enhance the robustness of the model with respect to various masks. In order to yield more reasonable results, we further introduce a frequency-based loss in addition to the traditional spatial reconstruction loss and adversarial loss. In particular, we introduce an effective reconstruction loss both in the spatial and frequency domain to reduce the chessboard effect and ripples in the reconstructed image. Extensive experiments demonstrate that our method can boost the performance for each type of mask on FFHQ, ImageNet, Places2 and WikiArt dataset. The proposed GLaMa was ranked first in terms of PSNR, LPIPS and SSIM in the NTIRE 2022 Image Inpainting Challenge Track 1 Unsupervised.",
    "code_link": ""
  },
  "cvpr2022_ntire_anoddpmanomalydetectionwithdenoisingdiffusionprobabilisticmodelsusingsimplexnoise": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "AnoDDPM: Anomaly Detection With Denoising Diffusion Probabilistic Models Using Simplex Noise",
    "authors": [
      "Julian Wyatt",
      "Adam Leach",
      "Sebastian M. Schmon",
      "Chris G. Willcocks"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Generative models have been shown to provide a powerful mechanism for anomaly detection by learning to model healthy or normal reference data which can subsequently be used as a baseline for scoring anomalies. In this work we consider denoising diffusion probabilistic models (DDPMs) for unsupervised anomaly detection. DDPMs have superior mode coverage over generative adversarial networks (GANs) and higher sample quality than variational autoencoders (VAEs). However, this comes at the expense of poor scalability and increased sampling times due to the long Markov chain sequences required. We observe that within reconstruction-based anomaly detection a full-length Markov chain diffusion is not required. This leads us to develop a novel partial diffusion anomaly detection strategy that scales to high-resolution imagery, named AnoDDPM. A secondary problem is that Gaussian diffusion fails to capture larger anomalies; therefore we develop a multi-scale simplex noise diffusion process that gives control over the target anomaly size. AnoDDPM with simplex noise is shown to significantly outperform both f-AnoGAN and Gaussian diffusion for the tumorous dataset of 22 T1-weighted MRI scans (CCBS Edinburgh) qualitatively and quantitatively (improvement of +25.5% Sorensen-Dice coefficient, +17.6% IoU and +7.4% AUC).",
    "code_link": "https://github.com/JulianWyatt/AnoDDPM"
  },
  "cvpr2022_ntire_dual-domainimagesynthesisusingsegmentation-guidedgan": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Dual-Domain Image Synthesis Using Segmentation-Guided GAN",
    "authors": [
      "Dena Bazazian",
      "Andrew Calway",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Bazazian_Dual-Domain_Image_Synthesis_Using_Segmentation-Guided_GAN_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Bazazian_Dual-Domain_Image_Synthesis_Using_Segmentation-Guided_GAN_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We introduce a segmentation-guided approach to synthesise images that integrate features from two distinct domains. Images synthesised by our dual-domain model belong to one domain within the semantic-mask, and to another in the rest of the image - smoothly integrated. We build on the successes of few-shot StyleGAN and single-shot semantic segmentation to minimise the amount of training required in utilising two domains. The method combines few-shot cross-domain StyleGAN with a latent optimiser to achieve images containing features of two distinct domains. We use a segmentation-guided perceptual loss, which compares both pixel-level and activations between domain-specific and dual-domain synthetic images. Results demonstrate qualitatively and quantitatively that our model is capable of synthesising dual-domain images on a variety of objects (faces, horses, cats, cars), domains (natural, caricature, sketches) and part-based masks (eyes, nose, mouth, hair, car bonnet). Our code is publicly available at: https://github.com/denabazazian/Dual-Domain-Synthesis.",
    "code_link": ""
  },
  "cvpr2022_ntire_multi-encodernetworkforparameterreductionofakernel-basedinterpolationarchitecture": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Encoder Network for Parameter Reduction of a Kernel-Based Interpolation Architecture",
    "authors": [
      "Issa Khalifeh",
      "Marc Gorriz Blanch",
      "Ebroul Izquierdo",
      "Marta Mrak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Khalifeh_Multi-Encoder_Network_for_Parameter_Reduction_of_a_Kernel-Based_Interpolation_Architecture_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Khalifeh_Multi-Encoder_Network_for_Parameter_Reduction_of_a_Kernel-Based_Interpolation_Architecture_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Video frame interpolation involves the synthesis of new frames from existing ones. Convolutional neural networks (CNNs) have been at the forefront of the recent advances in this field. One popular CNN-based approach involves the application of generated kernels to the input frames to obtain an interpolated frame. Despite all the benefits interpolation methods offer, many of these networks require a lot of parameters, with more parameters meaning a heavier computational burden. Reducing the size of the model typically impacts performance negatively. This paper presents a method for parameter reduction for a popular flow-less kernel-based network (Adaptive Collaboration of Flows). Through our technique of removing the layers that require the most parameters and replacing them with smaller encoders, we reduce the number of parameters of the network and even achieve better performance compared to the original method. This is achieved by deploying rotation to force each individual encoder to learn different features from the input images. Ablations are conducted to justify design choices and an evaluation on how our method performs on full-length videos is presented.",
    "code_link": ""
  },
  "cvpr2022_ntire_acloserlookatblindsuper-resolutiondegradationmodels,baselines,andperformanceupperbounds": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds",
    "authors": [
      "Wenlong Zhang",
      "Guangyuan Shi",
      "Yihao Liu",
      "Chao Dong",
      "Xiao-Ming Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Zhang_A_Closer_Look_at_Blind_Super-Resolution_Degradation_Models_Baselines_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Zhang_A_Closer_Look_at_Blind_Super-Resolution_Degradation_Models_Baselines_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Degradation models play an important role in Blind super-resolution (SR). The classical degradation model, which mainly involves blur degradation, is too simple to simulate real-world scenarios. The recently proposed practical degradation model includes a full spectrum of degradation types, but only considers complex cases that use all degradation types in the degradation process, while ignoring many important corner cases that are common in the real world. To address this problem, we propose a unified gated degradation model to generate a broad set of degradation cases using a random gate controller. Based on the gated degradation model, we propose simple baseline networks that can effectively handle non-blind, classical, practical degradation cases as well as many other corner cases. To fairly evaluate the performance of our baseline networks against state-of-the-art methods and understand their limits, we introduce the performance upper bound of an SR network for every degradation type. Our empirical analysis shows that with the unified gated degradation model, the proposed baselines can achieve much better performance than existing methods in quantitative and qualitative results, which are close to the performance upper bounds.",
    "code_link": ""
  },
  "cvpr2022_ntire_identitypreservinglossforlearnedimagecompression": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Identity Preserving Loss for Learned Image Compression",
    "authors": [
      "Jiuhong Xiao",
      "Lavisha Aggarwal",
      "Prithviraj Banerjee",
      "Manoj Aggarwal",
      "Gerard Medioni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Xiao_Identity_Preserving_Loss_for_Learned_Image_Compression_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Xiao_Identity_Preserving_Loss_for_Learned_Image_Compression_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep learning model inference on embedded devices is challenging due to the limited availability of computation resources. A popular alternative is to perform model inference on the cloud, which requires transmitting images from the embedded device to the cloud. Image compression techniques are commonly employed in such cloud-based architectures to reduce transmission latency over low bandwidth networks. This work proposes an end-to-end image compression framework that learns domain-specific features to achieve higher compression ratios than standard HEVC/JPEG compression techniques while maintaining accuracy on downstream tasks (e.g., recognition). Our framework does not require fine-tuning of the downstream task, which allows us to drop-in any off-the-shelf downstream task model without retraining. We choose faces as an application domain due to the ready availability of datasets and off-the-shelf recognition models as representative downstream tasks. We present a novel Identity Preserving Reconstruction (IPR) loss function which achieves Bits-Per-Pixel (BPP) values that are38% and42% of CRF-23 HEVC compression for LFW (low-resolution) and CelebA-HQ (high-resolution) datasets, respectively, while maintaining parity in recognition accuracy. The superior compression ratio is achieved as the model learns to retain the domain-specific features (e.g., facial features) while sacrificing details in the background. Furthermore, images reconstructed by our proposed compression model are robust to changes in downstream model architectures. We show at-par recognition performance on the LFW dataset with an unseen recognition model while retaining a lower BPP value of38% of CRF-23 HEVC compression.",
    "code_link": ""
  },
  "cvpr2022_ntire_fastandmemory-efficientnetworktowardsefficientimagesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Fast and Memory-Efficient Network Towards Efficient Image Super-Resolution",
    "authors": [
      "Zongcai Du",
      "Ding Liu",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu",
      "Lean Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Du_Fast_and_Memory-Efficient_Network_Towards_Efficient_Image_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Du_Fast_and_Memory-Efficient_Network_Towards_Efficient_Image_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Runtime and memory consumption are two important aspects for efficient image super-resolution (EISR) models to be deployed on resource-constrained devices. Recent advances in EISR exploit distillation and aggregation strategies with plenty of channel split and concatenation operations to make full use of limited hierarchical features. In contrast, sequential network operations avoid frequently accessing preceding states and extra nodes, and thus are beneficial to reducing the memory consumption and runtime overhead. Following this idea, we design our lightweight network backbone by mainly stacking multiple highly optimized convolution and activation layers and decreasing the usage of feature fusion. We propose a novel sequential attention branch, where every pixel is assigned an important factor according to local and global contexts, to enhance high-frequency details. In addition, we tailor the residual block for EISR and propose an enhanced residual block (ERB) to further accelerate the network inference. Finally, combining all the above techniques, we construct a fast and memory-efficient network (FMEN) and its small version FMEN-S, which runs 33% faster and reduces 74% memory consumption compared with the state-of-the-art EISR model: E-RFDN, the champion in AIM 2020 efficient super-resolution challenge. Besides, FMEN-S achieves the lowest memory consumption and the second shortest runtime in NTIRE 2022 challenge on efficient super-resolution. Code is available at https://github.com/NJU-Jet/FMEN.",
    "code_link": "https://github.com/NJU-Jet/FMEN"
  },
  "cvpr2022_ntire_underwaterlightfieldretentionneuralrenderingforunderwaterimaging": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Underwater Light Field Retention: Neural Rendering for Underwater Imaging",
    "authors": [
      "Tian Ye",
      "Sixiang Chen",
      "Yun Liu",
      "Yi Ye",
      "Erkang Chen",
      "Yuche Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Ye_Underwater_Light_Field_Retention_Neural_Rendering_for_Underwater_Imaging_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Ye_Underwater_Light_Field_Retention_Neural_Rendering_for_Underwater_Imaging_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Underwater Image Rendering aims to generate a true-to-life underwater image from a given clean one, which could be applied to various practical applications such as underwater image enhancement, camera filter, and virtual gaming. We explore two less-touched but challenging problems in underwater image rendering, namely, i) how to render diverse underwater scenes by a single neural network? ii) how to adaptively learn the underwater light fields from natural exemplars, i,e., realistic underwater images? To this end, we propose a neural rendering method for underwater imaging, dubbed UWNR (Underwater Neural Rendering). Specifically, UWNR is a data-driven neural network that implicitly learns the natural degenerated model from authentic underwater images, avoiding introducing erroneous biases by hand-craft imaging models. Compared with existing underwater image generation methods, UWNR utilizes the natural light field to simulate the main characteristics of the underwater scene. Thus, it is able to synthesize a wide variety of underwater images from one clean image with various realistic underwater images. Extensive experiments demonstrate that our approach achieves better visual effects and quantitative metrics over previous methods. Moreover, we adopt UWNR to build an open Large Neural Rendering Underwater Dataset containing various types of water quality, dubbed LNRUD.",
    "code_link": "https://github.com/Ephemeral182/UWNR"
  },
  "cvpr2022_ntire_imagemulti-inpaintingviaprogressivegenerativeadversarialnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Image Multi-Inpainting via Progressive Generative Adversarial Networks",
    "authors": [
      "Jiayin Cai",
      "Changlin Li",
      "Xin Tao",
      "Yu-Wing Tai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Cai_Image_Multi-Inpainting_via_Progressive_Generative_Adversarial_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Cai_Image_Multi-Inpainting_via_Progressive_Generative_Adversarial_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image inpainting task aims to recover missing pixels naturally and realistically. However, previous deep learning approaches requires specific design for different types of masks and cannot generalize well to to complicated inpainting scenarios. Therefore in addition to most common stroke-type mask, we in this paper propose a unified framework to handle multiple types of masks simultaneously (e.g. strokes, object shapes, extrapolation, dense and periodic grids et al.) We address this problem by adapting a progressive learning scheme to an Semantic Aware Generative Adversarial Network (SA-PatchGAN), in order to design a mask independent network for high-quality results with the best perceptual quality. More specifically, the overall training proceeds in multiple stages so that the model gradually generate the output image from coarse to fine. In our experiments, we show that this strategy yields a large performance gain compared to the single-scale learning methods. We also introduce additional semantic conditioning to the discriminator which encourage high quality local style statistics, and show that this approach is effective on a wider scenario/tasks and could better adapt to various types of mask. Our method produces promising results on various mask types using one single model.",
    "code_link": ""
  },
  "cvpr2022_ntire_residuallocalfeaturenetworkforefficientsuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Residual Local Feature Network for Efficient Super-Resolution",
    "authors": [
      "Fangyuan Kong",
      "Mingxi Li",
      "Songwei Liu",
      "Ding Liu",
      "Jingwen He",
      "Yang Bai",
      "Fangmin Chen",
      "Lean Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Kong_Residual_Local_Feature_Network_for_Efficient_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Kong_Residual_Local_Feature_Network_for_Efficient_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep learning based approaches has achieved great performance in single image super-resolution (SISR). However, recent advances in efficient super-resolution focus on reducing the number of parameters and FLOPs, and they aggregate more powerful features by improving feature utilization through complex layer connection strategies. These structures may not be necessary to achieve higher running speed, which makes them difficult to be deployed to resource-constrained devices. In this work, we propose a novel Residual Local Feature Network (RLFN). The main idea is using three convolutional layers for residual local feature learning to simplify feature aggregation, which achieves a good trade-off between model performance and inference time. Moreover, we revisit the popular contrastive loss and observe that the selection of intermediate features of its feature extractor has great influence on the performance. Besides, we propose a novel multi-stage warm-start training strategy. In each stage, the pre-trained weights from previous stages are utilized to improve the model performance. Combined with the improved contrastive loss and training strategy, the proposed RLFN outperforms all the state-of-the-art efficient image SR models in terms of runtime while maintaining both PSNR and SSIM for SR. In addition, we won the first place in the runtime track of the NTIRE 2022 efficient super-resolution challenge. Code will be available at https://github.com/fyan111/RLFN.",
    "code_link": "https://github.com/fyan111/RLFN"
  },
  "cvpr2022_ntire_drcrnetdenseresidualchannelre-calibrationnetworkwithnon-localpurificationforspectralsuperresolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DRCR Net: Dense Residual Channel Re-Calibration Network With Non-Local Purification for Spectral Super Resolution",
    "authors": [
      "Jiaojiao Li",
      "Songcheng Du",
      "Chaoxiong Wu",
      "Yihong Leng",
      "Rui Song",
      "Yunsong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Li_DRCR_Net_Dense_Residual_Channel_Re-Calibration_Network_With_Non-Local_Purification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Li_DRCR_Net_Dense_Residual_Channel_Re-Calibration_Network_With_Non-Local_Purification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Spectral super resolution (SSR) aims to reconstruct the 3D hyperspectral signal from a 2D RGB image, which is prosperous with the proliferation of Convolutional Neural Networks (CNNs) and increased access to RGB/hyperspectral datasets. Nevertheless, most CNN-based spectral reconstruction (SR) algorithms can only perform high reconstruction accuracy when the input RGB image is relatively 'clean' with foregone spectral response functions. Unfortunately, in the real world, images are contaminated by mixed noise, bad illumination conditions, compression, artifacts etc. and the existing state-of-the-art (SOTA) methods are no longer working well. To conquer these drawbacks, we propose a novel dense residual channel re-calibration network (DRCR Net) with non-local purification for achieving robust SSR results, which first performs the interference removal through a non-local purification module (NPM) to refine the RGB inputs. To be specific, as the main component of backbone, the dense residual channel re-calibration (DRCR) block is cascaded with an encoder-decoder paradigm through several cross-layer dense residual connections, to capture the deep spatial-spectral interactions, which further improve the generalization ability of the network effectively. Furthermore, we customize dual channel re-calibration modules (CRMs) which are embedded in each DRCR block to adaptively re-calibrate channel-wise feature response for pursuing high-fidelity spectral recovery. In the NTIRE 2022 Spectral Reconstruction Challenge, our entry obtained the 3rd ranking. Code will be made available online at https://github.com/jojolee6513/DRCR-net.",
    "code_link": "https://github.com/jojolee6513/DRCR-net"
  },
  "cvpr2022_ntire_mstriqnoreferenceimagequalityassessmentbasedonswintransformerwithmulti-stagefusion": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer With Multi-Stage Fusion",
    "authors": [
      "Jing Wang",
      "Haotian Fan",
      "Xiaoxia Hou",
      "Yitian Xu",
      "Tao Li",
      "Xuechao Lu",
      "Lean Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wang_MSTRIQ_No_Reference_Image_Quality_Assessment_Based_on_Swin_Transformer_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wang_MSTRIQ_No_Reference_Image_Quality_Assessment_Based_on_Swin_Transformer_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Measuring the perceptual quality of images automatically is an essential task in the area of computer vision, as degradations on image quality can exist in many processes from image acquisition, transmission to enhancing. Many Image Quality Assessment(IQA) algorithms have been designed to tackle this problem. However, it still remains unsettled due to the various types of image distortions and the lack of large-scale human-rated datasets. In this paper, we propose a novel algorithm based on the Swin Transformer with fused features from multiple stages, which aggregates information from both local and global features to better predict the quality. To address the issues of small-scale datasets, relative rankings of images have been taken into account together with regression loss to simultaneously optimize the model. Furthermore, effective data augmentation strategies are also used to improve the performance. In comparisons with previous works, experiments are carried out on two standard IQA datasets and a challenge dataset. The results demonstrate the effectiveness of our work. The proposed method outperforms other methods on standard datasets and ranks 2nd in the no-reference track of NTIRE 2022 Perceptual Image Quality Assessment Challenge. It verifies that our method is promising in solving diverse IQA problems and thus can be used to real-word applications.",
    "code_link": ""
  },
  "cvpr2022_ntire_drtalightweightsingleimagederainingrecursivetransformer": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DRT: A Lightweight Single Image Deraining Recursive Transformer",
    "authors": [
      "Yuanchu Liang",
      "Saeed Anwar",
      "Yang Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Liang_DRT_A_Lightweight_Single_Image_Deraining_Recursive_Transformer_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Liang_DRT_A_Lightweight_Single_Image_Deraining_Recursive_Transformer_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Over parameterization is a common technique in deep learning to help models learn and generalize sufficiently to the given task; nonetheless, this often leads to enormous network structures and consumes considerable computing resources during training. Recent powerful transformer-based deep learning models on vision tasks usually have heavy parameters and bear training difficulty. However, many dense-prediction low-level computer vision tasks, such as rain streak removing, often need to be executed on devices with limited computing power and memory in practice. Hence, we introduce a recursive local window-based self-attention structure with residual connections and propose deraining a recursive transformer (DRT), which enjoys the superiority of the transformer but requires a small amount of computing resources. In particular, through recursive architecture, our proposed model uses only1.3% of the number of parameters of the current best performing model in deraining while exceeding the state-of-the-art methods on the Rain100L benchmark by at least 0.33 dB. Ablation studies also investigate the impact of recursions on derain outcomes. Moreover, since the model contains no deliberate design for deraining, it can also be applied to other image restoration tasks. Our experiment shows that it can achieve competitive results on desnowing. The source code and pretrained model can be found at https://github.com/YC-Liang/DRT.",
    "code_link": "https://github.com/YC-Liang/DRT"
  },
  "cvpr2022_ntire_towardsreal-worldshadowremovalwithashadowsimulationmethodandatwo-stageframework": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Towards Real-World Shadow Removal With a Shadow Simulation Method and a Two-Stage Framework",
    "authors": [
      "Jianhao Gao",
      "Quanlong Zheng",
      "Yandong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Gao_Towards_Real-World_Shadow_Removal_With_a_Shadow_Simulation_Method_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Gao_Towards_Real-World_Shadow_Removal_With_a_Shadow_Simulation_Method_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Shadow removal is an important yet challenging restoration task. State-of-the-art shadow removal methods usually require paired datasets for training. Existing shadow removal datasets lack large-scale quantity and scene diversity. Hence, models trained on such datasets have poor generalization ability. This paper proposes a simple yet robust shadow simulation method to simulate shadow on the grayscale. The proposed shadow simulation method can be applied to arbitrary shadow-free images and masks to generate corresponding shadow images. With our shadow simulation method, we can generate a large-scale and diverse paired shadow removal dataset. Besides, we introduce a two-stage framework, Gray-to-Color Shadow Removal Network (G2C-DeshadowNet) for shadow removal. The first stage is a Grayscale Enhancement Network, which attempts to remove shadows on the grayscale. The second stage is a Colorization Network, which attempts to colorize the grayscale shadow-free image. Extensive experiments on ISTD+, SRD, and SBU datasets show that G2C-DeshadowNet outperforms state-of-the-art methods and has better generalization ability.",
    "code_link": "https://github.com/jianhaogao/ShadowRemoval-with-Two-stage-Framework"
  },
  "cvpr2022_ntire_ntire2022challengeonlearningthesuper-resolutionspace": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on Learning the Super-Resolution Space",
    "authors": [
      "Andreas Lugmayr",
      "Martin Danelljan",
      "Radu Timofte",
      "Kang-wook Kim",
      "Younggeun Kim",
      "Jae-young Lee",
      "Zechao Li",
      "Jinshan Pan",
      "Dongseok Shim",
      "Ki-Ung Song",
      "Jinhui Tang",
      "Cong Wang",
      "Zhihao Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Lugmayr_NTIRE_2022_Challenge_on_Learning_the_Super-Resolution_Space_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Lugmayr_NTIRE_2022_Challenge_on_Learning_the_Super-Resolution_Space_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reviews the NTIRE 2022 challenge on learning the super-Resolution space. This challenge aims to raise awareness that the super-resolution problem is ill-posed. Since many high-resolution images map to the same low-resolution image, we asked the participants to create methods that sample diverse super-resolution from the space of possible high-resolution images given a low-resolution image. For evaluation, we use the same protocol as introduced in the last year's super-resolution space challenge of NTIRE 2021. We compare the submissions of the participating teams and relate them to the approaches from last year. This challenge contains two tracks: 4X and 8X scale factor. In total, 3 teams competed in the final testing phase.",
    "code_link": ""
  },
  "cvpr2022_ntire_dowhatyoucan,withwhatyouhavescale-awareandhighqualitymonoculardepthestimationwithoutrealworldlabels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Do What You Can, With What You Have: Scale-Aware and High Quality Monocular Depth Estimation Without Real World Labels",
    "authors": [
      "Kunal Swami",
      "Amrit Muduli",
      "Uttam Gurram",
      "Pankaj Bajpai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Swami_Do_What_You_Can_With_What_You_Have_Scale-Aware_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Swami_Do_What_You_Can_With_What_You_Have_Scale-Aware_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning robust and scale-aware monocular depth estimation (MDE) requires expensive data annotation efforts. Self-supervised approaches use unlabelled videos but, due to ambiguous photometric reprojection loss and no labelled supervision, produce inferior quality relative (scale ambiguous) depth maps with over-smoothed object boundaries. Approaches using synthetic training data suffer from the non-trivial domain adaptation problem; despite complicated unsupervised domain adaptation (UDA) techniques, these methods still do not generalize well to real datasets. This work presents a novel and effective training methodology to combine self-supervision from unlabelled monocular videos and dense supervision from the synthetic dataset synergistically without complicated UDA techniques. With our method, geometry and semantics are learned from monocular videos, whereas scale-awareness and qualitative attributes, e.g., sharp and smooth depth variations, that are crucial for practical use cases are learned from the synthetic dataset. Our method outperforms self-supervised, semi-supervised, and all the domain adaptation methods on standard benchmark datasets while being competitive with fully supervised methods. Furthermore, our method leads to qualitatively superior depth maps, which increases its practical utility compared to existing methods. We demonstrate this by applying our method to develop an MDE model for a real life application---DSLR-like shallow depth-of-field effect on smartphones. The new high quality synthetic depth dataset that we generate for this task will be available to the community.",
    "code_link": ""
  },
  "cvpr2022_ntire_multipledegradationandreconstructionnetworkforsingleimagedenoisingviaknowledgedistillation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multiple Degradation and Reconstruction Network for Single Image Denoising via Knowledge Distillation",
    "authors": [
      "Juncheng Li",
      "Hanhui Yang",
      "Qiaosi Yi",
      "Faming Fang",
      "Guangwei Gao",
      "Tieyong Zeng",
      "Guixu Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Li_Multiple_Degradation_and_Reconstruction_Network_for_Single_Image_Denoising_via_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Li_Multiple_Degradation_and_Reconstruction_Network_for_Single_Image_Denoising_via_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Single image denoising (SID) has achieved significant breakthroughs with the development of deep learning. However, the proposed methods are often accompanied by plenty of parameters, which greatly limits their application scenarios. Different from previous works that blindly increase the depth of the network, we explore the degradation mechanism of the noisy image and propose a lightweight Multiple Degradation and Reconstruction Network (MDRN) to progressively remove noise. Meanwhile, we propose two novel Heterogeneous Knowledge Distillation Strategies (HMDS) to enable MDRN to learn richer and more accurate features from heterogeneous models, which make it possible to reconstruct higher-quality denoised images under extreme conditions. Extensive experiments show that our MDRN achieves favorable performance against other SID models with fewer parameters. Meanwhile, plenty of ablation studies demonstrate that the introduced HMDS can improve the performance of tiny models or the model under high noise levels, which is extremely useful for related applications.",
    "code_link": ""
  },
  "cvpr2022_ntire_bidirectionalmotionestimationwithcycliccostvolumeforhighdynamicrangeimaging": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Bidirectional Motion Estimation With Cyclic Cost Volume for High Dynamic Range Imaging",
    "authors": [
      "An Gia Vien",
      "Seonghyun Park",
      "Truong Thanh Nhat Mai",
      "Gahyeon Kim",
      "Chul Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Vien_Bidirectional_Motion_Estimation_With_Cyclic_Cost_Volume_for_High_Dynamic_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Vien_Bidirectional_Motion_Estimation_With_Cyclic_Cost_Volume_for_High_Dynamic_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a high dynamic range (HDR) imaging algorithm based on bidirectional motion estimation. First, we develop a motion estimation network with the cyclic cost volume and spatial attention maps to estimate accurate optical flows between input low dynamic range (LDR) images. Then, we develop the dynamic local fusion network that combines the warped and reference inputs to generate a synthesized image by exploiting local information. Finally, to further improve the synthesis performance, we develop the global refinement network that generates a residual image by exploiting global information. Experimental results on the dataset from the NTIRE 2022 HDR Challenge Track 1 (Low-complexity constrain) demonstrate the effectiveness of the proposed HDR image synthesis algorithm.",
    "code_link": ""
  },
  "cvpr2022_ntire_asymmetricinformationdistillationnetworkforlightweightsuperresolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Asymmetric Information Distillation Network for Lightweight Super Resolution",
    "authors": [
      "Zhikai Zong",
      "Lin Zha",
      "Jiande Jiang",
      "Xiaoxiao Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Zong_Asymmetric_Information_Distillation_Network_for_Lightweight_Super_Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Zong_Asymmetric_Information_Distillation_Network_for_Lightweight_Super_Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The purpose of this paper is to design a lightweight network to achieve image super resolution performance equivalent to SRResNet. We design an asymmetric information distillation block (AIDB) with distillation information multiplexing and asymmetric information extraction capabilities to better achieve this goal. Distillation information multiplexing refers to the repeated processing of distilled information to supplement the ability of key information extraction. Asymmetric information enhancement block (AIEB) refers to identify different features in the image by the horizontal and vertical feature extraction. AIEB greatly reduces the number of parameters, and distillation information multiplexing works as a supplement to the lost high dimensional information. A large number of experiments show that our asymmetric information distillation network (AIDN) achieves a better balance of performance and complexity than SOTA model. Moreover, Our proposed AIDN ranked second in the model complexity track of NTIRE2022 efficient super resolution challenge. Compared with the first place in this track, we achieves higher PSNR performance on testset with a slight disadvantage in the number of parameters. The code is available at https://github.com/zzksdu/AIDN.",
    "code_link": "https://github.com/zzksdu/AIDN"
  },
  "cvpr2022_ntire_ahybridnetworkofcnnandtransformerforlightweightimagesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Hybrid Network of CNN and Transformer for Lightweight Image Super-Resolution",
    "authors": [
      "Jinsheng Fang",
      "Hanjiang Lin",
      "Xinyu Chen",
      "Kun Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Fang_A_Hybrid_Network_of_CNN_and_Transformer_for_Lightweight_Image_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Fang_A_Hybrid_Network_of_CNN_and_Transformer_for_Lightweight_Image_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recently, a number of CNN based methods have made great progress in single image super-resolution. However, these existing architectures commonly build massive number of network layers, bringing high computational complexity and heavy memory consumption, which is inappropriate to be applied on embedded terminals such as mobile platforms. In order to solve this problem, we propose a hybrid network of CNN and Transformer (HNCT) for lightweight image super-resolution. In general, HNCT consists of four parts, which are shallow feature extraction module, Hybrid Blocks of CNN and Transformer (HBCTs), dense feature fusion module and up-sampling module, respectively. By combining CNN and Transformer, HBCT extracts deep features beneficial for super-resolution reconstruction in consideration of both local and non-local priors, while being lightweight and flexible enough. Enhanced spatial attention is introduced in HBCT to further improve performance. Extensive experimental results show our HNCT is superior to the state-of-the-art methods in terms of super-resolution performance and model complexity. Moreover, we won the second best PSNR and the least activation operations in NTIRE 2022 Efficient SR Challenge. Code is available at https://github.com/lhjthp/HNCT.",
    "code_link": "https://github.com/lhjthp/HNCT"
  },
  "cvpr2022_ntire_boundary-awareimageinpaintingwithmultipleauxiliarycues": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Boundary-Aware Image Inpainting With Multiple Auxiliary Cues",
    "authors": [
      "Yohei Yamashita",
      "Kodai Shimosato",
      "Norimichi Ukita"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yamashita_Boundary-Aware_Image_Inpainting_With_Multiple_Auxiliary_Cues_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yamashita_Boundary-Aware_Image_Inpainting_With_Multiple_Auxiliary_Cues_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image inpainting (a.k.a. image completion) allows us to remove unexpected foreground objects from an observed image and to restore the removed region with background pixels. The performance of image inpainting is improved by auxiliary cues such as edge boundaries and segmentation regions. As a new auxiliary cue, this paper focuses on a depth image that is estimated from an input RGB image by monocular depth estimation. In the depth image, boundaries between different objects (e.g., objects located in different distances) with similar pixel values might be available, while those boundaries are difficult to be detected by edge detection and segmentation. Our proposed method employs those boundaries in the edge and depth images as auxiliary cues. Experiments demonstrate that our proposed method augmented by the depth image outperforms its baseline quantitatively (i.e., 1.17dB and 0.74dB PSNR gains on the Paris-StreetView and Places datasets, respectively) and qualitatively.",
    "code_link": ""
  },
  "cvpr2022_ntire_adaptivefeatureconsolidationnetworkforburstsuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Adaptive Feature Consolidation Network for Burst Super-Resolution",
    "authors": [
      "Nancy Mehta",
      "Akshay Dudhane",
      "Subrahmanyam Murala",
      "Syed Waqas Zamir",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Mehta_Adaptive_Feature_Consolidation_Network_for_Burst_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Mehta_Adaptive_Feature_Consolidation_Network_for_Burst_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Modern digital cameras generally count on image signal processing (ISP) pipelines for producing naturalistic RGB images. Nevertheless, in comparison to DSLR cameras, low-quality images are generally output from portable mobile devices due to their physical limitations. The synthesized low-quality images usually have multiple degradations - low-resolution owing to small camera sensors, mosaic patterns on account of camera filter array and sub-pixel shifts due to camera motion. Such degradation usually restrain the performance of single image super-resolution methodologies for retrieving high-resolution (HR) image from a single low-resolution (LR) image. Burst image super-resolution aims at restoring a photo-realistic HR image by capturing the abundant information from multiple LR images. Lately, the soaring popularity of burst photography has made multi-frame processing an attractive solution for overcoming the limitations of single image processing. In our work, we thus aim to propose a generic architecture, adaptive feature consolidation network (AFCNet) for multi-frame processing. To alleviate the challenge of effectively modelling the long-range dependency problem, that multi-frame approaches struggle to solve, we utilize encoder-decoder based transformer backbone which learns multi-scale local-global representations. We propose feature alignment module to align LR burst frame features. Further, the aligned features are fused and reconstructed by abridged pseudo-burst fusion module and adaptive group upsampling modules, respectively. Our proposed approach clearly outperforms the other existing state-of-the-art techniques on benchmark datasets. The experimental results illustrate the effectiveness and generality of our proposed framework in upgrading the visual quality of HR images.",
    "code_link": ""
  },
  "cvpr2022_ntire_self-calibratedefficienttransformerforlightweightsuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Self-Calibrated Efficient Transformer for Lightweight Super-Resolution",
    "authors": [
      "Wenbin Zou",
      "Tian Ye",
      "Weixin Zheng",
      "Yunchen Zhang",
      "Liang Chen",
      "Yi Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Zou_Self-Calibrated_Efficient_Transformer_for_Lightweight_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Zou_Self-Calibrated_Efficient_Transformer_for_Lightweight_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recently, deep learning has been successfully applied to the single-image super-resolution (SISR) with remarkable performance. However, most existing methods focus on building a more complex network with a large number of layers, which can entail heavy computational costs and memory storage. To address this problem, we present a lightweight Self-Calibrated Efficient Transformer (SCET) network to solve this problem. The architecture of SCET mainly consists of the self-calibrated module and efficient transformer block, where the self-calibrated module adopts the pixel attention mechanism to extract image features effectively. To further exploit the contextual information from features, we employ an efficient transformer to help the network obtain similar features over long distances and thus recover sufficient texture details. We provide comprehensive results on different settings of the overall network. Our proposed method achieves more remarkable performance than baseline methods. The source code and pre-trained models are available at https://github.com/AlexZou14/SCET.",
    "code_link": ""
  },
  "cvpr2022_ntire_blindnon-uniformmotiondeblurringusingatrousspatialpyramiddeformableconvolutionanddeblurring-reblurringconsistency": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Blind Non-Uniform Motion Deblurring Using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency",
    "authors": [
      "Dong Huo",
      "Abbas Masoumzadeh",
      "Yee-Hong Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Huo_Blind_Non-Uniform_Motion_Deblurring_Using_Atrous_Spatial_Pyramid_Deformable_Convolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Huo_Blind_Non-Uniform_Motion_Deblurring_Using_Atrous_Spatial_Pyramid_Deformable_Convolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Many deep learning based methods are designed to remove non-uniform (spatially variant) motion blur caused by object motion and camera shake without knowing the blur kernel. Some methods directly output the latent sharp image in one stage, while others utilize a multi-stage strategy (e.g. multi-scale, multi-patch, or multi-temporal) to gradually restore the sharp image. However, these methods have the following two main issues: 1) The computational cost of multi-stage is high; 2) The same convolution kernel is applied in different regions, which is not an ideal choice for non-uniform blur. Hence, non-uniform motion deblurring is still a challenging and open problem. In this paper, we propose a new architecture which consists of multiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to deblur an image end-to-end with more flexibility. Multiple ASPDC modules implicitly learn the pixel-specific motion with different dilation rates in the same layer to handle movements of different magnitude. To improve the training, we also propose a reblurring network to map the deblurred output back to the blurred input, which constrains the solution space. Our experimental results show that the proposed method outperforms state-of-the-art methods on the benchmark datasets. The code is available at https://github.com/Dong-Huo/ASPDC.",
    "code_link": "https://github.com/Dong-Huo/ASPDC"
  },
  "cvpr2022_ntire_nonuniformlydehazenetworkforvisibleremotesensingimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Nonuniformly Dehaze Network for Visible Remote Sensing Images",
    "authors": [
      "Zhaojie Chen",
      "Qi Li",
      "Huajun Feng",
      "Zhihai Xu",
      "Yueting Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Chen_Nonuniformly_Dehaze_Network_for_Visible_Remote_Sensing_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Chen_Nonuniformly_Dehaze_Network_for_Visible_Remote_Sensing_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nonuniform haze on remote sensing images degrades image quality and hinders many high-level tasks. In this paper, we propose a Nonuniformly Dehaze Network towards nonuniform haze on visible remote sensing images. To extract robust haze-aware features, we propose Nonuniformly Excite (NE) module. Inspired by the well-known gather-excite attention module, NE module works in a map-excite manner. In the map operation, we utilize a proposed Dual Attention Dehaze block to extract local enhanced features. In the gather operation, we utilize a strided deformable convolution to nonuniformly process features and extract nonlocal haze-aware features. In the excite operation, we employ a pixel-wise attention between local enhanced features and nonlocal haze-aware features, to gain finer haze-aware features. Moreover, we recursively embed NE modules in a multi-scale framework. It helps not only significantly reduce network's parameters, but also recursively deliver and fuse haze-aware features from higher levels, which makes learning more efficient. Experiments demonstrate that the proposed network performs favorably against the state-of-the-art methods on both synthetic and real-world images.",
    "code_link": ""
  },
  "cvpr2022_ntire_onlinemetaadaptationforvariable-ratelearnedimagecompression": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Online Meta Adaptation for Variable-Rate Learned Image Compression",
    "authors": [
      "Wei Jiang",
      "Wei Wang",
      "Songnan Li",
      "Shan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Jiang_Online_Meta_Adaptation_for_Variable-Rate_Learned_Image_Compression_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Jiang_Online_Meta_Adaptation_for_Variable-Rate_Learned_Image_Compression_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This work addresses two major issues of end-to-end learned image compression (LIC) based on deep neural networks: variable-rate learning where separate networks are required to generate compressed images with varying qualities, and the train-test mismatch between differentiable approximate quantization and true hard quantization. We introduce an online meta-learning (OML) setting for LIC, which combines ideas from meta learning and online learning in the conditional variational auto-encoder (CVAE) framework. By treating the conditional variables as meta parameters and treating the generated conditional features as meta priors, the desired reconstruction can be controlled by the meta parameters to accommodate compression with variable qualities. The online learning framework is used to update the meta parameters so that the conditional reconstruction is adaptively tuned for the current image. Through the OML mechanism, the meta parameters can be effectively updated through SGD. The conditional reconstruction is directly based on the quantized latent representation in the decoder network, and therefore helps to bridge the gap between the training estimation and true quantized latent distribution. Experiments demonstrate that our OML approach can be flexibly applied to different state-of-the-art LIC methods to achieve additional performance improvements with little computation and transmission overhead.",
    "code_link": ""
  },
  "cvpr2022_ntire_conformerandblindnoisystudentsforimprovedimagequalityassessment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Conformer and Blind Noisy Students for Improved Image Quality Assessment",
    "authors": [
      "Marcos V. Conde",
      "Maxime Burchi",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Conde_Conformer_and_Blind_Noisy_Students_for_Improved_Image_Quality_Assessment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Conde_Conformer_and_Blind_Noisy_Students_for_Improved_Image_Quality_Assessment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Generative models for image restoration, enhancement, and generation have significantly improved the quality of the generated images. Surprisingly, these models produce more pleasant images to the human eye than other methods, yet, they may get a lower perceptual quality score using traditional perceptual quality metrics such as PSNR or SSIM. Therefore, it is necessary to develop a quantitative metric to reflect the performance of new algorithms, which should be well-aligned with the person's mean opinion score (MOS). Learning-based approaches for perceptual image quality assessment (IQA) usually require both the distorted and reference image for measuring the perceptual quality accurately. However, commonly only the distorted or generated image is available. In this work, we explore the performance of transformer-based full-reference IQA models. We also propose a method for IQA based on semi-supervised knowledge distillation from full-reference teacher models into blind student models using noisy pseudo-labeled data. Our approaches achieved competitive results on the NTIRE 2022 Perceptual Image Quality Assessment Challenge: our full-reference model was ranked 4th, and our blind noisy student was ranked 3rd among 70 participants, each in their respective track. Our code is available at: https://github.com/burchim/IQA-Conformer-BNS",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022spectraldemosaicingchallengeanddataset": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Spectral Demosaicing Challenge and Data Set",
    "authors": [
      "Boaz Arad",
      "Radu Timofte",
      "Rony Yahel",
      "Nimrod Morag",
      "Amir Bernat",
      "Yaqi Wu",
      "Xun Wu",
      "Zhihao Fan",
      "Chenjie Xia",
      "Feng Zhang",
      "Shuai Liu",
      "Yongqiang Li",
      "Chaoyu Feng",
      "Lei Lei",
      "Mingwei Zhang",
      "Kai Feng",
      "Xun Zhang",
      "Jiaxin Yao",
      "Yongqiang Zhao",
      "Suina Ma",
      "Fan He",
      "Yangyang Dong",
      "Shufang Yu",
      "Difa Qiu",
      "Jinhui Liu",
      "Mengzhao Bi",
      "Beibei Song",
      "WenFang Sun",
      "Jiesi Zheng",
      "Bowen Zhao",
      "Yanpeng Cao",
      "Jiangxin Yang",
      "Yanlong Cao",
      "Xiangyu Kong",
      "Jingbo Yu",
      "Yuanyang Xue",
      "Zheng Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Arad_NTIRE_2022_Spectral_Demosaicing_Challenge_and_Data_Set_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Arad_NTIRE_2022_Spectral_Demosaicing_Challenge_and_Data_Set_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents the first challenge on demosaicing of natural spectral images for snapshot hyperspectral imaging systems (HIS) which utilize a multi-spectral filer array (MSFA), i.e., the recovery of whole-scene hyperspectral information from spatially sub-sampled hyperspectral information. This challenge expands the ARAD_1K data set to a first-of-its-kind large-scale data set for multi-spectral filter array demosaicing of natural scenes containing 1,000 images. Challenge participants were required to recover hyperspectral information from synthetically generated MSFA images simulating capture by a known calibrated snapshot mosaic hyperspectral camera. The challenge was attended by 157 teams, with 29 teams competing in the final testing phase, 7 of which provided detailed descriptions of their methodology which are included in this report. The performance of these submissions is reviewed and provided here as a gauge for the current state-of-the-art in multi-spectral filter array demosaicing of natural images.",
    "code_link": ""
  },
  "cvpr2022_ntire_unpairedreal-worldsuper-resolutionwithpseudocontrollablerestoration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unpaired Real-World Super-Resolution With Pseudo Controllable Restoration",
    "authors": [
      "Andr\u00e9s Romero",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Romero_Unpaired_Real-World_Super-Resolution_With_Pseudo_Controllable_Restoration_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Romero_Unpaired_Real-World_Super-Resolution_With_Pseudo_Controllable_Restoration_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Current super-resolution methods rely on the bicubic down-sampling assumption in order to develop the ill-posed reconstruction of the low-resolution image. Not surprisingly, these approaches fail when using real-world low-resolution images due to the presence of artifacts and intrinsic noise absent in the bicubic setup. Consequently, attention is increasingly paid to techniques that alleviate this problem and super-resolve real-world images. As acquiring paired real-world datasets is a challenging problem, real-world super-resolution solutions are traditionally tackled as a blind problem or as an unpaired data-driven problem. The former makes assumptions about the downsampling operations, the latter uses unpaired training to learn the real distributions. Recently, blind approaches have dominated this problem by assuming a diverse bank of degradations, whereas the unpaired solutions have shown under-performance due to the two-staged training. In this paper, we propose an unpaired real-world super-resolution method that performs on par, or even better than blind paired approaches by introducing a pseudo-controllable restoration module in a fully end-to-end system.",
    "code_link": ""
  },
  "cvpr2022_ntire_arobustnon-blinddeblurringmethodusingdeepdenoiserprior": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Robust Non-Blind Deblurring Method Using Deep Denoiser Prior",
    "authors": [
      "Yingying Fang",
      "Hao Zhang",
      "Hok Shing Wong",
      "Tieyong Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Fang_A_Robust_Non-Blind_Deblurring_Method_Using_Deep_Denoiser_Prior_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Fang_A_Robust_Non-Blind_Deblurring_Method_Using_Deep_Denoiser_Prior_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The existing non-blind deblurring methods are mostly susceptible to noise in the given blurring kernel, which is usually estimated from the observed image. This will produce undesirable ringing artifacts around the recovered edges when the given kernel is not accurate enough. Besides, the noise and outliers in the observed images may also degrade the performance of the deblurring methods seriously. Considering these factors, we designed a robust non-blind deblurring method taking all these noises into account. In this paper, we propose a kernel error term to rectify the given kernel at the time of performing the deconvolution. A residual error term is also introduced to deal with the outliers caused by noise or saturation. A deep learning denoiser prior is adopted to reserve the fine textures in the recovered image. The experiments show clearly that the proposed method achieves remarkable progress in both the visual quality and the numerical results of the recovered images compared to the state-of-the-art deblurring methods.",
    "code_link": ""
  },
  "cvpr2022_ntire_efficientprogressivehighdynamicrangeimagerestorationviaattentionandalignmentnetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Efficient Progressive High Dynamic Range Image Restoration via Attention and Alignment Network",
    "authors": [
      "Gaocheng Yu",
      "Jin Zhang",
      "Zhe Ma",
      "Hongbin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yu_Efficient_Progressive_High_Dynamic_Range_Image_Restoration_via_Attention_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yu_Efficient_Progressive_High_Dynamic_Range_Image_Restoration_via_Attention_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "HDR is an important part of computational photography technology. In this paper, we propose a lightweight neural network called Efficient Attention-and-alignment-guided Progressive Network (EAPNet) for the challenge NTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional lightweight encoding module to extract features. Besides, we propose Progressive Dilated U-shape Block (PDUB) that can be a progressive plug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use fast and low-power feature-align module to deal with misalignment problem in place of the time-consuming Deformable Convolutional Network (DCN). The experiments show that our method achieves about 20 times compression on MAccs with better mu-PSNR and PSNR compared to the state-of-the-art method. We got the second place of both two tracks during the testing phase. Figure1. shows the visualized result of NTIRE 2022 HDR challenge.",
    "code_link": ""
  },
  "cvpr2022_ntire_multi-brackethighdynamicrangeimagingwitheventcameras": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Bracket High Dynamic Range Imaging With Event Cameras",
    "authors": [
      "Nico Messikommer",
      "Stamatios Georgoulis",
      "Daniel Gehrig",
      "Stepan Tulyakov",
      "Julius Erbach",
      "Alfredo Bochicchio",
      "Yuanyou Li",
      "Davide Scaramuzza"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Messikommer_Multi-Bracket_High_Dynamic_Range_Imaging_With_Event_Cameras_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Messikommer_Multi-Bracket_High_Dynamic_Range_Imaging_With_Event_Cameras_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low dynamic range (LDR) images captured at different exposure times. While these methods work well in static scenes, dynamic scenes remain a challenge since the LDR images still suffer from saturation and noise. In such scenarios, event cameras would be a valid complement, thanks to their higher temporal resolution and dynamic range. In this paper, we propose the first multibracket HDR pipeline combining a standard camera with an event camera. Our results show better overall robustness when using events, with improvements in PSNR by up to 5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a new dataset containing bracketed LDR images with aligned events and HDR ground truth.",
    "code_link": ""
  },
  "cvpr2022_ntire_patch-wisecontrastivestylelearningforinstagramfilterremoval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Patch-Wise Contrastive Style Learning for Instagram Filter Removal",
    "authors": [
      "Furkan K\u0131nl\u0131",
      "Bar\u0131\u015f \u00d6zcan",
      "Furkan K\u0131ra\u00e7"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Kinli_Patch-Wise_Contrastive_Style_Learning_for_Instagram_Filter_Removal_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Kinli_Patch-Wise_Contrastive_Style_Learning_for_Instagram_Filter_Removal_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image-level corruptions and perturbations degrade the performance of CNNs on different downstream vision tasks. Social media filters are one of the most common resources of various corruptions and perturbations for real-world visual analysis applications. The negative effects of these distractive factors can be alleviated by recovering the original images with their pure style for the inference of the downstream vision tasks. Assuming these filters substantially inject a piece of additional style information to the social media images, we can formulate the problem of recovering the original versions as a reverse style transfer problem. We introduce Contrastive Instagram Filter Removal Network (CIFR), which enhances this idea for Instagram filter removal by employing a novel multi-layer patch-wise contrastive style learning mechanism. Experiments show our proposed strategy produces better qualitative and quantitative results than the previous studies. Moreover, we present the results of our additional experiments for proposed architecture within different settings. Finally, we present the inference outputs and quantitative comparison of filtered and recovered images on localization and segmentation tasks to encourage the main motivation for this problem.",
    "code_link": ""
  },
  "cvpr2022_ntire_deepimageinterpolationaunifiedunsupervisedframeworkforpansharpening": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Image Interpolation: A Unified Unsupervised Framework for Pansharpening",
    "authors": [
      "Jianhao Gao",
      "Jie Li",
      "Xin Su",
      "Menghui Jiang",
      "Qiangqiang Yuan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Gao_Deep_Image_Interpolation_A_Unified_Unsupervised_Framework_for_Pansharpening_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Gao_Deep_Image_Interpolation_A_Unified_Unsupervised_Framework_for_Pansharpening_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Pansharpening, whose aim is to acquire high resolution multispectral data (HRMS) by the fusion of low resolution multispectral data (LRMS) and panchromatic data (PAN), is a specific mission of spatial-spectral fusion in remote sensing field. In recent years, deep learning methods have proved the most feasible methods for pansharpening task. However, these deep learning methods have difficulty in training in an unsupervised manner and become useless when it comes to the condition where no training dataset is available. In this paper, we propose a universal algorithm called deep image interpolation for pansharpening task. The main idea is achieving high-quality fusion results by interpolating low-quality results in a deep neural network. We apply it to two conditions: 1) training a network unsupervisedly when there are enough datasets; 2) optimizing the fusion result in an untrained manner when only a pair of PAN and LRMS are available. Simulation and real-data experiments are conducted on various kinds of satellite data. Quantitative and qualitative evaluation results illustrate that the proposed method outperforms traditional pansharpening methods and even catch up with those supervised methods to some extent.",
    "code_link": ""
  },
  "cvpr2022_ntire_nl-ffcnon-localfastfourierconvolutionforimagesuperresolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NL-FFC: Non-Local Fast Fourier Convolution for Image Super Resolution",
    "authors": [
      "Abhishek Kumar Sinha",
      "S. Manthira Moorthi",
      "Debajyoti Dhar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Sinha_NL-FFC_Non-Local_Fast_Fourier_Convolution_for_Image_Super_Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Sinha_NL-FFC_Non-Local_Fast_Fourier_Convolution_for_Image_Super_Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep neural networks have shown promising results in image super-resolution by learning a complex mapping from low resolution to high resolution image. However, most of the approaches learns to upsample by using convolution in spatial domain and are confined to local features. This results into restricting the receptive field of the network and therefore deteriorates the overall quality of the high-resolution image. To alleviate this issue, we propose a generative model based architecture that learns both local and global features, and fuses them together to generate high quality images. The network uses a non-local attention aided Fast Fourier Convolutions (NL-FFC) to widen the receptive field and learn long-range dependencies. The analyses further show that these Fourier features implicitly provide faster convergence on low frequency components only to learn prior for unobserved high frequency components. The model generalizes well to different datasets. We further investigate the role of non-local attention, and the ratio of local and global features to maximize the performance gain in the ablation study.",
    "code_link": ""
  },
  "cvpr2022_ntire_anewdatasetandtransformerforstereoscopicvideosuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A New Dataset and Transformer for Stereoscopic Video Super-Resolution",
    "authors": [
      "Hassan Imani",
      "Md Baharul Islam",
      "Lai-Kuan Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Imani_A_New_Dataset_and_Transformer_for_Stereoscopic_Video_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Imani_A_New_Dataset_and_Transformer_for_Stereoscopic_Video_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Stereo video super-resolution (SVSR) aims to enhance the spatial resolution of the low-resolution video by reconstructing the high-resolution video. The key challenges in SVSR are preserving the stereo-consistency and temporal-consistency, without which viewers may experience 3D fatigue. There are several notable works on stereoscopic image super-resolution, but there is little research on stereo video super-resolution. In this paper, we propose a novel Transformer-based model for SVSR, namely Trans-SVSR. Trans-SVSR comprises two key novel components: a spatio-temporal convolutional self-attention layer and an optical flow-based feed-forward layer that discovers the correlation across different video frames and aligns the features. The parallax attention mechanism (PAM) that uses the cross-view information to consider the significant disparities is used to fuse the stereo views. Due to the lack of a benchmark dataset suitable for the SVSR task, we collected a new stereoscopic video dataset, SVSR-Set, containing 71 full high-definition (HD) stereo videos captured using a professional stereo camera. Extensive experiments on the collected dataset, along with two other datasets, demonstrate that the Trans-SVSR can achieve competitive performance compared to the state-of-the-art methods. Project code and additional results are available at https://github.com/H-deep/Trans-SVSR/.",
    "code_link": ""
  },
  "cvpr2022_ntire_dualheterogeneouscomplementarynetworksforsingleimagederaining": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Dual Heterogeneous Complementary Networks for Single Image Deraining",
    "authors": [
      "Yuuto Nanba",
      "Hikaru Miyata",
      "Xian-Hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Nanba_Dual_Heterogeneous_Complementary_Networks_for_Single_Image_Deraining_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Nanba_Dual_Heterogeneous_Complementary_Networks_for_Single_Image_Deraining_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Single image deraining is an extreme challenge task since it requires to not only recover the spatial detail and high-level contextualized structure of the underlying image but also remove multiple rain layers with various blurring degrees and resolutions. Despite of the great performance advance with the deep learning networks, the dominated researches devote to either constructing deeper and complicated network architecture for recovering reliable detailed texture at the original resolution of the input image or exploiting multi-scale encoder-decode structure for learning semantic context in more larger receptive field while are still far from sufficiency to capture both complementary detailed and semantic contexts. This study proposes a novel dual heterogeneous complementary networks consisting of a main original resolution learning subnet and an auxiliary encoder-decoder subnet for exploring both detailed structure and semantic contexts. Specifically, to capture more plausible intermediate features in dual subnets, we concurrently evaluate the derainingg losses of both branches in training phase, and exploit an auxiliary pseudo-label supervised attention module to further guide the feature learning in the main subnet. Moreover, to reconstruct more nature and sharp images, we incorporate multiple losses for network training including An improved MSE, edge-based loss to recover reliable shape information, and perceptual loss by evaluating the reconstruction error on the feature map of the learned VGGNet model instead of pixel intensity. Experiments on several benchmark deraining datasets demonstrate great superiority over the state-of-the-arts methods.",
    "code_link": ""
  },
  "cvpr2022_ntire_unpairedfacerestorationvialearnablecross-qualityshift": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unpaired Face Restoration via Learnable Cross-Quality Shift",
    "authors": [
      "Yangyi Dong",
      "Xiaoyun Zhang",
      "Zhixin Wang",
      "Ya Zhang",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Dong_Unpaired_Face_Restoration_via_Learnable_Cross-Quality_Shift_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Dong_Unpaired_Face_Restoration_via_Learnable_Cross-Quality_Shift_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Face restoration aims to recover high-quality (HQ) face images from low-quality (LQ) ones with various unknown degradations. Unpaired face restoration approaches focus on the adaptation to unseen degradations, which is a more challenging setting. Recently, generative facial priors of StyleGAN are used to improve the restoration capability of paired face restoration methods. For unpaired methods, however, using face priors is a challenge due to the lack of paired supervision. To address this issue, we take advantage of the editing capabilities of StyleGAN's latent code and propose a novel learnable cross-quality shift. The proposed learnable cross-quality shift not only introduces the generative facial priors into the unpaired framework, but also enables the straight-forward addition/subtraction in the latent feature space to achieve quality conversion. Furthermore, we design a two-branch framework with the proposed cross-quality shift to deal with unpaired data and improve the fidelity of restoration. With the unpaired framework, our method can be fine-tuned on images with unseen degradation. Experimental results show that (i) compared to state-of-the-art methods, our method improves performances under moderate and severe degradation situations; and (ii) both the proposed learnable cross-quality shift and the two-branch framework benefit the restoration performance.",
    "code_link": ""
  },
  "cvpr2022_ntire_comparisonofcomodgans,lamaandglideforartinpaintingcompletingm.ceschersprintgallery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Comparison of CoModGans, LaMa and GLIDE for Art Inpainting Completing M.C Escher's Print Gallery",
    "authors": [
      "Lucia Cipolina-Kun",
      "Simone Caenazzo",
      "Gaston Mazzei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Cipolina-Kun_Comparison_of_CoModGans_LaMa_and_GLIDE_for_Art_Inpainting_Completing_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Cipolina-Kun_Comparison_of_CoModGans_LaMa_and_GLIDE_for_Art_Inpainting_Completing_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Digital art restoration has benefited from inpainting models to correct the degradation or missing sections of a painting. This work compares three current state-of-the art models for inpainting of large missing regions. We provide qualitative and quantitative comparison of the performance of CoModGans, LaMa and GLIDE in inpainting blurry and missing sections of images. We use Escher's incomplete painting Print Gallery as our test study since it presents several of the challenges commonly present in restorative inpainting.",
    "code_link": "https://github.com/zsyzzsoft/co-mod-gan"
  },
  "cvpr2022_ntire_nighttimeimagedehazingbasedonvariationaldecompositionmodel": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Nighttime Image Dehazing Based on Variational Decomposition Model",
    "authors": [
      "Yun Liu",
      "Zhongsheng Yan",
      "Aimin Wu",
      "Tian Ye",
      "Yuche Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Liu_Nighttime_Image_Dehazing_Based_on_Variational_Decomposition_Model_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Liu_Nighttime_Image_Dehazing_Based_on_Variational_Decomposition_Model_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Most of existing dehazing algorithms are unable to deal with nighttime hazy scenarios well due to complex degraded factors such as non-uniform illumination, low light and glows. To obtain high-quality image under nighttime haze imaging conditions, we present an effective single nighttime image dehazing framework based on a variational decomposition model to simultaneously address these undesirable issues. First, a variational decomposition model consisting of three regularization terms is proposed to simultaneously decompose a nighttime hazy image into a structure layer, a detail layer and a noise layer. Concretely, we employ L1 norm to constrain the structure component, adopt L0 sparsity term to enforce the piece-wise continuous of the detail layer, and use L2 norm to separate the noise layer. Next, the structure layer is recovered by means of inversing the physical model and the detail layers are revealed in a multi-scale gradient enhancement manner. Finally, the dehazed structure layer and the enhanced detail layers are integrated into a haze-free image. Experimental results show that the proposed framework achieves superior performance on nighttime haze removal and noise suppression compared with several state-of-the-art dehazing techniques.",
    "code_link": ""
  },
  "cvpr2022_ntire_zoom-to-inpaintimageinpaintingwithhigh-frequencydetails": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Zoom-to-Inpaint: Image Inpainting With High-Frequency Details",
    "authors": [
      "Soo Ye Kim",
      "Kfir Aberman",
      "Nori Kanazawa",
      "Rahul Garg",
      "Neal Wadhwa",
      "Huiwen Chang",
      "Nikhil Karnad",
      "Munchurl Kim",
      "Orly Liba"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Kim_Zoom-to-Inpaint_Image_Inpainting_With_High-Frequency_Details_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Kim_Zoom-to-Inpaint_Image_Inpainting_With_High-Frequency_Details_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Although deep learning has enabled a huge leap forward in image inpainting, current methods are often unable to synthesize realistic high-frequency details. In this paper, we propose applying super-resolution to coarsely reconstructed outputs, refining them at high resolution, and then downscaling the output to the original resolution. By introducing high-resolution images to the refinement network, our framework is able to reconstruct finer details that are usually smoothed out due to spectral bias -- the tendency of neural networks to reconstruct low frequencies better than high frequencies. To assist training the refinement network on large upscaled holes, we propose a progressive learning technique in which the size of the missing regions increases as training progresses. Our zoom-in, refine and zoom-out strategy, combined with high-resolution supervision and progressive learning, constitutes a framework-agnostic approach for enhancing high-frequency details that can be applied to any CNN-based inpainting method. We provide qualitative and quantitative evaluations along with an ablation analysis to show the effectiveness of our approach. This seemingly simple, yet powerful approach, outperforms existing inpainting methods.",
    "code_link": ""
  },
  "cvpr2022_ntire_attentionshelpcnnsseebetterattention-basedhybridimagequalityassessmentnetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Attentions Help CNNs See Better: Attention-Based Hybrid Image Quality Assessment Network",
    "authors": [
      "Shanshan Lao",
      "Yuan Gong",
      "Shuwei Shi",
      "Sidi Yang",
      "Tianhe Wu",
      "Jiahao Wang",
      "Weihao Xia",
      "Yujiu Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Lao_Attentions_Help_CNNs_See_Better_Attention-Based_Hybrid_Image_Quality_Assessment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Lao_Attentions_Help_CNNs_See_Better_Attention-Based_Hybrid_Image_Quality_Assessment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image quality assessment (IQA) algorithm aims to quantify the human perception of image quality. Unfortunately, there is a performance drop when assessing the distortion images generated by generative adversarial network (GAN) with seemingly realistic texture. In this work, we conjecture that this maladaptation lies in the backbone of IQA models, where patch-level prediction methods use independent image patches as input to calculate their scores separately, but lack spatial relationship modeling among image patches. Therefore, we propose an Attention-based Hybrid Image Quality Assessment Network (AHIQ) to deal with the challenge and get better performance on the GAN-based IQA task. Firstly, we adopt a two-branch architecture, including a vision transformer (ViT) branch and a convolutional neural network (CNN) branch for feature extraction. The hybrid architecture combines interaction information among image patches captured by ViT and local texture details from CNN. To make the features from shallow CNN more focused on the visually salient region, a deformable convolution is applied with the help of semantic information from the ViT branch. Finally, we use a patch-wise score prediction module to obtain the final score. The experiments show that our model outperforms the state-of-the-art methods on four standard IQA datasets and AHIQ ranked first on the Full Reference (FR) track of the NTIRE 2022 Perceptual Image Quality Assessment Challenge.",
    "code_link": "https://github.com/IIGROUP/AHIQ"
  },
  "cvpr2022_ntire_motionawaredoubleattentionnetworkfordynamicscenedeblurring": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Motion Aware Double Attention Network for Dynamic Scene Deblurring",
    "authors": [
      "Dan Yang",
      "Mehmet Yamac"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yang_Motion_Aware_Double_Attention_Network_for_Dynamic_Scene_Deblurring_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yang_Motion_Aware_Double_Attention_Network_for_Dynamic_Scene_Deblurring_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Motion deblurring in dynamic scenes is a challenging task when the blurring is caused by one or a combination of various reasons such as moving objects, camera movement, etc. Since event cameras can detect changes in intensity with a low latency, necessary motion information is inherently captured in event data, which could be quite useful for deblurring standard camera images. The degradation intensity does not show homogeneity across an image due to factors like object depth, speed, etc. We propose a twobranch network structure, Motion Aware Double Attention Network (MADANet), that pays special attention to areas with high blur. As part of the network, event data is first used by the high blur region segmentation module that creates a probability-like score for areas exhibiting high relative motion to the camera. Then, the event data is also injected to feature maps in the main body, where there is a second attention mechanism available for each branch. The effective usage of event data and two-level attention mechanisms makes the network very compact. During the experiment, it was shown that the proposed network could achieve state-of-the-art performance not only on the benchmark dataset from GoPro, but also on two newly collected datasets, one of which contains real event data.",
    "code_link": ""
  },
  "cvpr2022_ntire_gamma-enhancedspatialattentionnetworkforefficienthighdynamicrangeimaging": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Gamma-Enhanced Spatial Attention Network for Efficient High Dynamic Range Imaging",
    "authors": [
      "Fangya Li",
      "Ruipeng Gang",
      "Chenghua Li",
      "Jinjing Li",
      "Sai Ma",
      "Chenming Liu",
      "Yizhen Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Li_Gamma-Enhanced_Spatial_Attention_Network_for_Efficient_High_Dynamic_Range_Imaging_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Li_Gamma-Enhanced_Spatial_Attention_Network_for_Efficient_High_Dynamic_Range_Imaging_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "High dynamic range(HDR) imaging is the task of recovering HDR image from one or multiple input Low Dynamic Range (LDR) images. In this paper, we present Gamma-enhanced Spatial Attention Network(GSANet), a novel framework for reconstructing HDR images. This problem comprises two intractable challenges of how to tackle overexposed and underexposed regions and how to overcome the paradox of performance and complexity trade-off. To address the former, after applying gamma correction on the LDR images, we adopt a spatial attention module to adaptively select the most appropriate regions of various exposure low dynamic range images for fusion. For the latter one, we propose an efficient channel attention module, which only involves a handful of parameters while bringing clear performance gain. Experimental results show that the proposed method achieves better visual quality on the HDR dataset. The code will be available at:https://github.com/fancyicookie/GSANet.",
    "code_link": "https://github.com/fancyicookie/GSANet"
  },
  "cvpr2022_ntire_fs-ncsrincreasingdiversityofthesuper-resolutionspaceviafrequencyseparationandnoise-conditionednormalizingflow": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow",
    "authors": [
      "Ki-Ung Song",
      "Dongseok Shim",
      "Kang-wook Kim",
      "Jae-young Lee",
      "Younggeun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Song_FS-NCSR_Increasing_Diversity_of_the_Super-Resolution_Space_via_Frequency_Separation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Song_FS-NCSR_Increasing_Diversity_of_the_Super-Resolution_Space_via_Frequency_Separation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Super-resolution suffers from an innate ill-posed problem that a single low-resolution (LR) image can be from multiple high-resolution (HR) images. Recent studies on the flow-based algorithm solve this ill-posedness by learning the super-resolution space and predicting diverse HR outputs. Unfortunately, the diversity of the super-resolution outputs is still unsatisfactory, and the outputs from the flow-based model usually suffer from undesired artifacts which causes low-quality outputs. In this paper, we propose FS-NCSR which produces diverse and high-quality super-resolution outputs using frequency separation and noise conditioning compared to the existing flow-based approaches. As the sharpness and high-quality detail of the image rely on its high-frequency information, FS-NCSR only estimates the high-frequency information of the high-resolution outputs without redundant low-frequency components. Through this, FS-NCSR significantly improves the diversity score without significant image quality degradation compared to the NCSR, the winner of the previous NTIRE 2021 challenge.",
    "code_link": ""
  },
  "cvpr2022_ntire_progressivetrainingofatwo-stageframeworkforvideorestoration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Progressive Training of a Two-Stage Framework for Video Restoration",
    "authors": [
      "Meisong Zheng",
      "Qunliang Xing",
      "Minglang Qiao",
      "Mai Xu",
      "Lai Jiang",
      "Huaida Liu",
      "Ying Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Zheng_Progressive_Training_of_a_Two-Stage_Framework_for_Video_Restoration_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Zheng_Progressive_Training_of_a_Two-Stage_Framework_for_Video_Restoration_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "As a widely studied task, video restoration aims to enhance the quality of the videos with multiple potential degradations, such as noises, blurs and compression artifacts. Among video restorations, compressed video quality enhancement and video super-resolution are two of the main tacks with significant values in practical scenarios. Recently, recurrent neural networks and transformers attract increasing research interests in this field, due to their impressive capability in sequence-to-sequence modeling. However, the training of these models is not only costly but also relatively hard to converge, with gradient exploding and vanishing problems. To cope with these problems, we proposed a two-stage framework including a multi-frame recurrent network and a single-frame transformer. Besides, multiple training strategies, such as transfer learning and progressive training, are developed to shorten the training time and improve the model performance. Benefiting from the above technical contributions, our solution wins two champions and a runner-up in the NTIRE 2022 super-resolution and quality enhancement of compressed video challenges.",
    "code_link": ""
  },
  "cvpr2022_ntire_nafssrstereoimagesuper-resolutionusingnafnet": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NAFSSR: Stereo Image Super-Resolution Using NAFNet",
    "authors": [
      "Xiaojie Chu",
      "Liangyu Chen",
      "Wenqing Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Chu_NAFSSR_Stereo_Image_Super-Resolution_Using_NAFNet_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Chu_NAFSSR_Stereo_Image_Super-Resolution_Using_NAFNet_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Stereo image super-resolution aims at enhancing the quality of super-resolution results by utilizing the complementary information provided by binocular systems. To obtain reasonable performance, most methods focus on finely designing modules, loss functions, and etc. to exploit information from another viewpoint. This has the side effect of increasing system complexity, making it difficult for researchers to evaluate new ideas and compare methods. This paper inherits a strong and simple image restoration model, NAFNet, for single-view feature extraction and extends it by adding cross attention modules to fuse features between views to adapt to binocular scenarios. The proposed baseline for stereo image super-resolution is noted as NAFSSR. Furthermore, training/testing strategies are proposed to fully exploit the performance of NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012, KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models will be released at https://github.com/megvii-research/NAFNet.",
    "code_link": "https://github.com/megvii-research/NAFNet"
  },
  "cvpr2022_ntire_alightweightnetworkforhighdynamicrangeimaging": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Lightweight Network for High Dynamic Range Imaging",
    "authors": [
      "Qingsen Yan",
      "Song Zhang",
      "Weiye Chen",
      "Yuhang Liu",
      "Zhen Zhang",
      "Yanning Zhang",
      "Javen Qinfeng Shi",
      "Dong Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yan_A_Lightweight_Network_for_High_Dynamic_Range_Imaging_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yan_A_Lightweight_Network_for_High_Dynamic_Range_Imaging_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-frame high dynamic range (HDR) reconstruction methods try to expand the range of illuminance with differently exposed images. They suffer from ghost artifacts when camera jittering or object moving. Several methods can generate high-quality HDR images with high computational complexity, but the inference process is too slow. However, the network with small parameters will produce unsatisfactory results. To balance the quality and computational complexity, we propose a lightweight network for HDR imaging that has small parameters and fast speed. Specifically, following AHDRNet, we employ a spatial attention module to detect the misaligned regions to avoid ghost artifacts. Considering the missing details in over-/under- exposure regions, we propose a dual attention module for selectively retaining information to force the fusion network to learn more details for degenerated regions. Furthermore, we employ an encoder-decoder structure with a lightweight block to achieve the fusion process. As a result, the high-quality content and features can be reconstructed after the attention module. Finally, we fuse high-resolution features and the encoder-decoder features into the HDR imaging results. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods, achieving a PSNR of 39.05 and a PSNR-mu of 37.27 with 156.12 GMAcs in NTIRE 2022 HDR Challenge (Track 2 Fidelity).",
    "code_link": ""
  },
  "cvpr2022_ntire_transformerforsingleimagesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Transformer for Single Image Super-Resolution",
    "authors": [
      "Zhisheng Lu",
      "Juncheng Li",
      "Hong Liu",
      "Chaoyan Huang",
      "Linlin Zhang",
      "Tieyong Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Lu_Transformer_for_Single_Image_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Lu_Transformer_for_Single_Image_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Single image super-resolution (SISR) has witnessed great strides with the development of deep learning. However, most existing studies focus on building more complex networks with a massive number of layers. Recently, more and more researchers start to explore the application of Transformer in computer vision tasks. However, the heavy computational cost and high GPU memory occupation of the vision Transformer cannot be ignored. In this paper, we propose a novel Efficient Super-Resolution Transformer (ESRT) for SISR. ESRT is a hybrid model, which consists of a Lightweight CNN Backbone (LCB) and a Lightweight Transformer Backbone (LTB). Among them, LCB can dynamically adjust the size of the feature map to extract deep features with a low computational cost. LTB is composed of a series of Efficient Transformers (ET), which occupies a small GPU memory occupation, thanks to the specially designed Efficient Multi-Head Attention (EMHA). Extensive experiments show that ESRT achieves competitive results with low computational costs. Compared with the original Transformer which occupies 16,057M GPU memory, ESRT only occupies 4,191M GPU memory. All codes are available at https://github.com/luissen/ESRT.",
    "code_link": "https://github.com/luissen/ESRT"
  },
  "cvpr2022_ntire_maniqamulti-dimensionattentionnetworkforno-referenceimagequalityassessment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "MANIQA: Multi-Dimension Attention Network for No-Reference Image Quality Assessment",
    "authors": [
      "Sidi Yang",
      "Tianhe Wu",
      "Shuwei Shi",
      "Shanshan Lao",
      "Yuan Gong",
      "Mingdeng Cao",
      "Jiahao Wang",
      "Yujiu Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yang_MANIQA_Multi-Dimension_Attention_Network_for_No-Reference_Image_Quality_Assessment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yang_MANIQA_Multi-Dimension_Attention_Network_for_No-Reference_Image_Quality_Assessment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual quality of images in accordance with human subjective perception. Unfortunately, existing NR-IQA methods are far from meeting the needs of predicting accurate quality scores on GAN-based distortion images. To this end, we propose Multi-dimension Attention Network for no-reference Image Quality Assessment (MANIQA) to improve the performance on GAN-based distortion. We firstly extract features via ViT, then to strengthen global and local interactions, we propose the Transposed Attention Block (TAB) and the Scale Swin Transformer Block (SSTB). These two modules apply attention mechanisms across the channel and spatial dimension, respectively. In this multi-dimensional manner, the modules cooperatively increase the interaction among different regions of images globally and locally. Finally, a dual branch structure for patch-weighted quality prediction is applied to predict the final score depending on the weight of each patch's score. Experimental results demonstrate that MANIQA outperforms state-of-the-art methods on four standard datasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our method ranked first place in the final testing phase of the NTIRE 2022 Perceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and models are available at https://github.com/IIGROUP/MANIQA.",
    "code_link": "https://github.com/IIGROUP/MANIQA"
  },
  "cvpr2022_ntire_genispneuralispforlow-lightmachinecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "GenISP: Neural ISP for Low-Light Machine Cognition",
    "authors": [
      "Igor Morawski",
      "Yu-An Chen",
      "Yu-Sheng Lin",
      "Shusil Dangi",
      "Kai He",
      "Winston H. Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Morawski_GenISP_Neural_ISP_for_Low-Light_Machine_Cognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Morawski_GenISP_Neural_ISP_for_Low-Light_Machine_Cognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Object detection in low-light conditions remains a challenging but important problem with many practical implications. Some recent works show that, in low-light conditions, object detectors using raw image data are more robust than detectors using image data processed by a traditional ISP pipeline. To improve detection performance in low-light conditions, one can fine-tune the detector to use raw image data or use a dedicated low-light neural pipeline trained with paired low- and normal-light data to restore and enhance the image. However, different camera sensors have different spectral sensitivity and learning-based models using raw images process data in the sensor-specific color space. Thus, once trained, they do not guarantee generalization to other camera sensors. We propose to improve generalization to unseen camera sensors by implementing a minimal neural ISP pipeline for machine cognition, named GenISP, that explicitly incorporates Color Space Transformation to a device-independent color space. We also propose a two-stage color processing implemented by two image-to-parameter modules that take down-sized image as input and regress global color correction parameters. Moreover, we propose to train our proposed GenISP under the guidance of a pre-trained object detector and avoid making assumptions about perceptual quality of the image, but rather optimize the image representation for machine cognition. At the inference stage, GenISP can be paired with any object detector. We perform extensive experiments to compare our proposed method to other low-light image restoration and enhancement methods in an extrinsic task-based evaluation and validate that GenISP can generalize to unseen sensors and object detectors. Finally, we contribute a low-light dataset of 7K raw images annotated with 46K bounding boxes for task-based benchmarking of future low-light image restoration and low-light object detection.",
    "code_link": ""
  },
  "cvpr2022_ntire_drhdradualbranchresidualnetworkformulti-brackethighdynamicrangeimaging": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DRHDR: A Dual Branch Residual Network for Multi-Bracket High Dynamic Range Imaging",
    "authors": [
      "Juan Mar\u00edn-Vega",
      "Michael Sloth",
      "Peter Schneider-Kamp",
      "Richard R\u00f6ttger"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Marin-Vega_DRHDR_A_Dual_Branch_Residual_Network_for_Multi-Bracket_High_Dynamic_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Marin-Vega_DRHDR_A_Dual_Branch_Residual_Network_for_Multi-Bracket_High_Dynamic_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We introduce DRHDR, a Dual branch Residual Convolutional Neural Network for Multi-Bracket HDR Imaging. In order to address the challenges of fusing multiple brackets from dynamic scenes, we propose an efficient dual branch network that operates on two different resolutions. The full resolution branch uses a Deformable Convolutional Block to align features and retain high-frequency details. A low resolution branch with a Spatial Attention Block aims to attend wanted areas from the non-reference brackets, and suppress displaced features that could incur on ghosting artifacts. By using a dual branch approach we are able to achieve high quality results while constraining the computational resources required to estimate the HDR results.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022spectralrecoverychallengeanddataset": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Spectral Recovery Challenge and Data Set",
    "authors": [
      "Boaz Arad",
      "Radu Timofte",
      "Rony Yahel",
      "Nimrod Morag",
      "Amir Bernat",
      "Yuanhao Cai",
      "Jing Lin",
      "Zudi Lin",
      "Haoqian Wang",
      "Yulun Zhang",
      "Hanspeter Pfister",
      "Luc Van Gool",
      "Shuai Liu",
      "Yongqiang Li",
      "Chaoyu Feng",
      "Lei Lei",
      "Jiaojiao Li",
      "Songcheng Du",
      "Chaoxiong Wu",
      "Yihong Leng",
      "Rui Song",
      "Mingwei Zhang",
      "Chongxing Song",
      "Shuyi Zhao",
      "Zhiqiang Lang",
      "Wei Wei",
      "Lei Zhang",
      "Renwei Dian",
      "Tianci Shan",
      "Anjing Guo",
      "Chengguo Feng",
      "Jinyang Liu",
      "Mirko Agarla",
      "Simone Bianco",
      "Marco Buzzelli",
      "Luigi Celona",
      "Raimondo Schettini",
      "Jiang He",
      "Yi Xiao",
      "Jiajun Xiao",
      "Qiangqiang Yuan",
      "Jie Li",
      "Liangpei Zhang",
      "Taesung Kwon",
      "Dohoon Ryu",
      "Hyokyoung Bae",
      "Hao-Hsiang Yang",
      "Hua-En Chang",
      "Zhi-Kai Huang",
      "Wei-Ting Chen",
      "Sy-Yen Kuo",
      "Junyu Chen",
      "Haiwei Li",
      "Song Liu",
      "Sabarinathan",
      "K Uma",
      "B Sathya Bama",
      "S. Mohamed Mansoor Roomi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reviews the third biennial challenge on spectral reconstruction from RGB images, i.e., the recovery of whole-scene hyperspectral (HS) information from a 3-channel RGB image. This challenge presents the \"ARAD_1K\" data set: a new, larger-than-ever natural hyperspectral image data set containing 1,000 images. Challenge participants were required to recover hyperspectral information from synthetically generated JPEG-compressed RGB images simulating capture by a known calibrated camera, operating under partially known parameters, in a setting which includes acquisition noise. The challenge was attended by 241 teams, with 60 teams competing in the final testing phase, 12 of which provided detailed descriptions of their methodology which are included in this report. The performance of these submissions is reviewed and provided here as a gauge for the current state-of-the-art in spectral reconstruction from natural RGB images.",
    "code_link": ""
  },
  "cvpr2022_ntire_ntire2022challengeonsuper-resolutionandqualityenhancementofcompressedvideodataset,methodsandresults": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results",
    "authors": [
      "Ren Yang",
      "Radu Timofte",
      "Meisong Zheng",
      "Qunliang Xing",
      "Minglang Qiao",
      "Mai Xu",
      "Lai Jiang",
      "Huaida Liu",
      "Ying Chen",
      "Youcheng Ben",
      "Xiao Zhou",
      "Chen Fu",
      "Pei Cheng",
      "Gang Yu",
      "Junyi Li",
      "Renlong Wu",
      "Zhilu Zhang",
      "Wei Shang",
      "Zhengyao Lv",
      "Yunjin Chen",
      "Mingcai Zhou",
      "Dongwei Ren",
      "Kai Zhang",
      "Wangmeng Zuo",
      "Pavel Ostyakov",
      "Vyal Dmitry",
      "Shakarim Soltanayev",
      "Chervontsev Sergey",
      "Zhussip Magauiya",
      "Xueyi Zou",
      "Youliang Yan",
      "Pablo Navarrete Michelini",
      "Yunhua Lu",
      "Diankai Zhang",
      "Shaoli Liu",
      "Si Gao",
      "Biao Wu",
      "Chengjian Zheng",
      "Xiaofeng Zhang",
      "Kaidi Lu",
      "Ning Wang",
      "Thuong Nguyen Canh",
      "Thong Bach",
      "Qing Wang",
      "Xiaopeng Sun",
      "Haoyu Ma",
      "Shijie Zhao",
      "Junlin Li",
      "Liangbin Xie",
      "Shuwei Shi",
      "Yujiu Yang",
      "Xintao Wang",
      "Jinjin Gu",
      "Chao Dong",
      "Xiaodi Shi",
      "Chunmei Nian",
      "Dong Jiang",
      "Jucai Lin",
      "Zhihuai Xie",
      "Mao Ye",
      "Dengyan Luo",
      "Liuhan Peng",
      "Shengjie Chen",
      "Xin Liu",
      "Xin Liu",
      "Qian Wang",
      "Boyang Liang",
      "Hang Dong",
      "Yuhao Huang",
      "Kai Chen",
      "Xingbei Guo",
      "Yujing Sun",
      "Huilei Wu",
      "Pengxu Wei",
      "Yulin Huang",
      "Junying Chen",
      "Ik Hyun Lee",
      "Sunder Ali Khowaja",
      "Jiseok Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yang_NTIRE_2022_Challenge_on_Super-Resolution_and_Quality_Enhancement_of_Compressed_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yang_NTIRE_2022_Challenge_on_Super-Resolution_and_Quality_Enhancement_of_Compressed_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0 dataset, which includes the LDV dataset (240 videos) and 95 additional videos. This challenge includes three tracks. Track 1 aims at enhancing the videos compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the super-resolution and quality enhancement of HEVC compressed video. They require x2 and x4 super-resolution, respectively. The three tracks totally attract more than 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams submitted the final results to Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution and quality enhancement of compressed video. The proposed LDV 2.0 dataset is available at https://github.com/RenYang-home/LDV_dataset. The homepage of this challenge (including open-sourced codes) is at https://github.com/RenYang-home/NTIRE22_VEnh_SR.",
    "code_link": ""
  },
  "cvpr2022_ntire_lanlightweightattention-basednetworkforraw-to-rgbsmartphoneimageprocessing": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "LAN: Lightweight Attention-Based Network for RAW-to-RGB Smartphone Image Processing",
    "authors": [
      "Daniel Wirzberger Raimundo",
      "Andrey Ignatov",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Raimundo_LAN_Lightweight_Attention-Based_Network_for_RAW-to-RGB_Smartphone_Image_Processing_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Raimundo_LAN_Lightweight_Attention-Based_Network_for_RAW-to-RGB_Smartphone_Image_Processing_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The number of pictures taken by smartphones is growing exponentially. However, the smartphones' limitations both in size and cost negatively impact on the quality of the implemented sensors. At the same time, their computing power has also been steadily improving, allowing the usage of more complex processing methods to enhance images. In prior works, deep neural networks trained with matched sensor outputs and DSLR images have shown to bring substantial improvements to the images, compared to classical and handcrafted methods. We propose a lightweight attention-based network (LAN) that employs a convolutional layer to learn the input mosaic and an unsupervised pre-training strategy. Our method is validated on standard benchmarks and shown to improve over the state-of-the-art in both perceptual and fidelity terms without hindering GPU inference time on smartphone devices.",
    "code_link": "https://github.com/draimundo/LAN"
  },
  "cvpr2022_ntire_exploitingdistortioninformationformulti-degradedimagerestoration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Exploiting Distortion Information for Multi-Degraded Image Restoration",
    "authors": [
      "Wooksu Shin",
      "Namhyuk Ahn",
      "Jeong-Hyeon Moon",
      "Kyung-Ah Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Shin_Exploiting_Distortion_Information_for_Multi-Degraded_Image_Restoration_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Shin_Exploiting_Distortion_Information_for_Multi-Degraded_Image_Restoration_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In recent years, tremendous studies have been performed on the image distortion restoration task and deep learning-based methods have shown prominent performance improvement. However, assuming only a single distortion to an image may not be applicable in many real-world scenarios. To mitigate the issue, some studies have proposed multi-distortion datasets by applying the corruptions sequentially or spatially. In this work, we integrate the two perspectives on the multi-distortion nature and propose a new dataset that is a holistic multi-distortion dataset. To restore the multi-distortion effectively, we introduce a distortion information-guided restoration network, which exploits the conditional distortion information when reconstructing a given image. To do that, our framework first predicts the distortion type and their strength and delivers these to the restoration module. In our experiments, we show that the proposed model exceeds the others and we also demonstrate that any backbone network benefits from receiving the distortion information as prior knowledge.",
    "code_link": ""
  },
  "cvpr2022_ntire_completeandtemporallyconsistentvideooutpainting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Complete and Temporally Consistent Video Outpainting",
    "authors": [
      "Lo\u00efc Dehan",
      "Wiebe Van Ranst",
      "Patrick Vandewalle",
      "Toon Goedem\u00e9"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Dehan_Complete_and_Temporally_Consistent_Video_Outpainting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Dehan_Complete_and_Temporally_Consistent_Video_Outpainting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We describe a novel method for video outpainting. The goal of outpainting is to fill in missing regions at the edges of video frames. Our focus lies on converting portrait (9:16) to landscape (16:9) video. In contrast, most video completion research is focused on inpainting: filling a masked section within the frame based on the remaining, known pixels. Our proposed method consists of three main aspects: (1) We form a background estimation using video object segmentation and video inpainting methods, (2) we use optical flow to form temporal consistency, and (3) we propose image shifting to improve individual frame completions. Our method is able to successfully broaden the aspect ratio of a video. On most videos, we achieve realistic results. Only on videos with complex camera motion and foreground objects leaving the frame, the quality is less. In contrast to other state-of-the-art methods, our method is able to reconstruct the full frame, including unseen image parts. Moreover, it is temporally consistent. We evaluate our method on the DAVIS and YouTube-VOS datasets. The code is publicly available.",
    "code_link": ""
  },
  "cvpr2022_ntire_deep-flexispathree-stageframeworkfornightphotographyrendering": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep-FlexISP: A Three-Stage Framework for Night Photography Rendering",
    "authors": [
      "Shuai Liu",
      "Chaoyu Feng",
      "Xiaotao Wang",
      "Hao Wang",
      "Ran Zhu",
      "Yongqiang Li",
      "Lei Lei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Liu_Deep-FlexISP_A_Three-Stage_Framework_for_Night_Photography_Rendering_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Liu_Deep-FlexISP_A_Three-Stage_Framework_for_Night_Photography_Rendering_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Night photography rendering is challenging due to images' high noise level, less vivid color, and low dynamic range. In this work, we propose a three-stage cascade framework named Deep-FlexISP, which decomposes the ISP into three weakly correlated sub-tasks: raw image denoising, white balance, and Bayer to sRGB mapping, for the following considerations. First, task decomposition can enhance the learning ability of the framework and make it easier to converge. Second, weak correlation sub-tasks do not influence each other too much, so the framework has a high degree of freedom. Finally, noise, color, and brightness are essential for night photographs. Our framework can flexibly adjust different styles according to personal preferences with the vital learning ability and the degree of freedom. Compared with the other Deep-ISP methods, our proposed Deep-FlexISP shows state-of-the-art performance and achieves first place in people's choice and photographer's choice in NTIRE 2022 Night Photography Render Challenge.",
    "code_link": ""
  },
  "cvpr2022_ntire_swinipassrswintransformerbasedparallaxattentionnetworkforstereoimagesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "SwiniPASSR: Swin Transformer Based Parallax Attention Network for Stereo Image Super-Resolution",
    "authors": [
      "Kai Jin",
      "Zeqiang Wei",
      "Angulia Yang",
      "Sha Guo",
      "Mingzhi Gao",
      "Xiuzhuang Zhou",
      "Guodong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Jin_SwiniPASSR_Swin_Transformer_Based_Parallax_Attention_Network_for_Stereo_Image_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Jin_SwiniPASSR_Swin_Transformer_Based_Parallax_Attention_Network_for_Stereo_Image_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With binocular cameras being widely accepted, the study of stereo image super resolution (Stereo SR) has received increasing attention. Different from single image super resolution (SISR) setting, it is more challenging for utilizing both intra-view and cross-view information. Although prior convolution-based works have achieved admirable progress, few attempts have explored the possibility of the Transformer-based architecture for stereo image SR, which has demonstrated promising performance in several visual tasks. In this paper, we propose a novel approach namely SwiniPASSR, which adopts Swin Transformer as the backbone, meanwhile incorporating it with the Bi-directional Parallax Attention Module (biPAM) to maximize auxiliary information given by the binocular mechanism. Even Transformer and parallax attention mechanism (PAM) have been separately proved usefulness by prior studies, we find that simply integrating convolution-based PAM with Transformer or directly optimizing for stereo SR problem was may not achieve desirable result. We therefore introduced a conversion layer to resolve integration and adopted progressive training strategy to learn disparity correspondence through progressively enlarged receptive fields. Both extensive experiments and ablation studies demonstrate the effectiveness of our proposed SwiniPASSR. In particular, in the NTIRE 2022: Stereo Image Super-Resolution Challenge, we report 23.71dB PSNR and 0.7295 SSIM performance which ranked 2nd place on the leaderboard. Source code is available at https://github.com/SMI-Lab/SwinIPASSR.",
    "code_link": ""
  },
  "cvpr2022_ntire_alphamattegenerationfromsingleinputforportraitmatting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Alpha Matte Generation From Single Input for Portrait Matting",
    "authors": [
      "Dogucan Yaman",
      "Hazim Kemal Ekenel",
      "Alexander Waibel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yaman_Alpha_Matte_Generation_From_Single_Input_for_Portrait_Matting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yaman_Alpha_Matte_Generation_From_Single_Input_for_Portrait_Matting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In the portrait matting, the goal is to predict an alpha matte that identifies the effect of each pixel on the foreground subject. Traditional approaches and most of the existing works utilized an additional input, e.g., trimap, background image, to predict alpha matte. However, (1) providing additional input is not always practical, and (2) models are too sensitive to these additional inputs. To address these points, in this paper, we introduce an additional input-free approach to perform portrait matting. We divide the task into two subtasks, segmentation and alpha matte prediction. We first generate a coarse segmentation map from the input image and then predict the alpha matte by utilizing the image and segmentation map. Besides, we present a segmentation encoding block to downsample the coarse segmentation map and provide useful feature representation to the residual block, since using a single encoder causes the vanishing of the segmentation information. We tested our model on four different benchmark datasets. The proposed method outperformed the MODNet and MGMatting methods that also take a single input. Besides, we obtained comparable results with BGM-V2 and FBA methods that require additional input.",
    "code_link": ""
  },
  "cvpr2022_earthvision_fastbuildingsegmentationfromsatelliteimageryandfewlocallabels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Fast Building Segmentation From Satellite Imagery and Few Local Labels",
    "authors": [
      "Caleb Robinson",
      "Anthony Ortiz",
      "Hogeun Park",
      "Nancy Lozano",
      "Jon Kher Kaw",
      "Tina Sederholm",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Robinson_Fast_Building_Segmentation_From_Satellite_Imagery_and_Few_Local_Labels_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Robinson_Fast_Building_Segmentation_From_Satellite_Imagery_and_Few_Local_Labels_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Innovations in computer vision algorithms for satellite image analysis can enable us to explore global challenges such as urbanization and land use change at the planetary level. However, domain shift problems are a common occurrence when trying to replicate models that drive these analyses to new areas, particularly in the developing world. If a model is trained with imagery and labels from one location, then it usually will not generalize well to new locations where the content of the imagery and data distributions are different. In this work, we consider the setting in which we have a single large satellite imagery scene over which we want to solve an applied problem -- building footprint segmentation. Here, we do not necessarily need to worry about creating a model that generalizes past the borders of our scene but can instead train a local model. We show that surprisingly few labels are needed to solve the building segmentation problem with very high-resolution (0.5m/px) satellite imagery with this setting in mind. Our best model trained with just 527 sparse polygon annotations (an equivalent of 1500 x 1500 densely labeled pixels) has a recall of 0.87 over held out footprints and a R2 of 0.93 on the task of counting the number of buildings in 200 x 200 meter windows. We apply our models over high-resolution imagery in Amman, Jordan in a case study on urban change detection.",
    "code_link": ""
  },
  "cvpr2022_earthvision_sat-nerflearningmulti-viewsatellitephotogrammetrywithtransientobjectsandshadowmodelingusingrpccameras": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras",
    "authors": [
      "Roger Mar\u00ed",
      "Gabriele Facciolo",
      "Thibaud Ehret"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Mari_Sat-NeRF_Learning_Multi-View_Satellite_Photogrammetry_With_Transient_Objects_and_Shadow_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Mari_Sat-NeRF_Learning_Multi-View_Satellite_Photogrammetry_With_Transient_Objects_and_Shadow_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF combines some of the latest trends in neural rendering with native satellite camera models, represented by rational polynomial coefficient (RPC) functions. The proposed method renders new views and infers surface models of similar quality to those obtained with traditional state-of-the-art stereo pipelines. Multi-date images exhibit significant changes in appearance, mainly due to varying shadows and transient objects (cars, vegetation). Robustness to these challenges is achieved by a shadow-aware irradiance model and uncertainty weighting to deal with transient phenomena that cannot be explained by the position of the sun. We evaluate Sat-NeRF using WorldView-3 images from different locations and stress the advantages of applying a bundle adjustment to the satellite camera models prior to training. This boosts the network performance and can optionally be used to extract additional cues for depth supervision.",
    "code_link": ""
  },
  "cvpr2022_earthvision_single-shotend-to-endroadgraphextraction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Single-Shot End-to-End Road Graph Extraction",
    "authors": [
      "Gaetan Bahl",
      "Mehdi Bahri",
      "Florent Lafarge"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Bahl_Single-Shot_End-to-End_Road_Graph_Extraction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Bahl_Single-Shot_End-to-End_Road_Graph_Extraction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Automatic road graph extraction from aerial and satellite images is a long-standing challenge. Existing algorithms are either based on pixel-level segmentation followed by vectorization, or on iterative graph construction using next move prediction. Both of these strategies suffer from severe drawbacks, in particular high computing resources and incomplete outputs. By contrast, we propose a method that directly infers the final road graph in a single pass. The key idea consists in combining a Fully Convolutional Network in charge of locating points of interest such as intersections, dead ends and turns, and a Graph Neural Network which predicts links between these points. Such a strategy is more efficient than iterative methods and allows us to streamline the training process by removing the need for generation of starting locations while keeping the training end-to-end. We evaluate our method against existing works on the popular RoadTracer dataset and achieve competitive results. We also benchmark the speed of our method and show that it outperforms existing approaches. Our method opens the possibility of in-flight processing on embedded devices for applications such as real-time road network monitoring and alerts for disaster response.",
    "code_link": ""
  },
  "cvpr2022_earthvision_self-supervisedvisiontransformersforland-coversegmentationandclassification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Self-Supervised Vision Transformers for Land-Cover Segmentation and Classification",
    "authors": [
      "Linus Scheibenreif",
      "Jo\u00eblle Hanna",
      "Michael Mommert",
      "Damian Borth"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Scheibenreif_Self-Supervised_Vision_Transformers_for_Land-Cover_Segmentation_and_Classification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Scheibenreif_Self-Supervised_Vision_Transformers_for_Land-Cover_Segmentation_and_Classification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Transformer models have recently approached or even surpassed the performance of ConvNets on computer vision tasks like classification and segmentation. To a large degree, these successes have been enabled by the use of large-scale labelled image datasets for supervised pre-training. This poses a significant challenge for the adaption of vision Transformers to domains where datasets with millions of labelled samples are not available. In this work, we bridge the gap between ConvNets and Transformers for Earth observation by self-supervised pre-training on large-scale unlabelled remote sensing data. We show that self-supervised pre-training yields latent task-agnostic representations that can be utilized for both land cover classification and segmentation tasks, where they significantly outperform the fully supervised baselines. Additionally, we find that subsequent fine-tuning of Transformers for specific downstream tasks performs on-par with commonly used ConvNet architectures. An ablation study further illustrates that the labelled dataset size can be reduced to one-tenth after self-supervised pre-training while still maintaining the performance of the fully supervised approach.",
    "code_link": "https://github.com/HSG-AIML/SSLTransformerRS"
  },
  "cvpr2022_earthvision_urbanbuildingclassification(ubc)-adatasetforindividualbuildingdetectionandclassificationfromsatelliteimagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Urban Building Classification (UBC) - A Dataset for Individual Building Detection and Classification From Satellite Imagery",
    "authors": [
      "Xingliang Huang",
      "Libo Ren",
      "Chenglong Liu",
      "Yixuan Wang",
      "Hongfeng Yu",
      "Michael Schmitt",
      "Ronny H\u00e4nsch",
      "Xian Sun",
      "Hai Huang",
      "Helmut Mayer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Huang_Urban_Building_Classification_UBC_-_A_Dataset_for_Individual_Building_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Huang_Urban_Building_Classification_UBC_-_A_Dataset_for_Individual_Building_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a dataset for building detection and classification from very high-resolution satellite imagery with the focus on object-level interpretation of individual buildings. It is meant to provide not only a flexible test platform for object detection algorithms but also a solid basis for the comparison of city morphologies and the investigation of urban planning. In most current open datasets, buildings are treated either as a class of landcover in the form of masks or as simple objects defined by separate contours (footprints). Our dataset, instead, represents individual buildings using in-depth object-level descriptions concerning geometry as well as functionality. Buildings are treated as objects with individual ID and boundary. Adjacent building blocks are also separated according to house numbers making a subsequent high-level classification of individual buildings possible. The buildings are classified into predefined roof types, such as flat, gable and hipped roof as well as functional purposes, i.e., residential, commercial, industrial, public, and their sub-classes, e.g., single-family house, office building and school. In the first version of the dataset we provide selected urban areas from two cities: Beijing in China and Munich in Germany. It, therefore, (1) allows to verify algorithms that are not only valid for specific regions but also work robustly in spite of the diversity of cities on different continents with various land forms and styles of architecture and at the same time (2) provides the possibility to quantitatively compare the statistics and morphology of different cities. It is planned to extend the dataset by a continuous integration of various urban areas worldwide.",
    "code_link": ""
  },
  "cvpr2022_earthvision_generalizedclassificationofsatelliteimagetimeserieswiththermalpositionalencoding": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Generalized Classification of Satellite Image Time Series With Thermal Positional Encoding",
    "authors": [
      "Joachim Nyborg",
      "Charlotte Pelletier",
      "Ira Assent"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Nyborg_Generalized_Classification_of_Satellite_Image_Time_Series_With_Thermal_Positional_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Nyborg_Generalized_Classification_of_Satellite_Image_Time_Series_With_Thermal_Positional_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Large-scale crop type classification is a task at the core of remote sensing efforts with applications of both economic and ecological importance. Current state-of-the-art deep learning methods are based on self-attention and use satellite image time series (SITS) to discriminate crop types based on their unique growth patterns. However, existing methods generalize poorly to regions not seen during training mainly due to not being robust to temporal shifts of the growing season caused by variations in climate. To this end, we propose Thermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike previous positional encoding based on calendar time (e.g. day-of-year), TPE is based on thermal time, which is obtained by accumulating daily average temperatures over the growing season. Since crop growth is directly related to thermal time, but not calendar time, TPE addresses the temporal shifts between different regions to improve generalization. We propose multiple TPE strategies, including learnable methods, to further improve results compared to the common fixed positional encodings. We demonstrate our approach on a crop classification task across four different European regions, where we obtain state-of-the-art generalization results.",
    "code_link": ""
  },
  "cvpr2022_earthvision_cross-datasetlearningforgeneralizablelandusesceneclassification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Cross-Dataset Learning for Generalizable Land Use Scene Classification",
    "authors": [
      "Dimitri Gominski",
      "Val\u00e9rie Gouet-Brunet",
      "Liming Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Gominski_Cross-Dataset_Learning_for_Generalizable_Land_Use_Scene_Classification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Gominski_Cross-Dataset_Learning_for_Generalizable_Land_Use_Scene_Classification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Few-shot and cross-domain land use scene classification methods propose solutions to classify unseen classes or unseen visual distributions, but are hardly applicable to real-world situations due to restrictive assumptions. Few-shot methods involve episodic training on restrictive training subsets with small feature extractors, while cross-domain methods are only applied to common classes. The underlying challenge remains open: can we accurately classify new scenes on new datasets? In this paper, we propose a new framework for few-shot, cross-domain classification. Our retrieval-inspired approach exploits the interrelations in both the training and testing data to output class labels using compact descriptors. Results show that our method can accurately produce land-use predictions on unseen datasets and unseen classes, going beyond the traditional few-shot or cross-domain formulation, and allowing cross-dataset training.",
    "code_link": "https://github.com/dgominski/generalizablersc"
  },
  "cvpr2022_earthvision_self-supervisedlearningtoguidescientificallyrelevantcategorizationofmartianterrainimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Self-Supervised Learning To Guide Scientifically Relevant Categorization of Martian Terrain Images",
    "authors": [
      "Tejas Panambur",
      "Deep Chakraborty",
      "Melissa Meyer",
      "Ralph Milliken",
      "Erik Learned-Miller",
      "Mario Parente"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Panambur_Self-Supervised_Learning_To_Guide_Scientifically_Relevant_Categorization_of_Martian_Terrain_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Panambur_Self-Supervised_Learning_To_Guide_Scientifically_Relevant_Categorization_of_Martian_Terrain_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Automatic terrain recognition in Mars rover images is an important problem not just for navigation, but for scientists interested in studying rock types, and by extension, conditions of the ancient Martian paleoclimate and habitability. Existing approaches to label Martian terrain either involve the use of non-expert annotators producing taxonomies of limited granularity (e.g. soil, sand, bedrock, float rock, etc.), or rely on generic class discovery approaches that tend to produce perceptual classes such as rover parts and landscape, which are irrelevant to geologic analysis. Expert-labeled datasets containing granular geological/geomorphological terrain categories are rare or inaccessible to public, and sometimes require the extraction of relevant categorical information from complex annotations. In order to facilitate the creation of a dataset with detailed terrain categories, we present a self-supervised method that can cluster sedimentary textures in images captured from the Mast camera onboard the Curiosity rover (Mars Science Laboratory). We then present a qualitative analysis of these clusters and describe their geologic significance via the creation of a set of granular terrain categories. The precision and geologic validation of these automatically discovered clusters suggest that our methods are promising for the rapid classification of important geologic features and will therefore facilitate our long-term goal of producing a large, granular, and publicly available dataset for Mars terrain recognition. Code and datasets are available at https://github.com/TejasPanambur/mastcam.",
    "code_link": "https://github.com/TejasPanambur/mastcam"
  },
  "cvpr2022_earthvision_multi-layermodelingofdensevegetationfromaeriallidarscans": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Multi-Layer Modeling of Dense Vegetation From Aerial LiDAR Scans",
    "authors": [
      "Ekaterina Kalinicheva",
      "Loic Landrieu",
      "Cl\u00e9ment Mallet",
      "Nesrine Chehata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Kalinicheva_Multi-Layer_Modeling_of_Dense_Vegetation_From_Aerial_LiDAR_Scans_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Kalinicheva_Multi-Layer_Modeling_of_Dense_Vegetation_From_Aerial_LiDAR_Scans_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The analysis of the multi-layer structure of wild forests is an important challenge of automated large-scale forestry. While modern aerial LiDARs offer geometric information across all vegetation layers, most datasets and methods focus only on the segmentation and reconstruction of the top of canopy. We release WildForest3D, which consists of 29 study plots and over 2000 individual trees across 47,000m2 with dense 3D annotation. We propose a 3D deep network architecture predicting for the first time both 3D point-wise labels and high-resolution layer occupancy rasters simultaneously. This allows us to produce a precise estimation of the thickness of each vegetation layer as well as the corresponding watertight meshes, therefore meeting most forestry purposes. Both the dataset and the model are released in open access: https://github.com/ekalinicheva/multi_layer_vegetation.",
    "code_link": ""
  },
  "cvpr2022_earthvision_prompt-rsvqapromptingvisualcontexttoalanguagemodelforremotesensingvisualquestionanswering": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Prompt-RSVQA: Prompting Visual Context to a Language Model for Remote Sensing Visual Question Answering",
    "authors": [
      "Christel Chappuis",
      "Val\u00e9rie Zermatten",
      "Sylvain Lobry",
      "Bertrand Le Saux",
      "Devis Tuia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Chappuis_Prompt-RSVQA_Prompting_Visual_Context_to_a_Language_Model_for_Remote_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Chappuis_Prompt-RSVQA_Prompting_Visual_Context_to_a_Language_Model_for_Remote_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Remote sensing visual question answering (RSVQA) was recently proposed with the aim of interfacing natural language and vision to ease the access of information contained in Earth Observation data for a wide audience, which is granted by simple questions in natural language. The traditional vision/language interface is an embedding obtained by fusing features from two deep models, one processing the image and another the question. Despite the success of early VQA models, it remains difficult to control the adequacy of the visual information extracted by its deep model, which should act as a context regularizing the work of the language model. We propose to extract this context information with a visual model, convert it to text and inject it, i.e. prompt it, into a language model. The language model is therefore responsible to process the question with the visual context, and extract features, which are useful to find the answer. We study the effect of prompting with respect to a black-box visual extractor and discuss the importance of training a visual model producing accurate context.",
    "code_link": ""
  },
  "cvpr2022_earthvision_opensentinelmapalarge-scalelandusedatasetusingopenstreetmapandsentinel-2imagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "OpenSentinelMap: A Large-Scale Land Use Dataset Using OpenStreetMap and Sentinel-2 Imagery",
    "authors": [
      "Noah Johnson",
      "Wayne Treible",
      "Daniel Crispell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Johnson_OpenSentinelMap_A_Large-Scale_Land_Use_Dataset_Using_OpenStreetMap_and_Sentinel-2_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Johnson_OpenSentinelMap_A_Large-Scale_Land_Use_Dataset_Using_OpenStreetMap_and_Sentinel-2_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Remote sensing data is plentiful, but downloading, organizing, and transforming large amounts of data into a format readily usable by modern machine learning methods is a challenging and labor-intensive task. We present the OpenSentinelMap dataset, which consists of 137,045 unique 3.7 km2 spatial cells, each containing multiple multispectral Sentinel-2 images captured over a 4 year time period and a set of corresponding per-pixel semantic labels derived from OpenStreetMap data. The labels are not necessarily mutually exclusive, and contain information about roads, buildings, water, and 12 land-use categories. The spatial cells are selected randomly on a global scale over areas of human activity, without regard to OpenStreetMap data availability or quality, making the dataset ideal for both supervised, semi-supervised, and unsupervised experimentation. To demonstrate the effectiveness of the dataset, we a) train an off-the-shelf convolutional neural network with minimal modification to predict land-use and building and road location from multispectral Sentinel-2 imagery and b) show that the learned embeddings are useful for downstream fine-grained classification tasks without any fine-tuning. The dataset is publicly available at https://visionsystemsinc.github.io/open-sentinel-map/.",
    "code_link": ""
  },
  "cvpr2022_earthvision_transformingtemporalembeddingstokeypointheatmapsfordetectionoftinyvehiclesinwideareamotionimagery(wami)sequences": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Transforming Temporal Embeddings to Keypoint Heatmaps for Detection of Tiny Vehicles in Wide Area Motion Imagery (WAMI) Sequences",
    "authors": [
      "Farhood Negin",
      "Mohsen Tabejamaat",
      "Renaud Fraisse",
      "Francois Bremond"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Negin_Transforming_Temporal_Embeddings_to_Keypoint_Heatmaps_for_Detection_of_Tiny_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Negin_Transforming_Temporal_Embeddings_to_Keypoint_Heatmaps_for_Detection_of_Tiny_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nowadays, due to its many applications, objects detection in wide area motion imagery (WAMI) sequences has received a lot of attention. Unlike natural images, object detection in WAMI faces unique challenges. Lack of appearance information due to the small size of objects makes object detection difficult for conventional methods. In addition, pixel noise, registration errors, sparse or densely populated objects, brings on pronounced artifacts which amplifies the difficulty of detection. This paper aims to address object detection problem in the presence of these issues by considering objects as keypoints in the relevant background and proposes a spatiotemporal anchor-free detector for tiny vehicles in WAMI images. Instead of background subtraction, a region of interest network refines large search space of sequences to indicates object clusters. For further investigation, clusters are encoded by a codebook which is learned through an unsupervised encoder-decoder network. To accurately generate the detections, a Transformer network is trained on cluster embeddings using ground-truth heatmaps that are described by Gaussian distribution rather than hard label annotation. The network is trained with a redesigned version of Focal loss comprising a shape prior regularizer which help the generated heatmaps to conform to the shape of the keypoints. Extensive experiments on WPAFB dataset demonstrate the high capability of our method for the detection of small vehicles where it achieves competitive performance when compared to the state-of-the-art.",
    "code_link": ""
  },
  "cvpr2022_earthvision_hephaestusalargescalemultitaskdatasettowardsinsarunderstanding": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Hephaestus: A Large Scale Multitask Dataset Towards InSAR Understanding",
    "authors": [
      "Nikolaos Ioannis Bountos",
      "Ioannis Papoutsis",
      "Dimitrios Michail",
      "Andreas Karavias",
      "Panagiotis Elias",
      "Isaak Parcharidis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Bountos_Hephaestus_A_Large_Scale_Multitask_Dataset_Towards_InSAR_Understanding_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Bountos_Hephaestus_A_Large_Scale_Multitask_Dataset_Towards_InSAR_Understanding_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Synthetic Aperture Radar (SAR) data and Interferometric SAR (InSAR) products in particular, are one of the largest sources of Earth Observation data. InSAR provides unique information on diverse geophysical processes and geology, and on the geotechnical properties of man-made structures. However, there are only a limited number of applications that exploit the abundance of InSAR data and deep learning methods to extract such knowledge. The main barrier has been the lack of a large curated and annotated InSAR dataset, which would be costly to create and would require an interdisciplinary team of experts experienced on InSAR data interpretation. In this work, we put the effort to create and make available the first of its kind, manually annotated dataset that consists of 19,919 individual Sentinel-1 interferograms acquired over 44 different volcanoes globally, which are split into 216,106 InSAR patches. The annotated dataset is designed to address different computer vision problems, including volcano state classification, semantic segmentation of ground deformation, detection and classification of atmospheric signals in InSAR imagery, interferogram captioning, text to InSAR generation, and InSAR image quality assessment.",
    "code_link": ""
  },
  "cvpr2022_earthvision_towardsassessingagriculturallandsuitabilitywithcausalmachinelearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Towards Assessing Agricultural Land Suitability With Causal Machine Learning",
    "authors": [
      "Georgios Giannarakis",
      "Vasileios Sitokonstantinou",
      "Roxanne Suzette Lorilla",
      "Charalampos Kontoes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Giannarakis_Towards_Assessing_Agricultural_Land_Suitability_With_Causal_Machine_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Giannarakis_Towards_Assessing_Agricultural_Land_Suitability_With_Causal_Machine_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Understanding the suitability of agricultural land for applying specific management practices is of great importance for sustainable and resilient agriculture against climate change. Recent developments in the field of causal machine learning enable the estimation of intervention impacts on an outcome of interest, for samples described by a set of observed characteristics. We introduce an extensible data-driven framework that leverages earth observations and frames agricultural land suitability as a geospatial impact assessment problem, where the estimated effects of agricultural practices on agroecosystems serve as a land suitability score and guide decision making. We formulate this as a causal machine learning task and discuss how this approach can be used for agricultural planning in a changing climate. Specifically, we extract the agricultural management practices of \"crop rotation\" and \"landscape crop diversity\" from crop type maps, account for climate and land use data, and use double machine learning to estimate their heterogeneous effect on Net Primary Productivity (NPP), within the Flanders region of Belgium from 2010 to 2020. We find that the effect of crop rotation was insignificant, while landscape crop diversity had a small negative effect on NPP. Finally, we observe considerable effect heterogeneity in space for both practices and analyze it.",
    "code_link": "https://github.com/microsoft/EconML"
  },
  "cvpr2022_earthvision_spacenet8-thedetectionoffloodedroadsandbuildings": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "SpaceNet 8 - The Detection of Flooded Roads and Buildings",
    "authors": [
      "Ronny H\u00e4nsch",
      "Jacob Arndt",
      "Dalton Lunga",
      "Matthew Gibb",
      "Tyler Pedelose",
      "Arnold Boedihardjo",
      "Desiree Petrie",
      "Todd M. Bacastow"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Hansch_SpaceNet_8_-_The_Detection_of_Flooded_Roads_and_Buildings_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Hansch_SpaceNet_8_-_The_Detection_of_Flooded_Roads_and_Buildings_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The frequency and intensity of natural disasters (i.e. wildfires, storms, floods) has increased over recent decades. Extreme weather can often be linked to climate change, and human population expansion and urbanization have led to a growing risk. In particular floods due to large amounts of rainfall are of rising severity and are causing loss of life, destruction of buildings and infrastructure, erosion of arable land, and environmental hazards around the world. Expanding urbanization along rivers and creeks often includes opening flood plains for building construction and river straightening and dredging speeding up the flow of water. In a flood event, rapid response is essential which requires knowledge which buildings are susceptible to flooding and which roads are still accessible. To this aim, SpaceNet 8 is the first remote sensing machine learning training dataset combining building footprint detection, road network extraction, and flood detection covering 850km 2, including 32k buildings and 1,300km roads of which 13% and 15% are flooded, respectively.",
    "code_link": ""
  },
  "cvpr2022_earthvision_unsupervisedchangedetectionbasedonimagereconstructionloss": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Unsupervised Change Detection Based on Image Reconstruction Loss",
    "authors": [
      "Hyeoncheol Noh",
      "Jingi Ju",
      "Minseok Seo",
      "Jongchan Park",
      "Dong-Geol Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Noh_Unsupervised_Change_Detection_Based_on_Image_Reconstruction_Loss_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Noh_Unsupervised_Change_Detection_Based_on_Image_Reconstruction_Loss_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "To train the change detector, bi-temporal images taken at different times in the same area are used. However, collecting labeled bi-temporal images is expensive and time consuming. To solve this problem, various unsupervised change detection methods have been proposed, but they still require unlabeled bi-temporal images. In this paper, we propose unsupervised change detection based on image reconstruction loss using only unlabeled single temporal single image. The image reconstruction model is trained to reconstruct the original source image by receiving the source image and the photometrically transformed source image as a pair. During inference, the model receives bi-temporal images as the input, and tries to reconstruct one of the inputs. The changed region between bi-temporal images shows high reconstruction loss. Our change detector showed significant performance in various change detection benchmark datasets even though only a single temporal single source image was used. The code and trained models will be publicly available for reproducibility.",
    "code_link": ""
  },
  "cvpr2022_earthvision_understandingtheroleofweatherdataforearthsurfaceforecastingusingaconvlstm-basedmodel": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Understanding the Role of Weather Data for Earth Surface Forecasting Using a ConvLSTM-Based Model",
    "authors": [
      "Codru\u021b-Andrei Diaconu",
      "Sudipan Saha",
      "Stephan G\u00fcnnemann",
      "Xiao Xiang Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Diaconu_Understanding_the_Role_of_Weather_Data_for_Earth_Surface_Forecasting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Diaconu_Understanding_the_Role_of_Weather_Data_for_Earth_Surface_Forecasting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Climate change is perhaps the biggest single threat to humankind and the environment, as it severely impacts our terrestrial surface, home to most of the living species. Inspired by video prediction and exploiting the availability of Copernicus Sentinel-2 images, recent studies have attempted to forecast the land surface evolution as a function of past land surface evolution, elevation, and weather. Further extending this paradigm, we propose a model based on convolutional long short-term memory (ConvLSTM) that is computationally efficient (lightweight), however obtains superior results to the previous baselines. By introducing a ConvLSTM-based architecture to this problem, we can not only ingest the heterogeneous data sources (Sentinel-2 time-series, weather data, and a Digital Elevation Model (DEM)) but also explicitly condition the future predictions on the weather. Our experiments confirm the importance of weather parameters in understanding the land cover dynamics and show that weather maps are significantly more important than the DEM in this task. Furthermore, we perform generative simulations to investigate how varying a single weather parameter can alter the evolution of the land surface. All studies are performed using the EarthNet2021 dataset. The code, additional materials and results can be found at https://github.com/dcodrut/weather2land.",
    "code_link": "https://github.com/dcodrut/weather2land"
  },
  "cvpr2022_lxcv_self-supervisedlearningforsonarimageclassification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - LatinX in Computer Vision Research",
    "title": "Self-Supervised Learning for Sonar Image Classification",
    "authors": [
      "Alan Preciado-Grijalva",
      "Bilal Wehbe",
      "Miguel Bande Firvida",
      "Matias Valdenegro-Toro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/html/Preciado-Grijalva_Self-Supervised_Learning_for_Sonar_Image_Classification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/papers/Preciado-Grijalva_Self-Supervised_Learning_for_Sonar_Image_Classification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Self-supervised learning has proved to be a powerful approach to learn image representations without the need of large labeled datasets. For underwater robotics, it is of great interest to design computer vision algorithms to improve perception capabilities such as sonar image classification. Due to the confidential nature of sonar imaging and the difficulty to interpret sonar images, it is challenging to create public large labeled sonar datasets to train supervised learning algorithms. In this work, we investigate the potential of three self-supervised learning methods (RotNet, Denoising Autoencoders, and Jigsaw) to learn high-quality sonar image representation without the need of human labels. We present pre-training and transfer learning results on real-life sonar image datasets. Our results indicate that self-supervised pre-training yields classification performance comparable to supervised pre-training in a few-shot transfer learning setup across all three methods. Code and self-supervised pre-trained models are available at https://github.com/agrija9/ssl-sonar-images.",
    "code_link": ""
  },
  "cvpr2022_lxcv_unpairedfacestocartoonsimprovingxgan": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - LatinX in Computer Vision Research",
    "title": "Unpaired Faces to Cartoons: Improving XGAN",
    "authors": [
      "Stev H. Ramos",
      "Joel Cabrera",
      "Daniel Ib\u00e1\u00f1ez",
      "Alejandro B. Jim\u00e9nez-Panta",
      "C\u00e9sar Beltr\u00e1n-Casta\u00f1o",
      "Edwin Villanueva"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/html/Ramos_Unpaired_Faces_to_Cartoons_Improving_XGAN_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/papers/Ramos_Unpaired_Faces_to_Cartoons_Improving_XGAN_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Domain Adaptation is a task that aims to translate an image from a source domain to a desired target domain. Current methods in domain adaptation use adversarial training based on Generative Adversarial Networks (GAN). In the present work, we focus on the task of domain adaptation from real faces to cartoon face images. We start from a baseline architecture called XGAN and introduce some improvements to it. Our proposed model is called W-XDGAN, which uses a form of GAN called Wasserstein-GAN, learns to approximate the Wasserstein Distance, and adds a denoiser to smooth the output cartoons. Whereas the original XGAN paper only presented a qualitative analysis, the advantages of this solution are demonstrated both quantitatively and qualitatively by comparing the results with models such as UNIT and original XGAN. Our code and models are publicly available at https://github.com/IAmigos/avatar-image-generator.",
    "code_link": "https://github.com/mingyuliutw/UNIT"
  },
  "cvpr2022_lxcv_generativeflowsasageneralpurposesolutionforinverseproblems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - LatinX in Computer Vision Research",
    "title": "Generative Flows as a General Purpose Solution for Inverse Problems",
    "authors": [
      "Jos\u00e9 A. Ch\u00e1vez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/html/Chavez_Generative_Flows_as_a_General_Purpose_Solution_for_Inverse_Problems_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/papers/Chavez_Generative_Flows_as_a_General_Purpose_Solution_for_Inverse_Problems_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Due to the success of generative flows to model data distributions, they have been explored in inverse problems. Given a pre-trained generative flow, previous work proposed to minimize the 2-norm of the latent variables as a regularization term. The intuition behind it was to ensure high likelihood latent variables that produce the closest restoration. However, high-likelihood latent variables may generate unrealistic samples as we show in our experiments. We therefore propose a solver to directly produce high-likelihood reconstructions. We hypothesize that our approach could make generative flows a general purpose solver for inverse problems. Furthermore, we propose 1 x 1 coupling functions to introduce permutations in a generative flow. It has the advantage that its inverse does not require to be calculated in the generation process. Finally, we evaluate our method for denoising, deblurring, inpainting, and colorization. We observe a compelling improvement of our method over prior works.",
    "code_link": ""
  },
  "cvpr2022_lxcv_guideddeepmetriclearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - LatinX in Computer Vision Research",
    "title": "Guided Deep Metric Learning",
    "authors": [
      "Jorge Gonzalez-Zapata",
      "Iv\u00e1n Reyes-Amezcua",
      "Daniel Flores-Araiza",
      "Mauricio Mendez-Ruiz",
      "Gilberto Ochoa-Ruiz",
      "Andres Mendez-Vazquez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/html/Gonzalez-Zapata_Guided_Deep_Metric_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/papers/Gonzalez-Zapata_Guided_Deep_Metric_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep Metric Learning (DML) methods have been proven relevant for visual similarity learning. However, they sometimes lack generalization properties because they are trained often using an inappropriate sample selection strategy or due to the difficulty of the dataset caused by a distributional shift in the data. These represent a significant drawback when attempting to learn the underlying data manifold. Therefore, there is a pressing need to develop better ways of obtaining generalization and representation of the underlying manifold. In this paper, we propose a novel approach to DML that we call Guided Deep Metric Learning, a novel architecture oriented to learning more compact clusters, improving generalization under distributional shifts in DML. This novel architecture consists of two independent models: A multi-branch master model, inspired from a Few-Shot Learning (FSL) perspective, generates a reduced hypothesis space based on prior knowledge from labeled data, which guides or regularizes the decision boundary of a student model during training under an offline knowledge distillation scheme. Experiments have shown that the proposed method is capable of a better manifold generalization and representation to up to 40% improvement (Recall@1, CIFAR10), using guidelines suggested by Musgrave et al. to perform a more fair and realistic comparison, which is currently absent in the literature.",
    "code_link": ""
  },
  "cvpr2022_lxcv_adeeperlookintoaleatoricandepistemicuncertaintydisentanglement": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - LatinX in Computer Vision Research",
    "title": "A Deeper Look Into Aleatoric and Epistemic Uncertainty Disentanglement",
    "authors": [
      "Matias Valdenegro-Toro",
      "Daniel Saromo Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/html/Valdenegro-Toro_A_Deeper_Look_Into_Aleatoric_and_Epistemic_Uncertainty_Disentanglement_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/LXCV/papers/Valdenegro-Toro_A_Deeper_Look_Into_Aleatoric_and_Epistemic_Uncertainty_Disentanglement_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural networks are ubiquitous in many tasks, but trusting their predictions is an open issue. Uncertainty quantification is required for many applications, and disentangled aleatoric and epistemic uncertainties are best. In this paper, we generalize methods to produce disentangled uncertainties to work with different uncertainty quantification methods, and evaluate their capability to produce disentangled uncertainties. Our results show that: there is an interaction between learning aleatoric and epistemic uncertainty, which is unexpected and violates assumptions on aleatoric uncertainty, some methods like Flipout produce zero epistemic uncertainty, aleatoric uncertainty is unreliable in the out-of-distribution setting, and Ensembles provide overall the best disentangling quality. We also explore the error produced by the number of samples hyper-parameter in the sampling softmax function, recommending N > 100 samples. We expect that our formulation and results help practitioners and researchers choose uncertainty methods and expand the use of disentangled uncertainties, as well as motivate additional research into this topic.",
    "code_link": ""
  },
  "cvpr2022_ego4d-epic_selfsupervisedscanpathpredictionframeworkforpaintingimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Ego4D-EPIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Joint Ego4D and Egocentric Perception, Interaction & Computing",
    "title": "Self Supervised Scanpath Prediction Framework for Painting Images",
    "authors": [
      "Marouane Tliba",
      "Mohamed Amine Kerkouri",
      "Aladine Chetouani",
      "Alessandro Bruno"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/html/Tliba_Self_Supervised_Scanpath_Prediction_Framework_for_Painting_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/papers/Tliba_Self_Supervised_Scanpath_Prediction_Framework_for_Painting_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In our paper, we propose a novel strategy to learn distortion invariant latent representation from painting pictures for visual attention modelling downstream task. In further detail, we design an unsupervised framework that jointly maximises the mutual information over different painting styles. To show the effectiveness of our approach, we firstly propose a lightweight scanpath baseline model and compare its performance to some state-of-the-art methods. Secondly, we train the encoder of our baseline model on large-scale painting images to study the efficiency of the proposed self-supervised strategy. The lightweight decoder proves effective in learning from the self-supervised pre-trained encoder with better performances than the end-to-end fine-tuned supervised baseline on two painting datasets, including a proposed new visual attention modelling dataset.",
    "code_link": ""
  },
  "cvpr2022_ego4d-epic_wheredidileavemykeys?-episodic-memory-basedquestionansweringonegocentricvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Ego4D-EPIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Joint Ego4D and Egocentric Perception, Interaction & Computing",
    "title": "Where Did I Leave My Keys? - Episodic-Memory-Based Question Answering on Egocentric Videos",
    "authors": [
      "Leonard B\u00e4rmann",
      "Alex Waibel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/html/Barmann_Where_Did_I_Leave_My_Keys_-_Episodic-Memory-Based_Question_Answering_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/papers/Barmann_Where_Did_I_Leave_My_Keys_-_Episodic-Memory-Based_Question_Answering_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Humans have a remarkable ability to organize, compress and retrieve episodic memories throughout their daily life. Current AI systems, however, lack comparable capabilities as they are mostly constrained to an analysis with access to the raw input sequence, assuming an unlimited amount of data storage which is not feasible in realistic deployment scenarios. For instance, existing Video Question Answering (VideoQA) models typically reason over the video while already being aware of the question, thus requiring to store the complete video in case the question is not known in advance. In this paper, we address this challenge with three main contributions: First, we propose the Episodic Memory Question Answering (EMQA) task as a specialization of VideoQA. Specifically, EMQA models are constrained to keep only a constant-sized representation of the video input, thus automatically limiting the computation requirements at query time. Second, we introduce a new egocentric VideoQA dataset called QaEgo4D, far larger than existing egocentric VideoQA datasets and featuring video length unprecedented in VideoQA datasets in general. Third, we present extensive experiments on the new dataset, comparing various baseline models in both the VideoQA and the EMQA setting. To facilitate future research on egocentric VideoQA as well as episodic memory representation and retrieval, we publish our code and dataset.",
    "code_link": ""
  },
  "cvpr2022_ego4d-epic_egocentricindoorlocalizationfromcoplanartwo-lineroomlayouts": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Ego4D-EPIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Joint Ego4D and Egocentric Perception, Interaction & Computing",
    "title": "Egocentric Indoor Localization From Coplanar Two-Line Room Layouts",
    "authors": [
      "Xiaowei Chen",
      "Guoliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/html/Chen_Egocentric_Indoor_Localization_From_Coplanar_Two-Line_Room_Layouts_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/papers/Chen_Egocentric_Indoor_Localization_From_Coplanar_Two-Line_Room_Layouts_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The coplanar two-line room layout with two parallel junction lines is often seen in an egocentric indoor vision when facing a wall or walking in a corridor. However, camera pose estimation from this kind of room layouts cannot be handled by existing vanishing point-based algorithms or PnL (Perspective-n-Line) methods due to the lack of line correspondences. This includes a recently proposed PnL-IOC approach that introduces image outer corners (IOCs), i.e., the intersecting points between room layout boundaries and image borders, to create more auxiliary lines. In this paper, a new coplanar P3L (CP3L) method is proposed to handle the coplanar two-line room layouts by embedding a P3L (Perspective-three-Line) method into the NSGA-II, a multi-objective optimization method. The proposed CP3L algorithm jointly estimates the initial camera pose and the 3D correspondence of four IOCs related to the two junction lines, and optimizes the camera pose in the iterative Gauss-Newton algorithm. We also study and compare the robustness of CP3L solutions under different configurations of auxiliary lines from estimated IOCs. Experiment results on both simulated images and real ones from the Matterport3D-Layout database demonstrate the accuracy and robustness of the proposed method.",
    "code_link": ""
  },
  "cvpr2022_ego4d-epic_weakly-supervisedactiondetectionguidedbyaudionarration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Ego4D-EPIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Joint Ego4D and Egocentric Perception, Interaction & Computing",
    "title": "Weakly-Supervised Action Detection Guided by Audio Narration",
    "authors": [
      "Keren Ye",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/html/Ye_Weakly-Supervised_Action_Detection_Guided_by_Audio_Narration_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Ego4D-EPIC/papers/Ye_Weakly-Supervised_Action_Detection_Guided_by_Audio_Narration_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Videos are more well-organized curated data sources for visual concept learning than images. Unlike the 2-dimensional images which only involve the spatial information, the additional temporal dimension bridges and synchronizes multiple modalities. However, in most video detection benchmarks, these additional modalities are not fully utilized. For example, EPIC Kitchens is the largest dataset in first-person (egocentric) vision, yet it still relies on crowdsourced information to refine the action boundaries to provide instance-level action annotations. We explored how to eliminate the expensive annotations in video detection data which provide refined boundaries. We propose a model to learn from the narration supervision and utilize multimodal features, including RGB, motion flow, and ambient sound. Our model learns to attend to the frames related to the narration label while suppressing the irrelevant frames from being used. Our experiments show that noisy audio narration suffices to learn a good action detection model, thus reducing annotation expenses.",
    "code_link": ""
  },
  "cvpr2022_biometrics_minnetminutiapatchembeddingnetworkforautomatedlatentfingerprintrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "MinNet: Minutia Patch Embedding Network for Automated Latent Fingerprint Recognition",
    "authors": [
      "Halil \u0130brahim \u00d6zt\u00fcrk",
      "Berkay Selbes",
      "Yusuf Artan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Ozturk_MinNet_Minutia_Patch_Embedding_Network_for_Automated_Latent_Fingerprint_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Ozturk_MinNet_Minutia_Patch_Embedding_Network_for_Automated_Latent_Fingerprint_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this study, we proposed a novel minutia patch embedding network (MinNet) model for latent fingerprint recognition problem. Embedding vectors generated for a fixed-size patch extracted around a minutia are used in the local similarity assignment algorithm to produce a global similarity match score. Unlike earlier minutia embedding models that aim to discriminate between latent image and sensor image minutia pair embeddings using L2 distance between the embedding vectors in the training process, MinNet model jointly optimizes the spatial and angular distribution of neighboring minutiae and ridge flows of the patches. Even though the proposed model is trained using weakly labeled training data, it produces state-of-the-art results thanks to it ability to generate discriminative embeddings. Proposed method has been evaluated on several public and private datasets and compared to popular latent fingerprint recognition methods presented in earlier studies. Our proposed method significantly outperforms existing methods on all three databases utilized in our study.",
    "code_link": "https://github.com/FingerGeneration/Generated"
  },
  "cvpr2022_biometrics_privacy-friendlysyntheticdataforthedevelopmentoffacemorphingattackdetectors": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "Privacy-Friendly Synthetic Data for the Development of Face Morphing Attack Detectors",
    "authors": [
      "Naser Damer",
      "C\u00e9sar Augusto Fontanillo L\u00f3pez",
      "Meiling Fang",
      "No\u00e9mie Spiller",
      "Minh Vu Pham",
      "Fadi Boutros"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Damer_Privacy-Friendly_Synthetic_Data_for_the_Development_of_Face_Morphing_Attack_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Damer_Privacy-Friendly_Synthetic_Data_for_the_Development_of_Face_Morphing_Attack_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The main question this work aims at answering is: \"can morphing attack detection (MAD) solutions be successfully developed based on synthetic data?\". Towards that, this work introduces the first synthetic-based MAD development dataset, namely the Synthetic Morphing Attack Detection Development dataset (SMDD). This dataset is utilized successfully to train three MAD backbones where it proved to lead to high MAD performance, even on completely unknown attack types. Additionally, an essential aspect of this work is the detailed legal analyses of the challenges of using and sharing real biometric data, rendering our proposed SMDD dataset extremely essential. The SMDD dataset, consisting of 30,000 attack and 50,000 bona fide samples, is publicly available for research purposes.",
    "code_link": ""
  },
  "cvpr2022_biometrics_ontheeffectofatmosphericturbulenceinthefeaturespaceofdeepfacerecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "On the Effect of Atmospheric Turbulence in the Feature Space of Deep Face Recognition",
    "authors": [
      "Wes Robbins",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Robbins_On_the_Effect_of_Atmospheric_Turbulence_in_the_Feature_Space_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Robbins_On_the_Effect_of_Atmospheric_Turbulence_in_the_Feature_Space_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "When captured over long distances, image quality is degraded by inconsistent refractive indexes in the atmosphere. This effect, known as Atmospheric Turbulence (AT), leads to lower performance for vision-based biometric systems such as face recognition. To account for AT, the literature has proposed methods to restore face-images from atmospheric turbulence, but has limited success. There is still a need to understand how atmospheric turbulence breaks recognition performance. We offer a first-look in this direction by providing a study on the effect of atmospheric turbulence in the feature space of deep-learning-based face recognition. We present results on recognition performance and feature space transformation caused by a wide range of AT levels. In deep feature space, we find interesting phenomena such as increasing feature magnitudes, which contradicts the expected result from the literature. From our results, we are able to identify an effect that makes face recognition under atmospheric turbulence uniquely difficult, which we call feature defection. In total, our findings suggest several areas of available improvement which can be used as a guideline for further progress in building models that are robust to AT.",
    "code_link": ""
  },
  "cvpr2022_biometrics_trueblack-boxexplanationinfacialanalysis": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "True Black-Box Explanation in Facial Analysis",
    "authors": [
      "Domingo Mery"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Mery_True_Black-Box_Explanation_in_Facial_Analysis_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Mery_True_Black-Box_Explanation_in_Facial_Analysis_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "When explaining a recognition approach that can be used in facial analysis, e.g, face verification, face detection, attribute recognition, etc., the task is to answer: how relevant are the parts of a given image to establish the recognition. In many cases, however, the trained models cannot be manipulated and must be treated as \"black-boxes\". In this paper, we present a saliency map methodology, called MinPlus, that can be used to explain any facial analysis approach with no manipulation inside of the recognition model, because it only needs the input-output function of the black-box fx. The key idea of the method is based on how the probability of recognition of the given image changes when it is perturbed. Our method removes and aggregates different parts of the image, and measures contributions of these parts individually and in-collaboration as well. We test and compare our method in four different scenarios: face verification (with ArcFace), face expression recognition (with Xception), face detection (with MTCNN) and masked face detection (with YOLOv5s). We conclude that MinPlus achieves saliency maps that are stable and interpretable to humans. In addition, our method shows promising results in comparison with other state-of-the-art methods like AVG, LIME and RISE.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "cvpr2022_biometrics_elasticfaceelasticmarginlossfordeepfacerecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "ElasticFace: Elastic Margin Loss for Deep Face Recognition",
    "authors": [
      "Fadi Boutros",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Boutros_ElasticFace_Elastic_Margin_Loss_for_Deep_Face_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Boutros_ElasticFace_Elastic_Margin_Loss_for_Deep_Face_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning discriminative face features plays a major role in building high-performing face recognition models. The recent state-of-the-art face recognition solutions proposed to incorporate a fixed penalty margin on commonly used classification loss function, softmax loss, in the normalized hypersphere to increase the discriminative power of face recognition models, by minimizing the intra-class variation and maximizing the inter-class variation. Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the geodesic distance between and within the different identities can be equally learned using a fixed penalty margin. However, such a learning objective is not realistic for real data with inconsistent inter-and intra-class variation, which might limit the discriminative and generalizability of the face recognition model. In this paper, we relax the fixed penalty margin constrain by proposing elastic penalty margin loss (ElasticFace) that allows flexibility in the push for class separability. The main idea is to utilize random margin values drawn from a normal distribution in each training iteration. This aims at giving the decision boundary chances to extract and retract to allow space for flexible class separability learning. We demonstrate the superiority of our ElasticFace loss over ArcFace and CosFace losses, using the same geometric transformation, on a large set of mainstream benchmarks. From a wider perspective, our ElasticFace has advanced the state-of-the-art face recognition performance on seven out of nine mainstream benchmarks.",
    "code_link": ""
  },
  "cvpr2022_biometrics_residualfeaturepyramidnetworkforenhancementofvascularpatterns": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "Residual Feature Pyramid Network for Enhancement of Vascular Patterns",
    "authors": [
      "Ketan Kotwal",
      "S\u00e9bastien Marcel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Kotwal_Residual_Feature_Pyramid_Network_for_Enhancement_of_Vascular_Patterns_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Kotwal_Residual_Feature_Pyramid_Network_for_Enhancement_of_Vascular_Patterns_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The accuracy of finger vein recognition systems gets degraded due to low and uneven contrast between veins and surroundings, often resulting in poor detection of vein patterns. We propose a finger-vein enhancement technique, ResFPN (Residual Feature Pyramid Network), as a generic preprocessing method agnostic to the recognition pipeline. A bottom-up pyramidal architecture using the novel Structure Detection block (SDBlock) facilitates extraction of veins of varied widths. Using a feature aggregation module (FAM), we combine these vein-structures, and train the proposed ResFPN for detection of veins across scales. With enhanced presentations, our experiments indicate a reduction upto 5% in the average recognition errors for commonly used recognition pipeline over two publicly available datasets. These improvements are persistent even in cross-dataset scenario where the dataset used to train the ResFPN is different from the one used for recognition.",
    "code_link": ""
  },
  "cvpr2022_biometrics_towardsadeeperunderstandingofskeleton-basedgaitrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Biometrics",
    "title": "Towards a Deeper Understanding of Skeleton-Based Gait Recognition",
    "authors": [
      "Torben Teepe",
      "Johannes Gilg",
      "Fabian Herzog",
      "Stefan H\u00f6rmann",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/html/Teepe_Towards_a_Deeper_Understanding_of_Skeleton-Based_Gait_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Teepe_Towards_a_Deeper_Understanding_of_Skeleton-Based_Gait_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Gait recognition is a promising biometric with unique properties for identifying individuals from a long distance by their walking patterns. In recent years, most gait recognition methods used the person's silhouette to extract the gait features. However, silhouette images can lose fine-grained spatial information, suffer from (self) occlusion, and be challenging to obtain in real-world scenarios. Furthermore, these silhouettes also contain other visual clues that are not actual gait features and can be used for identification, but also to fool the system. Model-based methods do not suffer from these problems and are able to represent the temporal motion of body joints, which are real gait features. The advances in human pose estimation started a new era for model-based gait recognition with skeleton-based gait recognition. In this work, we propose an approach based on Graph Convolutional Networks (GCNs) that combines higher-order inputs, and residual networks to an efficient architecture for gait recognition. Extensive experiments on the two popular gait datasets, CASIA-B and OUMVLP-Pose, show a massive improvement (3x) of the state-of-the-art on the largest gait dataset OUMVLP-Pose and strong temporal modeling capabilities. Finally, we visualize our method to understand skeleton-based gait recognition better.",
    "code_link": "https://github.com/tteepe/GaitGraph2"
  },
  "cvpr2022_agrivision_transferlearningfromsyntheticin-vitrosoybeanpodsdatasetforin-situsegmentationofon-branchsoybeanpods": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Transfer Learning From Synthetic In-Vitro Soybean Pods Dataset for In-Situ Segmentation of On-Branch Soybean Pods",
    "authors": [
      "Si Yang",
      "Lihua Zheng",
      "Xieyuanli Chen",
      "Laura Zabawa",
      "Man Zhang",
      "Minjuan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Yang_Transfer_Learning_From_Synthetic_In-Vitro_Soybean_Pods_Dataset_for_In-Situ_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Yang_Transfer_Learning_From_Synthetic_In-Vitro_Soybean_Pods_Dataset_for_In-Situ_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The mature soybean plants are of complex architecture with pods frequently touching each other, posing a challenge for in-situ segmentation of on-branch soybean pods. Deep learning-based methods can achieve accurate training and strong generalization capabilities, but it demands massive labeled data, which is often a limitation, especially for agricultural applications. As lacking the labeled data to train an in-situ segmentation model for on-branch soybean pods, we propose a transfer learning from synthetic in-vitro soybean pods. First, we present a novel automated image generation method to rapidly generate a synthetic in-vitro soybean pods dataset with plenty of annotated samples. The in-vitro soybean pods samples are overlapped to simulate the frequently physically touching of on-branch soybean pods. Then, we design a two-step transfer learning. In the first step, we finetune an instance segmentation network pretrained by a source domain (MS COCO dataset) with a synthetic target domain (in-vitro soybean pods dataset). In the second step, transferring from simulation to reality is performed by finetuning on a few real-world mature soybean plant samples. The experimental results show the effectiveness of the proposed two-step transfer learning method, such that AP50 was 0.80 for the real-world mature soybean plant test dataset, which is higher than that of direct adaptation and its AP50 was 0.77. Furthermore, the visualizations of in-situ segmentation results of on-branch soybean pods show that our method performs better than other methods, especially when soybean pods overlap densely.",
    "code_link": ""
  },
  "cvpr2022_agrivision_usingpurepollenspecieswhentrainingacnntosegmentpollenmixtures": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Using Pure Pollen Species When Training a CNN To Segment Pollen Mixtures",
    "authors": [
      "Nana Yang",
      "Victor Joos",
      "Anne-Laure Jacquemart",
      "Christel Buyens",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Yang_Using_Pure_Pollen_Species_When_Training_a_CNN_To_Segment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Yang_Using_Pure_Pollen_Species_When_Training_a_CNN_To_Segment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recognizing the types of pollen grains and estimating their proportion in pollen mixture samples collected in a specific geographical area is important for agricultural, medical, and ecosystem research. Our paper adopts a convolutional neural network for the automatic segmentation of pollen species in microscopy images, and proposes an original strategy to train such network at reasonable manual annotation cost. Our approach is founded on a large dataset composed of pure pollen images. It first (semi-)manually segments foreground, i.e. pollen grains, and background in a fraction of those images, and use the resulting annotated dataset to train a universal pollen segmentation CNN. In the second step, this model is used to automatically segment a large number of additional pure pollen images, so as to supervise the training of a pollen species segmentation model. Despite the fact that it has been trained from pure images only, the model is shown to provide accurate segmentation of species in pollen mixtures. Our experiments also demonstrate that dedicating a model to the segmentation of a subset of the available pure pollen species makes it possible to train a bin pollen class, corresponding to pollen species that are not in the subset of species recognized by the model. This strategy is useful to cope with unexpected species in a mixture.",
    "code_link": ""
  },
  "cvpr2022_agrivision_3dpointcloudinstancesegmentationoflettucebasedonpartnet": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "3D Point Cloud Instance Segmentation of Lettuce Based on PartNet",
    "authors": [
      "Luhan Wang",
      "Lihua Zheng",
      "Minjuan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Wang_3D_Point_Cloud_Instance_Segmentation_of_Lettuce_Based_on_PartNet_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Wang_3D_Point_Cloud_Instance_Segmentation_of_Lettuce_Based_on_PartNet_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Organ level instance segmentation (e.g., individual leaves) based on computer vision techniques is a key step in the measurement of plant phenotypes. Since plant organs, especially leaves, are self-occluded and emerged-occluded, single-view images affect the acquisition of some effective information. However, 3D global images contain much more plant morphological information than single-view images, and it is of great significance for plant phenotype research. In this paper, lettuce was taken as the research object, its 3D point cloud images were obtained and instance segmentation was carried out based on the deep learning method. The result showed that the 3D point cloud of each leaf was segmented and identified accurately. Specifically, we constructed a lettuce point cloud dataset consisting of 620 real and virtual point clouds and fused them together to train a 3D instance segmentation network--PartNet, which directly takes 3D point clouds as input and its output is the instance segmentation results of leaves. The experimental results showed that, when tested with 40 point clouds in the validation set, the instance segmentation accuracy AP (%) with IoU < 0.25 reaches 97.2%, and the instance segmentation accuracy AP with IoU < 0.5 reaches 92.4%, indicating that the constructed PartNet network has the potential to accurately segment the 3D point cloud leaf instances for lettuce.",
    "code_link": ""
  },
  "cvpr2022_agrivision_pseudo-labelgenerationforagriculturalroboticsapplications": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Pseudo-Label Generation for Agricultural Robotics Applications",
    "authors": [
      "Thomas A. Ciarfuglia",
      "Ionut Marian Motoi",
      "Leonardo Saraceni",
      "Daniele Nardi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Ciarfuglia_Pseudo-Label_Generation_for_Agricultural_Robotics_Applications_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Ciarfuglia_Pseudo-Label_Generation_for_Agricultural_Robotics_Applications_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In the context of table grape cultivation there is rising interest in robotic solutions for harvesting, pruning, precision spraying and other agronomic tasks. Perception algorithms at the core of these systems require large amounts of labelled data, which in this context is often not available. In this work, we propose a semi-supervised solution to reduce the data needed to get state-of-the-art detection and segmentation of fruits in orchards. We present the case of table grape vineyards in southern Lazio (Italy) since grapes are a difficult fruit to segment due to occlusion, color and general illumination conditions. We consider the concrete scenario where the source labelled data is wine grape, while the target data is table grape, with considerable covariate shift. Starting from a simple video input, our method generates first bounding box labels, leveraging the structure from motion information, then segmentation masks, using the same weakly generated bounding box labels and a refining step based on Grab Cut. This system is able to produce labels that considerably reduce the covariate shift from source to target data and that requires very limited data acquisition effort. Comparisons with SotA supervised solutions show how our methods are able to train new models that achieve high performances with few labelled images, with very simple labelling.",
    "code_link": ""
  },
  "cvpr2022_agrivision_optimizingnitrogenmanagementwithdeepreinforcementlearningandcropsimulations": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Optimizing Nitrogen Management With Deep Reinforcement Learning and Crop Simulations",
    "authors": [
      "Jing Wu",
      "Ran Tao",
      "Pan Zhao",
      "Nicolas F. Martin",
      "Naira Hovakimyan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Wu_Optimizing_Nitrogen_Management_With_Deep_Reinforcement_Learning_and_Crop_Simulations_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Wu_Optimizing_Nitrogen_Management_With_Deep_Reinforcement_Learning_and_Crop_Simulations_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nitrogen (N) management is critical to sustain soil fertility and crop production while minimizing the negative environmental impact, but is challenging to optimize. This paper proposes an intelligent N management system using deep reinforcement learning (RL) and crop simulations with Decision Support System for Agrotechnology Transfer (DSSAT). We first formulate the N management problem as an RL problem. We then train management policies with deep Q-network and soft actor-critic algorithms, and the soil and Gym-DSSAT interface that allows for daily interactions between the simulated crop environment and RL agents. According to the experiments on the maize crop in both Iowa and Florida in the US, our RL-trained policies outperform previous empirical methods by achieving higher or similar yield while using less fertilizers.",
    "code_link": ""
  },
  "cvpr2022_agrivision_aaformeramulti-modaltransformernetworkforaerialagriculturalimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "AAFormer: A Multi-Modal Transformer Network for Aerial Agricultural Images",
    "authors": [
      "Yao Shen",
      "Lei Wang",
      "Yue Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Shen_AAFormer_A_Multi-Modal_Transformer_Network_for_Aerial_Agricultural_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Shen_AAFormer_A_Multi-Modal_Transformer_Network_for_Aerial_Agricultural_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The semantic segmentation of agricultural aerial images is very important for the recognition and analysis of farmland anomaly patterns, such as drydown, endrow, nutrient deficiency, etc. General semantic segmentation algorithms such as Fully Convolutional Networks can extract rich semantic feature information, but it is difficult to exploit the long-range vision information. Recently, vision Transformer architectures have made outstanding performances in image segmentation tasks, but it has not been fully explored in the field of agriculture. Therefore, we propose a novel architecture called Agricultural Aerial Transformer (AAFormer) to solve the semantic segmentation of aerial farmland images. We adopt Mix Transformer (MiT) in the encoder stage to enhance the ability of field anomaly pattern recognition and leverage the Squeeze-and-Excitation (SE) module in the decoder stage to improve the effectiveness of key channels. The boundary maps of farmland are introduced into the decoder. Evaluated on the Agriculture-Vision validation set, the mIoU of our proposed model reaches 45.44%.",
    "code_link": "https://github.com/open-mmlab/mmsegmentation"
  },
  "cvpr2022_agrivision_augmentationinvarianceandadaptivesamplinginsemanticsegmentationofagriculturalaerialimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Augmentation Invariance and Adaptive Sampling in Semantic Segmentation of Agricultural Aerial Images",
    "authors": [
      "Antonio Tavera",
      "Edoardo Arnaudo",
      "Carlo Masone",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Tavera_Augmentation_Invariance_and_Adaptive_Sampling_in_Semantic_Segmentation_of_Agricultural_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Tavera_Augmentation_Invariance_and_Adaptive_Sampling_in_Semantic_Segmentation_of_Agricultural_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we investigate the problem of Semantic Segmentation for agricultural aerial imagery. We observe that the existing methods used for this task are designed without considering two characteristics of the aerial data: (i) the top-down perspective implies that the model cannot rely on a fixed semantic structure of the scene, because the same scene may be experienced with different rotations of the sensor; (ii) there can be a strong imbalance in the distribution of semantic classes because the relevant objects of the scene may appear at extremely different scales (e.g., a field of crops and a small vehicle). We propose a solution to these problems based on two ideas: (i) we use together a set of suitable augmentation and a consistency loss to guide the model to learn semantic representations that are invariant to the photometric and geometric shifts typical of the top-down perspective (Augmentation Invariance); (ii) we use a sampling method (Adaptive Sampling) that selects the training images based on a measure of pixel-wise distribution of classes and actual network confidence. With an extensive set of experiments conducted on the Agriculture-Vision dataset, we demonstrate that our proposed strategies improve the performance of the current state-of-the-art method.",
    "code_link": "https://github.com/taveraantonio/AIAS"
  },
  "cvpr2022_agrivision_high-resolutionuavimagegenerationforsorghumpanicledetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "High-Resolution UAV Image Generation for Sorghum Panicle Detection",
    "authors": [
      "Enyu Cai",
      "Zhankun Luo",
      "Sriram Baireddy",
      "Jiaqi Guo",
      "Changye Yang",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Cai_High-Resolution_UAV_Image_Generation_for_Sorghum_Panicle_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Cai_High-Resolution_UAV_Image_Generation_for_Sorghum_Panicle_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The number of panicles (or heads) of Sorghum plants is an important phenotypic trait for plant development and grain yield estimation. The use of Unmanned Aerial Vehicles (UAVs) enables the capability of collecting and analyzing Sorghum images on a large scale. Deep learning can provide methods for estimating phenotypic traits from UAV images but requires a large amount of labeled data. The lack of training data due to the labor-intensive ground truthing of UAV images causes a major bottleneck in developing methods for Sorghum panicle detection and counting. In this paper, we present an approach that uses synthetic training images from generative adversarial networks (GANs) for data augmentation to enhance the performance of Sorghum panicle detection and counting. Our method can generate synthetic high-resolution UAV RGB images with panicle labels by using image-to-image translation GANs with a limited ground truth dataset of real UAV RGB images. The results show the improvements in panicle detection and counting using our data augmentation approach.",
    "code_link": ""
  },
  "cvpr2022_agrivision_unsuperviseddomainadaptationandsuperresolutionondroneimagesforautonomousdryherbagebiomassestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Unsupervised Domain Adaptation and Super Resolution on Drone Images for Autonomous Dry Herbage Biomass Estimation",
    "authors": [
      "Paul Albert",
      "Mohamed Saadeldin",
      "Badri Narayanan",
      "Brian Mac Namee",
      "Deirdre Hennessy",
      "Noel E. O'Connor",
      "Kevin McGuinness"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Albert_Unsupervised_Domain_Adaptation_and_Super_Resolution_on_Drone_Images_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Albert_Unsupervised_Domain_Adaptation_and_Super_Resolution_on_Drone_Images_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Herbage mass yield and composition estimation is an important tool for farmers to ensure an adequate supply of high quality herbage for grazing and subsequently milk production. By accurately estimating herbage mass and composition, targeted nitrogen fertilizer application strategies can be deployed to improve localized regions in a herbage field, effectively reducing the negative impacts of over-fertilization on biodiversity and the environment. In this context, deep learning algorithms offer a tempting alternative to the usual means of sward composition estimation, which involve the destructive process of cutting a sample from the herbage field and sorting by hand all plant species in the herbage. Deep learning has been successfully applied in this context on images collected by high-resolution cameras on the ground. Moving the deep learning solution to drone imaging, however, has the potential to further improve the herbage mass yield and composition estimation task by extending the ground-level estimation to the large surfaces occupied by fields/paddocks. Drone images come at the cost of lower resolution views of the fields taken from a high altitude and requires further herbage ground-truth collection from the large surfaces covered by drone images. This paper proposes to transfer knowledge learned on ground-level images to raw drone images in an unsupervised manner. To do so, we use unpaired image style translation to enhance the resolution of drone images by a factor of eight and modify them to appear closer to their ground-level counterparts. We then use the enhanced drone images to train a semi-supervised algorithm that uses ground-truthed, ground-level images as the labelled data together with a large amount of unlabeled drone images. We validate our results on a small held-out drone image test set to show the validity of our approach, which opens the way for automated dry herbage biomass monitoring. Our code is available on github.",
    "code_link": ""
  },
  "cvpr2022_clic_imagequalityassessmentwithtransformersandmulti-metricfusionmodules": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Image Quality Assessment With Transformers and Multi-Metric Fusion Modules",
    "authors": [
      "Wei Jiang",
      "Litian Li",
      "Yi Ma",
      "Yongqi Zhai",
      "Zheng Yang",
      "Ronggang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Jiang_Image_Quality_Assessment_With_Transformers_and_Multi-Metric_Fusion_Modules_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Jiang_Image_Quality_Assessment_With_Transformers_and_Multi-Metric_Fusion_Modules_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image quality assessment is crucial for low-level vision tasks such as compression, super-resolution, denoising and etc. It guides researchers how to design networks, design loss functions, and decide the optimization direction of networks. A good quality assessment metric should comform to people's subjective feelings as much as possible. Traditional PSNR and MS-SSIM have more and more obvious shortcomings in quality evaluation With the popularity of GANs. Inspired by metrics such as LPIPS, IQT, etc., we decided to design a metric that is learned by the network itself. In this paper, we use a ConvNeXt-Tiny network to extract features and calculate nonlinear residuals between reference images and distorted images. We feed residuals into a transformer to compare the degree of distortion. In addition, we use multi-metric fusion to improve the performance of our network. Our model achieves 0.780 accuracy on CLIC validation set.",
    "code_link": "https://github.com/JiangWeibeta/IQA-TMFM"
  },
  "cvpr2022_clic_aneural-networkenhancedvideocodingframeworkbeyondvvc": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "A Neural-Network Enhanced Video Coding Framework Beyond VVC",
    "authors": [
      "Junru Li",
      "Yue Li",
      "Chaoyi Lin",
      "Kai Zhang",
      "Li Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Li_A_Neural-Network_Enhanced_Video_Coding_Framework_Beyond_VVC_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Li_A_Neural-Network_Enhanced_Video_Coding_Framework_Beyond_VVC_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents a hybrid video compression framework, aiming at providing a demonstration of applying deep learning-based approaches beyond conventional coding framework. The proposed hybrid framework is established over the Enhanced Compression Model (ECM) of which the core is the Versatile Video Coding (VVC) standard. We propose to integrate a series of enhanced coding tools, such as block partitioning, intra prediction, and inter prediction to further remove the spatial and temporal redundancy. Moreover, deep learning-based technologies including loop filter and super resolution are involved to restore the compression distortion. Compared with the VVC software VTM-11.0, experimental results demonstrate the effectiveness of the proposed learning-based framework, leading to 25.81%, 35.08%, and 37.54% bit-rate savings for Y, Cb and Cr components, respectively under random access configuration. In addition, the proposed framework achieves 39.313 and 32.050 PSNRs in the test set under 1 Mbps and 0.1 Mbps video compression tracks of CLIC-2022. 33.522, 30.758, and 28.300 in terms of PSNR are obtained in 0.3 bpp, 0.15 bpp, and 0.075 bpp image compression tracks.",
    "code_link": ""
  },
  "cvpr2022_clic_focusedfeaturedifferentiationnetworkforimagequalityassessment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Focused Feature Differentiation Network for Image Quality Assessment",
    "authors": [
      "Gang He",
      "Yong Wang",
      "Li Xu",
      "Wenli Zhang",
      "Ming Sun",
      "Xing Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/He_Focused_Feature_Differentiation_Network_for_Image_Quality_Assessment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/He_Focused_Feature_Differentiation_Network_for_Image_Quality_Assessment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image quality assessment (IQA) intended to assess the perceptual quality of images has been an essential problem in both human and machine vision. Recently, with the help of deep neural network (DNN), IQA algorithms can extract more valuable differences between the distorted and reference images than the traditional algorithms, and thus the performance of DNN-based algorithms is more satisfactory than that of previous algorithms. However, the accuracy for different distorted images preference rating of the existing DNN-based quality assessment methods will be decreased when multiple distorted images are quite similar to each other or to the reference image. To tackle this problem, we propose a focused feature differentiation network (FFDN) to highlight the feature maps with greater distorted and reference differentiation. Furthermore, we use the multi-scale feature fusion module to fuse the focused differentiation features at different scale receptive fields. To further improve the accuracy of our method, we predict the mean opinion score and differentiation score by stages and combine them with different self-learning weights. Finally, we convert the weighted score into different image preference degrees. Experimental results on the validation dataset of CLIC2022 and test dataset of CLIC2021 show that the accuracy of our model FFDN is higher than other excellent quality assessment methods.",
    "code_link": ""
  },
  "cvpr2022_clic_learnedcompressionofhighdimensionalimagedatasets": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Learned Compression of High Dimensional Image Datasets",
    "authors": [
      "Elizabeth Cole",
      "Qingxi Meng",
      "John Pauly",
      "Shreyas Vasanawala"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Cole_Learned_Compression_of_High_Dimensional_Image_Datasets_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Cole_Learned_Compression_of_High_Dimensional_Image_Datasets_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In many applications, such as burst photography and magnetic resonance imaging (MRI), multiple images are acquired to reduce the noise of the eventual reconstructed image. However, this leads to very high dimensional datasets which have redundant information across the various acquired images. In MRI, multiple images are acquired via multiple RF coil arrays in the scanner. Afterwards, coil compression is performed to convert the original set of coil images into a smaller set of virtual coil images to enable smaller datasets and faster computation time. However, traditional iterative coil compression methods are lossy and time-consuming. In this work, we propose a novel neural network-based coil compression method in pursuit of higher reconstruction accuracy and faster coil compression. Our learned compression method achieves up to 1.5x lower NRMSE and up to 10 times runtime speed compared to traditional methods on a benchmark test dataset.",
    "code_link": ""
  },
  "cvpr2022_clic_slimmablevideocodec": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Slimmable Video Codec",
    "authors": [
      "Zhaocheng Liu",
      "Luis Herranz",
      "Fei Yang",
      "Saiping Zhang",
      "Shuai Wan",
      "Marta Mrak",
      "Marc G\u00f3rriz Blanch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Liu_Slimmable_Video_Codec_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Liu_Slimmable_Video_Codec_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural video compression has emerged as a novel paradigm combining trainable multilayer neural networks and machine learning, achieving competitive rate-distortion (RD) performances, but still remaining impractical due to heavy neural architectures, with large memory and computational demands. In addition, models are usually optimized for a single RD tradeoff. Recent slimmable image codecs can dynamically adjust their model capacity to gracefully reduce the memory and computation requirements, without harming RD performance. In this paper we propose a slimmable video codec (SlimVC), by integrating a slimmable temporal entropy model in a slimmable autoencoder. Despite a significantly more complex architecture, we show that slimming remains a powerful mechanism to control rate, memory footprint, computational cost and latency, all being important requirements for practical video compression.",
    "code_link": ""
  },
  "cvpr2022_clic_self-supervisedvariablerateimagecompressionusingvisualattention": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Self-Supervised Variable Rate Image Compression Using Visual Attention",
    "authors": [
      "Abhishek Kumar Sinha",
      "S. Manthira Moorthi",
      "Debajyoti Dhar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Sinha_Self-Supervised_Variable_Rate_Image_Compression_Using_Visual_Attention_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Sinha_Self-Supervised_Variable_Rate_Image_Compression_Using_Visual_Attention_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The recent success of self-supervised learning relies on its ability to learn the representations from self-defined pseudo-labels that are applied to several downstream tasks. Motivated by this ability, we present a deep image compression technique, which learns the lossy reconstruction of raw images from the self-supervised learned representation of SimCLR ResNet-50 architecture. Our framework uses a feature pyramid to achieve the variable rate compression of the image using a self-attention map for the optimal allocation of bits. The paper provides an overview to observe the effects of contrastive self-supervised representations and the self-attention map on the distortion and perceptual quality of the reconstructed image. The experiments are performed on a different class of images to show that the proposed method outperforms the other variable rate deep compression models without compromising the perceptual quality of the images.",
    "code_link": ""
  },
  "cvpr2022_clic_neuralfacevideocompressionusingmultipleviews": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Neural Face Video Compression Using Multiple Views",
    "authors": [
      "Anna Volokitin",
      "Stefan Brugger",
      "Ali Benlalah",
      "Sebastian Martin",
      "Brian Amberg",
      "Michael Tschannen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Volokitin_Neural_Face_Video_Compression_Using_Multiple_Views_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Volokitin_Neural_Face_Video_Compression_Using_Multiple_Views_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent advances in deep generative models led to the development of neural face video compression codecs that use an order of magnitude less bandwidth than engineered codecs. These neural codecs reconstruct the current frame by warping a source frame and using a generative model to compensate for imperfections in the warped source frame. Thereby, the warp is encoded and transmitted using a small number of keypoints rather than a dense flow field, which leads to massive savings compared to traditional codecs. However, by relying on a single source frame only, these methods lead to inaccurate reconstructions (e.g. one side of the head becomes unoccluded when turning the head and has to be synthesized). Here, we aim to tackle this issue by relying on multiple source frames (views of the face) and present encouraging results.",
    "code_link": ""
  },
  "cvpr2022_clic_adaptivebitratequantizationschemewithoutcodebookforlearnedimagecompression": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Adaptive Bitrate Quantization Scheme Without Codebook for Learned Image Compression",
    "authors": [
      "Jonas L\u00f6hdefink",
      "Jonas Sitzmann",
      "Andreas B\u00e4r",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Lohdefink_Adaptive_Bitrate_Quantization_Scheme_Without_Codebook_for_Learned_Image_Compression_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Lohdefink_Adaptive_Bitrate_Quantization_Scheme_Without_Codebook_for_Learned_Image_Compression_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a generic approach to quantization without codebook in learned image compression called one-hot max (OHM) quantization. It reorganizes the feature space resulting in an additional dimension, along which vector quantization yields one-hot vectors by comparing activations. Furthermore, we show how to integrate OHM quantization into a compression system with bitrate adaptation, i.e., full control over bitrate during inference. We perform experiments on both MNIST and Kodak and report on rate-distortion trade-offs comparing with the integer rounding reference. For low bitrates (< 0.4 bpp), our proposed quantizer yields better performance while exhibiting also other advantageous training and inference properties. Code is available at https://github.com/ifnspaml/OHMQ.",
    "code_link": "https://github.com/ifnspaml/OHMQ"
  },
  "cvpr2022_clic_hybridvideocodingschemebasedonvvcandspatio-temporalattentionconvolutionneuralnetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Hybrid Video Coding Scheme Based on VVC and Spatio-Temporal Attention Convolution Neural Network",
    "authors": [
      "Gang He",
      "Kepeng Xu",
      "Chang Wu",
      "Zijia Ma",
      "Xing Wen",
      "Ming Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/He_Hybrid_Video_Coding_Scheme_Based_on_VVC_and_Spatio-Temporal_Attention_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/He_Hybrid_Video_Coding_Scheme_Based_on_VVC_and_Spatio-Temporal_Attention_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we propose a hybrid video coding framework. The framework is built on the basis of VVC (Versatile Video Coding) video coding standard and constructs an implicitly aligned multi-frame fusion model to accomplish subjective video quality enhancement. The proposed framework mainly optimizes video compression efficiency from two perspectives. First is the sequence-level dynamic rate control algorithm, which assigns the appropriate bitrate to each video to obtain the highest overall video quality. Second is the MAQE, a multi frame implicit alignment video quality enhancement model, which performs motion alignment through multiple convolutional kernels of different sizes, uses a residual aggregation layer to fuse features of different frames, and then uses an enhanced attention module to adaptively deflate features based on spatio-temporal contextual features, so as to more effectively fuse feature of multiple frames and obtain higher quality reconstructed frames. The proposed method is validated on two tracks of 0.1M code rate and 1M code rate on CLIC-2022 video compression task, Experimental results show that the proposed method achieves PSNR of 30.301 and 37.251 and obtains MS-SSIM of 0.9368 and 0.9875. This paper is a comprehensive presentation of the scheme used by the Night-Watch team of the CLIC-2022 video track.",
    "code_link": ""
  },
  "cvpr2022_clic_non-linearmotionestimationforvideoframeinterpolationusingspace-timeconvolutions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Non-Linear Motion Estimation for Video Frame Interpolation Using Space-Time Convolutions",
    "authors": [
      "Saikat Dutta",
      "Arulkumar Subramaniam",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Dutta_Non-Linear_Motion_Estimation_for_Video_Frame_Interpolation_Using_Space-Time_Convolutions_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Dutta_Non-Linear_Motion_Estimation_for_Video_Frame_Interpolation_Using_Space-Time_Convolutions_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Video frame interpolation aims to synthesize one or multiple frames between two consecutive frames in a video. It has a wide range of applications including slow-motion video generation, video compression and developing video codecs. Some older works tackled this problem by assuming per-pixel linear motion between video frames. However, objects often follow a non-linear motion pattern in the real domain and some recent methods attempt to model per-pixel motion by non-linear models (e.g., quadratic). A quadratic model can also be inaccurate, especially in the case of motion discontinuities over time (i.e. sudden jerks) and occlusions, where some of the flow information may be invalid or inaccurate. In our paper, we propose to approximate the per-pixel motion using a space-time convolution network that is able to adaptively select the motion model to be used. Specifically, we are able to softly switch between a linear and a quadratic model. Towards this end, we use an end-to-end 3D CNN encoder-decoder architecture over bidirectional optical flows and occlusion maps to estimate the non-linear motion model of each pixel. Further, a motion refinement module is employed to refine the non-linear motion and the interpolated frames are estimated by a simple warping of the neighboring frames with the estimated per-pixel motion. We show that our method outperforms state-of-the-art algorithms on four datasets.",
    "code_link": ""
  },
  "cvpr2022_clic_super-resolutionbasedvideocodingscheme": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Super-Resolution Based Video Coding Scheme",
    "authors": [
      "Hyun min Cho",
      "Kiho Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Cho_Super-Resolution_Based_Video_Coding_Scheme_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Cho_Super-Resolution_Based_Video_Coding_Scheme_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we present a super-resolution based video coding scheme that compresses video data by combining traditional hybrid video coding and convolutional neural network-based video coding. During video encoding, downsampling reduces the resolution of an original video in both horizontal and vertical directions to reduce original video data, and convolutional neural network-based super-resolution is employed after the decoding process to recover the resolution of the reconstructed video during upsampling. For core encoding and decoding processes, the latest video coding standard (i.e., VVC/H.266) is conducted. The experimental results show that the proposed method can provide efficient coding performance while maintaining good visual quality.",
    "code_link": ""
  },
  "cvpr2022_clic_asoft-rankedindexfusionframeworkwithsaliencyweightingforimagequalityassessment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "A Soft-Ranked Index Fusion Framework With Saliency Weighting for Image Quality Assessment",
    "authors": [
      "Liangwei Yu",
      "Zhao Wang",
      "Yan Ye",
      "Lingyu Zhu",
      "Shiqi Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Yu_A_Soft-Ranked_Index_Fusion_Framework_With_Saliency_Weighting_for_Image_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Yu_A_Soft-Ranked_Index_Fusion_Framework_With_Saliency_Weighting_for_Image_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The compression technique is widely adopted for efficient data storage and transmission. Accurate image quality assessment (IQA) measures are urgently desired to evaluate the compression performance. To obtain a more robust evaluation, we propose a soft-ranked index fusion framework for the perceptual preference prediction task, with a combination of different quality measures. The derived soft-ranked indices are fully leveraged to provide the strong discriminability of ranking information. Furthermore, a saliency weighting approach is utilized to investigate the impact of visual attention on our framework. Experimental results indicate that our method achieves a promising prediction accuracy compared with the state-of-the-art quality measures.",
    "code_link": ""
  },
  "cvpr2022_clic_user-guidedvariableratelearnedimagecompression": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "User-Guided Variable Rate Learned Image Compression",
    "authors": [
      "Rushil Gupta",
      "Suryateja BV",
      "Nikhil Kapoor",
      "Rajat Jaiswal",
      "Sharmila Reddy Nangi",
      "Kuldeep Kulkarni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Gupta_User-Guided_Variable_Rate_Learned_Image_Compression_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Gupta_User-Guided_Variable_Rate_Learned_Image_Compression_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a learning-based image compression method that achieves any arbitrary input bitrate via user-guided bit allocation to preferred regions. We verify our hypothesis of incorporating user guidance for bitrate control by experimenting with alternatives that do not have any guidance. We conduct extensive evaluation on CelebA-HQ and CityScapes dataset using standard quantitative metrics and human studies showing that our single model for multiple bitrates achieves similar or better performance as compared to previous learned image compression methods that require re-training for each new bitrate.",
    "code_link": ""
  },
  "cvpr2022_clic_swiniqalearnedswindistanceforcompressedimagequalityassessment": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "SwinIQA: Learned Swin Distance for Compressed Image Quality Assessment",
    "authors": [
      "Jianzhao Liu",
      "Xin Li",
      "Yanding Peng",
      "Tao Yu",
      "Zhibo Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Liu_SwinIQA_Learned_Swin_Distance_for_Compressed_Image_Quality_Assessment_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Liu_SwinIQA_Learned_Swin_Distance_for_Compressed_Image_Quality_Assessment_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image compression has raised widespread concern recently due to its significant importance for multimedia storage and transmission. Meanwhile, a reliable image quality assessment (IQA) for compressed images can not only help to verify the performance of various compression algorithms but also help to guide the compression optimization in turn. In this paper, we design a full-reference image quality assessment metric SwinIQA to measure the perceptual quality of compressed images in a learned Swin distance space. It is known that the compression artifacts are usually non-uniformly distributed with diverse distortion types and degrees. To warp the compressed images into the shared representation space while maintaining the complex distortion information, we extract the hierarchical feature representations from each of the stage of the Swin Transformer. Besides, we utilize cross attention operation to map the extracted feature representations into a learned Swin distance space. Experimental results show that the proposed metric achieves higher consistency with human's perceptual judgment comapred with both traditional methods and learning-based methods on CLIC dataset.",
    "code_link": ""
  },
  "cvpr2022_clic_rdonetrate-distortionoptimizedlearnedimagecompressionwithvariabledepth": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "RDONet: Rate-Distortion Optimized Learned Image Compression With Variable Depth",
    "authors": [
      "Fabian Brand",
      "Kristian Fischer",
      "Alexander Kopte",
      "Marc Windsheimer",
      "Andr\u00e9 Kaup"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Brand_RDONet_Rate-Distortion_Optimized_Learned_Image_Compression_With_Variable_Depth_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Brand_RDONet_Rate-Distortion_Optimized_Learned_Image_Compression_With_Variable_Depth_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Rate-distortion optimization (RDO) is responsible for large gains in image and video compression. While RDO is a standard tool in traditional image and video coding, it is not yet widely used in novel end-to-end trained neural methods. The major reason is that the decoding function is trained once and does not have free parameters. In this paper, we present RDONet, a network containing state-of-the-art components, which is perceptually optimized and capable of rate-distortion optimization. With this network, we are able to outperform VVC Intra on MS-SSIM and two different perceptual LPIPS metrics. This paper is part of the CLIC challenge, where we participate under the team name RDONet_FAU.",
    "code_link": ""
  },
  "cvpr2022_clic_learnedlowbitratevideocompressionwithspace-timesuper-resolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Learned Low Bitrate Video Compression With Space-Time Super-Resolution",
    "authors": [
      "Jiayu Yang",
      "Chunhui Yang",
      "Fei Xiong",
      "Feng Wang",
      "Ronggang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Yang_Learned_Low_Bitrate_Video_Compression_With_Space-Time_Super-Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Yang_Learned_Low_Bitrate_Video_Compression_With_Space-Time_Super-Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents a learned low bitrate video compression framework that consists of pre-processing, compression and post-processing. In pre-processing stage, the source videos are optionally reduced to low-resolution or low-frame-rate ones to better meet with the limited bandwidth. In compression stage, inter-frame prediction is performed by deformable convolution (DCN). The predicted frame is then used as temporal conditions to compress the current frame. In post-processing stage, the decoded videos are fed into a Space-Time Super-Resolution module, in which the videos are restored to original spatial and temporal resolutions. Experimental results on CLIC22 video test conditions demonstrate that the proposed method shows better performance on both objective and subjective quality at low bitrate. Our team name is PKUSZ-LVC.",
    "code_link": ""
  },
  "cvpr2022_clic_po-elicperception-orientedefficientlearnedimagecoding": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "PO-ELIC: Perception-Oriented Efficient Learned Image Coding",
    "authors": [
      "Dailan He",
      "Ziming Yang",
      "Hongjiu Yu",
      "Tongda Xu",
      "Jixiang Luo",
      "Yuan Chen",
      "Chenjian Gao",
      "Xinjie Shi",
      "Hongwei Qin",
      "Yan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/He_PO-ELIC_Perception-Oriented_Efficient_Learned_Image_Coding_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/He_PO-ELIC_Perception-Oriented_Efficient_Learned_Image_Coding_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In the past years, learned image compression (LIC) has achieved remarkable performance. The recent LIC methods outperform VVC in both PSNR and MS-SSIM. However, the low bit-rate reconstructions of LIC suffer from artifacts such as blurring, color drifting and texture missing. Moreover, those varied artifacts make image quality metrics correlate badly with human perceptual quality. In this paper, we propose PO-ELIC, i.e., Perception-Oriented Efficient Learned Image Coding. To be specific, we adapt ELIC, one of the state-of-the-art LIC models, with adversarial training techniques. We apply a mixture of losses including hinge-form adversarial loss, Charbonnier loss, and style loss, to finetune the model towards better perceptual quality. Experimental results demonstrate that our method achieves comparable perceptual quality with HiFiC with much lower bitrate.",
    "code_link": ""
  },
  "cvpr2022_clic_perceptualin-loopfilterforimageandvideocompression": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Perceptual In-Loop Filter for Image and Video Compression",
    "authors": [
      "Huairui Wang",
      "Guangjie Ren",
      "Tong Ouyang",
      "Junxi Zhang",
      "Wenwei Han",
      "Zizheng Liu",
      "Zhenzhong Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Wang_Perceptual_In-Loop_Filter_for_Image_and_Video_Compression_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Wang_Perceptual_In-Loop_Filter_for_Image_and_Video_Compression_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we introduce our hybrid image and video compression scheme enhanced by CNN-optimized in-loop filter. Specifically, a Structure Preserving in-Loop Filter (SPiLF) is incorporated in the hybrid video codec Enhanced Compression Model (ECM), where two branches, i.e., gradient branch and pixel branch, are developed based on the dense residual unit (DRU). To provide pleasant visual quality, the Generative adversarial networks (GAN) loss and LPIPS loss are further considered. Therefore, the proposal is mainly focusing on perceptual-friendly image compression for human vision, whilst video compression could be further investigated. The experiments show that the proposed method achieves advanced visual quality when compared to the traditional methods.",
    "code_link": ""
  },
  "cvpr2022_clic_neuralnetwork-basedin-loopfilterforclic2022": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Challenge on Learned Image Compression",
    "title": "Neural Network-Based In-Loop Filter for CLIC 2022",
    "authors": [
      "Yonghua Wang",
      "Jingchi Zhang",
      "Zhengang Li",
      "Xing Zeng",
      "Zhen Zhang",
      "Diankai Zhang",
      "Yunlin Long",
      "Ning Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/html/Wang_Neural_Network-Based_In-Loop_Filter_for_CLIC_2022_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Wang_Neural_Network-Based_In-Loop_Filter_for_CLIC_2022_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "A hybrid video codec comprised of an optimized VVC codec and a convolutional neural network-based loop filter (CNNLF), was submitted in the video compression track in Challenge on Learned Image Compression (CLIC) 2022[1]. This paper presents the traditional methods and deep learning scheme in video coding optimization, which were adopted in the hybrid codec based on VTM-15.0. Traditional methods include QP adaptive adjustment of I frame and rate-distortion optimization based on SSIM. Meanwhile, the deep learning scheme proposes an adaptive CNNLF, which is turned on / off based on the rate-distortion optimization at CTU and frame level. The network architecture mainly consists of the attention residual module and the convolution feature maps module, which help extract image features and improve image quality. To balance performance and complexity, the proposed scheme sets different training parameters for 0.1 Mbps and 1 Mbps, respectively. The experimental results show that compared with VTM-15.0, the proposed traditional methods and adding CNNLF improve the PSNR by 0.4dB and 0.8dB at 0.1Mbps, respectively; 0.2dB and 0.5dB at 1Mbps, respectively, which proves the superiority of our method.",
    "code_link": ""
  },
  "cvpr2022_cvmi_self-supervisedvoxel-levelrepresentationrediscoverssubcellularstructuresinvolumeelectronmicroscopy": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Self-Supervised Voxel-Level Representation Rediscovers Subcellular Structures in Volume Electron Microscopy",
    "authors": [
      "Hongqing Han",
      "Mariia Dmitrieva",
      "Alexander Sauer",
      "Ka Ho Tam",
      "Jens Rittscher"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Han_Self-Supervised_Voxel-Level_Representation_Rediscovers_Subcellular_Structures_in_Volume_Electron_Microscopy_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Han_Self-Supervised_Voxel-Level_Representation_Rediscovers_Subcellular_Structures_in_Volume_Electron_Microscopy_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Making sense of large volumes of biological imaging data without human annotation often relies on unsupervised representation learning. Although efforts have been made to representing cropped-out microscopy images of single cells and single molecules, a more robust and general model that effectively maps every voxel in a whole cell volume onto a latent space is still lacking. Here, we use variational auto-encoder and metric learning to obtain a voxel-level representation, and explore using it for unsupervised segmentation. To our knowledge we are the first to present self-supervised voxel-level representation and subsequent unsupervised segmentation results for a complete cell. We improve upon earlier work by proposing an innovative approach to separate latent space into a semantic subspace and a transformational subspace, and only use the semantic representation for segmentation. We show that in the learned semantic representation the major subcellular components are visually distinguishable and the semantic subspace is more transformation-invariant than another sample latent subspace of equal dimension. For unsupervised segmentation we found that our model manages to automatically rediscover and separate the major classes with errors demonstrating spatial patterns, and further dissect the class not specified by reference segmentation into areas with consistent textures. Our segmentation outperforms a baseline by a large margin.",
    "code_link": ""
  },
  "cvpr2022_cvmi_cellselection-baseddatareductionpipelineforwholeslideimageanalysisofacutemyeloidleukemia": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Cell Selection-Based Data Reduction Pipeline for Whole Slide Image Analysis of Acute Myeloid Leukemia",
    "authors": [
      "Jacqueline Kockwelp",
      "Sebastian Thiele",
      "Pascal Kockwelp",
      "Jannis Bartsch",
      "Christoph Schliemann",
      "Linus Angenendt",
      "Benjamin Risse"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Kockwelp_Cell_Selection-Based_Data_Reduction_Pipeline_for_Whole_Slide_Image_Analysis_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Kockwelp_Cell_Selection-Based_Data_Reduction_Pipeline_for_Whole_Slide_Image_Analysis_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Computer-aided analyses of cells in Whole Slide Images (WSIs) have become an important topic in digital pathology. Despite the recent success of deep learning in biomedical research, these methods are still difficult to apply to multi-gigabyte WSIs. To overcome this difficulty, a variety of patch-based solutions have been introduced, which however all suffer from certain limitations compared to manual examinations and often fail to meet the specificities of cytological inspections. Here we introduce an alternative scheme which incorporates clinical expertise in the selection process to automatically identify the clinically relevant areas. By using a bone marrow smear dataset containing 22-gigapixel images of 153 patients, we introduce a novel pipeline combining unsupervised and supervised methodologies to gradually select the most appropriate single-cell regions, which are subsequently used in multiple medically crucial Acute Myeloid Leukemia (AML) predictions. Our approach is capable of dealing with a variety of common WSI challenges, massively limits the manual annotation effort, reduces the data by a factor of up to 99.9% and achieves super-human performance on the final cytological prediction tasks.",
    "code_link": ""
  },
  "cvpr2022_cvmi_anensemblelearningandslicefusionstrategyforthree-dimensionalnucleiinstancesegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "An Ensemble Learning and Slice Fusion Strategy for Three-Dimensional Nuclei Instance Segmentation",
    "authors": [
      "Liming Wu",
      "Alain Chen",
      "Paul Salama",
      "Kenneth W. Dunn",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Wu_An_Ensemble_Learning_and_Slice_Fusion_Strategy_for_Three-Dimensional_Nuclei_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Wu_An_Ensemble_Learning_and_Slice_Fusion_Strategy_for_Three-Dimensional_Nuclei_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Automated microscopy image analysis is a fundamental step for digital pathology and computer aided diagnosis. Most existing deep learning methods typically require postprocessing to achieve instance segmentation and are computationally expensive when directly used with 3D microscopy volumes. Supervised learning methods generally need large amounts of ground truth annotations for training whereas manually annotating ground truth masks is laborious especially for a 3D volume. To address these issues, we propose an ensemble learning and slice fusion strategy for 3D nuclei instance segmentation that we call Ensemble Mask R-CNN (EMR-CNN) which uses different object detectors to generate nuclei segmentation masks for each 2D slice of a volume and propose a 2D ensemble fusion and a 2D to 3D slice fusion to merge these 2D segmentation masks into a 3D segmentation mask. Our method does not need any ground truth annotations for training and can inference on any large size volumes. Our proposed method was tested on a variety of microscopy volumes collected from multiple regions of organ tissues. The execution time and robustness analyses show that our method is practical and effective.",
    "code_link": "https://github.com/facebookresearch/detectron2"
  },
  "cvpr2022_cvmi_fourierimagetransformer": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Fourier Image Transformer",
    "authors": [
      "Tim-Oliver Buchholz",
      "Florian Jug"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Buchholz_Fourier_Image_Transformer_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Buchholz_Fourier_Image_Transformer_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Transformer architectures show spectacular performance on NLP tasks and have recently also been used for tasks such as image completion or image classification. Here we propose to use a sequential image representation, where each prefix of the complete sequence describes the whole image at reduced resolution. Using such Fourier Domain Encodings (FDEs), an auto-regressive image completion task is equivalent to predicting a higher resolution output given a low-resolution input. Additionally, we show that an encoder-decoder setup can be used to query arbitrary Fourier coefficients given a set of Fourier domain observations. We demonstrate the practicality of this approach in the context of computed tomography (CT) image reconstruction. In summary, we show that Fourier Image Transformer (FIT) can be used to solve relevant image analysis tasks in Fourier space, a domain inherently inaccessible to convolutional architectures.",
    "code_link": ""
  },
  "cvpr2022_cvmi_bcibreastcancerimmunohistochemicalimagegenerationthroughpyramidpix2pix": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "BCI: Breast Cancer Immunohistochemical Image Generation Through Pyramid Pix2pix",
    "authors": [
      "Shengjie Liu",
      "Chuang Zhu",
      "Feng Xu",
      "Xinyu Jia",
      "Zhongyue Shi",
      "Mulan Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Liu_BCI_Breast_Cancer_Immunohistochemical_Image_Generation_Through_Pyramid_Pix2pix_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Liu_BCI_Breast_Cancer_Immunohistochemical_Image_Generation_Through_Pyramid_Pix2pix_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The evaluation of human epidermal growth factor receptor 2 (HER2) expression is essential to formulate a precise treatment for breast cancer. The routine evaluation of HER2 is conducted with immunohistochemical techniques (IHC), which is very expensive. Therefore, for the first time, we propose a breast cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data directly with the paired hematoxylin and eosin (HE) stained images. The dataset contains 4870 registered image pairs, covering a variety of HER2 expression levels. Based on BCI, as a minor contribution, we further build a pyramid pix2pix image generation method, which achieves better HE to IHC translation results than the other current popular algorithms. Extensive experiments demonstrate that BCI poses new challenges to the existing image translation research. Besides, BCI also opens the door for future pathology studies in HER2 expression evaluation based on the synthesized IHC images. BCI dataset can be downloaded from https://bupt-ai-cz.github.io/BCI.",
    "code_link": ""
  },
  "cvpr2022_cvmi_multi-classcelldetectionusingmodifiedself-attention": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multi-Class Cell Detection Using Modified Self-Attention",
    "authors": [
      "Tatsuhiko Sugimoto",
      "Hiroaki Ito",
      "Yuki Teramoto",
      "Akihiko Yoshizawa",
      "Ryoma Bise"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Sugimoto_Multi-Class_Cell_Detection_Using_Modified_Self-Attention_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Sugimoto_Multi-Class_Cell_Detection_Using_Modified_Self-Attention_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-class cell detection (cancer or non-cancer) from a whole slide image (WSI) is an important task for pathological diagnosis. Cancer and non-cancer cells often have a similar appearance, so it is difficult even for experts to classify a cell from a patch image of individual cells. They usually identify the cell type not only on the basis of the appearance of a single cell but also on the context from the surrounding cells. For using such information, we propose a multi-class cell-detection method that introduces a modified self-attention to aggregate the surrounding image features of both classes. Experimental results demonstrate the effectiveness of the proposed method; our method achieved the best performance compared with a method, which simply use the standard self-attention method.",
    "code_link": ""
  },
  "cvpr2022_cvmi_bloodvesselsegmentationfromlow-contrastandwide-fieldopticalmicroscopicimagesofcranialwindowbyattention-gate-basednetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Blood Vessel Segmentation From Low-Contrast and Wide-Field Optical Microscopic Images of Cranial Window by Attention-Gate-Based Network",
    "authors": [
      "Yunheng Wu",
      "Masahiro Oda",
      "Yuichiro Hayashi",
      "Takanori Takebe",
      "Shogo Nagata",
      "Cheng Wang",
      "Kensaku Mori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Wu_Blood_Vessel_Segmentation_From_Low-Contrast_and_Wide-Field_Optical_Microscopic_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Wu_Blood_Vessel_Segmentation_From_Low-Contrast_and_Wide-Field_Optical_Microscopic_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The stereomicroscope, which is an optical microscope, is used to observe the organoids cultured in cranial windows. A cranial window is a light accessible observation window made on the brain of mice through craniotomy. Organoids research is often conducted on cranial windows. Hence, the observation of blood vessels in them is important for organoid research, like organoid vascularization. Therefore, achieving a simple, low-cost method that extracts blood vessel structures would significantly help researchers observe the blood vessels in cranial windows from microscopic images. However, wide-field optical microscopic images taken by stereomicroscope suffer from low contrast and dura mater occlusion, complicating the observation of the blood vessels in such images. To address such problems and assist researchers who are observing vascular structures, we propose a method that segments blood vessels in cranial windows from low-contrast and wide-field microscopic images. Our method is based on the Attention U-Net framework and clDice, which considers the connectivity of blood vessels. In addition, for low-contrast and partial occlusion problems, we used contrast enhancement and dehazing as preprocessing steps. Our method achieved a Dice score of 75.56%, a clDice score of 79.95%, and the Accuracy of 91.41% on our microscopic image dataset, suggesting that our method can extract blood vessels from low-contrast and wide-field microscopic images better than other methods.",
    "code_link": ""
  },
  "cvpr2022_cvmi_multistaingraphfusionformultimodalintegrationinpathology": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multi Stain Graph Fusion for Multimodal Integration in Pathology",
    "authors": [
      "Chaitanya Dwivedi",
      "Shima Nofallah",
      "Maryam Pouryahya",
      "Janani Iyer",
      "Kenneth Leidal",
      "Chuhan Chung",
      "Timothy Watkins",
      "Andrew Billin",
      "Robert Myers",
      "John Abel",
      "Ali Behrooz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/html/Dwivedi_Multi_Stain_Graph_Fusion_for_Multimodal_Integration_in_Pathology_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Dwivedi_Multi_Stain_Graph_Fusion_for_Multimodal_Integration_in_Pathology_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In pathology, tissue samples are assessed using multiple staining techniques to enhance contrast in unique histologic features. In this paper, we introduce a multimodal CNN-GNN based graph fusion approach that leverages complementary information from multiple non-registered histopathology images to predict pathologic scores. We demonstrate this approach in nonalcoholic steatohepatitis (NASH) by predicting CRN fibrosis stage and NAFLD Activity Score (NAS). Primary assessment of NASH typically requires liver biopsy evaluation on two histological stains: Trichrome (TC) and hematoxylin and eosin (H&E). Our multimodal approach learns to extract complementary information from TC and H&E graphs corresponding to each stain while simultaneously learning an optimal policy to combine this information. We report up to 20% improvement in predicting fibrosis stage and NAS component grades over single-stain modeling approaches, measured by computing linearly weighted Cohen's kappa between machine-derived vs. pathologist consensus scores. Broadly, this paper demonstrates the value of leveraging diverse pathology images for improved ML-powered histologic assessment.",
    "code_link": ""
  },
  "cvpr2022_vocvalc_couplingvisionandproprioceptionfornavigationofleggedrobots": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Coupling Vision and Proprioception for Navigation of Legged Robots",
    "authors": [
      "Zipeng Fu",
      "Ashish Kumar",
      "Ananye Agarwal",
      "Haozhi Qi",
      "Jitendra Malik",
      "Deepak Pathak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/html/Fu_Coupling_Vision_and_Proprioception_for_Navigation_of_Legged_Robots_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Fu_Coupling_Vision_and_Proprioception_for_Navigation_of_Legged_Robots_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We exploit the complementary strengths of vision and proprioception to develop a point-goal navigation system for legged robots, called VP-Nav. Legged systems are capable of traversing more complex terrain than wheeled robots, but to fully utilize this capability, we need a high-level path planner in the navigation system to be aware of the walking capabilities of the low-level locomotion policy in varying environments. We achieve this by using proprioceptive feedback to ensure the safety of the planned path by sensing unexpected obstacles like glass walls, terrain properties like slipperiness or softness of the ground and robot properties like extra payload that are likely missed by vision. The navigation system uses onboard cameras to generate an occupancy map and a corresponding cost map to reach the goal. A fast marching planner then generates a target path. A velocity command generator takes this as input to generate the desired velocity for the walking policy. A safety advisor module adds sensed unexpected obstacles to the occupancy map and environment-determined speed limits to the velocity command generator. We show superior performance compared to wheeled robot baselines, and ablation studies which have disjoint high-level planning and low-level control. We also show the real-world deployment of VP-Nav on a quadruped robot with onboard sensors and computation. Videos at https://navigation-locomotion.github.io.",
    "code_link": ""
  },
  "cvpr2022_vocvalc_revisitingthereceptivefieldofconv-gruindroid-slam": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Revisiting the Receptive Field of Conv-GRU in DROID-SLAM",
    "authors": [
      "Antyanta Bangunharcana",
      "Soohyun Kim",
      "Kyung-Soo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/html/Bangunharcana_Revisiting_the_Receptive_Field_of_Conv-GRU_in_DROID-SLAM_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Bangunharcana_Revisiting_the_Receptive_Field_of_Conv-GRU_in_DROID-SLAM_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "his work focuses on improving the Conv-GRU-based optical flow update within a DROID-SLAM framework. Prior optical flow models typically follow a UNet or coarse-to-fine architecture in order to extract long-range cross-correlation and context cues. This helps flow estimation in the presence of large motion and challenging image regions, e.g., textureless regions. We propose modifications to the Conv-GRU module which follows the rationale of these prior models by integrating (Atrous) Spatial Pyramid Pooling and global self-attention into the Conv-GRU block. By enlarging the receptive field through the aforementioned modifications, the model is able to integrate information from a larger context window, thus improving the robustness even when given inputs that comprise challenging image regions. We show empirically through extensive experiments the gain in accuracy through these modifications.",
    "code_link": ""
  },
  "cvpr2022_vocvalc_exploringmotioninformationfordistractorsuppressioninvisualtracking": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Exploring Motion Information for Distractor Suppression in Visual Tracking",
    "authors": [
      "Kaiwen Liu",
      "Jin Gao",
      "Haowei Liu",
      "Liang Li",
      "Bing Li",
      "Weiming Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/html/Liu_Exploring_Motion_Information_for_Distractor_Suppression_in_Visual_Tracking_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Liu_Exploring_Motion_Information_for_Distractor_Suppression_in_Visual_Tracking_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In the past few years, Siamese networks have achieved outstanding improvements in visual object tracking. However, visual distractors with similar semantics can be easily misclassified as the target by Siamese networks and may consequently result in the drift problem. Besides, the Hanning window penalty, which is generally used to suppress distractors, could fail in many challengeable scenes. Notably, most failures violate the assumption of motion continuity. Thus, in this work, we explore motion information to mitigate the drift problem in visual tracking. First, we introduce a simple linear Kalman filter to predict the bounding box of the target in the current frame, which acts as a reference for decisions. Second, an IoU-Guided penalty is assembled in the post-processing to suppress distractors effectively. It's worth mentioning that our method is almost cost-free. We conduct numerous experimental validations and analyses of our approach on several challenging sequences and datasets. Our tracker runs at approximately 40 fps and performs well on those sequences which include the Background Clutter attribute. Finally, by simultaneously integrating the IoU-Guided penalty and the Hanning window penalty with a strong baseline tracker TransT, our method achieves favorable gains by 69.1 to 71.5, 65.7 to 66.7, 64.9 to 65.9 success on OTB-100, LaSOT, NFS.",
    "code_link": ""
  },
  "cvpr2022_vocvalc_parallelgenerativeadversarialnetworkforthird-persontofirst-personimagegeneration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Parallel Generative Adversarial Network for Third-Person to First-Person Image Generation",
    "authors": [
      "Gaowen Liu",
      "Hugo Latapie",
      "Ozkan Kilic",
      "Adam Lawrence"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/html/Liu_Parallel_Generative_Adversarial_Network_for_Third-Person_to_First-Person_Image_Generation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Liu_Parallel_Generative_Adversarial_Network_for_Third-Person_to_First-Person_Image_Generation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Cross-view image generation has been recently proposed to generate images of one view from another dramatically different view. In this paper, we investigate third-person (exocentric) view to first-person (egocentric) view image generation. This is a challenging task since egocentric view sometimes is remarkably different from exocentric view. Thus, transforming the appearances across the two views is a non-trivial task. To this end, we propose a novel Parallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss to learn the shared information for generating egocentric images from exocentric view. We also incorporate a novel contextual feature loss in the learning procedure to capture the contextual information in images. Extensive experiments on the Exo-Ego datasets show that our model outperforms the state-of-the-art approaches.",
    "code_link": ""
  },
  "cvpr2022_nas_spidernethybriddifferentiable-evolutionaryarchitecturesearchviatrain-freemetrics": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NAS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Neural Architecture Search: Lightweight NAS Challenge",
    "title": "SpiderNet: Hybrid Differentiable-Evolutionary Architecture Search via Train-Free Metrics",
    "authors": [
      "Rob Geada",
      "Andrew Stephen McGough"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Geada_SpiderNet_Hybrid_Differentiable-Evolutionary_Architecture_Search_via_Train-Free_Metrics_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Geada_SpiderNet_Hybrid_Differentiable-Evolutionary_Architecture_Search_via_Train-Free_Metrics_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural Architecture Search (NAS) algorithms are intended to remove the burden of manual neural network design, and have shown to be capable of designing excellent models for a variety of well-known problems. However, these algorithms require a variety of design parameters in the form of user configuration or hard-coded decisions which limit the variety of networks that can be discovered. This means that NAS algorithms do not eliminate model design tuning, they instead merely shift the burden of where that tuning needs to be applied. In this paper, we present SpiderNet, a hybrid differentiable-evolutionary and hardware-aware algorithm that rapidly and efficiently produces state-of-the-art networks. More importantly, SpiderNet is a proof-of-concept of a minimally-configured NAS algorithm; the majority of design choices seen in other algorithms are incorporated into SpiderNet's dynamically-evolving search space, minimizing the number of user choices to just two: reduction cell count and initial channel count. SpiderNet produces models highly-competitive with the state-of-the-art, and outperforms random search in accuracy, runtime, memory size, and parameter count.",
    "code_link": ""
  },
  "cvpr2022_nas_dnasadecoupledglobalneuralarchitecturesearchmethod": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NAS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Neural Architecture Search: Lightweight NAS Challenge",
    "title": "DNAS:A Decoupled Global Neural Architecture Search Method",
    "authors": [
      "Kepeng Xu",
      "Gang He"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Xu_DNASA_Decoupled_Global_Neural_Architecture_Search_Method_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Xu_DNASA_Decoupled_Global_Neural_Architecture_Search_Method_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural Architecture Search (NAS) can automatically design model architecture with better performance. Current researchers have searched for local architecture similar to block, then stacked to construct entire models, or searched the entire model based on a manually designed benchmark module. There is no method to directly search the architecture of the global(entire) model at the operation level. The purpose of this article is to search the entire model directly in the operation level search space. We analyzed the search space of past methods which searching for local architectures, then a working mode for global model architecture search named CAM is proposed. Proposed CAM decouples the architectural parameters of the entire model which can complete the entire model architecture search with few architecture parameters. In the experiment, the test error 2.68% in CIFAR-10 is obtained by the proposed method at the global architecture level, which can compare with the stage-of-art local architecture search methods.",
    "code_link": ""
  },
  "cvpr2022_nas_lessismoreproxydatasetsinnasapproaches": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NAS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Neural Architecture Search: Lightweight NAS Challenge",
    "title": "Less Is More: Proxy Datasets in NAS Approaches",
    "authors": [
      "Brian Moser",
      "Federico Raue",
      "J\u00f6rn Hees",
      "Andreas Dengel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Moser_Less_Is_More_Proxy_Datasets_in_NAS_Approaches_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Moser_Less_Is_More_Proxy_Datasets_in_NAS_Approaches_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural Architecture Search (NAS) defines the design of Neural Networks as a search problem. Unfortunately, NAS is computationally intensive because of various possibilities depending on the number of elements in the design and the possible connections between them. In this work, we extensively analyze the role of the dataset size based on several sampling approaches for reducing the dataset size (unsupervised and supervised cases) as an agnostic approach to reduce search time. We compared these techniques with four common NAS approaches in NAS-Bench-201 in roughly 1,400 experiments on CIFAR-100. One of our surprising findings is that in most cases we can reduce the amount of training data to 25 %, consequently also reducing search time to 25 %, while at the same time maintaining the same accuracy as if training on the full dataset. In addition, some designs derived from subsets out-perform designs derived from the full dataset by up to 22 p.p. accuracy.",
    "code_link": ""
  },
  "cvpr2022_nas_hot-startednasfortask-specificembeddedapplications": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NAS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Neural Architecture Search: Lightweight NAS Challenge",
    "title": "Hot-Started NAS for Task-Specific Embedded Applications",
    "authors": [
      "Lotte Hendrickx",
      "Wiebe Van Ranst",
      "Toon Goedem\u00e9"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Hendrickx_Hot-Started_NAS_for_Task-Specific_Embedded_Applications_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Hendrickx_Hot-Started_NAS_for_Task-Specific_Embedded_Applications_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural architecture search (NAS) has proven its worth in discovering new neural networks. Combining the possibility to satisfy multiple objectives in one search, it is especially useful for getting the most out of embedded devices with limited resources. However, research into small and efficient neural networks precedes NAS. We investigate the influence of combining this pre-existing knowledge with NAS techniques, for which we propose to hot-start the NAS search with a human-designed optimal network. Our experiments show that doing so speeds up the NAS process significantly, but the resulting optimal model at the end is only marginally better. Since embedded devices are often used for a specific task, we also explore the impact of using a task-specific dataset in the NAS process. Our experiments demonstrate that for a constrained problem, a smaller network can be found as compared to a general problem.",
    "code_link": ""
  },
  "cvpr2022_nas_networkamplificationwithefficientmacsallocation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NAS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Neural Architecture Search: Lightweight NAS Challenge",
    "title": "Network Amplification With Efficient MACs Allocation",
    "authors": [
      "Chuanjian Liu",
      "Kai Han",
      "An Xiao",
      "Ying Nie",
      "Wei Zhang",
      "Yunhe Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Liu_Network_Amplification_With_Efficient_MACs_Allocation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Liu_Network_Amplification_With_Efficient_MACs_Allocation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent studies on deep convolutional neural networks present a simple paradigm of architecture design, i.e., models with more MACs typically achieve better accuracies, such as EfficientNet and RegNet. These works try to enlarge the network architecture with one unified rule by sampling and statistical methods. However, the rule is not prospective to the design of large networks because it is obtained from the experience of researchers on small network architectures. In this paper, we propose to enlarge the capacity of CNN models by fine-grained MACs allocation for the width, depth and resolution on the stage level. In particular, starting from a base small model, we gradually add extra channels, layers or resolution by using a dynamic programming manner. With step-by-step modifying the computations on different stages, the enlarged network will be equipped with optimal allocation and utilization of MACs. On EfficientNet, our method consistently outperforms the performance of the original scaling method. In particular, the proposed method is used to enlarge models sourced by GhostNet, we achieve state-of-the-art 80.9% and 84.3% ImageNet top-1 accuracies under the setting of 600M and 4.4B MACs, respectively.",
    "code_link": ""
  },
  "cvpr2022_nas_searchingforenergy-efficienthybridadder-convolutionneuralnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "NAS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Neural Architecture Search: Lightweight NAS Challenge",
    "title": "Searching for Energy-Efficient Hybrid Adder-Convolution Neural Networks",
    "authors": [
      "Wenshuo Li",
      "Xinghao Chen",
      "Jinyu Bai",
      "Xuefei Ning",
      "Yunhe Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Li_Searching_for_Energy-Efficient_Hybrid_Adder-Convolution_Neural_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Li_Searching_for_Energy-Efficient_Hybrid_Adder-Convolution_Neural_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "As convolutional neural networks (CNNs) are more and more widely used in computer vision area, the energy consumption of CNNs has become the focus of researchers. For edge devices, both the battery life and the inference latency are critical and directly affect user experience. Recently, great progress has been made in the design of neural architectures and new operators. The emergence of neural architecture search technology has improved the performance of network step by step, and liberated the productivity of engineers to a certain extent. New operators, such as AdderNets, make it possible to further improve the energy efficiency of neural networks. In this paper, we explore the fusion of new adder operators and common convolution operators into state-of-the-art light-weight networks, GhostNet, to search for models with better energy efficiency and performance. Our proposed search equilibrium strategy ensures that the adder and convolution operators can be treated fairly in the search, and the resulting model achieves the same accuracy of 73.9% with GhostNet on the ImageNet dataset at an extremely low power consumption of 0.612 mJ. When keeping the same energy consumption, the accuracy reaches 74.3% which is 0.4% higher than original GhostNet.",
    "code_link": ""
  },
  "cvpr2022_wicv_generativeprobabilisticnoveltydetectionwithisometricadversarialautoencoders": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "Generative Probabilistic Novelty Detection With Isometric Adversarial Autoencoders",
    "authors": [
      "Ranya Almohsen",
      "Matthew R. Keaton",
      "Donald A. Adjeroh",
      "Gianfranco Doretto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Almohsen_Generative_Probabilistic_Novelty_Detection_With_Isometric_Adversarial_Autoencoders_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Almohsen_Generative_Probabilistic_Novelty_Detection_With_Isometric_Adversarial_Autoencoders_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning the manifold of a complex distribution is a fundamental challenge for novelty or anomaly detection. We introduce a revised learning and inference procedure that takes into account a key underlying assumption made by the framework of generative probabilistic novelty detection. The traditional framework implies the ability to not only learn the manifold of the generative distribution of inliers but also to compute non-linear orthogonal projections onto this manifold from the ambient space. We augment the original training with priors that endow the model with this property, and prove that inference becomes easier and computationally more efficient. We show experimentally that the new framework leads to improved and more stable results.",
    "code_link": ""
  },
  "cvpr2022_wicv_rv-ganrecurrentganforunconditionalvideogeneration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "RV-GAN: Recurrent GAN for Unconditional Video Generation",
    "authors": [
      "Sonam Gupta",
      "Arti Keshari",
      "Sukhendu Das"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Gupta_RV-GAN_Recurrent_GAN_for_Unconditional_Video_Generation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Gupta_RV-GAN_Recurrent_GAN_for_Unconditional_Video_Generation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Generative models aiming to generate content from noise have achieved high-fidelity synthesis for image data. However, obtaining comparable performance in the field of unconditional video generation still remains challenging. In this work, we propose a recurrent GAN architecture to model the high-dimensional video data distribution. Recurrent networks by design are able to generate complex, long sequences in an autoregressive fashion. However, the standard LSTM unit for videos (ConvLSTM) is not ideally suited for the task of unconditional video generation. Therefore, we propose a simple yet effective LSTM variant called as TransConv LSTM (TC-LSTM) by modulating the conventional ConvLSTM to have a transpose convolutional structure in input-to-state transitions. This enables the network to model both spatial and temporal relationships across layers simultaneously inside the TC-LSTM unit. TC-LSTM unit acts as a building block of our generator. Extensive quantitative and qualitative analysis shows that RV-GAN outperforms state-of-the-art methods by a significant margin on Moving MNIST, MUG, Weizmann and UCF101 datasets. Additionally, owing to the recurrent structure, our method is able to generate high-quality videos, up to 2 times longer (32 frames) than training videos at inference time. Further analysis confirms that the proposed architecture is generic and can be easily adapted to other tasks like class-conditional video synthesis and text-to-video synthesis.",
    "code_link": ""
  },
  "cvpr2022_wicv_autoencoders-acomparativeanalysisintherealmofanomalydetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "Autoencoders - A Comparative Analysis in the Realm of Anomaly Detection",
    "authors": [
      "Sarah Schneider",
      "Doris Antensteiner",
      "Daniel Soukup",
      "Matthias Scheutz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Schneider_Autoencoders_-_A_Comparative_Analysis_in_the_Realm_of_Anomaly_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Schneider_Autoencoders_-_A_Comparative_Analysis_in_the_Realm_of_Anomaly_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We applied convolutional versions of a \"standard\" autoencoder (CAE), a variational autoencoder (VAE) and an adversarial autoencoder (AAE) to two different publicly available datasets and compared their anomaly detection performances. We used the MNIST dataset [14] as a simple anomaly detection scenario. The CIFAR10 dataset [13] was used to examine the autoencoders in a more complex anomaly detection task. The anomaly detection performance of our different autoencoder types is compared in a qualitative and quantitative manner. The time needed for training the models is measured to capture their complexity. The simplest model demanding the simplest training, the CAE, computes results which are nearly as accurate and for some cases even better than results achieved by the VAE and AAE. We show that all three autoencoder types computed convincing anomaly detection results for the more simple-structured MNIST scenario. However, none of the autoencoder types proved to capture a good representation of the relevant features of the more complex CIFAR10 dataset, leading to moderately good anomaly detection performances.",
    "code_link": ""
  },
  "cvpr2022_wicv_detectingobjectsinlessresponsetimeforprocessingmultimediaeventsinsmartcities": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "Detecting Objects in Less Response Time for Processing Multimedia Events in Smart Cities",
    "authors": [
      "Asra Aslam"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Aslam_Detecting_Objects_in_Less_Response_Time_for_Processing_Multimedia_Events_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Aslam_Detecting_Objects_in_Less_Response_Time_for_Processing_Multimedia_Events_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Due to increase in multimedia traffic in smart cities, we are facing the problem of processing unseen classes in real-time. Existing neural-network based object detectors may support this growing demand of multimedia data but have the limitation of availability of trained classifiers for unseen concepts. This results in a long waiting time for users who want to detect unseen classes. In this paper, we proposed three approaches where we can utilize existing object detection models and can train unseen classes within short training time. Our approaches are based on similarity of unseen classes with seen classes, and availability (presence or absence) of bounding boxes. Our results indicate that the proposed framework can achieve accuracy between 95.14% to 98.53% within response time of0.01 min to30 min for seen and partially unseen classes. Moreover we achieve state of the art results (68.78 mAP within 10 min) for unseen classes that have only image-level labels for training and no bounding boxes. Our qualitative results indicate that our approaches can work well for any unseen class (not only for conventional object detection datasets).",
    "code_link": ""
  },
  "cvpr2022_wicv_deepdensityestimationbasedonmulti-spectralremotesensingdataforin-fieldcropyieldforecasting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "Deep Density Estimation Based on Multi-Spectral Remote Sensing Data for In-Field Crop Yield Forecasting",
    "authors": [
      "Liana Baghdasaryan",
      "Razmik Melikbekyan",
      "Arthur Dolmajain",
      "Jennifer Hobbs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Baghdasaryan_Deep_Density_Estimation_Based_on_Multi-Spectral_Remote_Sensing_Data_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Baghdasaryan_Deep_Density_Estimation_Based_on_Multi-Spectral_Remote_Sensing_Data_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Yield forecasting has been a central task in computational agriculture because of its impact on agricultural management from the individual farmer to the government level. With advances in remote sensing technology, computational processing power, and machine learning, the ability to forecast yield has improved substantially over the past years. However, most previous work has been done leveraging low-resolution satellite imagery and forecasting yield at the region, county, or occasionally farm-level. In this work, we use high-resolution aerial imagery and output from high-precision harvesters to predict in-field harvest values for corn-raising farms in the US Midwest. By using the harvester information, we are able to cast the problem of yield-forecasting as a density estimation problem and predict a harvest rate, in bushels/acre, at every pixel in the field image. This approach provides the farmer with a detailed view of which areas of the farm may be performing poorly so he can make the appropriate management decisions in addition to providing an improved prediction of total yield. We evaluate both traditional machine learning approaches with hand-crafted features alongside deep learning methods. We demonstrate the superiority of our pixel-level approach based on an encoder-decoder framework which produces a 5.41% MAPE at the field-level.",
    "code_link": ""
  },
  "cvpr2022_wicv_enrichedrobustmulti-viewkernelsubspaceclustering": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "Enriched Robust Multi-View Kernel Subspace Clustering",
    "authors": [
      "Mengyuan Zhang",
      "Kai Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Zhang_Enriched_Robust_Multi-View_Kernel_Subspace_Clustering_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Zhang_Enriched_Robust_Multi-View_Kernel_Subspace_Clustering_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Subspace clustering is to find underlying low-dimensional subspaces and cluster the data points correctly. In this paper, we propose a novel multi-view subspace clustering method. Most existing methods suffer from two critical issues. First, they usually adopt a two-stage framework and isolate the processes of affinity learning, multi-view information fusion and clustering. Second, they assume the data lies in a linear subspace which may fail in practice as most real-world datasets may have non-linearity structures. To address the above issues, in this paper we propose a novel Enriched Robust Multi-View Kernel Subspace Clustering framework where the consensus affinity matrix is learned from both multi-view data and spectral clustering. Due to the objective and constraints which is difficult to optimize, we propose an iterative optimization method which is easy to implement and can yield closed solution in each step. Extensive experiments have validated the superiority of our method over state-of-the-art clustering methods.",
    "code_link": ""
  },
  "cvpr2022_wicv_materialswappingfor3dscenesusingalearntmaterialsimilaritymeasure": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Women in Computer Vision",
    "title": "Material Swapping for 3D Scenes Using a Learnt Material Similarity Measure",
    "authors": [
      "Maxine Perroni-Scharf",
      "Kalyan Sunkavalli",
      "Jonathan Eisenmann",
      "Yannick Hold-Geoffroy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Perroni-Scharf_Material_Swapping_for_3D_Scenes_Using_a_Learnt_Material_Similarity_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Perroni-Scharf_Material_Swapping_for_3D_Scenes_Using_a_Learnt_Material_Similarity_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a method for augmenting photo-realistic 3D scene assets by automatically recognizing, matching, and swapping their materials. Our method proposes a material matching pipeline for the efficient replacement of unknown materials with perceptually similar PBR materials from a database, enabling the quick creation of many variations of a given 3D synthetic scene. At the heart of this method is a novel material similarity feature that is learnt, in conjunction with optimal lighting conditions, by fine-tuning a deep neural network on a material classification task using our proposed dataset. Our evaluation demonstrates that lighting optimization improves CNN-based texture feature extraction methods and better estimates material properties. We conduct a series of experiments showing our method's ability to augment photo-realistic indoor scenes using both standard and procedurally generated PBR materials.",
    "code_link": ""
  },
  "cvpr2022_cvpm_perfusionassessmentvialocalremotephotoplethysmography(rppg)": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Perfusion Assessment via Local Remote Photoplethysmography (rPPG)",
    "authors": [
      "Benjamin Kossack",
      "Eric Wisotzky",
      "Peter Eisert",
      "Sebastian P. Schraven",
      "Brigitta Globke",
      "Anna Hilsmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Kossack_Perfusion_Assessment_via_Local_Remote_Photoplethysmography_rPPG_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Kossack_Perfusion_Assessment_via_Local_Remote_Photoplethysmography_rPPG_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents an approach to assess the perfusion of visible human tissue from RGB video files. We propose metrics derived from remote photoplethysmography (rPPG) signals to detect whether a tissue is adequately supplied with blood. The perfusion analysis is done in three different scales, offering a flexible approach for different applications. We perform a plane-orthogonal-to-skin rPPG independently for locally defined regions of interest on each scale. From the extracted signals, we derive the signal-to-noise ratio, magnitude in the frequency domain, heart rate, perfusion index as well as correlation between specific rPPG signals in order to locally assess the perfusion of a specific region of human tissue. We show that locally resolved rPPG has a broad range of applications. As exemplary applications, we present results in intraoperative perfusion analysis and visualization during skin and organ transplantation as well as an application for liveliness assessment for the detection of presentation attacks to authentication systems.",
    "code_link": ""
  },
  "cvpr2022_cvpm_rtrppganultralight3dcnnforreal-timeremotephotoplethysmography": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "RTrPPG: An Ultra Light 3DCNN for Real-Time Remote Photoplethysmography",
    "authors": [
      "Deivid Botina-Monsalve",
      "Yannick Benezeth",
      "Johel Miteran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Botina-Monsalve_RTrPPG_An_Ultra_Light_3DCNN_for_Real-Time_Remote_Photoplethysmography_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Botina-Monsalve_RTrPPG_An_Ultra_Light_3DCNN_for_Real-Time_Remote_Photoplethysmography_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The acquisition of remote photoplethysmography (rPPG) signals is important in multiple applications. Recently, deep-learning-based approaches such as 3D convolutional networks (3DCNNs) have outperformed traditional hand-crafted methods. However, despite their robust modeling ability, it is well known that large 3DCNN models have high computational costs and may be unsuitable for real-time applications. In this paper, we propose a study of the 3DCNN architecture, finding the best compromise between heart rate measurement precision and inference time. The fast inference is obtained decreasing the input size while the precision performance is obtained introducing a new time and frequency-based loss function by adding the signal-to-noise-ratio component to the regular Pearson's correlation loss function. In addition, changing the input color space from RGB to YUV also improved heart rate measurement precision. Using the VIPL-HR database, we retained the HR mean absolute error at 3.99 bpm which is comparable to 3.87 bpm of the state-of-the-art, while the GPU and CPU inference process improved around 88% from 51.77 ms to 2.32 ms in GPU and from 241.57 ms to 28.65 ms in CPU. The resulting network is called Real-Time rPPG (RTrPPG). We release the RTrPPG source code to encourage reproducibility.",
    "code_link": ""
  },
  "cvpr2022_cvpm_optimisingrppgsignalextractionbyexploitingfacialsurfaceorientation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Optimising rPPG Signal Extraction by Exploiting Facial Surface Orientation",
    "authors": [
      "Kwan Long Wong",
      "Jing Wei Chin",
      "Tsz Tai Chan",
      "Ismoil Odinaev",
      "Kristian Suhartono",
      "Kang Tianqu",
      "Richard H.Y. So"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Wong_Optimising_rPPG_Signal_Extraction_by_Exploiting_Facial_Surface_Orientation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Wong_Optimising_rPPG_Signal_Extraction_by_Exploiting_Facial_Surface_Orientation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Remote photoplethysmography (rPPG) is a contactless method to measure human vital signs by detecting subtle skin color changes through a camera. Although many studies have used region of interest (ROI) selection tools to improve rPPG signal extraction, no study has investigated the influence of the ROI's surface orientation. We propose a novel 'angle map' representation of the face to study the effects of the surface orientation on the extracted rPPG signal. The angle map is generated by mapping each facial pixel to an angle of reflection (angle between the skin surface and the camera) calculated from the surface normal of the facial landmarks and the camera axis. Our results show that surface orientation significantly affects the correlation between the extracted rPPG signal and ground truth blood volume pulse (BVP). Regions with small angles of reflection contained stronger signals, which explains why areas near the cheeks and forehead are often chosen for rPPG signal extraction. Moreover, we applied a thresholding method to the angle map and demonstrated its potential for dynamic ROI selection, thereby optimising the rPPG signal extraction process.",
    "code_link": ""
  },
  "cvpr2022_cvpm_remoteheartrateestimationbysignalqualityattentionnetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Remote Heart Rate Estimation by Signal Quality Attention Network",
    "authors": [
      "Haoyuan Gao",
      "Xiaopei Wu",
      "Jidong Geng",
      "Yang Lv"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Gao_Remote_Heart_Rate_Estimation_by_Signal_Quality_Attention_Network_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Gao_Remote_Heart_Rate_Estimation_by_Signal_Quality_Attention_Network_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Heart rate estimation is very important for heart health monitoring. As a non-invasive optical technology, remote photoplethysmography (rPPG) has the advantages of non-contact, portability and low-price. However, motion and noise artifacts bring additional uncertainty to the results of heart rate estimation. Based on the signal quality assessment method, we propose a new remote heart estimation algorithm by signal quality attention mechanism and long short-term memory (LSTM) networks. The model consists of three parts: firstly, an LSTM network is used to estimate the heart rate sampling point by sampling point; secondly, a similar LSTM network predicts the signal quality; finally, an attention-based model uses the heart rates and quality scores predicted above to calculate the average heart rate of a period of time. The model allocates higher weights to the reliable heart rates estimated from high-quality signals, meanwhile, ignores unreliable results estimated from low-quality signals. Experiments show that LSTM with attention mechanism accurately estimates heart rate from corruption rPPG signal and it performs well on cross-subject tasks and cross-dataset tasks. The results also demonstrate that the scores predicted by the signal quality model is valuable to extract reliable heart rate.",
    "code_link": ""
  },
  "cvpr2022_cvpm_straindetectionbasedonbreathandmotionfeaturesobtainedbyaforcesensorforsmarttoiletsystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Strain Detection Based on Breath and Motion Features Obtained by a Force Sensor for Smart Toilet Systems",
    "authors": [
      "Rina Akaho",
      "Mototaka Yoshioka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Akaho_Strain_Detection_Based_on_Breath_and_Motion_Features_Obtained_by_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Akaho_Strain_Detection_Based_on_Breath_and_Motion_Features_Obtained_by_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Aging people may be prone to accidents in bathrooms and toilets. The detection of strain motion for a smart toilet application has not been studied sufficiently. In this paper, we propose a method for strain detection from a force sensor placed on a toilet seat for a smart toilet healthcare application. The method first extracts breath and motion features that are assumed to be key components for the strain detection. The method then learns the discriminator model based on the random forest classifier using the aforementioned features. Finally, the method recognizes actions in the toilet room. There were five detection actions: seating, taking up toilet paper, wiping bottom, which are normal actions when sitting on a toilet seat, and strain actions (strong and weak). An experiment with 19 subjects was also conducted. Compared with a microwave sensor-based recognition, which is a conventional method (accuracy = 61.6%), our method was able to recognize the actions with high accuracy of 80.2% (significant test: T = 12.7, P < 0.01) in the experiment. Our strain detection method has the potential to be used as a smart toilet system to prevent blood pressure elevation and collapse caused by strain in the future.",
    "code_link": ""
  },
  "cvpr2022_cvpm_remotepulseestimationinthepresenceoffacemasks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Remote Pulse Estimation in the Presence of Face Masks",
    "authors": [
      "Jeremy Speth",
      "Nathan Vance",
      "Patrick Flynn",
      "Kevin Bowyer",
      "Adam Czajka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Speth_Remote_Pulse_Estimation_in_the_Presence_of_Face_Masks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Speth_Remote_Pulse_Estimation_in_the_Presence_of_Face_Masks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Remote photoplethysmography (rPPG), a family of techniques for monitoring blood volume changes, may be especially useful for contactless health monitoring via face videos from consumer-grade cameras. The COVID-19 pandemic caused widespread use of protective face masks, which results in a domain shift from the typical region of interest. In this paper we show that augmenting unmasked face videos by adding patterned synthetic face masks forces the deep learning-based rPPG model to attend to the periocular and forehead regions, improving performance and closing the gap between masked and unmasked pulse estimation. This paper offers several novel contributions: (a) deep learning-based method designed for remote photoplethysmography in a presence of face masks, (b) new dataset acquired from 54 masked subjects with recordings of their face and ground-truth pulse waveforms, (c) data augmentation method to add a synthetic mask to a face video, and (d) evaluations of handcrafted algorithms and two 3D convolutional neural network-based architectures trained on videos of unmasked faces and with masks synthetically added.",
    "code_link": ""
  },
  "cvpr2022_cvpm_remoteestimationofcontinuousbloodpressurebyaconvolutionalneuralnetworktrainedonspatialpatternsoffacialpulsewaves": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Remote Estimation of Continuous Blood Pressure by a Convolutional Neural Network Trained on Spatial Patterns of Facial Pulse Waves",
    "authors": [
      "Kaito Iuchi",
      "Ryogo Miyazaki",
      "George C. Cardoso",
      "Keiko Ogawa-Ochiai",
      "Norimichi Tsumura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Iuchi_Remote_Estimation_of_Continuous_Blood_Pressure_by_a_Convolutional_Neural_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Iuchi_Remote_Estimation_of_Continuous_Blood_Pressure_by_a_Convolutional_Neural_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a remote method to estimate continuous blood pressure based on spatial information of a pulse wave at a single point in time. By setting regions of interest to cover a face in a mutually exclusive and collectively exhaustive manner, RGB facial video is converted into a spatial pulse wave signal. The spatial pulse wave signal is converted into spatial signals of contours of each segmented pulse beat and relationships of each segmented pulse beat. The spatial signal is represented as a time-continuous value based on a representation of a pulse contour in a time axis and a phase axis and an interpolation along with the time axis. A relationship between the spatial signals and blood pressure is modeled by a convolutional neural network. A dataset was built to demonstrate the effectiveness of the proposed method. The dataset consists of continuous blood pressure and facial RGB videos of ten healthy volunteers. Comparison of conventional methods with the proposed method shows superior accuracy for the latter. The results show an adequate estimation of the performance of the proposed method, when compared to the ground truth, in both the correlation coefficient (0.85) and mean absolute error (5.4 mmHg).",
    "code_link": ""
  },
  "cvpr2022_cvpm_federatedremotephysiologicalmeasurementwithimperfectdata": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Federated Remote Physiological Measurement With Imperfect Data",
    "authors": [
      "Xin Liu",
      "Mingchuan Zhang",
      "Ziheng Jiang",
      "Shwetak Patel",
      "Daniel McDuff"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Liu_Federated_Remote_Physiological_Measurement_With_Imperfect_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Liu_Federated_Remote_Physiological_Measurement_With_Imperfect_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The growing need for technology that supports remote healthcare is being acutely highlighted by an aging population and the COVID-19 pandemic. In health-related machine learning applications the ability to learn predictive models without data leaving a private device is attractive, especially when these data might contain features (e.g., photographs or videos of the body) that make identifying a subject trivial and/or the training data volume is large (e.g., uncompressed video). Camera-based remote physiological sensing facilitates scalable and low-cost measurement, but is a prime example of a task that involves analysing high bit-rate videos containing identifiable images and sensitive health information. Federated learning enables privacy-preserving decentralized training which has several properties beneficial for camera-based sensing. We develop the first mobile federated learning camera-based sensing system and show that it can perform competitively with traditional state-of-the-art supervised approaches. However, in the presence of corrupted data (e.g., video or label noise) from a few devices the performance of weight averaging quickly degrades. To address this, we leverage knowledge about the expected noise profile within the video to intelligently adjust how the model weights are averaged on the server. Our results show that this significantly improves upon the robustness of models even when the signal-to-noise ratio is low.",
    "code_link": ""
  },
  "cvpr2022_cvpm_gatedrecurrentunit-basedrnnforremotephotoplethysmographysignalsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Gated Recurrent Unit-Based RNN for Remote Photoplethysmography Signal Segmentation",
    "authors": [
      "Rita Meziati Sabour",
      "Yannick Benezeth"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Sabour_Gated_Recurrent_Unit-Based_RNN_for_Remote_Photoplethysmography_Signal_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Sabour_Gated_Recurrent_Unit-Based_RNN_for_Remote_Photoplethysmography_Signal_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Remote Photoplethysmography (rPPG) enables quantifying blood volume variations in the skin tissues from an input video recording, using a regular RGB camera. Obtained pulse signals often contain noisy portions due to motion, leading researchers to put aside a great number of rPPG signals in their studies. In this paper, an approach using a Gated Recurrent Unit-based neural network model in order to identify reliable portions in rPPG signals is proposed. This is done by classifying rPPG signal samples into reliable and unreliable samples. For this purpose, rPPG and electrocardiography signals (ECG) were collected from 11 participants, rPPG signal samples were labeled (ECG was used as ground truth), and data were augmented to reach a total number of 11000 1-minute-long rPPG signals. We developed a model composed of a unidimensional CNN and a Bidirectional GRU (1D-CNN+B-GRU) for this study, and obtained an accuracy rate of 85.88%.",
    "code_link": ""
  },
  "cvpr2022_cvpm_regressionorclassification?reflectiononbppredictionfromppgdatausingdeepneuralnetworksinthescopeofpracticalapplications": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Regression or Classification? Reflection on BP Prediction From PPG Data Using Deep Neural Networks in the Scope of Practical Applications",
    "authors": [
      "Fabian Schrumpf",
      "Paul Rudi Serdack",
      "Mirco Fuchs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Schrumpf_Regression_or_Classification_Reflection_on_BP_Prediction_From_PPG_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Schrumpf_Regression_or_Classification_Reflection_on_BP_Prediction_From_PPG_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Photoplethysmographic (PPG) signals offer diagnostic potential beyond heart rate analysis or blood oxygen level monitoring. In the recent past, research focused extensively on non-invasive PPG-based approaches to blood pressure (BP) estimation. These approaches can be subdivided into regression and classification methods. The latter assign PPG signals to predefined BP intervals that represent clinically relevant ranges. The former predict systolic (SBP) and diastolic (DBP) BP as continuous variables and are of particular interest to the research community. However, the reported accuracies of BP regression methods vary widely among publications with some authors even questioning the feasibility of PPG-based BP regression altogether. In our work, we compare BP regression and classification approaches. We argue that BP classification might provide diagnostic value that is equivalent to regression in many clinically relevant scenarios while being similar or even superior in terms of performance. We compare several established neural architectures using publicly available PPG data for SBP regression and classification with and without personalization using subject-specific data. We found that classification and regression models perform similar before personalization. However, after personalization, the accuracy of classification based methods outperformed regression approaches. We conclude that BP classification might be preferable over BP regression in certain scenarios where a coarser segmentation of the BP range is sufficient.",
    "code_link": ""
  },
  "cvpr2022_cvpm_predictingmind-wanderingwithfacialvideosinonlinelectures": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Predicting Mind-Wandering With Facial Videos in Online Lectures",
    "authors": [
      "Taeckyung Lee",
      "Dain Kim",
      "Sooyoung Park",
      "Dongwhi Kim",
      "Sung-Ju Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Lee_Predicting_Mind-Wandering_With_Facial_Videos_in_Online_Lectures_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Lee_Predicting_Mind-Wandering_With_Facial_Videos_in_Online_Lectures_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The importance of online education has been brought to the forefront due to COVID. Understanding students' attentional states are crucial for lecturers, but this could be more difficult in online settings than in physical classrooms. Existing methods that gauge online students' attention status typically require specialized sensors such as eye-trackers and thus are not easily deployable to every student in real-world settings. To tackle this problem, we utilize facial video from student webcams for attention state prediction in online lectures. We conduct an experiment in the wild with 37 participants, resulting in a dataset consisting of 15 hours of lecture-taking students' facial recordings with corresponding 1,100 attentional state probings. We present PAFE (Predicting Attention with Facial Expression), a facial-video-based framework for attentional state prediction that focuses on the vision-based representation of traditional physiological mind-wandering features related to partial drowsiness, emotion, and gaze. Our model only requires a single camera and outperforms gaze-only baselines.",
    "code_link": ""
  },
  "cvpr2022_cvpm_humanstoolsclassificationforgastrointestinalhealthbasedonanimprovedresnet18modelwithdualattentionmechanism": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Human Stools Classification for Gastrointestinal Health Based on an Improved ResNet18 Model With Dual Attention Mechanism",
    "authors": [
      "Jing Zhang",
      "Tao Wen",
      "Tao He",
      "Xiangzhou Wang",
      "Ruqian Hao",
      "Juanxiu Liu",
      "Xiaohui Du",
      "Lin Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Zhang_Human_Stools_Classification_for_Gastrointestinal_Health_Based_on_an_Improved_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Zhang_Human_Stools_Classification_for_Gastrointestinal_Health_Based_on_an_Improved_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The human stools are directly related to the health of human gastrointestinal function. Preliminary classification of the shape and colour of stools can diagnose the health status of peoples, therefore automatic recognition of stools is the current development direction of smart toilets. Due to the difficulty in identification with complex image content, this paper proposed a convolutional neural network called StoolNet to solve the current challenges. The architecture of StoolNet is based on ResNet and contains two output branches which perform colour and shape recognition, respectively. To improve the recognition performance, the dual attention mechanism was introduced into feature extraction stage. The accuracy value of our proposed model could achieve 99.7% and 94.4% for color and shape recognition on our test set, respectively. Experimental results show that, compared with other stool classification algorithms, our method possesses better capability of category discrimination on real dataset.",
    "code_link": ""
  },
  "cvpr2022_cvpm_multimodaltransformerfornursingactivityrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Multimodal Transformer for Nursing Activity Recognition",
    "authors": [
      "Momal Ijaz",
      "Renato Diaz",
      "Chen Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Ijaz_Multimodal_Transformer_for_Nursing_Activity_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Ijaz_Multimodal_Transformer_for_Nursing_Activity_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In an aging population, elderly patient safety is a primary concern at hospitals and nursing homes, which demands for increased nurse care. By performing nurse activity recognition, we can not only make sure that all patients get an equal desired care, but it can also free nurses from manual documentation of activities they perform, leading to a fair and safe place of care for the elderly. In this work, we present a multimodal transformer-based network, which extracts features from skeletal joints and acceleration data, and fuses them to perform nurse activity recognition. Our method achieves state-of-the-art performance of 81.8% accuracy on the benchmark dataset available for nurse activity recognition from the Nurse Care Activity Recognition Challenge. We perform ablation studies to show that our fusion model is better than single modality transformer variants (using only acceleration or skeleton joints data). Our solution also outperforms state-of-the-art ST-GCN, GRU and other classical hand-crafted-feature-based classifier solutions by a margin of 1.6%, on the NCRC dataset. Code is available at https://github.com/ Momilijaz96/MMT_for_NCRC.",
    "code_link": ""
  },
  "cvpr2022_cvpm_deeplearningclassifierforadvancingvideomonitoringofatrialfibrillation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Deep Learning Classifier for Advancing Video Monitoring of Atrial Fibrillation",
    "authors": [
      "Kamil Bukum",
      "Celal Savur",
      "Gill R. Tsouri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Bukum_Deep_Learning_Classifier_for_Advancing_Video_Monitoring_of_Atrial_Fibrillation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Bukum_Deep_Learning_Classifier_for_Advancing_Video_Monitoring_of_Atrial_Fibrillation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Video-based non-contact monitoring of cardiac conditions offers an attractive alternative to contact-based monitoring using sensors attached to the skin. Specifically, video monitoring can significantly improve the monitoring of atrial fibrillation; a prevalent and growing cardiac disease affecting millions around the world. We propose and investigate the performance of a deep learning classifier for the detection of atrial fibrillation. We compare the performance of the proposed classifier with a benchmark of five existing classifiers based on traditional signal processing and machine learning. In addition, we compare performance across various sensing modalities, including a high-end camera, a webcam, an earlobe oximeter, and an electrocardiogram holter. To this end, we conduct a clinical study with 55 atrial fibrillation patients in a hospital setting. Results show that the proposed classifier outperforms the benchmark, especially when using a low-cost webcam, and provides consistently accurate detection when applied to an electrocardiogram, a photo plethysmography sensor, and two video camera sensors, thereby placing video monitoring on par with its contract-based counterparts.",
    "code_link": ""
  },
  "cvpr2022_cvpm_shoulditakeawalk?estimatingenergyexpenditurefromvideodata": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Should I Take a Walk? Estimating Energy Expenditure From Video Data",
    "authors": [
      "Kunyu Peng",
      "Alina Roitberg",
      "Kailun Yang",
      "Jiaming Zhang",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Peng_Should_I_Take_a_Walk_Estimating_Energy_Expenditure_From_Video_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Peng_Should_I_Take_a_Walk_Estimating_Energy_Expenditure_From_Video_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We explore the problem of automatically inferring the amount of kilocalories used by human during physical activity from his/her video observation. To study this under researched task, we introduce Vid2Burn - an omni-source benchmark for estimating caloric expenditure from video data featuring both, high- and low-intensity activities for which we derive energy expenditure annotations based on models established in medical literature. In practice, a training set would only cover a certain amount of activity types, and it is important to validate, if the model indeed captures the essence of energy expenditure, (e.g., how many and which muscles are involved and how intense they work) instead of memorizing fixed values of specific activity categories seen during training. Ideally, the models should look beyond such category-specific biases and regress the caloric cost in videos depicting activity categories not explicitly present during training. With this property in mind, Vid2Burn is accompanied with a cross category benchmark, where the task is to regress caloric expenditure for types of physical activities not present during training. An extensive evaluation of state-of-the-art approaches for video recognition modified for the energy expenditure estimation task demonstrates the difficulty of this problem, especially for new activity types at test time, marking a new research direction. We will make the dataset, code and models publicly available to the community.",
    "code_link": ""
  },
  "cvpr2022_cvpm_efficientremotephotoplethysmographywithtemporalderivativemodulesandtime-shiftinvariantloss": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Efficient Remote Photoplethysmography With Temporal Derivative Modules and Time-Shift Invariant Loss",
    "authors": [
      "Joaquim Comas",
      "Adri\u00e0 Ruiz",
      "Federico Sukno"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Comas_Efficient_Remote_Photoplethysmography_With_Temporal_Derivative_Modules_and_Time-Shift_Invariant_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Comas_Efficient_Remote_Photoplethysmography_With_Temporal_Derivative_Modules_and_Time-Shift_Invariant_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a lightweight neural model for remote heart rate estimation focused on the efficient spatio-temporal learning of facial photoplethysmography (PPG) based on i) modelling of PPG dynamics by combinations of multiple convolutional derivatives, and ii) increased flexibility of the model to learn possible offsets between the facial video PPG and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module (TDM) constructed by the incremental aggregation of multiple convolutional derivatives, emulating a Taylor series expansion up to the desired order. Robustness to ground truth offsets is handled by the introduction of TALOS (Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based models. We verify the effectiveness of our model by reporting accuracy and efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to existing models, our approach shows competitive heart rate estimation accuracy with a much lower number of parameters and lower computational cost.",
    "code_link": "https://github.com/ZitongYu/PhysNet"
  },
  "cvpr2022_cvpm_contactlessbloodpressuremeasurementviaremotephotoplethysmographywithsyntheticdatagenerationusinggenerativeadversarialnetwork": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Contactless Blood Pressure Measurement via Remote Photoplethysmography With Synthetic Data Generation Using Generative Adversarial Network",
    "authors": [
      "Bing-Fei Wu",
      "Li-Wen Chiu",
      "Yi-Chiao Wu",
      "Chun-Chih Lai",
      "Pao-Hsien Chu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Wu_Contactless_Blood_Pressure_Measurement_via_Remote_Photoplethysmography_With_Synthetic_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Wu_Contactless_Blood_Pressure_Measurement_via_Remote_Photoplethysmography_With_Synthetic_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deriving blood pressure in a non-invasive way via photoplethysmography (PPG) signals has become a familiar topic. With the knowledge of the relation between PPG and blood pressure, we expect to further make the measurement contactless for convenience reasons. An alternative signal source is remote photoplethysmography (rPPG) signals. There are mainly two kinds of approaches for exploiting blood pressure through PPG signals, one is by calculating the pulse transit time of the arterial pulse wave at two consecutive sites and the other is based on waveform feature analysis from a single signal. The calibration procedure is necessary for the former way, which leads to some limitations in general use. On the other hand, the properties of the rPPG waveform are far from PPG signals. Hence, the known waveform features in PPG signals are hard to be leveraged in the case of rPPG signals. Recently, convolutional neural networks are also applied for solving this problem. However, the lack of data is an obstacle to the training procedure and evaluation. In this study, a multi-channel rPPG-based blood pressure estimator is proposed. To ease the data scarcity issue, the generative adversarial network is adopted to augment synthetic waveform data. Besides, as we know that some physiological states like age and BMI are dominant factors in blood pressure. InfoGAN is chosen in this work to generate the synthetic data with the blood pressure value fluctuating correspondingly to the controlled age and BMI combination. The proposed model outperforms the state-of-the-art methods on MIMIC III and Cuffless datasets. With the synthetic data generation, the mean absolute error (MAE) is reduced to 6.72 and 5.95 mmHg in MAP and DBP respectively. The standard deviations of the MAEs are also reduced. In the rPPG case, the MAE of SBP is 9.13 and 8.76 mmHg for DBP.",
    "code_link": ""
  },
  "cvpr2022_cvpm_pruningrppgnetworkstowardsmalldensenetworkwithlimitednumberoftrainingsamples": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Physiological Measurement",
    "title": "Pruning rPPG Networks: Toward Small Dense Network With Limited Number of Training Samples",
    "authors": [
      "Changchen Zhao",
      "Pengcheng Cao",
      "Shoushuai Xu",
      "Zhengguo Li",
      "Yuanjing Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/html/Zhao_Pruning_rPPG_Networks_Toward_Small_Dense_Network_With_Limited_Number_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVPM/papers/Zhao_Pruning_rPPG_Networks_Toward_Small_Dense_Network_With_Limited_Number_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural network pruning reduces network complexity and storage by removing unimportant connections in the network, enabling network miniaturization, fast training and inference, easy deployment to portable devices, etc. The emerging lottery ticket hypotheses and sparse initialization technique have shed new lights on the pruning research. However, few research focuses on the pruning of the networks for remote photoplethysmography (rPPG) pulse signal extraction. Opposite to the existing pruning researches that prune large network, rPPG networks are relatively small. It is interesting to see how it behaves when the pruning is applied. In this paper, we investigate the behavior of common pruning techniques when applied to an existing rPPG network. Experiments on PURE dataset show that the pruning rate decay is beneficial to the performance improvement, whereas the connection regeneration has a detrimental effect. Given the same final sparsity, dense initialization generally performs better than sparse initialization. The network seems insensitive to initial sparsity. The combination s_i=1.0, s_f=0.1, with decay, and without regeneration is the best trade-off between SNR and FLOPs, achieving average SNR 9.78 dB, increased by 0.48 dB in comparison with the original PhysNet.",
    "code_link": ""
  },
  "cvpr2022_cvfad_artisticstylenovelviewsynthesisbasedonasingleimage": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Artistic Style Novel View Synthesis Based on a Single Image",
    "authors": [
      "Kuan-Wei Tseng",
      "Yao-Chih Lee",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Tseng_Artistic_Style_Novel_View_Synthesis_Based_on_a_Single_Image_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Tseng_Artistic_Style_Novel_View_Synthesis_Based_on_a_Single_Image_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent progress in 3D display technologies has raised the demand in stylized 3D digital contents. Previous approaches either perform style transfer on stereoscopic image pairs or reconstruct 3D environment with multiple view images. In this paper, we propose a novel view stylization framework that can convert a single 2D image into multiple stylized views. It is a two-stage solution that contains view synthesis and neural style transfer. We estimate dense optical flow between source and novel views so that the style transfer model can produce consistent results. Experimental results show that our method significantly improves the consistency among views compared to the baseline method.",
    "code_link": ""
  },
  "cvpr2022_cvfad_dresscodehigh-resolutionmulti-categoryvirtualtry-on": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Dress Code: High-Resolution Multi-Category Virtual Try-On",
    "authors": [
      "Davide Morelli",
      "Matteo Fincato",
      "Marcella Cornia",
      "Federico Landi",
      "Fabio Cesari",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Morelli_Dress_Code_High-Resolution_Multi-Category_Virtual_Try-On_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Morelli_Dress_Code_High-Resolution_Multi-Category_Virtual_Try-On_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image-based virtual try-on strives to transfer the appearance of a clothing item onto the image of a target person. Existing literature focuses mainly on upper-body clothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body items. This shortcoming arises from a main factor: current publicly available datasets for image-based virtual try-on do not account for this variety, thus limiting progress in the field. In this research activity, we introduce Dress Code, a novel dataset which contains images of multi-category clothes. Dress Code is more than 3x larger than publicly available datasets for image-based virtual try-on and features high-resolution paired images (1024x768) with front-view, full-body reference models. To generate HD try-on images with high visual quality and rich in details, we propose to learn fine-grained discriminating features. Specifically, we leverage a semantic-aware discriminator that makes predictions at pixel-level instead of image- or patch-level. The Dress Code dataset is publicly available at https://github.com/aimagelab/dress-code.",
    "code_link": "https://github.com/aimagelab/dress-code"
  },
  "cvpr2022_cvfad_neuralimagerecolorizationforcreativedomains": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Neural Image Recolorization for Creative Domains",
    "authors": [
      "Boyi Li",
      "Serge Belongie",
      "Ser-nam Lim",
      "Abe Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Li_Neural_Image_Recolorization_for_Creative_Domains_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Li_Neural_Image_Recolorization_for_Creative_Domains_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a self-supervised approach to recolorization of images from design-oriented domains. Our approach can recolor images based on image exemplars or target color palettes provided by a user. In contrast with previous approaches, our method can reproduce color palettes with luminance distributions that differ significantly from input, and our method is the first palette-based approach to distinguish between recolorings that match reflectance and those that match illumination, making it particularly well-suited to visualizing different aesthetic decisions in design applications. The key to our approach is first to learn latent representations for texture and color in a setting where self-supervision is especially straightforward, and then to learn a mapping to our color representation from input color palettes and scene illumination, which offers a more intuitive space for controlling and exploring recolorization.",
    "code_link": ""
  },
  "cvpr2022_cvfad_dual-branchcollaborativetransformerforvirtualtry-on": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Dual-Branch Collaborative Transformer for Virtual Try-On",
    "authors": [
      "Emanuele Fenocchi",
      "Davide Morelli",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Fabio Cesari",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Fenocchi_Dual-Branch_Collaborative_Transformer_for_Virtual_Try-On_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Fenocchi_Dual-Branch_Collaborative_Transformer_for_Virtual_Try-On_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image-based virtual try-on has recently gained a lot of attention in both the scientific and fashion industry communities due to its challenging setting and practical real-world applications. While pure convolutional approaches have been explored to solve the task, Transformer-based architectures have not received significant attention yet. Following the intuition that self- and cross-attention operators can deal with long-range dependencies and hence improve the generation, in this paper we extend a Transformer-based virtual try-on model by adding a dual-branch collaborative module that can exploit cross-modal information at generation time. We perform experiments on the VITON dataset, which is the standard benchmark for the task, and on a recently collected virtual try-on dataset with multi-category clothing, Dress Code. Experimental results demonstrate the effectiveness of our solution over previous methods and show that Transformer-based architectures can be a viable alternative for virtual try-on.",
    "code_link": ""
  },
  "cvpr2022_cvfad_towardsdetailedcharacteristic-preservingvirtualtry-on": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Towards Detailed Characteristic-Preserving Virtual Try-On",
    "authors": [
      "Sangho Lee",
      "Seoyoung Lee",
      "Joonseok Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Lee_Towards_Detailed_Characteristic-Preserving_Virtual_Try-On_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Lee_Towards_Detailed_Characteristic-Preserving_Virtual_Try-On_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "While virtual try-on has rapidly progressed recently, existing virtual try-on methods still struggle to faithfully represent various details of the clothes when worn. In this paper, we propose a simple yet effective method to better preserve details of the clothing and person by introducing an additional fitting step after geometric warping. This minimal modification enables disentangling representations of the clothing from the wearer, hence we are able to preserve the wearer-agnostic structure and details of the clothing, to fit a garment naturally to a variety of poses and body shapes. Moreover, we propose a novel evaluation framework applicable to any metric, to better reflect the semantics of clothes fitting. From extensive experiments, we empirically verify that the proposed method not only learns to disentangle clothing from the wearer, but also preserves details of the clothing on the try-on results.",
    "code_link": ""
  },
  "cvpr2022_cvfad_outfittransformeroutfitrepresentationsforfashionrecommendation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "OutfitTransformer: Outfit Representations for Fashion Recommendation",
    "authors": [
      "Rohan Sarkar",
      "Navaneeth Bodla",
      "Mariya Vasileva",
      "Yen-Liang Lin",
      "Anurag Beniwal",
      "Alan Lu",
      "Gerard Medioni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Sarkar_OutfitTransformer_Outfit_Representations_for_Fashion_Recommendation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Sarkar_OutfitTransformer_Outfit_Representations_for_Fashion_Recommendation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Predicting outfit compatibility and retrieving complementary items are critical components for a fashion recommendation system. We present a scalable framework, OutfitTransformer, that learns compatibility of the entire outfit and supports large-scale complementary item retrieval. We model outfits as an unordered set of items and leverage self-attention mechanism to learn the relationships between items. We train the framework using a proposed set-wise outfit ranking loss to generate a target item embedding given an outfit, and a target item specification. The generated target item embedding is then used to retrieve compatible items that match the outfit. Experimental results demonstrate that our approach outperforms state-of-the-art methods on compatibility prediction, fill-in-the-blank, and complementary item retrieval tasks.",
    "code_link": "https://github.com/nmslib/hnswlib"
  },
  "cvpr2022_cvfad_datrnetdisentanglingfashionattributeembeddingforsubstituteitemretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "DAtRNet: Disentangling Fashion Attribute Embedding for Substitute Item Retrieval",
    "authors": [
      "Gaurab Bhattacharya",
      "Nikhil Kilari",
      "Jayavardhana Gubbi",
      "Bagya Lakshmi V.",
      "Arpan Pal",
      "Balamuralidhar P."
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Bhattacharya_DAtRNet_Disentangling_Fashion_Attribute_Embedding_for_Substitute_Item_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Bhattacharya_DAtRNet_Disentangling_Fashion_Attribute_Embedding_for_Substitute_Item_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Interactive substitute recommendation for fashion products improves the online retail customer experience. Traditional fashion search platforms incorporate product metadata between the query products and the products to be retrieved. In this paper, we propose DAtRNet, an attribute representation network to disentangle the features in the query product. It is used to recommend attribute-aware substitute items based on the conditional similarity of the retrieved products. The proposed architecture relies on attribute-level similarity providing a fine-grained recommendation. In addition, a concurrent axial attention mechanism is proposed that generates global information embedding and adaptively re-calibrates the soft attention masks. Overall, the end-to-end framework enables the system to disentangle the attribute features and independently deals with them to enhance its flexibility towards one or multiple attributes. The proposed method outperforms the state-of-the-art by a significant margin.",
    "code_link": ""
  },
  "cvpr2022_cvfad_uigrunifiedinteractivegarmentretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "UIGR: Unified Interactive Garment Retrieval",
    "authors": [
      "Xiao Han",
      "Sen He",
      "Li Zhang",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Han_UIGR_Unified_Interactive_Garment_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Han_UIGR_Unified_Interactive_Garment_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Interactive garment retrieval (IGR) aims to retrieve a target garment image based on a reference garment image along with user feedback on what to change on the reference garment. Two IGR tasks have been studied extensively: text-guided garment retrieval (TGR) and visually compatible garment retrieval (VCR). The user feedback for the former indicates what semantic attributes to change with the garment category preserved, while the category is the only thing to be changed explicitly for the latter, with an implicit requirement on style preservation. Despite the similarity between these two tasks and the practical need for an efficient system tackling both, they have never been unified and modeled jointly. In this paper, we propose a Unified Interactive Garment Retrieval (UIGR) framework to unify TGR and VCR. To this end, we first contribute a large-scale benchmark suited for both problems. We further propose a strong baseline architecture to integrate TGR and VCR in one model. Extensive experiments suggest that unifying two tasks in one framework is not only more efficient by requiring a single model only, it also leads to better performance.",
    "code_link": ""
  },
  "cvpr2022_cvfad_wearableimagenetsynthesizingtileabletexturesviadatasetdistillation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Wearable ImageNet: Synthesizing Tileable Textures via Dataset Distillation",
    "authors": [
      "George Cazenavette",
      "Tongzhou Wang",
      "Antonio Torralba",
      "Alexei A. Efros",
      "Jun-Yan Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Cazenavette_Wearable_ImageNet_Synthesizing_Tileable_Textures_via_Dataset_Distillation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Cazenavette_Wearable_ImageNet_Synthesizing_Tileable_Textures_via_Dataset_Distillation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent methods for Dataset Distillation are able to take in a large set of images of a specific class (e.g., from ImageNet) and synthesize a single image, such that a classifier trained on that image could perform similarly to one trained on the original dataset. It was noticed that the resulting \"distilled images\" are often quite visually pleasing. In this paper, we describe a simple method for generating tileable distilled textures by sampling random crops from a toroidal canvas of synthetic pixels while enforcing that all such crops serve as effective distilled training data. Such distilled textures not only summarize a given image category in a visually interesting way, but also allow for generation of infinite texture patterns suitable for printing on fabric, clothing, etc. This paper might be just the first step in making the ImageNet dataset into a fashion statement.",
    "code_link": ""
  },
  "cvpr2022_cvfad_themulti-modaluniverseoffast-fashionthevisuelle2.0benchmark": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "The Multi-Modal Universe of Fast-Fashion: The Visuelle 2.0 Benchmark",
    "authors": [
      "Geri Skenderi",
      "Christian Joppi",
      "Matteo Denitto",
      "Berniero Scarpa",
      "Marco Cristani"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Skenderi_The_Multi-Modal_Universe_of_Fast-Fashion_The_Visuelle_2.0_Benchmark_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Skenderi_The_Multi-Modal_Universe_of_Fast-Fashion_The_Visuelle_2.0_Benchmark_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present Visuelle 2.0, the first dataset useful for facing diverse prediction problems that a fast-fashion company has to manage routinely. Furthermore, we demonstrate how the use of computer vision is substantial in this scenario. Visuelle 2.0 contains data for 6 seasons / 5355 clothing products of Nuna Lie, a famous Italian company with hundreds of shops located in different areas within the country. In particular, we focus on a specific prediction problem, namely short-observation new product sale forecasting (SO-fore). SO-fore assumes that the season has started and a set of new products is on the shelves of the different stores. The goal is to forecast the sales for a particular horizon, given a short, available past (few weeks), since no earlier statistics are available. To be successful, SO-fore approaches should capture this short past and exploit other modalities or exogenous data. To these aims, Visuelle 2.0 is equipped with disaggregated data at the item-shop level and multi-modal information for each clothing item, allowing computer vision approaches to come into play. The main message that we deliver is that the use of image data with deep networks boosts performances obtained when using only the time series in long-term forecasting scenarios, ameliorating the WAPE by 8.2% and the MAE by 7.7%. The dataset is available at: https://humaticslab.github.io/forecasting/visuelle.",
    "code_link": ""
  },
  "cvpr2022_cvfad_outfitganlearningcompatibleitemsforgenerativefashionoutfits": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "OutfitGAN: Learning Compatible Items for Generative Fashion Outfits",
    "authors": [
      "Maryam Moosaei",
      "Yusan Lin",
      "Ablaikhan Akhazhanov",
      "Huiyuan Chen",
      "Fei Wang",
      "Hao Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Moosaei_OutfitGAN_Learning_Compatible_Items_for_Generative_Fashion_Outfits_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Moosaei_OutfitGAN_Learning_Compatible_Items_for_Generative_Fashion_Outfits_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Fashion-on-demand is becoming an important concept for fashion industries. Many attempts have been made to leverage machine learning methods to generate fashion designs tailored to customers' tastes. However, how to assemble items together (e.g., compatibility) is crucial in designing high-quality outfits for synthesis images. Here we propose a fashion generation model, named OutfitGAN, which contains two core modules: a Generative Adversarial Network and a Compatibility Network. The generative module is able to generate new realistic high quality fashion items from a specific category, while the compatibility network ensures reasonable compatibility among all items. The experimental results show the superiority of our OutfitGAN.",
    "code_link": ""
  },
  "cvpr2022_cvfad_paintinstyleone-shotdiscoveryofinterpretabledirectionsbypainting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "PaintInStyle: One-Shot Discovery of Interpretable Directions by Painting",
    "authors": [
      "Berkay Doner",
      "Elif Sema Balcioglu",
      "Merve Rabia Barin",
      "Umut Kocasari",
      "Mert Tiftikci",
      "Pinar Yanardag"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Doner_PaintInStyle_One-Shot_Discovery_of_Interpretable_Directions_by_Painting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Doner_PaintInStyle_One-Shot_Discovery_of_Interpretable_Directions_by_Painting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The search for interpretable directions in latent spaces of pre-trained Generative Adversarial Networks (GANs) has become a topic of interest. These directions can be utilized to perform semantic manipulations on the GAN generated images. The discovery of such directions is performed either in a supervised way, which requires manual annotation or pre-trained classifiers, or in an unsupervised way, which requires the user to interpret what these directions represent. In this work, we propose a framework that finds a specific manipulation direction using only a single simple sketch drawn on an image. Our method finds directions consisting of channels in the style space of the StyleGAN2 architecture responsible for the desired edits and performs image manipulations comparable with state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2022_cvfad_gp22acarstylingdatasetforautomotivedesigners": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "GP22: A Car Styling Dataset for Automotive Designers",
    "authors": [
      "Gyunpyo Lee",
      "Taesu Kim",
      "Hyeon-Jeong Suk"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Lee_GP22_A_Car_Styling_Dataset_for_Automotive_Designers_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Lee_GP22_A_Car_Styling_Dataset_for_Automotive_Designers_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "An automated design data archiving could reduce the time wasted by designers from working creatively and effectively. Though many datasets on classifying, detecting, and instance segmenting on car exterior exist, these large datasets are not relevant for design practices as the primary purpose lies in autonomous driving or vehicle verification. Therefore, we release GP22, composed of car styling features defined by automotive designers. The dataset contains 1480 car side profile images from 37 brands and ten car segments. It also contains annotations of design features that follow the taxonomy of the car exterior design features defined in the eye of the automotive designer. We trained the baseline model using YOLO v5 as the design feature detection model with the dataset. The presented model resulted in an mAP score of 0.995 and a recall of 0.984. Furthermore, exploration of the model performance on sketches and rendering images of the car side profile implies the scalability of the dataset for design purposes.",
    "code_link": ""
  },
  "cvpr2022_cvfad_corecolorregressionformulticolorfashiongarments": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "CoRe: Color Regression for Multicolor Fashion Garments",
    "authors": [
      "Alexandre Ram\u00e9",
      "Arthur Douillard",
      "Charles Ollion"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Rame_CoRe_Color_Regression_for_Multicolor_Fashion_Garments_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Rame_CoRe_Color_Regression_for_Multicolor_Fashion_Garments_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Developing deep networks that analyze fashion garments has many real-world applications. Among all fashion attributes, color is one of the most important yet challenging to detect. Existing approaches are classification-based and thus cannot go beyond the list of discrete predefined color names. In this paper, we handle color detection as a regression problem to predict the exact RGB values. That's why in addition to a first color classifier, we include a second regression stage for refinement in our newly proposed architecture. This second step combines two attention models: the first depends on the type of clothing, the second depends on the color previously detected by the classifier. Our final prediction is the weighted spatial pooling over the image pixels RGB values, where the illumination has been corrected. This architecture is modular and easily expanded to detect the RGBs of all colors in a multicolor garment. In our experiments, we show the benefits of each component of our architecture.",
    "code_link": ""
  },
  "cvpr2022_cvfad_rankinstylearanking-basedapproachtofindinterpretabledirections": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Rank in Style: A Ranking-Based Approach To Find Interpretable Directions",
    "authors": [
      "Umut Kocasari",
      "Kerem Zaman",
      "Mert Tiftikci",
      "Enis Simsar",
      "Pinar Yanardag"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Kocasari_Rank_in_Style_A_Ranking-Based_Approach_To_Find_Interpretable_Directions_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVFAD/papers/Kocasari_Rank_in_Style_A_Ranking-Based_Approach_To_Find_Interpretable_Directions_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent work such as StyleCLIP aims to harness the power of CLIP embeddings for controlled manipulations. Although these models are capable of manipulating images based on a text prompt, the success of the manipulation often depends on careful selection of the appropriate text for the desired manipulation. This limitation makes it particularly difficult to perform text-based manipulations in domains where the user lacks expertise, such as fashion. To address this problem, we propose a method for automatically determining the most successful and relevant text-based edits using a pre-trained StyleGAN model. Our approach consists of a novel mechanism that uses CLIP to guide beam-search decoding, and a ranking method that identifies the most relevant and successful edits based on a list of keywords. We also demonstrate the capabilities of our framework in several domains, including fashion.",
    "code_link": ""
  },
  "cvpr2022_abaw_modellevelensembleforfacialactionunitrecognitionatthe3rdabawchallenge": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Model Level Ensemble for Facial Action Unit Recognition at the 3rd ABAW Challenge",
    "authors": [
      "Wenqiang Jiang",
      "Yannan Wu",
      "Fengsheng Qiao",
      "Liyu Meng",
      "Yuanyuan Deng",
      "Chuanhe Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Jiang_Model_Level_Ensemble_for_Facial_Action_Unit_Recognition_at_the_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Jiang_Model_Level_Ensemble_for_Facial_Action_Unit_Recognition_at_the_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we present our latest work on Action Unit Detection, which is a part of the Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition. Our proposed network is based on the IResnet100. First of all, We utilize feature pyramid networks (FPN) and single stage headless (SSH) to enlarge the receptive field and extract more facial texture features. Then we employ the ML-ROS data balancing and the BCE Loss plus Multi-label Loss to solve the multi-label imbalance problem. We also use three different models as the base model to fine-tune the Aff-Wild2 dataset. The pre-train backbones are the AU detection model, expression model and face recognition model. Finally, we adopt an ensemble methodology to get the final result. Our f1 score achieved 49.82 on the AU test set and ranked second in this challenge with a very small difference from the first team 49.89.",
    "code_link": ""
  },
  "cvpr2022_abaw_facialexpressionclassificationusingfusionofdeepneuralnetworkinvideo": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Facial Expression Classification Using Fusion of Deep Neural Network in Video",
    "authors": [
      "Kim Ngan Phan",
      "Hong-Hai Nguyen",
      "Van-Thong Huynh",
      "Soo-Hyung Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Phan_Facial_Expression_Classification_Using_Fusion_of_Deep_Neural_Network_in_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Phan_Facial_Expression_Classification_Using_Fusion_of_Deep_Neural_Network_in_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "For computers to recognize human emotions, expression classification is an equally important problem in the human-computer interaction area. In the 3rd Affective Behavior Analysis In-The-Wild competition, the task of expression classification includes eight classes with six basic expressions of human faces from videos. In this paper, we employ a transformer mechanism to encode the robust representation from the backbone. Fusion of the robust representations plays an important role in the expression classification task. Our approach achieves 30.35% and 28.60% for the F1 score on the validation set and the test set, respectively. This result shows the effectiveness of the proposed architecture based on the Aff-Wild2 dataset and our team archives 5th for the expression classification task in the 3rd Affective Behavior Analysis In-The-Wild competition.",
    "code_link": ""
  },
  "cvpr2022_abaw_continuousemotionrecognitionusingvisual-audio-linguisticinformationatechnicalreportforabaw3": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Continuous Emotion Recognition Using Visual-Audio-Linguistic Information: A Technical Report for ABAW3",
    "authors": [
      "Su Zhang",
      "Ruyi An",
      "Yi Ding",
      "Cuntai Guan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Zhang_Continuous_Emotion_Recognition_Using_Visual-Audio-Linguistic_Information_A_Technical_Report_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Zhang_Continuous_Emotion_Recognition_Using_Visual-Audio-Linguistic_Information_A_Technical_Report_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a cross-modal co-attention model for continuous emotion recognition using visual-audio-linguistic information. The model consists of four blocks. The visual, audio, and linguistic blocks are used to learn the spatial-temporal features of the multi-modal input. A co-attention block is designed to fuse the learned features with the multi-head co-attention mechanism. The visual encoding from the visual block is concatenated with the attention feature to emphasize the visual information. To make full use of the data and alleviate over-fitting, cross-validation is carried out on the training and validation set. The concordance correlation coefficient (CCC) centering is used to merge the results from each fold. The achieved CCC on the test set is 0.520 for valence and 0.602 for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.180 and 0.170 for valence and arousal, respectively. The code is available at https://github.com/sucv/ABAW3.",
    "code_link": ""
  },
  "cvpr2022_abaw_long-termactionforecastingusingmulti-headedattention-basedvariationalrecurrentneuralnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Long-Term Action Forecasting Using Multi-Headed Attention-Based Variational Recurrent Neural Networks",
    "authors": [
      "Siyuan Brandon Loh",
      "Debaditya Roy",
      "Basura Fernando"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Loh_Long-Term_Action_Forecasting_Using_Multi-Headed_Attention-Based_Variational_Recurrent_Neural_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Loh_Long-Term_Action_Forecasting_Using_Multi-Headed_Attention-Based_Variational_Recurrent_Neural_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Systems developed for predicting both the action and the amount of time someone might take to perform that action need to be aware of the inherent uncertainty in what humans do. Here, we present a novel hybrid generative model for action anticipation that attempts to capture the uncertainty in human actions. Our model uses a multi-headed attention-based variational generative model for action prediction (MAVAP), and Gaussian log-likelihood maximization to predict the corresponding action's duration. During training, we optimise three losses: a variational loss, a negative log-likelihood loss, and a discriminative cross-entropy loss. We evaluate our model on standard datasets (i.e., Breakfast and 50Salads) for action forecasting tasks, and demonstrate improvements over prior methods using both ground truth observations and predicted features from an action segmentation network (i.e., MS-TCN++). We also show that factorizing the latent space across multiple Gaussian heads predicts better plausible future action sequences compared to a single Gaussian.",
    "code_link": ""
  },
  "cvpr2022_abaw_classificationoffacialexpressionin-the-wildbasedonensembleofmulti-headcrossattentionnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Classification of Facial Expression In-the-Wild Based on Ensemble of Multi-Head Cross Attention Networks",
    "authors": [
      "Jae-Yeop Jeong",
      "Yeong-Gi Hong",
      "Daun Kim",
      "Jin-Woo Jeong",
      "Yuchul Jung",
      "Sang-Ho Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Jeong_Classification_of_Facial_Expression_In-the-Wild_Based_on_Ensemble_of_Multi-Head_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Jeong_Classification_of_Facial_Expression_In-the-Wild_Based_on_Ensemble_of_Multi-Head_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "How to build a system for robust classification and recognition of facial expressions has been one of the most important research issues for successful interactive computing applications. However, previous datasets and studies mainly focused on facial expression recognition in a controlled/lab setting, therefore, could hardly be generalized in a more practical and real-life environment. The Affective Behavior Analysis in-the-wild (ABAW) 2022 competition released a dataset consisting of various video clips of facial expressions in-the-wild. In this paper, we propose a method based on the ensemble of multi-head cross attention networks to address the facial expression classification task introduced in the ABAW 2022 competition. We built a uni-task approach for this task, achieving the average F1-score of 34.60 on the validation set and 33.77 on the test set, ranking second place on the final leaderboard.",
    "code_link": ""
  },
  "cvpr2022_abaw_time-continuousaudiovisualfusionwithrecurrencevsattentionforin-the-wildaffectrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Time-Continuous Audiovisual Fusion With Recurrence vs Attention for In-the-Wild Affect Recognition",
    "authors": [
      "Vincent Karas",
      "Mani Kumar Tellamekala",
      "Adria Mallol-Ragolta",
      "Michel Valstar",
      "Bj\u00f6rn W. Schuller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Karas_Time-Continuous_Audiovisual_Fusion_With_Recurrence_vs_Attention_for_In-the-Wild_Affect_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Karas_Time-Continuous_Audiovisual_Fusion_With_Recurrence_vs_Attention_for_In-the-Wild_Affect_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents our contribution to the 3rd Affective Behavior Analysis in-the-Wild (ABAW) challenge. Exploiting the complementarity among multimodal data streams is of vital importance to recognise dimensional affect from inthe-wild audiovisual data, as the contribution affect-wise of the involved modalities might change over time. Recurrence and attention are two of the most widely used modelling mechanisms in the literature for capturing the temporal dependencies of audiovisual data sequences. To clearly understand the performance differences between recurrent and attention models in audiovisual affect recognition, we present a comprehensive evaluation of fusion models based on LSTM-RNNs, self-attention, and cross-modal attention, trained for valence and arousal estimation. Particularly, we study the impact of some key design choices: the modelling complexity of CNN backbones that provide features to temporal models, with and without end-to-end learning. We train the audiovisual affect recognition models on the in-the-wild Aff-wild2 corpus by systematically tuning the hyper-parameters involved in the network architecture design and training optimisation. Our extensive evaluation of the audiovisual fusion models indicate that under various experimental settings, compared to RNNs, attention models may not necessarily be the optimal choice for time-continuous multimodal fusion for emotion recognition.",
    "code_link": ""
  },
  "cvpr2022_abaw_coarse-to-finecascadednetworkswithsmoothpredictingforvideofacialexpressionrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Coarse-To-Fine Cascaded Networks With Smooth Predicting for Video Facial Expression Recognition",
    "authors": [
      "Fanglei Xue",
      "Zichang Tan",
      "Yu Zhu",
      "Zhongsong Ma",
      "Guodong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Xue_Coarse-To-Fine_Cascaded_Networks_With_Smooth_Predicting_for_Video_Facial_Expression_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Xue_Coarse-To-Fine_Cascaded_Networks_With_Smooth_Predicting_for_Video_Facial_Expression_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Facial expression recognition plays an important role in human-computer interaction. In this paper, we propose the Coarse-to-Fine Cascaded network with Smooth Predicting (CFC-SP) to improve the performance of facial expression recognition. CFC-SP contains two core components, namely Coarse-to-Fine Cascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups several similar emotions to form a rough category, and then employs a network to conduct a coarse but accurate classification. Later, an additional network for these grouped emotions is further used to obtain fine-grained predictions. For SP, it improves the recognition capability of the model by capturing both universal and unique expression features. To be specific, the universal features denote the general characteristic of facial emotions within a period and the unique features denote the specific characteristic at this moment. Experiments on Aff-Wild2 show the effectiveness of the proposed CFSP. We achieved 3rd place in the Expression Classification Challenge of the 3rd Competition on Affective Behavior Analysis in-the-wild. The code will be released at https://github.com/BR-IDL/PaddleViT.",
    "code_link": "https://github.com/BR-IDL/PaddleViT"
  },
  "cvpr2022_abaw_anensembleapproachforfacialbehavioranalysisin-the-wildvideo": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "An Ensemble Approach for Facial Behavior Analysis In-the-Wild Video",
    "authors": [
      "Hong-Hai Nguyen",
      "Van-Thong Huynh",
      "Soo-Hyung Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Nguyen_An_Ensemble_Approach_for_Facial_Behavior_Analysis_In-the-Wild_Video_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Nguyen_An_Ensemble_Approach_for_Facial_Behavior_Analysis_In-the-Wild_Video_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human emotions recognization contributes to the development of human-computer interaction. The machines understanding human emotions in the real world will significantly contribute to life in the future. This paper introduces the 3rd Affective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. We focused on solving the problem of the Valence-Arousal (VA) estimation and Action Unit (AU) detection. For valence-arousal estimation, we conducted two stages: creating new features from multimodel and temporal learning to predict valence-arousal. First, we make new features; the Gated Recurrent Unit (GRU) and Transformer are combined using a Regular Networks (RegNet) feature, which is extracted from the image. The next step is the GRU combined with local attention to predict valence-arousal. The Concordance Correlation Coefficient (CCC) was used to evaluate the model. The result achieved 0.450 for valence and 0.445 for arousal on the test set, outperforming the baseline method with a corresponding CCC of 0.180 for valence and 0.170 for arousal. We also performed additional experiments on the action unit task with simple transformer blocks. We achieved a score of 49.04 on the test set in terms of F1 score, which outperforms the baseline method with a corresponding F1 score of 36.50. Our submission to ABAW3 2022 ranks 3rd for both tasks.",
    "code_link": ""
  },
  "cvpr2022_abaw_thebestofbothworldscombiningmodel-basedandnonparametricapproachesfor3dhumanbodyestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "The Best of Both Worlds: Combining Model-Based and Nonparametric Approaches for 3D Human Body Estimation",
    "authors": [
      "Zhe Wang",
      "Jimei Yang",
      "Charless Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Wang_The_Best_of_Both_Worlds_Combining_Model-Based_and_Nonparametric_Approaches_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Wang_The_Best_of_Both_Worlds_Combining_Model-Based_and_Nonparametric_Approaches_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nonparametric-based methods have recently shown promising results in reconstructing human bodies from monocular images while model-based methods can help correct these estimates and improve prediction. However, estimating model parameters from global image features may lead to noticeable misalignment between the estimated meshes and image evidence. To address this issue and leverage the best of both worlds, we propose a framework of three consecutive modules. A dense map prediction module explicitly establishes the dense UV correspondence between the image evidence and each part of the body model. The inverse kinematics module refines the key point prediction and generates a posed template mesh. Finally, a UV inpainting module relies on the corresponding feature, prediction, and the posed template, and completes the predictions of occluded body shape. Our framework leverages the best of non-parametric and model-based methods and is also robust to partial occlusion. Experiments demonstrate that our framework outperforms existing 3D human estimation methods on multiple public benchmarks.",
    "code_link": ""
  },
  "cvpr2022_abaw_anattention-basedmethodformulti-labelfacialactionunitdetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "An Attention-Based Method for Multi-Label Facial Action Unit Detection",
    "authors": [
      "Duy Le Hoai",
      "Eunchae Lim",
      "Eunbin Choi",
      "Sieun Kim",
      "Sudarshan Pant",
      "Guee-Sang Lee",
      "Soo-Huyng Kim",
      "Hyung-Jeong Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Le_Hoai_An_Attention-Based_Method_for_Multi-Label_Facial_Action_Unit_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Le_Hoai_An_Attention-Based_Method_for_Multi-Label_Facial_Action_Unit_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Facial Action Coding System is an approach for modeling the complexity of human emotional expression. Automatic action unit (AU) detection is a crucial research area in human-computer interaction. This paper describes our submission to the third Affective Behavior Analysis in-the-wild (ABAW) competition 2022. We proposed a method for detecting facial action units in the video. In the first stage, a lightweight CNN-based feature extractor is employed to extract the feature map from each video frame. Then, an attention module is applied to refine the attention map. The attention encoded vector is derived using a weighted sum of the feature map and the attention scores later. Finally, the sigmoid function is used at the output layer to make the prediction suitable for multi-label AUs detection. We achieved a macro F1 score of 0.48 on the validation set and 0.4206 on the test set compared to 0.39 and 0.3650 from the ABAW challenge baseline model.",
    "code_link": ""
  },
  "cvpr2022_abaw_crosstransferringactivityrecognitiontowordlevelsignlanguagedetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Cross Transferring Activity Recognition to Word Level Sign Language Detection",
    "authors": [
      "Srijith Radhakrishnan",
      "Nikhil C Mohan",
      "Manisimha Varma",
      "Jaithra Varma",
      "Smitha N Pai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Radhakrishnan_Cross_Transferring_Activity_Recognition_to_Word_Level_Sign_Language_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Radhakrishnan_Cross_Transferring_Activity_Recognition_to_Word_Level_Sign_Language_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The lack of large scale labelled datasets in word-level sign language recognition (WSLR) poses a challenge to detecting sign language from videos. Most WSLR approaches operate on datasets that do not model real-world settings very well, as they do not have a high degree of variability in terms of signers, background, lighting and inter signer variation. We chose the MS-ASL dataset to overcome these limitations as they model open-world settings very well. This paper benchmarks successful action recognition architectures on the MS-ASL dataset using transfer learning. We have achieved new state-of-the-art accuracy (92.35%) with an improvement of 7.03% over the previous state-of-the-art introduced by the MS-ASL paper. We have analyzed how action-recognition architectures fair in the task of WSLR, and we propose SlowFast 8x8 ResNet 101 as a robust and suitable architecture for the task of WSLR.",
    "code_link": ""
  },
  "cvpr2022_abaw_neuralannotneuralannotatorfor3dhumanmeshtrainingsets": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets",
    "authors": [
      "Gyeongsik Moon",
      "Hongsuk Choi",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Moon_NeuralAnnot_Neural_Annotator_for_3D_Human_Mesh_Training_Sets_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Moon_NeuralAnnot_Neural_Annotator_for_3D_Human_Mesh_Training_Sets_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Most 3D human mesh regressors are fully supervised with 3D pseudo-GT human model parameters and weakly supervised with GT 2D/3D joint coordinates as the 3D pseudo-GTs bring great performance gain. The 3D pseudo-GTs are obtained by annotators, systems that iteratively fit 3D human model parameters to GT 2D/3D joint coordinates of training sets in the pre-processing stage of the regressors. The fitted 3D parameters at the last fitting iteration become the 3D pseudo-GTs, used to fully supervise the regressors. Optimization-based annotators, such as SMPLify-X, have been widely used to obtain the 3D pseudo-GTs. However, they often produce wrong 3D pseudo-GTs as they fit the 3D parameters to GT of each sample independently. To overcome the limitation, we present NeuralAnnot, a neural network-based annotator. The main idea of NeuralAnnot is to employ a neural network-based regressor and dedicate it for the annotation. Assuming no 3D pseudo-GTs are available, NeuralAnnot is weakly supervised with GT 2D/3D joint coordinates of training sets. The testing results on the same training sets become 3D pseudo-GTs, used to fully supervise the regressors. We show that 3D pseudo-GTs of NeuralAnnot are highly beneficial to train the regressors. We made our 3D pseudo-GTs publicly available.",
    "code_link": ""
  },
  "cvpr2022_abaw_abawvalence-arousalestimation,expressionrecognition,actionunitdetection&multi-tasklearningchallenges": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges",
    "authors": [
      "Dimitrios Kollias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Kollias_ABAW_Valence-Arousal_Estimation_Expression_Recognition_Action_Unit_Detection__Multi-Task_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Kollias_ABAW_Valence-Arousal_Estimation_Expression_Recognition_Action_Unit_Detection__Multi-Task_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper describes the third Affective Behavior Analysis in-the-wild (ABAW) Competition, held in conjunction with IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. The 3rd ABAW Competition is a continuation of the Competitions held at ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017 Conferences, and aims at automatically analyzing affect. This year the Competition encompasses four Challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression Classification, iii) uni-task Action Unit Detection, and iv) Multi-Task-Learning. All the Challenges are based on a common benchmark database, Aff-Wild2, which is a large scale in-the-wild database and the first one to be annotated in terms of valence-arousal, expressions and action units. In this paper, we present the four Challenges, with the utilized Competition corpora, we outline the evaluation metrics and present both the baseline systems and the top performing teams' per Challenge. Finally we illustrate the obtained results of the baseline systems and of all participating teams. More information regarding the Competition and the leaderboard for each Challenge can be found in the competition's website: http://ibug.doc.ic.ac.uk/resources/cvpr-2022-3rd-abaw.",
    "code_link": ""
  },
  "cvpr2022_abaw_accurate3dhandposeestimationforwhole-body3dhumanmeshestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation",
    "authors": [
      "Gyeongsik Moon",
      "Hongsuk Choi",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Moon_Accurate_3D_Hand_Pose_Estimation_for_Whole-Body_3D_Human_Mesh_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Moon_Accurate_3D_Hand_Pose_Estimation_for_Whole-Body_3D_Human_Mesh_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Whole-body 3D human mesh estimation aims to reconstruct the 3D human body, hands, and face simultaneously. Although several methods have been proposed, accurate prediction of 3D hands, which consist of 3D wrist and fingers, still remains challenging due to two reasons. First, the human kinematic chain has not been carefully considered when predicting the 3D wrists. Second, previous works utilize body features for the 3D fingers, where the body feature barely contains finger information. To resolve the limitations, we present Hand4Whole, which has two strong points over previous works. First, we design Pose2Pose, a module that utilizes joint features for 3D joint rotations. Using Pose2Pose, Hand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints largely contribute to 3D wrist rotations in the human kinematic chain. Second, Hand4Whole discards the body feature when predicting 3D finger rotations. Our Hand4Whole is trained in an end-to-end manner and produces much better 3D hand results than previous whole-body 3D human mesh estimation methods. The codes are available here (https://github.com/mks0601/Hand4Whole_RELEASE).",
    "code_link": ""
  },
  "cvpr2022_abaw_video-basedframe-levelfacialanalysisofaffectivebehavioronmobiledevicesusingefficientnets": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Video-Based Frame-Level Facial Analysis of Affective Behavior on Mobile Devices Using EfficientNets",
    "authors": [
      "Andrey V. Savchenko"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Savchenko_Video-Based_Frame-Level_Facial_Analysis_of_Affective_Behavior_on_Mobile_Devices_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Savchenko_Video-Based_Frame-Level_Facial_Analysis_of_Affective_Behavior_on_Mobile_Devices_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we consider the problem of real-time video-based facial emotion analytics, namely, facial expression recognition, prediction of valence and arousal and detection of action unit points. We propose the novel frame-level emotion recognition algorithm by extracting facial features with the single EfficientNet model pre-trained on AffectNet. The predictions for sequential frames are smoothed using mean or median filters. It is demonstrated that our approach may be implemented even for video analytics on mobile devices. Experimental results for the large scale Aff-Wild2 database from the third Affective Behavior Analysis in-the-wild Competition demonstrate that our simple model is significantly better when compared to the VggFace baseline. In particular, our method is characterized by 0.1-0.5 higher performance measures for test sets in the uni-task Expression Classification, Valence-Arousal Estimation, Action Unit Detection and Multi-Task Learning. Our team took the 3rd place in the multi-task learning challenge and 4th places in Valence-Arousal and Expression challenges. Due to simplicity, the proposed approach may be considered as a new baseline for all four sub-challenges.",
    "code_link": ""
  },
  "cvpr2022_abaw_multi-tasklearningforhumanaffectpredictionwithauditory-visualsynchronizedrepresentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Multi-Task Learning for Human Affect Prediction With Auditory-Visual Synchronized Representation",
    "authors": [
      "Euiseok Jeong",
      "Geesung Oh",
      "Sejoon Lim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Jeong_Multi-Task_Learning_for_Human_Affect_Prediction_With_Auditory-Visual_Synchronized_Representation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Jeong_Multi-Task_Learning_for_Human_Affect_Prediction_With_Auditory-Visual_Synchronized_Representation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the development of the big data and deep learning technologies, research on predicting human affects in the wild using deep neural networks is being actively conducted. Many researchers use image and audio together to improve the affect prediction performance. However, the synchronization between image and audio data has not yet been achieved. Moreover, many different ways can be employed to annotate human affects, and the annotations in many datasets are not identical. The data cannot be utilized in supervised learning without the annotation of the task to be predicted. This study proposes a multi-task human affect prediction model with multimodal input and knowledge distillation to address the abovementioned problems. We used SoundNet, which was trained to transfer visual knowledge into auditory representations, to extract synchronized auditory-visual representations. Knowledge distillation was applied to utilize all datasets with incomplete labels. This model used image and audio data to predict the valence-arousal, expression, and action units and was validated using the Aff-Wild2 dataset. When auditory-visual synchronized representation was used, the performance improved by 11.83% and 230.16%, respectively, compared to when visual or auditory representation was used alone. When knowledge distillation was applied, the performance improved by 15.38% compared to when it was not. Consequently, the proposed model achieved a 0.95 performance for the multi-task learning task on the Aff-Wild2 test dataset. This performance is equivalent to that of the second place in the 3rd Affective Behavior Analysis in the wild Multi-task Learning Challenge.",
    "code_link": ""
  },
  "cvpr2022_abaw_ajointcross-attentionmodelforaudio-visualfusionindimensionalemotionrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition",
    "authors": [
      "R Gnana Praveen",
      "Wheidima Carneiro de Melo",
      "Nasib Ullah",
      "Haseeb Aslam",
      "Osama Zeeshan",
      "Th\u00e9o Denorme",
      "Marco Pedersoli",
      "Alessandro L. Koerich",
      "Simon Bacon",
      "Patrick Cardinal",
      "Eric Granger"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Praveen_A_Joint_Cross-Attention_Model_for_Audio-Visual_Fusion_in_Dimensional_Emotion_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Praveen_A_Joint_Cross-Attention_Model_for_Audio-Visual_Fusion_in_Dimensional_Emotion_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-modal emotion recognition has recently gained much attention since it can leverage diverse and complementary relationships over multiple modalities, such as audio, visual, and bio-signals. Most state-of-the-art methods for audio-visual (A-V) fusion rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos. Specifically, we propose a joint cross-attention model that relies on the complementary relationships to extract the salient features across A-V modalities, allowing for accurate prediction of continuous values of valence and arousal. The proposed fusion model efficiently leverages the inter-modal relationships, while reducing the heterogeneity between features. In particular, it computes cross-attention weights based on the correlation between joint feature representations, and that of individual modalities. By deploying a joint A-V feature representation into the cross-attention module, the performance of our fusion module improves significantly over the vanilla cross-attention module. Experimental results on the AffWild2 dataset highlight the robustness of our proposed A-V fusion model. It has achieved a concordance correlation coefficient (CCC) of 0.374 (0.663) and 0.363 (0.584) for valence and arousal, respectively, on test set (validation set). This is a significant improvement over the baseline of third challenge of Affective Behavior Analysis in-the-wild (ABAW3) competition, with a CCC of 0.180 (0.310) and 0.170 (0.170).",
    "code_link": ""
  },
  "cvpr2022_abaw_mixaugment&mixupaugmentationmethodsforfacialexpressionrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "MixAugment & Mixup: Augmentation Methods for Facial Expression Recognition",
    "authors": [
      "Andreas Psaroudakis",
      "Dimitrios Kollias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Automatic Facial Expression Recognition (FER) has attracted increasing attention in the last 20 years since facial expressions play a central role in human communication. Most FER methodologies utilize Deep Neural Networks (DNNs) that are powerful tools when it comes to data analysis. However, despite their power, these networks are prone to overfitting, as they often tend to memorize the training data. What is more, there are not currently a lot of in-the-wild (i.e. in unconstrained environment) large databases for FER. To alleviate this issue, a number of data augmentation techniques have been proposed. Data augmentation is a way to increase the diversity of available data by applying constrained transformations on the original data. One such technique, which has positively contributed to various classification tasks, is Mixup. According to this, a DNN is trained on convex combinations of pairs of examples and their corresponding labels. In this paper, we examine the effectiveness of Mixup for in-the-wild FER in which data have large variations in head poses, illumination conditions, backgrounds and contexts. We then propose a new data augmentation strategy which is based on Mixup, called MixAugment. According to this, the network is trained concurrently on a combination of virtual examples and real examples; all these examples contribute to the overall loss function. We conduct an extensive experimental study that proves the effectiveness of MixAugment over Mixup and various state-of-the-art methods. We further investigate the combination of dropout with Mixup and MixAugment, as well as the combination of other data augmentation techniques with MixAugment.",
    "code_link": ""
  },
  "cvpr2022_abaw_tiktokforgoodcreatingadiverseemotionexpressiondatabase": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "TikTok for Good: Creating a Diverse Emotion Expression Database",
    "authors": [
      "Saimourya Surabhi",
      "Bhavik Shah",
      "Peter Washington",
      "Onur Cezmi Mutlu",
      "Emilie Leblanc",
      "Prathamesh Mohite",
      "Arman Husic",
      "Aaron Kline",
      "Kaitlyn Dunlap",
      "Maya McNealis",
      "Bennett Liu",
      "Nick Deveaux",
      "Essam Sleiman",
      "Dennis P. Wall"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Surabhi_TikTok_for_Good_Creating_a_Diverse_Emotion_Expression_Database_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Surabhi_TikTok_for_Good_Creating_a_Diverse_Emotion_Expression_Database_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Facial expression recognition (FER) is a critical computer vision task for a variety of applications. Despite the widespread use of FER, there is a dearth of racially diverse facial emotion datasets which are enriched for children, teens, and adults. To bridge this gap, we have built a diverse expression recognition database using publicly available videos from TikTok, a video-focused social networking service. We describe the construction of the TikTok Facial expression recognition (FER) database. The dataset is extracted from 6428 videos scraped from TikTok. The videos consist of 9392 distinct individuals and labels for 15 emotion-related prompts. We were able to achieve a F1 score 0.78 for Ekman emotions on expression classification using transfer learning. We hope that the scale and diversity of the TikTokFER dataset will be of use to affective computing practitioners.",
    "code_link": ""
  },
  "cvpr2022_abaw_bridgingthegapbetweenautomatedandhumanfacialemotionperception": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Bridging the Gap Between Automated and Human Facial Emotion Perception",
    "authors": [
      "Derek Stratton",
      "Emily Hand"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Stratton_Bridging_the_Gap_Between_Automated_and_Human_Facial_Emotion_Perception_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Stratton_Bridging_the_Gap_Between_Automated_and_Human_Facial_Emotion_Perception_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Understanding the complex relationship between emotions and facial expressions is important for both psychologists and computer scientists. A large body of research in psychology investigates facial expressions, emotions, and how emotions are perceived from facial expressions. As computer scientists look to incorporate this research into automatic emotion perception systems, it is important to understand the nature and limitations of human emotion perception. These principles of emotion science affect the way datasets are created, methods are implemented, and results are interpreted in automated emotion perception. This paper aims to distill and align prior work in automated and human facial emotion perception to facilitate future discussions and research at the intersection of the two disciplines.",
    "code_link": ""
  },
  "cvpr2022_abaw_estimatingmultipleemotiondescriptorsbyseparatingdescriptionandinference": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Estimating Multiple Emotion Descriptors by Separating Description and Inference",
    "authors": [
      "Didan Deng",
      "Bertram E. Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Deng_Estimating_Multiple_Emotion_Descriptors_by_Separating_Description_and_Inference_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Deng_Estimating_Multiple_Emotion_Descriptors_by_Separating_Description_and_Inference_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "To describe complex emotional states, psychologists have proposed multiple emotion descriptors: sparse descriptors like facial action units, continuous descriptors like valence and arousal, and discrete class descriptors like the expressions of happiness and anger. According to Cohn et al. [1], facial action units are sign vehicles that convey the emotion message, while discrete or continuous emotion descriptors are the messages perceived by observers. They differ in their focuses. Sign vehicles focus on describing facial behavior. Emotion messages focus on an observer's inference about the underlying state of the subject from facial behavior. We describe a novel architecture for multiple emotion descriptor estimation that incorporates this prior knowledge about the differences between descriptive labels (sign vehicles, like facial action units) and inferential labels (emotion messages like discrete emotion expressions, valence, and arousal). In our multi-level architecture, a common set of low-level features of facial regions are fed into two separate branches: one for descriptive labels and the other for inferential labels. The differences between these two branches reflects the differences between the two types of labels. Sign vehicles are typically more specific and spatially localized. Emotion messages are reflected across the entire face. Our experiments on the ABAW3 challenge dataset demonstrate this approach outperforms all other submitted approaches to multi-task learning. Code is available at https://github.com/HKUST-NISL/ABAW3_MultiEmotionNet.",
    "code_link": ""
  },
  "cvpr2022_abaw_transformer-basedmultimodalinformationfusionforfacialexpressionanalysis": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Transformer-Based Multimodal Information Fusion for Facial Expression Analysis",
    "authors": [
      "Wei Zhang",
      "Feng Qiu",
      "Suzhen Wang",
      "Hao Zeng",
      "Zhimeng Zhang",
      "Rudong An",
      "Bowen Ma",
      "Yu Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Zhang_Transformer-Based_Multimodal_Information_Fusion_for_Facial_Expression_Analysis_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Zhang_Transformer-Based_Multimodal_Information_Fusion_for_Facial_Expression_Analysis_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human affective behavior analysis has received much attention in human-computer interaction (HCI). In this paper, we introduce our submission to the CVPR 2022 Competition on Affective Behavior Analysis in-the-wild (ABAW). To fully exploit affective knowledge from multiple views, we utilize the multimodal features of spoken words, speech prosody, and facial expression, which are extracted from the video clips in the Aff-Wild2 dataset. Based on these features, we propose a unified transformer-based multimodal framework for Action Unit detection and also expression recognition. Specifically, the static vision feature is first encoded from the current frame image. At the same time, we clip its adjacent frames by a sliding window and extract three kinds of multimodal features from the sequence of images, audio, and text. Then, we introduce a transformer-based fusion module that integrates the static vision features and the dynamic multimodal features. The cross-attention module in the fusion module makes the output integrated features focus on the crucial parts that facilitate the downstream detection tasks. We also leverage some data balancing techniques, data augmentation techniques, and postprocessing methods to further improve the model performance. In the official test of ABAW3 Competition, our model ranks first in the EXPR and AU tracks. The extensive quantitative evaluations, as well as ablation studies on the Aff-Wild2 dataset, prove the effectiveness of our proposed method.",
    "code_link": ""
  },
  "cvpr2022_abaw_valenceandarousalestimationbasedonmultimodaltemporal-awarefeaturesforvideosinthewild": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Valence and Arousal Estimation Based on Multimodal Temporal-Aware Features for Videos in the Wild",
    "authors": [
      "Liyu Meng",
      "Yuchen Liu",
      "Xiaolong Liu",
      "Zhaopei Huang",
      "Wenqiang Jiang",
      "Tenggan Zhang",
      "Chuanhe Liu",
      "Qin Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Meng_Valence_and_Arousal_Estimation_Based_on_Multimodal_Temporal-Aware_Features_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Meng_Valence_and_Arousal_Estimation_Based_on_Multimodal_Temporal-Aware_Features_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents our submission to the Valence-Arousal Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW) competition. Based on multi-modal feature representations that fuse the visual and aural information, we utilize two types of temporal encoder to capture the temporal context information in the video, including the transformer based encoder and LSTM based encoder. With the temporal context-aware representations, we employ fully-connected layers to predict the valence and arousal values of the video frames. In addition, smoothing processing is applied to refine the initial predictions, and a model ensemble strategy is used to combine multiple results from different model setups. Our system achieves the performance in Concordance Correlation Coefficients (ccc) of 0.606 for valence, 0.602 for arousal, and mean ccc of 0.601, which ranks the first place in the challenge.",
    "code_link": ""
  },
  "cvpr2022_abaw_threestreamgraphattentionnetworkusingdynamicpatchselectionfortheclassificationofmicro-expressions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Three Stream Graph Attention Network Using Dynamic Patch Selection for the Classification of Micro-Expressions",
    "authors": [
      "Ankith Jain Rakesh Kumar",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Kumar_Three_Stream_Graph_Attention_Network_Using_Dynamic_Patch_Selection_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Kumar_Three_Stream_Graph_Attention_Network_Using_Dynamic_Patch_Selection_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "To understand the genuine emotions expressed by humans during social interactions, it is necessary to recognize the subtle changes on the face (micro-expressions) demonstrated by an individual. Facial micro-expressions are brief, rapid, spontaneous gestures and non-voluntary facial muscle movements beneath the skin. Therefore, it is a challenging task to classify facial micro-expressions. This paper presents an end-to-end novel three-stream graph attention network model to capture the subtle changes on the face and recognize micro-expressions (MEs) by exploiting the relationship between optical flow magnitude, optical flow direction, and the node locations features. A facial graph representational structure is used to extract the spatial and temporal information using the three frames. The varying dynamic patch size of optical flow features is used to extract the local texture information across each landmark point. The network only utilizes the landmark points location features and optical flow information across these points and generates good results for the classification of MEs. A comprehensive evaluation of SAMM and the CASME II datasets demonstrates the high efficacy, efficiency, and generalizability of the proposed approach and achieves better results than the state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2022_abaw_actionunitdetectionbyexploitingspatial-temporalandlabel-wiseattentionwithtransformer": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Action Unit Detection by Exploiting Spatial-Temporal and Label-Wise Attention With Transformer",
    "authors": [
      "Lingfeng Wang",
      "Jin Qi",
      "Jian Cheng",
      "Kenji Suzuki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Wang_Action_Unit_Detection_by_Exploiting_Spatial-Temporal_and_Label-Wise_Attention_With_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Wang_Action_Unit_Detection_by_Exploiting_Spatial-Temporal_and_Label-Wise_Attention_With_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The facial action units (FAU) defined by the Facial Action Coding System (FACS) has become an important approach of facial expression analysis. Most work on FAU detection only considers the spatial-temporal feature and ignores the label-wise AU correlation. In practice, the strong relationships between facial AUs can help AU detection. We proposed a transformer based FAU detection model by leverage both the local spatial-temporal features and label-wise FAU correlation. To be specific, we firstly designed a visual spatial-temporal transformer based model and a convolution based audio model to extract action unit specific features. Secondly, inspired by the relationship between FAUs, we proposed a transformer based correlation module to learn correlation between AUs. The action unit specific features from aural and visual models are further aggregated in the correlation modules to produce per-frame prediction of 12 AUs. Our model was trained on Aff-Wild2 dataset of the ABAW3 challenge and achieved state of art performance in the FAU task, which verified that the effectiveness of the proposed network.",
    "code_link": ""
  },
  "cvpr2022_abaw_video-basedmultimodalspontaneousemotionrecognitionusingfacialexpressionsandphysiologicalsignals": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ABAW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Affective Behavior Analysis In-the-Wild",
    "title": "Video-Based Multimodal Spontaneous Emotion Recognition Using Facial Expressions and Physiological Signals",
    "authors": [
      "Yassine Ouzar",
      "Fr\u00e9d\u00e9ric Bousefsaf",
      "Djamaleddine Djeldjli",
      "Choubeila Maaoui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/html/Ouzar_Video-Based_Multimodal_Spontaneous_Emotion_Recognition_Using_Facial_Expressions_and_Physiological_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Ouzar_Video-Based_Multimodal_Spontaneous_Emotion_Recognition_Using_Facial_Expressions_and_Physiological_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human's affective state recognition remains a challenging topic due to the complexity of emotions, which involves experiential, behavioral, and physiological elements. Since it is difficult to comprehensively describe emotion in terms of single modalities, recent studies have focused on fusion strategy to exploit the complementarity of multimodal signals. In this article, we study the feasibility of fusing facial expressions with physiological cues on human emotion recognition accuracy. The contributions of this work are threefold: 1) We propose a new spatiotemporal network for facial expression recognition using a 3D squeeze and exitation based 3D Xception architecture (squeeze and exitation Xception network). 2) We adopt the first multiple modalities fusion using single input source which, to the best of our knowledge, no existing multimodal emotion recognition system has attempted to identify emotional state from only facial videos using facial expressions and physiological signals features. 3) We compare the performance of the unimodal approach using only facial expressions or physiological data, to multimodal systems fusing facial expressions with video-based physiological cues. In our experiments, physiological signals such as the iPPG signal and features of heart rate variability measured remotely using the imaging photoplethysmography (iPPG) method are used. The preliminary results show that the multimodal fusion model improves the accuracy of emotion recognition, and merging facial expressions features with iPPG signal gives the best accuracy with 71.90%.",
    "code_link": ""
  },
  "cvpr2022_precognition_unsuperviseddomainadaptationforcardiacsegmentationtowardsstructuremutualinformationmaximization": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure Mutual Information Maximization",
    "authors": [
      "Changjie Lu",
      "Shen Zheng",
      "Gaurav Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Lu_Unsupervised_Domain_Adaptation_for_Cardiac_Segmentation_Towards_Structure_Mutual_Information_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Lu_Unsupervised_Domain_Adaptation_for_Cardiac_Segmentation_Towards_Structure_Mutual_Information_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Unsupervised domain adaptation approaches have recently succeeded in various medical image segmentation tasks. The reported works often tackle the domain shift problem by aligning the domain-invariant features and minimizing the domain-specific discrepancies. That strategy works well when the difference between a specific domain and between different domains is slight. However, the generalization ability of these models on diverse imaging modalities remains a significant challenge. This paper introduces UDA-VAE++, an unsupervised domain adaptation framework for cardiac segmentation with a compact loss function lower bound. To estimate this new lower bound, we develop a novel Structure Mutual Information Estimation (SMIE) block with a global estimator, a local estimator, and a prior information matching estimator to maximize the mutual information between the reconstruction and segmentation tasks. Specifically, we design a novel sequential reparameterization scheme that enables information flow and variance correction from the low-resolution latent space to the high-resolution latent space. Comprehensive experiments on benchmark cardiac segmentation datasets demonstrate that our model outperforms previous state-of-the-art qualitatively and quantitatively.",
    "code_link": ""
  },
  "cvpr2022_precognition_persistent-transientdualityinhumanbehaviormodeling": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Persistent-Transient Duality in Human Behavior Modeling",
    "authors": [
      "Hung Tran",
      "Vuong Le",
      "Svetha Venkatesh",
      "Truyen Tran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Tran_Persistent-Transient_Duality_in_Human_Behavior_Modeling_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Tran_Persistent-Transient_Duality_in_Human_Behavior_Modeling_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose to model the persistent-transient duality in human behavior using a parent-child multi-channel neural network, which features a parent persistent channel that manages the global dynamics and children transient channels that are initiated and terminated on-demand to handle detailed interactive actions. The short-lived transient sessions are managed by a proposed Transient Switch. The neural framework is trained to discover the structure of the duality automatically. Our model shows superior performances in human-object interaction motion prediction.",
    "code_link": ""
  },
  "cvpr2022_precognition_s2f2single-stageflowforecastingforfuturemultipletrajectoriesprediction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "S2F2: Single-Stage Flow Forecasting for Future Multiple Trajectories Prediction",
    "authors": [
      "Yu-Wen Chen",
      "Hsuan-Kung Yang",
      "Chu-Chi Chiu",
      "Chun-Yi Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Chen_S2F2_Single-Stage_Flow_Forecasting_for_Future_Multiple_Trajectories_Prediction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Chen_S2F2_Single-Stage_Flow_Forecasting_for_Future_Multiple_Trajectories_Prediction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this work, we present a single-stage framework, named S2F2, for forecasting multiple human trajectories from raw video images by predicting future optical flows. S2F2 differs from the previous two-stage approaches in that it performs detection, Re-ID, and forecasting of multiple pedestrians at the same time. Unlike the prior approaches, the computational burden of S2F2 remains consistent even as the number of pedestrians grows. The experimental results demonstrate that S2F2 is able to outperform two conventional forecasting algorithms and a recent learning-based two-stage model, while maintaining its tracking performance on par with the contemporary MOT models.",
    "code_link": ""
  },
  "cvpr2022_precognition_importanceisinyourattentionagentimportancepredictionforautonomousdriving": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Importance Is in Your Attention: Agent Importance Prediction for Autonomous Driving",
    "authors": [
      "Christopher Hazard",
      "Akshay Bhagat",
      "Balarama Raju Buddharaju",
      "Zhongtao Liu",
      "Yunming Shao",
      "Lu Lu",
      "Sammy Omari",
      "Henggang Cui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Hazard_Importance_Is_in_Your_Attention_Agent_Importance_Prediction_for_Autonomous_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Hazard_Importance_Is_in_Your_Attention_Agent_Importance_Prediction_for_Autonomous_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Trajectory prediction is an important task in autonomous driving. State-of-the-art trajectory prediction models often use attention mechanisms to model the interaction between agents. In this paper, we show that the attention information from such models can also be used to measure the importance of each agent with respect to the ego vehicle's future planned trajectory. Our experiment results on the nuPlans dataset show that our method can effectively find and rank surrounding agents by their impact on the ego's plan.",
    "code_link": ""
  },
  "cvpr2022_precognition_hr-stanhigh-resolutionspatio-temporalattentionnetworkfor3dhumanmotionprediction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "HR-STAN: High-Resolution Spatio-Temporal Attention Network for 3D Human Motion Prediction",
    "authors": [
      "Omar Medjaouri",
      "Kevin Desai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Medjaouri_HR-STAN_High-Resolution_Spatio-Temporal_Attention_Network_for_3D_Human_Motion_Prediction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Medjaouri_HR-STAN_High-Resolution_Spatio-Temporal_Attention_Network_for_3D_Human_Motion_Prediction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "3D human motion prediction requires making sense of the complex spatio-temporal dynamics which underpin human motion to make highly accurate predictions. Part of this complexity is due to the trade-off between long-term (>400ms) and short-term predictions (<400ms) which require different levels of granularity to observe patterns. Several works have explored methods of improving long-term prediction performance by utilizing longer motion histories but this typically comes at the cost of very short-term (<200ms) performance. Inspired by high-resolution network architectures, we propose a novel high-resolution spatio-temporal attention network (HR-STAN) which leverages parallel feature branches and dilated convolutions to observe human motion at different scales. Furthermore, we augment this architecture with split spatial and temporal attention mechanisms to efficiently capture spatio-temporal dependencies within a given motion. We evaluate the ability of our HR-STAN architecture at incorporating long-term motion histories while producing short-term predictions and show that it improves over several state-of-the-art methods on both the AMASS and Human3.6M benchmarks.",
    "code_link": ""
  },
  "cvpr2022_precognition_goal-drivenself-attentiverecurrentnetworksfortrajectoryprediction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Goal-Driven Self-Attentive Recurrent Networks for Trajectory Prediction",
    "authors": [
      "Luigi Filippo Chiara",
      "Pasquale Coscia",
      "Sourav Das",
      "Simone Calderara",
      "Rita Cucchiara",
      "Lamberto Ballan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Chiara_Goal-Driven_Self-Attentive_Recurrent_Networks_for_Trajectory_Prediction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Chiara_Goal-Driven_Self-Attentive_Recurrent_Networks_for_Trajectory_Prediction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human trajectory forecasting is a key component of autonomous vehicles, social-aware robots and advanced video-surveillance applications. This challenging task typically requires knowledge about past motion, the environment and likely destination areas. In this context, multi-modality is a fundamental aspect and its effective modeling can be beneficial to any architecture. Inferring accurate trajectories is nevertheless challenging, due to the inherently uncertain nature of the future. To overcome these difficulties, recent models use different inputs and propose to model human intentions using complex fusion mechanisms. In this respect, we propose a lightweight attention-based recurrent backbone that acts solely on past observed positions. Although this backbone already provides promising results, we demonstrate that its prediction accuracy can be improved considerably when combined with a scene-aware goal-estimation module. To this end, we employ a common goal module, based on a U-Net architecture, which additionally extracts semantic information to predict scene-compliant destinations. We conduct extensive experiments on publicly-available datasets (i.e. SDD, inD, ETH/UCY) and show that our approach performs on par with state-of-the-art techniques while reducing model complexity.",
    "code_link": "https://github.com/luigifilippochiara/Goal-SAR"
  },
  "cvpr2022_precognition_jointforecastingofpanopticsegmentationswithdifferenceattention": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Joint Forecasting of Panoptic Segmentations With Difference Attention",
    "authors": [
      "Colin Graber",
      "Cyril Jazra",
      "Wenjie Luo",
      "Liangyan Gui",
      "Alexander Schwing"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Graber_Joint_Forecasting_of_Panoptic_Segmentations_With_Difference_Attention_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Graber_Joint_Forecasting_of_Panoptic_Segmentations_With_Difference_Attention_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Forecasting of a representation is important for safe and effective autonomy. For this, panoptic segmentations have been studied as a compelling representation in recent work. However, recent state-of-the-art on panoptic segmentation forecasting suffers from two issues: first, individual object instances are treated independently of each other; second, individual object instance forecasts are merged in a heuristic manner. To address both issues, we study a new panoptic segmentation forecasting model that jointly forecasts all object instances in a scene using a transformer model based on 'difference attention'. It further refines the predictions by taking depth estimates into account. We evaluate the proposed model on the Cityscapes and AIODrive datasets. We find difference attention to be particularly suitable for forecasting because the difference of quantities like locations enables a model to explicitly reason about velocities and acceleration. Because of this, we attain state-of-the-art on panoptic segmentation forecasting metrics.",
    "code_link": ""
  },
  "cvpr2022_precognition_multi-cameramultiple3dobjecttrackingonthemoveforautonomousvehicles": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Multi-Camera Multiple 3D Object Tracking on the Move for Autonomous Vehicles",
    "authors": [
      "Pha Nguyen",
      "Kha Gia Quach",
      "Chi Nhan Duong",
      "Ngan Le",
      "Xuan-Bac Nguyen",
      "Khoa Luu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The development of autonomous vehicles provides an opportunity to have a complete set of camera sensors capturing the environment around the car. Thus, it is important for object detection and tracking to address new challenges, such as achieving consistent results across views of cameras. To address these challenges, this work presents a new Global Association Graph Model with Link Prediction approach to predict existing tracklets location and link detections with tracklets via cross-attention motion modeling and appearance re-identification. This approach aims at solving issues caused by inconsistent 3D object detection. Moreover, our model exploits to improve the detection accuracy of a standard 3D object detector in the nuScenes detection challenge. The experimental results on the nuScenes dataset demonstrate the benefits of the proposed method to produce SOTA performance on the existing vision-based tracking dataset.",
    "code_link": ""
  },
  "cvpr2022_precognition_seasituationalawareness(seasaw)dataset": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Sea Situational Awareness (SeaSAW) Dataset",
    "authors": [
      "Parneet Kaur",
      "Arslan Aziz",
      "Darshan Jain",
      "Harshil Patel",
      "Jonathan Hirokawa",
      "Lachlan Townsend",
      "Christoph Reimers",
      "Fiona Hua"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Kaur_Sea_Situational_Awareness_SeaSAW_Dataset_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Kaur_Sea_Situational_Awareness_SeaSAW_Dataset_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The oceans provide 90% global trade by vessels. Situational awareness from intelligent vessel systems can enable enhanced safety and decision-making for mariners. As the foundation for these intelligent systems, advanced perception technology requires sufficient real-world operational data to leverage recent AI technologies. In this work, we introduce the Sea Situational Awareness (SeaSAW) dataset - a novel dataset that comprises 1.9 million images with 14.6 million objects associated 20.4 million attributes from 12 object classes, making it the largest maritime dataset for object detection, fine-grained classification and tracking. Furthermore, this dataset consists 9 sources in combination with various RGB cameras mounted on different moving vessels operating in different geographic locations globally, having variations in scenario, weather and illumination conditions. This work assembles the data collected across 4 years with rigorous efforts on data selection, annotation, management and analysis to enhance the marine perception technology.",
    "code_link": ""
  },
  "cvpr2022_precognition_informationelevationnetworkforonlineactiondetectionandanticipation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Precognition: Seeing Through the Future",
    "title": "Information Elevation Network for Online Action Detection and Anticipation",
    "authors": [
      "Sunah Min",
      "Jinyoung Moon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Min_Information_Elevation_Network_for_Online_Action_Detection_and_Anticipation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Min_Information_Elevation_Network_for_Online_Action_Detection_and_Anticipation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Given a partially observed video segment, online action detection and anticipation aim to identify a current action and forecast future actions, respectively. To detect actions in a streaming video for monitoring applications including surveillance and autonomous driving, online action detection methods have been proposed. Considering the importance of current action in online action detection, we introduce a novel information elevation unit (IEU) that lifts and accumulates the past information relevant to the current action, to compensate for forgotten essential information. Using the IEUs, we propose an information elevation network (IEN) that effectively identifies a current action and anticipates future actions through the dense prediction of past and current action classes within the video segment. For its practical use in online monitoring applications, our IEN takes visual features extracted from a fast action recognition using only RGB frames because extracting optical flows requires heavy computation overhead. On THUMOS-14 and TVSeries, our IEN outperforms state-of-the-art methods using only RGB frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2022_ecv_anonce-for-allbudgetedpruningframeworkforconvnetsconsideringinputresolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "An Once-for-All Budgeted Pruning Framework for ConvNets Considering Input Resolution",
    "authors": [
      "Wenyu Sun",
      "Jian Cao",
      "Pengtao Xu",
      "Xiangcheng Liu",
      "Yuan Zhang",
      "Yuan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Sun_An_Once-for-All_Budgeted_Pruning_Framework_for_ConvNets_Considering_Input_Resolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sun_An_Once-for-All_Budgeted_Pruning_Framework_for_ConvNets_Considering_Input_Resolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose an efficient once-for-all budgeted pruning framework (OFARPruning) to find many compact network architectures close to winner tickets in the early training stage considering the effect of input resolution during the pruning process. In architecture searching stage, we measure the similarity of the pruning mask to get high-quality network architecture with low energy and time consumption. After searching stage, our proposed method randomly sample the compact architectures with different pruning rates and input resolution to achieve joint optimization. Ultimately, we can obtain a cohort of compact networks adaptive to various resolution to meet dynamic FLOPs constraints on different edge devices with only once training. The experiments based on image classification and object detection show that OFARPruning has a higher accuracy than the once-for-all compression methods such as US-Net and MutualNet (1-2% better with less FLOPs), and achieve competitive performance as the conventional pruning methods (72.6% vs. 70.5% on MobileNetv2 under 170 MFLOPs) with much higher efficiency.",
    "code_link": ""
  },
  "cvpr2022_ecv_maplemicroprocessoraprioriforlatencyestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "MAPLE: Microprocessor a Priori for Latency Estimation",
    "authors": [
      "Saad Abbasi",
      "Alexander Wong",
      "Mohammad Javad Shafiee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Abbasi_MAPLE_Microprocessor_a_Priori_for_Latency_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Abbasi_MAPLE_Microprocessor_a_Priori_for_Latency_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Modern deep neural networks must demonstrate state-of-the-art accuracy while exhibiting low latency and energy consumption. As such, neural architecture search (NAS) algorithms take these two constraints into account when generating a new architecture. However, efficiency metrics such as latency are typically hardware dependent requiring the NAS algorithm to either measure or predict the architecture latency. Measuring the latency of every evaluated architecture adds a significant amount of time to the NAS process. Here we propose Microprocessor A Priori for Latency Estimation (MAPLE) that leverages hardware characteristics to predict deep neural network latency on previously unseen hardware devices. MAPLE takes advantage of a novel quantitative strategy to characterize the underlying microprocessor by measuring relevant hardware performance metrics, yielding a fine-grained and expressive hardware descriptor. The CPU-specific performance metrics are also able to characterize GPUs, resulting in a versatile descriptor that does not rely on the availability of hardware counters on GPUs or other deep learning accelerators. We provide experimental insight into this novel strategy. Through this hardware descriptor, MAPLE can generalize to new hardware via a few shot adaptation strategy, requiring as few as 3 samples from the target hardware to yield 6% improvement over state-of-the-art methods requiring as much as 10 samples. Experimental results showed that, increasing the few shot adaptation samples to 10 improves the accuracy significantly over the state-of-the-art methods by 12%. We also demonstrate MAPLE identification of Pareto-optimal DNN architectures exhibit superlative accuracy and efficiency. The proposed technique provides a versatile and practical latency prediction methodology for DNN run-time inference on multiple hardware devices while not imposing any significant overhead for sample collection.",
    "code_link": "https://github.com/torvalds/linux"
  },
  "cvpr2022_ecv_activeobjectdetectionwithepistemicuncertaintyandhierarchicalinformationaggregation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Active Object Detection With Epistemic Uncertainty and Hierarchical Information Aggregation",
    "authors": [
      "Younghyun Park",
      "Soyeong Kim",
      "Wonjeong Choi",
      "Dong-Jun Han",
      "Jaekyun Moon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Park_Active_Object_Detection_With_Epistemic_Uncertainty_and_Hierarchical_Information_Aggregation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Park_Active_Object_Detection_With_Epistemic_Uncertainty_and_Hierarchical_Information_Aggregation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite the huge success of object detection, the training process still requires an immense amount of labeled data. Active learning has been proposed as a practical solution, but existing works on active object detection do not utilize the concept of epistemic uncertainty, which is an important metric for capturing the usefulness of the sample. Previous works also pay little attention to the relation between bounding boxes when computing the informativeness of an image. In this paper, we propose a new active object detection strategy that improves these two shortcomings of existing methods. We specifically consider a Bayesian framework and propose a new module termed model evidence head (MEH), to take advantage of epistemic uncertainty in object detection. We also propose hierarchical uncertainty aggregation (HUA), which realigns all bounding boxes into multiple levels and aggregates uncertainties in a bottom-up order, to compute the informativeness of an image. Experimental results show that our method outperforms existing state-of-the-art methods by a considerable margin.",
    "code_link": ""
  },
  "cvpr2022_ecv_da3dynamicadditiveattentionadaptionformemory-efficienton-devicemulti-domainlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "DA3: Dynamic Additive Attention Adaption for Memory-Efficient On-Device Multi-Domain Learning",
    "authors": [
      "Li Yang",
      "Adnan Siraj Rakin",
      "Deliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Yang_DA3_Dynamic_Additive_Attention_Adaption_for_Memory-Efficient_On-Device_Multi-Domain_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Yang_DA3_Dynamic_Additive_Attention_Adaption_for_Memory-Efficient_On-Device_Multi-Domain_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nowadays, one practical limitation of deep neural network (DNN) is its high degree of specialization to a single task or domain (e.g., one visual domain). It motivates researchers to develop algorithms that can adapt DNN model to multiple domains sequentially, while still performing well on the past domains, which is known as multi-domain learning. Almost all conventional methods only focus on improving accuracy with minimal parameter update, while ignoring high computing and memory cost during training, which makes it difficult to deploy multi-domain learning into more and more widely used resource-limited edge devices, like mobile phone, IoT, embedded system, etc. During our study in multi-domain training process, we observe that large memory used for activation storage is the bottleneck that largely limits the training time and cost on edge devices. To reduce training memory usage, while keeping the domain adaption accuracy performance, we propose Dynamic Additive Attention Adaption (DA3), a novel memory-efficient on-device multi-domain learning method. DA3 learns a novel additive attention adaptor module, while freezing the weights of the pre-trained backbone model for each domain. Differentiating from prior works, such module not only mitigates activation memory buffering for reducing memory usage during training, but also serves as a dynamic gating mechanism to reduce the computation cost for fast inference. We validate DA3 on multiple datasets against state-of-the-art methods, which shows great improvement in both accuracy and training time. Moreover, we deployed DA3 into the popular NIVDIA Jetson Nano edge GPU, where the measured experimental results show our proposed \\mldam reduces the on-device training memory consumption by 19x-37x, and training time by 2x, in comparison to the baseline methods (e.g., standard fine-tuning, Parallel and Series Res. adaptor, and Piggyback).",
    "code_link": ""
  },
  "cvpr2022_ecv_searchingforefficientneuralarchitecturesforon-devicemlonedgetpus": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs",
    "authors": [
      "Berkin Akin",
      "Suyog Gupta",
      "Yun Long",
      "Anton Spiridonov",
      "Zhuo Wang",
      "Marie White",
      "Hao Xu",
      "Ping Zhou",
      "Yanqi Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Akin_Searching_for_Efficient_Neural_Architectures_for_On-Device_ML_on_Edge_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Akin_Searching_for_Efficient_Neural_Architectures_for_On-Device_ML_on_Edge_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "On-device ML accelerators are becoming a standard in modern mobile system-on-chips (SoC). Neural architecture search (NAS) comes to the rescue for efficiently utilizing the high compute throughput offered by these accelerators. However, existing NAS frameworks have several practical limitations in scaling to multiple tasks and different target platforms. In this work, we provide a two-pronged approach to this challenge: (i) a NAS-enabling infrastructure that decouples model cost evaluation, search space design, and the NAS algorithm to rapidly target various on-device ML tasks, and (ii) search spaces crafted from group convolution based inverted bottleneck (IBN) variants that provide flexible quality/performance trade-offs on ML accelerators, complementing the existing full and depthwise convolution based IBNs. Using this approach we target a state-of-the-art mobile platform, Google Tensor SoC, and demonstrate neural architectures that improve the quality-performance pareto frontier for various computer vision (classification, detection, segmentation) as well as natural language processing tasks.",
    "code_link": ""
  },
  "cvpr2022_ecv_yolo-poseenhancingyoloformultipersonposeestimationusingobjectkeypointsimilarityloss": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss",
    "authors": [
      "Debapriya Maji",
      "Soyeb Nagori",
      "Manu Mathew",
      "Deepak Poddar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Maji_YOLO-Pose_Enhancing_YOLO_for_Multi_Person_Pose_Estimation_Using_Object_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Maji_YOLO-Pose_Enhancing_YOLO_for_Multi_Person_Pose_Estimation_Using_Object_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We introduce YOLO-pose, a novel heatmap-free approach for joint detection, and 2D multi-person pose estimation in an image based on the popular YOLO object detection framework. Existing heatmap based two-stage approaches are sub-optimal as they are not end-to-end trainable and training relies on a surrogate L1 loss that is not equivalent to maximizing the evaluation metric, i.e. Object Keypoint Similarity (OKS). Our framework allows us to train the model end-to-end and optimize the OKS metric itself. The proposed model learns to jointly detect bounding boxes for multiple persons and their corresponding 2D poses in a single forward pass and thus bringing in the best of both top-down and bottom-up approaches. Proposed approach doesn't require the post-processing of bottom-up approaches to group detected keypoints into a skeleton as each bounding box has an associated pose, resulting in an inherent grouping of the keypoints. Unlike top-down approaches, multiple forward passes are done away with since all persons are localized along with their pose in a single inference. YOLO-pose achieves new state-of-the-art results on COCO validation (90.2% AP50) and test-dev set (90.3% AP50), surpassing all existing bottom-up approaches in a single forward pass without flip test, multi-scale testing, or any other test time augmentation. All experiments and results reported in this paper are without any test time augmentation, unlike traditional approaches that use flip-test and multi-scale testing to boost performance. Our training code and models will be made publicly available at https://github.com/TexasInstruments/edgeai-yolov5 and https://github.com/TexasInstruments/edgeai-yolox.",
    "code_link": ""
  },
  "cvpr2022_ecv_resnestsplit-attentionnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "ResNeSt: Split-Attention Networks",
    "authors": [
      "Hang Zhang",
      "Chongruo Wu",
      "Zhongyue Zhang",
      "Yi Zhu",
      "Haibin Lin",
      "Zhi Zhang",
      "Yue Sun",
      "Tong He",
      "Jonas Mueller",
      "R. Manmatha",
      "Mu Li",
      "Alexander Smola"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Zhang_ResNeSt_Split-Attention_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Zhang_ResNeSt_Split-Attention_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The ability to learn richer network representations generally boosts the performance of deep learning models. To improve representation-learning in convolutional neural networks, we present a multi-branch architecture, which applies channel-wise attention across different network branches to leverage the complementary strengths of both feature-map attention and multi-path representation. Our proposed Split-Attention module provides a simple and modular computation block that can serve as a drop-in replacement for the popular residual block, while producing more diverse representations via cross-feature interactions. Adding a Split-Attention module into the architecture design space of RegNet-Y and FBNetV2 directly improves the performance of the resulting network. Replacing residual blocks with our Split-Attention module, we further design a new variant of the ResNet model, named ResNeSt, which outperforms EfficientNet in terms of the accuracy/latency trade-off.",
    "code_link": ""
  },
  "cvpr2022_ecv_hybridconsistencytrainingwithprototypeadaptationforfew-shotlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Hybrid Consistency Training With Prototype Adaptation for Few-Shot Learning",
    "authors": [
      "Meng Ye",
      "Xiao Lin",
      "Giedrius Burachas",
      "Ajay Divakaran",
      "Yi Yao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Ye_Hybrid_Consistency_Training_With_Prototype_Adaptation_for_Few-Shot_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Ye_Hybrid_Consistency_Training_With_Prototype_Adaptation_for_Few-Shot_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Few-Shot Learning (FSL) aims to improve a model's generalization capability in low data regimes. Recent FSL works have made steady progress via metric learning, meta learning, representation learning, etc. However, FSL remains challenging due to the following longstanding difficulties. 1) The seen and unseen classes are disjoint, resulting in a distribution shift between training and testing. 2) During testing, labeled data of previously unseen classes is sparse, making it difficult to reliably extrapolate from labeled support examples to unlabeled query examples. To tackle the first challenge, we introduce Hybrid Consistency Training to jointly leverage two types of consistency: 1) interpolation consistency, which interpolates hidden features to imposes linear behavior locally, and 2) data augmentation consistency, which learns robust embeddings against sample variations. As for the second challenge, we use unlabeled examples to iteratively normalize features and adapt prototypes, as opposed to commonly used one-time update, for more reliable prototype-based transductive inference. We show that our method generates a 2% to 5% improvement over the state-of-the-art methods with similar backbones on five FSL datasets and, more notably, a 7% to 8% improvement for more challenging cross-domain FSL.",
    "code_link": ""
  },
  "cvpr2022_ecv_disentangledlossforlow-bitquantization-awaretraining": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Disentangled Loss for Low-Bit Quantization-Aware Training",
    "authors": [
      "Thibault Allenet",
      "David Briand",
      "Olivier Bichler",
      "Olivier Sentieys"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Allenet_Disentangled_Loss_for_Low-Bit_Quantization-Aware_Training_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Allenet_Disentangled_Loss_for_Low-Bit_Quantization-Aware_Training_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Quantization-Aware Training (QAT) has recently showed a lot of potential for low-bit settings in the context of image classification. Approaches based on QAT are using the Cross Entropy Loss function which is the reference loss function in this domain. We investigate quantization-aware training with disentangled loss functions. We qualify a loss to disentangle as it encourages the network output space to be easily discriminated with linear functions. We introduce a new method, Disentangled Loss Quantization Aware Training, as our tool to empirically demonstrate that the quantization procedure benefits from those loss functions. Results show that the proposed method substantially reduces the loss in top-1 accuracy for low-bit quantization on CIFAR10, CIFAR100 and ImageNet. Our best result brings the top-1 Accuracy of a Resnet-18 from 63.1% to 64.0% with binary weights and 2-bit activations when trained on ImageNet.",
    "code_link": ""
  },
  "cvpr2022_ecv_antadaptnetworkacrosstimeforefficientvideoprocessing": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "ANT: Adapt Network Across Time for Efficient Video Processing",
    "authors": [
      "Feng Liang",
      "Ting-Wu Chin",
      "Yang Zhou",
      "Diana Marculescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Liang_ANT_Adapt_Network_Across_Time_for_Efficient_Video_Processing_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Liang_ANT_Adapt_Network_Across_Time_for_Efficient_Video_Processing_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Abundant redundancies exist in video streams, thereby pointing to opportunities to save computations. Towards this end, we propose the Adaptive Network across Time (ANT) framework to harness these redundancies for reducing the computational cost of video processing. Unlike most dynamic networks that adapt their structures to different static inputs, our method adapts networks along the temporal dimension. By inspecting the semantic differences between frames, the proposed ANT chooses a purpose-fit network at test time to reduce overall computation, i.e., switching to a smaller network when observing mild differences. The proposed ANT adapts the structured networks within a supernet, making it hardware-friendly and therefore achieves actual acceleration in real-world scenarios. The proposed ANT is powered by (1). a fusion module that utilizes the past features and (2). a dynamic gate to adjust the network in a predictive fashion with negligible extra cost. To ensure the generality of each subnet and the gate's fairness, we propose a two-stage training scheme. We first train a weight-sharing supernet and then jointly train fusion modules and gates. Evaluation of the video detection task with the modern EfficientDet reveals the effectiveness of our approach.",
    "code_link": ""
  },
  "cvpr2022_ecv_conjugateaddernet(caddnet)-aspace-efficientapproximatecnn": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Conjugate Adder Net (CAddNet) - A Space-Efficient Approximate CNN",
    "authors": [
      "Lulan Shen",
      "Maryam Ziaeefard",
      "Brett Meyer",
      "Warren Gross",
      "James J. Clark"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Shen_Conjugate_Adder_Net_CAddNet_-_A_Space-Efficient_Approximate_CNN_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Shen_Conjugate_Adder_Net_CAddNet_-_A_Space-Efficient_Approximate_CNN_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The AdderNet was recently developed as a way to implement deep neural networks without needing multiplication operations to combine weights and inputs. Instead, absolute values of the difference between weights and inputs are used, greatly reducing the gate-level implementation complexity. Training of AdderNets is challenging, however, and the loss curves during training tend to fluctuate significantly. In this paper we propose the Conjugate Adder Network, or CAddNet, which uses the difference between the absolute values of conjugate pairs of inputs and the weights. We show that this can be implemented simply via a single minimum operation, resulting in a roughly 50% reduction in logic gate complexity as compared with AdderNets. The CAddNet method also stabilizes training as compared with AdderNets, yielding training curves similar to standard CNNs.",
    "code_link": ""
  },
  "cvpr2022_ecv_simulatedquantization,realpowersavings": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Simulated Quantization, Real Power Savings",
    "authors": [
      "Mart van Baalen",
      "Brian Kahne",
      "Eric Mahurin",
      "Andrey Kuzmin",
      "Andrii Skliar",
      "Markus Nagel",
      "Tijmen Blankevoort"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/van_Baalen_Simulated_Quantization_Real_Power_Savings_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/van_Baalen_Simulated_Quantization_Real_Power_Savings_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Reduced precision hardware-based matrix multiplication accelerators are commonly employed to reduce power consumption of neural network inference. Multiplier designs used in such accelerators possess an interesting property: When the same bit is 0 for two consecutive compute cycles, the multiplier consumes less power. In this paper we show that this effect can be used to reduce power consumption of neural networks by simulating low bit-width quantization on higher bit-width hardware. We show that simulating 4 bit quantization on 8 bit hardware can yield up to 17% relative reduction in power consumption on commonly used networks. Furthermore, we show that in this context, bit operations (BOPs) are a good proxy for power efficiency, and that learning mixed-precision configurations that target lower BOPs can achieve better trade-offs between accuracy and power efficiency.",
    "code_link": ""
  },
  "cvpr2022_ecv_alowmemoryfootprintquantizedneuralnetworkfordepthcompletionofverysparsetime-of-flightdepthmaps": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "A Low Memory Footprint Quantized Neural Network for Depth Completion of Very Sparse Time-of-Flight Depth Maps",
    "authors": [
      "Xiaowen Jiang",
      "Valerio Cambareri",
      "Gianluca Agresti",
      "Cynthia Ifeyinwa Ugwu",
      "Adriano Simonetto",
      "Fabien Cardinaux",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Jiang_A_Low_Memory_Footprint_Quantized_Neural_Network_for_Depth_Completion_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Jiang_A_Low_Memory_Footprint_Quantized_Neural_Network_for_Depth_Completion_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Sparse active illumination enables precise time-of-flight depth sensing as it maximizes signal-to-noise ratio for low power budgets. However, depth completion is required to produce dense depth maps for 3D perception. We address this task with realistic illumination and sensor resolution constraints by simulating ToF datasets for indoor 3D perception with challenging sparsity levels. We propose a quantized convolutional encoder-decoder network for this task. Our model achieves optimal depth map quality by means of input pre-processing and carefully tuned training with a geometry-preserving loss function. We also achieve low memory footprint for weights and activations by means of mixed precision quantization-at-training techniques. The resulting quantized models are comparable to the state of the art in terms of quality, but they require very low GPU times and achieve up to 14-fold memory size reduction for the weights w.r.t. their floating point counterpart with minimal impact on quality metrics.",
    "code_link": "https://github.com/sony/ai"
  },
  "cvpr2022_ecv_tormentordeterministicdynamic-path,dataaugmentationswithfractals": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "TorMentor: Deterministic Dynamic-Path, Data Augmentations With Fractals",
    "authors": [
      "Anguelos Nicolaou",
      "Vincent Christlein",
      "Edgar Riba",
      "Jian Shi",
      "Georg Vogeler",
      "Mathias Seuret"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Nicolaou_TorMentor_Deterministic_Dynamic-Path_Data_Augmentations_With_Fractals_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Nicolaou_TorMentor_Deterministic_Dynamic-Path_Data_Augmentations_With_Fractals_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose the use of fractals as mean of efficient data augmentation. Specifically, we employ plasma fractals as a means of adapting global image augmentation transformations into continuous local transforms. We formulate the diamond square algorithm as a cascade of simple convolution operations allowing efficient computation of plasma fractals on the GPU. We present the TorMentor image augmentation framework that is totally modular and deterministic across images and point-clouds. All image augmentation operations can be combined through pipelining and random branching to form flow networks of arbitrary width and depth. We demonstrate the efficiency of the proposed approach with experiments on document image segmentation (binarization) with the DIBCO datasets. The proposed approach demonstrates superior performance to traditional image augmentation techniques. Finally, we use extended synthetic binary text images in a self-supervision regiment and outperform the same model when trained with limited data and simple extensions.",
    "code_link": "https://github.com/anguelos/tormentor"
  },
  "cvpr2022_ecv_areaundertheroccurvemaximizationformetriclearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Area Under the ROC Curve Maximization for Metric Learning",
    "authors": [
      "Bojana Gaji\u0107",
      "Ariel Amato",
      "Ramon Baldrich",
      "Joost van de Weijer",
      "Carlo Gatta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Gajic_Area_Under_the_ROC_Curve_Maximization_for_Metric_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Gajic_Area_Under_the_ROC_Curve_Maximization_for_Metric_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Most popular metric learning losses have no direct relation with the evaluation metrics that are subsequently applied to evaluate their performance. We hypothesize that training a metric learning model by maximizing the area under the ROC curve (which is a typical performance measure of recognition systems) can induce an implicit ranking suitable for retrieval problems. This hypothesis is supported by previous work that proved that a curve dominates in ROC space if and only if it dominates in Precision-Recall space. To test this hypothesis, we design and maximize an approximated, derivable relaxation of the area under the ROC curve. The proposed AUC loss achieves state-of-the-art results on two large scale retrieval benchmark datasets (Stanford Online Products and DeepFashion In-Shop). Moreover, the AUC loss achieves comparable performance to more complex, domain specific, state-of-the-art methods for vehicle re-identification.",
    "code_link": ""
  },
  "cvpr2022_ecv_linearcombinationapproximationoffeatureforchannelpruning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Linear Combination Approximation of Feature for Channel Pruning",
    "authors": [
      "Donggyu Joo",
      "Doyeon Kim",
      "Eojindl Yi",
      "Junmo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Joo_Linear_Combination_Approximation_of_Feature_for_Channel_Pruning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Joo_Linear_Combination_Approximation_of_Feature_for_Channel_Pruning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Network pruning is an effective method that reduces the computation of neural networks while maintaining high performance. This enables the operation of deep neural networks in resource-limited environments. In a general large network, the roles of each channel often inevitably overlap with those of others. Therefore, for more effective pruning, it is important to observe the correlation between features in the network. In this paper, we propose a novel channel pruning method, namely, the linear combination approximation of features (LCAF). We approximate each feature map by a linear combination of other feature maps in the same layer, and then remove the most approximated one. Additionally, by exploiting the linearity of the convolution operation, we propose a supporting method called weight modification, to further reduce the loss change that occurs during pruning. Extensive experiments show that LCAF achieves state-of-the-art performance in several benchmarks. Furthermore, ablations on the LCAF demonstrate the effectiveness of our approach in a variety of ways.",
    "code_link": ""
  },
  "cvpr2022_ecv_tinyopsimagenetscaledeeplearningonmicrocontrollers": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "TinyOps: ImageNet Scale Deep Learning on Microcontrollers",
    "authors": [
      "Sulaiman Sadiq",
      "Jonathon Hare",
      "Partha Maji",
      "Simon Craske",
      "Geoff V. Merrett"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Sadiq_TinyOps_ImageNet_Scale_Deep_Learning_on_Microcontrollers_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sadiq_TinyOps_ImageNet_Scale_Deep_Learning_on_Microcontrollers_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep Learning on microcontroller (MCU) based IoT devices is extremely challenging due to memory constraints. Prior approaches focus on using internal memory or external memories exclusively which limit either accuracy or latency. We find that a hybrid method using internal and external MCU memories outperforms both approaches in accuracy and latency. We develop TinyOps, an inference engine which accelerates inference latency of models in slow external memory, using a partitioning and overlaying scheme via the available Direct Memory Access (DMA) peripheral to combine the advantages of external memory (size) and internal memory (speed). Experimental results show that architectures deployed with TinyOps significantly outperform models designed for internal memory with up to 6% higher accuracy and importantly, 1.3-2.2x faster inference latency to set the state-of-the-art in TinyML ImageNet classification. Our work shows that the TinyOps space is more efficient compared to the internal or external memory design spaces and should be explored further for TinyML applications.",
    "code_link": ""
  },
  "cvpr2022_ecv_semi-supervisedfew-shotlearningfromadependency-discriminantperspective": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Semi-Supervised Few-Shot Learning From a Dependency-Discriminant Perspective",
    "authors": [
      "Zejiang Hou",
      "Sun-Yuan Kung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Hou_Semi-Supervised_Few-Shot_Learning_From_a_Dependency-Discriminant_Perspective_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Hou_Semi-Supervised_Few-Shot_Learning_From_a_Dependency-Discriminant_Perspective_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We study the few-shot learning (FSL) problem, where a model learns to recognize new objects with extremely few labeled training data per category. Most of previous FSL approaches resort to the meta-learning paradigm, where the model accumulates inductive bias through learning from many training tasks, in order to solve new unseen few-shot tasks. In contrast, we propose a simple semi-supervised FSL approach to exploit unlabeled data accompanying the few-shot task to improve FSL performance. More exactly, to train a classifier, we propose a Dependency Maximization loss based on the Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the statistical dependency between the embedded feature of the unlabeled data and their label predictions, together with the supervised loss over the support set. The obtained classifier is used to infer the pseudo-labels of the unlabeled data. Furthermore, we propose an Instance Discriminant Analysis to evaluate the credibility of the pseudo-labeled examples and select the faithful ones into an augmented support set, which is used to retrain the classifier. We iterate the process until the pseudo-labels of the unlabeled data becomes stable. Through extensive experiments on four widely used few-shot classification benchmarks, including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS, the proposed method outperforms previous state-of-the-art FSL methods.",
    "code_link": ""
  },
  "cvpr2022_ecv_momentumcontrastivepruning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Momentum Contrastive Pruning",
    "authors": [
      "Siyuan Pan",
      "Yiming Qin",
      "Tingyao Li",
      "Xiaoshuang Li",
      "Liang Hou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Pan_Momentum_Contrastive_Pruning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Pan_Momentum_Contrastive_Pruning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Momentum contrast (MoCo) for unsupervised visual representation learning has a close performance to supervised learning, but it sometimes possesses excess parameters. Extracting a subnetwork from an over-parameterized unsupervised network without sacrificing performance is of particular interest to accelerate inference speed. Typical pruning methods are not applicable for MoCo, because in the fine-tune stage after pruning, the slow update of the momentum encoder will undermine the pretrained encoder. In this paper, we propose a Momentum Contrastive Pruning (MCP) method, which prunes the momentum encoder instead to obtain a momentum subnet. It maintains an un-pruned momentum encoder as a smooth transition scheme to alleviate the representation gap between the encoder and momentum subnet. To fulfill the sparsity requirements of the encoder, alternating direction method of multipliers (ADMM) is adopted. Experiments prove that our MCP method can obtain a momentum subnet that has almost equal performance as the over-parameterized MoCo when transferred to downstream tasks, meanwhile has much less parameters and float operations per second (FLOPs).",
    "code_link": ""
  },
  "cvpr2022_ecv_cyclicalpruningforsparseneuralnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Cyclical Pruning for Sparse Neural Networks",
    "authors": [
      "Suraj Srinivas",
      "Andrey Kuzmin",
      "Markus Nagel",
      "Mart van Baalen",
      "Andrii Skliar",
      "Tijmen Blankevoort"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Srinivas_Cyclical_Pruning_for_Sparse_Neural_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Srinivas_Cyclical_Pruning_for_Sparse_Neural_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Current methods for pruning neural network weights iteratively apply magnitude-based pruning on the model weights and re-train the resulting model to recover lost accuracy. In this work, we show that such strategies do not allow for the recovery of erroneously pruned weights. To enable weight recovery, we propose a simple strategy called cyclical pruning which requires the pruning schedule to be periodic and allows for weights pruned erroneously in one cycle to recover in subsequent ones. Experimental results on both linear models and large-scale deep neural networks show that cyclical pruning outperforms existing pruning algorithms, especially at high sparsity ratios. Our approach is easy to tune and can be readily incorporated into existing pruning pipelines to boost performance.",
    "code_link": ""
  },
  "cvpr2022_ecv_eventtransformer.asparse-awaresolutionforefficienteventdataprocessing": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Event Transformer. A Sparse-Aware Solution for Efficient Event Data Processing",
    "authors": [
      "Alberto Sabater",
      "Luis Montesano",
      "Ana C. Murillo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Sabater_Event_Transformer._A_Sparse-Aware_Solution_for_Efficient_Event_Data_Processing_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sabater_Event_Transformer._A_Sparse-Aware_Solution_for_Efficient_Event_Data_Processing_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Event cameras are sensors of great interest for many applications that run in low-resource and challenging environments. They log sparse illumination changes with high temporal resolution and high dynamic range, while they present minimal power consumption. However, top-performing methods often ignore specific event-data properties, leading to the development of generic but computationally expensive algorithms. Efforts toward efficient solutions usually do not achieve top-accuracy results for complex tasks. This work proposes a novel framework, Event Transformer (EvT), that effectively takes advantage of event-data properties to be highly efficient and accurate. We introduce a new patch-based event representation and a compact transformer-like architecture to process it. EvT is evaluated on different event-based benchmarks for action and gesture recognition. Evaluation results show better or comparable accuracy to the state-of-the-art while requiring significantly less computation resources, which makes EvT able to work with minimal latency both on GPU and CPU.",
    "code_link": "https://github.com/AlbertoSabater/EventTransformer"
  },
  "cvpr2022_ecv_whennasmeetstreesanefficientalgorithmforneuralarchitecturesearch": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "When NAS Meets Trees: An Efficient Algorithm for Neural Architecture Search",
    "authors": [
      "Guocheng Qian",
      "Xuanyang Zhang",
      "Guohao Li",
      "Chen Zhao",
      "Yukang Chen",
      "Xiangyu Zhang",
      "Bernard Ghanem",
      "Jian Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Qian_When_NAS_Meets_Trees_An_Efficient_Algorithm_for_Neural_Architecture_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Qian_When_NAS_Meets_Trees_An_Efficient_Algorithm_for_Neural_Architecture_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space and substantially reduce the exploration size. TNAS performs a modified bi-level Breadth-First Search in the proposed trees to discover a high-performance architecture. Impressively, TNAS finds the global optimal architecture on CIFAR-10 with test accuracy of 94.37% in four GPU hours in NAS-Bench-201. The average test accuracy is 94.35%, which outperforms the state-of-the-art. Code is available at: https://github.com/guochengqian/TNAS.",
    "code_link": "https://github.com/guochengqian/TNAS"
  },
  "cvpr2022_ecv_integratingposeandmaskpredictionsformulti-personinvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Integrating Pose and Mask Predictions for Multi-Person in Videos",
    "authors": [
      "Miran Heo",
      "Sukjun Hwang",
      "Seoung Wug Oh",
      "Joon-Young Lee",
      "Seon Joo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Heo_Integrating_Pose_and_Mask_Predictions_for_Multi-Person_in_Videos_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Heo_Integrating_Pose_and_Mask_Predictions_for_Multi-Person_in_Videos_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In real-world applications for video editing, humans are arguably the most important objects. When editing videos of humans, the efficient tracking of fine-grained masks and body joints is the fundamental requirement. In this paper, we propose a simple and efficient system for jointly tracking pose and segmenting high-quality masks for all humans in the video. We design a pipeline that globally tracks pose and locally segments fine-grained masks. Specifically, CenterTrack is first employed to track human poses by viewing the whole scene, and then the proposed local segmentation network leverages the pose information as a powerful query to carry out high-quality segmentation. Furthermore, we adopt a highly light-weight MLP-Mixer layer within the segmentation network that can efficiently propagate the query pose throughout the region of interest with minimal overhead. For the evaluation, we collect a new benchmark called KineMask which includes various appearances and actions. The experimental results demonstrate that our method has superior fine-grained segmentation performance. Moreover, it runs at 33 fps, achieving a great balance of speed and accuracy compared to the prevailing online Video Instance Segmentation methods.",
    "code_link": ""
  },
  "cvpr2022_ecv_discriminability-enforcinglosstoimproverepresentationlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Discriminability-Enforcing Loss To Improve Representation Learning",
    "authors": [
      "Florinel-Alin Croitoru",
      "Diana-Nicoleta Grigore",
      "Radu Tudor Ionescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Croitoru_Discriminability-Enforcing_Loss_To_Improve_Representation_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Croitoru_Discriminability-Enforcing_Loss_To_Improve_Representation_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "During the training process, deep neural networks implicitly learn to represent the input data samples through a hierarchy of features, where the size of the hierarchy is determined by the number of layers. In this paper, we focus on enforcing the discriminative power of the high-level representations, that are typically learned by the deeper layers (closer to the output). To this end, we introduce a new loss term inspired by the Gini impurity, which is aimed at minimizing the entropy (increasing the discriminative power) of individual high-level features with respect to the class labels. Although our Gini loss induces highly-discriminative features, it does not ensure that the distribution of the high-level features matches the distribution of the classes. As such, we introduce another loss term to minimize the Kullback-Leibler divergence between the two distributions. We conduct experiments on two image classification data sets (CIFAR-100 and Caltech 101), considering multiple neural architectures ranging from convolutional networks (ResNet-17, ResNet-18, ResNet-50) to transformers (CvT). Our empirical results show that integrating our novel loss terms into the training objective consistently outperforms the models trained with cross-entropy alone, without increasing the inference time at all.",
    "code_link": ""
  },
  "cvpr2022_ecv_peaimprovingtheperformanceofrelunetworksforfreebyusingprogressiveensembleactivations": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "PEA: Improving the Performance of ReLU Networks for Free by Using Progressive Ensemble Activations",
    "authors": [
      "\u00c1kos Utasi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Utasi_PEA_Improving_the_Performance_of_ReLU_Networks_for_Free_by_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Utasi_PEA_Improving_the_Performance_of_ReLU_Networks_for_Free_by_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In recent years novel activation functions have been proposed to improve the performance of neural networks, and they show superior performance compared to the ReLU counterpart. However, there are environments, where the availability of complex activations is limited, and usually only the ReLU is supported. In this paper we propose methods that can be used to improve the performance of ReLU networks by using these efficient novel activations during model training. More specifically, we propose ensemble activations that are composed of the ReLU and one of these novel activations. Furthermore, the coefficients of the ensemble are neither fixed nor learned, but are progressively updated during the training process in a way that by the end of the training only the ReLU activations remain active in the network and the other activations can be removed. This means that in inference time the network contains ReLU activations only. We perform extensive evaluations on the ImageNet classification task using various compact network architectures and various novel activation functions. Results show 0.2-0.8% top-1 accuracy gain, which confirms the applicability of the proposed methods. Furthermore, we demonstrate the proposed methods on semantic segmentation and we boost the performance of a compact segmentation network by 0.34% mIOU on the Cityscapes dataset.",
    "code_link": "https://github.com/facebookarchive/fb.resnet"
  },
  "cvpr2022_ecv_towardsefficientfeaturesharinginmimoarchitectures": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Towards Efficient Feature Sharing in MIMO Architectures",
    "authors": [
      "R\u00e9my Sun",
      "Alexandre Ram\u00e9",
      "Cl\u00e9ment Masson",
      "Nicolas Thome",
      "Matthieu Cord"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Sun_Towards_Efficient_Feature_Sharing_in_MIMO_Architectures_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sun_Towards_Efficient_Feature_Sharing_in_MIMO_Architectures_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-input multi-output architectures propose to train multiple subnetworks within one base network and then average the subnetwork predictions to benefit from ensembling for free. Despite some relative success, these architectures are wasteful in their use of parameters. Indeed, we highlight in this paper that the learned subnetwork fail to share even generic features which limits their applicability on smaller mobile and AR/VR devices. We posit this behavior stems from an ill-posed part of the multi-input multi-output framework. To solve this issue, we propose a novel unmixing step in MIMO architectures that allows subnetworks to properly share features. Preliminary experiments on CIFAR 100 show our adjustments allow feature sharing and improve model performance for small architectures.",
    "code_link": ""
  },
  "cvpr2022_ecv_squeezenerffurtherfactorizedfastnerfformemory-efficientinference": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "SqueezeNeRF: Further Factorized FastNeRF for Memory-Efficient Inference",
    "authors": [
      "Krishna Wadhwani",
      "Tamaki Kojima"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Wadhwani_SqueezeNeRF_Further_Factorized_FastNeRF_for_Memory-Efficient_Inference_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Wadhwani_SqueezeNeRF_Further_Factorized_FastNeRF_for_Memory-Efficient_Inference_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for novel view generation of complex scenes, but is very slow during inference. Recently, there have been multiple works on speeding up NeRF inference, but the state of the art methods for real-time NeRF inference rely on caching the neural network output, which occupies several giga-bytes of disk space that limits their real-world applicability. As caching the neural network of original NeRF network is not feasible, Garbin et.al. proposed \"FastNeRF\" which factorizes the problem into 2 sub-networks - one which depends only on the 3D coordinate of a sample point and one which depends only on the 2D camera viewing direction. Although this factorization enables them to reduce the cache size and perform inference at over 200 frames per second, the memory overhead is still substantial. In this work, we propose SqueezeNeRF, which is more than 60 times memory-efficient than the sparse cache of FastNeRF and is still able to render at more than 190 frames per second on a high spec GPU during inference.",
    "code_link": ""
  },
  "cvpr2022_ecv_simpleandefficientarchitecturesforsemanticsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Simple and Efficient Architectures for Semantic Segmentation",
    "authors": [
      "Dushyant Mehta",
      "Andrii Skliar",
      "Haitam Ben Yahia",
      "Shubhankar Borse",
      "Fatih Porikli",
      "Amirhossein Habibian",
      "Tijmen Blankevoort"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Mehta_Simple_and_Efficient_Architectures_for_Semantic_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Mehta_Simple_and_Efficient_Architectures_for_Semantic_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Though the state-of-the architectures for semantic segmentation, such as HRNet, demonstrate impressive accuracy, the complexity arising from their salient design choices hinders a range of model acceleration tools, and further they make use of operations that are inefficient on current hardware. This paper demonstrates that a simple encoder-decoder architecture with a ResNet-like backbone and a small multi-scale head, performs on-par or better than complex semantic segmentation architectures such as HRNet, FANet and DDRNets. Naively applying deep backbones designed for Image Classification to the task of Semantic Segmentation leads to sub-par results, owing to a much smaller effective receptive field of these backbones. Implicit among the various design choices put forth in works like HRNet, DDRNet, and FANet are networks with a large effective receptive field. It is natural to ask if a simple encoder-decoder architecture would compare favorably if comprised of backbones that have a larger effective receptive field, though without the use of inefficient operations like dilated convolutions. We show that with minor and inexpensive modifications to ResNets enlarging the receptive field, very simple and competitive baselines can be created for Semantic Segmentation. We present a family of such simple architectures for desktop as well as mobile targets, which match or exceed the performance of complex models on the Cityscapes dataset. We hope that our work provides simple yet effective baselines for practitioners to develop efficient semantic segmentation models. The model definitions and pre-trained weights are available at https://github.com/Qualcomm-AI-research/FFNet.",
    "code_link": "https://github.com/NVIDIA/semantic"
  },
  "cvpr2022_fade-tcv_class-wisethresholdingforrobustout-of-distributiondetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Class-Wise Thresholding for Robust Out-of-Distribution Detection",
    "authors": [
      "Matteo Guarrera",
      "Baihong Jin",
      "Tung-Wei Lin",
      "Maria A. Zuluaga",
      "Yuxin Chen",
      "Alberto Sangiovanni-Vincentelli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Guarrera_Class-Wise_Thresholding_for_Robust_Out-of-Distribution_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Guarrera_Class-Wise_Thresholding_for_Robust_Out-of-Distribution_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We consider the problem of detecting Out-of-Distribution(OoD) input data when using deep neural networks, and we propose a simple yet effective way to improve the robustness of several popular OoD detection methods against label shift. Our work is motivated by the observation that most existing OoD detection algorithms consider all training/test data as a whole, regardless of which class entry each input activates (inter-class differences). Through extensive experimentation, we have found that such practice leads to a detector whose performance is sensitive and vulnerable to label shift. To address this issue, we propose a class-wise thresholding scheme that can apply to most existing OoD detection algorithms and can maintain similar OoD detection performance even in the presence of label shift in the test distribution.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_medxganvisualexplanationsformedicalclassifiersthroughagenerativelatentspace": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "medXGAN: Visual Explanations for Medical Classifiers Through a Generative Latent Space",
    "authors": [
      "Amil Dravid",
      "Florian Schiffers",
      "Boqing Gong",
      "Aggelos K. Katsaggelos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Dravid_medXGAN_Visual_Explanations_for_Medical_Classifiers_Through_a_Generative_Latent_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Dravid_medXGAN_Visual_Explanations_for_Medical_Classifiers_Through_a_Generative_Latent_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite the surge of deep learning in the past decade, some users are skeptical to deploy these models in practice due to their black-box nature. Specifically, in the medical space where there are severe potential repercussions, we need to develop methods to gain confidence in the models' decisions. To this end, we propose a novel medical imaging generative adversarial framework, medXGAN (medical eXplanation GAN), to visually explain what a medical classifier focuses on in its binary predictions. By encoding domain knowledge of medical images, we are able to disentangle anatomical structure and pathology, leading to fine-grained visualization through latent interpolation. Furthermore, we optimize the latent space such that interpolation explains how the features contribute to the classifier's output. Our method outperforms baselines such as Gradient-Weighted Class Activation Mapping (Grad-CAM) and Integrated Gradients in localization and explanatory ability. Additionally, a combination of the medXGAN with Integrated Gradients can yield explanations more robust to noise. The project page with code is available at: https://avdravid.github.io/medXGAN page/.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_anexaminationofbiasoffacialanalysisbasedbmipredictionmodels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "An Examination of Bias of Facial Analysis Based BMI Prediction Models",
    "authors": [
      "Hera Siddiqui",
      "Ajita Rattani",
      "Karl Ricanek",
      "Twyla Hill"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Siddiqui_An_Examination_of_Bias_of_Facial_Analysis_Based_BMI_Prediction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Siddiqui_An_Examination_of_Bias_of_Facial_Analysis_Based_BMI_Prediction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Obesity is one of the most important public health problems that the world is facing today. A recent trend is in the development of intervention tools that predict BMI using facial images for weight monitoring and management to combat obesity. Most of these studies used BMI annotated facial image datasets that mainly consisted of Caucasian subjects. Research on bias evaluation of face-based gender-, age-classification, and face recognition systems suggest that these technologies perform poorly for women, dark-skinned people, and older adults. The bias of facial analysis-based BMI prediction tools has not been systematically studied until now. This paper evaluates the bias of facial-analysis-based BMI prediction models across Caucasian and African-American Males and Females. Experimental investigations on the gender, race, and BMI balanced version of the modified MORPH-II dataset suggested that the error rate in BMI prediction was least for Black Males and highest for White Females. Further, the psychology-related facial features correlated with weight suggested that as the BMI increases, the changes in the facial region are more prominent for Black Males and the least for White Females. This is the reason for the least error rate of the facial analysis-based BMI prediction tool for Black Males and highest for White Females.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_opadanoptimizedpolicy-basedactivelearningframeworkfordocumentcontentanalysis": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "OPAD: An Optimized Policy-Based Active Learning Framework for Document Content Analysis",
    "authors": [
      "Sumit Shekhar",
      "Bhanu Prakash Reddy Guda",
      "Ashutosh Chaubey",
      "Ishan Jindal",
      "Avneet Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Shekhar_OPAD_An_Optimized_Policy-Based_Active_Learning_Framework_for_Document_Content_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Shekhar_OPAD_An_Optimized_Policy-Based_Active_Learning_Framework_for_Document_Content_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Documents are central to many business systems, and include forms, reports, contracts, invoices or purchase orders. The information in documents is typically in natural language, but can be organized in various layouts and formats. There have been recent spurt of interest in understanding document content with novel deep learning architectures. However, document understanding tasks need dense information annotations, which are costly to scale and generalize. Several active learning techniques have been proposed to reduce the overall budget of annotation while maintaining the performance of the underlying deep learning model. However, most of these techniques work only for classification problems. But content detection is a more complex task, and has been scarcely explored in active learning literature. In this paper, we propose OPAD, a novel framework using reinforcement policy for active learning in content detection tasks for documents. The proposed framework learns the acquisition function to decide the samples to be selected while optimizing performance metrics that the tasks typically have. Furthermore, we extend to weak labelling scenarios to further reduce the cost of annotation significantly. We propose novel rewards to account for class imbalance and user feedback in the annotation interface, to improve the active learning method. We show superior performance of the proposed OPAD framework for active learning for various tasks related to document understanding like layout parsing and object detection. Ablation studies for human feedback and class imbalance rewards are presented, along with a comparison of annotation times for different approaches.",
    "code_link": "https://github.com/facebookresearch/detectron2"
  },
  "cvpr2022_fade-tcv_doppelgangersaliencytowardsmoreethicalpersonre-identification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Doppelganger Saliency: Towards More Ethical Person Re-Identification",
    "authors": [
      "Brandon RichardWebster",
      "Brian Hu",
      "Keith Fieldhouse",
      "Anthony Hoogs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/RichardWebster_Doppelganger_Saliency_Towards_More_Ethical_Person_Re-Identification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/RichardWebster_Doppelganger_Saliency_Towards_More_Ethical_Person_Re-Identification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Modern surveillance systems have become increasingly dependent on artificial intelligence to provide actionable information for real-time decision making. A critical question relates to how these systems handle difficult ethical dilemmas, such as the re-identification of similar looking individuals. Potential misidentification of individuals can have severe negative consequences, as evidenced by recent headlines of individuals who were wrongly targeted for crimes they did not commit based on false matches. A computer vision-based saliency algorithm is proposed to help identify pixel-level differences in pairs of images containing visually similar individuals, which we term \"doppelgangers.\" The computed saliency maps can alert human users of the presence of doppelgangers and provide important visual evidence to reduce the potential of false matches in these high-stakes situations. We show both qualitative and quantitative saliency results on doppelgangers found in a video-based person re-identification dataset (MARS) using three different state-of-the-art models. Our results suggest that this novel use of visual saliency can improve overall outcomes by helping human users in the person re-identification setting, while assuring the ethical and trusted operation of surveillance systems.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_segmentingacrossplacestheneedforfairtransferlearningwithsatelliteimagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Segmenting Across Places: The Need for Fair Transfer Learning With Satellite Imagery",
    "authors": [
      "Miao Zhang",
      "Harvineet Singh",
      "Lazarus Chok",
      "Rumi Chunara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Zhang_Segmenting_Across_Places_The_Need_for_Fair_Transfer_Learning_With_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Zhang_Segmenting_Across_Places_The_Need_for_Fair_Transfer_Learning_With_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The increasing availability of high-resolution satellite imagery has enabled the use of machine learning to support land-cover measurement and inform policy-making. However, labelling satellite images is expensive and is available for only some locations. This prompts the use of transfer learning to adapt models from data-rich locations to others. Given the potential for high-impact applications of satellite imagery across geographies, a systematic assessment of transfer learning implications is warranted. In this work, we consider the task of land-cover segmentation and study the fairness implications of transferring models across locations. We leverage a large satellite image segmentation benchmark with 5987 images from 18 districts (9 urban and 9 rural). Via fairness metrics we quantify disparities in model performance along two axes -- across urban-rural locations and across land-cover classes. Findings show that state-of-the-art models have better overall accuracy in rural areas compared to urban areas, through unsupervised domain adaptation methods transfer learning better to urban versus rural areas and enlarge fairness gaps. In analysis of reasons for these findings, we show that raw satellite images are overall more dissimilar between source and target districts for rural than for urban locations. This work highlights the need to conduct fairness analysis for satellite imagery segmentation models and motivates the development of methods for fair transfer learning in order not to introduce disparities between places, particularly urban and rural locations.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_isneuroncoverageneededtomakepersondetectionmorerobust?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Is Neuron Coverage Needed To Make Person Detection More Robust?",
    "authors": [
      "Svetlana Pavlitskaya",
      "\u015eiyar Y\u0131km\u0131\u015f",
      "J. Marius Z\u00f6llner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Pavlitskaya_Is_Neuron_Coverage_Needed_To_Make_Person_Detection_More_Robust_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Pavlitskaya_Is_Neuron_Coverage_Needed_To_Make_Person_Detection_More_Robust_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The growing use of deep neural networks (DNNs) in safety- and security-critical areas like autonomous driving raises the need for their systematic testing. Coverage-guided testing (CGT) is an approach that applies mutation or fuzzing according to a predefined coverage metric to find inputs that cause misbehavior. With the introduction of a neuron coverage metric, CGT has also recently been applied to DNNs. In this work, we apply CGT to the task of person detection in crowded scenes. The proposed pipeline uses YOLOv3 for person detection and includes finding DNN bugs via sampling and mutation, and subsequent DNN retraining on the updated training set. To be a bug, we require a mutated image to cause a significant performance drop compared to a clean input. In accordance with the CGT, we also consider an additional requirement of increased coverage in the bug definition. In order to explore several types of robustness, our approach includes natural image transformations, corruptions, and adversarial examples generated with the Daedalus attack. The proposed framework has uncovered several thousand cases of incorrect DNN behavior. The relative change in mAP performance of the retrained models reached on average between 26.21% and 64.24% for different robustness types. However, we have found no evidence that the investigated coverage metrics can be advantageously used to improve robustness.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_colorinvariantskinsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Color Invariant Skin Segmentation",
    "authors": [
      "Han Xu",
      "Abhijit Sarkar",
      "A. Lynn Abbott"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Xu_Color_Invariant_Skin_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Xu_Color_Invariant_Skin_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper addresses the problem of automatically detecting human skin in images without reliance on color information. A primary motivation of the work has been to achieve results that are consistent across the full range of skin tones, even while using a training dataset that is significantly biased toward lighter skin tones. Previous skin-detection methods have used color cues almost exclusively, and we present a new approach that performs well in the absence of such information. A key aspect of the work is dataset repair through augmentation that is applied strategically during training, with the goal of color invariant feature learning to enhance generalization. We have demonstrated the concept using two architectures, and experimental results show improvements in both precision and recall for most Fitzpatrick skin tones in the benchmark ECU dataset. We further tested the system with the RFW dataset to show that the proposed method performs much more consistently across different ethnicities, thereby reducing the chance of bias based on skin color. To demonstrate the effectiveness of our work, extensive experiments were performed on grayscale images as well as images obtained under unconstrained illumination and with artificial filters. Source code will be provided with the final version of this paper.",
    "code_link": "https://github.com/HanXuMartin/Color-InvariantSkin-Segmentation"
  },
  "cvpr2022_fade-tcv_epistemicuncertainty-weightedlossforvisualbiasmitigation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Epistemic Uncertainty-Weighted Loss for Visual Bias Mitigation",
    "authors": [
      "Rebecca S Stone",
      "Nishant Ravikumar",
      "Andrew J Bulpitt",
      "David C Hogg"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Stone_Epistemic_Uncertainty-Weighted_Loss_for_Visual_Bias_Mitigation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Stone_Epistemic_Uncertainty-Weighted_Loss_for_Visual_Bias_Mitigation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Deep neural networks are highly susceptible to learning biases in visual data. While various methods have been proposed to mitigate such bias, the majority require explicit knowledge of the biases present in the training data in order to mitigate. We argue the relevance of exploring methods which are completely ignorant of the presence of any bias, but are capable of identifying and mitigating them. Furthermore, we propose using Bayesian neural networks with an epistemic uncertainty-weighted loss function to dynamically identify potential bias in individual training samples and to weight them during training. We find a positive correlation between samples subject to bias and higher epistemic uncertainties. Finally, we show the method has potential to mitigate visual bias on a bias benchmark dataset and on a real-world face detection problem, and we consider the merits and weaknesses of our approach.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_visualdomainbridgeasource-freedomainadaptationforcross-domainfew-shotlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Visual Domain Bridge: A Source-Free Domain Adaptation for Cross-Domain Few-Shot Learning",
    "authors": [
      "Moslem Yazdanpanah",
      "Parham Moradi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Yazdanpanah_Visual_Domain_Bridge_A_Source-Free_Domain_Adaptation_for_Cross-Domain_Few-Shot_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Yazdanpanah_Visual_Domain_Bridge_A_Source-Free_Domain_Adaptation_for_Cross-Domain_Few-Shot_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Due to the covariate shift, deep neural networks performance always degrades when applied to novel domains. In order to mitigate this problem, domain adaptation techniques require samples from target data during the feature extraction training, which is not always applicable in realworld scenarios. Batch Normalization is a known component of computer vision models, aiming at reducing the training-time covariate shift. However, facing distribution shift results in an internal state mismatch inside the BatchNorm layers during the inference time. In favor of alleviating the induced mismatch, this paper proposes a sourcefree, lightweight and straightforward approach by introducing the \"Visual Domain Bridge\" concept reducing the BatchNorm's internal mismatch in the cross-domain settings. Compared to the other BatchNorm-based sourcefree domain adaptation techniques such as AdaBN and Prediction-BN, our method formed a new state-of-the-art cross-domain few-shot fine-tuning method neglecting extra augmentations; while improving the performance in neardomain settings too. The proposed method can integrate with other domain adaptation methods and enhance their performance requiring just a few lines of modification in the BatchNorm's implementation. Implementations are available in https://github.com/MosyMosy/VDB.",
    "code_link": "https://github.com/MosyMosy/VDB"
  },
  "cvpr2022_fade-tcv_pyramidalattentionforsaliencydetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Pyramidal Attention for Saliency Detection",
    "authors": [
      "Tanveer Hussain",
      "Abbas Anwar",
      "Saeed Anwar",
      "Lars Petersson",
      "Sung Wook Baik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Hussain_Pyramidal_Attention_for_Saliency_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Hussain_Pyramidal_Attention_for_Saliency_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Salient object detection (SOD) extracts meaningful contents from an input image. RGB-based SOD methods lack the complementary depth clues; hence, providing limited performance for complex scenarios. Similarly, RGB-D models process RGB and depth inputs, but the depth data availability during testing may hinder the model's practical applicability. This paper exploits only RGB images, estimates depth from RGB, and leverages the intermediate depth features. We employ a pyramidal attention structure to extract multi-level convolutional-transformer features to process initial stage representations and further enhance the subsequent ones. At each stage, the backbone transformer model produces global receptive fields and computing in parallel to attain fine-grained global predictions refined by our residual convolutional attention decoder for optimal saliency prediction. We report significantly improved performance against 21 and 40 state-of-the-art SOD methods on eight RGB and RGB-D datasets, respectively. Consequently, we present a new SOD perspective of generating RGB-D SOD without acquiring depth data during training and testing and assist RGB methods with depth clues for improved performance. The code and trained models are available at https://github.com/tanveer-hussain/EfficientSOD2.",
    "code_link": ""
  },
  "cvpr2022_fade-tcv_desideepfakesourceidentifierforsocialmedia": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FaDE-TCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "DeSI: Deepfake Source Identifier for Social Media",
    "authors": [
      "Kartik Narayan",
      "Harsh Agarwal",
      "Surbhi Mittal",
      "Kartik Thakral",
      "Suman Kundu",
      "Mayank Vatsa",
      "Richa Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Narayan_DeSI_Deepfake_Source_Identifier_for_Social_Media_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Narayan_DeSI_Deepfake_Source_Identifier_for_Social_Media_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Social media holds the power to influence a significant change in the population. Through social media, people all around the world can connect and share their views. However, this social space is now infected due to the infiltration of fraudulent, obscene, fake and possibly, influential media. According to a UNESCO report, prevalence of fake news and deepfake content possess the potential of spreading fake propaganda and can lead to political and social unrest. Trust on social media is an emerging problem and there is an urgent need to address the same. There has been some research around approaches that detect fake news and deepfakes, however, identification of the source of these deepfakes posted on social media platforms is an equally important but relatively unexplored challenge. This paper proposes a novel Deepfake Source Identification (DeSI) algorithm that identifies the sources of deepfakes posted on Twitter. The proposed DeSI algorithm allows for two input modalities - text and images. We rigorously test our algorithm in both constrained and unconstrained experimental setups and report the observed results. In the constrained setting, the algorithm correctly identifies all the deepfake tweets as well their sources. The complete framework is further encased in a web portal to facilitate intuitive use and analysis of the results.",
    "code_link": ""
  },
  "cvpr2022_dlgc_graphwalksefficientshapeagnosticgeodesicshortestpathestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "GraphWalks: Efficient Shape Agnostic Geodesic Shortest Path Estimation",
    "authors": [
      "Rolandos Alexandros Potamias",
      "Alexandros Neofytou",
      "Kyriaki Margarita Bintsi",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Potamias_GraphWalks_Efficient_Shape_Agnostic_Geodesic_Shortest_Path_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Potamias_GraphWalks_Efficient_Shape_Agnostic_Geodesic_Shortest_Path_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Geodesic paths and distances are among the most popular intrinsic properties of 3D surfaces. Traditionally, geodesic paths on discrete polygon surfaces were computed using shortest path algorithms, such as Dijkstra. However, such algorithms have two major limitations. They are non-differentiable which limits their direct usage in learnable pipelines and they are considerably time demanding. To address such limitations and alleviate the computational burden, we propose a learnable network to approximate geodesic paths. The proposed method is comprised by three major components: a graph neural network that encodes node positions in a high dimensional space, a path embedding that describes previously visited nodes and a point classifier that selects the next point in the path. The proposed method provides efficient approximations of the shortest paths and geodesic distances estimations. Given that all of the components of our method are fully differentiable, it can be directly plugged into any learnable pipeline as well as customized under any differentiable constraint. We extensively evaluate the proposed method with several qualitative and quantitative experiments.",
    "code_link": ""
  },
  "cvpr2022_dlgc_shapeenhancedkeypointslearningwithgeometricpriorfor6dobjectposetracking": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "Shape Enhanced Keypoints Learning With Geometric Prior for 6D Object Pose Tracking",
    "authors": [
      "Mateusz Majcher",
      "Bogdan Kwolek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Majcher_Shape_Enhanced_Keypoints_Learning_With_Geometric_Prior_for_6D_Object_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Majcher_Shape_Enhanced_Keypoints_Learning_With_Geometric_Prior_for_6D_Object_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Until now, there has not been much research in exploiting geometric reasoning on object shape and keypoints in object pose estimation. First, the current RGB image and quaternion representing rotation in the previous frame are fed to a multi-branch neural network responsible for regressing sparse object keypoints. The initial object pose is estimated using PnP, which is adjusted in a least-square optimization. The weights of boundary and keypoints components are determined in each iteration via geometric reasoning on the projected and segmented 3D object boundary, object shape extracted by a pretrained neural network and keypoints extracted by our network. Different from previous methods, our voting scheme is object boundary-based. We demonstrate experimentally that the accuracy of pose estimation is competitive in comparison to the accuracy of SOTA algorithms achieved on challenging YCB-Video dataset.",
    "code_link": ""
  },
  "cvpr2022_dlgc_conceptactivationvectorsforgeneratinguser-defined3dshapes": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "Concept Activation Vectors for Generating User-Defined 3D Shapes",
    "authors": [
      "Stefan Druc",
      "Aditya Balu",
      "Peter Wooldridge",
      "Adarsh Krishnamurthy",
      "Soumik Sarkar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Druc_Concept_Activation_Vectors_for_Generating_User-Defined_3D_Shapes_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Druc_Concept_Activation_Vectors_for_Generating_User-Defined_3D_Shapes_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We explore the interpretability of 3D geometric deep learning models in the context of Computer-Aided Design (CAD). The field of parametric CAD can be limited by the difficulty of expressing high-level design concepts in terms of a few numeric parameters. In this paper, we use a deep learning architectures to encode high dimensional 3D shapes into a vectorized latent representation that can be used to describe arbitrary concepts. Specifically, we train a simple auto-encoder to parameterize a dataset of complex shapes. To understand the latent encoded space, we use the idea of Concept Activation Vectors (CAV) to reinterpret the latent space in terms of user-defined concepts. This allows modification of a reference design to exhibit more or fewer characteristics of a chosen concept or group of concepts. We also test the statistical significance of the identified concepts and determine the sensitivity of a physical quantity of interest across the dataset.",
    "code_link": ""
  },
  "cvpr2022_dlgc_camioncascademulti-inputmulti-outputnetworkforskeletonextraction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "CAMION: Cascade Multi-Input Multi-Output Network for Skeleton Extraction",
    "authors": [
      "Sheng Fang",
      "Kaiyu Li",
      "Zhe Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Fang_CAMION_Cascade_Multi-Input_Multi-Output_Network_for_Skeleton_Extraction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Skeletonization is an important process of extracting the medial axis of the object shape while maintaining the original geometric and topological properties. Some recent studies have demonstrated that deep learning-based segmentation models can extract the main skeleton from objects more robustly. However, we find that the skeleton extracted by a vanilla segmentation process is always discontinuous and not accurate enough. In this paper, we propose a general cascade deep learning pipeline that achieves competitive performance only using a simple U-shape network. The semantic information contained in the shapes is limited, so we introduce a ConvNet with multi-source input and multi-task output, CAMION for short, on top of the basic shape-to-skeleton network. With the multi-source inputs, CAMION can converge faster than using only binary shapes; and with the introduction of multi-task learning, relevant and suitable auxiliary tasks (e.g., feature point detection and contour extraction) bring considerable gains for the extraction of skeleton. Our code used in Pixel SkelNetOn - CVPR 2022 challenge will be released at https://github.com/likyoo/CAMION-CVPRW2022.",
    "code_link": "https://github.com/likyoo/CAMION-CVPRW2022"
  },
  "cvpr2022_dlgc_vg-vaeavenatusgeometrypoint-cloudvariationalauto-encoder": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "VG-VAE: A Venatus Geometry Point-Cloud Variational Auto-Encoder",
    "authors": [
      "Tejas Anvekar",
      "Ramesh Ashok Tabib",
      "Dikshit Hegde",
      "Uma Mudengudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Anvekar_VG-VAE_A_Venatus_Geometry_Point-Cloud_Variational_Auto-Encoder_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Anvekar_VG-VAE_A_Venatus_Geometry_Point-Cloud_Variational_Auto-Encoder_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we propose VG-VAE: Venatus Geometric Variational Auto-Encoder for capturing unsupervised hierarchical local and global geometric signatures in pointcloud. Recent research emphasises the significance of the underlying intrinsic geometry for pointcloud processing. Our contribution is to extract and analyse the morphology of the pointcloud using the proposed Geometric Proximity Correlator (GPC) and variational sampling of the latent. The extraction of local geometric signatures is facilitated by the GPC, whereas the extraction of global geometry is facilitated by variational sampling. Furthermore, we apply a naive mix of vector algebra and 3D geometry to extract the basic per-point geometric signature, which assists the unsupervised hypothesis. We provide statistical analyses of local and global geometric signatures. The impacts of our geometric features are demonstrated on pointcloud classification as downstream task using the classic pointcloud feature extractor PointNet. We demonstrate our analysis on ModelNet40 a benchmark dataset, and compare with state-of-the-art techniques.",
    "code_link": ""
  },
  "cvpr2022_dlgc_contextattentionnetworkforskeletonextraction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "Context Attention Network for Skeleton Extraction",
    "authors": [
      "Zixuan Huang",
      "Yunfeng Wang",
      "Zhiwen Chen",
      "Xin Gao",
      "Ruili Feng",
      "Xiaobo Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Huang_Context_Attention_Network_for_Skeleton_Extraction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Huang_Context_Attention_Network_for_Skeleton_Extraction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Skeleton extraction is a task focused on providing a simple representation of an object by extracting the skeleton from the given binary or RGB image. In recent years many attractive works in skeleton extraction have been made. But as far as we know, there is little research on how to utilize the context information in the binary shape of objects. In this paper, we propose an attention-based model called Context Attention Network (CANet), which integrates the context extraction module in a UNet architecture and can effectively improve the network's ability to extract the skeleton pixels. Meanwhile, we also use some novel techniques including distance transform, weight focal loss to achieve good results on the given dataset. Finally, without model ensemble and with only 80% of the training images, our method achieves 0.822 F1 score during the development phase and 0.8507 F1 score during the final phase of the Pixel SkelNetOn Competition, ranking 1st place on the leaderboard.",
    "code_link": ""
  },
  "cvpr2022_dlgc_multimodalshapecompletionviaimplicitmaximumlikelihoodestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "DLGC",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Deep Learning for Geometric Computing",
    "title": "Multimodal Shape Completion via Implicit Maximum Likelihood Estimation",
    "authors": [
      "Himanshu Arora",
      "Saurabh Mishra",
      "Shichong Peng",
      "Ke Li",
      "Ali Mahdavi-Amiri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Arora_Multimodal_Shape_Completion_via_Implicit_Maximum_Likelihood_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Arora_Multimodal_Shape_Completion_via_Implicit_Maximum_Likelihood_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Shape completion is the problem of completing partial input shapes such as partial scans. This problem finds important applications in computer vision and robotics due to issues such as occlusion or sparsity in real-world data. However, most of the existing research related to shape completion has been focused on completing shapes by learning a one-to-one mapping which limits the diversity and creativity of the produced results. We propose a novel multimodal shape completion technique that is effectively able to learn a one-to-many mapping and generates diverse complete shapes. Our approach is based on the conditional Implicit Maximum Likelihood Estimation (IMLE) technique wherein we condition our inputs on partial 3D point clouds. We extensively evaluate our approach by comparing it to various baselines both quantitatively and qualitatively. We show that our method is superior to alternatives in terms of completeness and diversity of shapes.",
    "code_link": ""
  },
  "cvpr2022_v4as_doodlenetdoubledeeplabenhancedfeaturefusionforthermal-colorsemanticsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "V4AS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-Color Semantic Segmentation",
    "authors": [
      "Oriel Frigo",
      "Lucien Martin-Gaffe",
      "Catherine Wacongne"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/html/Frigo_DooDLeNet_Double_DeepLab_Enhanced_Feature_Fusion_for_Thermal-Color_Semantic_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/papers/Frigo_DooDLeNet_Double_DeepLab_Enhanced_Feature_Fusion_for_Thermal-Color_Semantic_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper we present a new approach for feature fusion between RGB and LWIR Thermal images for the task of semantic segmentation for driving perception. We propose the DooDLeNet, a double DeepLab architecture with specialized encoder-decoders for thermal and color modalities and a shared decoder for final segmentation. We combine two strategies for feature fusion: confidence weighting and correlation weighting. We report state-of-the-art mean IoU results on MF dataset.",
    "code_link": ""
  },
  "cvpr2022_v4as_anefficientdomain-incrementallearningapproachtodriveinallweatherconditions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "V4AS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "An Efficient Domain-Incremental Learning Approach To Drive in All Weather Conditions",
    "authors": [
      "M. Jehanzeb Mirza",
      "Marc Masana",
      "Horst Possegger",
      "Horst Bischof"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/html/Mirza_An_Efficient_Domain-Incremental_Learning_Approach_To_Drive_in_All_Weather_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/papers/Mirza_An_Efficient_Domain-Incremental_Learning_Approach_To_Drive_in_All_Weather_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Although deep neural networks enable impressive visual perception performance for autonomous driving, their robustness to varying weather conditions still requires attention. When adapting these models for changed environments, such as different weather conditions, they are prone to forgetting previously learned information. This catastrophic forgetting is typically addressed via incremental learning approaches which usually re-train the model by either keeping a memory bank of training samples or keeping a copy of the entire model or model parameters for each scenario. While these approaches show impressive results, they can be prone to scalability issues and their applicability for autonomous driving in all weather conditions has not been shown. In this paper we propose DISC -- Domain Incremental through Statistical Correction -- a simple online zero-forgetting approach which can incrementally learn new tasks (i.e weather conditions) without requiring re-training or expensive memory banks. The only information we store for each task are the statistical parameters as we categorize each domain by the change in first and second order statistics. Thus, as each task arrives, we simply 'plug and play' the statistical vectors for the corresponding task into the model and it immediately starts to perform well on that task. We show the efficacy of our approach by testing it for object detection in a challenging domain-incremental autonomous driving scenario where we encounter different adverse weather conditions, such as heavy rain, fog, and snow.",
    "code_link": ""
  },
  "cvpr2022_v4as_restorex-aiacontrastiveapproachtowardsguidingimagerestorationviaexplainableaisystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "V4AS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "RestoreX-AI: A Contrastive Approach Towards Guiding Image Restoration via Explainable AI Systems",
    "authors": [
      "Aboli Marathe",
      "Pushkar Jain",
      "Rahee Walambe",
      "Ketan Kotecha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/html/Marathe_RestoreX-AI_A_Contrastive_Approach_Towards_Guiding_Image_Restoration_via_Explainable_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/papers/Marathe_RestoreX-AI_A_Contrastive_Approach_Towards_Guiding_Image_Restoration_via_Explainable_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Modern applications such as self-driving cars and drones rely heavily upon robust object detection techniques. However, weather corruptions can hinder the object detectability and pose a serious threat to their navigation and reliability. Thus, there is a need for efficient denoising, deraining, and restoration techniques. Generative adversarial networks and transformers have been widely adopted for image restoration. However, the training of these methods is often unstable and time-consuming. Furthermore, when used for object detection (OD), the output images generated by these methods may provide unsatisfactory results despite image clarity. In this work, we propose a contrastive approach towards mitigating this problem, by evaluating images generated by restoration models during and post training. This approach leverages OD scores combined with attention maps for predicting the usefulness of restored images for the OD task. We conduct experiments using two novel use-cases of conditional GANs and two transformer methods that probe the robustness of the proposed approach on multi-weather corruptions in the OD task. Our approach achieves an averaged 178 percent increase in mAP between the input and restored images under adverse weather conditions like dust tornadoes and snowfall. We report unique cases where greater denoising does not improve OD performance and conversely where noisy generated images demonstrate good results. We conclude the need for explainability frameworks to bridge the gap between human and machine perception, especially in the context of robust object detection for autonomous vehicles.",
    "code_link": ""
  },
  "cvpr2022_v4as_acategorizedreflectionremovaldatasetwithdiversereal-worldscenes": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "V4AS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "A Categorized Reflection Removal Dataset With Diverse Real-World Scenes",
    "authors": [
      "Chenyang Lei",
      "Xuhua Huang",
      "Chenyang Qi",
      "Yankun Zhao",
      "Wenxiu Sun",
      "Qiong Yan",
      "Qifeng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/html/Lei_A_Categorized_Reflection_Removal_Dataset_With_Diverse_Real-World_Scenes_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/papers/Lei_A_Categorized_Reflection_Removal_Dataset_With_Diverse_Real-World_Scenes_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Due to the lack of a large-scale reflection removal dataset with diverse real-world scenes, many existing reflection removal methods are trained on synthetic data plus a small amount of real-world data, which makes it difficult to evaluate the strengths or weaknesses of different reflection removal methods thoroughly. Furthermore, existing real-world benchmarks and datasets do not categorize image data based on the types and appearances of reflection (e.g., smoothness, intensity), making it hard to analyze reflection removal methods. Hence, we construct a new reflection removal dataset that is categorized, diverse, and real-world (CDR). A pipeline based on RAW data is used to capture perfectly aligned input images and transmission images. The dataset is constructed using diverse glass types under various environments to ensure diversity. By analyzing several reflection removal methods and conducting extensive experiments on our dataset, we show that state-of-the-art reflection removal methods generally perform well on blurry reflection but fail in obtaining satisfying performance on other types of real-world reflection. We believe our dataset can help develop novel methods to remove real-world reflection better.",
    "code_link": ""
  },
  "cvpr2022_v4as_physicsbasedimagedeshadowingusinglocallinearmodel": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "V4AS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "Physics Based Image Deshadowing Using Local Linear Model",
    "authors": [
      "Tamir Einy",
      "Efrat Immer",
      "Gilad Vered",
      "Shai Avidan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/html/Einy_Physics_Based_Image_Deshadowing_Using_Local_Linear_Model_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/V4AS/papers/Einy_Physics_Based_Image_Deshadowing_Using_Local_Linear_Model_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image deshadowing algorithms remove shadows from images. This requires both detecting where the shadow is and, once detected, removing it from the image. This work focuses on the shadow removal part. We follow a common physical shadow formation model and learn its parameters using a deep neural network. Our model consists of an existing network for shadow detection, and a novel network for shadow removal. The shadow removal network gets the predicted mask of the shadow region and the shadow image and predicts six parameters per pixel. Remarkably, a straightforward network architecture, that is considerably smaller compared to alternative methods, produces better results on standard datasets.",
    "code_link": ""
  },
  "cvpr2022_mobileai_anefficienthybridmodelforlow-lightimageenhancementinmobiledevices": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MobileAI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Mobile AI",
    "title": "An Efficient Hybrid Model for Low-Light Image Enhancement in Mobile Devices",
    "authors": [
      "Zhicheng Fu",
      "Miao Song",
      "Chao Ma",
      "Joseph Nasti",
      "Vivek Tyagi",
      "Grant Lloyd",
      "Wei Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/html/Fu_An_Efficient_Hybrid_Model_for_Low-Light_Image_Enhancement_in_Mobile_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/papers/Fu_An_Efficient_Hybrid_Model_for_Low-Light_Image_Enhancement_in_Mobile_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the help of continuous optimizations in hardware and software, smartphones can now capture vivid, detailed macro pictures as well as high-resolution videos. However, taking photos/videos in a low-light environment with smartphones would still result in underexposed and bad-quality photos/videos due to their physical limitations -- small sensor size, compact lenses, and the lack of specific hardware and software. A variety of low-light enhancement techniques have been proposed, but their effectiveness is limited by their high complexity and the limited computational resources of smartphones. In this paper, we present an efficient hybrid solution, named as LLNet, to generate a high-resolution enhanced image given the corresponding high-resolution low-light image on mobile devices. LLNet consists of two main parts: 1) a lightweight convolutional neural network for features restoration that takes a low-resolution low-light image scaled down from the high-resolution input and predicts an enhanced low-resolution output; 2) a non-trainable transformation estimation model that approximates a linear transformation between the low-resolution input and predicted low-resolution output. By applying the estimated transformation on a high-resolution low-light image, the corresponding enhanced image can be predicted efficiently. To support the development of this learning-based solution, we introduce a dataset of normal-exposure low-light images, with corresponding long-exposure reference images, and all the images were captured by smartphones under real-world low-light scenes. Experiments demonstrate that LLNet can provide a real-time (around 32ms) smartphone preview (1440*1080 resolution) with outstanding image enhancement under low-light environments with affordable resources consumption. One real viewfinder video demo is attached as supplementary material to indicate the practicality of LLNet on real smartphones.",
    "code_link": ""
  },
  "cvpr2022_mobileai_updatecompressionfordeepneuralnetworksontheedge": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MobileAI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Mobile AI",
    "title": "Update Compression for Deep Neural Networks on the Edge",
    "authors": [
      "Bo Chen",
      "Ali Bakhshi",
      "Gustavo Batista",
      "Brian Ng",
      "Tat-Jun Chin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/html/Chen_Update_Compression_for_Deep_Neural_Networks_on_the_Edge_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/papers/Chen_Update_Compression_for_Deep_Neural_Networks_on_the_Edge_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "An increasing number of artificial intelligence (AI) applications involve the execution of deep neural networks (DNNs) on edge devices. Many practical reasons motivate the need to update the DNN model on the edge device post-deployment, such as refining the model, concept drift, or outright change in the learning task. In this paper, we consider the scenario where retraining can be done on the server side based on a copy of the DNN model, with only the necessary data transmitted to the edge to update the deployed model. However, due to bandwidth constraints, we want to minimise the transmission required to achieve the update. We develop a simple approach based on matrix factorisation to compress the model update---this differs from compressing the model itself. The key idea is to preserve existing knowledge in the current model and optimise only small additional parameters for the update which can be used to reconstitute the model on the edge. We compared our method to similar techniques used in federated learning; our method usually requires less than half of the update size of existing methods to achieve the same accuracy.",
    "code_link": ""
  },
  "cvpr2022_mobileai_smm-convscalarmatrixmultiplicationwithzeropackingforacceleratedconvolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MobileAI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Mobile AI",
    "title": "SMM-Conv: Scalar Matrix Multiplication With Zero Packing for Accelerated Convolution",
    "authors": [
      "Amir Ofir",
      "Gil Ben-Artzi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/html/Ofir_SMM-Conv_Scalar_Matrix_Multiplication_With_Zero_Packing_for_Accelerated_Convolution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/papers/Ofir_SMM-Conv_Scalar_Matrix_Multiplication_With_Zero_Packing_for_Accelerated_Convolution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a novel approach for accelerating convolutions during inference for CPU-based architectures. The most common method of computation involves packing the image into the columns of a matrix (im2col) and performing general matrix multiplication (GEMM) with a matrix of weights. This results in two main drawbacks: (a) im2col requires a large memory buffer and can experience inefficient memory access, and (b) while GEMM is highly optimized for scientific matrices multiplications, it is not well suited for convolutions. We propose an approach that takes advantage of scalar-matrix multiplication and reduces memory overhead. Our experiments with commonly used network architectures demonstrate a significant speedup compared to existing indirect methods.",
    "code_link": ""
  },
  "cvpr2022_mobileai_phonedepthadatasetformonoculardepthestimationonmobiledevices": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MobileAI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Mobile AI",
    "title": "PhoneDepth: A Dataset for Monocular Depth Estimation on Mobile Devices",
    "authors": [
      "Fausto Tapia Benavides",
      "Andrey Ignatov",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/html/Benavides_PhoneDepth_A_Dataset_for_Monocular_Depth_Estimation_on_Mobile_Devices_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/papers/Benavides_PhoneDepth_A_Dataset_for_Monocular_Depth_Estimation_on_Mobile_Devices_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Monocular depth estimation has been studied as a classic and learning based computer vision problem for decades. However, little attention received the efficiency and the deployment of methods on mobile hardware. All publicly available datasets have severe limitations related to their applicability to camera data captured with real mobile devices. For instance, the main issues with current datasets include (but not exhaustively) low quality of images due the cameras or collection methods, domain specifically generated datasets as is the case for autonomous driving, small number of samples, sparse depthmaps, etc. In response, we introduce PhoneDepth, a novel dataset that aims to take advantage of modern phones hardware and professional stereo cameras. Depthmaps are acquired from three sources: Time of Flight sensor, Dual Pixel sensor and stereo camera; while the images correspond to mobile phone photos. We prove its high value by training neural networks with multiple depth supervision, fine-tuning on other datasets and for depth refinement. Along with the dataset we present benchmark models and a toolbox to facilitate the dataset usage.",
    "code_link": ""
  },
  "cvpr2022_mobileai_rendersralightweightsuper-resolutionmodelformobilegamingupscaling": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MobileAI",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Mobile AI",
    "title": "RenderSR: A Lightweight Super-Resolution Model for Mobile Gaming Upscaling",
    "authors": [
      "Tingxing (Tim) Dong",
      "Hao Yan",
      "Mayank Parasar",
      "Raun Krisch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/html/Dong_RenderSR_A_Lightweight_Super-Resolution_Model_for_Mobile_Gaming_Upscaling_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MobileAI/papers/Dong_RenderSR_A_Lightweight_Super-Resolution_Model_for_Mobile_Gaming_Upscaling_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Mobile game play can be a prime use case where an efficient SR network can lead to both performance boosts and power savings. In this paper, we present RenderSR (RSR), a bandwidth aware super-resolution network designed for use in mobile game upscaling. We explore how different factors affect the resulting image quality: color space, the inclusion of the depth channel, sharpening. With a 40K parameter size, RenderSR without sharpening achieves a PSNR value difference ranging -0.41 to 0.36dB from several much larger SR models. RenderSR with sharpening super resolved large objects such as rocks, buildings, tree trunks are almost identical to the ground truth. Based on our performance experiment, we propose that RenderSR upscales the GPU rendered image on NPU or DSP on the mobile SoC.",
    "code_link": "https://github.com/gpuopen-effects/fidelityfx-fsr"
  },
  "cvpr2022_aicity_density-guidedlabelsmoothingfortemporallocalizationofdrivingactions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Density-Guided Label Smoothing for Temporal Localization of Driving Actions",
    "authors": [
      "Tunc Alkanat",
      "Erkut Akdag",
      "Egor Bondarev",
      "Peter H.N. de With"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Alkanat_Density-Guided_Label_Smoothing_for_Temporal_Localization_of_Driving_Actions_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Alkanat_Density-Guided_Label_Smoothing_for_Temporal_Localization_of_Driving_Actions_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies. However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization. In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization. To this end, we first develop a density-guided label smoothing technique based on label probability distributions to facilitate better learning from boundary video-segments that typically include multiple labels. Second, we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives. Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271.",
    "code_link": ""
  },
  "cvpr2022_aicity_naturallanguage-basedvehicleretrievalwithexplicitcross-modalrepresentationlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Natural Language-Based Vehicle Retrieval With Explicit Cross-Modal Representation Learning",
    "authors": [
      "Bocheng Xu",
      "Yihua Xiong",
      "Rui Zhang",
      "Yanyi Feng",
      "Haifeng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Xu_Natural_Language-Based_Vehicle_Retrieval_With_Explicit_Cross-Modal_Representation_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Xu_Natural_Language-Based_Vehicle_Retrieval_With_Explicit_Cross-Modal_Representation_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "On the account of the explosive growth in the large-scale transportation videos, vehicle retrieval plays an important role in the public transportation security and the intelligent transport system recently. Most vehicle retrieval algorithms are vision-based and consist of vehicle re-identification and vehicle tracking. However, the performance of vision-based vehicle retrieval algorithms is constrained as the limited information provided by traffic video streams. In this paper, we propose a contrastive cross-modal vehicle retrieval solution, maximizing the value of the complementation between natural language representation and vision representation. The framework of the proposed solution includes: (1) Preprocess a source video in four ways for generating local motional semantics and global motional semantics; (2) Correspondingly, preprocess relevant description sentences in two ways, including Textual Local Instance Semantics Extraction (TLISE) and Textual Local Motional Semantics Extraction (TLMSE); (3) Use a two-stream architecture model with four visual encoders and four text encoders to extract visual features and textual embeddings; (4) Fuse visual features and textual embeddings respectively by concatenating them along the feature channel in the order of importance, and use them for retrieval. By using the proposed solution, we achieved MRR score of 33.20%, ranking the 7th place in the AI City Challenge 2022 Track 2. The code is publicly available at https://github.com/Katherinaxxx/2022AICITY_T2.",
    "code_link": "https://github.com/Katherinaxxx/2022AICITY_T2"
  },
  "cvpr2022_aicity_federatedlearning-baseddriveractivityrecognitionforedgedevices": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Federated Learning-Based Driver Activity Recognition for Edge Devices",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Doshi_Federated_Learning-Based_Driver_Activity_Recognition_for_Edge_Devices_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Doshi_Federated_Learning-Based_Driver_Activity_Recognition_for_Edge_Devices_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Video action recognition has been an active area of research for the past several years. However, the majority of research is concentrated on recognizing a diverse range of activities in distinct environments. On the other hand, Driver Activity Recognition (DAR) is significantly more difficult since there is a much finer distinction between various actions. Moreover, training robust DAR models requires diverse training data from multiple sources, which might not be feasible for a centralized setup due to privacy and security concerns. Furthermore, it is critical to develop efficient models due to limited computational resources available on vehicular edge devices. Federated Learning (FL), which allows data parties to collaborate on machine learning models while preserving data privacy and reducing communication requirements, can be used to overcome these challenges. Despite significant progress on various computer vision tasks, FL for DAR has been largely unexplored. In this work, we propose an FL-based DAR model and extensively benchmark the model performance on two datasets under various practical setups. Our results indicate that the proposed approach performs competitively under the centralized (non-FL) and decentralized (FL) settings.",
    "code_link": ""
  },
  "cvpr2022_aicity_aregion-baseddeeplearningapproachtoautomatedretailcheckout": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "A Region-Based Deep Learning Approach to Automated Retail Checkout",
    "authors": [
      "Maged Shoman",
      "Armstrong Aboah",
      "Alex Morehead",
      "Ye Duan",
      "Abdulateef Daud",
      "Yaw Adu-Gyamfi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Shoman_A_Region-Based_Deep_Learning_Approach_to_Automated_Retail_Checkout_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Shoman_A_Region-Based_Deep_Learning_Approach_to_Automated_Retail_Checkout_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Automating the product checkout process at conventional retail stores is a task poised to have large impacts on society generally speaking. Towards this end, reliable deep learning models that enable automated product counting for fast customer checkout can make this goal a reality. In this work, we propose a novel, region-based deep learning approach to automate product counting using a customized YOLOv5 object detection pipeline and the DeepSORT algorithm. Our results on challenging, real-world test videos demonstrate that our method can generalize its predictions to a sufficient level of accuracy and with a fast enough runtime to warrant deployment to real-world commercial settings. Our proposed method won 4th place in the 2022 AI City Challenge, Track 4, with an F1 score of 0.4400 on experimental validation data.",
    "code_link": ""
  },
  "cvpr2022_aicity_textquerybasedtrafficvideoeventretrievalwithglobal-localfusionembedding": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Text Query Based Traffic Video Event Retrieval With Global-Local Fusion Embedding",
    "authors": [
      "Thang-Long Nguyen-Ho",
      "Minh-Khoi Pham",
      "Tien-Phat Nguyen",
      "Hai-Dang Nguyen",
      "Minh N. Do",
      "Tam V. Nguyen",
      "Minh-Triet Tran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Nguyen-Ho_Text_Query_Based_Traffic_Video_Event_Retrieval_With_Global-Local_Fusion_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Nguyen-Ho_Text_Query_Based_Traffic_Video_Event_Retrieval_With_Global-Local_Fusion_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Retrieving event videos based on textual description is a promising research topic in the fast-growing data field. However, traffic data increases every day, so it is essential to need intelligent traffic system management in conjunction with humans to speed up the search. We propose a multi-module system that delivers accurate results that meet objectives, including explainability and scalability at the same time. Our solution considers neighbors entities related to the mentioned object to represent an event by rule-based, which can represent an event by the relationship of multiple objects. We also propose to add a modified model from last year's Alibaba model with an explainable architecture. As the traffic data is vehicle-centric, we apply two language and image modules to analyze the input data and obtain the global properties of the context and the internal attributes of the vehicle. We introduce a one-on-one dual training strategy for each representation vector to optimize the interior features for the query. Finally, a refinement module gathers previous results to enhance the final retrieval result. We benchmarked our approach on the data of the AI City Challenge 2022 and got the best results at an MMR of 0.3611. We were ranked in the top 4 on 50% of the test set and in the top 5 on the full dataset.",
    "code_link": ""
  },
  "cvpr2022_aicity_multi-cameravehicletrackingsystemforaicitychallenge2022": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Multi-Camera Vehicle Tracking System for AI City Challenge 2022",
    "authors": [
      "Fei Li",
      "Zhen Wang",
      "Ding Nie",
      "Shiyi Zhang",
      "Xingqun Jiang",
      "Xingxing Zhao",
      "Peng Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Li_Multi-Camera_Vehicle_Tracking_System_for_AI_City_Challenge_2022_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Li_Multi-Camera_Vehicle_Tracking_System_for_AI_City_Challenge_2022_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-Target Multi-Camera tracking is a fundamental task for intelligent traffic systems. The track 1 of AI City Challenge 2022 aims at the city-scale multi-camera vehicle tracking task. In this paper we propose an accurate vehicle tracking system composed of 4 parts, including: (1) State-of-the-art detection and re-identification models for vehicle detection and feature extraction. (2) Single camera tracking, where we introduce augmented tracks prediction and multi-level association method on top of tracking-by-detection paradigm.(3) Zone-based singe-camera tracklet merging strategy. (4) Multi-camera spatial-temporal matching and clustering strategy. The proposed system achieves promising results and ranks the second place in Track 1 of the AI City Challenge 2022 with a IDF1 score of 0.8437.",
    "code_link": ""
  },
  "cvpr2022_aicity_vistavisiontransformerenhancedbyu-netandimagecolorfulnessframefiltrationforautomaticretailcheckout": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "VISTA: Vision Transformer Enhanced by U-Net and Image Colorfulness Frame Filtration for Automatic Retail Checkout",
    "authors": [
      "Md. Istiak Hossain Shihab",
      "Nazia Tasnim",
      "Hasib Zunair",
      "Labiba Kanij Rupty",
      "Nabeel Mohammed"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Shihab_VISTA_Vision_Transformer_Enhanced_by_U-Net_and_Image_Colorfulness_Frame_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Shihab_VISTA_Vision_Transformer_Enhanced_by_U-Net_and_Image_Colorfulness_Frame_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-class product counting and recognition identifies product items from images or videos for automated retail checkout. The task is challenging due to the real-world scenario of occlusions where product items overlap, fast movement in conveyor belt, large similarity in overall appearance of the items being scanned, novel products, the negative impact of misidentifying items. Further there is a domain bias between training and test sets, specifically the provided training dataset consists of synthetic images and the test set videos consist of foreign objects such as hands and tray. To address these aforementioned issues, we propose to segment and classify individual frames from a video sequence. The segmentation method consists of a unified single product item- and hand-segmentation followed by entropy masking to address the domain bias problem. The multi-class classification method is based on Vision Transformers (ViT). To identify the frames with target objects, we utilize several image processing methods and propose a custom metric to discard frames not having any product items. Combining all these mechanisms, our best system achieves 3rd place in the AI City Challenge 2022 Track 4 with F1 score of 0.4545.",
    "code_link": ""
  },
  "cvpr2022_aicity_detectingvehiclesontheedgeknowledgedistillationtoimproveperformanceinheterogeneousroadtraffic": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Detecting Vehicles on the Edge: Knowledge Distillation To Improve Performance in Heterogeneous Road Traffic",
    "authors": [
      "Manoj Bharadhwaj",
      "Gitakrishnan Ramadurai",
      "Balaraman Ravindran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Bharadhwaj_Detecting_Vehicles_on_the_Edge_Knowledge_Distillation_To_Improve_Performance_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Bharadhwaj_Detecting_Vehicles_on_the_Edge_Knowledge_Distillation_To_Improve_Performance_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The drastic growth in the number of vehicles in the last few decades has necessitated significantly better traffic management and planning. To manage the traffic efficiently, traffic volume is an essential parameter. Most methods solve the vehicle counting problem under the assumption of state-of-the-art computation power. With the recent growth in cost-effective Internet of Things (IoT) devices and edge computing, several machine learning models are being tailored for such devices. Solving the traffic count problem on these devices will enable us to create a real-time dashboard of network-wide live traffic analytics. This paper proposes a Detect-Track-Count (DTC) framework to count vehicles efficiently on edge devices. The proposed solution aims at improving the performance of tiny vehicle detection models using an ensemble knowledge distillation technique. Experimental results on multiple datasets show that the custom knowledge distillation setup helps generalize a tiny object detector better.",
    "code_link": ""
  },
  "cvpr2022_aicity_the6thaicitychallenge": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "The 6th AI City Challenge",
    "authors": [
      "Milind Naphade",
      "Shuo Wang",
      "David C. Anastasiu",
      "Zheng Tang",
      "Ming-Ching Chang",
      "Yue Yao",
      "Liang Zheng",
      "Mohammed Shaiqur Rahman",
      "Archana Venkatachalapathy",
      "Anuj Sharma",
      "Qi Feng",
      "Vitaly Ablavsky",
      "Stan Sclaroff",
      "Pranamesh Chakraborty",
      "Alice Li",
      "Shangru Li",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Naphade_The_6th_AI_City_Challenge_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Naphade_The_6th_AI_City_Challenge_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The 6th edition of the AI City Challenge specifically focuses on problems in two domains where there is tremendous unlocked potential at the intersection of computer vision and artificial intelligence: Intelligent Traffic Systems (ITS), and brick and mortar retail businesses. The four challenge tracks of the 2022 AI City Challenge received participation requests from 254 teams across 27 countries. Track 1 addressed city-scale multi-target multi-camera (MTMC) vehicle tracking. Track 2 addressed natural-language-based vehicle track retrieval. Track 3 was a brand new track for naturalistic driving analysis, where the data were captured by several cameras mounted inside the vehicle focusing on driver safety, and the task was to classify driver actions. Track 4 was another new track aiming to achieve retail store automated checkout using only a single view camera. We released two leader boards for submissions based on different methods, including a public leader board for the contest, where no use of external data is allowed, and a general leader board for all submitted results. The top performance of participating teams established strong baselines and even outperformed the state-of-the-art in the proposed challenge tracks.",
    "code_link": ""
  },
  "cvpr2022_aicity_temporaldriveractionlocalizationusingactionclassificationmethods": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Temporal Driver Action Localization Using Action Classification Methods",
    "authors": [
      "Munirah Alyahya",
      "Shahad Alghannam",
      "Taghreed Alhussan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Alyahya_Temporal_Driver_Action_Localization_Using_Action_Classification_Methods_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Alyahya_Temporal_Driver_Action_Localization_Using_Action_Classification_Methods_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Driver distraction recognition is an essential computer vision task that can play a key role in increasing traffic safety and reducing traffic accidents. In this paper, we propose a temporal driver action localization (TDAL) framework for classifying driver distraction actions, as well as identifying the start and end time of a given driver action. The TDAL framework consists of three stages: preprocessing, which takes untrimmed video as input and generates multiple clips; action classification, which classifies the clips; and finally, the classifier output is sent to the temporal action localization to generate the start and end times of the distracted actions. The proposed framework achieves an F1 score of 27.06% on Track 3 A2 dataset of NVIDIA AI City 2022 Challenge. The findings show that the TDAL framework contributes to fine-grained driver distraction recognition and paves the way for the development of smart and safe transportation. Code will be available soon.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "cvpr2022_aicity_improvingmulti-targetmulti-cameratrackingbytrackrefinementandcompletion": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Improving Multi-Target Multi-Camera Tracking by Track Refinement and Completion",
    "authors": [
      "Andreas Specker",
      "Lucas Florin",
      "Mickael Cormier",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Specker_Improving_Multi-Target_Multi-Camera_Tracking_by_Track_Refinement_and_Completion_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Specker_Improving_Multi-Target_Multi-Camera_Tracking_by_Track_Refinement_and_Completion_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-camera tracking of vehicles on a city-wide level is a core component of modern traffic monitoring systems. For this task, single-camera tracking failures are the most common causes of errors concerning automatic multi-target multi-camera tracking systems. To address these problems, we propose several modules that aim at improving single-camera tracklets, e.g., appearance-based tracklet splitting, single-camera clustering, and track completion. After these track refinement steps, hierarchical clustering is used to associate the enhanced single-camera tracklets. During this stage, we leverage vehicle re-identification features as well as prior knowledge about the scene's topology. Last, the proposed track completion strategy is adopted for the cross-camera association task to obtain the final multi-camera tracks. Our method proves itself competitive: With it, we achieved 4th place in track 1 of the 2022 AI City Challenge.",
    "code_link": ""
  },
  "cvpr2022_aicity_amulti-granularityretrievalsystemfornaturallanguage-basedvehicleretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "A Multi-Granularity Retrieval System for Natural Language-Based Vehicle Retrieval",
    "authors": [
      "Jiacheng Zhang",
      "Xiangru Lin",
      "Minyue Jiang",
      "Yue Yu",
      "Chenting Gong",
      "Wei Zhang",
      "Xiao Tan",
      "Yingying Li",
      "Errui Ding",
      "Guanbin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Zhang_A_Multi-Granularity_Retrieval_System_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Zhang_A_Multi-Granularity_Retrieval_System_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We focus on the task of the Natural language-based vehicle track retrieval of the 6th AI City Challenge. Performing target vehicle retrieval using natural language descriptions is a comprehensive task, requiring a model to first understand the semantics of the language and vision modalities and then match them to generate accurate retrieval results. However, this task involves the following challenges: (1) the ambiguity of the natural language descriptions towards a target vehicle; (2) the matching between the linguistic semantics of the language descriptions and the corresponding static and dynamic properties of the target vehicle; (3) the shortage of the annotated language and target vehicle pairs. Obviously, focusing on solving a subset of the problems cannot generate a robust retrieval model. Therefore, we propose a multi-granularity retrieval system to solve this task, consisting of three main modules: (1) Language parsing module that aims to obtain the fine-grained vehicle attributes (e.g. color, type and motion) from the language descriptions; (2) Language-augmented multi-query vehicle track retrieval module that serves as our baseline model to incorporate information from multiple imperfect queries; (3) Target vehicle attributes enhancement module that explicitly fuses the static and dynamic properties of the target vehicle to generate the final retrieval results. Our system has achieved the 1st place on the 6th AI City Challenge, yielding a strong performance on the private test set.",
    "code_link": ""
  },
  "cvpr2022_aicity_keypoint-baseddriveractivityrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Key Point-Based Driver Activity Recognition",
    "authors": [
      "Arpita Vats",
      "David C. Anastasiu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Vats_Key_Point-Based_Driver_Activity_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Vats_Key_Point-Based_Driver_Activity_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a key point-based activity recognition framework, built upon pre-trained human pose estimation and facial feature detection models. Our method extracts complex static and movement-based features from key frames in videos, which are used to predict a sequence of key-frame activities. Finally, a merge procedure is employed to identify robust activity segments while ignoring outlier frame activity predictions. We analyze the different components of our framework via a wide array of experiments and draw conclusions with regards to the utility of the model and ways it can be improved. Results show our model is competitive, taking the 11th place place out of 27 teams submitting to Track 3 of the 2022 AI City Challenge.",
    "code_link": ""
  },
  "cvpr2022_aicity_multi-cameravehicletrackingbasedonocclusion-awareandinter-vehicleinformation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Multi-Camera Vehicle Tracking Based on Occlusion-Aware and Inter-Vehicle Information",
    "authors": [
      "Yuming Liu",
      "Xiaochun Zhang",
      "Bingzhen Zhang",
      "Xiaoyong Zhang",
      "Sen Wang",
      "Jianrong Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Liu_Multi-Camera_Vehicle_Tracking_Based_on_Occlusion-Aware_and_Inter-Vehicle_Information_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liu_Multi-Camera_Vehicle_Tracking_Based_on_Occlusion-Aware_and_Inter-Vehicle_Information_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the demands of analyzing and predicting traffic flow for applications in smart cities, Multi-Target Multi-Camera vehicle Tracking(MTMCT) at the city scale has become a fundamental problem. The MTMCT is challenging due to the view variations, frequent occlusions, and similar vehicle models in the same camera. This work proposes an MTMCT framework based on occlusion-aware and inter-vehicle information that can effectively match vehicle tracklets. The occlusion-aware module segments the tracklets of an occluded and occluding vehicle pair. It recalculates the similarity of the complete tracklets, which can handle the occlusions and suppress false detections. This work proposes an inter-vehicle information module to improve the matching accuracy. The module can enhance the ability to distinguish similar vehicles under the same camera at different times. The proposed whole framework consists of four modules: (1) vehicle detection and feature extraction by re-identification models, (2) single-camera tracking (SCT) to produce initial tracklets with an occlusion-aware module, (3) tracklets similarity by inter-vehicle association, (4) clustering in adjacent cameras for multi-camera tracklets matching. The proposed method obtains IDF1 score of 0.8285 on the Track-1 multi-camera vehicle tracking task of the 2022 AI City Challenge.",
    "code_link": ""
  },
  "cvpr2022_aicity_learninggeneralizedfeaturefortemporalactiondetectionapplicationfornaturaldrivingactionrecognitionchallenge": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Learning Generalized Feature for Temporal Action Detection: Application for Natural Driving Action Recognition Challenge",
    "authors": [
      "Chuong Nguyen",
      "Ngoc Nguyen",
      "Su Huynh",
      "Vinh Nguyen",
      "Son Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Nguyen_Learning_Generalized_Feature_for_Temporal_Action_Detection_Application_for_Natural_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Nguyen_Learning_Generalized_Feature_for_Temporal_Action_Detection_Application_for_Natural_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper reports our approach for the 2022 AI City Challenge - Naturalistic Driving Action Recognition (Track 3), where the objective is to detect when and what kinds of actions that a driver performs in a long, untrimmed video. Our solution is built upon the single stage ActionFormer detector, in which temporal location and classification are predicted simultaneously for efficiency. The input feature for the detector is extracted offline using our proposed backbone, which we named \"ConvNext-Video\". However, due to the small size of the dataset, training the model to avoid over-fitting becomes challenging. To address this problem, we focus on training techniques that can improve the generalization of underlying features. Specifically, we utilize two methods: \"learning without forgetting\" and semi-weak supervised learning on the unlabeled data A2. Finally, we also add a second-stage classifier (SSC) using our ConvNeXt-Video backbone. The SSC Classifer is designed to combine information from multi-clips and multi-view cameras to improve the prediction precision. Our best result achieves 29.1 F1 score on the public test set. Our source code is released at \\href https://github.com/cybercore-co-ltd/AICity2022-Track3link .",
    "code_link": "https://github.com/open-mmlab/mmaction2"
  },
  "cvpr2022_aicity_mv-talmulit-viewtemporalactionlocalizationinnaturalisticdriving": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "MV-TAL: Mulit-View Temporal Action Localization in Naturalistic Driving",
    "authors": [
      "Wei Li",
      "Shimin Chen",
      "Jianyang Gu",
      "Ning Wang",
      "Chen Chen",
      "Yandong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Li_MV-TAL_Mulit-View_Temporal_Action_Localization_in_Naturalistic_Driving_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Li_MV-TAL_Mulit-View_Temporal_Action_Localization_in_Naturalistic_Driving_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human risky behavior in driving is an important visual recognition problem. In this paper, we propose a multi-view temporal action localization system based on the grayscale video to achieve action recognition in naturalistic driving. Specifically, we adopted SwinTransformer as feature extractor, and a single framework to detect boundary and class at the same time. Also, we improve multiple loss function for explicit constraints of embedded feature distributions. Our proposed framework achieves the overall F1-score of 0.3154 on A2 dataset.",
    "code_link": ""
  },
  "cvpr2022_aicity_arobusttraffic-awarecity-scalemulti-cameravehicletrackingofvehicles": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "A Robust Traffic-Aware City-Scale Multi-Camera Vehicle Tracking of Vehicles",
    "authors": [
      "Duong Nguyen-Ngoc Tran",
      "Long Hoang Pham",
      "Hyung-Joon Jeon",
      "Huy-Hung Nguyen",
      "Hyung-Min Jeon",
      "Tai Huu-Phuong Tran",
      "Jae Wook Jeon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Tran_A_Robust_Traffic-Aware_City-Scale_Multi-Camera_Vehicle_Tracking_of_Vehicles_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Tran_A_Robust_Traffic-Aware_City-Scale_Multi-Camera_Vehicle_Tracking_of_Vehicles_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-Target Multi-Camera Tracking (MTMC) has an immense domain of Intelligent Traffic Surveillance System applications. Multifarious tasks manage to apply MTMC trackings, such as crowd analysis and city-scale traffic management. This paper describes our framework using spatial constraints for the Task of the Track 1 multi-camera vehicle tracking in the 2022 AI City Challenge. The framework includes single-camera detection and tracking, vehicle re-identification, and multi-camera track matching. To improve the system's accuracy, we proposed Region-Aware for the precision of vehicle detection and tracking, leading to the effective service of vehicle re-identification models to extract targets and appearance features. We use Crossing-Aware for a tracker to utilize the rich feature to find the tracklets and operate trajectory matching for multi-camera tracklets connection. Finally, the Inter-Camera Matching generated the global identification for vehicle trajectory. Our method acquired an IDF1 score of 0.8129 on the AI City 2022 Challenge Track 1 public leaderboard.",
    "code_link": ""
  },
  "cvpr2022_aicity_omgobservemultiplegranularitiesfornaturallanguage-basedvehicleretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "OMG: Observe Multiple Granularities for Natural Language-Based Vehicle Retrieval",
    "authors": [
      "Yunhao Du",
      "Binyu Zhang",
      "Xiangning Ruan",
      "Fei Su",
      "Zhicheng Zhao",
      "Hong Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Du_OMG_Observe_Multiple_Granularities_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Du_OMG_Observe_Multiple_Granularities_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Retrieving tracked-vehicles by natural language descriptions plays a critical role in smart city construction. It aims to find the best match for the given texts from a set of tracked vehicles in surveillance videos. Existing works generally solve it by a dual-stream framework, which consists of a text encoder, a visual encoder and a cross-modal loss function. Although some progress has been made, they failed to fully exploit the information at various levels of granularity. To tackle this issue, we propose a novel framework for the natural language-based vehicle retrieval task, OMG, which Observes Multiple Granularities with respect to visual representation, textual representation and objective functions. For the visual representation, target features, context features and motion features are encoded separately. For the textual representation, one global embedding, three local embeddings and a color-type prompt embedding are extracted to represent various granularities of semantic features. Finally, the overall framework is optimized by a cross-modal multi-granularity contrastive loss function. Experiments demonstrate the effectiveness of our method. Our OMG significantly outperforms all previous methods and ranks the 9th on the 6th AI City Challenge Track2. The codes are available at https://github.com/dyhBUPT/OMG.",
    "code_link": "https://github.com/dyhBUPT/OMG"
  },
  "cvpr2022_aicity_persongoneimageinpaintingforautomatedcheckoutsolution": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "PersonGONE: Image Inpainting for Automated Checkout Solution",
    "authors": [
      "Vojt\u011bch Bartl",
      "Jakub \u0160pa\u0148hel",
      "Adam Herout"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Bartl_PersonGONE_Image_Inpainting_for_Automated_Checkout_Solution_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Bartl_PersonGONE_Image_Inpainting_for_Automated_Checkout_Solution_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we present a solution for automatic checkout in a retail store as a part of AI City Challenge 2022. We propose a novel approach that uses the \"removal\" of unwanted objects -- in this case, body parts of operating staff, which are localized and further removed from video by an image inpainting method. Afterwards, a neural network detector can detect products with a decreased detection false positive rate. A part of our solution is also automatic detection of ROI (the place where products are shown to the system). We reached 0.4167 F1-Score with 0.3704 precision and 0.4762 recall which placed us at the 7th place of AI City Challenge 2022 in corresponding Track 4. The code is made public and available on GitHub.",
    "code_link": ""
  },
  "cvpr2022_aicity_aneffectivetemporallocalizationmethodwithmulti-view3dactionrecognitionforuntrimmednaturalisticdrivingvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "An Effective Temporal Localization Method With Multi-View 3D Action Recognition for Untrimmed Naturalistic Driving Videos",
    "authors": [
      "Manh Tung Tran",
      "Minh Quan Vu",
      "Ngoc Duong Hoang",
      "Khac-Hoai Nam Bui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Tran_An_Effective_Temporal_Localization_Method_With_Multi-View_3D_Action_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Tran_An_Effective_Temporal_Localization_Method_With_Multi-View_3D_Action_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Naturalistic driving studies with computer vision techniques have become an emergent research issue. The objective is to classify the distracted behavior actions by drivers. Specifically, this issue is regarded as temporal action localization (TAL) of untrimmed videos, which is a challenging task in the research field of video analysis. Particularly, TAL remains as one of the most challenging unsolved problems in computer vision that requires not only the recognition of action but the localization of the start and end times of each action. Most state-of-the-art approaches adopt complex architectures, which are expensive training and inefficient inference time. In this study, we propose a new framework for untrimmed naturalistic driving videos by utilizing the results from 3D action recognition with video clip classification for short temporal and spatial correlation. Then, simple post-processing based on data-driven is presented for long temporal correlation in untrimmed videos. The proposed method is evaluated on the AI City Challenge 2022 dataset for Naturalistic Driving Action Recognition. Accordingly, our method achieves the top 1 on the public leaderboard of the challenge.",
    "code_link": "https://github.com/facebookresearch/SlowFast"
  },
  "cvpr2022_aicity_stargazeratransformer-baseddriveractiondetectionsystemforintelligenttransportation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Stargazer: A Transformer-Based Driver Action Detection System for Intelligent Transportation",
    "authors": [
      "Junwei Liang",
      "He Zhu",
      "Enwei Zhang",
      "Jun Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Distracted driver actions can be dangerous and cause severe accidents. Thus, it is important to detect and eliminate distracted driving behaviors on the road to save lives. To this end, we study driver action detection using videos captured inside the vehicle. We propose Stargazer, an efficient, transformer-based system exploiting rich temporal features about the human behavioral information, with a simple yet effective action temporal localization framework. The core of our system contains an improved version of the multi-scale vision transformer network, which learns a hierarchy of robust representations. We then use a sliding-window classification strategy to facilitate temporal localization of actions-of-interest. The proposed system wins the second place in the Naturalistic Driving Action Recognition of AI City Challenge 2022 (Track 3). The code and models are released.",
    "code_link": ""
  },
  "cvpr2022_aicity_pandpreciseactionrecognitiononnaturalisticdriving": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "PAND: Precise Action Recognition on Naturalistic Driving",
    "authors": [
      "Hangyue Zhao",
      "Yuchao Xiao",
      "Yanyun Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Zhao_PAND_Precise_Action_Recognition_on_Naturalistic_Driving_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Zhao_PAND_Precise_Action_Recognition_on_Naturalistic_Driving_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Temporal action localization for untrimmed videos is a difficult problem in computer vision. It is challenge to infer the start and end of activity instances on small-scale datasets covering multi-view information accurately. In this paper, we propose an effective activity temporal localization and classification method to lo-calize the temporal boundaries and predict the class la-bel of activities for naturalistic driving. Our approach includes (i) a distraction behavior recognition and lo-calization method in naturalistic driving videos on small-scale data sets, (ii) a strategy that uses mul-ti-branch network to make full use of information from different channels, (iii)a post-processing method for se-lecting and correcting temporal range to ensure that our system finds accurate boundaries. In addition, the frame-level object detection information is also utilized. Extensive experiments prove the effectiveness of our method and we rank the 6th on the Test-A2 of the 6th AI City Challenge Track 3.",
    "code_link": ""
  },
  "cvpr2022_aicity_acoarse-to-fineboundarylocalizationmethodfornaturalisticdrivingactionrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "A Coarse-To-Fine Boundary Localization Method for Naturalistic Driving Action Recognition",
    "authors": [
      "Guanchen Ding",
      "Wenwei Han",
      "Chenglong Wang",
      "Mingpeng Cui",
      "Lin Zhou",
      "Dianbo Pan",
      "Jiayi Wang",
      "Junxi Zhang",
      "Zhenzhong Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Ding_A_Coarse-To-Fine_Boundary_Localization_Method_for_Naturalistic_Driving_Action_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Ding_A_Coarse-To-Fine_Boundary_Localization_Method_for_Naturalistic_Driving_Action_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Naturalistic driving action recognition plays an important role in understanding drivers' distraction behavior in the traffic environment. The main challenge of this task is the accurate localization of the temporal boundary for each distraction driving behavior in the video. Although many temporal action localization methods can identify action classes, it is difficult to predict accurate temporal boundaries for this task since the driving actions of the same category usually present large intra-class variation. In this paper, we introduce a Coarse-to-Fine Boundary Localization method called CFBL, which obtains fine-grained temporal boundaries progressively through three stages. Concretely, in the first coarse boundary generation stage, we adopt a modified anchor-free model Anchor-Free Saliency-based Detector (AFSD) to make an interval estimation of the temporal boundaries of distraction behavior. In the second boundary refinement stage, we use the Dense Boundary Generation (DBG) model to adjust the estimated interval of the temporal boundaries. In the final boundary decision stage, we build a Localization Boundary Refinement Module to determine the final boundaries of different actions. Besides, we adopt a voting strategy to combine the results of different camera views to enhance the model's distraction driving action classification ability. The experiments conducted on the Track 3 validation set of the 2022 AI City Challenge demonstrate competitive performance of the proposed method.",
    "code_link": "https://github.com/open-mmlab/denseflow"
  },
  "cvpr2022_aicity_box-grainedrerankingmatchingformulti-cameramulti-targettracking": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Box-Grained Reranking Matching for Multi-Camera Multi-Target Tracking",
    "authors": [
      "Xipeng Yang",
      "Jin Ye",
      "Jincheng Lu",
      "Chenting Gong",
      "Minyue Jiang",
      "Xiangru Lin",
      "Wei Zhang",
      "Xiao Tan",
      "Yingying Li",
      "Xiaoqing Ye",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Yang_Box-Grained_Reranking_Matching_for_Multi-Camera_Multi-Target_Tracking_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Yang_Box-Grained_Reranking_Matching_for_Multi-Camera_Multi-Target_Tracking_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-Camera Multi-Target tracking (MCMT) is an essential task in intelligent transportation systems. It is highly challenging due to several problems such as heavy occlusion and appearance variance caused by various camera perspectives and congested vehicles. In this paper, we propose a practical framework for dealing with the city-scale MCMT task, consisting of four modules. The vehicles detection and ReID feature extraction are the first two modules, which locate all vehicles and extract the appearance features for all cameras. The third module is Single-Camera Multi-Target tracking (SCMT), which tracks multiple vehicles to generate candidate trajectories within each camera on the basis of the detected boxes and appearance features. The last module is Inter-Camera Association (ICA), which associates all candidate trajectories between two successive cameras using the K-reciprocal nearest neighbors algorithm, and combines all successively matched trajectories for final results. The ICA module takes the constraints of traveling time, road topology structures, and traffic rules into consideration to reduce the searching space and accelerate the matching speed. Experiments results on the public test set of 2022 AI CITY CHALLENGE Track1 demonstrate the effectiveness of our method, which achieves IDF1 of 84.86%, ranking 1st on the leaderboard.",
    "code_link": ""
  },
  "cvpr2022_aicity_tracked-vehicleretrievalbynaturallanguagedescriptionswithdomainadaptiveknowledge": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Tracked-Vehicle Retrieval by Natural Language Descriptions With Domain Adaptive Knowledge",
    "authors": [
      "Huy Dinh-Anh Le",
      "Quang Qui-Vinh Nguyen",
      "Vuong Ai Nguyen",
      "Thong Duy-Minh Nguyen",
      "Nhat Minh Chung",
      "Tin-Trung Th\u00e1i",
      "Synh Viet-Uyen Ha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Le_Tracked-Vehicle_Retrieval_by_Natural_Language_Descriptions_With_Domain_Adaptive_Knowledge_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Le_Tracked-Vehicle_Retrieval_by_Natural_Language_Descriptions_With_Domain_Adaptive_Knowledge_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper introduces our solution for Track 2 in AI City Challenge 2022. Track 2 task is TrackedVehicle Retrieval by Natural Language Descriptions with a real-world dataset with different scenarios and multi-camera. We mainly focus on developing a robust natural language-based vehicle retrieval system to address the domain bias problem due to unseen scenarios and multi-view multi-camera vehicle tracks. Specifically, we apply CLIP to effectively extract both visual and textual representation for contrastive representation learning. Furthermore, Since there are new scenarios in the test set, we propose a new Domain Adaptive Training method that utilizes the information from labeled data and transfers it to unlabeled data to generate pseudo labels. By using this simple and effective strategy, we not only breach the domain gap between the training set and test set but also require less computation cost and data compared to previous top performance methods. Finally, we use a post-processing method called pruning to eliminate the wrong retrieved vehicle track. Taking one step further, we also investigate the impact of different text formats and the number of pseudo labels data for the fine-tuning process. Our proposed method has achieved 3rd place on the AI City Challenge 2022, yielding a competitive performance of 47.73% MRR accuracy on the private test set, which verified the effectiveness and scalability of the proposed solution.",
    "code_link": ""
  },
  "cvpr2022_aicity_city-scalemulti-cameravehicletrackingbasedonspace-time-appearancefeatures": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "City-Scale Multi-Camera Vehicle Tracking Based on Space-Time-Appearance Features",
    "authors": [
      "Hui Yao",
      "Zhizhao Duan",
      "Zhen Xie",
      "Jingbo Chen",
      "Xi Wu",
      "Duo Xu",
      "Yutao Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Yao_City-Scale_Multi-Camera_Vehicle_Tracking_Based_on_Space-Time-Appearance_Features_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Yao_City-Scale_Multi-Camera_Vehicle_Tracking_Based_on_Space-Time-Appearance_Features_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multi-Camera Multi-Vehicle Tracking (MCMVT) is an essential task in the field of city-scale traffic management, which usually consists of three sub-tasks: object detection and re-identification (ReID), single-camera tracking, cross-camera trajectory association. Compared with existing methods, two challenges are considered and addressed in this paper: (1) low-confidence objects could be missed without extra data annotation, (2) precise association of trajectories from different cameras is affected by multiple factors. For the first challenge, a cascaded tracking method based on detection, appearance features and trajectory interpolation is proposed, exploiting potential real targets in low-confidence objects to improve detection and identification recall. For the second challenge, space, time and appearance features are proposed to be the most crucial factors for trajectory association, so a zone-gate and time-decay based matching mechanism is proposed to adjust original appearance matrix to link tracklets more precisely from different cameras. Extensive experimental results validate the effectiveness of the proposed innovative technologies.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "cvpr2022_aicity_aneffectiveframeworkofmulti-classproductcountingandrecognitionforautomatedretailcheckout": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "An Effective Framework of Multi-Class Product Counting and Recognition for Automated Retail Checkout",
    "authors": [
      "Junfeng Wan",
      "Shuhao Qian",
      "Zihan Tian",
      "Yanyun Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Wan_An_Effective_Framework_of_Multi-Class_Product_Counting_and_Recognition_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Wan_An_Effective_Framework_of_Multi-Class_Product_Counting_and_Recognition_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "As the field of computer vision grows, Automated Retail Checkout has become a highly anticipated development goal. The key of this task is to improve the accuracy rate. If there is an error, it will bring serious losses to the business and awful experience for customers which is not our expected. This competition gives us an opportunity to simulate check-out in a real world scenario, so that we can identify problems and solve them, not only for the competition, but also for the practical application. As one of the participating teams in this task, we pursue the goal of avoiding misdetection and misclassification, and build a complete set of framework to achieve high-precise, high-recall performance. In addition, there is an excessive difference between the training data and test data. How to use limited data to make up for the differences in this part is also one of the highlights of our framework. In general, our framework consists of three main parts. Firstly, the Pre-Processing module to make up for the differences between training and test data. The DTC module completes the overall process of automatic recognition. Finally the MTCR module is proposed to post-process the output of the DTC module. On the TestA data of AICITY2022 Task 4, we have achieved significant result compared to the other teams. Finally, our model is ranked 1st in AICITY2022 Task 4. The code is available at: https://github.com/w-sugar/DTC_AICITY2022.",
    "code_link": ""
  },
  "cvpr2022_aicity_deepacoarobustdeeplearning-basedautomaticcheckoutsystem": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "DeepACO: A Robust Deep Learning-Based Automatic Checkout System",
    "authors": [
      "Long Hoang Pham",
      "Duong Nguyen-Ngoc Tran",
      "Huy-Hung Nguyen",
      "Tai Huu-Phuong Tran",
      "Hyung-Joon Jeon",
      "Hyung-Min Jeon",
      "Jae Wook Jeon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Pham_DeepACO_A_Robust_Deep_Learning-Based_Automatic_Checkout_System_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Pham_DeepACO_A_Robust_Deep_Learning-Based_Automatic_Checkout_System_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The retail industry has seen an increasing growth of artificial intelligence and computer vision applications. Of the various topics, automatic checkout (ACO) in retail stores or supermarkets has emerged as one of the critical tasks in this area. Several problems stem from real-world scenarios such as object occlusion, blurring from scanning motion, and similarity in scanned items. Moreover, the challenge also comes from the difficulty of collecting training images that reflect the realistic checkout scenarios due to continuous updates of the products. This paper proposes a deep learning-based automatic checkout system (DeepACO) to recognize, localize, track, and count products as they move along a retail check-out conveyor belt. The DeepACO follows the detect-and-track approach, i.e., applying trackers on detected bounding boxes. It also provides a completed pipeline for generating large training datasets under various environments from synthetic data. The proposed system has been evaluated on the 2022 AI City Challenge Track 4 benchmark (Multi-Class Product Counting & Recognition for Automated Retail Checkout). Compared to other state-of-the-art solutions, it has shown outstanding results, achieving top-2 on the test-set A with the F1 score of 0.4783.",
    "code_link": ""
  },
  "cvpr2022_aicity_multi-cameramulti-vehicletrackingwithdomaingeneralizationandcontextualconstraints": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Multi-Camera Multi-Vehicle Tracking With Domain Generalization and Contextual Constraints",
    "authors": [
      "Nhat Minh Chung",
      "Huy Dinh-Anh Le",
      "Vuong Ai Nguyen",
      "Quang Qui-Vinh Nguyen",
      "Thong Duy-Minh Nguyen",
      "Tin-Trung Th\u00e1i",
      "Synh Viet-Uyen Ha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Chung_Multi-Camera_Multi-Vehicle_Tracking_With_Domain_Generalization_and_Contextual_Constraints_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Chung_Multi-Camera_Multi-Vehicle_Tracking_With_Domain_Generalization_and_Contextual_Constraints_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we develop and propose a system for Multi-Camera Multi-Target (MCMT) Vehicle Tracking in Track 1 of AI City Challenge 2022. There are many technical difficulties to the MCMT problem such as a common lack of labelled data in real scenarios, a distortion of vehicle detailed appearances in recording, and ambiguity between highly similar vehicles. Taking those into account, we develop a 3-component MCMT system that exploits vehicle behavior, leverages synthetic data and augmentation techniques to exploit as much labeled data as possible, and enforce contextual constraints to address ambiguity in terms of vehicle appearances. Specifically, our system involves a motion-driven vehicle tracker, applying MixStyle domain generalization on the TransReID model, and experiment with various constraints such as neighbour matching.",
    "code_link": ""
  },
  "cvpr2022_aicity_symmetricnetworkwithspatialrelationshipmodelingfornaturallanguage-basedvehicleretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - AI City Challenge",
    "title": "Symmetric Network With Spatial Relationship Modeling for Natural Language-Based Vehicle Retrieval",
    "authors": [
      "Chuyang Zhao",
      "Haobo Chen",
      "Wenyuan Zhang",
      "Junru Chen",
      "Sipeng Zhang",
      "Yadong Li",
      "Boxun Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Zhao_Symmetric_Network_With_Spatial_Relationship_Modeling_for_Natural_Language-Based_Vehicle_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Zhao_Symmetric_Network_With_Spatial_Relationship_Modeling_for_Natural_Language-Based_Vehicle_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Natural language (NL) based vehicle retrieval aims to search specific vehicle given text description. Different from the image-based vehicle retrieval, NL-based vehicle retrieval requires considering not only vehicle appearance, but also surrounding environment and temporal relations. In this paper, we propose a Symmetric Network with Spatial Relationship Modeling (SSM) method for NL-based vehicle retrieval. Specifically, we design a symmetric network to learn the unified cross-modal representations between text descriptions and vehicle images, where vehicle appearance details and vehicle trajectory global information are preserved. Besides, to make better use of location information, we propose a spatial relationship modeling methods to take surrounding environment and mutual relationship between vehicles into consideration. The qualitative and quantitative experiments verify the effectiveness of the proposed method. We achieve 43.92% MRR accuracy on the test set of the 6th AI City Challenge on natural language-based vehicle retrieval track, yielding the 4th place on the public leaderboard. The code will be available at https://github.com/hbchen121/AICITY2022_Track2_SSM.",
    "code_link": "https://github.com/hbchen121/AICITY2022_Track2"
  },
  "cvpr2022_fedvision_adaptivedifferentialfiltersforfastandcommunication-efficientfederatedlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FedVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Federated Learning for Computer Vision",
    "title": "Adaptive Differential Filters for Fast and Communication-Efficient Federated Learning",
    "authors": [
      "Daniel Becking",
      "Heiner Kirchhoffer",
      "Gerhard Tech",
      "Paul Haase",
      "Karsten M\u00fcller",
      "Heiko Schwarz",
      "Wojciech Samek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/html/Becking_Adaptive_Differential_Filters_for_Fast_and_Communication-Efficient_Federated_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/papers/Becking_Adaptive_Differential_Filters_for_Fast_and_Communication-Efficient_Federated_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Federated learning (FL) scenarios inherently generate a large communication overhead by frequently transmitting neural network updates between clients and server. To minimize the communication cost, introducing sparsity in conjunction with differential updates is a commonly used technique. However, sparse model updates can slow down convergence speed or unintentionally skip certain update aspects, e.g., learned features, if error accumulation is not properly addressed. In this work, we propose a new scaling method operating at the granularity of convolutional filters which 1) compensates for highly sparse updates in FL processes, 2) adapts the local models to new data domains by enhancing some features in the filter space while diminishing others and 3) motivates extra sparsity in updates and thus achieves higher compression ratios, i.e., savings in the overall data transfer. Compared to unscaled updates and previous work, experimental results on different computer vision tasks (Pascal VOC, CIFAR10, Chest X-Ray) and neural networks (ResNets, MobileNets, VGGs) in uni-, bidirectional and partial update FL settings show that the proposed method improves the performance of the central server model while converging faster and reducing the total amount of transmitted data by up to 377 times.",
    "code_link": ""
  },
  "cvpr2022_fedvision_mpafmodelpoisoningattackstofederatedlearningbasedonfakeclients": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FedVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Federated Learning for Computer Vision",
    "title": "MPAF: Model Poisoning Attacks to Federated Learning Based on Fake Clients",
    "authors": [
      "Xiaoyu Cao",
      "Neil Zhenqiang Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/html/Cao_MPAF_Model_Poisoning_Attacks_to_Federated_Learning_Based_on_Fake_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/papers/Cao_MPAF_Model_Poisoning_Attacks_to_Federated_Learning_Based_on_Fake_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Existing model poisoning attacks to federated learning assume that an attacker has access to a large fraction of compromised genuine clients. However, such assumption is not realistic in production federated learning systems that involve millions of clients. In this work, we propose the first Model Poisoning Attack based on Fake clients called MPAF. Specifically, we assume the attacker injects fake clients to a federated learning system and sends carefully crafted fake local model updates to the cloud server during training, such that the learnt global model has low accuracy for many indiscriminate test inputs. Towards this goal, our attack drags the global model towards an attacker-chosen base model that has low accuracy. Specifically, in each round of federated learning, the fake clients craft fake local model updates that point to the base model and scale them up to amplify their impact before sending them to the cloud server. Our experiments show that MPAF can significantly decrease the test accuracy of the global model, even if classical defenses and norm clipping are adopted, highlighting the need for more advanced defenses.",
    "code_link": ""
  },
  "cvpr2022_fedvision_fediristowardsmoreaccurateandprivacy-preservingirisrecognitionviafederatedtemplatecommunication": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FedVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Federated Learning for Computer Vision",
    "title": "FedIris: Towards More Accurate and Privacy-Preserving Iris Recognition via Federated Template Communication",
    "authors": [
      "Zhengquan Luo",
      "Yunlong Wang",
      "Zilei Wang",
      "Zhenan Sun",
      "Tieniu Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/html/Luo_FedIris_Towards_More_Accurate_and_Privacy-Preserving_Iris_Recognition_via_Federated_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/papers/Luo_FedIris_Towards_More_Accurate_and_Privacy-Preserving_Iris_Recognition_via_Federated_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "As biometric data undergo rapidly growing privacy concerns, building large-scale datasets has become more difficult. Unfortunately, current iris databases are mostly in small scale, e.g., thousands of iris images from hundreds of identities. What's worse, the heterogeneity among decentralized iris datasets hinders the current deep learning (DL) frameworks from obtaining recognition performance with robust generalization. It motivates us to leverage the merits of federated learning (FL) to solve these problems. However, traditional FL algorithms often employ model sharing for knowledge transfer, wherein the simple averaging aggregation lacks interpretability, and divergent optimization directions of clients lead to performance degradation. To overcome this interference, we propose FedIris with solid theoretical foundations, which attempts to employ the iris template as the communication carrier and formulate federated triplet (Fed-Triplet) for knowledge transfer. Furthermore, the massive heterogeneity among iris datasets may induce negative transfer and unstable optimization. The modified Wasserstein distance is embedded into the FedTriplet loss to reweight global aggregation, which drives the clients with similar data distributions to contribute more mutually. Extensive experimental results demonstrate that the proposed FedIris outperforms SOLO training, model-sharing-based FL training, and even centralized training.",
    "code_link": ""
  },
  "cvpr2022_fedvision_communication-efficientfederateddataaugmentationonnon-iiddata": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FedVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Federated Learning for Computer Vision",
    "title": "Communication-Efficient Federated Data Augmentation on Non-IID Data",
    "authors": [
      "Hui Wen",
      "Yue Wu",
      "Jingjing Li",
      "Hancong Duan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/html/Wen_Communication-Efficient_Federated_Data_Augmentation_on_Non-IID_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/papers/Wen_Communication-Efficient_Federated_Data_Augmentation_on_Non-IID_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Federated learning (FL) is an attractive distributed machine learning framework due to the property of privacy preservation. The implementation of FL encounters the challenge of the Non-Independent and Identically Distributed (Non-IID) data across devices. This work focuses on mitigating the impact of Non-IID datasets in wireless communications. To achieve this goal, we propose a generative models-based federated data augmentation strategy (FedDA) with privacy preservation and communication efficiency. In FedDA, the Conditional AutoEncoder (CVAE) is adopted to generate the missing samples on Non-IID datasets. The Knowledge Distillation Mechanism is introduced to achieve Federated learning, through which knowledge is shared, rather than model parameters or gradients. The knowledge is designed based on the hidden-layer features to reduce the communication overhead and protect raw data privacy. Meanwhile, to generate cross-class samples that are easy to classify, the latent variables in CVAE are constrained and the attention mechanism is introduced. Extensive experiments are conducted on Fashion-MNIST and CIFAR-10 with different data distributions. The results show that FedDA can improve the model accuracy by up to 8% while reducing the communication overhead by up to 2x, compared to classic baselines with highly Non-IID data.",
    "code_link": ""
  },
  "cvpr2022_fedvision_doesfederateddropoutactuallywork?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "FedVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Federated Learning for Computer Vision",
    "title": "Does Federated Dropout Actually Work?",
    "authors": [
      "Gary Cheng",
      "Zachary Charles",
      "Zachary Garrett",
      "Keith Rush"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/html/Cheng_Does_Federated_Dropout_Actually_Work_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/FedVision/papers/Cheng_Does_Federated_Dropout_Actually_Work_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Model sizes are limited in Federated Learning due to network bandwidth and on-device memory constraints. The success of increasing model sizes in other machine learning domains motivates the development of methods for training large-scale models in Federated Learning. To this end, Caldas et al. draws inspiration from dropout and proposes Federated Dropout: an algorithm where clients train randomly selected subsets of a larger server model. Despite the promising empirical results and the many other works that build on it, we argue in this paper that the metrics used to measure the performance of Federated Dropout and its variants are misleading. We propose and perform new experiments which suggest that Federated Dropout is actually detrimental to scaling efforts. We show how a simple ensembling technique outperforms Federated Dropout and other baselines. We perform ablations that suggest that the best performing variations of Federated Dropout approximate ensembling. The simplicity of ensembling allows for easy, practical implementations. Furthermore, ensembling naturally leverages the parallelizable nature of Federated Learning---recall that it is easy to train several models independently because there are a lot of clients and server-compute is not the bottleneck. Ensembling's strong performance against our baselines suggests that Federated Learning models may be more easily scaled than previously thought with more sophisticated ensembling strategies e.g., via boosting.",
    "code_link": ""
  },
  "cvpr2022_rose_pose-basedcontrastivelearningfordomainagnosticactivityrepresentations": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "RoSe",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Robustness in Sequential Data",
    "title": "Pose-Based Contrastive Learning for Domain Agnostic Activity Representations",
    "authors": [
      "David Schneider",
      "Saquib Sarfraz",
      "Alina Roitberg",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/html/Schneider_Pose-Based_Contrastive_Learning_for_Domain_Agnostic_Activity_Representations_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Schneider_Pose-Based_Contrastive_Learning_for_Domain_Agnostic_Activity_Representations_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "While recognition accuracies of video classification models trained on conventional benchmarks are gradually saturating, recent studies raise alarm about the learned representations not generalizing well across different domains. Learning abstract concepts behind an activity instead of overfitting to the appearances and biases of a specific benchmark domain is vital for building generalizable behaviour understanding models. In this paper, we introduce Pose-based High Level View Contrasting (P-HLVC), a novel method that leverages human pose dynamics as a supervision signal aimed at learning domain-invariant activity representations. Our model learns to link image sequences to more abstract body pose information through iterative contrastive clustering and the Sinkhorn-Knopp algorithm, providing us with video representations more resistant to domain shifts. We demonstrate the effectiveness of our approach in a cross-domain action recognition setting and achieve significant improvements on the synthetic-to-real Sims4Action benchmark.",
    "code_link": "https://github.com/simplexsigil/p-hlvc"
  },
  "cvpr2022_rose_analysisandextensionsofadversarialtrainingforvideoclassification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "RoSe",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Robustness in Sequential Data",
    "title": "Analysis and Extensions of Adversarial Training for Video Classification",
    "authors": [
      "Kaleab A. Kinfu",
      "Ren\u00e9 Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/html/Kinfu_Analysis_and_Extensions_of_Adversarial_Training_for_Video_Classification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Kinfu_Analysis_and_Extensions_of_Adversarial_Training_for_Video_Classification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Adversarial training (AT) is a simple yet effective defense against adversarial attacks to image classification systems, which is based on augmenting the training set with attacks that maximize the loss. However, the effectiveness of AT as a defense for video classification has not been thoroughly studied. Our first contribution is to show that generating optimal attacks for video requires carefully tuning the attack parameters, especially the step size. Notably, we show that the optimal step size varies linearly with the attack budget. Our second contribution is to show that using a smaller (sub-optimal) attack budget at training time leads to a more robust performance at test time. Based on these findings, we propose three defenses against attacks with variable attack budgets. The first one, Adaptive AT, is a technique where the attack budget is drawn from a distribution that is adapted as training iterations proceed. The second, Curriculum AT, is a technique where the attack budget is increased as training iterations proceed. The third, Generative AT, further couples AT with a denoising generative adversarial network to boost robust performance. Experiments on the UCF-101 dataset demonstrate that the proposed methods improve adversarial robustness against multiple attack types.",
    "code_link": ""
  },
  "cvpr2022_rose_cenetconsolidation-and-explorationnetworkforcontinuousdomainadaptation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "RoSe",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Robustness in Sequential Data",
    "title": "CENet: Consolidation-and-Exploration Network for Continuous Domain Adaptation",
    "authors": [
      "Chi Zhang",
      "Yalu Cheng",
      "Pengxu Wei",
      "Hongliang He",
      "Jie Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/html/Zhang_CENet_Consolidation-and-Exploration_Network_for_Continuous_Domain_Adaptation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Zhang_CENet_Consolidation-and-Exploration_Network_for_Continuous_Domain_Adaptation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Unsupervised Domain Adaptation (UDA) deals with transferring knowledge from labeled source domains to an unlabeled target domain under domain shift. However, this does not reflect the breadth of scenarios that arise in real-world applications since source domains could increase. A plausible conjecture is: can we train a lifelong learning model learned on continuous source domains given the target without the presence of labels? We formalize this task as the Continuous Domain Adaptation (CDA) and empirically show that conventional domain adaptation methods may suffer severe generalization deterioration due to the limited incremental transferability and negative transfer. To tackle this issue, we propose a novel sample-to-sample framework---Consolidation-and-Exploration Network (CENet) to facilitate incremental transferring. This method underscores both the qualitative and quantitative relationship between samples. Moreover, we conduct comprehensive experiments to evaluate the effectiveness of each component in our pair-based method. Extensive experiments show that our approach achieves significant improvement over related state-of-the-art methods. Our source code will be publicly available.",
    "code_link": "https://github.com/GekFreeman/continuous_da"
  },
  "cvpr2022_rose_tragedyplustimecapturingunintendedhumanactivitiesfromweakly-labeledvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "RoSe",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Robustness in Sequential Data",
    "title": "Tragedy Plus Time: Capturing Unintended Human Activities From Weakly-Labeled Videos",
    "authors": [
      "Arnav Chakravarthy",
      "Zhiyuan Fang",
      "Yezhou Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/html/Chakravarthy_Tragedy_Plus_Time_Capturing_Unintended_Human_Activities_From_Weakly-Labeled_Videos_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Chakravarthy_Tragedy_Plus_Time_Capturing_Unintended_Human_Activities_From_Weakly-Labeled_Videos_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In videos that contain actions performed unintentionally, agents do not achieve their desired goals. In such videos, it is challenging for computer vision systems to understand high-level concepts such as goal-directed behavior, an ability present in humans from a very early age. Inculcating this ability in artificially intelligent agents would make them better social learners by allowing them to evaluate human action under a teleological lens. To validate this ability of deep learning models to perform this task, we curate the W-Oops dataset, built upon the Oops dataset. W-Oops consists of 2,100 unintentional human action videos, with 44 goal-directed and 30 unintentional video-level activity labels collected through human annotations. Due to the expensive segment annotation procedure, we propose a weakly supervised algorithm for localizing the goal-directed as well as unintentional temporal regions in the video leveraging solely video-level labels. In particular, we employ an attention mechanism based strategy that predicts the temporal regions which contributes the most to a classification task. Meanwhile, our designed overlap regularization allows the model to focus on distinct portions of the video for inferring the goal-directed and unintentional activity, while guaranteeing their temporal ordering. Extensive quantitative experiments verify the validity of our localization method. We further conduct a video captioning experiment which demonstrates that the proposed localization module does indeed assist teleological action understanding. Project website can be found at: https://asu-apg.github.io/TragedyPlusTime.",
    "code_link": ""
  },
  "cvpr2022_rose_continualactiveadaptationtoevolvingdistributionalshifts": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "RoSe",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Robustness in Sequential Data",
    "title": "Continual Active Adaptation to Evolving Distributional Shifts",
    "authors": [
      "Amrutha Machireddy",
      "Ranganath Krishnan",
      "Nilesh Ahuja",
      "Omesh Tickoo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/html/Machireddy_Continual_Active_Adaptation_to_Evolving_Distributional_Shifts_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Machireddy_Continual_Active_Adaptation_to_Evolving_Distributional_Shifts_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Building neural network models that are adaptable to evolving data distributions without suffering catastrophic forgetting is important for real-world deployment in many applications. In real-world setting, the observed data distribution changes over time due to non-stationary environment. In this paper, we consider the problem of evolving covariate shift and propose source-free active adaptation method to fine-tune the neural networks to continually evolving data without catastrophic forgetting. We evaluate the model performance with respect to adaptation as well as forgetting under sequential evolution of data based on fifteen different common corruptions and perturbations from CIFAR10-C related to shift in lighting, weather, noise etc. We demonstrate the proposed method improves model accuracy to the continually evolving data by 21.3% on an average over the different covariate shifts without catastrophic forgetting.",
    "code_link": ""
  },
  "cvpr2022_cvsports_end-to-endhigh-risktackledetectionsystemforrugby": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "End-to-End High-Risk Tackle Detection System for Rugby",
    "authors": [
      "Naoki Nonaka",
      "Ryo Fujihira",
      "Monami Nishio",
      "Hidetaka Murakami",
      "Takuya Tajima",
      "Mutsuo Yamada",
      "Akira Maeda",
      "Jun Seita"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Nonaka_End-to-End_High-Risk_Tackle_Detection_System_for_Rugby_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Nonaka_End-to-End_High-Risk_Tackle_Detection_System_for_Rugby_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Reducing risk of severe injury such as concussion is a high priority for any contact sports. In rugby, Head Injury Assessment (HIA) protocol has been introduced to identify and protect players showing symptoms of concussion and having potential risk of concussion. However, on-field decisions by officials are sometimes difficult and subjective, and HIA is affordable only for elite leagues since it requires medical specialists. To make rugby matches more safe, we aim to develop a system to detect high-risk tackles, potential triggers of concussion, based on deep learning models. Our system takes rugby match video, then first identifies frame with tackle, subsequently detects location of tackle and estimate pose of the ball carrier and the tackler, and finally evaluate the risk of tackle using posture pair of players. Among the model combinations we have examined, the best performance was achieved with the combination of ResNet (2+1)D as tackle frame selection model, RetinaNet as tackle detection model and CenterTrack as pose estimation model. Evaluation using test data, a set of short clips from broadcasted rugby match videos, showed our system was able to detect 50% of high-risk tackles without any human intervention. This result opens a path for automated systems to detect high-risk events, leading to less expensive and more objective monitoring not only for rugby but also for any contact sports.",
    "code_link": ""
  },
  "cvpr2022_cvsports_recognitionoffreelyselectedkeypointsonhumanlimbs": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Recognition of Freely Selected Keypoints on Human Limbs",
    "authors": [
      "Katja Ludwig",
      "Daniel Kienzle",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Ludwig_Recognition_of_Freely_Selected_Keypoints_on_Human_Limbs_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Ludwig_Recognition_of_Freely_Selected_Keypoints_on_Human_Limbs_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nearly all Human Pose Estimation (HPE) datasets consist of a fixed set of keypoints. Standard HPE models trained on such datasets can only detect these keypoints. If more points are desired, they have to be manually annotated and the model needs to be retrained. Our approach leverages the Vision Transformer architecture to extend the capability of the model to detect arbitrary keypoints on the limbs of persons. We propose two different approaches to encode the desired keypoints. (1) Each keypoint is defined by its position along the line between the two enclosing keypoints from the fixed set and its relative distance between this line and the edge of the limb. (2) Keypoints are defined as coordinates on a norm pose. Both approaches are based on the TokenPose architecture, while the keypoint tokens that correspond to the fixed keypoints are replaced with our novel module. Experiments show that our approaches achieve similar results to TokenPose on the fixed keypoints and are capable of detecting arbitrary keypoints on the limbs.",
    "code_link": ""
  },
  "cvpr2022_cvsports_fencenetfine-grainedfootworkrecognitioninfencing": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "FenceNet: Fine-Grained Footwork Recognition in Fencing",
    "authors": [
      "Kevin Zhu",
      "Alexander Wong",
      "John McPhee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Zhu_FenceNet_Fine-Grained_Footwork_Recognition_in_Fencing_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Zhu_FenceNet_Fine-Grained_Footwork_Recognition_in_Fencing_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Current data analysis for the Canadian Olympic fencing team is primarily done manually by coaches and analysts. Due to the highly repetitive, yet dynamic and subtle movements in fencing, manual data analysis can be inefficient and inaccurate. We propose FenceNet as a novel architecture to automate the classification of fine-grained footwork techniques in fencing. FenceNet takes 2D pose data as input and classifies actions using a skeleton-based action recognition approach that incorporates temporal convolutional networks to capture temporal information. We train and evaluate FenceNet on the Fencing Footwork Dataset (FFD), which contains 10 fencers performing 6 different footwork actions for 10-11 repetitions each (652 total videos). FenceNet achieves 85.4% accuracy under 10-fold cross-validation, where each fencer is left out as the test set. This accuracy is within 1% of the current state-of-the-art method, JLJA (86.3%), which selects and fuses features engineered from skeleton data, depth videos, and inertial measurement units. BiFenceNet, a variant of FenceNet that captures the \"bidirectionality\" of human movement through two separate networks, achieves 87.6% accuracy, outperforming JLJA. Since neither FenceNet nor BiFenceNet requires data from wearable sensors, unlike JLJA, they could be directly applied to most fencing videos, using 2D pose data as input extracted from off-the-shelf 2D human pose estimators. In comparison to JLJA, our methods are also simpler as they do not require manual feature engineering, selection, or fusion.",
    "code_link": ""
  },
  "cvpr2022_cvsports_3dballlocalizationfromasinglecalibratedimage": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "3D Ball Localization From a Single Calibrated Image",
    "authors": [
      "Gabriel Van Zandycke",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Van_Zandycke_3D_Ball_Localization_From_a_Single_Calibrated_Image_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Van_Zandycke_3D_Ball_Localization_From_a_Single_Calibrated_Image_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "3D ball localization in team sports has various applications including automatic offside detection in soccer, or shot release localization in basketball. Today, this task is either resolved by using expensive multi-views setups, or by restricting the analysis to ballistic trajectories. In this work, we propose to address the task on a single image from a calibrated monocular camera by estimating ball diameter in pixels and use the knowledge of real ball diameter in meters. This approach is suitable to any game situation where the ball is (even partly) visible. To achieve this, we use a small neural network trained on image patches around candidates generated by a conventional ball detector. Beside predicting ball diameter, our network outputs the confidence of having a ball in the image patch. Validations on 3 basketball datasets reveals that our model gives remarkable predictions on ball 3D localization. In addition, through its confidence output, our model improves the detection rate by filtering the candidates produced by the detector. The contributions of this work are (i) the first model to address 3D ball localization on a single image, (ii) an effective method for ball 3D annotation from single calibrated images, (iii) a high quality 3D ball evaluation dataset annotated from a single viewpoint. In addition, the code to reproduce this research is made freely available.",
    "code_link": ""
  },
  "cvpr2022_cvsports_semi-supervisedtrainingtoimproveplayerandballdetectioninsoccer": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Semi-Supervised Training To Improve Player and Ball Detection in Soccer",
    "authors": [
      "Renaud Vandeghen",
      "Anthony Cioppa",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Vandeghen_Semi-Supervised_Training_To_Improve_Player_and_Ball_Detection_in_Soccer_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Vandeghen_Semi-Supervised_Training_To_Improve_Player_and_Ball_Detection_in_Soccer_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Accurate player and ball detection has become increasingly important in recent years for sport analytics. As most state-of-the-art methods rely on training deep learning networks in a supervised fashion, they require huge amounts of annotated data, which are rarely available. In this paper, we present a novel generic semi-supervised method to train a network based on a labeled image dataset by leveraging a large unlabeled dataset of soccer broadcast videos. More precisely, we design a teacher-student approach in which the teacher produces surrogate annotations on the unlabeled data to be used later for training a student which has the same architecture as the teacher. Furthermore, we introduce three training loss parametrizations that allow the student to doubt the predictions of the teacher during training depending on the proposal confidence score. We show that including unlabeled data in the training process allows to substantially improve the performances of the detection network trained only on the labeled data. Finally, we provide a thorough performance study including different proportions of labeled and unlabeled data, and establish the first benchmark on the new SoccerNet-v3 detection task, with an mAP of 52.3%. Our code is available at [https://github.com/rvandeghen/SST].",
    "code_link": "https://github.com/rvandeghen/SST"
  },
  "cvpr2022_cvsports_sportsfieldregistrationviakeypoints-awarelabelcondition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Sports Field Registration via Keypoints-Aware Label Condition",
    "authors": [
      "Yen-Jui Chu",
      "Jheng-Wei Su",
      "Kai-Wen Hsiao",
      "Chi-Yu Lien",
      "Shu-Ho Fan",
      "Min-Chun Hu",
      "Ruen-Rone Lee",
      "Chih-Yuan Yao",
      "Hung-Kuo Chu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Chu_Sports_Field_Registration_via_Keypoints-Aware_Label_Condition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Chu_Sports_Field_Registration_via_Keypoints-Aware_Label_Condition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a novel deep learning framework for sports field registration. The typical algorithmic flow for sports field registration involves extracting field-specific features (e.g., corners, lines, etc.) from field image and estimating the homography matrix between a 2D field template and the field image using the extracted features. Unlike previous methods that strive to extract sparse field features from field images with uniform appearance, we tackle the problem differently. First, we use a grid of uniformly distributed keypoints as our field-specific features to increase the likelihood of having sufficient field features under various camera poses. Then we formulate the keypoints detection problem as an instance segmentation with dynamic filter learning. In our model, the convolution filters are generated dynamically, conditioned on the field image and associated keypoint identity, thus improving the robustness of prediction results. To extensively evaluate our method, we introduce a new soccer dataset, called TS-WorldCup, with detailed field markings on 3812 time-sequence images from 43 videos of Soccer World Cup 2014 and 2018. The experimental results demonstrate that our method outperforms state-of-the-arts on the TS-WorldCup dataset in both quantitative and qualitative evaluations. Both the code and dataset are available online.",
    "code_link": ""
  },
  "cvpr2022_cvsports_passreceiverpredictioninsoccerusingvideoandplayerstrajectories": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Pass Receiver Prediction in Soccer Using Video and Players' Trajectories",
    "authors": [
      "Yutaro Honda",
      "Rei Kawakami",
      "Ryota Yoshihashi",
      "Kenta Kato",
      "Takeshi Naemura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Honda_Pass_Receiver_Prediction_in_Soccer_Using_Video_and_Players_Trajectories_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Honda_Pass_Receiver_Prediction_in_Soccer_Using_Video_and_Players_Trajectories_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In soccer, passing is one of the most fundamental actions for building tactics. Automatic prediction of the pass receiver can be useful in many situations, such as in player and team analysis and entertainment. In previous studies, the prediction is based on tracking data, in particular, time-series data of the two-dimensional positions of the players on the field, and little use has been made of video information such as the players' own posture and facial orientation. Thus, this paper aims to build a pass receiver prediction model that combines visual information with the trajectories of the players and the ball. We extract the features of the players' body movements from the video and the features of their movements on the field from the trajectories by using 3D convolutional networks and long short-term memory and learn the interactions between each player by using a transformer. Our study evaluation used wide-angle video and tracking data of 20 players, i.e., all players on the field excluding the goalkeepers. The results show that the prediction accuracy is greatly improved by using the video information.",
    "code_link": ""
  },
  "cvpr2022_cvsports_watchandactdualinteractingagentsforautomaticgenerationofpossessionstatisticsinsoccer": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Watch and Act: Dual Interacting Agents for Automatic Generation of Possession Statistics in Soccer",
    "authors": [
      "Saikat Sarkar",
      "Dipti Prasad Mukherjee",
      "Amlan Chakrabarti"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Sarkar_Watch_and_Act_Dual_Interacting_Agents_for_Automatic_Generation_of_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Sarkar_Watch_and_Act_Dual_Interacting_Agents_for_Automatic_Generation_of_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Pass localization and team identification are two primary tasks for pass-count based possession statistics generation of a soccer match. While the existing works perform these two tasks separately, we propose dual interacting reinforcement learning agents to jointly perform these tasks. The proposed model has a localization agent, that decides which direction to move a temporal window to localize a pass. On the other hand, there is an identification agent that decides if the temporal window contains a pass for team-A (or team-B), or the localization agent needs to readjust the temporal window further. In this multi-agent setup, an agent may communicate by sharing some message to guide the other agent to achieve its task. To achieve this inter-agent communication, we extend the Dueling DQN architecture and share the value of a state as a message to the other agent. Two agents watch, act independently and cooperate with each other in order to detect a valid pass in a soccer video. A novel reward function is proposed that helps the agents to learn the optimal policy. Experiments performed on online videos show that our method is 3% better at localization of pass than the competitive methods.",
    "code_link": ""
  },
  "cvpr2022_cvsports_interactionclassificationwithkeyactordetectioninmulti-personsportsvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Interaction Classification With Key Actor Detection in Multi-Person Sports Videos",
    "authors": [
      "Farzaneh Askari",
      "Rohit Ramaprasad",
      "James J. Clark",
      "Martin D. Levine"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Askari_Interaction_Classification_With_Key_Actor_Detection_in_Multi-Person_Sports_Videos_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Askari_Interaction_Classification_With_Key_Actor_Detection_in_Multi-Person_Sports_Videos_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Interaction recognition from multi-person videos is a challenging yet essential task in computer vision. Often the videos depict actions with multiple actors involved, some of whom participate in the main event, and the rest are present in the scene without being part of the actual event. This paper proposes a model to tackle the problem of interaction recognition from multi-person videos. Our model consists of a Recurrent Neural Network (RNN) equipped with a time-varying attention mechanism. It receives scene features and localized actors features to predict the interaction class. Additionally, the attention model identifies the people responsible for the main event. We chose penalty classification from ice hockey broadcast videos as our application. These videos are multi-persons and depict complex interactions between players in a non-laboratory recording setup. We evaluate our model on a new dataset of ice hockey penalty videos and report 93.93% classification accuracy. We include a qualitative analysis of the attention mechanism by visualizing the attention weights. Our code is publicly available.",
    "code_link": "https://github.com/SummerVideoAnalysis/Interaction-Classificationwith-Key-Actor-Detection-in-Videos"
  },
  "cvpr2022_cvsports_soccernet-trackingmultipleobjecttrackingdatasetandbenchmarkinsoccervideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos",
    "authors": [
      "Anthony Cioppa",
      "Silvio Giancola",
      "Adrien Deli\u00e8ge",
      "Le Kang",
      "Xin Zhou",
      "Zhiyu Cheng",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Cioppa_SoccerNet-Tracking_Multiple_Object_Tracking_Dataset_and_Benchmark_in_Soccer_Videos_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Cioppa_SoccerNet-Tracking_Multiple_Object_Tracking_Dataset_and_Benchmark_in_Soccer_Videos_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Tracking objects in soccer videos is extremely important to gather both player and team statistics, whether it is to estimate the total distance run, the ball possession or the team formation. Video processing can help automating the extraction of those information, without the need of any invasive sensor, hence applicable to any team on any stadium. Yet, the availability of datasets to train learnable models and benchmarks to evaluate methods on a common testbed is very limited. In this work, we propose a novel dataset for multiple object tracking composed of 200 sequences of 30s each, representative of challenging soccer scenarios, and a complete 45-minutes half-time for long-term tracking. The dataset is fully annotated with bounding boxes and tracklet IDs, enabling the training of MOT baselines in the soccer domain and a full benchmarking of those methods on our segregated challenge sets. Our analysis shows that multiple player, referee and ball tracking in soccer videos is far from being solved, with several improvement required in case of fast motion or in scenarios of severe occlusion.",
    "code_link": "https://github.com/JonathonLuiten/TrackEval"
  },
  "cvpr2022_cvsports_posetutoranexplainablesystemforposecorrectioninthewild": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Pose Tutor: An Explainable System for Pose Correction in the Wild",
    "authors": [
      "Bhat Dittakavi",
      "Divyagna Bavikadi",
      "Sai Vikas Desai",
      "Soumi Chakraborty",
      "Nishant Reddy",
      "Vineeth N Balasubramanian",
      "Bharathi Callepalli",
      "Ayon Sharma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Dittakavi_Pose_Tutor_An_Explainable_System_for_Pose_Correction_in_the_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Dittakavi_Pose_Tutor_An_Explainable_System_for_Pose_Correction_in_the_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Under the new norm of working from home, demand for fitness from home is on the rise. Different exercise forms solve different fitness needs for different people. Yoga gives flexibility and relieves stress. Pilates strengthens the muscles. Kung Fu brings balance. It is not feasible for everyone to hire a personal trainer. In this paper, we develop Pose Tutor, an AI based explainable pose recognition and correction system. Pose Tutor combines vision and pose skeleton models in a novel coarse-to-fine framework to obtain pose class predictions. An angle-likelihood mechanism is used to explain which human joints maximally caused the pose class predictions and also correct any wrongly formed joints. Even without keypoint level training, Pose Tutor shows promising results on Yoga-82, Pilates-32 and Kungfu-7 datasets. Additionally, user studies conducted with multiple domain experts validate the explanations provided by our framework.",
    "code_link": ""
  },
  "cvpr2022_cvsports_efficienttrackingofteamsportplayerswithfewgame-specificannotations": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Efficient Tracking of Team Sport Players With Few Game-Specific Annotations",
    "authors": [
      "Adrien Maglo",
      "Astrid Orcesi",
      "Quoc-Cuong Pham"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Maglo_Efficient_Tracking_of_Team_Sport_Players_With_Few_Game-Specific_Annotations_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Maglo_Efficient_Tracking_of_Team_Sport_Players_With_Few_Game-Specific_Annotations_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "One of the requirements for team sports analysis is to track and recognize players. Many tracking and re-identification methods have been proposed in the context of video surveillance. They show very convincing results when tested on public datasets such as the MOT challenge. However, the performance of these methods are not as satisfactory when applied to player tracking. Indeed, in addition to moving very quickly and often being occluded, the players wear the same jersey, which makes the task of re-identification very complex. Some recent tracking methods have been developed more specifically for the team sport context. Due to the lack of public data, these methods use private datasets that make impossible a comparison with them. In this paper, we propose a new generic method to track team sport players during a full game thanks to few human annotations collected via a semi-interactive system. Non-ambiguous tracklets and their appearance features are automatically generated with a detection and a re-identification network both pre-trained on public datasets. Then an incremental learning mechanism trains a Transformer to classify identities using few game-specific human annotations. Finally, tracklets are linked by an association algorithm. We demonstrate the efficiency of our approach on a challenging rugby sevens dataset. To overcome the lack of public sports tracking dataset, we publicly release this dataset at https://kalisteo.cea.fr/index.php/free-resources/. We also show that our method is able to track rugby sevens players during a full match, if they are observable at a minimal resolution, with the annotation of only 6 few seconds length tracklets per player.",
    "code_link": ""
  },
  "cvpr2022_cvsports_monotrackshuttletrajectoryreconstructionfrommonocularbadmintonvideo": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "MonoTrack: Shuttle Trajectory Reconstruction From Monocular Badminton Video",
    "authors": [
      "Paul Liu",
      "Jui-Hsien Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Liu_MonoTrack_Shuttle_Trajectory_Reconstruction_From_Monocular_Badminton_Video_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Liu_MonoTrack_Shuttle_Trajectory_Reconstruction_From_Monocular_Badminton_Video_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Trajectory estimation is a fundamental component of racket sport analytics, as the trajectory contains information not only about the winning and losing of each point, but also how it was won or lost. In sports such as badminton, players benefit from knowing the full 3D trajectory, as the height of shuttlecock or ball provides valuable tactical information. Unfortunately, 3D reconstruction is a notoriously hard problem, and standard trajectory estimators can only track 2D pixel coordinates. In this work, we present the first complete end-to-end system for the extraction and segmentation of 3D shuttle trajectories from monocular badminton videos. Our system integrates badminton domain knowledge such as court dimension, shot placement, physical laws of motion, along with vision-based features such as player poses and shuttle tracking. We find that significant engineering efforts and model improvements are needed to make the overall system robust, and as a by-product of our work, improve state-of-the-art results on court recognition, 2D trajectory estimation, and hit recognition.",
    "code_link": "https://github.com/openmmlab/mmpose"
  },
  "cvpr2022_cvsports_soccertrackadatasetandtrackingalgorithmforsoccerwithfish-eyeanddronevideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "SoccerTrack: A Dataset and Tracking Algorithm for Soccer With Fish-Eye and Drone Videos",
    "authors": [
      "Atom Scott",
      "Ikuma Uchida",
      "Masaki Onishi",
      "Yoshinari Kameda",
      "Kazuhiro Fukui",
      "Keisuke Fujii"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Scott_SoccerTrack_A_Dataset_and_Tracking_Algorithm_for_Soccer_With_Fish-Eye_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Scott_SoccerTrack_A_Dataset_and_Tracking_Algorithm_for_Soccer_With_Fish-Eye_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Tracking devices that can track both players and balls are critical to the performance of sports teams. Recently, significant effort has been focused on building larger broadcast sports video datasets. However, broadcast videos do not show the entire pitch and only provides partial information about the game. On the other hand, other camera perspectives can capture the whole field in a single frame, such as fish-eye and bird-eye view (drone) cameras. Unfortunately, there has not been a dataset where such data has been publicly shared until now. This paper proposes SoccerTrack, a dataset set consisting of GNSS and bounding box tracking data annotated on video captured with a 8K-resolution fish-eye camera and a 4K-resolution drone camera. In addition to a benchmark tracking algorithm, we include code for camera calibration and other preprocessing. Finally, we evaluate the tracking accuracy among a GNSS, fish-eye camera and drone camera data. SoccerTrack is expected to provide a more robust foundation for designing MOT algorithms that are less reliant on visual cues and more reliant on motion analysis. The dataset and related project code are available at https://github.com/AtomScott/SoccerTrack.",
    "code_link": "https://github.com/AtomScott/SoccerTrack"
  },
  "cvpr2022_cvsports_icehockeyplayeridentificationviatransformersandweaklysupervisedlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Computer Vision in Sports",
    "title": "Ice Hockey Player Identification via Transformers and Weakly Supervised Learning",
    "authors": [
      "Kanav Vats",
      "William McNally",
      "Pascale Walters",
      "David A. Clausi",
      "John S. Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Vats_Ice_Hockey_Player_Identification_via_Transformers_and_Weakly_Supervised_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CVSports/papers/Vats_Ice_Hockey_Player_Identification_via_Transformers_and_Weakly_Supervised_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Identifying players in video is a foundational step in computer vision-based sports analytics. Obtaining player identities is essential for analyzing the game and is used in downstream tasks such as game event recognition. Transformers are the existing standard in natural language processing (NLP) and are swiftly gaining traction in computer vision. Motivated by the increasing success of transformers in computer vision, we introduce a transformer network for recognizing players through their jersey numbers in broadcast National Hockey League (NHL) videos. The transformer takes temporal sequences of player frames (called player tracklets) as input and outputs the probabilities of jersey numbers present in the frames. The proposed network performs better than the previous benchmark on the same dataset. We implement a weakly-supervised training approach by generating approximate frame-level labels for jersey number presence and use the frame-level labels for faster training. We also utilize player shifts available in the NHL play-by-play data by reading the game time using optical character recognition (OCR) to get the players on the ice rink at a certain game time. Using player-shifts improved the player identification accuracy by 6%.",
    "code_link": "https://github.com/JaidedAI/EasyOCR"
  },
  "cvpr2022_evw_on-sensorbinarizedfullyconvolutionalneuralnetworkforlocalisationandcoarsesegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "On-Sensor Binarized Fully Convolutional Neural Network for Localisation and Coarse Segmentation",
    "authors": [
      "Yanan Liu",
      "Yao Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Liu_On-Sensor_Binarized_Fully_Convolutional_Neural_Network_for_Localisation_and_Coarse_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Liu_On-Sensor_Binarized_Fully_Convolutional_Neural_Network_for_Localisation_and_Coarse_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This work presents a method to implement fully convolutional neural networks (FCNs) on Pixel Processor Array (PPA) sensors, and demonstrates coarse segmentation and object localisation tasks. We design and train binarized FCN for both binary weights and activations using batchnorm, group convolution, and learnable threshold for binarization, producing networks small enough to be embedded on the focal plane of the PPA, with limited local memory resources, and using parallel elementary add/subtract, shifting, and bit operations only. We demonstrate the first implementation of an FCN on a PPA device, performing three convolution layers entirely in the pixel-level processors. We use this architecture to demonstrate inference generating heat maps for object segmentation and localisation at over 280 FPS using the SCAMP-5 PPA vision chip.",
    "code_link": ""
  },
  "cvpr2022_evw_symdnnsimple&effectiveadversarialrobustnessforembeddedsystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "SymDNN: Simple & Effective Adversarial Robustness for Embedded Systems",
    "authors": [
      "Swarnava Dey",
      "Pallab Dasgupta",
      "Partha P Chakrabarti"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Dey_SymDNN_Simple__Effective_Adversarial_Robustness_for_Embedded_Systems_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Dey_SymDNN_Simple__Effective_Adversarial_Robustness_for_Embedded_Systems_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose SymDNN, a Deep Neural Network (DNN) inference scheme, to segment an input image into small patches, replace those patches with representative symbols, and use the reconstructed image for CNN inference. This approach of deconstruction of images, and the reconstruction from cluster centroids trained on clean images, enhances robustness against adversarial attacks. The input transform used in SymDNN is learned from very large datasets, making it difficult to approximate for adaptive adversarial attacks. For example, SymDNN achieves 23% and 42% robust accuracy at L-infinity attack strengths of 8/255 and 4/255 respectively, against BPDA under a complete white box setting, where most input processing based defenses break completely. SymDNN is not a future-proof adversarial defense that can defend any attack, but it is one of the few readily usable defenses in resource-limited embedded systems that defends against a wide range of attacks. Our code is available at: https://github.com/swadeykgp/SymDNN",
    "code_link": ""
  },
  "cvpr2022_evw_imagesigasignaturetransformforultra-lightweightimagerecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "ImageSig: A Signature Transform for Ultra-Lightweight Image Recognition",
    "authors": [
      "Mohamed R. Ibrahim",
      "Terry Lyons"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Ibrahim_ImageSig_A_Signature_Transform_for_Ultra-Lightweight_Image_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Ibrahim_ImageSig_A_Signature_Transform_for_Ultra-Lightweight_Image_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper introduces a new lightweight method for image recognition. ImageSig is based on computing signatures and does not require a convolutional structure or an attention-based encoder. It is striking to the authors that it achieves: a) an accuracy for 64 X 64 RGB images that exceeds many of the state-of-the-art methods and simultaneously b) requires orders of magnitude less FLOPS, power and memory footprint. The pretrained model can be as small as 44.2 KB in size. ImageSig shows unprecedented performance on hardware such as Raspberry Pi and Jetson-nano. ImageSig treats images as streams with multiple channels. These streams are parameterized by spatial directions. We contribute to the functionality of signature and rough path theory to stream-like data and vision tasks on static images beyond temporal streams. With very few parameters and small size models, the key advantage is that one could have many of these \"detectors\" assembled on the same chip; moreover, the feature acquisition can be performed once and shared between different models of different tasks - further accelerating the process. This contributes to energy efficiency and the advancements of embedded AI at the edge. The python code and weights for the pretrained models are provided in supplementary.",
    "code_link": ""
  },
  "cvpr2022_evw_maple-edgearuntimelatencypredictorforedgedevices": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "MAPLE-Edge: A Runtime Latency Predictor for Edge Devices",
    "authors": [
      "Saeejith Nair",
      "Saad Abbasi",
      "Alexander Wong",
      "Mohammad Javad Shafiee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Nair_MAPLE-Edge_A_Runtime_Latency_Predictor_for_Edge_Devices_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Nair_MAPLE-Edge_A_Runtime_Latency_Predictor_for_Edge_Devices_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Neural Architecture Search (NAS) has enabled automatic discovery of more efficient neural network architectures, especially for mobile and embedded vision applications. Although recent research has proposed ways of quickly estimating latency on unseen hardware devices with just a few samples, little focus has been given to the challenges of estimating latency on runtimes using optimized graphs, such as TensorRT and specifically for edge devices. As devices like NVIDIA's Jetsons get more popular in embedded computing and robotics, we observe a pressing need to more accurately estimate inference latency of neural network architectures on diverse runtimes, including highly optimized ones. In this work, we propose MAPLE-Edge, an edge device-oriented extension of MAPLE, the state-of-the-art latency predictor for general purpose hardware, where we train a regression network on architecture-latency pairs in conjunction with a hardware-runtime descriptor to effectively estimate latency on a diverse pool of edge devices. Compared to MAPLE, MAPLE-Edge can describe the runtime and target device platform using a much smaller set of CPU performance counters that are widely available on all Linux kernels, while still achieving up to +49.6% accuracy gains against previous state-of-the-art baseline methods on optimized edge device runtimes, using just 10 measurements from an unseen target device. We also demonstrate that unlike MAPLE which performs best when trained on a pool of devices sharing a common runtime, MAPLE-Edge can effectively generalize across runtimes by applying a trick of normalizing performance counters by the operator latency, in the measured hardware-runtime descriptor. Lastly, we show that for runtimes exhibiting lower than desired accuracy, performance can be boosted by collecting additional samples from the target device, with an extra 90 samples translating to gains of nearly +40%.",
    "code_link": ""
  },
  "cvpr2022_evw_multi-dimensionalvisiontransformercompressionviadependencyguidedgaussianprocesssearch": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "Multi-Dimensional Vision Transformer Compression via Dependency Guided Gaussian Process Search",
    "authors": [
      "Zejiang Hou",
      "Sun-Yuan Kung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Hou_Multi-Dimensional_Vision_Transformer_Compression_via_Dependency_Guided_Gaussian_Process_Search_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Hou_Multi-Dimensional_Vision_Transformer_Compression_via_Dependency_Guided_Gaussian_Process_Search_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed model's accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
    "code_link": ""
  },
  "cvpr2022_evw_real-timehyper-dimensionalreconfigurationattheedgeusinghardwareaccelerators": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "Real-Time Hyper-Dimensional Reconfiguration at the Edge Using Hardware Accelerators",
    "authors": [
      "Indhumathi Kandaswamy",
      "Saurabh Farkya",
      "Zachary Daniels",
      "Gooitzen van der Wal",
      "Aswin Raghavan",
      "Yuzheng Zhang",
      "Jun Hu",
      "Michael Lomnitz",
      "Michael Isnardi",
      "David Zhang",
      "Michael Piacentino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Kandaswamy_Real-Time_Hyper-Dimensional_Reconfiguration_at_the_Edge_Using_Hardware_Accelerators_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Kandaswamy_Real-Time_Hyper-Dimensional_Reconfiguration_at_the_Edge_Using_Hardware_Accelerators_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper we present Hyper-Dimensional Reconfigurable Analytics at the Tactical Edge (HyDRATE) using low-SWaP embedded hardware that can perform real-time reconfiguration at the edge leveraging non-MAC (free of floating-point Multiply-ACcumulate operations) deep neural nets (DNN) combined with hyperdimensional (HD) computing accelerators. We describe the algorithm, trained quantized model generation, and simulated performance of a feature extractor free of multiply-accumulates feeding a hyperdimensional logic-based classifier. Then we show how performance increases with the number of hyperdimensions. We describe the realized low-SWaP FPGA hardware and embedded software system compared to traditional DNNs and detail the implemented hardware accelerators. We discuss the measured system latency and power, noise robustness due to use of learnable quantization and HD computing, actual versus simulated system performance for a video activity classification task and demonstration of reconfiguration on this same dataset. We show that reconfigurability in the field is achieved by retraining only the feed-forward HD classifier without gradient descent backpropagation (gradient-free), using few-shot learning of new classes at the edge. Initial work performed used LRCN DNN and is currently extended to use Two-stream DNN with improved performance.",
    "code_link": ""
  },
  "cvpr2022_evw_doesinterferenceexistwhentrainingaonce-for-allnetwork?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "Does Interference Exist When Training a Once-for-All Network?",
    "authors": [
      "Jordan Shipard",
      "Arnold Wiliem",
      "Clinton Fookes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Shipard_Does_Interference_Exist_When_Training_a_Once-for-All_Network_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Shipard_Does_Interference_Exist_When_Training_a_Once-for-All_Network_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The Once-For-All (OFA) method offers an excellent pathway to deploy a trained neural network model into multiple target platforms by utilising the supernet-subnet architecture. Once trained, a subnet can be derived from the supernet (both architecture and trained weights) and deployed directly to the target platform with little to no retraining or fine-tuning. To train the subnet population, OFA uses a novel training method called Progressive Shrinking (PS) which is designed to limit the negative impact of interference during training. It is believed that higher interference during training results in lower subnet population accuracies. In this work we take a second look at this interference effect. Surprisingly, we find that interference mitigation strategies do not have a large impact on the overall subnet population performance. Instead, we find the subnet architecture selection bias during training to be a more important aspect. To show this, we propose a simple-yet-effective method called Random Subnet Sampling (RSS), which does not have mitigation on the interference effect. Despite no mitigation, RSS is able to produce a better performing subnet population than PS in four small to-medium-sized datasets; suggesting that the interference effect does not play a pivotal role in these datasets. Due to its simplicity, RSS provides a 1.9x reduction in training times compared to PS. A 6.1x reduction can also be achieved with a reasonable drop in performance when the number of RSS training epochs are reduced.",
    "code_link": "https://github.com/Jordan-HS/RSSInterference-CVPRW2022"
  },
  "cvpr2022_evw_efficientmulti-purposecross-attentionbasedimagealignmentblockforedgedevices": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Embedded Vision",
    "title": "Efficient Multi-Purpose Cross-Attention Based Image Alignment Block for Edge Devices",
    "authors": [
      "Bahri Batuhan Bilecen",
      "Alparslan Fi\u015fne",
      "Mustafa Ayazo\u011flu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/html/Bilecen_Efficient_Multi-Purpose_Cross-Attention_Based_Image_Alignment_Block_for_Edge_Devices_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Bilecen_Efficient_Multi-Purpose_Cross-Attention_Based_Image_Alignment_Block_for_Edge_Devices_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Image alignment, also known as image registration, is a critical block used in many computer vision problems. One of the key factors in alignment is efficiency, as inefficient aligners can cause significant overhead to the overall problem. In the literature, there are some blocks that appear to do the alignment operation, although most do not focus on efficiency. Therefore, an image alignment block which can both work in time and/or space and can work on edge devices would be beneficial for almost all networks dealing with multiple images. Given its wide usage and importance, we propose an efficient, cross-attention-based, multi-purpose image alignment block (XABA) suitable to work within edge devices. Using cross-attention, we exploit the relationships between features extracted from images. To make cross-attention feasible for real-time image alignment problems and handle large motions, we provide a pyramidal block based cross-attention scheme. This also captures local relationships besides reducing memory requirements and number of operations. Efficient XABA models achieve real-time requirements of running above 20 FPS performance on NVIDIA Jetson Xavier with 30W power consumption compared to other powerful computers. Used as a sub-block in a larger network, XABA also improves multi-image super-resolution network performance in comparison to other alignment methods.",
    "code_link": ""
  },
  "cvpr2022_clvision_out-of-distributiondetectioninunsupervisedcontinuallearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Out-of-Distribution Detection in Unsupervised Continual Learning",
    "authors": [
      "Jiangpeng He",
      "Fengqing Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/He_Out-of-Distribution_Detection_in_Unsupervised_Continual_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/He_Out-of-Distribution_Detection_in_Unsupervised_Continual_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Unsupervised continual learning aims to learn new tasks incrementally without requiring human annotations. However, most existing methods, especially those targeted on image classification, only work in a simplified scenario by assuming all new data belong to new tasks, which is not realistic if the class labels are not provided. Therefore, to perform unsupervised continual learning in real life applications, an out-of-distribution detector is required at beginning to identify whether each new data corresponds to a new task or already learned tasks, which still remains under-explored yet. In this work, we formulate the problem for Out-of-distribution Detection in Unsupervised Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In addition, we propose a novel OOD detection method by correcting the output bias at first and then enhancing the output confidence for in-distribution data based on task discriminativeness, which can be applied directly without modifying the learning procedures and objectives of continual learning. Our method is evaluated on CIFAR-100 dataset by following the proposed evaluation protocol and we show improved performance compared with existing OOD detection methods under the unsupervised continual learning scenario.",
    "code_link": ""
  },
  "cvpr2022_clvision_continuallearningwithtransformersforimageclassification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Continual Learning With Transformers for Image Classification",
    "authors": [
      "Beyza Ermis",
      "Giovanni Zappella",
      "Martin Wistuba",
      "Aditya Rawal",
      "C\u00e9dric Archambeau"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Ermis_Continual_Learning_With_Transformers_for_Image_Classification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Ermis_Continual_Learning_With_Transformers_for_Image_Classification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In many real-world scenarios, data to train machine learning models become available over time. However, neural network models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is often difficult to prevent due to practical constraints, such as the amount of data that can be stored or the limited computation sources that can be used. Moreover, training large neural networks, such as Transformers, from scratch is very costly and requires a vast amount of training data, which might not be available in the application domain of interest. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efficiently in continual learning, but this needs complex tuning to balance the growing number of parameters and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we validate in the computer vision domain a recent solution called Adaptive Distillation of Adapters (ADA), which is developed to perform continual learning using pre-trained Transformers and Adapters on text classification tasks. We empirically demonstrate on different classification tasks that this method maintains a good predictive performance without retraining the model or increasing the number of model parameters over the time. Besides it is significantly faster at inference time compared to the state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2022_clvision_continualhippocampussegmentationwithtransformers": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Continual Hippocampus Segmentation With Transformers",
    "authors": [
      "Amin Ranem",
      "Camila Gonz\u00e1lez",
      "Anirban Mukhopadhyay"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Ranem_Continual_Hippocampus_Segmentation_With_Transformers_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Ranem_Continual_Hippocampus_Segmentation_With_Transformers_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In clinical settings, where acquisition conditions and patient populations change over time, continual learning is key for ensuring the safe use of deep neural networks. Yet most existing work focuses on convolutional architectures and image classification. Instead, radiologists prefer to work with segmentation models that outline specific regions-of-interest, for which Transformer-based architectures are gaining traction. The self-attention mechanism of Transformers could potentially mitigate catastrophic forgetting, opening the way for more robust medical image segmentation. In this work, we explore how recently-proposed Transformer mechanisms for semantic segmentation behave in sequential learning scenarios, and analyse how best to adapt continual learning strategies for this setting. Our evaluation on hippocampus segmentation shows that Transformer mechanisms mitigate catastrophic forgetting for medical image segmentation compared to purely convolutional architectures, and demonstrates that regularising ViT modules should be done with caution.",
    "code_link": ""
  },
  "cvpr2022_clvision_multi-tasklearningforvideosurveillancewithlimiteddata": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Multi-Task Learning for Video Surveillance With Limited Data",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Doshi_Multi-Task_Learning_for_Video_Surveillance_With_Limited_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Doshi_Multi-Task_Learning_for_Video_Surveillance_With_Limited_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning from limited data in video surveillance is important for sustainable performance while adapting to new information in a scene over time or adapting to a different scene. In a real-world scene, for an anomaly detection algorithm, all possible nominal patterns and behaviors are not typically available immediately for a single training session. In contrast, labeled nominal data patterns may become available irregularly over a long time horizon, and the anomaly detection algorithm needs to quickly learn such new patterns from limited samples for acceptable performance. Otherwise, it would suffer from frequent false alarms. Additionally, the anomaly detection algorithm needs to continually learn new nominal patterns in multiple training sessions without forgetting the previous knowledge and losing performance. Cross-domain adaptability (i.e., transfer learning to another surveillance scene) is another task where the anomaly detection algorithm has to quickly learn from limited nominal training data to achieve acceptable performance. To overcome these challenges, we design a modular framework and use it to extract semantic embeddings, which we then train on by using deep metric learning. Particularly, we study these three problems (few-shot learning, continual learning, cross-domain adaptability) in a multi-task learning setting. We also compare our proposed framework to existing state-of-the-art approaches using various evaluation metrics. The empirical results indicate that the proposed approach is able to outperform the existing approaches on all three tasks for three benchmark datasets.",
    "code_link": ""
  },
  "cvpr2022_clvision_attenuatingcatastrophicforgettingbyjointcontrastiveandincrementallearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Attenuating Catastrophic Forgetting by Joint Contrastive and Incremental Learning",
    "authors": [
      "Quentin Ferdinand",
      "Benoit Clement",
      "Quentin Oliveau",
      "Gilles Le Chenadec",
      "Panagiotis Papadakis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Ferdinand_Attenuating_Catastrophic_Forgetting_by_Joint_Contrastive_and_Incremental_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Ferdinand_Attenuating_Catastrophic_Forgetting_by_Joint_Contrastive_and_Incremental_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In class incremental learning, discriminative models are trained to classify images while adapting to new instances and classes incrementally. Training a model to adapt to new classes without total access to previous class data, however, leads to the known problem of catastrophic forgetting of the previously learnt classes. To alleviate this problem, we show how we can build upon recent progress on contrastive learning methods. In particular, we develop an incremental learning approach for deep neural networks operating both at classification and representation level which alleviates forgetting and learns more general features for data classification. Experiments performed on several datasets demonstrate the superiority of the proposed method with respect to well known state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2022_clvision_modelingmissingannotationsforincrementallearninginobjectdetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Modeling Missing Annotations for Incremental Learning in Object Detection",
    "authors": [
      "Fabio Cermelli",
      "Antonino Geraci",
      "Dario Fontanel",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Cermelli_Modeling_Missing_Annotations_for_Incremental_Learning_in_Object_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Cermelli_Modeling_Missing_Annotations_for_Incremental_Learning_in_Object_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite the recent advances in the field of object detection, common architectures are still ill-suited to incrementally detect new categories over time. They are vulnerable to catastrophic forgetting: they forget what has been already learned while updating their parameters in absence of the original training data. Previous works extended standard classification methods in the object detection task, mainly adopting the knowledge distillation framework. However, we argue that object detection introduces an additional problem, which has been overlooked. While objects belonging to new classes are learned thanks to their annotations, if no supervision is provided for other objects that may still be present in the input, the model learns to associate them to background regions. We propose to handle these missing annotations by revisiting the standard knowledge distillation framework. Our approach outperforms current state-of-the-art methods in every setting of the Pascal-VOC dataset. We further propose an extension to instance segmentation, outperforming the other baselines.",
    "code_link": "https://github.com/fcdl94/MMA"
  },
  "cvpr2022_clvision_onlineunsuperviseddomainadaptationforpersonre-identification": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Online Unsupervised Domain Adaptation for Person Re-Identification",
    "authors": [
      "Hamza Rami",
      "Matthieu Ospici",
      "St\u00e9phane Lathuili\u00e8re"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Rami_Online_Unsupervised_Domain_Adaptation_for_Person_Re-Identification_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Rami_Online_Unsupervised_Domain_Adaptation_for_Person_Re-Identification_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Unsupervised domain adaptation for person re-identification (Person Re-ID) is the task of transferring the learned knowledge on the labeled source domain to the unlabeled target domain. Most of the recent papers that address this problem adopt an offline training setting. More precisely, the training of the Re-ID model is done assuming that we have access to the complete training target domain data set. In this paper, we argue that the target domain generally consists of a stream of data in a practical real-world application, where data is continuously increasing from the different network's cameras. The Re-ID solutions are also constrained by confidentiality regulations stating that the collected data can be stored for only a limited period, hence the model can no longer get access to previously seen target images. Therefore, we present a new yet practical online setting for Unsupervised Domain Adaptation for person Re-ID with two main constraints: Online Adaptation and Privacy Protection. We then adapt and evaluate the state-of-the-art UDA algorithms on this new online setting using the well-known Market-1501, Duke, and MSMT17 benchmarks.",
    "code_link": ""
  },
  "cvpr2022_clvision_cnllasemi-supervisedapproachforcontinualnoisylabellearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "CNLL: A Semi-Supervised Approach for Continual Noisy Label Learning",
    "authors": [
      "Nazmul Karim",
      "Umar Khalid",
      "Ashkan Esmaeili",
      "Nazanin Rahnavard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Karim_CNLL_A_Semi-Supervised_Approach_for_Continual_Noisy_Label_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Karim_CNLL_A_Semi-Supervised_Approach_for_Continual_Noisy_Label_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The task of continual learning requires careful design of algorithms that can tackle catastrophic forgetting. However, the noisy label, which is inevitable in a real-world scenario, seems to exacerbate the situation. While very few studies have addressed the issue of continual learning under noisy labels, long training time and complicated training schemes limit their applications in most cases. In contrast, we propose a simple purification technique to effectively cleanse the online data stream that is both cost-effective and more accurate. After purification, we perform fine-tuning in a semi-supervised fashion that ensures the participation of all available samples. Training in this fashion helps us learn a better representation that results in state-of-the-art (SOTA) performance. Through extensive experimentation on 3 benchmark datasets, MNIST, CIFAR10, and CIFAR100, we show the effectiveness of our proposed approach. We achieve a 24.8% performance gain for CIFAR10 with 20% noise over previous SOTA methods. Our code is publicly available.",
    "code_link": "https://github.com/nazmul-karim170/CNLL"
  },
  "cvpr2022_clvision_alleviatingrepresentationalshiftforcontinualfine-tuning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Alleviating Representational Shift for Continual Fine-Tuning",
    "authors": [
      "Shibo Jie",
      "Zhi-Hong Deng",
      "Ziheng Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Jie_Alleviating_Representational_Shift_for_Continual_Fine-Tuning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Jie_Alleviating_Representational_Shift_for_Continual_Fine-Tuning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We study a practical setting of continual learning: fine-tuning on a pre-trained model continually. Previous work has found that, when training on new tasks, the features (penultimate layer representations) of previous data will change, called representational shift. Besides the shift of features, we reveal that the intermediate layers' representational shift (IRS) also matters since it disrupts batch normalization, which is another crucial cause of catastrophic forgetting. Motivated by this, we propose ConFiT, a fine-tuning method incorporating two components, cross-convolution batch normalization (Xconv BN) and hierarchical fine-tuning. Xconv BN maintains pre-convolution running means instead of post-convolution, and recovers post-convolution ones before testing, which corrects the inaccurate estimates of means under IRS. Hierarchical fine-tuning leverages a multi-stage strategy to fine-tune the pre-trained network, preventing massive changes in Conv layers and thus alleviating IRS. Experimental results on four datasets show that our method remarkably outperforms several state-of-the-art methods with lower storage overhead.",
    "code_link": "https://github.com/JieShibo/ConFiT"
  },
  "cvpr2022_clvision_transferringunconditionaltoconditionalganswithhyper-modulation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Transferring Unconditional to Conditional GANs With Hyper-Modulation",
    "authors": [
      "H\u00e9ctor Laria",
      "Yaxing Wang",
      "Joost van de Weijer",
      "Bogdan Raducanu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Laria_Transferring_Unconditional_to_Conditional_GANs_With_Hyper-Modulation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Laria_Transferring_Unconditional_to_Conditional_GANs_With_Hyper-Modulation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "GANs have matured in recent years and are able to generate high-resolution, realistic images. However, the computational resources and the data required for the training of high-quality GANs are enormous, and the study of transfer learning of these models is therefore an urgent topic. Many of the available high-quality pretrained GANs are unconditional (like StyleGAN). For many applications, however, conditional GANs are preferable, because they provide more control over the generation process, despite often suffering more training difficulties. Therefore, in this paper, we focus on transferring from high-quality pretrained unconditional GANs to conditional GANs. This requires architectural adaptation of the pretrained GAN to perform the conditioning. To this end, we propose hyper-modulated generative networks that allow for shared and complementary supervision.",
    "code_link": ""
  },
  "cvpr2022_clvision_spacinglossfordiscoveringnovelcategories": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Spacing Loss for Discovering Novel Categories",
    "authors": [
      "K J Joseph",
      "Sujoy Paul",
      "Gaurav Aggarwal",
      "Soma Biswas",
      "Piyush Rai",
      "Kai Han",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Joseph_Spacing_Loss_for_Discovering_Novel_Categories_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Joseph_Spacing_Loss_for_Discovering_Novel_Categories_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Novel Class Discovery (NCD) is a learning paradigm, where a machine learning model is tasked to semantically group instances from unlabeled data, by utilizing labeled instances from a disjoint set of classes. In this work, we first characterize existing NCD approaches into single-stage and two-stage methods based on whether they require access to labeled and unlabeled data together while discovering new classes. Next, we devise a simple yet powerful loss function that enforces separability in the latent space using cues from multi-dimensional scaling, which we refer to as Spacing Loss. Our proposed formulation can either operate as a standalone method or can be plugged into existing methods to enhance them. We validate the efficacy of Spacing Loss with thorough experimental evaluation across multiple settings on CIFAR-10 and CIFAR-100 datasets.",
    "code_link": "https://github.com/JosephKJ/Awesome-Novel-Class-Discovery"
  },
  "cvpr2022_clvision_towardsexemplar-freecontinuallearninginvisiontransformersanaccountofattention,functionalandweightregularization": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Towards Exemplar-Free Continual Learning in Vision Transformers: An Account of Attention, Functional and Weight Regularization",
    "authors": [
      "Francesco Pelosin",
      "Saurav Jha",
      "Andrea Torsello",
      "Bogdan Raducanu",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Pelosin_Towards_Exemplar-Free_Continual_Learning_in_Vision_Transformers_An_Account_of_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Pelosin_Towards_Exemplar-Free_Continual_Learning_in_Vision_Transformers_An_Account_of_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we investigate the continual learning of Vision Transformers (ViT) for the challenging exemplar-free scenario, with special focus on how to efficiently distill the knowledge of its crucial self-attention mechanism (SAM). Our work takes an initial step towards a surgical investigation of SAM for designing coherent continual learning methods in ViTs. We first carry out an evaluation of established continual learning regularization techniques. We then examine the effect of regularization when applied to two key enablers of SAM: (a) the contextualized embedding layers, for their ability to capture well-scaled representations with respect to the values, and (b) the prescaled attention maps, for carrying value-independent global contextual information. We depict the perks of each distilling strategy on two image recognition benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall accuracy, (b) helps enhance the rigidity by maintaining competitive performances. Furthermore, we identify the limitation imposed by the symmetric nature of regularization losses. To alleviate this, we propose an asymmetric variant and apply it to the pooled output distillation (POD) loss adapted for ViTs. Our experiments confirm that introducing asymmetry to POD boosts its plasticity while retaining stability across (a) and (b). Moreover, we acknowledge low forgetting measures for all the compared methods, indicating that ViTs might be naturally inclined continual learners.",
    "code_link": ""
  },
  "cvpr2022_clvision_visualgoal-directedmeta-imitationlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Visual Goal-Directed Meta-Imitation Learning",
    "authors": [
      "Corban G. Rivera",
      "David A. Handelman",
      "Christopher R. Ratto",
      "David Patrone",
      "Bart L. Paulhamus"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Rivera_Visual_Goal-Directed_Meta-Imitation_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Rivera_Visual_Goal-Directed_Meta-Imitation_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The goal of meta-learning is to generalize to new tasks and goals as quickly as possible. Ideally, we would like approaches that generalize to new goals and tasks on the first attempt. Requiring a policy to perform on a new task on the first attempt without even a single example trajectory is a zero-shot problem formulation. When tasks are identified by goal images, the tasks can be considered visually goal-directed. In this work, we explore the problem of visual goal-directed zero-shot meta-imitation learning. Inspired by several popular approaches to Meta-RL, we composed several core ideas related to task-embedding and planning by gradient descent to attempt to explore this problem. To evaluate these approaches, we adapted the Metaworld benchmark tasks to create 24 distinct visual goal-directed manipulation tasks. We found that 7 out of 24 tasks could be successfully completed on the first attempt by at least one of the approaches we tested. We demonstrated that goal-directed zero-shot approaches can translate to a physical robot with a demonstration based on Jenga block manipulation tasks using a Kinova Jaco robotic arm.",
    "code_link": ""
  },
  "cvpr2022_clvision_continuallearningbasedonooddetectionandtaskmasking": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Continual Learning Based on OOD Detection and Task Masking",
    "authors": [
      "Gyuhak Kim",
      "Sepideh Esmaeilpour",
      "Changnan Xiao",
      "Bing Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Kim_Continual_Learning_Based_on_OOD_Detection_and_Task_Masking_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Kim_Continual_Learning_Based_on_OOD_Detection_and_Task_Masking_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Existing continual learning techniques focus on either task incremental learning (TIL) or class incremental learning (CIL) problem, but not both. CIL and TIL differ mainly in that the task-id is provided for each test sample during testing for TIL, but not provided for CIL. Continual learning methods intended for one problem have limitations on the other problem. This paper proposes a novel unified approach based on out-of-distribution (OOD) detection and task masking, called CLOM, to solve both problems. The key novelty is that each task is trained as an OOD detection model rather than a traditional supervised learning model, and a task mask is trained to protect each task to prevent forgetting. Our evaluation shows that CLOM outperforms existing state-of-the-art baselines by large margins. The average TIL/CIL accuracy of CLOM over six experiments is 87.6/67.9% while that of the best baselines is only 84.4/55.0%. The code of our system is available at https://github.com/k-gyuhak/CLOM.",
    "code_link": "https://github.com/k-gyuhak/CLOM"
  },
  "cvpr2022_clvision_multi-headdistillationforcontinualunsuperviseddomainadaptationinsemanticsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Multi-Head Distillation for Continual Unsupervised Domain Adaptation in Semantic Segmentation",
    "authors": [
      "Antoine Saporta",
      "Arthur Douillard",
      "Tuan-Hung Vu",
      "Patrick P\u00e9rez",
      "Matthieu Cord"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Saporta_Multi-Head_Distillation_for_Continual_Unsupervised_Domain_Adaptation_in_Semantic_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Saporta_Multi-Head_Distillation_for_Continual_Unsupervised_Domain_Adaptation_in_Semantic_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Unsupervised Domain Adaptation (UDA) is a transfer learning task which aims at training on an unlabeled target domain by leveraging a labeled source domain. Beyond the traditional scope of UDA with a single source domain and a single target domain, real-world perception systems face a variety of scenarios to handle, from varying lighting conditions to many cities around the world. In this context, UDAs with several domains increase the challenges with the addition of distribution shifts within the different target domains. This work focuses on a novel framework for learning UDA, continuous UDA, in which models operate on multiple target domains discovered sequentially, without access to previous target domains. We propose MuHDi, for Multi-Head Distillation, a method that solves the catastrophic forgetting problem, inherent in continual learning tasks. MuHDi performs distillation at multiple levels from the previous model as well as an auxiliary target-specialist segmentation head. We report both extensive ablation and experiments on challenging multi-target UDA semantic segmentation benchmarks to validate the proposed learning scheme and architecture.",
    "code_link": ""
  },
  "cvpr2022_clvision_ex-modelcontinuallearningfromastreamoftrainedmodels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Ex-Model: Continual Learning From a Stream of Trained Models",
    "authors": [
      "Antonio Carta",
      "Andrea Cossu",
      "Vincenzo Lomonaco",
      "Davide Bacciu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Carta_Ex-Model_Continual_Learning_From_a_Stream_of_Trained_Models_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Carta_Ex-Model_Continual_Learning_From_a_Stream_of_Trained_Models_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning continually from non-stationary data streams is a challenging research topic of growing popularity in the last few years. Being able to learn, adapt, and generalize continually in an efficient, effective, and scalable way is fundamental for a sustainable development of Artificial Intelligent systems. However, an agent-centric view of continual learning requires learning directly from raw data, which limits the interaction between independent agents, the efficiency, and the privacy of current approaches. Instead, we argue that continual learning systems should exploit the availability of compressed information in the form of trained models. In this paper, we introduce and formalize a new paradigm named \"Ex-Model Continual Learning\" (ExML), where an agent learns from a sequence of previously trained models instead of raw data. We further contribute with three ex-model continual learning algorithms and an empirical setting comprising three datasets (MNIST, CIFAR-10 and CORe50), and eight scenarios, where the proposed algorithms are extensively tested. Finally, we highlight the peculiarities of the ex-model paradigm and we point out interesting future research directions.",
    "code_link": "https://github.com/AntonioCarta/ex_model_cl"
  },
  "cvpr2022_clvision_continuallylearningself-supervisedrepresentationswithprojectedfunctionalregularization": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Continually Learning Self-Supervised Representations With Projected Functional Regularization",
    "authors": [
      "Alex Gomez-Villa",
      "Bartlomiej Twardowski",
      "Lu Yu",
      "Andrew D. Bagdanov",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Gomez-Villa_Continually_Learning_Self-Supervised_Representations_With_Projected_Functional_Regularization_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Gomez-Villa_Continually_Learning_Self-Supervised_Representations_With_Projected_Functional_Regularization_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent self-supervised learning methods are able to learn high-quality image representations and are closing the gap with supervised approaches. However, these methods are unable to acquire new knowledge incrementally -- they are, in fact, mostly used only as a pre-training phase over IID data. In this work we investigate self-supervised methods in continual learning regimes without any replay mechanism. We show that naive functional regularization, also known as feature distillation, leads to lower plasticity and limits continual learning performance. Instead, we propose Projected Functional Regularization in which a separate temporal projection network ensures that the newly learned feature space preserves information of the previous one, while at the same time allowing for the learning of new features. This prevents forgetting while maintaining the plasticity of the learner. Comparison with other incremental learning approaches applied to self-supervision demonstrates that our method obtains competitive performance in different scenarios and on multiple datasets.",
    "code_link": "https://github.com/alviur/CVPR_PFR"
  },
  "cvpr2022_clvision_unsupervisedcontinuallearningforgraduallyvaryingdomains": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Unsupervised Continual Learning for Gradually Varying Domains",
    "authors": [
      "Abu Md Niamul Taufique",
      "Chowdhury Sadman Jahan",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Taufique_Unsupervised_Continual_Learning_for_Gradually_Varying_Domains_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Taufique_Unsupervised_Continual_Learning_for_Gradually_Varying_Domains_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In Unsupervised Domain Adaptation (UDA), a network is trained on a source domain and adapted on a target domain where no labeled data is available. Existing UDA techniques consider having the entire target domain available at once, which may not be feasible during deployment in realistic settings where batches of target data are acquired over time. Continual Learning (CL) has been dealing with data constrained paradigms in a supervised manner, where batches of labeled samples are sequentially presented to the network and the network continually learns from the new data without forgetting what was previously learned. Our method for unsupervised continual learning serves as a bridge between the UDA and CL paradigms. This research addresses a gradually evolving target domain fragmented into multiple sequential batches where the model continually adapts to the gradually varying stream of data in an unsupervised manner. To tackle this challenge, we propose a source free method based on episodic memory replay with buffer management. A contrastive loss is incorporated for better alignment of the buffer samples and the continual stream of batches. Our experiments on the rotating MNIST and CORe50 datasets confirm the benefits of our unsupervised continual learning method for gradually varying domains. The codes are available at https://github.com/abutaufique/ucl-gv.git.",
    "code_link": "https://github.com/abutaufique/ucl-gv.git"
  },
  "cvpr2022_clvision_csg0continualurbanscenegenerationwithzeroforgetting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "CSG0: Continual Urban Scene Generation With Zero Forgetting",
    "authors": [
      "Himalaya Jain",
      "Tuan-Hung Vu",
      "Patrick P\u00e9rez",
      "Matthieu Cord"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Jain_CSG0_Continual_Urban_Scene_Generation_With_Zero_Forgetting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Jain_CSG0_Continual_Urban_Scene_Generation_With_Zero_Forgetting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the rapid advances in generative adversarial networks (GANs), the visual quality of synthesised scenes keeps improving, including for complex urban scenes with applications to automated driving. We address in this work a continual scene generation setup in which GANs are trained on a stream of distinct domains; ideally, the learned models should eventually be able to generate new scenes in all seen domains. This setup reflects the real-life scenario where data are continuously acquired in different places at different times. In such a continual setup, we aim for learning with zero forgetting, i.e., with no degradation in synthesis quality over earlier domains due to catastrophic forgetting. To this end, we introduce a novel framework that not only (i) enables seamless knowledge transfer in continual training but also (ii) guarantees zero forgetting with a small overhead cost. While being more memory efficient, thanks to continual learning, our model obtains better synthesis quality as compared against the brute-force solution that trains one full model for each domain. Especially, under extreme low-data regimes, our approach outperforms the brute-force one by a large margin.",
    "code_link": ""
  },
  "cvpr2022_clvision_variablefewshotclassincrementalandopenworldlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Variable Few Shot Class Incremental and Open World Learning",
    "authors": [
      "Touqeer Ahmad",
      "Akshay Raj Dhamija",
      "Mohsen Jafarzadeh",
      "Steve Cruz",
      "Ryan Rabinowitz",
      "Chunchun Li",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Ahmad_Variable_Few_Shot_Class_Incremental_and_Open_World_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Ahmad_Variable_Few_Shot_Class_Incremental_and_Open_World_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Prior work on few-shot class incremental learning has operated with an unnatural assumption: the number of ways and number of shots are assumed to be known and fixed e.g., 10-ways 5-shots, 5-ways 5-shots, etc. Hence, we refer to this setting as Fixed-Few-Shot Class Incremental Learning (FFSCIL). In practice, the pre-specified fixed number of classes and examples per class may not be available, meaning one cannot update the model. Evaluation of FSCIL approaches in such unnatural settings renders their applicability questionable for practical scenarios where such assumptions do not hold. To mitigate the limitation of FFSCIL, we propose Variable-Few-Shot Class Incremental Learning (VFSCIL) and demonstrate it with Up-to N-Ways, Up-to K-Shots class incremental learning; wherein each incremental session, a learner may have up to N classes and up to K samples per class. Consequently, conventional FFSCIL is a special case of herein introduced VFSCIL. Further, we extend VFSCIL to a more practical problem of Variable-Few-Shot Open-World Learning (VFSOWL), where an agent is not only required to perform incremental learning, but must detect unknown samples and enroll only those that it detects correctly. We formulate and study VFSCIL and VFSOWL on two benchmark datasets conventionally employed for FFSCIL i.e., Caltech-UCSD Birds-200-2011 (CUB200) and miniImageNet. First, to serve as a strong baseline, we extend the state-of-the-art FSCIL approach to operate in Up-to N-Ways, Up-to K-Shots class incremental and open-world settings. Then, we propose a novel but simple approach for VFSCIL/VFSOWL where we leverage the current advancements in self-supervised feature learning. Utilizing both benchmark datasets, our proposed approach outperforms the strong baseline on the conventional FFSCIL setting and newly introduced VFSCIL/VFSOWL settings. Our code is available at: https://github.com/TouqeerAhmad/VFSOWL",
    "code_link": "https://github.com/TouqeerAhmad/VFSOWL"
  },
  "cvpr2022_clvision_medusauniversalfeaturelearningviaattentionalmultitasking": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Medusa: Universal Feature Learning via Attentional Multitasking",
    "authors": [
      "Jaime Spencer",
      "Richard Bowden",
      "Simon Hadfield"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Spencer_Medusa_Universal_Feature_Learning_via_Attentional_Multitasking_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Spencer_Medusa_Universal_Feature_Learning_via_Attentional_Multitasking_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent approaches to multi-task learning (MTL) have focused on modelling connections between tasks at the decoder level. This leads to a tight coupling between tasks, which need retraining if a new task is inserted or removed. We argue that MTL is a stepping stone towards universal feature learning (UFL), which is the ability to learn generic features that can be applied to new tasks without retraining. We propose Medusa to realize this goal, designing task heads with dual attention mechanisms. The shared feature attention masks relevant backbone features for each task, allowing it to learn a generic representation. Meanwhile, a novel Multi-Scale Attention head allows the network to better combine per-task features from different scales when making the final prediction. We show the effectiveness of Medusa in UFL (+13.18% improvement), while maintaining MTL performance and being 25% more efficient than previous approaches.",
    "code_link": ""
  },
  "cvpr2022_clvision_entropy-basedstability-plasticityforlifelonglearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Entropy-Based Stability-Plasticity for Lifelong Learning",
    "authors": [
      "Vladimir Araujo",
      "Julio Hurtado",
      "Alvaro Soto",
      "Marie-Francine Moens"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Araujo_Entropy-Based_Stability-Plasticity_for_Lifelong_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Araujo_Entropy-Based_Stability-Plasticity_for_Lifelong_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The ability to continuously learn remains elusive for deep learning models. Unlike humans, models cannot accumulate knowledge in their weights when learning new tasks, mainly due to an excess of plasticity and the low incentive to reuse weights when training a new task. To address the stability-plasticity dilemma in neural networks, we propose a novel method called Entropy-based Stability-Plasticity (ESP). Our approach can decide dynamically how much each model layer should be modified via a plasticity factor. We incorporate branch layers and an entropy-based criterion into the model to find such factor. Our experiments in the domains of natural language and vision show the effectiveness of our approach in leveraging prior knowledge by reducing interference. Also, in some cases, it is possible to freeze layers during training leading to speed up in training.",
    "code_link": "https://github.com/vgaraujov/ESP-CL"
  },
  "cvpr2022_clvision_incrementalmeta-learningviaepisodicreplaydistillationforfew-shotimagerecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Continual Learning in Computer Vision",
    "title": "Incremental Meta-Learning via Episodic Replay Distillation for Few-Shot Image Recognition",
    "authors": [
      "Kai Wang",
      "Xialei Liu",
      "Andrew D. Bagdanov",
      "Luis Herranz",
      "Shangling Jui",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Wang_Incremental_Meta-Learning_via_Episodic_Replay_Distillation_for_Few-Shot_Image_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Wang_Incremental_Meta-Learning_via_Episodic_Replay_Distillation_for_Few-Shot_Image_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper we consider the problem of incremental meta-learning in which classes are presented incrementally in discrete tasks. We propose Episodic Replay Distillation (ERD), that mixes classes from the current task with class exemplars from previous tasks when sampling episodes for meta-learning. To allow the training to benefit from a large as possible variety of classes, which leads to more generalizable feature representations, we propose the cross-task meta loss. Furthermore, we propose episodic replay distillation that also exploits exemplars for improved knowledge distillation. Experiments on four datasets demonstrate that ERD surpasses the state-of-the-art. In particular, on the more challenging one-shot, long task sequence scenarios, we reduce the gap between Incremental Meta-Learning and the joint-training upper bound from 3.5% / 10.1% / 13.4% / 11.7% with the current state-of-the-art to 2.6% / 2.9% / 5.0% / 0.2% with our method on Tiered-ImageNet / Mini-ImageNet / CIFAR100 / CUB, respectively.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_unsupervisedsalientobjectdetectionwithspectralclustervoting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Unsupervised Salient Object Detection With Spectral Cluster Voting",
    "authors": [
      "Gyungin Shin",
      "Samuel Albanie",
      "Weidi Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Shin_Unsupervised_Salient_Object_Detection_With_Spectral_Cluster_Voting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Shin_Unsupervised_Salient_Object_Detection_With_Spectral_Cluster_Voting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we tackle the challenging task of unsupervised salient object detection (SOD) by leveraging spectral clustering on self-supervised features. We make the following contributions: (i) We revisit spectral clustering and demonstrate its potential to group the pixels of salient objects across various self-supervised features, e.g., MoCov2, SwAV, and DINO; (ii) Given mask proposals from multiple applications of spectral clustering on image features computed from different self-supervised models, we propose a simple but effective winner-takes-all voting mechanism for selecting the salient masks, leveraging object priors based on framing and distinctiveness; (iii) Using the selected object segmentation as pseudo groundtruth masks, we train a salient object detector, termed SelfMask, which outperforms prior approaches on three unsupervised SOD benchmarks. Code is publicly available at https://github.com/NoelShin/selfmask.",
    "code_link": "https://github.com/NoelShin/selfmask"
  },
  "cvpr2022_l3d-ivu_tdtteachingdetectorstotrackwithoutfullyannotatedvideos": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "TDT: Teaching Detectors To Track Without Fully Annotated Videos",
    "authors": [
      "Shuzhi Yu",
      "Guanhang Wu",
      "Chunhui Gu",
      "Mohammed E. Fathy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Yu_TDT_Teaching_Detectors_To_Track_Without_Fully_Annotated_Videos_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Yu_TDT_Teaching_Detectors_To_Track_Without_Fully_Annotated_Videos_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recently, one-stage trackers that use a joint model to predict both detections and appearance embeddings in one forward pass received much attention and achieved state-of-the-art results on the Multi-Object Tracking (MOT) benchmarks. However, their success depends on the availability of videos that are fully annotated with tracking data, which is expensive and hard to obtain. This can limit the model generalization. In comparison, the two-stage approach, which performs detection and embedding separately, is slower but easier to train as their data are easier to annotate. We propose to combine the best of the two worlds through a data distillation approach. Specifically, we use a teacher embedder, trained on Re-ID datasets, to generate pseudo appearance embedding labels for the detection datasets. Then, we use the augmented dataset to train a detector that is also capable of regressing these pseudo-embeddings in a fully-convolutional fashion. Our proposed one-stage solution matches the two-stage counterpart in quality but is 3 times faster. Even though the teacher embedder has not seen any tracking data during training, our proposed tracker achieves competitive performance with some popular trackers (e.g. JDE) trained with fully labeled tracking data.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_efficientconditionalpre-trainingfortransferlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Efficient Conditional Pre-Training for Transfer Learning",
    "authors": [
      "Shuvam Chakraborty",
      "Burak Uzkent",
      "Kumar Ayush",
      "Kumar Tanmay",
      "Evan Sheehan",
      "Stefano Ermon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Chakraborty_Efficient_Conditional_Pre-Training_for_Transfer_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Chakraborty_Efficient_Conditional_Pre-Training_for_Transfer_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Almost all the state-of-the-art neural networks for computer vision tasks are trained by (1) pre-training on a large-scale dataset and (2) finetuning on the target dataset. This strategy helps reduce dependence on the target dataset and improves convergence rate and generalization on the target task. Although pre-training on large-scale datasets is very useful for new methods or models, its foremost disadvantage is high training cost. To address this, we propose efficient filtering methods to select relevant subsets from the pre-training dataset. Additionally, we discover that lowering image resolutions in the pre-training step offers a great trade-off between cost and performance. We validate our techniques by pre-training on ImageNet in both the unsupervised and supervised settings and finetuning on a diverse collection of target datasets and tasks. Our proposed methods drastically reduce pre-training cost and provide strong performance boosts. Finally, we improve the current standard of ImageNet pre-training by 1-3% by tuning available models on our subsets and pre-training on a dataset filtered from a larger scale dataset.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_cdadacommondailyactiondatasetwithcollectedhardnegativesamples": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "CDAD: A Common Daily Action Dataset With Collected Hard Negative Samples",
    "authors": [
      "Wangmeng Xiang",
      "Chao Li",
      "Ke Li",
      "Biao Wang",
      "Xian-sheng Hua",
      "Lei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xiang_CDAD_A_Common_Daily_Action_Dataset_With_Collected_Hard_Negative_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The research on action understanding has achieved significant progress with the establishment of various benchmark datasets. However, the results of action understanding are far from satisfactory in practice. One reason is that the existing action datasets ignore the existence of many hard negative samples in real-world scenarios, which are usually undefined confusion actions, e.g., holding a pen near the mouth vs. smoking. In this work, we focus on the common actions in our daily life and present a novel Common Daily Action Dataset (CDAD), which consists of 57,824 video clips of 23 well-defined common daily actions with rich manual annotations. Particularly, for each daily action, we collect not only diverse positive samples but also various hard negative samples that have minor differences (share similarities) in action with the positive ones. The established CDAD dataset could not only serve as a benchmark for several important daily action understanding tasks, including multi-label action recognition, temporal action localization, and spatial-temporal action detection but also provide a testbed for researchers to investigate the influence of highly similar negative samples in learning action understanding models. The established CDAD dataset will be released for research purposes.",
    "code_link": "https://github.com/MartinXM/CDAD"
  },
  "cvpr2022_l3d-ivu_towardsopen-setobjectdetectionanddiscovery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Towards Open-Set Object Detection and Discovery",
    "authors": [
      "Jiyang Zheng",
      "Weihao Li",
      "Jie Hong",
      "Lars Petersson",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Zheng_Towards_Open-Set_Object_Detection_and_Discovery_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Zheng_Towards_Open-Set_Object_Detection_and_Discovery_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the human pursuit of knowledge, open-set object detection (OSOD) has been designed to identify unknown objects in a dynamic world. However, an issue with the current setting is that all the predicted unknown objects share the same category as \"unknown\", which require incremental learning via a human-in-the-loop approach to label novel classes. In order to address this problem, we present a new task, namely Open-Set Object Detection and Discovery (OSODD). This new task aims to extend the ability of open-set object detectors to further discover the categories of unknown objects based on their visual appearance without human effort. We propose a two-stage method that first uses an open-set object detector to predict both known and unknown objects. Then, we study the representation of predicted objects in an unsupervised manner and discover new categories from the set of unknown objects. With this method, a detector is able to detect objects belonging to known classes and define novel categories for objects of unknown classes with minimal supervision. We show the performance of our model on the MS-COCO dataset under a thorough evaluation protocol. We hope that our work will promote further research towards a more robust real-world detection system.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_scvrlshuffledcontrastivevideorepresentationlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "SCVRL: Shuffled Contrastive Video Representation Learning",
    "authors": [
      "Michael Dorkenwald",
      "Fanyi Xiao",
      "Biagio Brattoli",
      "Joseph Tighe",
      "Davide Modolo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Dorkenwald_SCVRL_Shuffled_Contrastive_Video_Representation_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Dorkenwald_SCVRL_Shuffled_Contrastive_Video_Representation_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose SCVRL, a novel contrastive-based framework for self-supervised learning for videos. Differently from previous contrast learning based methods that mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable of learning both semantic and motion patterns. For that, we reformulate the popular shuffling pretext task within a modern contrastive learning paradigm. We show that our transformer-based network has a natural capacity to learn motion in self-supervised settings and achieves strong performance, outperforming CVRL on four benchmarks.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_denoisingpretrainingforsemanticsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Denoising Pretraining for Semantic Segmentation",
    "authors": [
      "Emmanuel Asiedu Brempong",
      "Simon Kornblith",
      "Ting Chen",
      "Niki Parmar",
      "Matthias Minderer",
      "Mohammad Norouzi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Brempong_Denoising_Pretraining_for_Semantic_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Brempong_Denoising_Pretraining_for_Semantic_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Semantic segmentation labels are expensive and time consuming to acquire. To improve label efficiency of semantic segmentation models, we revisit denoising autoencoders and study the use of a denoising objective for pretraining UNets. We pretrain a Transformer-based UNet as a denoising autoencoder, followed by fine-tuning on semantic segmentation using few labeled examples. Denoising pretraining outperforms training from random initialization, and even supervised ImageNet-21K pretraining of the encoder when the number of labeled images is small. A key advantage of denoising pretraining over supervised pretraining of the backbone is the ability to pretrain the decoder, which would otherwise be randomly initialized. We thus propose a novel Decoder Denoising Pretraining (DDeP) method, in which we initialize the encoder using supervised learning and pretrain only the decoder using the denoising objective. Despite its simplicity, DDeP achieves state-of-the art results on label-efficient semantic segmentation, offering considerable gains on the Cityscapes, Pascal Context, and ADE20K datasets.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_contrastiveregularizationforsemi-supervisedlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Contrastive Regularization for Semi-Supervised Learning",
    "authors": [
      "Doyup Lee",
      "Sungwoong Kim",
      "Ildoo Kim",
      "Yeongjae Cheon",
      "Minsu Cho",
      "Wook-Shin Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Lee_Contrastive_Regularization_for_Semi-Supervised_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Lee_Contrastive_Regularization_for_Semi-Supervised_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Consistency regularization on label predictions becomes a fundamental technique in semi-supervised learning, but it still requires a large number of training iterations for high performance. In this study, we analyze that the consistency regularization restricts the propagation of labeling information due to the exclusion of samples with unconfident pseudo-labels in the model updates. Then, we propose contrastive regularization to improve both efficiency and accuracy of the consistency regularization by well-clustered features of unlabeled data. In specific, after strongly augmented samples are assigned to clusters by their pseudo-labels, our contrastive regularization updates the model so that the features with confident pseudo-labels aggregate the features in the same cluster, while pushing away features in different clusters. As a result, the information of confident pseudo-labels can be effectively propagated into more unlabeled samples during training by the well-clustered features. On benchmarks of semi-supervised learning tasks, our contrastive regularization improves the previous consistency-based methods and achieves state-of-the-art results, especially with fewer training iterations. Our method also shows robust performance on open-set semi-supervised learning where unlabeled data includes out-of-distribution samples.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_vitolvisiontransformerforweaklysupervisedobjectlocalization": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "ViTOL: Vision Transformer for Weakly Supervised Object Localization",
    "authors": [
      "Saurav Gupta",
      "Sourav Lakhotia",
      "Abhay Rawat",
      "Rahul Tallamraju"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Gupta_ViTOL_Vision_Transformer_for_Weakly_Supervised_Object_Localization_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Gupta_ViTOL_Vision_Transformer_for_Weakly_Supervised_Object_Localization_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Weakly supervised object localization (WSOL) aims at predicting object locations in an image using only image-level category labels. Common challenges that image classification models encounter when localizing objects are, (a) they tend to look at the most discriminative features in an image that confines the localization map to a very small region, (b) the localization maps are class agnostic, and the models highlight objects of multiple classes in the same image and, (c) the localization performance is affected by background noise. To alleviate the above challenges we introduce the following simple changes through our proposed method ViTOL. We leverage the vision-based transformer for self-attention and introduce a patch-based attention dropout layer (p-ADL) to increase the coverage of the localization map and a gradient attention rollout mechanism to generate class-dependent attention maps. We conduct extensive quantitative, qualitative and ablation experiments on the ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2 localization scores of 70.47% and 73.17% on the two datasets respectively. Code is available on https://github.com/Saurav-31/ViTOL.",
    "code_link": "https://github.com/Saurav-31/ViTOL"
  },
  "cvpr2022_l3d-ivu_consistency-basedactivelearningforobjectdetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Consistency-Based Active Learning for Object Detection",
    "authors": [
      "Weiping Yu",
      "Sijie Zhu",
      "Taojiannan Yang",
      "Chen Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Yu_Consistency-Based_Active_Learning_for_Object_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Yu_Consistency-Based_Active_Learning_for_Object_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Active learning aims to improve the performance of the task model by selecting the most informative samples with a limited budget. Unlike most recent works that focus on applying active learning for image classification, we propose an effective Consistency-based Active Learning method for object Detection (CALD), which fully explores the consistency between the original and augmented data. CALD has three appealing benefits. (i) CALD is systematically designed by investigating the weaknesses of existing active learning methods, which do not take the unique challenges of object detection into account. (ii) CALD unifies box regression and classification with a single metric, which is not concerned with active learning methods for classification. CALD also focuses on the most informative local region rather than the whole image, which is beneficial for object detection. (iii) CALD not only gauges individual information for sample selection but also leverages mutual information to encourage a balanced data distribution. Extensive experiments show that CALD significantly outperforms existing state-of-the-art task-agnostic and detection-specific active learning methods on general object detection datasets. Based on the Faster R-CNN detector, CALD consistently surpasses the baseline method (random selection) by 2.9/2.8/0.8 mAP on average on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO.",
    "code_link": "https://github.com/we1pingyu/CALD"
  },
  "cvpr2022_l3d-ivu_cfaconstraint-basedfinetuningapproachforgeneralizedfew-shotobjectdetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "CFA: Constraint-Based Finetuning Approach for Generalized Few-Shot Object Detection",
    "authors": [
      "Karim Guirguis",
      "Ahmed Hendawy",
      "George Eskandar",
      "Mohamed Abdelsamad",
      "Matthias Kayser",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Guirguis_CFA_Constraint-Based_Finetuning_Approach_for_Generalized_Few-Shot_Object_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Guirguis_CFA_Constraint-Based_Finetuning_Approach_for_Generalized_Few-Shot_Object_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Few-shot object detection (FSOD) seeks to detect novel categories with limited data by leveraging prior knowledge from abundant base data. Generalized few-shot object detection (G-FSOD) aims to tackle FSOD without forgetting previously seen base classes and, thus, accounts for a more realistic scenario, where both classes are encountered during test time. While current FSOD methods suffer from catastrophic forgetting, G-FSOD addresses this limitation yet exhibits a performance drop on novel tasks compared to the state-of-the-art FSOD. In this work, we propose a constraint-based fine-tuning approach (CFA) to alleviate catastrophic forgetting, while achieving competitive results on the novel task without increasing the model capacity. CFA adapts a continual learning method, namely average Gradient Episodic Memory (A-GEM) to G-FSOD. Specifically, more constraints on the gradient search strategy are imposed from which a new gradient update rule is derived, allowing for better knowledge exchange between base and novel classes. To evaluate our method, we conduct extensive experiments on MS-COCO and PASCAL-VOC datasets. Our method outperforms current FSOD and G-FSOD approaches on the novel task with minor degeneration on the base task. Moreover, CFA is orthogonal to FSOD approaches and operates as a plug-and-play module without increasing the model capacity or inference time.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_attentionconsistencyonvisualcorruptionsforsingle-sourcedomaingeneralization": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Attention Consistency on Visual Corruptions for Single-Source Domain Generalization",
    "authors": [
      "Ilke Cugu",
      "Massimiliano Mancini",
      "Yanbei Chen",
      "Zeynep Akata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Cugu_Attention_Consistency_on_Visual_Corruptions_for_Single-Source_Domain_Generalization_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Cugu_Attention_Consistency_on_Visual_Corruptions_for_Single-Source_Domain_Generalization_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Generalizing visual recognition models trained on a single distribution to unseen input distributions (i.e. domains) requires making them robust to superfluous correlations in the training set. In this work, we achieve this goal by altering the training images to simulate new domains and imposing consistent visual attention across the different views of the same sample. We discover that the first objective can be simply and effectively met through visual corruptions. Specifically, we alter the content of the training images using the nineteen corruptions of the ImageNet-C benchmark and three additional transformations based on Fourier transform. Since these corruptions preserve object locations, we propose an attention consistency loss to ensure that class activation maps across original and corrupted versions of the same training sample are aligned. We name our model Attention Consistency on Visual Corruptions (ACVC). We show that ACVC consistently achieves the state of the art on three single-source domain generalization benchmarks, PACS, COCO, and the large-scale DomainNet.",
    "code_link": "https://github.com/ExplainableML/ACVC"
  },
  "cvpr2022_l3d-ivu_few-shotclassincrementallearningleveragingself-supervisedfeatures": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Few-Shot Class Incremental Learning Leveraging Self-Supervised Features",
    "authors": [
      "Touqeer Ahmad",
      "Akshay Raj Dhamija",
      "Steve Cruz",
      "Ryan Rabinowitz",
      "Chunchun Li",
      "Mohsen Jafarzadeh",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Ahmad_Few-Shot_Class_Incremental_Learning_Leveraging_Self-Supervised_Features_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Ahmad_Few-Shot_Class_Incremental_Learning_Leveraging_Self-Supervised_Features_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Few-Shot Class Incremental Learning (FSCIL) is a recently introduced Class Incremental Learning (CIL) setting that operates under more constrained assumptions: only very few samples per class are available in each incremental session, and the number of samples/classes is known ahead of time. Due to limited data for class incremental learning, FSCIL suffers more from over-fitting and catastrophic forgetting than general CIL. In this paper we study leveraging the advances due to self-supervised learning to remedy over-fitting and catastrophic forgetting and significantly advance the state-of-the-art FSCIL. We explore training a lightweight feature fusion plus classifier on a concatenation of features emerging from supervised and self-supervised models. The supervised model is trained on data from a base session, where a relatively larger amount of data is available in FSCIL. Whereas a self-supervised model is learned using an abundance of unlabeled data. We demonstrate a classifier trained on the fusion of such features outperforms classifiers trained independently on either of these representations. We experiment with several existing self-supervised models and provide results for three popular benchmarks for FSCIL including Caltech-UCSD Birds-200-2011 (CUB200), miniImageNet, and CIFAR100 where we advance the state-of-the-art for each benchmark. Code is available at: https://github.com/TouqeerAhmad/FeSSSS",
    "code_link": "https://github.com/TouqeerAhmad/FeSSSS"
  },
  "cvpr2022_l3d-ivu_faster,lighter,robusteraweakly-supervisedcrowdanalysisenhancementnetworkandagenericfeatureextractionframework": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Faster, Lighter, Robuster: A Weakly-Supervised Crowd Analysis Enhancement Network and a Generic Feature Extraction Framework",
    "authors": [
      "Shaokai Wu",
      "Zhaogeng Liu",
      "Wencheng Pei",
      "Jianbo Hong",
      "Zhanshan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Wu_Faster_Lighter_Robuster_A_Weakly-Supervised_Crowd_Analysis_Enhancement_Network_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Wu_Faster_Lighter_Robuster_A_Weakly-Supervised_Crowd_Analysis_Enhancement_Network_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With bounding box labels needed for training, object detection is viewed unfavorably in terms of crowd analysis, due to the intensive labor for labeling and the unsatisfactory performance in clutters and severe occlusions. Another feasible method, density-based regression, despite its proficiency in counting and only point-level labels used for training, cannot get the location of each person, and the time and space consumption is relatively high. In this paper, we propose a generic feature extraction framework, Adaptive Pyramid Score (APS), based on object detection and designed specifically for extracting quantitative and spatial-semantic features. Moreover, as an intuitive and feasible solution regarding crowd analysis, we propose the weakly-supervised Confidence-Threshold-Foresight Network (CTFNet) under our APS feature extraction framework, which only needs count-level labels for training and improves the performance of various methods dramatically. Our system realizes the triple enhancement of counting, localization, and detection, which is also proved to be faster than advanced crowd analysis methods, lighter to be transplanted to various object detection methods, and robuster to tackle tasks of extreme scenes. Furthermore, the weakly-supervised paradigm leverage the intensive labor for labeling profoundly.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_few-shotsupervisedprototypealignmentforpedestriandetectiononfisheyeimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Few-Shot Supervised Prototype Alignment for Pedestrian Detection on Fisheye Images",
    "authors": [
      "Thadd\u00e4us Wiedemer",
      "Stefan Wolf",
      "Arne Schumann",
      "Kaisheng Ma",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Wiedemer_Few-Shot_Supervised_Prototype_Alignment_for_Pedestrian_Detection_on_Fisheye_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Wiedemer_Few-Shot_Supervised_Prototype_Alignment_for_Pedestrian_Detection_on_Fisheye_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Complete and pre-trained models are readily available for download for object detection and can perform well on datasets containing everyday images. Domain adaptation is used to transfer models to more specific datasets with characteristics not present in pre-training. We propose the novel adaptation setting of pedestrian detection in fisheye images, where target samples are scarce but annotated. Our setting provides interesting new challenges for adaptation due to global perspective changes and geometric distortions not found in existing adaptation tasks. To this end, we introduce loss coupling for unsupervised adversarial adaptation and boost prototype-based adaptation with ground-truth information. We additionally propose a novel supervised adaptation head for features in the bounding box regressor. Our method leads to more stable adversarial training and outperforms supervised and unsupervised baselines. Our method requires half the amount of training samples for small datasets to achieve the same performance as supervised fine-tuning.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_auxiliarylearningforself-supervisedvideorepresentationviasimilarity-basedknowledgedistillation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Auxiliary Learning for Self-Supervised Video Representation via Similarity-Based Knowledge Distillation",
    "authors": [
      "Amirhossein Dadashzadeh",
      "Alan Whone",
      "Majid Mirmehdi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Dadashzadeh_Auxiliary_Learning_for_Self-Supervised_Video_Representation_via_Similarity-Based_Knowledge_Distillation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Dadashzadeh_Auxiliary_Learning_for_Self-Supervised_Video_Representation_via_Similarity-Based_Knowledge_Distillation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite the outstanding success of self-supervised pretraining methods for video representation learning, they generalise poorly when the unlabeled dataset for pretraining is small or the domain difference between unlabelled data in source task (pretraining) and labeled data in target task (finetuning) is significant. To mitigate these issues, we propose a novel approach to complement self-supervised pretraining via an auxiliary pretraining phase, based on knowledge similarity distillation, auxSKD, for better generalisation with a significantly smaller amount of video data, e.g. Kinetics-100 rather than Kinetics-400. Our method deploys a teacher network that iteratively distils its knowledge to the student model by capturing the similarity information between segments of unlabelled video data. The student model meanwhile solves a pretext task by exploiting this prior knowledge. We also introduce a novel pretext task, Video Segment Pace Prediction or VSPP, which requires our model to predict the playback speed of a randomly selected segment of the input video to provide more reliable self-supervised representations. Our experimental results show superior results to the state of the art on both UCF101 and HMDB51 datasets when pretraining on K100 in apple-to-apple comparisons. Additionally, we show that our auxiliary pretraining, auxSKD, when added as an extra pretraining phase to recent state of the art self-supervised methods (i.e. VCOP, VideoPace, and RSPNet), improves their results on UCF101 and HMDB51. Our code is available at https://github.com/Plrbear/auxSKD.",
    "code_link": "https://github.com/Plrbear/auxSKD"
  },
  "cvpr2022_l3d-ivu_open-setdomainadaptationunderfewsource-domainlabeledsamples": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Open-Set Domain Adaptation Under Few Source-Domain Labeled Samples",
    "authors": [
      "Sayan Rakshit",
      "Balasubramanian S",
      "Hmrishav Bandyopadhyay",
      "Piyush Bharambe",
      "Sai Nandan Desetti",
      "Biplab Banerjee",
      "Subhasis Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Rakshit_Open-Set_Domain_Adaptation_Under_Few_Source-Domain_Labeled_Samples_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Rakshit_Open-Set_Domain_Adaptation_Under_Few_Source-Domain_Labeled_Samples_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recently, the notion of closed-set few-shot domain adaptation (FSDA) has been introduced where limited supervision is present in the source domain. However, FSDA overlooks the fact that the unlabeled target domain may contain new classes unseen in the source domain. To this end, we introduce the novel problem definition of few-shot open-set DA (FosDA) where the source domain contains few labeled samples together with a large pool of unlabeled data, and the target domain consists of test samples from the known as well as new categories. We propose an end-to-end model called FosDANet to tackle such a scenario which operates on two principles: to generate confident pseudo-labels for the unlabeled source samples and to classwise align the source and target domains for the known classes while rejecting the unknown-class data. A combination of a self-supervised loss and a novel triplet-based relation learning module is devised to aid in confident pseudo-labeling, and a dual adversarial learning scheme is proposed for domain alignment. Extensive experiments were performed on five datasets: Office-31, Office-Home, Adoptiape, and two new datasets we designed: mini-domainnet and a remote sensing benchmark called NPU-RSDA. FosDANet is found to consistently outperform the relevant literature.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_codocontrastivelearningwithdownstreambackgroundinvariancefordetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "CoDo: Contrastive Learning With Downstream Background Invariance for Detection",
    "authors": [
      "Bing Zhao",
      "Jun Li",
      "Hong Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Zhao_CoDo_Contrastive_Learning_With_Downstream_Background_Invariance_for_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Zhao_CoDo_Contrastive_Learning_With_Downstream_Background_Invariance_for_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The prior self-supervised learning researches mainly select image-level instance discrimination as pretext task. It achieves a fantastic classification performance that is comparable to supervised learning methods. However, with degraded transfer performance on downstream tasks such as object detection. To bridge the performance gap, we propose a novel object-level self-supervised learning method, called Contrastive learning with Downstream background invariance (CoDo). The pretext task is converted to focus on instance location modeling for various backgrounds, especially for downstream datasets. The ability of background invariance is considered vital for object detection. Firstly, a data augmentation strategy is proposed to paste the instances onto background images, and then jitter the bounding box to involve background information. Secondly, we implement architecture alignment between our pretraining network and the mainstream detection pipelines. Thirdly, hierarchical and multi views contrastive learning is designed to improve performance of visual representation learning. Experiments on MSCOCO demonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields strong transfer learning results for object detection.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_bootstrappedrepresentationlearningforskeleton-basedactionrecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Bootstrapped Representation Learning for Skeleton-Based Action Recognition",
    "authors": [
      "Olivier Moliner",
      "Sangxia Huang",
      "Kalle \u00c5str\u00f6m"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Moliner_Bootstrapped_Representation_Learning_for_Skeleton-Based_Action_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Moliner_Bootstrapped_Representation_Learning_for_Skeleton-Based_Action_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this work, we study self-supervised representation learning for 3D skeleton-based action recognition. We extend Bootstrap Your Own Latent (BYOL) for representation learning on skeleton sequence data and propose a new data augmentation strategy including two asymmetric transformation pipelines. We also introduce a multi-viewpoint sampling method that leverages multiple viewing angles of the same action captured by different cameras. In the semi-supervised setting, we show that the performance can be further improved by knowledge distillation from wider networks, leveraging once more the unlabeled samples. We conduct extensive experiments on the NTU-60, NTU-120 and PKU-MMD datasets to demonstrate the performance of our proposed method. Our method consistently outperforms the current state of the art on linear evaluation, semi-supervised and transfer learning benchmarks.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_black-boxtest-timeshaperefinementforsingleview3dreconstruction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Black-Box Test-Time Shape REFINEment for Single View 3D Reconstruction",
    "authors": [
      "Brandon Leung",
      "Chih-Hui Ho",
      "Nuno Vasconcelos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Leung_Black-Box_Test-Time_Shape_REFINEment_for_Single_View_3D_Reconstruction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Leung_Black-Box_Test-Time_Shape_REFINEment_for_Single_View_3D_Reconstruction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Much recent progress has been made in reconstructing 3D object shape from an image, i.e. single view 3D reconstruction. However, due to the difficulty of collecting large datasets in the wild with 3D ground truth, it remains a significant challenge for methods to generalize across domain, viewpoint, and class. Current methods also tend to produce averaged \"nearest-neighbor\" memorized shapes instead of genuinely understanding the image, thus eliminating important details. To address this we propose REFINE, a postprocessing mesh refinement step easily integratable into the pipeline of any black-box method in the literature. At test time, REFINE optimizes a network per mesh instance, to encourage consistency between the mesh and the given object view. This, with a novel combination of losses addressing degenerate solutions, reduces domain gap and restores details to achieve state of the art performance. A new hierarchical multiview, multidomain image dataset with 3D meshes called 3D-ODDS is also proposed as a uniquely challenging benchmark. We believe that the novel REFINE paradigm and 3D-ODDS are important steps towards truly robust, accurate 3D reconstructions.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_auxmixsemi-supervisedlearningwithunconstrainedunlabeleddata": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "AuxMix: Semi-Supervised Learning With Unconstrained Unlabeled Data",
    "authors": [
      "Amin Banitalebi-Dehkordi",
      "Pratik Gujjar",
      "Yong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Banitalebi-Dehkordi_AuxMix_Semi-Supervised_Learning_With_Unconstrained_Unlabeled_Data_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Banitalebi-Dehkordi_AuxMix_Semi-Supervised_Learning_With_Unconstrained_Unlabeled_Data_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Semi-supervised learning (SSL) has seen great strides when labeled data is scarce but unlabeled data is abundant. Critically, most recent work assume that such unlabeled data is drawn from the same distribution as the labeled data. In this work, we show that state-of-the-art SSL algorithms suffer a degradation in performance in the presence of unlabeled auxiliary data that does not necessarily possess the same class distribution as the labeled set. We term this problem as Auxiliary-SSL and propose AuxMix, an algorithm that leverages self-supervised learning tasks to learn generic features in order to mask auxiliary data that are not semantically similar to the labeled set. We also propose to regularize learning by maximizing the predicted entropy for dissimilar auxiliary samples. We show an improvement of 5% over existing baselines on a ResNet-50 model when trained on CIFAR10 dataset with 4k labeled samples and all unlabeled data is drawn from the Tiny-Imagenet dataset. We report competitive results on several datasets and conduct ablation studies.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_revisitingvicinalriskminimizationforpartiallysupervisedmulti-labelclassificationunderdatascarcity": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity",
    "authors": [
      "Nanqing Dong",
      "Jiayi Wang",
      "Irina Voiculescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Dong_Revisiting_Vicinal_Risk_Minimization_for_Partially_Supervised_Multi-Label_Classification_Under_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Dong_Revisiting_Vicinal_Risk_Minimization_for_Partially_Supervised_Multi-Label_Classification_Under_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_uniformpriorsfordata-efficientlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Uniform Priors for Data-Efficient Learning",
    "authors": [
      "Samarth Sinha",
      "Karsten Roth",
      "Anirudh Goyal",
      "Marzyeh Ghassemi",
      "Zeynep Akata",
      "Hugo Larochelle",
      "Animesh Garg"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Sinha_Uniform_Priors_for_Data-Efficient_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Sinha_Uniform_Priors_for_Data-Efficient_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models. It is therefore crucial to find properties that encourage more transferable features in deep networks for generalization. In this paper, we show that models that learn uniformly distributed features from the training data, are able to perform better transfer learning at test-time. Motivated by this, we evaluate our method: uniformity regularization (\\mathcal UR ) on its ability to facilitate adaptation to unseen tasks and data on six distinct domains: Few-Learning with Images, Few-shot Learning with Language, Deep Metric Learning, Zero-Shot Domain Adaptation, Out-of-Distribution classification, and Neural Radiance Fields. Across all experiments, we show that using \\mathcal UR , we are able to learn robust vision systems which consistently offer benefits over baselines trained without uniformity regularization and are able to achieve state-of-the-art performance in Deep Metric Learning, Few-shot learning with images and language.",
    "code_link": "https://github.com/tristandeleu/pytorch-meta"
  },
  "cvpr2022_l3d-ivu_cluster-to-adaptfewshotdomainadaptationforsemanticsegmentationacrossdisjointlabels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Cluster-To-Adapt: Few Shot Domain Adaptation for Semantic Segmentation Across Disjoint Labels",
    "authors": [
      "Tarun Kalluri",
      "Manmohan Chandraker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Kalluri_Cluster-To-Adapt_Few_Shot_Domain_Adaptation_for_Semantic_Segmentation_Across_Disjoint_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Kalluri_Cluster-To-Adapt_Few_Shot_Domain_Adaptation_for_Semantic_Segmentation_Across_Disjoint_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Domain adaptation for semantic segmentation across datasets consisting of the same categories has seen several recent successes. However, a more general scenario is when the source and target datasets correspond to non-overlapping label spaces. For example, categories in segmentation datasets change vastly depending on the type of environment or application, yet share many valuable semantic relations. Existing approaches based on feature alignment or discrepancy minimization do not take such category shift into account. In this work, we present Cluster-to-Adapt (C2A), a computationally efficient clustering-based approach for domain adaptation across segmentation datasets with completely different, but possibly related categories. We show that such a clustering objective enforced in a transformed feature space serves to automatically select categories across source and target domains that can be aligned for improving the target performance, while preventing negative transfer for unrelated categories. We demonstrate the effectiveness of our approach through experiments on the challenging problem of outdoor to indoor adaptation for semantic segmentation in few-shot as well as zero-shot settings, with consistent improvements in performance over existing approaches and baselines in all cases.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_self-supervisedlearningofpose-informedlatents": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Self-Supervised Learning of Pose-Informed Latents",
    "authors": [
      "Rapha\u00ebl Jean",
      "Pierre-Luc St-Charles",
      "S\u00f6ren Pirk",
      "Simon Brodeur"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Jean_Self-Supervised_Learning_of_Pose-Informed_Latents_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Jean_Self-Supervised_Learning_of_Pose-Informed_Latents_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Siamese network architectures trained for self-supervised instance recognition can learn powerful visual representations that are useful in various tasks. Many such approaches maximize the similarity between representations of augmented images of the same object. In this paper, we depart from traditional self-supervised learning benchmarks by defining a novel methodology for new challenging tasks such as pose estimation. Our goal is to show that common Siamese networks can effectively be trained on frame pairs from video sequences to generate pose-informed representations. Unlike parallel efforts that focus on introducing new image-space operators for data augmentation, we argue that extending the augmentation strategy by using different frames of a video leads to more powerful representations. To show the effectiveness of this approach, we use the Objectron and UCF101 datasets to learn representations and evaluate them on pose estimation, action recognition, and object re-identification. Furthermore, we carefully validate our method against a number of baselines.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_transformaly-two(featurespaces)arebetterthanone": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Transformaly - Two (Feature Spaces) Are Better Than One",
    "authors": [
      "Matan Jacob Cohen",
      "Shai Avidan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Cohen_Transformaly_-_Two_Feature_Spaces_Are_Better_Than_One_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Cohen_Transformaly_-_Two_Feature_Spaces_Are_Better_Than_One_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Anomaly detection is a well-established research area that seeks to identify samples outside of a predetermined distribution. An anomaly detection pipeline is comprised of two main stages: (1) feature extraction and (2) normality score assignment. Recent papers used pre-trained networks for feature extraction achieving state-of-the-art results. However, the use of pre-trained networks does not fully-utilize the normal samples that are available at train time. This paper suggests taking advantage of this information by using teacher-student training. In our setting, a pretrained teacher network is used to train a student network on the normal training samples. Since the student network is trained only on normal samples, it is expected to deviate from the teacher network in abnormal cases. This difference can serve as a complementary representation to the pre-trained feature vector. Our method - Transformaly - exploits a pre-trained Vision Transformer (ViT) to extract both feature vectors: the pre-trained (agnostic) features and the teacher-student (fine-tuned) features. We report state of-the-art AUROC results in both the common unimodal setting, where one class is considered normal and the rest are considered abnormal, and the multimodal setting, where all classes but one are considered normal, and just one class is considered abnormal.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_vicinalcountingnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Vicinal Counting Networks",
    "authors": [
      "Viresh Ranjan",
      "Minh Hoai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Ranjan_Vicinal_Counting_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Ranjan_Vicinal_Counting_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We tackle the task of Few-Shot Counting. Given an image containing multiple objects of a novel visual category and few exemplar bounding boxes depicting the visual category of interest, we want to count all of the instances of the desired visual category in the image. A key challenge in building an accurate few-shot visual counter is the scarcity of annotated training data due to the laborious effort needed for collecting and annotating the data. To address this challenge, we propose Vicinal Counting Networks, which learn to augment the existing training data along with learning to count. A Vicinal Counting Network consists of a generator and a counting network. The generator takes as input an image along with a random noise vector and generates an augmented version of the input image. The counting network learns to count the objects in the original and augmented images. The training signal for the generator comes from the counting loss of the counting network, and the generator aims to synthesize images which result in a small counting loss. Unlike GANs which are trained in an adversarial setting, Vicinal Counting Networks are trained in a cooperative setting where the generator aims to help the counting network in achieving accurate predictions on the synthesized images. We also show that our proposed data augmentation framework can be extended to other counting tasks like crowd counting.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_whatshouldbeequivariantinself-supervisedlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "What Should Be Equivariant in Self-Supervised Learning",
    "authors": [
      "Yuyang Xie",
      "Jianhong Wen",
      "Kin Wai Lau",
      "Yasar Abbas Ur Rehman",
      "Jiajun Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Xie_What_Should_Be_Equivariant_in_Self-Supervised_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Xie_What_Should_Be_Equivariant_in_Self-Supervised_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Self-supervised learning (SSL) aims to learn feature representation without human-annotated data. Existing methods approach this goal by encouraging the feature representations to be invariant under a set of task-irrelevant transformations and distortions defined a priori. However, multiple studies have shown that such an assumption often limits the expressive power of the representations and model would perform poorly when downstream tasks violate this assumption. For example, being invariant to rotations would prevent features from retaining enough information to estimate object rotation angles. This suggests additional manual work and domain knowledge are required for selecting augmentation types during SSL. In this work, we relax the transformation-invariance assumption by introducing a SSL framework that encourages the feature representations to preserve the order of transformation scale in embedding space for some transformations while maintaining invariance to other transformations. This allows the learned feature representations to retain information about task-relevant transformations. In addition, this framework gives rise to a handy mechanism to determine the augmentation types to which the features representations should be invariant and equivariant during SSL. We demonstrate the effectiveness of our method on various datasets such as Fruits 360, Caltech-UCSD Birds 200, and Blood cells dataset.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_zero-shotlearningusingmultimodaldescriptions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Zero-Shot Learning Using Multimodal Descriptions",
    "authors": [
      "Utkarsh Mall",
      "Bharath Hariharan",
      "Kavita Bala"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Mall_Zero-Shot_Learning_Using_Multimodal_Descriptions_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Mall_Zero-Shot_Learning_Using_Multimodal_Descriptions_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Zero-shot learning (ZSL) tackles the problem of recognition of unseen classes using only semantic descriptions, e.g., attributes. Current zero-shot learning techniques all assume that a single vector of attributes suffices to describe each category. We show that this assumption is incorrect. Many classes in real-world problems have multiple modes of appearance: male and female birds vary in appearance, for instance. Domain experts know this and can provide attribute descriptions of the chief modes of appearance for each class. Motivated by this, we propose the task of multimodal zero-shot learning, where the learner must learn from these multimodal attribute descriptions. We present new benchmarks for this task on CUB, SUN, and DeepFashion and a multimodal ZSL technique that outperform the unimodal counterpart significantly. Because it allows annotators to provide more than one description, we posit that multimodal ZSL is more practical for real-world deployment.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_few-shotimageclassificationalongsparsegraphs": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Few-Shot Image Classification Along Sparse Graphs",
    "authors": [
      "Joseph F. Comer",
      "Philip L. Jacobson",
      "Heiko Hoffmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Comer_Few-Shot_Image_Classification_Along_Sparse_Graphs_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Comer_Few-Shot_Image_Classification_Along_Sparse_Graphs_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Few-shot learning remains a challenging problem, with unsatisfactory 1-shot accuracies for most real-world data. Here, we present a new perspective for data distributions in the feature space of a deep network and show how to exploit this perspective for few-shot learning. First, we observe that nearest neighbors in the feature space are with high probability members of the same class while generally two random points from one class are not much closer to each other than two points between classes. This observation suggests that classes in feature space form sparse, loosely connected graphs instead of dense clusters. To exploit this property, we propose using label propagation to the nearest unlabeled data and then using a kernel PCA reconstruction error as decision boundary in feature-space for the data distribution of each class. Using this method, which we call \"K-Prop,\" we demonstrate largely improved few-shot learning performances (e.g., 83% accuracy for 1-shot 5-way classification on the RESISC45 satellite-images dataset) for datasets for which a backbone network can be trained to produce high within-class nearest-neighbor probabilities. We demonstrate this relationship using six different datasets.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_semanticposeverificationforoutdoorvisuallocalizationwithself-supervisedcontrastivelearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Semantic Pose Verification for Outdoor Visual Localization With Self-Supervised Contrastive Learning",
    "authors": [
      "Semih Orhan",
      "Jose J. Guerrero",
      "Yal\u0131n Ba\u015ftanlar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Orhan_Semantic_Pose_Verification_for_Outdoor_Visual_Localization_With_Self-Supervised_Contrastive_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Orhan_Semantic_Pose_Verification_for_Outdoor_Visual_Localization_With_Self-Supervised_Contrastive_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Any city-scale visual localization system has to overcome long-term appearance changes, such as varying illumination conditions or seasonal changes between query and database images. Since semantic content is more robust to such changes, we exploit semantic information to improve visual localization. In our scenario, the database consists of gnomonic views generated from panoramic images (e.g. Google Street View) and query images are collected with a standard field-of-view camera at a different time. To improve localization, we check the semantic similarity between query and database images, which is not trivial since the position and viewpoint of the cameras do not exactly match. To learn similarity, we propose training a CNN in a self-supervised fashion with contrastive learning on a dataset of semantically segmented images. With experiments we showed that this semantic similarity estimation approach works better than measuring the similarity at pixel-level. Finally, we used the semantic similarity scores to verify the retrievals obtained by a state-of-the-art visual localization method and observed that contrastive learning-based pose verification increases top-1 recall value to 0.90 which corresponds to a 2% improvement.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_compositionalmixturerepresentationsforvisionandtext": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Compositional Mixture Representations for Vision and Text",
    "authors": [
      "Stephan Alaniz",
      "Marco Federici",
      "Zeynep Akata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Alaniz_Compositional_Mixture_Representations_for_Vision_and_Text_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Alaniz_Compositional_Mixture_Representations_for_Vision_and_Text_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning a common representation space between vision and language allows deep networks to relate objects in the image to the corresponding semantic meaning. We present a model that learns a shared Gaussian mixture representation imposing the compositionality of the text onto the visual domain without having explicit location supervision. By combining the spatial transformer with a representation learning approach we learn to split images into separately encoded patches to associate visual and textual representations in an interpretable manner. On variations of MNIST and CIFAR10, our model is able to perform weakly supervised object detection and demonstrates its ability to extrapolate to unseen combination of objects.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_self-supervisedvideorepresentationlearningwithcascadepositiveretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Self-Supervised Video Representation Learning With Cascade Positive Retrieval",
    "authors": [
      "Cheng-En Wu",
      "Farley Lai",
      "Yu Hen Hu",
      "Asim Kadav"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Wu_Self-Supervised_Video_Representation_Learning_With_Cascade_Positive_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Wu_Self-Supervised_Video_Representation_Learning_With_Cascade_Positive_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Self-supervised video representation learning has been shown to effectively improve downstream tasks such as video retrieval and action recognition. In this paper, we present the Cascade Positive Retrieval (CPR) that successively mines positive examples w.r.t. the query for contrastive learning in a cascade of stages. Specifically, CPR exploits multiple views of a query example in different modalities, where an alternative view may help find another positive example dissimilar in the query view. We explore the effects of possible CPR configurations in ablations including the number of mining stages, the top similar example selection ratio in each stage, and progressive training with an incremental number of the final Top-k selection. The overall mining quality is measured to reflect the recall across training set classes. CPR reaches a median class mining recall of 83.3%, outperforming previous work by 5.5%. Implementation-wise, CPR is complementary to pretext tasks and can be easily applied to previous work. In the evaluation of pretraining on UCF101, CPR consistently improves existing work and even achieves state-of-the-art R@1 of 56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action recognition on UCF101 and HMDB51. The code is available at https://github.com/necla-ml/CPR.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_candomainadaptationmakeobjectrecognitionworkforeveryone?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "Can Domain Adaptation Make Object Recognition Work for Everyone?",
    "authors": [
      "Viraj Prabhu",
      "Ramprasaath R. Selvaraju",
      "Judy Hoffman",
      "Nikhil Naik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Prabhu_Can_Domain_Adaptation_Make_Object_Recognition_Work_for_Everyone_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Prabhu_Can_Domain_Adaptation_Make_Object_Recognition_Work_for_Everyone_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite the rapid progress in deep visual recognition, modern computer vision datasets significantly overrepresent the developed world and models trained on such datasets underperform on images from unseen geographies. We investigate the effectiveness of unsupervised domain adaptation (UDA) of such models across geographies at closing this performance gap. To do so, we first curate two shifts from existing datasets to study the Geographical DA problem, and discover new challenges beyond data distribution shift: context shift, wherein object surroundings may change significantly across geographies, and subpopulation shift, wherein the intra-category distributions may shift. We demonstrate the inefficacy of standard DA methods at Geographical DA, highlighting the need for specialized geographical adaptation solutions to address the challenge of making object recognition work for everyone.",
    "code_link": ""
  },
  "cvpr2022_l3d-ivu_sarself-adaptiverefinementonpseudolabelsformulticlass-imbalancedsemi-supervisedlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "L3D-IVU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Learning With Limited Labelled Data for Image and Video Understanding",
    "title": "SaR: Self-Adaptive Refinement on Pseudo Labels for Multiclass-Imbalanced Semi-Supervised Learning",
    "authors": [
      "Zhengfeng Lai",
      "Chao Wang",
      "Sen-ching Cheung",
      "Chen-Nee Chuah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Lai_SaR_Self-Adaptive_Refinement_on_Pseudo_Labels_for_Multiclass-Imbalanced_Semi-Supervised_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Lai_SaR_Self-Adaptive_Refinement_on_Pseudo_Labels_for_Multiclass-Imbalanced_Semi-Supervised_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Class-imbalanced datasets can severely deteriorate the performance of semi-supervised learning (SSL). This is due to the confirmation bias especially when the pseudo labels are highly biased towards the majority classes. Traditional resampling or reweighting techniques may not be directly applicable when the unlabeled data distribution is unknown. Inspired by the threshold-moving method that performs well in supervised learning-based binary classification tasks, we provide a simple yet effective scheme to address the multiclass imbalance issue of SSL. This scheme, named SaR, is a Self-adaptive Refinement of soft labels before generating pseudo labels. The pseudo labels generated post-SaR will be less biased, resulting in higher quality data for training the classifier. We show that SaR can consistently improve recent consistency-based SSL algorithms on various image classification problems across different imbalanced ratios. We also show that SaR is robust to the situations where unlabeled data have different distributions as labeled data. Hence, SaR does not rely on the assumptions that unlabeled data share the same distribution as the labeled data.",
    "code_link": ""
  },
  "cvpr2022_ug2_locatingurbantreesnearelectricwiresusinggooglestreetviewphotosanewdatasetandasemi-supervisedlearningapproachinthewild": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Locating Urban Trees Near Electric Wires Using Google Street View Photos: A New Dataset and a Semi-Supervised Learning Approach in the Wild",
    "authors": [
      "Artur Andr\u00e9 A. M. Oliveira",
      "Zhangyang Wang",
      "Roberto Hirata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Oliveira_Locating_Urban_Trees_Near_Electric_Wires_Using_Google_Street_View_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Oliveira_Locating_Urban_Trees_Near_Electric_Wires_Using_Google_Street_View_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Vegetation is desirable in most urban spaces, but its management is not easy, mainly the intersection between trees and sidewalks, or trees and electric wires. This work presents a method to automatically detect the latter using ground-level images instead of aerial images. Real-world ground-level urban images are cheap to collect, but they may be hard to label and classify because neural networks tend to be overconfident, and manually labeling thousands of images may be cumbersome and unfeasible. We propose using Focal Loss to calibrate an overconfident neural network and the use of the training protocol Noisy Student to lessen the burden of manually labeling images. Our results show that these methods improve the results over the Cross-Entropy loss, and the confidence levels of the predictions can be used in an Active Learning system to improve the overall accuracy.",
    "code_link": ""
  },
  "cvpr2022_ug2_deepscale-spaceminingnetworkforsingleimagederaining": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Deep Scale-Space Mining Network for Single Image Deraining",
    "authors": [
      "Pengpeng Li",
      "Jiyu Jin",
      "Guiyue Jin",
      "Lei Fan",
      "Xiao Gao",
      "Tianyu Song",
      "Xiang Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Li_Deep_Scale-Space_Mining_Network_for_Single_Image_Deraining_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Li_Deep_Scale-Space_Mining_Network_for_Single_Image_Deraining_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Images captured by outdoor vision systems can often be affected by rain weather, resulting in severe degradation of the visual quality of the captured images. Therefore, image deraining has attracted attention as urgent and challenging research. Many current data-driven approaches achieve better performance but are limited in recovering image details. This is because these methods do not fully mine the correlation of scale-space, which are beneficial for rain removal. In this paper, we design an end-to-end Deep Scale-space Mining Network (DSM-Net) for single image deraining to solve these problems. The proposed network with multi-scale extraction, concurrent attention distillation, and hierarchical information fusion accurately captures scale-space features and learns richer information for better deraining. For better feature extraction, a Multi-scale Attention Block (MAB) is introduced to obtain multi-scale rain streak features by different dilated convolutions. Besides, a Concurrent Attention Distillation Block (CADB) is developed which combined channel attention and subspace attention to calibrate the image features obtained from multiscale acquisition and hierarchical learning, then eliminate redundant features. Importantly, the overall architecture of DSM-Net is inspired by the HourglassNet and DenseNet, which progressively explores and fuses local and global features at different scales in a hierarchical manner instead of direct concatenation. Extensive experiments on synthetic and real datasets show that the proposed DSM-Net outperforms recent state-of-the-art deraining algorithms in terms of both performance and preservation of image details.",
    "code_link": ""
  },
  "cvpr2022_ug2_tardettwo-stageanchor-freerotatingobjectdetectorinaerialimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "TARDet: Two-Stage Anchor-Free Rotating Object Detector in Aerial Images",
    "authors": [
      "Longgang Dai",
      "Hongming Chen",
      "Yufeng Li",
      "Caihua Kong",
      "Zhentao Fan",
      "Jiyang Lu",
      "Xiang Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Dai_TARDet_Two-Stage_Anchor-Free_Rotating_Object_Detector_in_Aerial_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Dai_TARDet_Two-Stage_Anchor-Free_Rotating_Object_Detector_in_Aerial_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Detection of rotating object in aerial images is a practical and challenging task. Nowadays, most detectors rely on anchor boxes with different scales, aspect ratios and angles for aerial objects that are usually distributed in arbitrary directions and show huge variations in scale and aspect ratios. However, the detection performance of these detectors is very sensitive to the anchoring hyperparameters. To address this issue, in this paper, we propose a Two-stage Anchor-free Rotating object Detector (TARDet). Our TARDet first aggregates feature pyramid context information by a feature refinement module, and generates rough localization boxes in an anchor-free manner by a directed generation module (DGM) in the first stage, and then refines it to a higher quality localization scheme. Furthermore, we design an alignment convolution module to extract alignment features and introduce RiRoI to adaptively extract rotationally invariant features from isovariant features. Finally, we apply a modified fast R-CNN head to generate the final detection results. Our approach achieves state-of-the-art performance on two popular aerial objects datasets, DOTA and HRSC2016.",
    "code_link": ""
  },
  "cvpr2022_ug2_z-domainentropyadaptableflexforsemi-supervisedactionrecognitioninthedark": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Z-Domain Entropy Adaptable Flex for Semi-Supervised Action Recognition in the Dark",
    "authors": [
      "Zhi Chen",
      "Zijun Fan",
      "Yongjie Li",
      "Huaien Gao",
      "Shan Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Chen_Z-Domain_Entropy_Adaptable_Flex_for_Semi-Supervised_Action_Recognition_in_the_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Chen_Z-Domain_Entropy_Adaptable_Flex_for_Semi-Supervised_Action_Recognition_in_the_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The subtask of Human Action Recognition (AR) in the dark is gaining a lot of traction nowadays, which takes a significant place in the field of computer vision. The implementation of its application includes self-driving at night, human-pose estimation, night surveillance, etc. Currently, solutions such as DLN for AR have emerged. However, due to the poor accuracy even when leveraging on large amounts of datasets and complex architectures, the development of AR in the dark has been slow to progress. In this paper, we propose a novel and straightforward method: Z-Domain Entropy Adaptable Flex. This constructs a neural network architecture R(2+1)D, including (i) a self- attention mechanism, which combines and extracts corresponding and complementary features from the dual path- ways; (ii) Zero-DCE low light image enhancement, which improves enhanced quality; and (iii) FlexMatch method, which can generates the pseudo-labels flexibly. With the help of pseudo-labels from FlexMatch, our proposed Z- DEAF method facilitates the process of gaining desired classification boundaries. This works by repeating Expand- ing Entropy and Shrinking Entropy. It aims to solve the problem of unclear classification boundaries between the categories. Our model obtains superior performance in experiments, and achieves state-of-the-art results on ARID.",
    "code_link": ""
  },
  "cvpr2022_ug2_contrastivelearning-basedrobustobjectdetectionundersmokyconditions": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Contrastive Learning-Based Robust Object Detection Under Smoky Conditions",
    "authors": [
      "Wei Wu",
      "Hao Chang",
      "Yonghua Zheng",
      "Zhu Li",
      "Zhiwen Chen",
      "Ziheng Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Wu_Contrastive_Learning-Based_Robust_Object_Detection_Under_Smoky_Conditions_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Wu_Contrastive_Learning-Based_Robust_Object_Detection_Under_Smoky_Conditions_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Object detection is to effectively find out interested targets in images and then accurately determine their categories and positions. Recently many excellent methods have been developed to provide powerful detection capability. However, their performance may degrade significantly under severe weather such as smoky conditions. In this paper, we propose a contrastive learning-based robust object detection algorithm for smoke images. The proposed object detector consists of two modules: contrastive learning module and object bounding box prediction module. The first module learns representation vectors by maximizing agreement between different augmented views of the same smoke image. These representations are then sent to the second module to yield the bounding box for each object. In addition, we also propose a novel affine data augmentation method. Extensive experiments have been conducted on A2I2-Haze dataset which is the first real haze dataset with in-situ smoke measurement aligned to aerial and ground imagery. This dataset is also the only dataset used in the 5th UG2+ challenges of CVPR 2022 for both training and testing. Compared with state-of-the-art methods, evaluation results show the superiority of our proposed object detector.",
    "code_link": "https://github.com/ultralytics/yolov5"
  },
  "cvpr2022_ug2_detecting,trackingandcountingmotorcycleridertrafficviolationsonunconstrainedroads": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Detecting, Tracking and Counting Motorcycle Rider Traffic Violations on Unconstrained Roads",
    "authors": [
      "Aman Goyal",
      "Dev Agarwal",
      "Anbumani Subramanian",
      "C.V. Jawahar",
      "Ravi Kiran Sarvadevabhatla",
      "Rohit Saluja"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Goyal_Detecting_Tracking_and_Counting_Motorcycle_Rider_Traffic_Violations_on_Unconstrained_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Goyal_Detecting_Tracking_and_Counting_Motorcycle_Rider_Traffic_Violations_on_Unconstrained_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In many Asian countries with unconstrained road traffic conditions, driving violations such as not wearing helmets and triple-riding are a significant source of fatalities involving motorcycles. Identifying and penalizing such riders is vital in curbing road accidents and improving citizens' safety. With this motivation, we propose an approach for detecting, tracking, and counting motorcycle riding violations in videos taken from a vehicle-mounted dashboard camera. We employ a curriculum learning-based object detector to better tackle challenging scenarios such as occlusions. We introduce a novel trapezium-shaped object boundary representation to increase robustness and tackle the rider-motorcycle association. We also introduce an amodal regressor that generates bounding boxes for the occluded riders. Experimental results on a large-scale unconstrained driving dataset demonstrate the superiority of our approach compared to existing approaches and other ablative variants.",
    "code_link": "https://github.com/iHubData-Mobility/public-motorcycle-violations"
  },
  "cvpr2022_ug2_domainadaptablenormalizationforsemi-supervisedactionrecognitioninthedark": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Domain Adaptable Normalization for Semi-Supervised Action Recognition in the Dark",
    "authors": [
      "Zixi Liang",
      "Jiajun Chen",
      "Rui Chen",
      "Bingbing Zheng",
      "Mingyue Zhou",
      "Huaien Gao",
      "Shan Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/html/Liang_Domain_Adaptable_Normalization_for_Semi-Supervised_Action_Recognition_in_the_Dark_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/UG2/papers/Liang_Domain_Adaptable_Normalization_for_Semi-Supervised_Action_Recognition_in_the_Dark_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Action recognition in the dark is gaining more and more attention with the rapid development of intelligent recognition applications in real-world applications, e.g. self-driving at night and night surveillance. However, limited by the expensive labeling cost, it is impractical to produce a large-scale labeled dataset only for dark environments. Therefore, a practical solution adopted is to transfer models trained from clear environments to dark environments through semi-supervised learning. However, prior works rely heavily on additional efforts such as extra annotations, or extra sensors. To this end, we proposed a novel and simple Domain Adaptable Normalization (DANorm) method to align different domains directly, which consists of feature normalization, angle constraint and the Pseudo-Label. Specifically, the proposed DANorm method enables the model automatically learning the associated features between labeled source domain and unlabeled target domain by constraining the feature subspace vectors. Experimental results show that our model achieves superiority performance on Semi-supervised ARID dataset. Code is available at: https://github.com/NikkiElwin/DANorm.",
    "code_link": "https://github.com/NikkiElwin/DANorm"
  },
  "cvpr2022_hcis_hmiway-envaframeworkforsimulatingbehaviorsandpreferencestosupporthuman-aiteamingindriving": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "HCIS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Human-Centered Intelligent Services: Safe and Trustworthy",
    "title": "HMIway-Env: A Framework for Simulating Behaviors and Preferences To Support Human-AI Teaming in Driving",
    "authors": [
      "Deepak Gopinath",
      "Jonathan DeCastro",
      "Guy Rosman",
      "Emily Sumner",
      "Allison Morgan",
      "Shabnam Hakimi",
      "Simon Stent"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Gopinath_HMIway-Env_A_Framework_for_Simulating_Behaviors_and_Preferences_To_Support_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Gopinath_HMIway-Env_A_Framework_for_Simulating_Behaviors_and_Preferences_To_Support_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We introduce a lightweight simulation and modeling framework, HMIway-env, for studying human-machine teaming in the context of driving. The goal of the framework is to accelerate the development of adaptive AI systems which can respond to individual driver states, traits, and preferences, by serving as a data-generation engine and training environment for learning personalized human-AI teaming policies. We extend highway-env, an OpenAI Gym-based simulator environment, to enable specification of human driver behavior, and design of vehicle-driver interactions and outcomes. We describe one instance of our framework incorporating models for distracted and cautious driving, which we validate through crowd-sourced feedback, and show early experimental results toward the training of better intervention policies.",
    "code_link": ""
  },
  "cvpr2022_hcis_improvingrobustnesstotexturebiasviashape-focusedaugmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "HCIS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Human-Centered Intelligent Services: Safe and Trustworthy",
    "title": "Improving Robustness to Texture Bias via Shape-Focused Augmentation",
    "authors": [
      "Sangjun Lee",
      "Inwoo Hwang",
      "Gi-Cheon Kang",
      "Byoung-Tak Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite significant progress of deep neural networks in image classification, it has been reported that CNNs trained on ImageNet have heavily focused on local texture information, rather than capturing complex visual concepts of the objects. To delve into this phenomenon, recent studies proposed to generate images with modified texture information for training the model. However, these methods largely sacrifice the classification accuracy on the in-domain dataset while achieving improved performance on the out-of-distribution dataset. Motivated by the fact that human tends to focus on shape information, we aim to resolve this issue by proposing a shape-focused augmentation where the texture in the object's foreground and background are separately changed. Key idea is that by applying different modifications to the inside and outside of an object, not only the bias toward texture is reduced but also the model is induced to focus on shape. Experiments show that the proposed method successfully reduces texture bias and also improves the classification performance on the original dataset.",
    "code_link": ""
  },
  "cvpr2022_hcis_efficienttwo-stagemodelretrainingformachineunlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "HCIS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Human-Centered Intelligent Services: Safe and Trustworthy",
    "title": "Efficient Two-Stage Model Retraining for Machine Unlearning",
    "authors": [
      "Junyaup Kim",
      "Simon S. Woo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kim_Efficient_Two-Stage_Model_Retraining_for_Machine_Unlearning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Kim_Efficient_Two-Stage_Model_Retraining_for_Machine_Unlearning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "With the rise of the General Data Protection Regulation (GDPR), user data holders should guarantee the \"individual's right to be forgotten\". It means user data holders must completely remove user data when they receive the request. However, enabling a deep learning model to exclude specific data used during training is challenging. We cannot easily define the meaning of \"forgetting\" in deep learning and how to achieve it. To address this issue, we propose an efficient machine unlearning architecture to be used for computer vision classification models. Our approach consists of two-stage models, where in the first stage we enables a deep learning model that loses information with contrastive labels in the requested dataset. Second, we retrain the first stage output model with knowledge distillation (KD). Using this two-stage approach, we can substantiate the removal or forgetness of the requested dataset in the deep learning model. With various datasets used for multimedia applications, we demonstrate that our approach achieves performance on par or even higher accuracy than the original model, while effectively removing the requested data.",
    "code_link": ""
  },
  "cvpr2022_hcis_personre-identificationmethodbasedoncolorattackandjointdefence": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "HCIS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Human-Centered Intelligent Services: Safe and Trustworthy",
    "title": "Person Re-Identification Method Based on Color Attack and Joint Defence",
    "authors": [
      "Yunpeng Gong",
      "Liqing Huang",
      "Lifei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Gong_Person_Re-Identification_Method_Based_on_Color_Attack_and_Joint_Defence_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Gong_Person_Re-Identification_Method_Based_on_Color_Attack_and_Joint_Defence_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The main challenges of ReID is the intra-class variations caused by color deviation under different camera conditions. Simultaneously, we find that most of the existing adversarial metric attacks are realized by interfering with the color characteristics of the sample. Based on this observation, we first propose a local transformation attack (LTA) based on color variation. It uses more obvious color variation to randomly disturb the color of the retrieved image, rather than adding random noise. Experiments show that the performance of the proposed LTA method is better than the advanced attack methods. Furthermore, considering that the contour feature is the main factor of the robustness of adversarial training, and the color feature will directly affect the success rate of attack. Therefore, we further propose joint adversarial defense (JAD) method, which includes proactive defense and passive defense. Proactive defense fuse multi-modality images to enhance the contour feature and color feature, and considers local homomorphic transformation to solve the over-fitting problem. Passive defense exploits the invariance of contour feature during image scaling to mitigate the adversarial disturbance on contour feature. Finally, a series of experimental results show that the proposed joint adversarial defense method is more competitive than a state-of-the-art method.",
    "code_link": ""
  },
  "cvpr2022_hcis_pytorch-oodalibraryforout-of-distributiondetectionbasedonpytorch": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "HCIS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Human-Centered Intelligent Services: Safe and Trustworthy",
    "title": "PyTorch-OOD: A Library for Out-of-Distribution Detection Based on PyTorch",
    "authors": [
      "Konstantin Kirchheim",
      "Marco Filax",
      "Frank Ortmeier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kirchheim_PyTorch-OOD_A_Library_for_Out-of-Distribution_Detection_Based_on_PyTorch_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Kirchheim_PyTorch-OOD_A_Library_for_Out-of-Distribution_Detection_Based_on_PyTorch_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Machine Learning models based on Deep Neural Networks behave unpredictably when presented with inputs that do not stem from the training distribution and sometimes make egregiously wrong predictions with high confidence. This property undermines the trustworthiness of systems depending on such models and potentially threatens the safety of their users. Out-of-Distribution (OOD) detection mechanisms can be used to prevent errors by detecting inputs that are so dissimilar from the training set that the model can not be expected to make reliable predictions. In this paper, we present PyTorch-OOD, a Python library for OOD detection based on PyTorch. Its primary goals are to accelerate OOD detection research and improve the reproducibility and comparability of experiments. PyTorch-OOD provides well-tested and documented implementations of OOD detection methods with a unified interface, as well as training and benchmark datasets, architectures, pre-trained models, and utility functions. The library is available online under the permissive Apache 2.0 license and can be installed via Python Package Index (PyPI).",
    "code_link": "https://gitlab.com/kkirchheim/pytorch-ood"
  },
  "cvpr2022_hcis_holisticapproachtomeasuresample-leveladversarialvulnerabilityanditsutilityinbuildingtrustworthysystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "HCIS",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Human-Centered Intelligent Services: Safe and Trustworthy",
    "title": "Holistic Approach To Measure Sample-Level Adversarial Vulnerability and Its Utility in Building Trustworthy Systems",
    "authors": [
      "Gaurav Kumar Nayak",
      "Ruchit Rawal",
      "Rohit Lal",
      "Himanshu Patil",
      "Anirban Chakraborty"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Nayak_Holistic_Approach_To_Measure_Sample-Level_Adversarial_Vulnerability_and_Its_Utility_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Nayak_Holistic_Approach_To_Measure_Sample-Level_Adversarial_Vulnerability_and_Its_Utility_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Adversarial attack perturbs an image with an imperceptible noise, leading to incorrect model prediction. Recently, a few works showed inherent bias associated with such attack (robustness bias), where certain subgroups in a dataset (e.g. based on class, gender, etc.) are less robust than others. This bias not only persists even after adversarial training, but often results in severe performance discrepancies across these subgroups. Existing works characterize the subgroup's robustness bias by only checking individual sample's proximity to the decision boundary. In this work, we argue that this measure alone is not sufficient and validate our argument via extensive experimental analysis. It has been observed that adversarial attacks often corrupt the high-frequency components of the input image. We, therefore, propose a holistic approach for quantifying adversarial vulnerability of a sample by combining these different perspectives, i.e., degree of model's reliance on high-frequency features and the (conventional) sample-distance to the decision boundary. We demonstrate that by reliably estimating adversarial vulnerability at the sample level using the proposed holistic metric, it is possible to develop a trustworthy system where humans can be alerted about the incoming samples that are highly likely to be misclassified at test time. This is achieved with better precision when our holistic metric is used over individual measures. To further corroborate the utility of the proposed holistic approach, we perform knowledge distillation in a limited-sample setting. We observe that the student network trained with the subset of samples selected using our combined metric performs better than both the competing baselines, viz., where samples are selected randomly or based on their distances to the decision boundary.",
    "code_link": ""
  },
  "cvpr2022_wad_towardsrobustsemanticsegmentationofaccidentscenesviamulti-sourcemixedsamplingandmeta-learning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning",
    "authors": [
      "Xinyu Luo",
      "Jiaming Zhang",
      "Kailun Yang",
      "Alina Roitberg",
      "Kunyu Peng",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Luo_Towards_Robust_Semantic_Segmentation_of_Accident_Scenes_via_Multi-Source_Mixed_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Luo_Towards_Robust_Semantic_Segmentation_of_Accident_Scenes_via_Multi-Source_Mixed_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Autonomous vehicles utilize urban scene segmentation to understand the real world like a human and react accordingly. Semantic segmentation of normal scenes has experienced a remarkable rise in accuracy on conventional benchmarks. However, a significant portion of real-life accidents features abnormal scenes, such as those with object deformations, overturns, and unexpected traffic behaviors. Since even small mis-segmentation of driving scenes can lead to serious threats to human lives, the robustness of such models in accident scenarios is an extremely important factor in ensuring safety of intelligent transportation systems. In this paper, we propose a Multi-source Meta-learning Unsupervised Domain Adaptation (MMUDA) framework, to improve the generalization of segmentation transformers to extreme accident scenes. In MMUDA, we make use of Multi-Domain Mixed Sampling to augment the images of multiple-source domains (normal scenes) with the target data appearances (abnormal scenes). To train our model, we intertwine and study a meta-learning strategy in the multi-source setting for robustifying the segmentation results. We further enhance the segmentation backbone (SegFormer) with a HybridASPP decoder design, featuring large window attention spatial pyramid pooling and strip pooling, to efficiently aggregate long-range contextual dependencies. Our approach achieves a mIoU score of 46.97% on the DADA-seg benchmark, surpassing the previous state-of-the-art model by more than 7.50%.",
    "code_link": ""
  },
  "cvpr2022_wad_reconstructfromtopviewa3dlanedetectionapproachbasedongeometrystructureprior": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Reconstruct From Top View: A 3D Lane Detection Approach Based on Geometry Structure Prior",
    "authors": [
      "Chenguang Li",
      "Jia Shi",
      "Ya Wang",
      "Guangliang Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Li_Reconstruct_From_Top_View_A_3D_Lane_Detection_Approach_Based_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Li_Reconstruct_From_Top_View_A_3D_Lane_Detection_Approach_Based_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we propose an advanced approach in targeting the problem of monocular 3D lane detection by leveraging geometry structure underneath the process of 2D to 3D lane reconstruction. Inspired by previous methods, we first analyze the geometry heuristic between the 3D lane and its 2D representation on the ground and propose to impose explicit supervision based on the structure prior, which makes it achievable to build inter-lane and intra-lane relationships to facilitate the reconstruction of 3D lanes from local to global. Second, to reduce the structure loss in 2D lane representation, we directly extract top view lane information from front view images, which tremendously eases the confusion of distant lane features in previous methods. Furthermore, we propose a novel task-specific data augmentation method by synthesizing new training data for both segmentation and reconstruction tasks in our pipeline, to counter the imbalanced data distribution of camera pose and ground slope to improve generalization on unseen data. Our work marks the first attempt to employ the geometry prior information into DNN-based 3D lane detection and makes it achievable for detecting lanes in an extra-long distance, doubling the original detection range. The proposed method can be smoothly adopted by other frameworks without extra costs. Experimental results show that our work outperforms state-of-the-art approaches by 3.8% F-Score on Apollo 3D synthetic dataset at real-time speed of 82 FPS without introducing extra parameters.",
    "code_link": ""
  },
  "cvpr2022_wad_pseudoproprobustpseudo-labelgenerationforsemi-supervisedobjectdetectioninautonomousdrivingsystems": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "PseudoProp: Robust Pseudo-Label Generation for Semi-Supervised Object Detection in Autonomous Driving Systems",
    "authors": [
      "Shu Hu",
      "Chun-Hao Liu",
      "Jayanta Dutta",
      "Ming-Ching Chang",
      "Siwei Lyu",
      "Naveen Ramakrishnan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Hu_PseudoProp_Robust_Pseudo-Label_Generation_for_Semi-Supervised_Object_Detection_in_Autonomous_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Hu_PseudoProp_Robust_Pseudo-Label_Generation_for_Semi-Supervised_Object_Detection_in_Autonomous_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Semi-supervised object detection methods are widely used in autonomous driving systems, where only a fraction of objects are labeled. To propagate information from the labeled objects to the unlabeled ones, pseudo-labels for unlabeled objects must be generated. Although pseudo-labels have proven to improve the performance of semi-supervised object detection significantly, the applications of image-based methods to video frames result in numerous miss or false detections using such generated pseudo-labels. In this paper, we propose a new approach, PseudoProp, to generate robust pseudo-labels by leveraging motion continuity in video frames. Specifically, PseudoProp uses a novel bidirectional pseudo-label propagation approach to compensate for misdetection. A feature-based fusion technique is also used to suppress inference noise. Extensive experiments on the large-scale Cityscapes dataset demonstrate that our method outperforms the state-of-the-art semi-supervised object detection methods by 7.4% on mAP75.",
    "code_link": ""
  },
  "cvpr2022_wad_h-netunsupervisedattention-basedstereodepthestimationleveragingepipolargeometry": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "H-Net: Unsupervised Attention-Based Stereo Depth Estimation Leveraging Epipolar Geometry",
    "authors": [
      "Baoru Huang",
      "Jian-Qing Zheng",
      "Stamatia Giannarou",
      "Daniel S. Elson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Huang_H-Net_Unsupervised_Attention-Based_Stereo_Depth_Estimation_Leveraging_Epipolar_Geometry_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Huang_H-Net_Unsupervised_Attention-Based_Stereo_Depth_Estimation_Leveraging_Epipolar_Geometry_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Depth estimation from a stereo image pair has become one of the most explored applications in computer vision, with most previous methods relying on fully supervised learning settings. However, due to the difficulty in acquiring accurate and scalable ground truth data, the training of fully supervised methods is challenging. As an alternative, self-supervised methods are becoming more popular to mitigate this challenge. In this paper, we introduce the H-Net, a deep-learning framework for unsupervised stereo depth estimation that leverages epipolar geometry to refine stereo matching. For the first time, a Siamese autoencoder architecture is used for depth estimation which allows mutual information between rectified stereo images to be extracted. To enforce the epipolar constraint, the mutual epipolar attention mechanism has been designed which gives more emphasis to correspondences of features that lie on the same epipolar line while learning mutual information between the input stereo pair. Stereo correspondences are further enhanced by incorporating semantic information to the proposed attention mechanism. More specifically, the optimal transport algorithm is used to suppress attention and eliminate outliers in areas not visible in both cameras. Extensive experiments on KITTI2015 and Cityscapes show that the proposed modules are able to improve the performance of the unsupervised stereo depth estimation methods while closing the gap with the fully supervised approaches.",
    "code_link": ""
  },
  "cvpr2022_wad_triplettrack3dobjecttrackingusingtripletembeddingsandlstm": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "TripletTrack: 3D Object Tracking Using Triplet Embeddings and LSTM",
    "authors": [
      "Nicola Marinello",
      "Marc Proesmans",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Marinello_TripletTrack_3D_Object_Tracking_Using_Triplet_Embeddings_and_LSTM_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Marinello_TripletTrack_3D_Object_Tracking_Using_Triplet_Embeddings_and_LSTM_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "3D object tracking is a critical task in autonomous driving systems. It plays an essential role for the system's awareness about the surrounding environment. At the same time there is an increasing interest in algorithms for autonomous cars that solely rely on inexpensive sensors, such as cameras. In this paper we investigate the use of triplet embeddings in combination with motion representations for 3D object tracking. We start from an off-the-shelf 3D object detector, and apply a tracking mechanism where objects are matched by an affinity score computed on local object feature embeddings and motion descriptors. The feature embeddings are trained to include information about the visual appearance and monocular 3D object characteristics, while motion descriptors provide a strong representation of object trajectories. We will show that our approach effectively re-identifies objects, and also behaves reliably and accurately in case of occlusions, missed detections and can detect re-appearance across different field of views. Experimental evaluation shows that our approach outperforms state-of-the-art on nuScenes by a large margin. We also obtain competitive results on KITTI.",
    "code_link": ""
  },
  "cvpr2022_wad_anomalydetectioninautonomousdrivingasurvey": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Anomaly Detection in Autonomous Driving: A Survey",
    "authors": [
      "Daniel Bogdoll",
      "Maximilian Nitsche",
      "J. Marius Z\u00f6llner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Bogdoll_Anomaly_Detection_in_Autonomous_Driving_A_Survey_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Bogdoll_Anomaly_Detection_in_Autonomous_Driving_A_Survey_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Nowadays, there are outstanding strides towards a future with autonomous vehicles on our roads. While the perception of autonomous vehicles performs well under closed-set conditions, they still struggle to handle the unexpected. This survey provides an extensive overview of anomaly detection techniques based on camera, lidar, radar, multimodal and abstract object level data. We provide a systematization including detection approach, corner case level, ability for an online application, and further attributes. We outline the state-of-the-art and point out current research gaps.",
    "code_link": ""
  },
  "cvpr2022_wad_multi-modal3dhumanposeestimationwith2dweaksupervisioninautonomousdriving": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Multi-Modal 3D Human Pose Estimation With 2D Weak Supervision in Autonomous Driving",
    "authors": [
      "Jingxiao Zheng",
      "Xinwei Shi",
      "Alexander Gorban",
      "Junhua Mao",
      "Yang Song",
      "Charles R. Qi",
      "Ting Liu",
      "Visesh Chari",
      "Andre Cornman",
      "Yin Zhou",
      "Congcong Li",
      "Dragomir Anguelov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Zheng_Multi-Modal_3D_Human_Pose_Estimation_With_2D_Weak_Supervision_in_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Zheng_Multi-Modal_3D_Human_Pose_Estimation_With_2D_Weak_Supervision_in_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "3D human pose estimation (3D HPE) in autonomous vehicles (AV) differs from other use cases in many factors, including the 3D resolution and range of data, absence of dense depth maps, failure modes for LiDAR, relative location between the camera and LiDAR, and a high bar for estimation accuracy. Data collected for other use cases (such as virtual reality, gaming, and animation) may therefore not be usable for AV applications. This necessitates the collection and annotation of a large amount of 3D data for HPE in AV, which is time consuming and expensive. In this paper, we propose one of the first approaches to alleviate this problem in the AV setting. Specifically, we propose a multi-modal approach which uses 2D labels on RGB images as weak supervision to perform 3D HPE. The proposed multi-modal architecture incorporates LiDAR and camera inputs with an auxiliary segmentation branch. On the Waymo Open Dataset, we achieve a22% relative improvement over camera-only 2D HPE baseline, and6% improvement over LiDAR-only model. Finally, careful ablation studies and parts based analysis illustrate the advantages of each of our contributions.",
    "code_link": ""
  },
  "cvpr2022_wad_raisingcontextawarenessinmotionforecasting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Raising Context Awareness in Motion Forecasting",
    "authors": [
      "H\u00e9di Ben-Younes",
      "\u00c9loi Zablocki",
      "Micka\u00ebl Chen",
      "Patrick P\u00e9rez",
      "Matthieu Cord"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Ben-Younes_Raising_Context_Awareness_in_Motion_Forecasting_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Ben-Younes_Raising_Context_Awareness_in_Motion_Forecasting_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agent's current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics - dispersion and convergence-to-range - to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark. The code is available at github.com/valeoai/CAB.",
    "code_link": "https://github.com/valeoai/CAB"
  },
  "cvpr2022_wad_carlascenesasyntheticdatasetforodometryinautonomousdriving": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "CarlaScenes: A Synthetic Dataset for Odometry in Autonomous Driving",
    "authors": [
      "Andreas Kloukiniotis",
      "Andreas Papandreou",
      "Christos Anagnostopoulos",
      "Aris Lalos",
      "Petros Kapsalas",
      "Duong-Van Nguyen",
      "Konstantinos Moustakas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Kloukiniotis_CarlaScenes_A_Synthetic_Dataset_for_Odometry_in_Autonomous_Driving_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Kloukiniotis_CarlaScenes_A_Synthetic_Dataset_for_Odometry_in_Autonomous_Driving_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite the great scientific effort to capture adequately the complex environments in which autonomous vehicles (AVs) operate there are still uses-cases that even SoA methods fail to handle. Specifically in odometry problems, on the one hand, geometric solutions operate with certain assumptions that are often breached in AVs, and on the other hand, deep learning methods do not achieve high accuracy. To contribute to that we present CarlaScenes, a large-scale simulation dataset captured using the CARLA simulator. The dataset is oriented to address the challenging odometry scenarios that cause the current state of art odometers to deviate from their normal operations. Based on a case study of failures presented in experiments we distinguished 7 different sequences of data. CarlaScenes besides providing consistent reference poses, includes data with semantic annotation at the instance level for both image and lidar. The full dataset is available at https://github.com/CarlaScenes/CarlaSence.git.",
    "code_link": "https://github.com/CarlaScenes/CarlaSence.git"
  },
  "cvpr2022_wad_k-lanelidarlanedatasetandbenchmarkforurbanroadsandhighways": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways",
    "authors": [
      "Dong-Hee Paek",
      "Seung-Hyung Kong",
      "Kevin Tirta Wijaya"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Paek_K-Lane_Lidar_Lane_Dataset_and_Benchmark_for_Urban_Roads_and_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Paek_K-Lane_Lidar_Lane_Dataset_and_Benchmark_for_Urban_Roads_and_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Lane detection is a critical function for autonomous driving. With the recent development of deep learning and the publication of camera lane datasets and benchmarks, camera lane detection networks (CLDNs) have been remarkably developed. Unfortunately, CLDNs rely on camera images which are often distorted near the vanishing line and prone to poor lighting condition. This is in contrast with Lidar lane detection networks (LLDNs), which can directly extract the lane lines on the bird's eye view (BEV) for motion planning and operate robustly under various lighting conditions. However, LLDNs have not been actively studied, mostly due to the absence of large public lidar lane datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first and the largest public urban road and highway lane dataset for Lidar. K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions, e.g., occluded roads of multiple occlusion levels, roads at day and night times, merging (converging and diverging) and curved lanes. We also provide baseline networks we term Lidar lane detection networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the spatial characteristics of lane lines on the point cloud, which are sparse, thin, and stretched along the entire ground plane of the point cloud. From experimental results, LLDN-GFC achieves the state-of-the-art performance with an F1-score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong performance under various lighting conditions, which is unlike CLDNs, and also robust even in the case of severe occlusions, unlike LLDNs using the conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and complete development kits including evaluation, visualization and annotation tools are available at https://github.com/kaist-avelab/k-lane.",
    "code_link": "https://github.com/kaist-avelab/k-lane"
  },
  "cvpr2022_wad_proposal-freelidarpanopticsegmentationwithpillar-levelaffinity": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Proposal-Free Lidar Panoptic Segmentation With Pillar-Level Affinity",
    "authors": [
      "Qi Chen",
      "Sourabh Vora"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Chen_Proposal-Free_Lidar_Panoptic_Segmentation_With_Pillar-Level_Affinity_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Chen_Proposal-Free_Lidar_Panoptic_Segmentation_With_Pillar-Level_Affinity_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a simple yet effective proposal-free architecture for lidar panoptic segmentation. We jointly optimize both semantic segmentation and class-agnostic instance classification in a single network using a pillar-based bird's-eye view representation. The instance classification head learns pairwise affinity between pillars to determine whether the pillars belong to the same instance or not. We further propose a local clustering algorithm to propagate instance ids by merging semantic segmentation and affinity predictions. Our experiments on nuScenes dataset show that our approach outperforms previous proposal-free method and is comparable to proposal-based method which requires extra annotation from object detection.",
    "code_link": ""
  },
  "cvpr2022_wad_multi-leveldomainadaptationforlanedetection": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Multi-Level Domain Adaptation for Lane Detection",
    "authors": [
      "Chenguang Li",
      "Boheng Zhang",
      "Jia Shi",
      "Guangliang Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Li_Multi-Level_Domain_Adaptation_for_Lane_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Li_Multi-Level_Domain_Adaptation_for_Lane_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We focus on bridging domain discrepancy in lane detection among different scenarios to greatly reduce extra annotation and re-training costs for autonomous driving. Critical factors hinder the performance improvement of cross-domain lane detection that conventional methods only focus on pixel-wise loss while ignoring shape and position priors of lanes. To address the issue, we propose the Multi-level Domain Adaptation (MLDA) framework, a new perspective to handle cross-domain lane detection at three complementary semantic levels of pixel, instance and category. Specifically, at pixel level, we propose to apply cross-class confidence constraints in self-training to tackle the imbalanced confidence distribution of lane and background. At instance level, we go beyond pixels to treat segmented lanes as instances and facilitate discriminative features in target domain with triplet learning, which effectively rebuilds the semantic context of lanes and contributes to alleviating the feature confusion. At category level, we propose an adaptive inter-domain embedding module to utilize the position prior of lanes during adaptation. In two challenging datasets, i.e. TuSimple and CULane, our approach improves lane detection performance by a large margin with gains of 8.8% on accuracy and 7.4% on F1-score respectively, compared with state-of-the-art domain adaptation algorithms.",
    "code_link": ""
  },
  "cvpr2022_wad_trustyourimuconsequencesofignoringtheimudrift": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Trust Your IMU: Consequences of Ignoring the IMU Drift",
    "authors": [
      "Marcus Valtonen \u00d6rnhag",
      "Patrik Persson",
      "M\u00e5rten Wadenb\u00e4ck",
      "Kalle \u00c5str\u00f6m",
      "Anders Heyden"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Ornhag_Trust_Your_IMU_Consequences_of_Ignoring_the_IMU_Drift_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Ornhag_Trust_Your_IMU_Consequences_of_Ignoring_the_IMU_Drift_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we argue that modern pre-integration methods for inertial measurement units (IMUs) are accurate enough to ignore the drift for short time intervals. This allows us to consider a simplified camera model, which in turn admits further intrinsic calibration. We develop the first-ever solver to jointly solve the relative pose problem with unknown and equal focal length and radial distortion profile while utilizing the IMU data. Furthermore, we show significant speed-up compared to state-of-the-art algorithms, with small or negligible loss in accuracy for partially calibrated setups. The proposed algorithms are tested on both synthetic and real data, where the latter is focused on navigation using unmanned aerial vehicles (UAVs). We evaluate the proposed solvers on different commercially available low-cost UAVs, and demonstrate that the novel assumption on IMU drift is feasible in real-life applications. The extended intrinsic auto-calibration enables us to use distorted input images, making tedious calibration processes obsolete, compared to current state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2022_wad_roadsawalarge-scaledatasetforcamera-basedroadsurfaceandwetnessestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "RoadSaW: A Large-Scale Dataset for Camera-Based Road Surface and Wetness Estimation",
    "authors": [
      "Kai Cordes",
      "Christoph Reinders",
      "Paul Hindricks",
      "Jonas Lammers",
      "Bodo Rosenhahn",
      "Hellward Broszio"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Cordes_RoadSaW_A_Large-Scale_Dataset_for_Camera-Based_Road_Surface_and_Wetness_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Cordes_RoadSaW_A_Large-Scale_Dataset_for_Camera-Based_Road_Surface_and_Wetness_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Automated driving is one of the most promising technologies for improving road safety. In real driving scenarios, knowledge about the road friction is crucial. For the estimation of the road friction, two properties are of main interest: the road surface type and the road condition. We propose a novel large-scale dataset to enable camera-based road surface and wetness estimation. It consists of video data captured by in-vehicle cameras and ground truth for the current surface type and wetness which is determined by the MARWIS (Mobile Advanced Road Weather Information Sensor). The wetness measurements are associated to high-resolution bird's eye view road image patches, derived from a calibrated sensor setup. Additionally, data for different distances to the vehicle is provided. The dataset is evaluated with state-of-the-art real-time capable approaches for road condition classification and uncertainty estimation. The results provide a valid baseline, but also demonstrate limitations of the generalization performance. The dataset enables new possibilities for future research on camera-based road friction estimation. It is the first dataset including accurate measurements for the wetness in real driving scenarios.",
    "code_link": ""
  },
  "cvpr2022_wad_pointmotionnetpoint-wisemotionlearningforlarge-scalelidarpointcloudssequences": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "PointMotionNet: Point-Wise Motion Learning for Large-Scale LiDAR Point Clouds Sequences",
    "authors": [
      "Jun Wang",
      "Xiaolong Li",
      "Alan Sullivan",
      "Lynn Abbott",
      "Siheng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We propose a point-based spatiotemporal pyramid architecture, called PointMotionNet, to learn motion information from a sequence of large-scale 3D LiDAR point clouds. A core component of PointMotionNet is a novel technique for point-based spatiotemporal convolution, which finds the point correspondences across time by leveraging a time-invariant spatial neighboring space and extracts spatiotemporal features. To validate PointMotionNet, we consider two motion-related tasks: point-based motion prediction and multisweep semantic segmentation. For each task, we design an end-to-end system where PointMotionNet is the core module that learns motion information. We conduct extensive experiments and show that i) for point-based motion prediction, PointMotionNet achieves less than 0.5m mean squared error on Argoverse dataset, which is a significant improvement over existing methods; and ii) for multisweep semantic segmentation, PointMotionNet with a pretrained segmentation backbone outperforms previous SOTA by over 3.3% mIoU on SemanticKITTI dataset with 25 classes including 6 moving objects.",
    "code_link": ""
  },
  "cvpr2022_wad_performancepredictionforsemanticsegmentationbyaself-supervisedimagereconstructiondecoder": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Performance Prediction for Semantic Segmentation by a Self-Supervised Image Reconstruction Decoder",
    "authors": [
      "Andreas B\u00e4r",
      "Marvin Klingner",
      "Jonas L\u00f6hdefink",
      "Fabian H\u00fcger",
      "Peter Schlicht",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Bar_Performance_Prediction_for_Semantic_Segmentation_by_a_Self-Supervised_Image_Reconstruction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Bar_Performance_Prediction_for_Semantic_Segmentation_by_a_Self-Supervised_Image_Reconstruction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In supervised learning, a deep neural network's performance is measured using ground truth data. In semantic segmentation, ground truth data is sparse, requires an expensive annotation process, and, most importantly, it is not available during online operation. To tackle this problem, recent works propose various forms of performance prediction. However, they either rely on inference data histograms, additional sensors, or additional training data. In this paper, we propose a novel per-image performance prediction for semantic segmentation, with (i) no need for additional sensors (sensor efficiency), (ii) no need for additional training data (data efficiency), and (iii) no need for a dedicated retraining of the semantic segmentation (training efficiency). Specifically, we extend an already trained semantic segmentation network having fixed parameters with an image reconstruction decoder. After training and a subsequent regression, the image reconstruction quality is evaluated to predict the semantic segmentation performance. We demonstrate our method's effectiveness with a new state-of-the-art benchmark both on KITTI and Cityscapes for image-only input methods, on Cityscapes even excelling a LiDAR-supported benchmark.",
    "code_link": ""
  },
  "cvpr2022_wad_mutr3damulti-cameratrackingframeworkvia3d-to-2dqueries": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "MUTR3D: A Multi-Camera Tracking Framework via 3D-to-2D Queries",
    "authors": [
      "Tianyuan Zhang",
      "Xuanyao Chen",
      "Yue Wang",
      "Yilun Wang",
      "Hang Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Zhang_MUTR3D_A_Multi-Camera_Tracking_Framework_via_3D-to-2D_Queries_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Zhang_MUTR3D_A_Multi-Camera_Tracking_Framework_via_3D-to-2D_Queries_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Accurate and consistent 3D tracking from multiple cameras is a key component in a vision-based autonomous driving system. It involves modeling 3D dynamic objects in complex scenes across multiple cameras. This problem is inherently challenging due to depth estimation, visual occlusions, appearance ambiguity, etc. Moreover, objects are not consistently associated across time and cameras. To address that, we propose an end-to-end MUlti-camera TRacking framework called MUTR3D. In contrast to prior works, MUTR3D does not explicitly rely on the spatial and appearance similarity of objects. Instead, our method introduces 3D track query to model spatial and appearance coherent track for each object that appears in multiple cameras and multiple frames. We use camera transformations to link 3D trackers with their observations in 2D images. Each tracker is further refined according to the features that are obtained from camera images. MUTR3D uses a set-to-set loss to measure the difference between the predicted tracking results and the ground truths. Therefore, it does not require any post-processing such as non-maximum suppression and/or bounding box association. MUTR3D outperforms state-of-the-art methods by 5.3 AMOTA on the nuScenes dataset. Code will be released.",
    "code_link": ""
  },
  "cvpr2022_wad_scenerepresentationinbirds-eyeviewfromsurroundingcameraswithtransformers": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Autonomous Driving",
    "title": "Scene Representation in Bird's-Eye View From Surrounding Cameras With Transformers",
    "authors": [
      "Yun Zhao",
      "Yu Zhang",
      "Zhan Gong",
      "Hong Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Zhao_Scene_Representation_in_Birds-Eye_View_From_Surrounding_Cameras_With_Transformers_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Zhao_Scene_Representation_in_Birds-Eye_View_From_Surrounding_Cameras_With_Transformers_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Scene representation in the bird's-eye-view (BEV) coordinate frame provides a succinct and effective way to understand surrounding environments for autonomous vehicles and robotics. In this work, we present an end-to-end architecture to generate the BEV representation from surrounding cameras. To generate the BEV representation, we propose a transformer-based encoder-decoder structure to translate the image features from different cameras into the BEV frame, which takes advantage of the context information in the individual image and the relationship between images in different views. We perform multiple semantic segmentation tasks using the BEV features. Experimental results show that our model outperforms the competitive baseline, which demonstrates the effectiveness and efficiency of our method.",
    "code_link": ""
  },
  "cvpr2022_mula_learningtoaskinformativesub-questionsforvisualquestionanswering": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Learning To Ask Informative Sub-Questions for Visual Question Answering",
    "authors": [
      "Kohei Uehara",
      "Nan Duan",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Uehara_Learning_To_Ask_Informative_Sub-Questions_for_Visual_Question_Answering_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Uehara_Learning_To_Ask_Informative_Sub-Questions_for_Visual_Question_Answering_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "VQA (Visual Question Answering) model tends to make incorrect inferences for questions that require reasoning over world knowledge. Recent study has shown that training VQA models with questions that provide lower-level perceptual information along with reasoning questions improves performance. Inspired by this, we propose a novel VQA model that generates questions to actively obtain auxiliary perceptual information useful for correct reasoning. Our model consists of a VQA model for answering questions, a Visual Question Generation (VQG) model for generating questions, and an Info-score model for estimating the amount of information the generated questions contain, which is useful in answering the original question. We train the VQG model to maximize the \"informativeness\" provided by the Info-score model to generate questions that contain as much information as possible, about the answer to the original question. Our experiments show that by inputting the generated questions and their answers as additional information to the VQA model, it can indeed predict the answer more correctly than the baseline model.",
    "code_link": ""
  },
  "cvpr2022_mula_modulatingbottom-upandtop-downvisualprocessingvialanguage-conditionalfilters": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters",
    "authors": [
      "Ilker Kesen",
      "Ozan Arkan Can",
      "Erkut Erdem",
      "Aykut Erdem",
      "Deniz Y\u00fcret"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Kesen_Modulating_Bottom-Up_and_Top-Down_Visual_Processing_via_Language-Conditional_Filters_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Kesen_Modulating_Bottom-Up_and_Top-Down_Visual_Processing_via_Language-Conditional_Filters_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a top-down manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts. Code is available at https://github.com/ilkerkesen/bvpr.",
    "code_link": "https://github.com/ilkerkesen/bvpr"
  },
  "cvpr2022_mula_reasoningwithmulti-structurecommonsenseknowledgeinvisualdialog": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Reasoning With Multi-Structure Commonsense Knowledge in Visual Dialog",
    "authors": [
      "Shunyu Zhang",
      "Xiaoze Jiang",
      "Zequn Yang",
      "Tao Wan",
      "Zengchang Qin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Zhang_Reasoning_With_Multi-Structure_Commonsense_Knowledge_in_Visual_Dialog_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Zhang_Reasoning_With_Multi-Structure_Commonsense_Knowledge_in_Visual_Dialog_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Visual Dialog requires an agent to engage in a conversation with humans grounded in an image. Many studies on Visual Dialog focus on the understanding of the dialog history or the content of image, while a considerable amount of commonsense-required questions are ignored. Handling these scenarios depends on logical reasoning that requires commonsense priors. How to capture relevant commonsense knowledge complementary to the history and the image remains a key challenge. In this paper, we propose a novel model by Reasoning with Multi-structure Commonsense Knowledge (RMK). In our model, the external knowledge is represented with sentence-level facts and graph-level facts, to properly suit the scenario of the composite of dialog history and image. On top of these multi-structure representations, our model can capture relevant knowledge and incorporate them into the vision and semantic features, via graph-based interaction and transformer-based fusion. Experimental results and analysis on VisDial v1.0 and VisDialCK datasets show that our proposed model effectively outperforms comparative methods.",
    "code_link": ""
  },
  "cvpr2022_mula_emphasizingcomplementarysamplesfornon-literalcross-modalretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Emphasizing Complementary Samples for Non-Literal Cross-Modal Retrieval",
    "authors": [
      "Christopher Thomas",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Thomas_Emphasizing_Complementary_Samples_for_Non-Literal_Cross-Modal_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Thomas_Emphasizing_Complementary_Samples_for_Non-Literal_Cross-Modal_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Existing cross-modal retrieval methods assume a straightforward relationship where images and text contain portrayals or mentions of the same objects. In contrast, real-world image-text pairs (e.g. an image and its caption in a news article) often feature more complex relations. Importantly, not all image-text pairs have the same relationship: in some pairs, image and text may be more closely aligned, while others are more loosely aligned hence complementary. In order to ensure the model learns a semantically robust space which captures nuanced relationships, care must be taken that loosely-aligned image-text pairs have a strong enough impact on learning. In this paper, we propose a novel approach to prioritize loosely-aligned samples. Unlike prior sample weighting methods, ours relies on estimating to what extent semantic similarity is preserved in the separate channels (images/text) in the learned multimodal space. In particular, the image-text pair weights in the retrieval loss focus learning towards samples from diverse or discrepant neighborhoods: samples where images or text that were close in a semantic space, are distant in the cross-modal space (diversity), or where neighbor relations are asymmetric (discrepancy). Experiments on three challenging datasets exhibiting abstract image-text relations, as well as COCO, demonstrate significant performance gains compared to recent state-of-the-art models and sample weighting approaches.",
    "code_link": ""
  },
  "cvpr2022_mula_transformerdecoderswithmultimodalregularizationforcross-modalfoodretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Transformer Decoders With MultiModal Regularization for Cross-Modal Food Retrieval",
    "authors": [
      "Mustafa Shukor",
      "Guillaume Couairon",
      "Asya Grechka",
      "Matthieu Cord"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Shukor_Transformer_Decoders_With_MultiModal_Regularization_for_Cross-Modal_Food_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Shukor_Transformer_Decoders_With_MultiModal_Regularization_for_Cross-Modal_Food_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Cross-modal image-recipe retrieval has gained significant attention in recent years. Most work focuses on improving cross-modal embeddings using unimodal encoders, that allow for efficient retrieval in large-scale databases, leaving aside cross-attention between modalities which is more computationally expensive. We propose a new retrieval framework, T-Food Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval) that exploits the interaction between modalities in a novel regularization scheme, while using only unimodal encoders at test time for efficient retrieval. We also capture the intra-dependencies between recipe entities with a dedicated recipe encoder, and propose new variants of triplet losses with dynamic margins that adapt to the difficulty of the task. Finally, we leverage the power of the recent Vision and Language Pretraining (VLP) models such as CLIP for the image encoder. Our approach outperforms existing approaches by a large margin on the Recipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6 R@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code is available here:https://github.com/mshukor/TFood.",
    "code_link": "https://github.com/mshukor/TFood"
  },
  "cvpr2022_mula_doublingdownsparsegroundingwithanadditional,almost-matchingcaptionfordetection-orientedmultimodalpretraining": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Doubling Down: Sparse Grounding With an Additional, Almost-Matching Caption for Detection-Oriented Multimodal Pretraining",
    "authors": [
      "Giacomo Nebbia",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Nebbia_Doubling_Down_Sparse_Grounding_With_an_Additional_Almost-Matching_Caption_for_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Nebbia_Doubling_Down_Sparse_Grounding_With_an_Additional_Almost-Matching_Caption_for_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "A common paradigm in deep learning applications for computer vision is self-supervised pretraining followed by supervised fine-tuning on a target task. In the self-supervision step, a model is trained in a supervised fashion, but the source of supervision needs to be implicitly defined by the data. Image-caption alignment is often used as such a source of implicit supervision in multimodal pretraining, and grounding (i.e., matching word tokens with visual tokens) is one way to exploit it. We introduce a strategy to take advantage of an underexplored structure in image-caption datasets: the relationship between captions matched with different images but mentioning the same objects. Given an image-caption pair, we find an additional caption that mentions one of the objects the first caption mentions, and we impose a sparse grounding between the image and the second caption so that only a few word tokens are grounded with the image. Our goal is to learn a better feature representation for the objects mentioned by both captions, encouraging grounding between the additional caption and the image to focus on the common objects only. We report superior grounding performance when comparing our approach with a previously-published pretraining strategy, and we show the benefit of our proposed double-caption grounding on two downstream detection tasks: supervised detection and open-vocabulary detection.",
    "code_link": "https://github.com/facebookresearch/maskrcnnbenchmark"
  },
  "cvpr2022_mula_improvingmultimodalspeechrecognitionbydataaugmentationandspeechrepresentations": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Improving Multimodal Speech Recognition by Data Augmentation and Speech Representations",
    "authors": [
      "Dan Onea\u021b\u0103",
      "Horia Cucu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Oneata_Improving_Multimodal_Speech_Recognition_by_Data_Augmentation_and_Speech_Representations_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Oneata_Improving_Multimodal_Speech_Recognition_by_Data_Augmentation_and_Speech_Representations_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multimodal speech recognition aims to improve the performance of automatic speech recognition (ASR) systems by leveraging additional visual information that is usually associated to the audio input. While previous approaches make crucial use of strong visual representations, e.g. by finetuning pretrained image recognition networks, significantly less attention has been paid to its counterpart: the speech component. In this work, we investigate ways of improving the base speech recognition system by following similar techniques to the ones used for the visual encoder, namely, transferring representations and data augmentation. First, we show that starting from a pretrained ASR significantly improves the state-of-the-art performance; remarkably, even when building upon a strong unimodal system, we still find gains by including the visual modality. Second, we employ speech data augmentation techniques to encourage the multimodal system to attend to the visual stimuli. This technique replaces previously used word masking and comes with the benefits of being conceptually simpler and yielding consistent improvements in the multimodal setting. We provide empirical results on three multimodal datasets, including the newly introduced Localized Narratives.",
    "code_link": ""
  },
  "cvpr2022_mula_theunreasonableeffectivenessofclipfeaturesforimagecaptioninganexperimentalanalysis": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "The Unreasonable Effectiveness of CLIP Features for Image Captioning: An Experimental Analysis",
    "authors": [
      "Manuele Barraco",
      "Marcella Cornia",
      "Silvia Cascianelli",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Barraco_The_Unreasonable_Effectiveness_of_CLIP_Features_for_Image_Captioning_An_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Barraco_The_Unreasonable_Effectiveness_of_CLIP_Features_for_Image_Captioning_An_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Generating textual descriptions from visual inputs is a fundamental step towards machine intelligence, as it entails modeling the connections between the visual and textual modalities. For years, image captioning models have relied on pre-trained visual encoders and object detectors, trained on relatively small sets of data. Recently, it has been observed that large-scale multi-modal approaches like CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, provide a strong zero-shot capability on various vision tasks. In this paper, we study the advantage brought by CLIP in image captioning, employing it as a visual encoder. Through extensive experiments, we show how CLIP can significantly outperform widely-used visual encoders and quantify its role under different architectures, variants, and evaluation protocols, ranging from classical captioning performance to zero-shot transfer.",
    "code_link": ""
  },
  "cvpr2022_mula_m2fnetmulti-modalfusionnetworkforemotionrecognitioninconversation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "M2FNet: Multi-Modal Fusion Network for Emotion Recognition in Conversation",
    "authors": [
      "Vishal Chudasama",
      "Purbayan Kar",
      "Ashish Gudmalwar",
      "Nirmesh Shah",
      "Pankaj Wasnik",
      "Naoyuki Onoe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Chudasama_M2FNet_Multi-Modal_Fusion_Network_for_Emotion_Recognition_in_Conversation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Chudasama_M2FNet_Multi-Modal_Fusion_Network_for_Emotion_Recognition_in_Conversation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Emotion Recognition in Conversations (ERC) is crucial in developing sympathetic human-machine interaction. In conversational videos, emotion can be present in multiple modalities, i.e., audio, video, and transcript. However, due to the inherent characteristics of these modalities, multi-modal ERC has always been considered a challenging undertaking. Existing ERC research focuses mainly on using text information in a discussion, ignoring the other two modalities. We anticipate that emotion recognition accuracy can be improved by employing a multi-modal approach. Thus, in this study, we propose a Multi-modal Fusion Network (M2FNet) that extracts emotion-relevant features from visual, audio, and text modality. It employs a multi-head attention-based fusion mechanism to combine emotion-rich latent representations of the input data. We introduce a new feature extractor to extract latent features from the audio and visual modality. The proposed feature extractor is trained with a novel adaptive margin-based triplet loss function to learn emotion-relevant features from the audio and visual data. In the domain of ERC, the existing methods perform well on one benchmark dataset but not on others. Our results show that the proposed M2FNet architecture outperforms all other methods in terms of weighted average F1 score on well-known MELD and IEMOCAP datasets and sets a new state-of-the-art performance in ERC.",
    "code_link": ""
  },
  "cvpr2022_mula_semanticallygroundedvisualembeddingsforzero-shotlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Semantically Grounded Visual Embeddings for Zero-Shot Learning",
    "authors": [
      "Shah Nawaz",
      "Jacopo Cavazza",
      "Alessio Del Bue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Nawaz_Semantically_Grounded_Visual_Embeddings_for_Zero-Shot_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Nawaz_Semantically_Grounded_Visual_Embeddings_for_Zero-Shot_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Zero-shot learning methods rely on fixed visual and semantic embeddings, extracted from independent vision and language models, both pre-trained for other large-scale tasks. This is a weakness of current zero-shot learning frameworks as such disjoint embeddings fail to adequately associate visual and textual information to their shared semantic content. Therefore, we propose to learn semantically grounded and enriched visual information by computing a joint image and text model with a two-stream network on a proxy task. To improve this alignment between image and textual representations, provided by attributes, we leverage ancillary captions to provide grounded semantic information. Our method, dubbed joint embeddings for zero-shot learning is evaluated on several benchmark datasets, improving the performance of existing state-of-the-art methods in both standard (+1.6% on aPY, +2.6% on FLO) and generalized (+2.1% on AWA2, +2.2% on CUB) zero-shot recognition.",
    "code_link": ""
  },
  "cvpr2022_mula_coarse-to-finereasoningforvisualquestionanswering": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Coarse-To-Fine Reasoning for Visual Question Answering",
    "authors": [
      "Binh X. Nguyen",
      "Tuong Do",
      "Huy Tran",
      "Erman Tjiputra",
      "Quang D. Tran",
      "Anh Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Nguyen_Coarse-To-Fine_Reasoning_for_Visual_Question_Answering_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Nguyen_Coarse-To-Fine_Reasoning_for_Visual_Question_Answering_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Bridging the semantic gap between image and question is an important step to improve the accuracy of the Visual Question Answering (VQA) task. However, most of the existing VQA methods focus on attention mechanisms or visual relations for reasoning the answer, while the features at different semantic levels are not fully utilized. In this paper, we present a new reasoning framework to fill the gap between visual features and semantic clues in the VQA task. Our method first extracts the features and predicates from the image and question. We then propose a new reasoning framework to effectively jointly learn these features and predicates in a coarse-to-fine manner. The intensively experimental results on three large-scale VQA datasets show that our proposed approach achieves superior accuracy comparing with other state-of-the-art methods. Furthermore, our reasoning framework also provides an explainable way to understand the decision of the deep neural network when predicting the answer.",
    "code_link": "https://github.com/aiozai/CFR_VQA"
  },
  "cvpr2022_mula_cascadedsiameseself-supervisedaudiotovideogan": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Cascaded Siamese Self-Supervised Audio to Video GAN",
    "authors": [
      "Nuha Aldausari",
      "Arcot Sowmya",
      "Nadine Marcus",
      "Gelareh Mohammadi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Aldausari_Cascaded_Siamese_Self-Supervised_Audio_to_Video_GAN_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Aldausari_Cascaded_Siamese_Self-Supervised_Audio_to_Video_GAN_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Generating meaningful videos that are synchronised to audio signals is a complex synthesis task that requires generation of not only realistic videos but also coherent video motions that conform to the provided audio signals. While tremendous effort has been expended on audio-to-video generative models, these models rely heavily on supervised signals such as face/body key points or 3D meshes. However, key point annotation requires time and effort. Besides, some dataset domains do not have predictable structure, which makes the extraction of points of interest infeasible. Our proposed model consists of a cascaded generator-discriminator architecture that works at the pixel level to generate videos according to the associated soundtracks. It adopts a new self-supervised temporal augmentation technique to optimise the correlation between the audio signal and the generated video instead of relying on supervised signals. The proposed architecture has proven its effectiveness in extensive experiments that compared different models across two datasets.",
    "code_link": ""
  },
  "cvpr2022_mula_probabilisticcompositionalembeddingsformultimodalimageretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Probabilistic Compositional Embeddings for Multimodal Image Retrieval",
    "authors": [
      "Andrei Neculai",
      "Yanbei Chen",
      "Zeynep Akata"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Neculai_Probabilistic_Compositional_Embeddings_for_Multimodal_Image_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Neculai_Probabilistic_Compositional_Embeddings_for_Multimodal_Image_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Existing works in image retrieval often consider retrieving images with one or two query inputs, which do not generalize to multiple queries. In this work, we investigate a more challenging scenario for composing multiple multimodal queries in image retrieval. Given an arbitrary number of query images and (or) texts, our goal is to retrieve target images containing the semantic concepts specified in multiple multimodal queries. To learn an informative embedding that can flexibly encode the semantics of various queries, we propose a novel multimodal probabilistic composer (MPC). Specifically, we model input images and texts as probabilistic embeddings, which can be further composed by a probabilistic composition rule to facilitate image retrieval with multiple multimodal queries. We propose a new benchmark based on the MS-COCO dataset and evaluate our model on various setups that compose multiple images and (or) text queries for multimodal image retrieval. Without bells and whistles, we show that our probabilistic model formulation significantly outperforms existing related methods on multimodal image retrieval while generalizing well to query with different amounts of inputs given in arbitrary visual and (or) textual modalities.",
    "code_link": "https://github.com/andreineculai/MPC"
  },
  "cvpr2022_mula_guidingattentionusingpartial-orderrelationshipsforimagecaptioning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Guiding Attention Using Partial-Order Relationships for Image Captioning",
    "authors": [
      "Murad Popattia",
      "Muhammad Rafi",
      "Rizwan Qureshi",
      "Shah Nawaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Popattia_Guiding_Attention_Using_Partial-Order_Relationships_for_Image_Captioning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Popattia_Guiding_Attention_Using_Partial-Order_Relationships_for_Image_Captioning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The use of attention models for automated image captioning has enabled many systems to produce accurate and meaningful descriptions for images. Over the years, many novel approaches have been proposed to enhance the attention process using different feature representations. In this paper, we extend this approach by creating a guided attention network mechanism, that exploits the relationship between the visual scene and text-descriptions using spatial features from the image, high-level information from the topics, and temporal context from caption generation, which are embedded together in an ordered embedding space. A pairwise ranking objective is used for training this embedding space which allows similar images, topics and captions in the shared semantic space to maintain a partial order in the visual-semantic hierarchy and hence, helps the model to produce more visually accurate captions. The experimental results based on MSCOCO dataset shows the competitiveness of our approach, with many state-of-the-art models on various evaluation metrics.",
    "code_link": "https://github.com/tylin/coco-caption"
  },
  "cvpr2022_mula_couplingvisionandproprioceptionfornavigationofleggedrobots": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Coupling Vision and Proprioception for Navigation of Legged Robots",
    "authors": [
      "Zipeng Fu",
      "Ashish Kumar",
      "Ananye Agarwal",
      "Haozhi Qi",
      "Jitendra Malik",
      "Deepak Pathak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Fu_Coupling_Vision_and_Proprioception_for_Navigation_of_Legged_Robots_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Fu_Coupling_Vision_and_Proprioception_for_Navigation_of_Legged_Robots_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We exploit the complementary strengths of vision and proprioception to achieve point goal navigation in a legged robot. Legged systems are capable of traversing more complex terrain than wheeled robots, but to fully exploit this capability, we need the high-level path planner in the navigation system to be aware of the walking capabilities of the low-level locomotion policy on varying terrains. We achieve this by using proprioceptive feedback to estimate the safe operating limits of the walking policy, and to sense unexpected obstacles and terrain properties like smoothness or softness of the ground that may be missed by vision. The navigation system uses onboard cameras to generate an occupancy map and a corresponding cost map to reach the goal. The FMM (Fast Marching Method) planner then generates a target path. The velocity command generator takes this as input to generate the desired velocity for the locomotion policy using as input additional constraints, from the safety advisor, of unexpected obstacles and terrain determined speed limits. We show superior performance compared to wheeled robot (LoCoBot) baselines, and other baselines which have disjoint high-level planning and low-level control. We also show the real-world deployment of our system on a quadruped robot with onboard sensors and compute. Videos at https://navigation-locomotion.github.io.",
    "code_link": ""
  },
  "cvpr2022_mula_multi-viewmulti-labelcanonicalcorrelationanalysisforcross-modalmatchingandretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Multimodal Learning and Applications",
    "title": "Multi-View Multi-Label Canonical Correlation Analysis for Cross-Modal Matching and Retrieval",
    "authors": [
      "Rushil Sanghavi",
      "Yashaswi Verma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Sanghavi_Multi-View_Multi-Label_Canonical_Correlation_Analysis_for_Cross-Modal_Matching_and_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Sanghavi_Multi-View_Multi-Label_Canonical_Correlation_Analysis_for_Cross-Modal_Matching_and_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we address the problem of cross-modal retrieval in presence of multi-view and multi-label data. For this, we present Multi-view Multi-label Canonical Correlation Analysis (or MVMLCCA), which is a generalization of CCA for multi-view data that also makes use of high-level semantic information available in the form of multi-label annotations in each view. While CCA relies on explicit pairings/associations of samples between two views (or modalities), MVMLCCA uses the available multi-label annotations to establish correspondence across multiple (two or more) views without the need of explicit pairing of multi-view samples. Extensive experiments on two multi-modal datasets demonstrate that the proposed approach offers much more flexibility than the related approaches without compromising on scalability and cross-modal retrieval performance. Our code and precomputed features are available at https://github.com/Rushil231100/MVMLCCA.",
    "code_link": "https://github.com/Rushil231100/MVMLCCA"
  },
  "cvpr2022_vdu_onthechoiceofdataforefficienttrainingandvalidationofend-to-enddrivingmodels": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "On the Choice of Data for Efficient Training and Validation of End-to-End Driving Models",
    "authors": [
      "Marvin Klingner",
      "Konstantin M\u00fcller",
      "Mona Mirzaie",
      "Jasmin Breitenstein",
      "Jan-Aike Term\u00f6hlen",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Klingner_On_the_Choice_of_Data_for_Efficient_Training_and_Validation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Klingner_On_the_Choice_of_Data_for_Efficient_Training_and_Validation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The emergence of data-driven machine learning (ML) has facilitated significant progress in many complicated tasks such as highly-automated driving. While much effort is put into improving the ML models and learning algorithms in such applications, little focus is put into how the training data and/or validation setting should be designed. In this paper we investigate the influence of several data design choices regarding training and validation of deep driving models trainable in an end-to-end fashion. Specifically, (i) we investigate how the amount of training data influences the final driving performance, and which performance limitations are induced through currently used mechanisms to generate training data. (ii) Further, we show by correlation analysis, which validation design enables the driving performance measured during validation to generalize well to unknown test environments. (iii) Finally, we investigate the effect of random seeding and non-determinism, giving insights which reported improvements can be deemed significant. Our evaluations using the popular CARLA simulator provide recommendations regarding data generation and driving route selection for an efficient future development of end-to-end driving models.",
    "code_link": ""
  },
  "cvpr2022_vdu_delvingintohigh-qualitysyntheticfaceocclusionsegmentationdatasets": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Delving Into High-Quality Synthetic Face Occlusion Segmentation Datasets",
    "authors": [
      "Kenny T. R. Voo",
      "Liming Jiang",
      "Chen Change Loy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Voo_Delving_Into_High-Quality_Synthetic_Face_Occlusion_Segmentation_Datasets_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Voo_Delving_Into_High-Quality_Synthetic_Face_Occlusion_Segmentation_Datasets_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper performs comprehensive analysis on datasets for occlusion-aware face segmentation, a task that is crucial for many downstream applications. The collection and annotation of such datasets are time-consuming and labor-intensive. Although some efforts have been made in synthetic data generation, the naturalistic aspect of data remains less explored. In our study, we propose two occlusion generation techniques, Naturalistic Occlusion Generation (NatOcc), for producing high-quality naturalistic synthetic occluded faces; and Random Occlusion Generation (RandOcc), a more general synthetic occluded data generation method. We empirically show the effectiveness and robustness of both methods, even for unseen occlusions. To facilitate model evaluation, we present two high-resolution real-world occluded face datasets with fine-grained annotations, RealOcc and RealOcc-Wild, featuring both careful alignment preprocessing and an in-the-wild setting for robustness test. We further conduct a comprehensive analysis on a newly introduced segmentation benchmark, offering insights for future exploration.",
    "code_link": "https://github.com/kennyvoo/faceocclusion-generation"
  },
  "cvpr2022_vdu_whatsinacaption?dataset-specificlinguisticdiversityanditseffectonvisualdescriptionmodelsandmetrics": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "What's in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics",
    "authors": [
      "David M. Chan",
      "Austin Myers",
      "Sudheendra Vijayanarasimhan",
      "David A. Ross",
      "Bryan Seybold",
      "John F. Canny"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Chan_Whats_in_a_Caption_Dataset-Specific_Linguistic_Diversity_and_Its_Effect_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Chan_Whats_in_a_Caption_Dataset-Specific_Linguistic_Diversity_and_Its_Effect_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "While there have been significant gains in the field of automated video description, the generalization performance of automated description models to novel domains remains a major barrier to using these systems in the real world. Most visual description methods are known to capture and exploit patterns in the training data leading to evaluation metric increases, but what are those patterns? In this work, we examine several popular visual description datasets, and capture, analyze, and understand the dataset-specific linguistic patterns that models exploit but do not generalize to new domains. At the token level, sample level, and dataset level, we find that caption diversity is a major driving factor behind the generation of generic and uninformative captions. We further show that state-of-the-art models even outperform held-out ground truth captions on modern metrics, and that this effect is an artifact of linguistic diversity in datasets. Understanding this linguistic diversity is key to building strong captioning models, we recommend several methods and approaches for maintaining diversity in the collection of new data, and dealing with the consequences of limited diversity when using current models and metrics.",
    "code_link": "https://github.com/CannyLab/vdtk"
  },
  "cvpr2022_vdu_self-supervisionversussyntheticdatasetswhichisthelesserevilinthecontextofvideodenoising?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Self-Supervision Versus Synthetic Datasets: Which Is the Lesser Evil in the Context of Video Denoising?",
    "authors": [
      "Val\u00e9ry Dewil",
      "Arnaud Barral",
      "Gabriele Facciolo",
      "Pablo Arias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Dewil_Self-Supervision_Versus_Synthetic_Datasets_Which_Is_the_Lesser_Evil_in_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Dewil_Self-Supervision_Versus_Synthetic_Datasets_Which_Is_the_Lesser_Evil_in_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Supervised training has led to state-of-the-art results in image and video denoising. However, its application to real data is limited since it requires large datasets of noisy-clean pairs that are difficult to obtain. For this reason, networks are often trained on realistic synthetic data. More recently, some self-supervised frameworks have been proposed for training such denoising networks directly on the noisy data without requiring ground truth. On synthetic denoising problems supervised training outperforms self-supervised approaches, however in recent years the gap has become narrower, especially for video. In this paper, we propose a study aiming to determine which is the best approach to train denoising networks for real raw videos: supervision on synthetic realistic data or self-supervision on real data. A complete study with quantitative results in case of natural videos with real motion is impossible since no dataset with clean-noisy pairs exists. We address this issue by considering three independent experiments in which we compare the two frameworks. We found that self-supervision on the real data outperforms supervision on synthetic data, and that in normal illumination conditions the drop in performance is due to the synthetic ground truth generation, not the noise model.",
    "code_link": ""
  },
  "cvpr2022_vdu_videoactiondetectionanalysinglimitationsandchallenges": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Video Action Detection: Analysing Limitations and Challenges",
    "authors": [
      "Rajat Modi",
      "Aayush Jung Rana",
      "Akash Kumar",
      "Praveen Tirupattur",
      "Shruti Vyas",
      "Yogesh Rawat",
      "Mubarak Shah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Modi_Video_Action_Detection_Analysing_Limitations_and_Challenges_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Modi_Video_Action_Detection_Analysing_Limitations_and_Challenges_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Beyond possessing large enough size to feed data hungry machines (eg, transformers), what attributes measure the quality of a dataset? Assuming that the definitions of such attributes do exist, how do we quantify among their relative existences? Our work attempts to explore these questions for video action detection. The task aims to spatio-temporally localize an actor and assign a relevant action class. We first analyze the existing datasets on video action detection and discuss their limitations. Next, we propose a new dataset, Multi Actor Multi Action (MAMA) which overcomes these limitations and is more suitable for real world applications. In addition, we perform a biasness study which analyzes a key property differentiating videos from static images: the temporal aspect. This reveals if the actions in these datasets really need the motion information of an actor, or whether they predict the occurrence of an action even by looking at a single frame. Finally, we investigate the widely held assumptions on the importance of temporal ordering: is temporal ordering important for detecting these actions? Such extreme experiments show existence of biases which have managed to creep into existing methods inspite of careful modeling.",
    "code_link": ""
  },
  "cvpr2022_vdu_deeppicdeepperceptualimageclusteringforidentifyingbiasinvisiondatasets": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "deepPIC: Deep Perceptual Image Clustering for Identifying Bias in Vision Datasets",
    "authors": [
      "Nikita Jaipuria",
      "Katherine Stevo",
      "Xianling Zhang",
      "Meghana L. Gaopande",
      "Ian Calle",
      "Jinesh Jain",
      "Vidya N. Murali"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Jaipuria_deepPIC_Deep_Perceptual_Image_Clustering_for_Identifying_Bias_in_Vision_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Jaipuria_deepPIC_Deep_Perceptual_Image_Clustering_for_Identifying_Bias_in_Vision_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Dataset bias in manually collected datasets is a known problem in computer vision. In safety-critical applications such as autonomous driving, these biases can lead to catastrophic errors from models trained on such datasets, jeopardizing the safety of users and their surroundings. Being able to unpuzzle the bias in a given dataset, and across datasets, is an essential tool for building safe and responsible AI. In this paper, we present deepPIC: deep Perceptual Image Clustering, a novel hierarchical clustering pipeline that leverages deep perceptual features to visualize and understand bias in unstructured and unlabeled datasets. It does so by effectively highlighting nuanced subcategories of information embedded within the data (such as multiple but repetitive shadow types) that typically are hard and/or expensive to annotate. Through experiments on a variety of image datasets, both open-source and internal, we demonstrate the effectiveness of deepPIC in (i) singling out errors in metadata from open-source datasets such as BDD100K; (ii) automatic nuanced metadata annotation; (iii) mining for edge cases; (iv) visualizing inherent bias both within and across multiple datasets; and (v) capturing synthetic data limitations; thus highlighting the wide variety of applications this pipeline can be applied to. All clustering results included here have been uploaded with image thumbnails on our project website - https://alchemz.github.io/unpuzzle_dataset_bias/ . We recommend zooming in for best impact.",
    "code_link": ""
  },
  "cvpr2022_vdu_few-shotimageclassificationbenchmarksaretoofarfromrealitybuildbackbetterwithsemantictasksampling": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Few-Shot Image Classification Benchmarks Are Too Far From Reality: Build Back Better With Semantic Task Sampling",
    "authors": [
      "Etienne Bennequin",
      "Myriam Tami",
      "Antoine Toubhans",
      "C\u00e9line Hudelot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Bennequin_Few-Shot_Image_Classification_Benchmarks_Are_Too_Far_From_Reality_Build_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Bennequin_Few-Shot_Image_Classification_Benchmarks_Are_Too_Far_From_Reality_Build_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Every day, a new method is published to tackle Few-Shot Image Classification, showing better and better performances on academic benchmarks. This is truly great news, yet we observe that these current benchmarks do not accurately represent the real industrial use cases that we encountered. In this work, through both qualitative and quantitative studies, we expose that the widely used benchmark tieredImageNet is strongly biased towards tasks composed of very semantically dissimilar classes, e.g. bathtub, cabbage, pizza, schipperke, and cardoon. This makes tieredImageNet (and similar benchmarks) irrelevant to evaluate the ability of a model to solve real-life use cases usually involving more fine-grained classification. We combat this bias using semantic information about the classes of tieredImageNet and generate an improved, balanced benchmark. Going further, we also introduce a new benchmark for Few-Shot Image Classification using the Danish Fungi 2020 dataset. This benchmark proposes a wide variety of evaluation tasks with various fine-graininess. Moreover, this benchmark includes many-way tasks (e.g., composed of 100 classes), which is a challenging setting yet very common in industrial applications. Our experiments bring out the correlation between the difficulty of a task and the semantic similarity between its classes, as well as a heavy performance drop of state-of-the-art methods on many-way few-shot classification, raising questions about the scaling abilities of our models. We hope that our work will encourage the community to further question the quality of standard evaluation processes and their relevance to real-life applications.",
    "code_link": "https://github.com/visipedia/fgvcx_fungi_comp"
  },
  "cvpr2022_vdu_investigatingneuralarchitecturesbysyntheticdatasetdesign": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Investigating Neural Architectures by Synthetic Dataset Design",
    "authors": [
      "Adrien Courtois",
      "Jean-Michel Morel",
      "Pablo Arias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Courtois_Investigating_Neural_Architectures_by_Synthetic_Dataset_Design_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Courtois_Investigating_Neural_Architectures_by_Synthetic_Dataset_Design_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent years have seen the emergence of many new neural network structures (architectures and layers). To solve a given task, a network requires a certain set of abilities reflected in its structure. The required abilities depend on each task. There is so far no systematic study of the real capacities of the proposed neural structures. The question of what each structure can and cannot achieve is only partially answered by its performance on common benchmarks. Indeed, natural data contain complex unknown statistical cues. It is therefore impossible to know what cues a given neural structure is taking advantage of in such data. In this work, we sketch a methodology to measure the effect of each structure on a network's ability, by designing ad hoc synthetic datasets. Each dataset is tailored to assess a given ability and is reduced to its simplest form: each input contains exactly the amount of information needed to solve the task. We illustrate our methodology by building three datasets to evaluate each of the three following network properties: a) the ability to link local cues to distant inferences, b) the translation covariance and c) the ability to group pixels with the same characteristics and share information among them. Using a first simplified depth estimation dataset, we pinpoint a serious nonlocal deficit of the U-Net. We then evaluate how to resolve this limitation by embedding its structure with nonlocal layers, which allow computing complex features with long-range dependencies. Using a second dataset, we compare different positional encoding methods and use the results to further improve the U-Net on the depth estimation task. The third introduced dataset serves to demonstrate the need for self-attention-like mechanisms for resolving more realistic depth estimation tasks.",
    "code_link": ""
  },
  "cvpr2022_vdu_mitigatingpaucityofdatainsinusoidcharacterizationusinggenerativesyntheticnoise": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Mitigating Paucity of Data in Sinusoid Characterization Using Generative Synthetic Noise",
    "authors": [
      "Sam Sattarzadeh",
      "Shervin Manzuri Shalmani",
      "Shervin Azad"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Sattarzadeh_Mitigating_Paucity_of_Data_in_Sinusoid_Characterization_Using_Generative_Synthetic_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Sattarzadeh_Mitigating_Paucity_of_Data_in_Sinusoid_Characterization_Using_Generative_Synthetic_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Although the remarkable breakthrough offered by Deep Learning (DL) models is numerous computer vision tasks, the need to acquire large amounts of high-quality natural data and fine-grained annotations is a shortcoming that fundamentally increases the cost and time devoted to training these models in real-world applications. Hence, synthetic datasets are considered reliable alternatives that can reduce the data acquisition by replacing or merging with natural data or effective pre-training of the models. To this end, in this work, we propose a novel approach to integrate structural data structures with the synthetic noise structures learned by unsupervised models that mimic the noise structures in natural data. Based on the proposed approach, we introduce the Sinusoid Feature Recognition (SFR) dataset, which contains hard-to-detect fixed-period sinusoid waves. While the previous works in this regard use generative models to sample synthetic data to inflate the training set, we instead apply unsupervised learning models to generate deep synthetic noise which makes training models in the proposed dataset more challenging. We evaluate the segmentation, image reconstruction, and sinusoid characterization models pre-trained or fully trained on the synthetic SFR dataset on a private dataset of grayscale Acoustic Tele-Viewer (ATV) images. Experimental results show that supervision on our proposed synthetic dataset can improve the accuracy of the models by 3-4% via pre-training, and by 17-27% via ad-hoc training while dealing with challenging, realistic real-world images.",
    "code_link": ""
  },
  "cvpr2022_vdu_achallengingbenchmarkofanimestylerecognition": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "A Challenging Benchmark of Anime Style Recognition",
    "authors": [
      "Haotang Li",
      "Shengtao Guo",
      "Kailin Lyu",
      "Xiao Yang",
      "Tianchen Chen",
      "Jianqing Zhu",
      "Huanqiang Zeng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Li_A_Challenging_Benchmark_of_Anime_Style_Recognition_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Li_A_Challenging_Benchmark_of_Anime_Style_Recognition_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Given two images of different anime roles, anime style recognition (ASR) aims to learn abstract painting style to determine whether the two images are from the same work, which is an interesting but challenging problem. Unlike biometric recognition, such as face recognition, iris recognition, and person re-identification, ASR suffers from a much larger semantic gap but receives less attention. In this paper, we propose a challenging ASR benchmark. Firstly, we collect a large-scale ASR dataset (LSASRD), which contains 20,937 images of 190 anime works and each work at least has ten different roles. In addition to the large-scale, LSASRD contains a list of challenging factors, such as complex illuminations, various poses, theatrical colors and exaggerated compositions. Secondly, we design a cross-role protocol to evaluate ASR performance, in which query and gallery images must come from different roles to validate an ASR model is to learn abstract painting style rather than learn discriminative features of roles. Finally, we apply two powerful person re-identification methods, namely, AGW and TransReID, to construct the baseline performance on LSASRD. Surprisingly, the recent transformer model (i.e., TransReID) only acquires a 42.24% mAP on LSASRD. Therefore, we believe that the ASR task of a huge semantic gap deserves deep and long-term research. We will open our dataset and code at https://github.com/nkjcqvcpi/ASR.",
    "code_link": "https://github.com/damo-cv/TransReID"
  },
  "cvpr2022_vdu_analysisoftemporaltensordatasetsonproductgrassmannmanifold": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Analysis of Temporal Tensor Datasets on Product Grassmann Manifold",
    "authors": [
      "Bojan Batalo",
      "Lincon S. Souza",
      "Bernardo B. Gatto",
      "Naoya Sogi",
      "Kazuhiro Fukui"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Batalo_Analysis_of_Temporal_Tensor_Datasets_on_Product_Grassmann_Manifold_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Batalo_Analysis_of_Temporal_Tensor_Datasets_on_Product_Grassmann_Manifold_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Growing abundance of multi-dimensional data creates a need for efficient data exploration and analysis. In this paper, we address this need by tackling the task of tensor dataset visualization and clustering, as tensors are a natural form of multi-dimensional data. Previous work has shown that representing individual tensor modes via respective linear subspaces and unifying them on the product Grassmann manifold (PGM) is an effective and memory-efficient way of representation. However, such representation may lead to loss of valuable temporal information. To address this issue, we model temporal tensor modes with a Hankel-like matrix, preserving sequence information and encoding it with a linear subspace, fully compatible with PGM. Unifying regular tensor modes and Hankel-like representation of regular tensor modes then enriches representation on the PGM, with minimal increase in computational complexity. By relying on geodesic distance on the manifold, we facilitate analysis of multi-dimensional datasets in two ways: 1) by enabling straightforward visualizations using algorithms such as t-SNE; and 2) by fostering clustering of data using distance- or similarity-based methods such as spectral clustering. We evaluate our approach on hand gesture and action recognition datasets as exemplars of temporal tensor datasets.",
    "code_link": ""
  },
  "cvpr2022_vdu_rethinkingilluminationforpersonre-identificationaunifiedview": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Rethinking Illumination for Person Re-Identification: A Unified View",
    "authors": [
      "Suncheng Xiang",
      "Guanjie You",
      "Leqi Li",
      "Mengyuan Guan",
      "Ting Liu",
      "Dahong Qian",
      "Yuzhuo Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Xiang_Rethinking_Illumination_for_Person_Re-Identification_A_Unified_View_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Xiang_Rethinking_Illumination_for_Person_Re-Identification_A_Unified_View_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "As a fundamental problem in video surveillance, person re-identification (re-ID) contributes a lot to the development of modern metro city. Recently, learning from synthetic data on re-ID task, which benefits from the popularity of synthetic data engine, has achieved remarkable performance in both supervised and unsupervised manner. However, previous researches mainly lay emphasis on employing synthetic data to achieve the state-of-the-art performance with a strong backbone, while neglects to perform quantitative studies on how visual factors affect re-ID system. To facilitate the research in this field, firstly, we manually construct a large-scale synthetic dataset named SynPerson, which has diversified human characters and distinguished attributes with accurate annotations. Secondly, we quantitatively analyze the influence of illumination on re-ID system. To our best knowledge, this is the first attempt to explicitly dissect person re-ID from the aspect of illumination on synthetic dataset. Comprehensive experiments help us have a deeper understanding of the fundamental problems in person re-ID. Furthermore, we will release SynPerson to the community, as part of efforts to alleviate the shortage of large-scale pedestrian dataset of future works.",
    "code_link": "https://github.com/Cysu/open-reid"
  },
  "cvpr2022_vdu_thetopologyandlanguageofrelationshipsinthevisualgenomedataset": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "The Topology and Language of Relationships in the Visual Genome Dataset",
    "authors": [
      "David Abou Chacra",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Chacra_The_Topology_and_Language_of_Relationships_in_the_Visual_Genome_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Chacra_The_Topology_and_Language_of_Relationships_in_the_Visual_Genome_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The Visual Genome Dataset is the de facto standard dataset used in Scene Graph generation. It contains a large collection of images with corresponding object and relationship labels. We explore the lingual aspect of the relationship predicates and find that very few symmetric/inverse relationships are represented in the dataset(for example, 'above' and 'under'). We believe this is linked to human spatial cognition, and posit that labelling bias stemming from human representations of relationships creates asymmetric relationship labels that span the whole dataset. We also perform a 2D topological analysis of the bounding boxes linked by different relationship predicates. This analysis sheds light on certain classes and their ambiguity wherein more frequent classes are semantically overloaded and therefore quite confusing. Finally we show that when reduced to more lingually and topologically well defined spatial relationships scene graph generation algorithm performance improves tremendously, but scene graph generators are still far from perfect.",
    "code_link": ""
  },
  "cvpr2022_vdu_datasetdistillationbymatchingtrainingtrajectories": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Dataset Distillation by Matching Training Trajectories",
    "authors": [
      "George Cazenavette",
      "Tongzhou Wang",
      "Antonio Torralba",
      "Alexei A. Efros",
      "Jun-Yan Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Dataset distillation is the task of synthesizing a small dataset such that a model trained on the synthetic set will match the test accuracy of the model trained on the full dataset. In this paper, we propose a new formulation that optimizes our distilled data to guide networks to a similar state as those trained on real data across many training steps. Given a network, we train it for several iterations on our distilled data and optimize the distilled data with respect to the distance between the synthetically trained parameters and the parameters trained on real data. To efficiently obtain the initial and target network parameters for large-scale datasets, we pre-compute and store training trajectories of expert networks trained on the real dataset. Our method handily outperforms existing methods and also allows us to distill higher-resolution visual data.",
    "code_link": ""
  },
  "cvpr2022_vdu_canwetrustboundingboxannotationsforobjectdetection?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Can We Trust Bounding Box Annotations for Object Detection?",
    "authors": [
      "Jeffri Murrugarra-Llerena",
      "Lucas N. Kirsten",
      "Claudio R. Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Murrugarra-Llerena_Can_We_Trust_Bounding_Box_Annotations_for_Object_Detection_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Murrugarra-Llerena_Can_We_Trust_Bounding_Box_Annotations_for_Object_Detection_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Object detection is a classical problem in computer vision, and the vast majority of approaches require large annotated datasets for training and evaluation purposes. The most popular representations are bounding boxes (BBs), usually defined as the minimal-area rectangle that encompasses the whole object region. However, the annotation process presents some subjectiveness (particularly when occlusions are present), and its quality might get degraded when the annotators get tired. Comparing BBs is crucial for evaluation purposes, and the Intersection-over-Union (IoU) is the standard similarity metric. In this paper, we provide theoretical and experimental results indicating that the IoU can be strongly affected even by small annotation discrepancies in popular datasets used for object detection. As a consequence, the Average Precision (AP) value commonly used to evaluate object detectors is also influenced by annotation bias or noise, particularly for small objects and tighter IoU thresholds.",
    "code_link": ""
  },
  "cvpr2022_vdu_canthemathematicalcorrectnessofobjectconfigurationsaffecttheaccuracyoftheirperception?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Can the Mathematical Correctness of Object Configurations Affect the Accuracy of Their Perception?",
    "authors": [
      "Han Jiang",
      "Zeqian Li",
      "Jacob Whitehill"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Jiang_Can_the_Mathematical_Correctness_of_Object_Configurations_Affect_the_Accuracy_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Jiang_Can_the_Mathematical_Correctness_of_Object_Configurations_Affect_the_Accuracy_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We investigate a new type of dataset bias based on the mathematical correctness of object configurations in visual scenes, and how this bias can affect the accuracy of computer vision models. Our experiments demonstrate how CNNs trained to detect and recognize individual objects are capable of implicitly learning simple mathematical relationships between them directly from pixel data; moreover, models that are trained with a dataset bias (e.g., all examples are mathematically correct) can suffer in performance when evaluated on test data without this bias. We found evidence for this effect in two settings: (1) object detection of math symbols in images of arithmetic expressions, and (2) object detection of moving particles from images produced by a physics simulator. Importantly, the semantic bias that we study is based not just on simple co-occurrence patterns in each image, but rather on higher-order semantic rules that generalize to unique combinations of objects not seen during training. While the magnitude of the effect was small, the accuracy difference was statistically reliable.",
    "code_link": ""
  },
  "cvpr2022_vdu_whyobjectdetectorsfailinvestigatingtheinfluenceofthedataset": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Why Object Detectors Fail: Investigating the Influence of the Dataset",
    "authors": [
      "Dimity Miller",
      "Georgia Goode",
      "Callum Bennie",
      "Peyman Moghadam",
      "Raja Jurdak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Miller_Why_Object_Detectors_Fail_Investigating_the_Influence_of_the_Dataset_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Miller_Why_Object_Detectors_Fail_Investigating_the_Influence_of_the_Dataset_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "A false negative in object detection describes an object that was not correctly localised and classified by a detector. In concurrent work, we introduced five 'false negative mechanisms' that identify the specific component inside the detector architecture that failed to detect the object. Using these mechanisms, we explore how different computer vision datasets and their inherent characteristics can influence object detector failures. Specifically, we investigate the false negative mechanisms of Faster R-CNN and RetinaNet across five computer vision datasets, namely Microsoft COCO, Pascal VOC, ExDark, ObjectNet, and COD10K. Our results show that object size and class influence the false negative mechanisms of object detectors. We also show that comparing the false negative mechanisms of a single object class across different datasets can highlight potentially unknown biases in datasets.",
    "code_link": ""
  },
  "cvpr2022_vdu_darkcorneronskinlesionimagedatasetdoesitmatter?": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Dark Corner on Skin Lesion Image Dataset: Does It Matter?",
    "authors": [
      "Samuel William Pewton",
      "Moi Hoon Yap"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Pewton_Dark_Corner_on_Skin_Lesion_Image_Dataset_Does_It_Matter_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Pewton_Dark_Corner_on_Skin_Lesion_Image_Dataset_Does_It_Matter_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Skin lesion image datasets gained popularity in recent years with the successes of ISIC datasets and challenges. While the users of these datasets are growing, the Dark Corner Artifact (DCA) phenomenon is under explored. This paper provides a better understanding of how and why DCA occurs, the types of DCAs and investigates the DCA within a curated ISIC image dataset. We introduce new labels of image artifacts on a curated balanced dataset of 9,810 images and identified 2,631 images with different intensities of DCA. Then, we improve the quality of this dataset by introducing automated DCA detection and removal methods. We evaluate the performance of our methods with image quality metrics on an unseen dataset (Dermofit), and achieved better SSIM score in every DCA intensity level. Further, we study the effects of DCA removal on a binary classification task (melanoma vs non-melanoma). Although deep learning performances in this task show marginal differences, we demonstrate that with DCA removal, it can help to shift the network activations to the skin lesions. All the artifact labels and codes are available at: https://github.com/Sam-Pewton/Dark_Corner_Artifact_Removal.",
    "code_link": ""
  },
  "cvpr2022_vdu_a3dstudyingpretrainedrepresentationswithprogrammabledatasets": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "A3D: Studying Pretrained Representations With Programmable Datasets",
    "authors": [
      "Ye Wang",
      "Norman Mu",
      "Daniele Grandi",
      "Nicolas Savva",
      "Jacob Steinhardt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Wang_A3D_Studying_Pretrained_Representations_With_Programmable_Datasets_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Wang_A3D_Studying_Pretrained_Representations_With_Programmable_Datasets_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Rendered images have been used to debug models, study inductive biases, and understand transfer learning. To scale up rendered datasets, we construct a pipeline with 40 classes of images including furniture and consumer products, backed by 48,716 distinct object models, 480 environments, and 563 materials. We can easily vary dataset diversity along four axes---object diversity, environment, material, and camera angle, making the dataset \"programmable\". Using this ability, we systematically study how these axes of data characteristics influence pretrained representations. We generate 21 datasets by reducing diversity along different axes, and study performance on five downstream tasks. We find that reducing environment has the biggest impact on performance and is harder to recover after fine-tuning. We corroborate this by visualizing the models' representations, findings that models trained on diverse environments learn more visually meaningful features.",
    "code_link": "https://github.com/whyzcandy/a3d"
  },
  "cvpr2022_vdu_bigdetectionalarge-scalebenchmarkforimprovedobjectdetectorpre-training": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "BigDetection: A Large-Scale Benchmark for Improved Object Detector Pre-Training",
    "authors": [
      "Likun Cai",
      "Zhi Zhang",
      "Yi Zhu",
      "Li Zhang",
      "Mu Li",
      "Xiangyang Xue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Cai_BigDetection_A_Large-Scale_Benchmark_for_Improved_Object_Detector_Pre-Training_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Cai_BigDetection_A_Large-Scale_Benchmark_for_Improved_Object_Detector_Pre-Training_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multiple datasets and open challenges for object detection have been introduced in recent years. To build more general and powerful object detection systems, in this paper, we construct a new large-scale benchmark termed BigDetection. Our goal is to simply leverage the training data from existing datasets (LVIS, OpenImages and Object365) with carefully designed principles, and curate a larger dataset for improved detector pre-training. Specifically, we generate a new taxonomy which unifies the heterogeneous label spaces from different sources. Our BigDetection dataset has 600 object categories and contains over 3.4M training images with 36M bounding boxes. It is much larger in multiple dimensions than previous benchmarks, which offers both opportunities and challenges. Extensive experiments demonstrate its validity as a new benchmark for evaluating different object detection methods and its effectiveness as a pre-training dataset. The code and models are available at https://github.com/amazon- research/bigdetection.",
    "code_link": "https://github.com/amazonresearch/bigdetection"
  },
  "cvpr2022_vdu_towardsexplainingimage-baseddistributionshifts": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "Towards Explaining Image-Based Distribution Shifts",
    "authors": [
      "Sean Kulinski",
      "David I. Inouye"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Kulinski_Towards_Explaining_Image-Based_Distribution_Shifts_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Kulinski_Towards_Explaining_Image-Based_Distribution_Shifts_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding such distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work has focused on either natively handling distribution shift (e.g., Domain Generalization) or merely detecting a shift while assuming any detected shift can be understood and handled appropriately by a human operator. For the latter, we hope to aid in these manual mitigation tasks by explaining the distribution shift to an operator. To this end, we suggest two methods: providing a set of interpretable mappings from the original distribution to the shifted one or providing a set of distributional counterfactual examples. We provide preliminary experiments on these two methods, and discuss important concepts and challenges for moving towards a better understanding of image-based distribution shifts.",
    "code_link": "https://github.com/inouye-lab/towards-explaining-image-distributionshifts"
  },
  "cvpr2022_vdu_theeffectofimprovingannotationqualityonobjectdetectiondatasetsapreliminarystudy": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "VDU",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Vision Datasets Understanding",
    "title": "The Effect of Improving Annotation Quality on Object Detection Datasets: A Preliminary Study",
    "authors": [
      "Jiaxin Ma",
      "Yoshitaka Ushiku",
      "Miori Sagara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Ma_The_Effect_of_Improving_Annotation_Quality_on_Object_Detection_Datasets_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Ma_The_Effect_of_Improving_Annotation_Quality_on_Object_Detection_Datasets_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this study, we partially reannotate conventional benchmark datasets for object detection and check whether there is performance improvement/drop compared with the original annotations. Recent studies on the annotation qualities of ImageNet for image classification revealed some issues of how to associate only a single label to each image accurately. Object detection, on the other hand, should have other nontrivial issues because there are multiple objects in a single image, and realizing consistency among bounding boxes is challenging. A team of professional annotators was formed for MS COCO and Google Open Images datasets. To realize highly-consistent annotations, we prepared carefully designed guidelines for each category and selected quality inspectors who checked the annotation quality of each annotator. Finally, we applied conventional object detection methods for reannotated parts of each dataset. We found mixed results: whether the performance dropped or improved depended on each category and dataset.",
    "code_link": ""
  },
  "cvpr2022_odrum_deepimageretrievalisnotrobusttolabelnoise": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Deep Image Retrieval Is Not Robust To Label Noise",
    "authors": [
      "Stanislav Dereka",
      "Ivan Karpukhin",
      "Sergey Kolesnikov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Dereka_Deep_Image_Retrieval_Is_Not_Robust_To_Label_Noise_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Dereka_Deep_Image_Retrieval_Is_Not_Robust_To_Label_Noise_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Large-scale datasets are essential for the success of deep learning in image retrieval. However, manual assessment errors and semi-supervised annotation techniques can lead to label noise even in popular datasets. As previous works primarily studied annotation quality in image classification tasks, it is still unclear how label noise affects deep learning approaches to image retrieval. In this work, we show that image retrieval methods are less robust to label noise than image classification ones. Furthermore, we, for the first time, investigate different types of label noise specific to image retrieval tasks and study their effect on model performance.",
    "code_link": ""
  },
  "cvpr2022_odrum_objectpriorembeddednetworkforquery-agnosticimageretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Object Prior Embedded Network for Query-Agnostic Image Retrieval",
    "authors": [
      "Yikang Li",
      "Jen-hao Hsiao",
      "Chiuman Ho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Li_Object_Prior_Embedded_Network_for_Query-Agnostic_Image_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Li_Object_Prior_Embedded_Network_for_Query-Agnostic_Image_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The Text-to-Image retrieval task plays an important role in bridging the gap between vision and language modalities. This task is challenging and far from being solved, because of the large visual-semantic discrepancy between language and vision. Recent studies on vision-language contrastive learning have shown that it can effectively learn good representations from massive image-text pairs. However, most existing methods simply concatenate image and text features as input and resort to the deep netowrk to learn the visual-semantic relationship between image and text in a brute force manner. The insufficient alignments information pose a challenging weakly-supervised learning task, and results in only limited accuracy in previous methods. Motivated by the observation that the salient objects in an image can be accurately detected and are often mentioned in the paired text, in this paper, we propose a novel cross-attention transformer that uses objects detected in image as anchor points and prior to significantly ease the learning of image-text alignments, and thus boost the text-to-image search accuracy. In addition, unlike the query-dependent architectures adopted by most previous methods, our proposed method is query-agnostic and is thus significantly faster in the inference process. The extensive experiments on Flickr30K and MSCOCO captions datasets demonstrate that our proposed method can outperform the SOTA method while preserving the inference efficiency.",
    "code_link": ""
  },
  "cvpr2022_odrum_cross-modaltargetretrievalfortrackingbynaturallanguage": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Cross-Modal Target Retrieval for Tracking by Natural Language",
    "authors": [
      "Yihao Li",
      "Jun Yu",
      "Zhongpeng Cai",
      "Yuwen Pan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Li_Cross-Modal_Target_Retrieval_for_Tracking_by_Natural_Language_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Li_Cross-Modal_Target_Retrieval_for_Tracking_by_Natural_Language_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Tracking by natural language specification in a video is a challenging task in computer vision. Distinct from initializing the target state only by the bounding box in the first frame, language specification has a strong potential to assist visual object trackers to capture appearance variation and eliminate semantic ambiguity of the tracked object. In this paper, we carefully design a unified local-global-search framework from the perspective of cross-modal retrieval, including a local tracker, an adaptive retrieval switch module, and a target-specific retrieval module. The adaptive retrieval switch module aligns semantics from the visual signal and the lingual description of the target using three sub-modules, i.e., object-aware attention memory, part-aware cross-attention, and vision-language contrast, which achieve an automatic switch between local search and global search. When booting the global search mechanism, the target-specific retrieval module re-localizes the missing target in the image-wide range via an efficient vision-language guided proposal selector and target-text match. Numerous experimental results on three prevailing benchmarks show the effectiveness and generalization of our framework.",
    "code_link": ""
  },
  "cvpr2022_odrum_deepnormalizedcross-modalhashingwithbi-directionrelationreasoning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning",
    "authors": [
      "Changchang Sun",
      "Hugo Latapie",
      "Gaowen Liu",
      "Yan Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Sun_Deep_Normalized_Cross-Modal_Hashing_With_Bi-Direction_Relation_Reasoning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Sun_Deep_Normalized_Cross-Modal_Hashing_With_Bi-Direction_Relation_Reasoning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.",
    "code_link": ""
  },
  "cvpr2022_odrum_conditionedandcomposedimageretrievalcombiningandpartiallyfine-tuningclip-basedfeatures": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Conditioned and Composed Image Retrieval Combining and Partially Fine-Tuning CLIP-Based Features",
    "authors": [
      "Alberto Baldrati",
      "Marco Bertini",
      "Tiberio Uricchio",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Baldrati_Conditioned_and_Composed_Image_Retrieval_Combining_and_Partially_Fine-Tuning_CLIP-Based_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Baldrati_Conditioned_and_Composed_Image_Retrieval_Combining_and_Partially_Fine-Tuning_CLIP-Based_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we present an approach for conditioned and composed image retrieval based on CLIP features. In this extension of content-based image retrieval (CBIR) an image is combined with a text that provides information regarding user intentions, and is relevant for application domains like e-commerce. The proposed method is based on an initial training stage where a simple combination of visual and textual features is used, to fine-tune the CLIP text encoder. Then in a second training stage we learn a more complex combiner network that merges visual and textual features. Contrastive learning is used in both stages. The proposed approach obtains state-of-the-art performance for conditioned CBIR on the FashionIQ dataset and for composed CBIR on the more recent CIRR dataset.",
    "code_link": ""
  },
  "cvpr2022_odrum_good,better,besttextualdistractorsgenerationformultiple-choicevisualquestionansweringviareinforcementlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning",
    "authors": [
      "Jiaying Lu",
      "Xin Ye",
      "Yi Ren",
      "Yezhou Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Lu_Good_Better_Best_Textual_Distractors_Generation_for_Multiple-Choice_Visual_Question_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Lu_Good_Better_Best_Textual_Distractors_Generation_for_Multiple-Choice_Visual_Question_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Multiple-choice VQA has drawn increasing attention from researchers and end-users recently. As the demand for automatically constructing large-scale multiple-choice VQA data grows, we introduce a novel task called textual Distractors Generation for VQA (DG-VQA) focusing on generating challenging yet meaningful distractors given the context image, question, and correct answer. The DG-VQA task aims at generating distractors without ground-truth training samples since such resources are rarely available. To tackle the DG-VQA unsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework that utilizes pre-trained VQA models as an alternative knowledge base to guide the distractor generation process. In Gobbet, a pre-trained VQA model serves as the environment in RL setting to provide feedback for the input multi-modal query, while a neural distractor generator serves as the agent to take actions accordingly. We propose to use existing VQA models' performance degradation as indicators of the quality of generated distractors. On the other hand, we show the utility of generated distractors through data augmentation experiments, since robustness is more and more important when AI models apply to unpredictable open-domain scenarios or security-sensitive applications. We further conduct a manual case study on the factors why distractors generated by Gobbet can fool existing models.",
    "code_link": ""
  },
  "cvpr2022_odrum_embeddingarithmeticofmultimodalqueriesforimageretrieval": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "ODRUM",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Open-Domain Retrieval Under Multi-Modal Settings",
    "title": "Embedding Arithmetic of Multimodal Queries for Image Retrieval",
    "authors": [
      "Guillaume Couairon",
      "Matthijs Douze",
      "Matthieu Cord",
      "Holger Schwenk"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Couairon_Embedding_Arithmetic_of_Multimodal_Queries_for_Image_Retrieval_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Couairon_Embedding_Arithmetic_of_Multimodal_Queries_for_Image_Retrieval_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Latent text representations exhibit geometric regularities, such as the famous analogy: queen is to king what woman is to man. Such structured semantic relations were not demonstrated on image representations. Recent works aiming at bridging this semantic gap embed images and text into a multimodal space, enabling the transfer of text-defined transformations to the image modality. We introduce the SIMAT dataset to evaluate the task of Image Retrieval with Multimodal queries. SIMAT contains 6k images and 18k textual transformation queries that aim at either replacing scene elements or changing pairwise relationships between scene elements. The goal is to retrieve an image consistent with the (source image, text transformation) query. We use an image/text matching oracle (OSCAR) to assess whether the image transformation is successful. The SIMAT dataset will be publicly available. We use SIMAT to evaluate the geometric properties of multimodal embedding spaces trained with an image/text matching objective, like CLIP. We show that vanilla CLIP embeddings are not very well suited to transform images with delta vectors, but that a simple finetuning on the COCO dataset can bring dramatic improvements. We also study whether it is beneficial to leverage pretrained universal sentence encoders (FastText, LASER and LaBSE).",
    "code_link": ""
  },
  "cvpr2022_gaze_characterizingtarget-absenthumanattention": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Characterizing Target-Absent Human Attention",
    "authors": [
      "Yupei Chen",
      "Zhibo Yang",
      "Souradeep Chakraborty",
      "Sounak Mondal",
      "Seoyoung Ahn",
      "Dimitris Samaras",
      "Minh Hoai",
      "Gregory Zelinsky"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Human efficiency in finding a target in an image has attracted the attention of machine learning researchers, but what about when no target is there? Knowing how people search in the absence of a target, and when they stop, is important for Human-computer-interaction systems attempting to predict human gaze behavior in the wild. Here we report a rigorous evaluation of target-absent search behavior using the COCO-Search18 dataset to train state-of-the-art models. We focus on two specific aims. First, we characterize the presence of a target guidance signal in target-absent search behavior by comparing it to target-present guidance and free viewing. We do this by comparing how well a model trained on one type of fixation behavior (target-present, target-absent, free viewing) can predict behavior in either the same or different task. To compare target-absent search to free viewing behavior we created COCO-FreeView, a dataset of free-viewing fixations for the same images used in COCO-Search18. These comparisons revealed the existence of a target guidance signal in target-absent search, albeit one much less dominant compared to when a target actually appeared in an image, and that the target-absent guidance signal was similar to free viewing in that saliency and center bias were both weighted more than guidance from target features. Our second aim focused on the stopping criteria, a question intrinsic to target-absent search. Here we propose to train a foveated target detector whose target detection representation is sensitive to the relationship between distance from the fovea. Then combining the predicted target detection representation with other information such as fixation history and subject ID, our model outperforms the baselines in predicting when a person stops moving his attention during target-absent search.",
    "code_link": ""
  },
  "cvpr2022_gaze_self-attentionwithconvolutionanddeconvolutionforefficienteyegazeestimationfromafullfaceimage": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Self-Attention With Convolution and Deconvolution for Efficient Eye Gaze Estimation From a Full Face Image",
    "authors": [
      "Jun O Oh",
      "Hyung Jin Chang",
      "Sang-Il Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/Oh_Self-Attention_With_Convolution_and_Deconvolution_for_Efficient_Eye_Gaze_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Oh_Self-Attention_With_Convolution_and_Deconvolution_for_Efficient_Eye_Gaze_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper proposes a whole new face image-based eye gaze estimation network to solve low generalization performance. Due to the high variance of facial appearance and environmental conditions, conventional methods in gaze estimation have low generalization performance and are easily overfitted to training subjects. To solve this problem, we adopt a self-attention mechanism that has better generalization performance. Nevertheless, applying self-attention directly to an image incurs a high computational cost. Thus, we introduce a new projection that uses convolution in the entire face image to accurate model the local context and reduce the computational cost of self-attention. The proposed model also includes deconvolution that transforms the down-sampled global context to the same size as the input so that spatial information is not lost. We confirmed through observations that the new method achieved state of the art on the EYEDIAP, MPIIFaceGaze, Gaze360 and RT-GENE datasets and achieved a performance increase of 0.02deg to 0.30deg compared to the other state of the art model. In addition, we show the generalization performance of the proposed model through a cross-dataset evaluation.",
    "code_link": ""
  },
  "cvpr2022_gaze_amodularmultimodalarchitectureforgazetargetpredictionapplicationtoprivacy-sensitivesettings": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings",
    "authors": [
      "Anshul Gupta",
      "Samy Tafasca",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/Gupta_A_Modular_Multimodal_Architecture_for_Gaze_Target_Prediction_Application_to_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gupta_A_Modular_Multimodal_Architecture_for_Gaze_Target_Prediction_Application_to_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Predicting where a person is looking is a complex task, requiring to understand not only the person's gaze and scene content, but also the 3D scene structure and the person's situation (are they manipulating? interacting or observing others? attentive?) to detect obstructions in the line of sight or apply attention priors that humans typically have when observing others. In this paper, we hypothesize that identifying and leveraging such priors can be better achieved through the exploitation of explicitly derived multimodal cues such as depth and pose. We thus propose a modular multimodal architecture allowing to combine these cues using an attention mechanism. The architecture can naturally be exploited in privacy-sensitive situations such as surveillance and health, where personally identifiable information cannot be released. We perform extensive experiments on the GazeFollow and VideoAttentionTarget public datasets, obtaining state-of-the-art performance and demonstrating very competitive results in the privacy setting case.",
    "code_link": ""
  },
  "cvpr2022_gaze_scanpathnetarecurrentmixturedensitynetworkforscanpathprediction": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "ScanpathNet: A Recurrent Mixture Density Network for Scanpath Prediction",
    "authors": [
      "Ryan Anthony Jalova de Belen",
      "Tomasz Bednarz",
      "Arcot Sowmya"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/de_Belen_ScanpathNet_A_Recurrent_Mixture_Density_Network_for_Scanpath_Prediction_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/de_Belen_ScanpathNet_A_Recurrent_Mixture_Density_Network_for_Scanpath_Prediction_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Understanding the mechanisms underlying human visual attention is an important research problem in cognitive neuroscience and computer vision. While existing models predict salient regions (i.e., saliency maps) and temporal sequences of eye fixations (i.e., scanpaths) in images, their designs often partially follow theoretical frameworks. Here, we introduce ScanpathNet, a deep learning model inspired by the latest theoretical model in neuroscience. It is 'guided' by a dynamic priority map influenced by semantic content and fixation history. The model leverages convolutional neural networks to extract rich semantic features, convolutional long short-term memory networks to model the inhibition of return mechanism and sequential dependencies of fixations, and mixture density networks to predict probability distributions of fixations for each pixel. Simulated human scanpaths can then be generated by sequentially sampling the output of the proposed model. Despite its simplicity, ScanpathNet showed promising qualitative and quantitative scanpath prediction performance in extensive experiments on numerous eye-tracking benchmark datasets.",
    "code_link": ""
  },
  "cvpr2022_gaze_unsupervisedmulti-viewgazerepresentationlearning": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Unsupervised Multi-View Gaze Representation Learning",
    "authors": [
      "John Gideon",
      "Shan Su",
      "Simon Stent"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "We present a method for unsupervised gaze representation learning from multiple synchronized views of a person's face. The key assumption is that images of the same eye captured from different viewpoints differ in certain respects while remaining similar in others. Specifically, the absolute gaze and absolute head pose of the same subject should be different from different viewpoints, while appearance characteristics and gaze angle relative to the head coordinate frame should remain constant. To leverage this, we adopt a cross-encoder learning framework, in which our encoding space consists of head pose, relative eye gaze, eye appearance and other common features. Image pairs which are assumed to have matching subsets of features should be able to swap those subsets among themselves without any loss of information, computed by decoding the mixed features back into images and measuring reconstruction loss. We show that by applying these assumptions to an unlabelled multi-view video dataset, we can generate more powerful representations than a standard gaze cross-encoder for few-shot gaze estimation. Furthermore, we introduce a new feature-mixing method which results in higher performance, faster training, improved testing flexibility with multiple views, and added interpretability with learned confidence.",
    "code_link": ""
  },
  "cvpr2022_gaze_learning-by-novel-view-synthesisforfull-faceappearance-based3dgazeestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation",
    "authors": [
      "Jiawei Qin",
      "Takuru Shimoyama",
      "Yusuke Sugano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Despite recent advances in appearance-based gaze estimation techniques, the need for training data that covers the target head pose and gaze distribution remains a crucial challenge for practical deployment. This work examines a novel approach for synthesizing gaze estimation training data based on monocular 3D face reconstruction. Unlike prior works using multi-view reconstruction, photorealistic CG models, or generative neural networks, our approach can manipulate and extend the head pose range of existing training data without any additional requirements. We introduce a projective matching procedure to align the reconstructed 3D facial mesh with the camera coordinate system and synthesize face images with accurate gaze labels. We also propose a mask-guided gaze estimation model and data augmentation strategies to further improve the estimation accuracy by taking advantage of synthetic training data. Experiments using multiple public datasets show that our approach significantly improves the estimation performance on challenging cross-dataset settings with non-overlapping gaze distributions.",
    "code_link": "https://github.com/cleardusk/3DDFA"
  },
  "cvpr2022_gaze_one-stageobjectreferringwithgazeestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "One-Stage Object Referring With Gaze Estimation",
    "authors": [
      "Jianhang Chen",
      "Xu Zhang",
      "Yue Wu",
      "Shalini Ghosh",
      "Pradeep Natarajan",
      "Shih-Fu Chang",
      "Jan Allebach"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/html/Chen_One-Stage_Object_Referring_With_Gaze_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_One-Stage_Object_Referring_With_Gaze_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The classic object referring task aims at localizing the referred object in the image and requires a reference image and a natural language description as inputs. Given the facts that gaze signal can be easily obtained by a modern human-computer interaction system with a camera and that human tends to look at the object when referring to it, we propose a novel gaze-assisted object referring framework. The formulation not only simplifies the state-of-the-art gaze-assisted object referring system requiring many input signals besides gaze, but also incorporates the one-stage object detection idea to improve the inference efficiency. More importantly, it implicitly considers all object candidates and thus resolves the main pain point of existing two-stage object referring solutions for proposing an appropriate number of candidates -- it cannot be too large, otherwise the computational cost can be prohibitive; it cannot be too small, otherwise the chance of missing a referred object can be significant. To utilize the gaze information, we propose to build a gaze heatmap by using the anchor position encoding map and the gaze prediction result. The gaze heatmap and the language feature are then merged into the feature pyramid in the object detection as the final one-stage referring system. In the CityScapes-OR dataset, the proposed method outperforms the state-of-the-art by 7.8% for Acc@1.",
    "code_link": ""
  },
  "cvpr2022_imw_detectingandsuppressingmarinesnowforunderwatervisualslam": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "Detecting and Suppressing Marine Snow for Underwater Visual SLAM",
    "authors": [
      "Lars Martin Hodne",
      "Eirik Leikvoll",
      "Mauhing Yip",
      "Andreas Langeland Teigen",
      "Annette Stahl",
      "Rudolf Mester"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Hodne_Detecting_and_Suppressing_Marine_Snow_for_Underwater_Visual_SLAM_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Hodne_Detecting_and_Suppressing_Marine_Snow_for_Underwater_Visual_SLAM_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Conventional SLAM methods which work very well in typical above-water situations, are based on detecting keypoints that are tracked between images, from which egomotion and the 3D structure of the scene are estimated. However, in underwater environments with marine snow -- small particles of organic matter which are carried by ocean currents throughout the water column -- keypoint detectors are prone to detect the marine snow particles. As the vast majority of SLAM front ends are sensitive against outliers, and the marine snow acts as severe \"motion noise\", failure of the regular egomotion and 3D structure estimation is expected. For this reason, we investigate the structure and appearance of marine snow and developed two schemes which classify keypoints into \"marine snow\" or \"clean\" based on either the image patches obtained from usual keypoint detectors or the descriptors computed from these patches. This way the subsequent SLAM pipeline is protected against 'false' keypoints. We quantitatively evaluate the performance of our marine snow classifier on both real underwater video scenes as well as on simulated underwater footage that contains marine snow. These simulated image sequences have been created by extracting real marine snow elements from real underwater footage, and subsequently overlaying these on \"clean\" underwater videos. Qualitative evaluation is also done on a nightime road sequence with snowfall to demonstrate applicability in other areas of autonomy. We furthermore evaluate the performance and the effect of marine snow detection & suppression by integrating the snow suppression module in a full SLAM pipeline based on the pySLAM system.",
    "code_link": "https://github.com/luigifreda/pyslam"
  },
  "cvpr2022_imw_da-aedisparity-alleviationauto-encodertowardscategorizationofheritageimagesforaggrandized3dreconstruction.": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "DA-AE: Disparity-Alleviation Auto-Encoder Towards Categorization of Heritage Images for Aggrandized 3D Reconstruction.",
    "authors": [
      "Dikshit Hegde",
      "Tejas Anvekar",
      "Ramesh Ashok Tabib",
      "Uma Mudengudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Hegde_DA-AE_Disparity-Alleviation_Auto-Encoder_Towards_Categorization_of_Heritage_Images_for_Aggrandized_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Hegde_DA-AE_Disparity-Alleviation_Auto-Encoder_Towards_Categorization_of_Heritage_Images_for_Aggrandized_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "In this paper, we propose DA-AE: Disparity Alleviation AutoEncoder for categorization of heritage images towards 3D reconstruction. Recent survey on preservation of heritage shows demand for the digitization and conservations of heritage sites owing to their susceptibility to natural disasters and human acts. Digital conservation can be facilitated via crowdsourcing of data useful for construction of 3D models. Data from multiple sites sourced may result in elimination of relevant images due to the limitations of the pipeline. Curation and categorization of the crowdsourced data enables better 3D reconstruction. 3D reconstruction pipelines demand correlation between the data and also tries to eliminate the irrelevant information. The reconstruction pipeline is sensitive to selection of initial pair for reconstruction. By categorising individual sites, crowdsourced data can be used to create better 3D reconstructed models. Categorization of crowdsourced data demands learning robust representations of data. Towards this, we propose DA-AE for improved representation and categorization of data in latent space, along with a disparity alleviation loss. We demonstrate categorization as an event, with clustering as a downstream task. We compare our results of clustering with state-of-the-art methods on benchmark datasets (MNIST, FashionMNIST, and USPS). We demonstrate the effects of our categorization using custom dataset IDH10 and compare the results with state-of-the-art methods. We show a systematic and qualitative influence of the proposed method on 3D reconstruction of data.",
    "code_link": ""
  },
  "cvpr2022_imw_learningco-segmentationbysegmentswappingforretrievalanddiscovery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "Learning Co-Segmentation by Segment Swapping for Retrieval and Discovery",
    "authors": [
      "Xi Shen",
      "Alexei A. Efros",
      "Armand Joulin",
      "Mathieu Aubry"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Shen_Learning_Co-Segmentation_by_Segment_Swapping_for_Retrieval_and_Discovery_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Shen_Learning_Co-Segmentation_by_Segment_Swapping_for_Retrieval_and_Discovery_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The goal of this work is to efficiently identify visually similar patterns from a pair of images, e.g. identifying an artwork detail copied between an engraving and an oil painting, or matching a night-time photograph with its daytime counterpart. Lack of training data is a key challenge for this co-segmentation task. We present a simple yet surprisingly effective approach to overcome this difficulty: we generate synthetic training pairs by selecting object segments in an image and copy-pasting them into another image. We then learn to predict the repeated object masks. We find that it is crucial to predict the correspondences as an auxiliary task and to use Poisson blending and style transfer on the training pairs to generalize on real data. We analyse results with two deep architectures relevant to our joint image analysis task: a transformer-based architecture and Sparse Nc-Net, a recent network designed to predict coarse correspondences using 4D convolutions. We show our approach provides clear improvements for artwork details retrieval on the Brueghel dataset and achieves competitive performance on two place recognition benchmarks, Tokyo247 and Pitts30K. We then demonstrate the potential of our approach by performing object discovery on the Internet object discovery dataset and the Brueghel dataset. Our code and data are available at http://imagine.enpc.fr/ shenx/SegSwap/.",
    "code_link": ""
  },
  "cvpr2022_imw_nerfelsrenderableneuralcodesforimprovedcameraposeestimation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "Nerfels: Renderable Neural Codes for Improved Camera Pose Estimation",
    "authors": [
      "Gil Avraham",
      "Julian Straub",
      "Tianwei Shen",
      "Tsun-Yi Yang",
      "Hugo Germain",
      "Chris Sweeney",
      "Vasileios Balntas",
      "David Novotny",
      "Daniel DeTone",
      "Richard Newcombe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Avraham_Nerfels_Renderable_Neural_Codes_for_Improved_Camera_Pose_Estimation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Avraham_Nerfels_Renderable_Neural_Codes_for_Improved_Camera_Pose_Estimation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper presents a framework that combines traditional keypoint-based camera pose optimization with an invertible neural rendering mechanism. Our proposed 3D scene representation, Nerfels, is locally dense yet globally sparse. As opposed to existing invertible neural rendering systems which overfit a model to the entire scene, we adopt a feature-driven approach for representing scene-agnostic, local 3D patches with renderable codes. By modelling a scene only where local features are detected, our framework effectively generalizes to unseen local regions in the scene via an optimizable code conditioning mechanism in the neural renderer, all while maintaining the low memory footprint of a sparse 3D map representation. Our model can be incorporated to existing state-of-the-art hand-crafted and learned local feature pose estimators, yielding improved performance when evaluating on ScanNet for wide camera baseline scenarios.",
    "code_link": ""
  },
  "cvpr2022_imw_featurequerynetworksneuralsurfacedescriptionforcameraposerefinement": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "Feature Query Networks: Neural Surface Description for Camera Pose Refinement",
    "authors": [
      "Hugo Germain",
      "Daniel DeTone",
      "Geoffrey Pascoe",
      "Tanner Schmidt",
      "David Novotny",
      "Richard Newcombe",
      "Chris Sweeney",
      "Richard Szeliski",
      "Vasileios Balntas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Germain_Feature_Query_Networks_Neural_Surface_Description_for_Camera_Pose_Refinement_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Germain_Feature_Query_Networks_Neural_Surface_Description_for_Camera_Pose_Refinement_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Accurate 6-DoF camera pose estimation in known environments can be a very challenging task, especially when the query image was captured at viewpoints strongly differing from the set of reference camera poses. While structure-based methods have proved to deliver accurate camera pose estimates, they rely on pre-computed 3D descriptors coming from reference images often misaligned with query images. This descriptor discrepancy can subsequently harm the downstream camera pose estimation task. In this paper we introduce the Feature Query Network (FQN), a ray-based descriptor regressor that can be used to query descriptors at known 3D locations under novel viewpoints. We show that the FQN is able to model viewpoint-dependency of high-dimensional state-of-the-art keypoint descriptors and bring significant relative improvements to structure-based visual localization baselines.",
    "code_link": ""
  },
  "cvpr2022_imw_unstructuredobjectmatchingusingco-salientregionsegmentation": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "Unstructured Object Matching Using Co-Salient Region Segmentation",
    "authors": [
      "Ioana-Sabina Stoian",
      "Ionut-Catalin Sandu",
      "Daniel Voinea",
      "Alin-Ionut Popa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Stoian_Unstructured_Object_Matching_Using_Co-Salient_Region_Segmentation_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Stoian_Unstructured_Object_Matching_Using_Co-Salient_Region_Segmentation_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Unstructured object matching is a less-explored and very challenging topic in the scientific literature. This includes matching scenarios where the context, appearance and the geometrical integrity of the objects to be matched changes drastically from one image to another (e.g. a pair of pyjamas which in one image is folded and in the other is worn by a person), making it impossible to determine a transformation which aligns the matched regions. Traditional approaches like keypoint-based feature matching perform poorly on this use case due to the high complexity in terms of viewpoint, scene context variety, background variations or high degrees of freedom concerning structural configurations. In this paper we propose a deep learning framework consisting of a twins based matching approach leveraging a co-salient region segmentation task and a cosine-similarity based region descriptor pairing technique. The importance of our proposed framework is demonstrated on a novel use case consisting of image pairs with various objects used by children. Additionally, we evaluate on Human3.6M and Market-1501, two datasets with humans depicting various appearances and kinematic configurations captured under different backgrounds.",
    "code_link": ""
  },
  "cvpr2022_imw_acaseforusingrotationinvariantfeaturesinstateoftheartfeaturematchers": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Image Matching: Local Features and Beyond",
    "title": "A Case for Using Rotation Invariant Features in State of the Art Feature Matchers",
    "authors": [
      "Georg B\u00f6kman",
      "Fredrik Kahl"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/html/Bokman_A_Case_for_Using_Rotation_Invariant_Features_in_State_of_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Bokman_A_Case_for_Using_Rotation_Invariant_Features_in_State_of_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "The aim of this paper is to demonstrate that a state of the art feature matcher (LoFTR) can be made more robust to rotations by simply replacing the backbone CNN with a steerable CNN which is equivariant to translations and image rotations. It is experimentally shown that this boost is obtained without reducing performance on ordinary illumination and viewpoint matching sequences.",
    "code_link": "https://github.com/georgbn/se2-loftr"
  },
  "cvpr2022_sketchdl_leveragingunlabeleddataforsketch-basedunderstanding": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Sketch-Oriented Deep Learning",
    "title": "Leveraging Unlabeled Data for Sketch-Based Understanding",
    "authors": [
      "Javier Morales",
      "Nils Murrugarra-Llerena",
      "Jose M. Saavedra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/html/Morales_Leveraging_Unlabeled_Data_for_Sketch-Based_Understanding_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Morales_Leveraging_Unlabeled_Data_for_Sketch-Based_Understanding_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Sketch-based understanding is a critical component of human cognitive learning and is a primitive communication means between humans. This topic has recently attracted the interest of the computer vision community as sketching represents a powerful tool to express static objects and dynamic scenes. Unfortunately, despite its broad application domains, the current sketch-based models strongly rely on labels for supervised training, ignoring knowledge from unlabeled data, thus limiting the underlying generalization and the applicability. Therefore, we present a study about the use of unlabeled data to improve a sketch-based model. To this end, we evaluate variations of VAE and semi-supervised VAE, and present an extension of BYOL to deal with sketches. Our results show the superiority of sketch-BYOL, which outperforms other self-supervised approaches increasing the retrieval performance for known and unknown categories. Furthermore, we show how other tasks can benefit from our proposal.",
    "code_link": ""
  },
  "cvpr2022_sketchdl_constellationsanoveldatasetforstudyingiterativeinferenceinhumansandai": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Sketch-Oriented Deep Learning",
    "title": "Constellations: A Novel Dataset for Studying Iterative Inference in Humans and AI",
    "authors": [
      "Tarun Khajuria",
      "Kadi Tulver",
      "Taavi Luik",
      "Jaan Aru"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/html/Khajuria_Constellations_A_Novel_Dataset_for_Studying_Iterative_Inference_in_Humans_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Khajuria_Constellations_A_Novel_Dataset_for_Studying_Iterative_Inference_in_Humans_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Under complex viewing conditions, human perception relies on generating hypotheses and revising them in an iterative fashion. We developed novel visual stimuli to study such iterative inference in humans and AI. In these stimuli, called \"constellations\", all local information about the object has been removed and it can only be recognized when taking into account the global pattern. We here describe the dataset and demonstrate that humans indeed use an iterative process of generating hypotheses and refining them to solve these images. We also provide code that allows researchers to create their own constellation images. The constellation dataset allows researchers to develop sketching algorithms for guessing the hidden object. As such algorithms used by humans appear to be iterative in nature, this dataset will facilitate the study of iterative inference in minds and machines.",
    "code_link": "https://github.com/deepmind/multi-object-datasets"
  },
  "cvpr2022_sketchdl_ssr-gnnsstroke-basedsketchrepresentationwithgraphneuralnetworks": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Sketch-Oriented Deep Learning",
    "title": "SSR-GNNs: Stroke-Based Sketch Representation With Graph Neural Networks",
    "authors": [
      "Sheng Cheng",
      "Yi Ren",
      "Yezhou Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/html/Cheng_SSR-GNNs_Stroke-Based_Sketch_Representation_With_Graph_Neural_Networks_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Cheng_SSR-GNNs_Stroke-Based_Sketch_Representation_With_Graph_Neural_Networks_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This paper follows cognitive studies to investigate a graph representation for sketches, where the information of strokes, i.e., parts of a sketch, are encoded on vertices and information of inter-stroke on edges. The resultant graph representation facilitates the training of a Graph Neural Networks for classification tasks, and achieves accuracy and robustness comparable to the state-of-the-art against translation and rotation attacks, as well as stronger attacks on graph vertices and topologies, i.e., modifications and addition of strokes, all without resorting to adversarial training. Prior studies on sketches, e.g., graph transformers, encode control points of stroke on vertices, which are not invariant to spatial transformations. In contrary, we encode vertices and edges using pairwise distances among control points to achieve invariance. Compared with existing generative sketch model for one-shot classification, our method does not rely on run-time statistical inference. Lastly, the proposed representation enables generation of novel sketches that are structurally similar to while separable from the existing dataset.",
    "code_link": ""
  },
  "cvpr2022_sketchdl_signaturedetection,restoration,andverificationanovelchinesedocumentsignatureforgerydetectionbenchmark": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Sketch-Oriented Deep Learning",
    "title": "Signature Detection, Restoration, and Verification: A Novel Chinese Document Signature Forgery Detection Benchmark",
    "authors": [
      "Kaihong Yan",
      "Ying Zhang",
      "Haoran Tang",
      "Chengkai Ren",
      "Jian Zhang",
      "Gaoang Wang",
      "Hongwei Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/html/Yan_Signature_Detection_Restoration_and_Verification_A_Novel_Chinese_Document_Signature_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Yan_Signature_Detection_Restoration_and_Verification_A_Novel_Chinese_Document_Signature_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Offline signature forgery detection has attracted many researchers in recent years. In real situations, signatures should be detected from the signed documents and verified by the forgery detection system. There are many challenges in the pipeline. First, some signatures have low resolutions and are difficult to be detected. Second, the cropped signatures may contain irrelevant background context of the document, making the signature hard to be verified. Third, some forgery signatures are very similar to genuine ones, increasing the challenge of verification. In addition, most existing datasets do not cover all the pipeline tasks. Moreover, publicly available Chinese-based signature datasets are rare for research purposes. In this paper, we construct a novel Chinese document offline signature forgery detection benchmark, namely ChiSig, which includes all pipeline tasks, i.e., signature detection, restoration, and verification. Besides, we extensively compare different deep learning-based approaches in these three tasks. The results show that our proposed dataset can effectively provide solutions for constructing pipeline systems for Chinese document signature forgery detection.",
    "code_link": ""
  },
  "cvpr2022_sketchdl_theroleofshapefordomaingeneralizationonsparsely-texturedimages": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Sketch-Oriented Deep Learning",
    "title": "The Role of Shape for Domain Generalization on Sparsely-Textured Images",
    "authors": [
      "Narges Honarvar Nazari",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/html/Nazari_The_Role_of_Shape_for_Domain_Generalization_on_Sparsely-Textured_Images_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Nazari_The_Role_of_Shape_for_Domain_Generalization_on_Sparsely-Textured_Images_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "State-of-the-art object recognition methods do not generalize well to unseen domains. Work in domain generalization has attempted to bridge domains by increasing feature compatibility, but has focused on standard, appearance-based representations. We show the potential of shape-based representations to increase domain robustness. We compare two types of shape-based representations: one trains a convolutional network over edge features, and another computes a soft, dense medial axis transform. We show the complementary strengths of these representations for different types of domains, and the effect of the amount of texture that is preserved. We show that our shape-based techniques better leverage data augmentations for domain generalization, and are more effective at texture bias mitigation than shape-inducing augmentations. Finally, we show that when the convolutional network in state-of-the-art domain generalization methods is replaced with one that explicitly captures shape, we obtain improved results.",
    "code_link": ""
  },
  "cvpr2022_omnicv_rethinkingsuperviseddepthestimationfor360degpanoramicimagery": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Rethinking Supervised Depth Estimation for 360deg Panoramic Imagery",
    "authors": [
      "Lu He",
      "Bing Jian",
      "Yangming Wen",
      "Haichao Zhu",
      "Kelin Liu",
      "Weiwei Feng",
      "Shan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/He_Rethinking_Supervised_Depth_Estimation_for_360deg_Panoramic_Imagery_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/He_Rethinking_Supervised_Depth_Estimation_for_360deg_Panoramic_Imagery_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Depth estimation from a single 360 panorama image is a difficult task. It is an ill-posed problem to estimate depth maps from an RGB panorama image due to the intrinsic scale ambiguity issue. To mitigate the scale inconsistency issue in the ground truth depth map, we propose a simple yet effective method to normalize the depth data based on estimated camera height. In addition, we design a multiple head planar-guided depth network, to provide more geometric constraints for depth estimation. Experimental results show that our relative depth estimation task is more accurate than the absolute depth estimation task, and our proposed model produces state-of-the-art performance on both Matterport3D and Stanford2D3D datasets.",
    "code_link": ""
  },
  "cvpr2022_omnicv_spinsimplifyingpolarinvarianceforneuralnetworksapplicationtovision-basedirradianceforecasting": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "SPIN: Simplifying Polar Invariance for Neural Networks Application to Vision-Based Irradiance Forecasting",
    "authors": [
      "Quentin Paletta",
      "Anthony Hu",
      "Guillaume Arbod",
      "Philippe Blanc",
      "Joan Lasenby"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/Paletta_SPIN_Simplifying_Polar_Invariance_for_Neural_Networks_Application_to_Vision-Based_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/Paletta_SPIN_Simplifying_Polar_Invariance_for_Neural_Networks_Application_to_Vision-Based_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Translational invariance induced by pooling operations is an inherent property of convolutional neural networks, which facilitates numerous computer vision tasks such as classification. Yet to leverage rotational invariant tasks, convolutional architectures require specific rotational invariant layers or extensive data augmentation to learn from diverse rotated versions of a given spatial configuration. Unwrapping the image into its polar coordinates provides a more explicit representation to train a convolutional architecture as the rotational invariance becomes translational, hence the visually distinct but otherwise equivalent rotated versions of a given scene can be learnt from a single image. We show with two common vision-based solar irradiance forecasting challenges (i.e. using ground-taken sky images or satellite images), that this preprocessing step significantly improves prediction results by standardising the scene representation, while decreasing training time by a factor of 4 compared to augmenting data with rotations. In addition, this transformation magnifies the area surrounding the centre of the rotation, leading to more accurate short-term irradiance predictions.",
    "code_link": ""
  },
  "cvpr2022_omnicv_poseestimationfortwo-viewpanoramasbasedonkeypointmatchingacomparativestudyandcriticalanalysis": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Pose Estimation for Two-View Panoramas Based on Keypoint Matching: A Comparative Study and Critical Analysis",
    "authors": [
      "Jeffri Murrugarra-Llerena",
      "Thiago L. T. da Silveira",
      "Claudio R. Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/Murrugarra-Llerena_Pose_Estimation_for_Two-View_Panoramas_Based_on_Keypoint_Matching_A_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/Murrugarra-Llerena_Pose_Estimation_for_Two-View_Panoramas_Based_on_Keypoint_Matching_A_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Pose estimation is a crucial problem in several computer vision and robotics applications. For the two-view scenario, the typical pipeline consists of finding point correspondences between the two views and using them to estimate the pose. However, most available keypoint extraction and matching methods were designed to work with perspective images and may fail under not-affine distortions present in wide-angle or omnidirectional media, which are becoming increasingly popular in recent years. This paper presents a comprehensive comparative analysis of different keypoint matching algorithms for panoramas coupled to different linear and non-linear approaches for pose estimation. As an additional contribution, we explore a recent approach for mitigating spherical distortions using tangent plane projections, which can be coupled with any planar descriptor, and allows the adaptation of recent learning-based methods. We evaluate the combination of keypoint matching and pose estimation methods using the rotation and translation error of the estimated pose in different scenarios (indoor and outdoor), and our results indicate that SPHORB and \"tangent SIFT\" are competitive algorithms. We also show that tangent plane adaptations frequently present competitive results, and some optimization steps consistently improve the performance in all methods.",
    "code_link": ""
  },
  "cvpr2022_omnicv_himodeahybridmonocularomnidirectionaldepthestimationmodel": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "HiMODE: A Hybrid Monocular Omnidirectional Depth Estimation Model",
    "authors": [
      "Masum Shah Junayed",
      "Arezoo Sadeghzadeh",
      "Md Baharul Islam",
      "Lai-Kuan Wong",
      "Tarkan Ayd\u0131n"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/Junayed_HiMODE_A_Hybrid_Monocular_Omnidirectional_Depth_Estimation_Model_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/Junayed_HiMODE_A_Hybrid_Monocular_Omnidirectional_Depth_Estimation_Model_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Monocular omnidirectional depth estimation is receiving considerable research attention due to its broad applications for sensing 360-degree surroundings. Existing approaches in this field suffer from limitations in recovering small object details and data lost during the ground-truth depth map acquisition. In this paper, a novel monocular omnidirectional depth estimation model, namely HiMODE is proposed based on a hybrid CNN+Transformer (encoder-decoder) architecture whose modules are efficiently designed to mitigate distortion and computational cost, without performance degradation. Firstly, we design a feature pyramid network based on the HNet block to extract high-resolution features near the edges. The performance is further improved, benefiting from a self and cross attention layer and spatial/temporal patches in the Transformer encoder and decoder, respectively. Besides, a spatial residual block is employed to reduce the number of parameters. By jointly passing the deep features extracted from an input image at each backbone block, along with the raw depth maps predicted by the transformer encoder-decoder, through a context adjustment layer, our model can produce resulting depth maps with better visual quality than the ground-truth. Comprehensive ablation studies demonstrate the significance of each individual module. Extensive experiments conducted on three datasets; Stanford3D, Matterport3D, and SunCG, demonstrate that HiMODE can achieve state-of-the-art performance for 360-degree monocular depth estimation. Complete project code and supplementary materials are available at https://github.com/himode5008/HiMODE.",
    "code_link": ""
  },
  "cvpr2022_omnicv_anewnon-centralmodelforfisheyecalibration": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "A New Non-Central Model for Fisheye Calibration",
    "authors": [
      "Radka Tezaur",
      "Avinash Kumar",
      "Oscar Nestares"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/Tezaur_A_New_Non-Central_Model_for_Fisheye_Calibration_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/Tezaur_A_New_Non-Central_Model_for_Fisheye_Calibration_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "A new non-central model suitable for calibrating fisheye cameras is proposed. It is a direct extension of the popular central model developed by Scaramuzza et al., used by Matlab Computer Vision Toolbox fisheye calibration tool. It allows adapting existing applications that are using this central model to a non-central projection that is more accurate, especially when objects captured in the images are close to the camera, and it makes it possible to switch easily between the more accurate non-central characterization of the fisheye camera and the more convenient central approximation, as needed. It is shown that the algorithms proposed by Scaramuzza et al. for their central model can be modified to accommodate the angle dependent axial viewpoint shift. This means, besides other, that a similar process can be used for calibration involving the viewpoint shift characterization and a user-friendly calibration tool can be produced with this new non-central model that does not require the user to provide detailed lens design specifications or an educated guess for the initial parameter values. Several other improvements to the Scaramuzza's central model are also introduced, helping to improve the performance of both the central model, and its non-central extension.",
    "code_link": ""
  },
  "cvpr2022_omnicv_3droomlayoutrecoverygeneralizingacrossmanhattanandnon-manhattanworlds": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "3D Room Layout Recovery Generalizing Across Manhattan and Non-Manhattan Worlds",
    "authors": [
      "Haijing Jia",
      "Hong Yi",
      "Hirochika Fujiki",
      "Hengzhi Zhang",
      "Wei Wang",
      "Makoto Odamaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/Jia_3D_Room_Layout_Recovery_Generalizing_Across_Manhattan_and_Non-Manhattan_Worlds_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/Jia_3D_Room_Layout_Recovery_Generalizing_Across_Manhattan_and_Non-Manhattan_Worlds_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "Recent 3D room layout recovery approaches mostly concentrate on Manhattan layouts, where the vertical walls are orthogonal with respect to each other, even though there are many rooms with non-Manhattan layouts in the real world. This paper presents a room layout recovery method generalizing across Manhattan and non-Manhattan worlds. Without introducing additional supervision, we extend current Manhattan layout recovery methods by predicting an extra surface normal feature, which is further used for an adaptive post-processing to reconstruct layouts of arbitrary shapes. Experimental results show that our method has a great improvement on non-Manhattan layouts while being capable of generalizing across Manhattan and non-Manhattan layouts.",
    "code_link": ""
  },
  "cvpr2022_omnicv_photometricvisualgyroscopeforfull-viewsphericalcamera": {
    "conf_id": "CVPR2022",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2022_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Photometric Visual Gyroscope for Full-View Spherical Camera",
    "authors": [
      "Antoine N. Andr\u00e9",
      "Guillaume Caron"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/html/Andre_Photometric_Visual_Gyroscope_for_Full-View_Spherical_Camera_CVPRW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2022W/OmniCV/papers/Andre_Photometric_Visual_Gyroscope_for_Full-View_Spherical_Camera_CVPRW_2022_paper.pdf",
    "published": "2022-06",
    "summary": "This article presents a visual gyroscope based on the spherical representation of dual-fisheye cameras. By using the fully available view of the scene thanks to a dual-fisheye camera and projecting the photometric information on a sphere, a highly precise orientation estimation along with a great convergence domain can be achieved. This method is validated with the help of images taken from the PanoraMIS dataset to evaluate quantitatively the performances.",
    "code_link": "https://github.com/PerceptionRobotique/libPeR_base"
  }
}