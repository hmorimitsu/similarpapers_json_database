{
  "iclr2016_main_neuralprogrammer-interpreters": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Neural Programmer-Interpreters",
    "authors": [
      "Scott Reed",
      "Nando de Freitas"
    ],
    "page_url": "http://arxiv.org/abs/1511.06279",
    "pdf_url": "https://arxiv.org/pdf/1511.06279",
    "published": "2016-05",
    "summary": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms. "
  },
  "iclr2016_main_regularizingrnnsbystabilizingactivations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Regularizing RNNs by Stabilizing Activations",
    "authors": [
      "David Krueger",
      "Roland Memisevic"
    ],
    "page_url": "http://arxiv.org/abs/1511.08400",
    "pdf_url": "https://arxiv.org/pdf/1511.08400",
    "published": "2016-05",
    "summary": "We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences. "
  },
  "iclr2016_main_blackoutspeedinguprecurrentneuralnetworklanguagemodelswithverylargevocabularies": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies",
    "authors": [
      "Shihao Ji",
      "Swaminathan Vishwanathan",
      "Nadathur Satish",
      "Michael Anderson",
      "Pradeep Dubey"
    ],
    "page_url": "http://arxiv.org/abs/1511.06909",
    "pdf_url": "https://arxiv.org/pdf/1511.06909",
    "published": "2016-05",
    "summary": "We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers. "
  },
  "iclr2016_main_thegoldilocksprinciplereadingchildrensbookswithexplicitmemoryrepresentations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations",
    "authors": [
      "Felix Hill",
      "Antoine Bordes",
      "Sumit Chopra",
      "Jason Weston"
    ],
    "page_url": "http://arxiv.org/abs/1511.02301",
    "pdf_url": "https://arxiv.org/pdf/1511.02301",
    "published": "2016-05",
    "summary": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance. "
  },
  "iclr2016_main_towardsuniversalparaphrasticsentenceembeddings": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Towards Universal Paraphrastic Sentence Embeddings",
    "authors": [
      "John Wieting",
      "Mohit Bansal",
      "Kevin Gimpel",
      "Karen Livescu"
    ],
    "page_url": "http://arxiv.org/abs/1511.08198",
    "pdf_url": "https://arxiv.org/pdf/1511.08198",
    "published": "2016-05",
    "summary": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in-domain data. However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperform LSTMs. Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use. In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification. We again find that the word averaging models perform well for sentence similarity and entailment, outperforming LSTMs. However, on sentiment classification, we find that the LSTM performs very strongly-even recording new state-of-the-art performance on the Stanford Sentiment Treebank. We then demonstrate how to combine our pretrained sentence embeddings with these supervised tasks, using them both as a prior and as a black box feature extractor. This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks. We release all of our resources to the research community with the hope that they can serve as the new baseline for further work on universal sentence embeddings. "
  },
  "iclr2016_main_convergentlearningdodifferentneuralnetworkslearnthesamerepresentations?": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Convergent Learning: Do different neural networks learn the same representations?",
    "authors": [
      "Yixuan Li",
      "Jason Yosinski",
      "Jeff Clune",
      "Hod Lipson",
      "John Hopcroft"
    ],
    "page_url": "http://arxiv.org/abs/1511.07543",
    "pdf_url": "https://arxiv.org/pdf/1511.07543",
    "published": "2016-05",
    "summary": "Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units. "
  },
  "iclr2016_main_net2netacceleratinglearningviaknowledgetransfer": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Net2Net: Accelerating Learning via Knowledge Transfer",
    "authors": [
      "Tianqi Chen",
      "Ian Goodfellow",
      "Jon Shlens"
    ],
    "page_url": "http://arxiv.org/abs/1511.05641",
    "pdf_url": "https://arxiv.org/pdf/1511.05641",
    "published": "2016-05",
    "summary": "We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset. "
  },
  "iclr2016_main_variationalgaussianprocess": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Variational Gaussian Process",
    "authors": [
      "Dustin Tran",
      "Rajesh Ranganath",
      "David Blei"
    ],
    "page_url": "http://arxiv.org/abs/1511.06499",
    "pdf_url": "https://arxiv.org/pdf/1511.06499",
    "published": "2016-05",
    "summary": "Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW. "
  },
  "iclr2016_main_thevariationalfairautoencoder": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "The Variational Fair Autoencoder",
    "authors": [
      "Christos Louizos",
      "Kevin Swersky",
      "Yujia Li",
      "Max Welling",
      "Richard Zemel"
    ],
    "page_url": "http://arxiv.org/abs/1511.00830",
    "pdf_url": "https://arxiv.org/pdf/1511.00830",
    "published": "2016-05",
    "summary": "We investigate the problem of learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational autoencoding architecture with priors that encourage independence between sensitive and latent factors of variation. Any subsequent processing, such as classification, can then be performed on this purged latent representation. To remove any remaining dependencies we incorporate an additional penalty term based on the Maximum Mean Discrepancy (MMD) measure. We discuss how these architectures can be efficiently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations. "
  },
  "iclr2016_main_anoteontheevaluationofgenerativemodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "A note on the evaluation of generative models",
    "authors": [
      "Lucas Theis",
      "A\u00e4ron van den Oord",
      "Matthias Bethge"
    ],
    "page_url": "http://arxiv.org/abs/1511.01844",
    "pdf_url": "https://arxiv.org/pdf/1511.01844",
    "published": "2016-05",
    "summary": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided. "
  },
  "iclr2016_main_deepcompressioncompressingdeepneuralnetworkswithpruning,trainedquantizationandhuffmancoding": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
    "authors": [
      "Song Han",
      "Huizi Mao",
      "Bill Dally"
    ],
    "page_url": "http://arxiv.org/abs/1510.00149",
    "pdf_url": "https://arxiv.org/pdf/1510.00149",
    "published": "2016-05",
    "summary": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce deep compression, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency. "
  },
  "iclr2016_main_neuralnetworkswithfewmultiplications": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Neural Networks with Few Multiplications",
    "authors": [
      "Zhouhan Lin",
      "Matthieu Courbariaux",
      "Roland Memisevic",
      "Yoshua Bengio"
    ],
    "page_url": "http://arxiv.org/abs/1510.03009",
    "pdf_url": "https://arxiv.org/pdf/1510.03009",
    "published": "2016-05",
    "summary": "For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks. "
  },
  "iclr2016_main_order-embeddingsofimagesandlanguage": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Order-Embeddings of Images and Language",
    "authors": [
      "Ivan Vendrov",
      "Ryan Kiros",
      "Sanja Fidler",
      "Raquel Urtasun"
    ],
    "page_url": "http://arxiv.org/abs/1511.06361",
    "pdf_url": "https://arxiv.org/pdf/1511.06361",
    "published": "2016-05",
    "summary": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval. "
  },
  "iclr2016_main_generatingimagesfromcaptionswithattention": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Generating Images from Captions with Attention",
    "authors": [
      "Elman Mansimov",
      "Emilio Parisotto",
      "Jimmy Ba",
      "Ruslan Salakhutdinov"
    ],
    "page_url": "http://arxiv.org/abs/1511.02793",
    "pdf_url": "https://arxiv.org/pdf/1511.02793",
    "published": "2016-05",
    "summary": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset. "
  },
  "iclr2016_main_densitymodelingofimagesusingageneralizednormalizationtransformation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Density Modeling of Images using a Generalized Normalization Transformation",
    "authors": [
      "Johannes Ball\u00e9",
      "Valero Laparra",
      "Eero Simoncelli"
    ],
    "page_url": "http://arxiv.org/abs/1511.06281",
    "pdf_url": "https://arxiv.org/pdf/1511.06281",
    "published": "2016-05",
    "summary": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture. "
  },
  "iclr2016_main_multi-scalecontextaggregationbydilatedconvolutions": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
    "authors": [
      "Fisher Yu",
      "Vladlen Koltun"
    ],
    "page_url": "http://arxiv.org/abs/1511.07122",
    "pdf_url": "https://arxiv.org/pdf/1511.07122",
    "published": "2016-05",
    "summary": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy. "
  },
  "iclr2016_main_learningtodiagnosewithlstmrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Learning to Diagnose with LSTM Recurrent Neural Networks",
    "authors": [
      "Zachary Lipton",
      "David Kale",
      "Charles Elkan",
      "Randall Wetzel"
    ],
    "page_url": "http://arxiv.org/abs/1511.03677",
    "pdf_url": "https://arxiv.org/pdf/1511.03677",
    "published": "2016-05",
    "summary": "Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features. "
  },
  "iclr2016_main_prioritizedexperiencereplay": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Prioritized Experience Replay",
    "authors": [
      "Tom Schaul",
      "John Quan",
      "Ioannis Antonoglou",
      "David Silver"
    ],
    "page_url": "http://arxiv.org/abs/1511.05952",
    "pdf_url": "https://arxiv.org/pdf/1511.05952",
    "published": "2016-05",
    "summary": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games. "
  },
  "iclr2016_main_importanceweightedautoencoders": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Importance Weighted Autoencoders",
    "authors": [
      "Yuri Burda",
      "Ruslan Salakhutdinov",
      "Roger Grosse"
    ],
    "page_url": "http://arxiv.org/abs/1509.00519",
    "pdf_url": "https://arxiv.org/pdf/1509.00519",
    "published": "2016-05",
    "summary": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks. "
  },
  "iclr2016_main_variationallyauto-encodeddeepgaussianprocesses": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Variationally Auto-Encoded Deep Gaussian Processes",
    "authors": [
      "Zhenwen Dai",
      "Andreas Damianou",
      "Javier Gonzalez",
      "Neil Lawrence"
    ],
    "page_url": "http://arxiv.org/abs/1511.06455",
    "pdf_url": "https://arxiv.org/pdf/1511.06455",
    "published": "2016-05",
    "summary": "We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization. "
  },
  "iclr2016_main_trainingconvolutionalneuralnetworkswithlow-rankfiltersforefficientimageclassification": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Training Convolutional Neural Networks with Low-rank Filters for Efficient Image Classification",
    "authors": [
      "Yani Ioannou",
      "Duncan Robertson",
      "Jamie Shotton",
      "roberto Cipolla",
      "Antonio Criminisi",
      "Jamie Shotton"
    ],
    "page_url": "http://arxiv.org/abs/1511.06744",
    "pdf_url": "https://arxiv.org/pdf/1511.06744",
    "published": "2016-05",
    "summary": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters. "
  },
  "iclr2016_main_reducingoverfittingindeepnetworksbydecorrelatingrepresentations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Reducing Overfitting in Deep Networks by Decorrelating Representations",
    "authors": [
      "Michael Cogswell",
      "Faruk Ahmed",
      "Ross Girshick",
      "Larry Zitnick",
      "Dhruv Batra"
    ],
    "page_url": "http://arxiv.org/abs/1511.06068",
    "pdf_url": "https://arxiv.org/pdf/1511.06068",
    "published": "2016-05",
    "summary": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout. "
  },
  "iclr2016_main_pushingtheboundariesofboundarydetectionusingdeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Pushing the Boundaries of Boundary Detection using Deep Learning",
    "authors": [
      "Iasonas Kokkinos"
    ],
    "page_url": "http://arxiv.org/abs/1511.07386",
    "pdf_url": "https://arxiv.org/pdf/1511.07386",
    "published": "2016-05",
    "summary": "In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 - while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-of-the-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second. "
  },
  "iclr2016_main_reasoningaboutentailmentwithneuralattention": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Reasoning about Entailment with Neural Attention",
    "authors": [
      "Tim Rockt\u00e4schel",
      "Edward Grefenstette",
      "Karl Moritz Hermann",
      "Tom\u00e1\u0161 Ko\u010disk\u00fd",
      "Phil Blunsom"
    ],
    "page_url": "http://arxiv.org/abs/1509.06664",
    "pdf_url": "https://arxiv.org/pdf/1509.06664",
    "published": "2016-05",
    "summary": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset. "
  },
  "iclr2016_main_convolutionalneuralnetworkswithlow-rankregularization": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Convolutional Neural Networks With Low-rank Regularization",
    "authors": [
      "Cheng Tai",
      "Tong Xiao",
      "Yi Zhang",
      "Xiaogang Wang",
      "Weinan E"
    ],
    "page_url": "http://arxiv.org/abs/1511.06067",
    "pdf_url": "https://arxiv.org/pdf/1511.06067",
    "published": "2016-05",
    "summary": "Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves $91.31\\%$ accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs. "
  },
  "iclr2016_main_unifyingdistillationandprivilegedinformation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Unifying distillation and privileged information",
    "authors": [
      "David Lopez-Paz",
      "Leon Bottou",
      "Bernhard Sch\u00f6lkopf",
      "Vladimir Vapnik"
    ],
    "page_url": "http://arxiv.org/abs/1511.03643",
    "pdf_url": "https://arxiv.org/pdf/1511.03643",
    "published": "2016-05",
    "summary": "Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data. "
  },
  "iclr2016_main_particularobjectretrievalwithintegralmax-poolingofcnnactivations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Particular object retrieval with integral max-pooling of CNN activations",
    "authors": [
      "Giorgos Tolias",
      "Ronan Sicre",
      "Herv\u00e9 J\u00e9gou"
    ],
    "page_url": "http://arxiv.org/abs/1511.05879",
    "pdf_url": "https://arxiv.org/pdf/1511.05879",
    "published": "2016-05",
    "summary": "Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets. "
  },
  "iclr2016_main_allyouneedisagoodinit": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "All you need is a good init",
    "authors": [
      "Dmytro Mishkin",
      "Jiri Matas"
    ],
    "page_url": "http://arxiv.org/abs/1511.06422",
    "pdf_url": "https://arxiv.org/pdf/1511.06422",
    "published": "2016-05",
    "summary": "Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets. "
  },
  "iclr2016_main_bayesianrepresentationlearningwithoracleconstraints": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Bayesian Representation Learning with Oracle Constraints",
    "authors": [
      "Theofanis Karaletsos",
      "Serge Belongie",
      "Gunnar R\u00e4tsch"
    ],
    "page_url": "http://arxiv.org/abs/1506.05011",
    "pdf_url": "https://arxiv.org/pdf/1506.05011",
    "published": "2016-05",
    "summary": "Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \\emph{generative unsupervised feature learning} with a \\emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables. "
  },
  "iclr2016_main_neuralprogrammerinducinglatentprogramswithgradientdescent": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent",
    "authors": [
      "Arvind Neelakantan",
      "Quoc Le",
      "Ilya Sutskever"
    ],
    "page_url": "http://arxiv.org/abs/1511.04834",
    "pdf_url": "https://arxiv.org/pdf/1511.04834",
    "published": "2016-05",
    "summary": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. "
  },
  "iclr2016_main_sparknettrainingdeepnetworksinspark": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "SparkNet: Training Deep Networks in Spark",
    "authors": [
      "Philipp Moritz",
      "Robert Nishihara",
      "Ion Stoica",
      "Michael Jordan"
    ],
    "page_url": "http://arxiv.org/abs/1511.06051",
    "pdf_url": "https://arxiv.org/pdf/1511.06051",
    "published": "2016-05",
    "summary": "Training deep networks is a time-consuming process, with networks for object recognition often requiring multiple days to train. For this reason, leveraging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster's communication overhead, and we benchmark our system's performance on the ImageNet dataset. "
  },
  "iclr2016_main_unsupervisedandsemi-supervisedlearningwithcategoricalgenerativeadversarialnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks",
    "authors": [
      "Jost Tobias Springenberg"
    ],
    "page_url": "http://arxiv.org/abs/1511.06390",
    "pdf_url": "https://arxiv.org/pdf/1511.06390",
    "published": "2016-05",
    "summary": "In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM). "
  },
  "iclr2016_main_mupropunbiasedbackpropagationforstochasticneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "MuProp: Unbiased Backpropagation For Stochastic Neural Networks",
    "authors": [
      "Shixiang Gu",
      "Sergey Levine",
      "Ilya Sutskever",
      "Andriy Mnih"
    ],
    "page_url": "http://arxiv.org/abs/1511.05176",
    "pdf_url": "https://arxiv.org/pdf/1511.05176",
    "published": "2016-05",
    "summary": "Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks. "
  },
  "iclr2016_main_datarepresentationandcompressionusinglinear-programmingapproximations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Data Representation and Compression Using Linear-Programming Approximations",
    "authors": [
      "Hristo Paskov",
      "John Mitchell",
      "Trevor Hastie"
    ],
    "page_url": "http://arxiv.org/abs/1511.06606",
    "pdf_url": "https://arxiv.org/pdf/1511.06606",
    "published": "2016-05",
    "summary": "We propose `Dracula', a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of $n$-grams that efficiently compresses a given corpus and recursively compresses its own dictionary; in effect, Dracula is a `deep' extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the compressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efficacy of Dracula's features. "
  },
  "iclr2016_main_diversitynetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Diversity Networks",
    "authors": [
      "Zelda Mariet",
      "Suvrit Sra"
    ],
    "page_url": "http://arxiv.org/abs/1511.05077",
    "pdf_url": "https://arxiv.org/pdf/1511.05077",
    "published": "2016-05",
    "summary": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches. "
  },
  "iclr2016_main_deepreinforcementlearninginparameterizedactionspace": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Deep Reinforcement Learning in Parameterized Action Space",
    "authors": [
      "Matthew Hausknecht",
      "Peter Stone"
    ],
    "page_url": "http://arxiv.org/abs/1511.04143",
    "pdf_url": "https://arxiv.org/pdf/1511.04143",
    "published": "2016-05",
    "summary": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs. "
  },
  "iclr2016_main_learningvisualpredictivemodelsofphysicsforplayingbilliards": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Learning VIsual PredictiveModels of Physics for Playing Billiards",
    "authors": [
      "Katerina Fragkiadaki",
      "Pulkit Agrawal",
      "Sergey Levine",
      "Jitendra Malik"
    ],
    "page_url": "http://arxiv.org/abs/1511.07404",
    "pdf_url": "https://arxiv.org/pdf/1511.07404",
    "published": "2016-05",
    "summary": "The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (visual imagination). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball. "
  },
  "iclr2016_main_towardsai-completequestionansweringasetofprerequisitetoytasks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
    "authors": [
      "Jason Weston",
      "Antoine Bordes",
      "Sumit Chopra",
      "Sasha Rush",
      "Bart van Merrienboer",
      "Armand Joulin",
      "Tomas Mikolov"
    ],
    "page_url": "http://arxiv.org/abs/1502.05698",
    "pdf_url": "https://arxiv.org/pdf/1502.05698",
    "published": "2016-05",
    "summary": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks. "
  },
  "iclr2016_main_evaluatingprerequisitequalitiesforlearningend-to-enddialogsystems": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems",
    "authors": [
      "Jesse Dodge",
      "Andreea Gane",
      "Xiang Zhang",
      "Antoine Bordes",
      "Sumit Chopra",
      "Alexander Miller",
      "Arthur Szlam",
      "Jason Weston"
    ],
    "page_url": "http://arxiv.org/abs/1511.06931",
    "pdf_url": "https://arxiv.org/pdf/1511.06931",
    "published": "2016-05",
    "summary": "A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering 75k movie entities and with 3.5M training examples. We present results of various models on these tasks, and evaluate their performance. "
  },
  "iclr2016_main_bettercomputergoplayerwithneuralnetworkandlong-termprediction": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Better Computer Go Player with Neural Network and Long-term Prediction",
    "authors": [
      "Yuandong Tian",
      "Yan Zhu"
    ],
    "page_url": "http://arxiv.org/abs/1511.06410",
    "pdf_url": "https://arxiv.org/pdf/1511.06410",
    "published": "2016-05",
    "summary": "Competing with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go's high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go's evaluation function could change drastically with one stone change. Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for pattern-matching approaches against MCTS-based approaches, even with looser search budgets. Against human players, the newest versions, darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a substantial improvement upon the estimated 4k-5k ranks for DCNN reported in Clark & Storkey (2015) based on games against other machine players. Adding MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000 rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al. (2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament. "
  },
  "iclr2016_main_distributionalsmoothingwithvirtualadversarialtraining": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Distributional Smoothing with Virtual Adversarial Training",
    "authors": [
      "Takeru Miyato",
      "Shin-ichi Maeda",
      "Masanori Koyama",
      "Ken Nakae",
      "Shin Ishii"
    ],
    "page_url": "http://arxiv.org/abs/1507.00677",
    "pdf_url": "https://arxiv.org/pdf/1507.00677",
    "published": "2016-05",
    "summary": "We propose local distributional smoothness (LDS), a new notion of smoothness for statistical model that can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets. "
  },
  "iclr2016_main_multi-tasksequencetosequencelearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Multi-task Sequence to Sequence Learning",
    "authors": [
      "Minh-Thang Luong",
      "Quoc Le",
      "Ilya Sutskever",
      "Oriol Vinyals",
      "Lukasz Kaiser"
    ],
    "page_url": "http://arxiv.org/abs/1511.06114",
    "pdf_url": "https://arxiv.org/pdf/1511.06114",
    "published": "2016-05",
    "summary": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought. "
  },
  "iclr2016_main_atestofrelativesimilarityformodelselectioningenerativemodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "A Test of Relative Similarity for Model Selection in Generative Models",
    "authors": [
      "Eugene Belilovsky",
      "Wacha Bounliphone",
      "Matthew Blaschko",
      "Ioannis Antonoglou",
      "Arthur Gretton"
    ],
    "page_url": "http://arxiv.org/abs/1511.04581",
    "pdf_url": "https://arxiv.org/pdf/1511.04581",
    "published": "2016-05",
    "summary": "Probabilistic generative models provide a powerful framework for representing data that avoids the expense of manual annotation typically needed by discriminative approaches. Model selection in this generative setting can be challenging, however, particularly when likelihoods are not easily accessible. To address this issue, we introduce a statistical test of relative similarity, which is used to determine which of two models generates samples that are significantly closer to a real-world reference dataset of interest. We use as our test statistic the difference in maximum mean discrepancies (MMDs) between the reference dataset and each model dataset, and derive a powerful, low-variance test based on the joint asymptotic distribution of the MMDs between each reference-model pair. In experiments on deep generative models, including the variational auto-encoder and generative moment matching network, the tests provide a meaningful ranking of model performance as a function of parameter and training settings. "
  },
  "iclr2016_main_compressionofdeepconvolutionalneuralnetworksforfastandlowpowermobileapplications": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications",
    "authors": [
      "Yong-Deok Kim",
      "Eunhyeok Park",
      "Sungjoo Yoo",
      "Taelim Choi",
      "Lu Yang",
      "Dongjun Shin"
    ],
    "page_url": "http://arxiv.org/abs/1511.06530",
    "pdf_url": "https://arxiv.org/pdf/1511.06530",
    "published": "2016-05",
    "summary": "Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme. "
  },
  "iclr2016_main_session-basedrecommendationswithrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Session-based recommendations with recurrent neural networks",
    "authors": [
      "Bal\u00e1zs Hidasi",
      "Alexandros Karatzoglou",
      "Linas Baltrunas",
      "Domonkos Tikk"
    ],
    "page_url": "http://arxiv.org/abs/1511.06939",
    "pdf_url": "https://arxiv.org/pdf/1511.06939",
    "published": "2016-05",
    "summary": "We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches. "
  },
  "iclr2016_main_continuouscontrolwithdeepreinforcementlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Continuous control with deep reinforcement learning",
    "authors": [
      "Timothy Lillicrap",
      "Jonathan Hunt",
      "Alexander Pritzel",
      "Nicolas Heess",
      "Tom Erez",
      "Yuval Tassa",
      "David Silver",
      "Daan Wierstra"
    ],
    "page_url": "http://arxiv.org/abs/1509.02971",
    "pdf_url": "https://arxiv.org/pdf/1509.02971",
    "published": "2016-05",
    "summary": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs. "
  },
  "iclr2016_main_recurrentgaussianprocesses": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Recurrent Gaussian Processes",
    "authors": [
      "C\u00e9sar Lincoln Mattos",
      "Zhenwen Dai",
      "Andreas Damianou",
      "Jeremy Forth",
      "Guilherme Barreto",
      "Neil Lawrence"
    ],
    "page_url": "http://arxiv.org/abs/1511.06644",
    "pdf_url": "https://arxiv.org/pdf/1511.06644",
    "published": "2016-05",
    "summary": "We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available. "
  },
  "iclr2016_main_modelingvisualrepresentationsdefiningpropertiesanddeepapproximations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Modeling Visual Representations:Defining Properties and Deep Approximations",
    "authors": [
      "Stefano Soatto",
      "Alessandro Chiuso"
    ],
    "page_url": "http://arxiv.org/abs/1411.7676",
    "pdf_url": "https://arxiv.org/pdf/1411.7676",
    "published": "2016-05",
    "summary": "Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization. "
  },
  "iclr2016_main_auxiliaryimageregularizationfordeepcnnswithnoisylabels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Auxiliary Image Regularization for Deep CNNs with Noisy Labels",
    "authors": [
      "Samaneh Azadi",
      "Jiashi Feng",
      "Stefanie Jegelka",
      "Trevor Darrell"
    ],
    "page_url": "http://arxiv.org/abs/1511.07069",
    "pdf_url": "https://arxiv.org/pdf/1511.07069",
    "published": "2016-05",
    "summary": "Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data. "
  },
  "iclr2016_main_policydistillation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Policy Distillation",
    "authors": [
      "Andrei Rusu",
      "Sergio Gomez",
      "Caglar Gulcehre",
      "Guillaume Desjardins",
      "James Kirkpatrick",
      "Razvan Pascanu",
      "Volodymyr Mnih",
      "Koray Kavukcuoglu",
      "Raia Hadsell"
    ],
    "page_url": "http://arxiv.org/abs/1511.06295",
    "pdf_url": "https://arxiv.org/pdf/1511.06295",
    "published": "2016-05",
    "summary": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent. "
  },
  "iclr2016_main_neuralrandom-accessmachines": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Neural Random-Access Machines",
    "authors": [
      "Karol Kurach",
      "Marcin Andrychowicz",
      "Ilya Sutskever"
    ],
    "page_url": "http://arxiv.org/abs/1511.06392",
    "pdf_url": "https://arxiv.org/pdf/1511.06392",
    "published": "2016-05",
    "summary": "In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions. "
  },
  "iclr2016_main_gatedgraphsequenceneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Gated Graph Sequence Neural Networks",
    "authors": [
      "Yujia Li",
      "Daniel Tarlow",
      "Marc Brockschmidt",
      "Richard Zemel",
      "CIFAR"
    ],
    "page_url": "http://arxiv.org/abs/1511.05493",
    "pdf_url": "https://arxiv.org/pdf/1511.05493",
    "published": "2016-05",
    "summary": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures. "
  },
  "iclr2016_main_metriclearningwithadaptivedensitydiscrimination": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Metric Learning with Adaptive Density Discrimination",
    "authors": [
      "Oren Rippel",
      "Manohar Paluri",
      "Piotr Dollar",
      "Lubomir Bourdev"
    ],
    "page_url": "http://arxiv.org/abs/1511.05939",
    "pdf_url": "https://arxiv.org/pdf/1511.05939",
    "published": "2016-05",
    "summary": "Distance metric learning (DML) approaches learn a transformation to a representation space where distance is in correspondence with a predefined notion of similarity. While such models offer a number of compelling benefits, it has been difficult for these to compete with modern classification algorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address a number of subtle yet important issues which have stymied earlier DML algorithms. It maintains an explicit model of the distributions of the different classes in representation space. It then employs this knowledge to adaptively assess similarity, and achieve local discrimination by penalizing class distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approach achieves state-of-the-art classification results on a number of fine-grained visual recognition datasets, surpassing the standard softmax classifier and outperforming triplet loss by a relative margin of 30-40%. In terms of computational performance, it alleviates training inefficiencies in the traditional triplet loss, reaching the same error in 5-30 times fewer iterations. Beyond classification, we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties, achieving 10-25% relative gains on the softmax classifier and 25-50% on triplet loss in these tasks. "
  },
  "iclr2016_main_censoringrepresentationswithanadversary": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Censoring Representations with an Adversary",
    "authors": [
      "Harrison Edwards",
      "Amos Storkey"
    ],
    "page_url": "http://arxiv.org/abs/1511.05897",
    "pdf_url": "https://arxiv.org/pdf/1511.05897",
    "published": "2016-05",
    "summary": "In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from unaligned training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model. "
  },
  "iclr2016_main_variablerateimagecompressionwithrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Variable Rate Image Compression with Recurrent Neural Networks",
    "authors": [
      "George Toderici",
      "Sean O'Malley",
      "Damien Vincent",
      "Sung Jin Hwang",
      "Michele Covell",
      "Shumeet Baluja",
      "Rahul Sukthankar",
      "David Minnen"
    ],
    "page_url": "http://arxiv.org/abs/1511.06085",
    "pdf_url": "https://arxiv.org/pdf/1511.06085",
    "published": "2016-05",
    "summary": "A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more. "
  },
  "iclr2016_main_delvingdeeperintoconvolutionalnetworksforlearningvideorepresentations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Delving Deeper into Convolutional Networks for Learning Video Representations",
    "authors": [
      "Nicolas Ballas",
      "Li Yao",
      "Pal Chris",
      "Aaron Courville"
    ],
    "page_url": "http://arxiv.org/abs/1511.06432",
    "pdf_url": "https://arxiv.org/pdf/1511.06432",
    "published": "2016-05",
    "summary": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call percepts using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features. "
  },
  "iclr2016_main_8-bitapproximationsforparallelismindeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "8-Bit Approximations for Parallelism in Deep Learning",
    "authors": [
      "Tim Dettmers"
    ],
    "page_url": "http://arxiv.org/abs/1511.04561",
    "pdf_url": "https://arxiv.org/pdf/1511.04561",
    "published": "2016-05",
    "summary": "The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs. "
  },
  "iclr2016_main_data-dependentinitializationsofconvolutionalneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Data-dependent initializations of Convolutional Neural Networks",
    "authors": [
      "Philipp Kraehenbuehl",
      "Carl Doersch",
      "Jeff Donahue",
      "Trevor Darrell"
    ],
    "page_url": "http://arxiv.org/abs/1511.06856",
    "pdf_url": "https://arxiv.org/pdf/1511.06856",
    "published": "2016-05",
    "summary": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training. "
  },
  "iclr2016_main_ordermatterssequencetosequenceforsets": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Order Matters: Sequence to sequence for sets",
    "authors": [
      "Oriol Vinyals",
      "Samy Bengio",
      "Manjunath Kudlur"
    ],
    "page_url": "http://arxiv.org/abs/1511.06391",
    "pdf_url": "https://arxiv.org/pdf/1511.06391",
    "published": "2016-05",
    "summary": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models. "
  },
  "iclr2016_main_high-dimensionalcontinuouscontrolusinggeneralizedadvantageestimation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
    "authors": [
      "John Schulman",
      "Philipp Moritz",
      "Sergey Levine",
      "Michael Jordan",
      "Pieter Abbeel"
    ],
    "page_url": "http://arxiv.org/abs/1506.02438",
    "pdf_url": "https://arxiv.org/pdf/1506.02438",
    "published": "2016-05",
    "summary": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time. "
  },
  "iclr2016_main_deepmultiscalevideopredictionbeyondmeansquareerror": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Deep Multi Scale Video Prediction Beyond Mean Square Error",
    "authors": [
      "Michael Mathieu",
      "camille couprie",
      "Yann Lecun"
    ],
    "page_url": "http://arxiv.org/abs/1511.05440",
    "pdf_url": "https://arxiv.org/pdf/1511.05440",
    "published": "2016-05",
    "summary": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset "
  },
  "iclr2016_main_gridlongshort-termmemory": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Grid Long Short-Term Memory",
    "authors": [
      "Nal Kalchbrenner",
      "Alex Graves",
      "Ivo Danihelka"
    ],
    "page_url": "http://arxiv.org/abs/1507.01526",
    "pdf_url": "https://arxiv.org/pdf/1507.01526",
    "published": "2016-05",
    "summary": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task. "
  },
  "iclr2016_main_predictingdistributionswithlinearizingbeliefnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Predicting distributions with Linearizing Belief Networks",
    "authors": [
      "Yann Dauphin",
      "David Grangier"
    ],
    "page_url": "http://arxiv.org/abs/1511.05622",
    "pdf_url": "https://arxiv.org/pdf/1511.05622",
    "published": "2016-05",
    "summary": "Conditional belief networks introduce stochastic binary variables in neural networks. Contrary to a classical neural network, a belief network can predict more than the expected value of the output $Y$ given the input $X$. It can predict a distribution of outputs $Y$ which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly relevant to inverse problems such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset. "
  },
  "iclr2016_main_fastandaccuratedeepnetworklearningbyexponentiallinearunits(elus)": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
    "authors": [
      "Djork-Arn\u00e9Clevert",
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "page_url": "http://arxiv.org/abs/1511.07289",
    "pdf_url": "https://arxiv.org/pdf/1511.07289",
    "published": "2016-05",
    "summary": "We introduce the exponential linear unit (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network. "
  },
  "iclr2016_main_actor-mimicdeepmultitaskandtransferreinforcementlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning",
    "authors": [
      "Emilio Parisotto",
      "Jimmy Ba",
      "Ruslan Salakhutdinov"
    ],
    "page_url": "http://arxiv.org/abs/1511.06342",
    "pdf_url": "https://arxiv.org/pdf/1511.06342",
    "published": "2016-05",
    "summary": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed Actor-Mimic, exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods. "
  },
  "iclr2016_main_segmentalrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Segmental Recurrent Neural Networks",
    "authors": [
      "Lingpeng Kong",
      "Chris Dyer",
      "Noah Smith"
    ],
    "page_url": "http://arxiv.org/abs/1511.06018",
    "pdf_url": "https://arxiv.org/pdf/1511.06018",
    "published": "2016-05",
    "summary": "We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these segment embeddings are used to define compatibility scores with output labels. These local compatibility scores are integrated using a global semi-Markov conditional random field. Both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward. Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies. "
  },
  "iclr2016_main_deeplineardiscriminantanalysis": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Deep Linear Discriminant Analysis",
    "authors": [
      "Matthias Dorfer",
      "Rainer Kelz",
      "Gerhard Widmer"
    ],
    "page_url": "http://arxiv.org/abs/1511.04707",
    "pdf_url": "https://arxiv.org/pdf/1511.04707",
    "published": "2016-05",
    "summary": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and CIFAR-10 and outperforms a network trained with categorical cross entropy (same architecture) on a supervised setting of STL-10. "
  },
  "iclr2016_main_large-scaleapproximatekernelcanonicalcorrelationanalysis": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Large-Scale Approximate Kernel Canonical Correlation Analysis",
    "authors": [
      "Weiran Wang",
      "Karen Livescu"
    ],
    "page_url": "http://arxiv.org/abs/1511.04773",
    "pdf_url": "https://arxiv.org/pdf/1511.04773",
    "published": "2016-05",
    "summary": "Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view representation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an $N\\times N$ eigenvalue system where $N$ is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to first transform the original inputs to an $M$-dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality $M$ of the random feature space may need to be very large in order to obtain a sufficiently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a stochastic optimization algorithm, recently proposed for linear CCA and its neural-network extension, to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with $1.4$ million training samples and a random feature space of dimensionality $M=100000$ on a typical workstation. "
  },
  "iclr2016_main_unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
    "authors": [
      "Alec Radford",
      "Luke Metz",
      "Soumith Chintala"
    ],
    "page_url": "http://arxiv.org/abs/1511.06434",
    "pdf_url": "https://arxiv.org/pdf/1511.06434",
    "published": "2016-05",
    "summary": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations. "
  },
  "iclr2016_main_learningrepresentationsfromeegwithdeeprecurrent-convolutionalneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks",
    "authors": [
      "Pouya Bashivan",
      "Irina Rish",
      "Mohammed Yeasin",
      "Noel Codella"
    ],
    "page_url": "http://arxiv.org/abs/1511.06448",
    "pdf_url": "https://arxiv.org/pdf/1511.06448",
    "published": "2016-05",
    "summary": "One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field. "
  },
  "iclr2016_main_diggingdeepintothelayersofcnnsinsearchofhowcnnsachieveviewinvariance": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance",
    "authors": [
      "Amr Bakry",
      "Mohamed Elhoseiny",
      "Tarek El-Gaaly",
      "Ahmed Elgammal"
    ],
    "page_url": "http://arxiv.org/abs/1508.01983",
    "pdf_url": "https://arxiv.org/pdf/1508.01983",
    "published": "2016-05",
    "summary": "This paper is focused on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNN). There are several questions that this paper aims to answer: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions we propose a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. We apply this methodology and report interesting results in this paper that answer the aforementioned questions. "
  },
  "iclr2016_main_anexplorationofsoftmaxalternativesbelongingtothesphericallossfamily": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family",
    "authors": [
      "Alexandre De Br\u00e9bisson",
      "Pascal Vincent"
    ],
    "page_url": "http://arxiv.org/abs/1511.05042",
    "pdf_url": "https://arxiv.org/pdf/1511.05042",
    "published": "2016-05",
    "summary": "In a multi-class classification problem, it is standard to model the output of a neural network as a categorical distribution conditioned on the inputs. The output must therefore be positive and sum to one, which is traditionally enforced by a softmax. This probabilistic mapping allows to use the maximum likelihood principle, which leads to the well-known log-softmax loss. However the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions. It is thus unclear why the log-softmax loss would perform better than other loss alternatives. In particular Vincent et al. (2015) recently introduced a class of loss functions, called the spherical family, for which there exists an efficient algorithm to compute the updates of the output weights irrespective of the output size. In this paper, we explore several loss functions from this family as possible alternatives to the traditional log-softmax. In particular, we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and the log-Taylor Softmax that we introduce. Although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks, they surprisingly outperform it in our experiments on MNIST and CIFAR-10, suggesting that they might be relevant in a broad range of applications. "
  },
  "iclr2016_main_data-dependentpathnormalizationinneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Data-Dependent Path Normalization in Neural Networks",
    "authors": [
      "Behnam Neyshabur",
      "Ryota Tomioka",
      "Ruslan Salakhutdinov",
      "Nathan Srebro"
    ],
    "page_url": "http://arxiv.org/abs/1511.06747",
    "pdf_url": "https://arxiv.org/pdf/1511.06747",
    "published": "2016-05",
    "summary": "We propose a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we investigate issue of invariance of the optimization, data dependence and the connection with natural gradients. "
  },
  "iclr2016_main_reasoninginvectorspaceanexploratorystudyofquestionanswering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Reasoning in Vector Space: An Exploratory Study of Question Answering",
    "authors": [
      "Moontae Lee",
      "Xiaodong He",
      "Wen-tau Yih",
      "Jianfeng Gao",
      "Li Deng",
      "Paul Smolensky"
    ],
    "page_url": "http://arxiv.org/abs/1511.06426",
    "pdf_url": "https://arxiv.org/pdf/1511.06426",
    "published": "2016-05",
    "summary": "Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms. "
  },
  "iclr2016_main_neuralgpuslearnalgorithms": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Neural GPUs Learn Algorithms",
    "authors": [
      "Lukasz Kaiser",
      "Ilya Sutskever"
    ],
    "page_url": "http://arxiv.org/abs/1511.08228",
    "pdf_url": "https://arxiv.org/pdf/1511.08228",
    "published": "2016-05",
    "summary": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization. "
  },
  "iclr2016_main_acdcastructuredefficientlinearlayer": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "ACDC: A Structured Efficient Linear Layer ",
    "authors": [
      "Marcin Moczulski",
      "Misha Denil",
      "Jeremy Appleyard",
      "Nando de Freitas"
    ],
    "page_url": "http://arxiv.org/abs/1511.05946",
    "pdf_url": "https://arxiv.org/pdf/1511.05946",
    "published": "2016-05",
    "summary": "The linear layer is one of the most pervasive modules in deep learning representations. However, it requires $O(N^2)$ parameters and $O(N^2)$ operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, $\\mathbf{A}$ and $\\mathbf{D}$, and the discrete cosine transform $\\mathbf{C}$. The core module, structured as $\\mathbf{ACDC^{-1}}$, has $O(N)$ parameters and incurs $O(N log N )$ operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of module. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also provides a connection between structured linear transforms used in deep learning and the field of Fourier optics, illustrating how ACDC could in principle be implemented with lenses and diffractive elements. "
  },
  "iclr2016_main_adversarialmanipulationofdeeprepresentations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Adversarial Manipulation of Deep Representations",
    "authors": [
      "Sara Sabour",
      "Yanshuai Cao",
      "Fartash Faghri",
      "David Fleet"
    ],
    "page_url": "http://arxiv.org/abs/1511.05122",
    "pdf_url": "https://arxiv.org/pdf/1511.05122",
    "published": "2016-05",
    "summary": "We show that the representation of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels, while we concentrate on the internal layers of DNN representations. In this way our new class of adversarial images differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, one from a different class, bearing little if any apparent similarity to the input; they appear generic and consistent with the space of natural images. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves. "
  },
  "iclr2016_main_geodesicsoflearnedrepresentations": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Geodesics of learned representations",
    "authors": [
      "Olivier H\u00e9naff",
      "Eero Simoncelli"
    ],
    "page_url": "http://arxiv.org/abs/1511.06394",
    "pdf_url": "https://arxiv.org/pdf/1511.06394",
    "published": "2016-05",
    "summary": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a representational geodesic). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations. "
  },
  "iclr2016_main_sequenceleveltrainingwithrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Sequence Level Training with Recurrent Neural Networks",
    "authors": [
      "Marc'Aurelio Ranzato",
      "Sumit Chopra",
      "Michael Auli",
      "Wojciech Zaremba"
    ],
    "page_url": "http://arxiv.org/abs/1511.06732",
    "pdf_url": "https://arxiv.org/pdf/1511.06732",
    "published": "2016-05",
    "summary": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster. "
  },
  "iclr2016_main_super-resolutionwithdeepconvolutionalsufficientstatistics": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICLR2016",
    "title": "Super-resolution with deep convolutional sufficient statistics",
    "authors": [
      "Joan Bruna",
      "Pablo Sprechmann",
      "Yann Lecun"
    ],
    "page_url": "http://arxiv.org/abs/1511.05666",
    "pdf_url": "https://arxiv.org/pdf/1511.05666",
    "published": "2016-05",
    "summary": "Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension. "
  },
  "iclr2016_workshop_deepmotifvisualizinggenomicsequenceclassifications": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Motif: Visualizing Genomic Sequence Classifications",
    "authors": [
      "Jack Lanchantin",
      "Ritambhara Singh",
      "Zeming Lin",
      "& Yanjun Qi"
    ],
    "page_url": "https://openreview.net/forum?id=k80kv3zjGtOYKX7ji4V7",
    "pdf_url": "https://openreview.net/pdf?id=k80kv3zjGtOYKX7ji4V7",
    "published": "2016-05",
    "summary": "This paper applies a deep convolutional/highway MLP framework to classify genomic sequences on the transcription factor binding site task. To make the model understandable, we propose an optimization driven strategy to extract \u201cmotifs\u201d, or symbolic patterns which visualize the positive class learned by the network. We show that our system, Deep Motif (DeMo), extracts motifs that are similar to, and in some cases outperform the current well known motifs. In addition, we find that a deeper model consisting of multiple convolutional and highway layers can outperform a single convolutional and fully connected layer in the previous state-of-the-art."
  },
  "iclr2016_workshop_lookaheadconvolutionlayerforunidirectionalrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks",
    "authors": [
      "Chong Wang",
      "Dani Yogatama",
      "Adam Coates",
      "Tony Han",
      "Awni Hannun",
      "Bo Xiao"
    ],
    "page_url": "https://openreview.net/forum?id=91EowxONgIkRlNvXUVog",
    "pdf_url": "https://openreview.net/pdf?id=91EowxONgIkRlNvXUVog",
    "published": "2016-05",
    "summary": "Recurrent neural networks (RNNs) have been shown to be very effective for many sequential prediction problems such as speech recognition, machine translation, part-of-speech tagging, and others. The best variant is typically a bidirectional RNN that learns representation for a sequence by performing a forward and a backward pass through the entire sequence. However, unlike unidirectional RNNs, bidirectional RNNs are challenging to deploy in an online and low-latency setting (e.g., in a speech recognition system), because they need to see an entire sequence before making a prediction. We introduce a lookahead convolution layer that incorporates information from future subsequences in a computationally efficient manner to improve unidirectional recurrent neural networks. We evaluate our method on speech recognition tasks for two languages---English and Chinese. Our experiments show that the proposed method outperforms vanilla unidirectional RNNs and is competitive with bidirectional RNNs in terms of character and word error rates."
  },
  "iclr2016_workshop_jointstochasticapproximationlearningofhelmoltzmachines": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Joint Stochastic Approximation Learning of Helmoltz Machines",
    "authors": [
      "HaotianXu",
      "Zhijian Ou"
    ],
    "page_url": "https://openreview.net/forum?id=XL9vKJ98DCXB8D1RUGV0",
    "pdf_url": "https://openreview.net/pdf?id=XL9vKJ98DCXB8D1RUGV0",
    "published": "2016-05",
    "summary": "Though with progress, model learning and performing posterior inference still re- mains a common challenge for using deep generative models, especially for han- dling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholz machines, which is characterized by pairing the genera- tive model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of al- gorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously mini- mize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA\u2019s performance is consis- tently superior to that of competing algorithms like RWS, for learning a range of difficult models."
  },
  "iclr2016_workshop_aminimalisticapproachtosum-productnetworklearningforrealapplications": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Minimalistic Approach to Sum-Product Network Learning for Real Applications",
    "authors": [
      "Viktoriya Krakovna",
      "Moshe Looks"
    ],
    "page_url": "https://openreview.net/forum?id=BNYYGWVA1F7PwR1riED4",
    "pdf_url": "https://openreview.net/pdf?id=BNYYGWVA1F7PwR1riED4",
    "published": "2016-05",
    "summary": "Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features."
  },
  "iclr2016_workshop_hardware-orientedapproximationofconvolutionalneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Hardware-Oriented Approximation of Convolutional Neural Networks",
    "authors": [
      "Philipp Gysel",
      "Mohammad Motamedi",
      "Soheil Ghiasi"
    ],
    "page_url": "https://openreview.net/forum?id=81DnLL9OEI6O2Pl0UV1w",
    "pdf_url": "https://openreview.net/pdf?id=81DnLL9OEI6O2Pl0UV1w",
    "published": "2016-05",
    "summary": "High computational complexity hinders the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile devices. Hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. One of the most important steps in accelerator development is hardware-oriented model approximation. In this paper we present Ristretto, a model approximation framework that analyzes a given CNN with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. Ristretto can condense models by using fixed point arithmetic and representation instead of floating point. Moreover, Ristretto fine-tunes the resulting fixed point network. Given a maximum error tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available."
  },
  "iclr2016_workshop_neurogenicdeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neurogenic Deep Learning",
    "authors": [
      "Timothy J. Draelos",
      "Nadine E. Miner",
      "Jonathan A. Cox",
      "Christopher C. Lamb",
      "Conrad D. James",
      "James B. Aimone"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xQwqKZh5Pk8ELfEzD",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xQwqKZh5Pk8ELfEzD",
    "published": "2016-05",
    "summary": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. "
  },
  "iclr2016_workshop_deepbayesianneuralnetsasdeepmatrixgaussianprocesses": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes",
    "authors": [
      "Christos Louizos",
      "Max Welling"
    ],
    "page_url": "https://openreview.net/forum?id=wVqzjWP0JfG0qV7mtLvp",
    "pdf_url": "https://openreview.net/pdf?id=wVqzjWP0JfG0qV7mtLvp",
    "published": "2016-05",
    "summary": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. "
  },
  "iclr2016_workshop_neuralnetworktrainingvariationsinspeechandsubsequentperformanceevaluation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Network Training Variations in Speech and Subsequent Performance Evaluation",
    "authors": [
      "Ewout van den Berg",
      "Bhuvana Ramabhadran",
      "Michael Picheny"
    ],
    "page_url": "https://openreview.net/forum?id=OM0jKROjrFp57ZJjtNkv",
    "pdf_url": "https://openreview.net/pdf?id=OM0jKROjrFp57ZJjtNkv",
    "published": "2016-05",
    "summary": "In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These observations have important implications on way results in the literature are reported and interpreted. "
  },
  "iclr2016_workshop_neuralvariationalrandomfieldlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Variational Random Field Learning",
    "authors": [
      "Volodymyr Kuleshov",
      "Stefano Ermon"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9x1mJ3zS5Pk8ELfEjD",
    "pdf_url": "https://openreview.net/pdf?id=ZY9x1mJ3zS5Pk8ELfEjD",
    "published": "2016-05",
    "summary": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning."
  },
  "iclr2016_workshop_improvingvariationalinferencewithinverseautoregressiveflow": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Improving Variational Inference with Inverse Autoregressive Flow",
    "authors": [
      "Diederik P. Kingma",
      "Tim Salimans",
      "Max Welling"
    ],
    "page_url": "https://openreview.net/forum?id=ANYzpXg3LcNrwlgXCq9G",
    "pdf_url": "https://openreview.net/pdf?id=ANYzpXg3LcNrwlgXCq9G",
    "published": "2016-05",
    "summary": "We propose a simple and practical method for improving the flexibility of the approximate posterior in variational auto-encoders (VAEs) through a transformation with autoregressive networks.Autoregressive networks, such as RNNs and RNADE networks, are very powerful models. However, their sequential nature makes them impractical for direct use with VAEs, as sequentially sampling the latent variables is slow when implemented on a GPU. Fortunately, we find that by inverting autoregressive networks we can obtain equally powerful data transformations that can be computed in parallel. We call these data transformations inverse autoregressive flows (IAF), and we show that they can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is computationally cheap, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series.The method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images."
  },
  "iclr2016_workshop_learninggenomicrepresentationstopredictclinicaloutcomesincancer": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Genomic Representations to Predict Clinical Outcomes in Cancer",
    "authors": [
      "Safoora Yousefi",
      "Congzheng Song",
      "Nelson Nauata",
      "Lee Cooper"
    ],
    "page_url": "https://openreview.net/forum?id=xnrAg7jmLF1m7RyVi3vG",
    "pdf_url": "https://openreview.net/pdf?id=xnrAg7jmLF1m7RyVi3vG",
    "published": "2016-05",
    "summary": "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data."
  },
  "iclr2016_workshop_understandingverydeepnetworksviavolumeconservation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Understanding Very Deep Networks via Volume Conservation",
    "authors": [
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "page_url": "https://openreview.net/forum?id=ROVmN8wyOSvnM0J1IpNm",
    "pdf_url": "https://openreview.net/pdf?id=ROVmN8wyOSvnM0J1IpNm",
    "published": "2016-05",
    "summary": "Recently, very deep neural networks set new records across many application domains, like Residual Networks at the ImageNet challenge and Highway Networks at language processing tasks. We expect further excellent performance improvements in different fields from these very deep networks. However these networks are still poorly understood, especially since they rely on non-standard architectures. In this contribution we analyze the learning dynamics which are required for successfully training very deep neural networks. For the analysis we use a symplectic network architecture which inherently conserves volume when mapping a representation from one to the next layer. Therefore it avoids the vanishing gradient problem, which in turn allows to effectively train thousands of layers. We consider highway and residual networks as well as the LSTM model, all of which have approximately volume conserving mappings. We identified two important factors for making deep architectures working: (1) (near) volume conserving mappings through $x = x + f(x)$ or similar (cf.\\ avoiding the vanishing gradient); (2) Controlling the drift effect, which increases/decreases $x$ during propagation toward the output (cf.\\ avoiding bias shifts);"
  },
  "iclr2016_workshop_fixedpointquantizationofdeepconvolutionalnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Fixed Point Quantization of Deep Convolutional Networks",
    "authors": [
      "Darryl D. Lin",
      "Sachin S. Talathi",
      "V. Sreekanth Annapureddy"
    ],
    "page_url": "https://openreview.net/forum?id=yovBjmpo1ur682gwszM7",
    "pdf_url": "https://openreview.net/pdf?id=yovBjmpo1ur682gwszM7",
    "published": "2016-05",
    "summary": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we formulate and solve an optimization problem to identify the optimal fixed point bit-width allocation across layers to enable efficient fixed point implementation of DCNs. Our experiments show that in comparison to equal bit-width settings, optimized bit-width allocation offers >20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark."
  },
  "iclr2016_workshop_cma-esforhyperparameteroptimizationofdeepneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks",
    "authors": [
      "Ilya Loshchilov",
      "Frank Hutter"
    ],
    "page_url": "https://openreview.net/forum?id=xnrA4qzmPu1m7RyVi38Z",
    "pdf_url": "https://openreview.net/pdf?id=xnrA4qzmPu1m7RyVi38Z",
    "published": "2016-05",
    "summary": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization.As an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. "
  },
  "iclr2016_workshop_understandingvisualconceptswithcontinuationlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Understanding Visual Concepts with Continuation Learning",
    "authors": [
      "William F. Whitney",
      "Michael Chang",
      "Tejas Kulkarni",
      "Joshua B. Tenenbaum"
    ],
    "page_url": "https://openreview.net/forum?id=r8lrDJ89Pf8wknpYt5zq",
    "pdf_url": "https://openreview.net/pdf?id=r8lrDJ89Pf8wknpYt5zq",
    "published": "2016-05",
    "summary": "We introduce a neural network architecture and a learning algorithm to produce fac- torized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games."
  },
  "iclr2016_workshop_input-convexdeepnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Input-Convex Deep Networks",
    "authors": [
      "(moved to May 3rd)"
    ],
    "page_url": "https://openreview.net/forum?id=ROVmA279BsvnM0J1IpNn",
    "pdf_url": "https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn",
    "published": "2016-05",
    "summary": "This paper introduces a new class of neural networks that we refer to as input-convex neural networks, networks that are convex in their inputs (as opposed to their parameters).We discuss the nature and representational power of these networks, illustrate how the prediction (inference) problem can be solved via convex optimization, and discuss their application to structured prediction problems.We highlight a few simple examples of these networks applied to classification tasks, where we illustrate that the networks perform substantially better than any other approximator we are aware of that is convex in its inputs."
  },
  "iclr2016_workshop_learningtosmile(s)": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning to SMILE(S)",
    "authors": [
      "Stanis\u0142\u0082aw Jastrz\u0119bski",
      "Damian Le\u015bniak",
      "Wojciech Marian Czarnecki"
    ],
    "page_url": "https://openreview.net/forum?id=VAVqG11WmSx0Wk76TAzp",
    "pdf_url": "https://openreview.net/pdf?id=VAVqG11WmSx0Wk76TAzp",
    "published": "2016-05",
    "summary": "This paper shows how one can directly apply natural language processing (NLP) methods to classification problems in cheminformatics.Connection between these seemingly separate fields is shown by considering standard textual representation of compound, SMILES.The problem of activity prediction against a target protein is considered, which is a crucial part of computer aided drug design process.Conducted experiments show that this way one can not only outrank state of the art results of hand crafted representations but also gets direct structural insights into the way decisions are made."
  },
  "iclr2016_workshop_learningretinaltilinginamodelofvisualattention": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Retinal Tiling in a Model of Visual Attention",
    "authors": [
      "Brian Cheung",
      "Eric Weiss",
      "Bruno Olshausen"
    ],
    "page_url": "https://openreview.net/forum?id=1WvOZJ0yDTMnPB1oinGN",
    "pdf_url": "https://openreview.net/pdf?id=1WvOZJ0yDTMnPB1oinGN",
    "published": "2016-05",
    "summary": "We describe a neural network model in which the tiling of the input array is learned by performing a joint localization and classification task. After training, the optimal tiling that emerges resembles the eccentricity dependent tiling of the human retina."
  },
  "iclr2016_workshop_hardware-friendlyconvolutionalneuralnetworkwitheven-numberfiltersize": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Hardware-Friendly Convolutional Neural Network with Even-Number Filter Size",
    "authors": [
      "Song Yao",
      "Song Han",
      "Kaiyuan Guo",
      "Jianqiao Wangni",
      "Yu Wang"
    ],
    "page_url": "https://openreview.net/forum?id=k80kn82ywfOYKX7ji42O",
    "pdf_url": "https://openreview.net/pdf?id=k80kn82ywfOYKX7ji42O",
    "published": "2016-05",
    "summary": "Convolutional Neural Network (CNN) has led to great advances in computer vision. Various customized CNN accelerators on embedded FPGA or ASIC platforms have been designed to accelerate CNN and improve energy efficiency. However, the odd-number filter size in existing CNN models prevents hardware accelerators from having optimal efficiency. In this paper, we analyze the influences of filter size on CNN accelerator performance and show that even-number filter size is much more hardware-friendly that can ensure high bandwidth and resource utilization. Experimental results on MNIST and CIFAR-10 demonstrate that hardware-friendly even kernel CNNs can reduce the FLOPs by 1.4x to 2x with comparable accuracy; With same FLOPs, even kernel can have even higher accuracy than odd size kernel."
  },
  "iclr2016_workshop_dodeepconvolutionalnetsreallyneedtobedeep(orevenconvolutional)?": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?",
    "authors": [
      "Gregor Urban",
      "Krzysztof J. Geras",
      "Samira Ebrahimi Kahou",
      "Ozlem Aslan Shengjie Wang",
      "Rich Caruana",
      "Abdelrahman Mohamed",
      "Matthai Philipose",
      "Matt Richardson"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOrG6lVsRNGwArs4qo",
    "pdf_url": "https://openreview.net/pdf?id=L7VOrG6lVsRNGwArs4qo",
    "published": "2016-05",
    "summary": "Yes, apparently they do.Previous research by Ba and Caruana (2014) demonstrated that shallow feed-forward nets sometimes can learn the complex functions pre- viously learned by deep nets while using a simi- lar number of parameters as the deep models they mimic. In this paper we investigate if shallow models can learn to mimic the functions learned by deep convolutional models. We experiment with shallow models and models with a vary- ing number of convolutional layers, all trained to mimic a state-of-the-art ensemble of CIFAR-10 models. We demonstrate that we are unable to train shallow models to be of comparable accu- racy to deep convolutional models. Although the student models do not have to be as deep as the teacher models they mimic, the student models apparently need multiple convolutional layers to learn functions of comparable accuracy. "
  },
  "iclr2016_workshop_generativeadversarialmetric": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Generative Adversarial Metric",
    "authors": [
      "Daniel Jiwoong Im",
      "Chris Dongjoo Kim",
      "Hui Jiang",
      "Roland Memisevic"
    ],
    "page_url": "https://openreview.net/forum?id=wVqzLo88YsG0qV7mtLq7",
    "pdf_url": "https://openreview.net/pdf?id=wVqzLo88YsG0qV7mtLq7",
    "published": "2016-05",
    "summary": "We introduced a new metric for comparing adversarial networks quantitatively. "
  },
  "iclr2016_workshop_revisesaturatedactivationfunctions": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Revise Saturated Activation Functions",
    "authors": [
      "Bing Xu",
      "Ruitong Huang",
      "Mu Li"
    ],
    "page_url": "https://openreview.net/forum?id=D1VDjyJjXF5jEJ1zfE53",
    "pdf_url": "https://openreview.net/pdf?id=D1VDjyJjXF5jEJ1zfE53",
    "published": "2016-05",
    "summary": "In this paper, we revise two commonly used saturated functions, the logistic sigmoid and the hyperbolic tangent (tanh).We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible reason making training deep networks with the logistic function difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid achieves comparable results with tanh.Then following the same argument, we improve tahn by penalizing in the negative part.We show that ``penalized tanh'' is comparable and even outperforms the state-of-the-art non-saturated functions including ReLU and leaky ReLU on deep convolution neural networks.Our results contradict to the conclusion of previous works that the saturation property causes the slow convergence. It suggests further investigation is necessary to better understand activation functions in deep architectures."
  },
  "iclr2016_workshop_multi-layerrepresentationlearningformedicalconcepts": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Multi-layer Representation Learning for Medical Concepts",
    "authors": [
      "Edward Choi",
      "Mohammad Taha Bahadori",
      "Jimeng Sun",
      "Elizabeth Searles",
      "Catherine Coffey"
    ],
    "page_url": "https://openreview.net/forum?id=mO9mQWp8Rij1gPZ3Ul5q",
    "pdf_url": "https://openreview.net/pdf?id=mO9mQWp8Rij1gPZ3Ul5q",
    "published": "2016-05",
    "summary": "Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification.Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit.This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit.In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation."
  },
  "iclr2016_workshop_alternativestructuresforcharacter-levelrnns": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Alternative structures for character-level RNNs",
    "authors": [
      "Piotr Bojanowski",
      "Armand Joulin",
      "Tomas Mikolov"
    ],
    "page_url": "https://openreview.net/forum?id=wVqzL1ypocG0qV7mtLqm",
    "pdf_url": "https://openreview.net/pdf?id=wVqzL1ypocG0qV7mtLqm",
    "published": "2016-05",
    "summary": "Recurrent neural networks are convenient and efficient models for learning patterns in sequential data. However, when applied to signals with very low cardinality such as character-level language modeling, they suffer from several problems. In order to success- fully model longer-term dependencies, the hidden layer needs to be large, which in turn implies high computational cost. Moreover, the accuracy of these models is significantly lower than that of baseline word-level models. We propose two structural modifications of the classic RNN LM architecture. The first one consists on conditioning the RNN both on the character-level and word-level information. The other one uses the recent history to condition the computation of the output probability. We evaluate the performance of the two proposed modifications on multi-lingual data. The experiments show that both modifications can improve upon the basic RNN architecture, which is even more visible in cases when the input and output signals are represented by single bits. These findings suggest that more research needs to be done to develop general RNN architecture that would perform optimally across wide range of tasks."
  },
  "iclr2016_workshop_inception-v4,inception-resnetandtheimpactofresidualconnectionsonlearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
    "authors": [
      "Christian Szegedy",
      "Sergey Ioffe",
      "Vincent Vanhoucke"
    ],
    "page_url": "https://openreview.net/forum?id=q7kqBkL33f8LEkD3t7X9",
    "pdf_url": "https://openreview.net/pdf?id=q7kqBkL33f8LEkD3t7X9",
    "published": "2016-05",
    "summary": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly, however, when fully trained, the final quality of the non-residual Inception variants seem to be close to those of residual versions. We present several new streamlined architectures for both residual and non-residual Inception networks. With an ensemble of three residual and one pure Inception-v4, we achieve 3.08\\% top-5 error on the test set of the ImageNet classification (CLS) challenge"
  },
  "iclr2016_workshop_revisitingdistributedsynchronoussgd": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Revisiting Distributed Synchronous SGD",
    "authors": [
      "Jianmin Chen",
      "Rajat Monga",
      "Samy Bengio",
      "Rafal Jozefowicz"
    ],
    "page_url": "https://openreview.net/forum?id=D1VDZ5kMAu5jEJ1zfEWL",
    "pdf_url": "https://openreview.net/pdf?id=D1VDZ5kMAu5jEJ1zfEWL",
    "published": "2016-05",
    "summary": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. Previous works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently."
  },
  "iclr2016_workshop_adifferentiabletransitionbetweenadditiveandmultiplicativeneurons": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Differentiable Transition Between Additive and Multiplicative Neurons",
    "authors": [
      "Wiebke Koepp",
      "Patrick van der Smagt",
      "Sebastian Urban"
    ],
    "page_url": "https://openreview.net/forum?id=MwVPvKwRvsqxwkg1t7kY",
    "pdf_url": "https://openreview.net/pdf?id=MwVPvKwRvsqxwkg1t7kY",
    "published": "2016-05",
    "summary": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure.We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure."
  },
  "iclr2016_workshop_deepautoresolutionnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Autoresolution Networks",
    "authors": [
      "Gabriel Pereyra",
      "Christian Szegedy"
    ],
    "page_url": "https://openreview.net/forum?id=E8VEozRYyi31v0m2iDwy",
    "pdf_url": "https://openreview.net/pdf?id=E8VEozRYyi31v0m2iDwy",
    "published": "2016-05",
    "summary": "Despite the success of very deep convolutional neural networks, they currently operate at very low resolutions relative to modern cameras. Visual attention mechanisms address this by allowing models to access higher resolutions only when necessary. However, in certain cases, this higher resolution isn\u2019t available. We show that autoresolution networks, which learn correspondences between lowresolution and high-resolution images, learn representations that improve lowresolution classification - without needing labeled high-resolution images."
  },
  "iclr2016_workshop_unsupervisedlearningwithimbalanceddataviastructureconsolidationlatentvariablemodel": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model",
    "authors": [
      "Fariba Yousefi",
      "Zhenwen Dai",
      "Carl Henrik Ek",
      "Neil Lawrence"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xMOwxPf5Pk8ELfEjV",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xMOwxPf5Pk8ELfEjV",
    "published": "2016-05",
    "summary": "Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current model is often dominated by the major category and ignores the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derive an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset."
  },
  "iclr2016_workshop_robustconvolutionalneuralnetworksunderadversarialnoise": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Robust Convolutional Neural Networks under Adversarial Noise",
    "authors": [
      "Jonghoon Jin",
      "Aysegul Dundar",
      "Eugenio Culurciello"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOOy8B6hRNGwArs4Bn",
    "pdf_url": "https://openreview.net/pdf?id=L7VOOy8B6hRNGwArs4Bn",
    "published": "2016-05",
    "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called adversarial examples. In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise."
  },
  "iclr2016_workshop_gradnetsdynamicinterpolationbetweenneuralarchitectures": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "GradNets: Dynamic Interpolation Between Neural Architectures",
    "authors": [
      "Diogo Almeida",
      "Nate Sauder"
    ],
    "page_url": "https://openreview.net/forum?id=Qn8lE8x17fkB2l8pUYPk",
    "pdf_url": "https://openreview.net/pdf?id=Qn8lE8x17fkB2l8pUYPk",
    "published": "2016-05",
    "summary": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers."
  },
  "iclr2016_workshop_resnetinresnetgeneralizingresidualarchitectures": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Resnet in Resnet: Generalizing Residual Architectures",
    "authors": [
      "Sasha Targ",
      "Diogo Almeida",
      "Kevin Lyman"
    ],
    "page_url": "https://openreview.net/forum?id=lx9l4r36gU2OVPy8Cv9g",
    "pdf_url": "https://openreview.net/pdf?id=lx9l4r36gU2OVPy8Cv9g",
    "published": "2016-05",
    "summary": "ResNets have recently achieved state-of-the-art results on challenging computer vision tasks. In this paper, we create a novel architecture that improves ResNets by adding the ability to forget and by making the residuals more expressive, yielding excellent results. ResNet in ResNet outperforms architectures with similar amounts of augmentation on CIFAR-10 and establishes a new state-of-the-art on CIFAR-100."
  },
  "iclr2016_workshop_doctoraipredictingclinicaleventsviarecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks",
    "authors": [
      "Edward Choi",
      "Mohammad Taha Bahadori",
      "Andy Schuetz",
      "Walter F. Stewart",
      "Joshua C. Denny",
      "Bradley A. Malin",
      "Jimeng Sun"
    ],
    "page_url": "https://openreview.net/forum?id=zvwDjZ3GDfM8kw3ZinXB",
    "pdf_url": "https://openreview.net/pdf?id=zvwDjZ3GDfM8kw3ZinXB",
    "published": "2016-05",
    "summary": "Large amount ofElectronic Health Record (EHR) data have been collected over millions of patients over multiple years. The rich longitudinal EHR data documented the collective experiences of physicians including diagnosis, medication prescription and procedures. We argue it is possible now to leverage the EHR data to model how physicians behave, and we call our model Doctor AI.Towards this direction of modeling clinical behavior of physicians,we develop a successful application of Recurrent Neural Networks (RNN) to jointly forecast the future disease diagnosis and medication prescription along with their timing. Unlike traditional classification models where a single target is of interest, our model can assess the entire history of patients and make continuous and multilabel predictions based on patients' historical data. We evaluate the performance of the proposed method on a large real-world EHR data over 260K patients over 8 years.We observed Doctor AI can perform differential diagnosis with similar accuracy to physicians. In particular, Doctor AI achieves up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by applying the resulting models on data from a completely different medication institution achieving comparable performance."
  },
  "iclr2016_workshop_on-the-flynetworkpruningforobjectdetection": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "On-the-fly Network Pruning for Object Detection",
    "authors": [
      "Marc Masana",
      "Joost van de Weijer",
      "Andrew D. Bagdanov"
    ],
    "page_url": "https://openreview.net/forum?id=p8jp5lzPWSnQVOGWfpDD",
    "pdf_url": "https://openreview.net/pdf?id=p8jp5lzPWSnQVOGWfpDD",
    "published": "2016-05",
    "summary": "Object detection with deep neural networks is often performed by passing a few thousand candidate bounding boxes through a deep neural network for each image. These bounding boxes are highly correlated since they originate from the same image. In this paper we investigate how to exploit featureoccurrence at the image scale to prune the neural network which is subsequently applied to all bounding boxes. We show that removing units which have near-zero activation in the image allows us to significantly reduce the number of parameters in the network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that up to 40% of units in some fully-connected layers can be entirely eliminated with little change in the detection result."
  },
  "iclr2016_workshop_deepdirectedgenerativemodelswithenergy-basedprobabilityestimation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Directed Generative Models with Energy-Based Probability Estimation",
    "authors": [
      "Taesup Kim",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=BNYAGZZj5S7PwR1riXzA",
    "pdf_url": "https://openreview.net/pdf?id=BNYAGZZj5S7PwR1riXzA",
    "published": "2016-05",
    "summary": "Energy-based probabilistic models have been confronted with intractable computations during the learning that requires to have appropriate samples drawn from the estimated probability distribution. It can be approximately achieved by a Monte Carlo Markov Chain sampling process, but still has mixing problems especially with deep models that slow the learning. We introduce an auxiliary deep model that deterministically generates samples based on the estimated distribution, and this makes the learning easier without any high cost sampling process. As a result, we propose a new framework to train the energy-based probabilistic models with two separate deep feed-forward models. The one is only to estimate the energy function, and the another is to deterministically generate samples based on it. Consequently, we can estimate the probability distribution and its corresponding deterministic generator with deep models."
  },
  "iclr2016_workshop_rectifiedfactornetworksforbiclustering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Rectified Factor Networks for Biclustering",
    "authors": [
      "Djork-Arn\u00e9 Clevert",
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "page_url": "https://openreview.net/forum?id=mO9m42yWgSj1gPZ3UlGA",
    "pdf_url": "https://openreview.net/pdf?id=mO9m42yWgSj1gPZ3UlGA",
    "published": "2016-05",
    "summary": "Biclustering is evolving into one of the major tools for analyzing large datasets given as matrix of samples times features. Biclustering has been successfully applied in life sciences, e.g. for drug design, in e-commerce, e.g. for internet retailing or recommender systems.FABIA is one of the most successful biclustering methods which excelled in different projects and is used by companies like Janssen, Bayer, or Zalando. FABIA is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated. Sample membership is difficult to determine because vectors do not have exact zero entries and can have both large positive and large negative values.We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of FABIA. RFNs efficiently construct very sparse, non-linear, high-dimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster.On 400 benchmark datasets with artificially implanted biclusters, RFN significantly outperformed 13 other biclustering competitors including FABIA. In biclustering experiments on three gene expression datasets with known clusters that were determined by separate measurements, RFN biclustering was two times significantly better than the other 13 methods and once on second place."
  },
  "iclr2016_workshop_randomoutusingaconvolutionalgradientnormtowinthefilterlottery": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "RandomOut: Using a convolutional gradient norm to win The Filter Lottery",
    "authors": [
      "Joseph Paul Cohen",
      "Henry Z. Lo",
      "Wei Ding"
    ],
    "page_url": "https://openreview.net/forum?id=2xwPmERVBtpKBZvXtQnD",
    "pdf_url": "https://openreview.net/pdf?id=2xwPmERVBtpKBZvXtQnD",
    "published": "2016-05",
    "summary": "Convolutional neural networks are sensitive to the random initialization of filters. We call this The Filter Lottery (TFL) because the random numbers used to initialize the network determine if you will ``win'' and converge to a satisfactory local minimum. This issue forces networks to contain more filters (be wider) to achieve higher accuracy because they have better odds of being transformed into highly discriminative features at the risk of introducing redundant features. To deal with this, we propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold.This consistently improves accuracy across two datasets by up to 1.8%. Our scheme RandomOut allows us to increase the number of filters explored without increasing the size of the network. This yields more compact networks which can train and predict with less computation, thus allowing more powerful CNNs to run on mobile devices. "
  },
  "iclr2016_workshop_persistentrnnsstashingweightsonchip": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Persistent RNNs: Stashing Weights on Chip",
    "authors": [
      "Greg Diamos",
      "Shubho Sengupta",
      "Bryan Catanzaro",
      "Mike Chrzanowski",
      "Adam Coates",
      "Erich Elsen",
      "Jesse Engel",
      "Awni Hannun",
      "Sanjeev Satheesh"
    ],
    "page_url": "https://openreview.net/forum?id=XL9v5ZZ2qtXB8D1RUG6V",
    "pdf_url": "https://openreview.net/pdf?id=XL9v5ZZ2qtXB8D1RUG6V",
    "published": "2016-05",
    "summary": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers."
  },
  "iclr2016_workshop_scalenormalization": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Scale Normalization",
    "authors": [
      "Henry Z Lo",
      "Kevin Amaral",
      "Wei Ding"
    ],
    "page_url": "https://openreview.net/forum?id=OM0jjYW3BHp57ZJjtNEO",
    "pdf_url": "https://openreview.net/pdf?id=OM0jjYW3BHp57ZJjtNEO",
    "published": "2016-05",
    "summary": "One of the difficulties of training deep neural networks is caused by improper scaling between layers.These scaling issues introduce exploding / gradient problems, and have typically been addressed by careful variance-preserving initialization.We consider this problem as one of preserving scale, rather than preserving variance.This leads to a simple method of scale-normalizing weight layers, which ensures that scale is approximately maintained between layers.Our method of scale-preservation ensures that forward propagation is impacted minimally, while backward passes maintain gradient scales.Preliminary experiments show that scale normalization effectively speeds up learning, without introducing additional hyperparameters or parameters. "
  },
  "iclr2016_workshop_close-to-cleanregularizationrelatesvirtualadversarialtraining,laddernetworksandothers": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others",
    "authors": [
      "Mudassar Abbas",
      "Jyri Kivinen",
      "Tapani Raiko"
    ],
    "page_url": "https://openreview.net/forum?id=6XAwLR8gysrVp0EvsEW3",
    "pdf_url": "https://openreview.net/pdf?id=6XAwLR8gysrVp0EvsEW3",
    "published": "2016-05",
    "summary": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods."
  },
  "iclr2016_workshop_guidedsequence-to-sequencelearningwithexternalrulememory": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Guided Sequence-to-Sequence Learning with External Rule Memory",
    "authors": [
      "Jiatao Gu",
      "Baotian Hu",
      "Zhengdong Lu",
      "Hang Li",
      "Victor O.K. Li"
    ],
    "page_url": "https://openreview.net/forum?id=BNYAA7gNBi7PwR1riXzR",
    "pdf_url": "https://openreview.net/pdf?id=BNYAA7gNBi7PwR1riXzR",
    "published": "2016-05",
    "summary": "External memory has been proven to be essential for the success of neural network-based systems on many tasks, including Question-Answering, classification, machine translation and reasoning. In all those models the memory is used to store instance representations of multiple levels, analogous to \u201cdata\u201d in the Von Neumann architecture of a computer, while the \u201cinstructions\u201d are stored in the weights. In this paper, we however propose to use the memory for storing part of the instructions, and more specifically, the transformation rules in sequence-to-sequence learning tasks, in an external memory attached to a neural system. This memory can be accessed both by the neural network and by the human experts, hence serving as an interface for a novel learning paradigm where not only the instances but also the rule can be taught to the neural network. Our empirical study on a synthetic but challenging dataset verifies that our model is effective."
  },
  "iclr2016_workshop_neuraltextunderstandingwithattentionsumreader": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Text Understanding with Attention Sum Reader",
    "authors": [
      "Rudolf Kadlec",
      "Martin Schmid",
      "Ond\u0159\u0099ej Bajgar",
      "Jan Kleindienst"
    ],
    "page_url": "https://openreview.net/forum?id=Qn8lxPngJFkB2l8pUYxg",
    "pdf_url": "https://openreview.net/pdf?id=Qn8lxPngJFkB2l8pUYxg",
    "published": "2016-05",
    "summary": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test. Thanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches. We present a new, simple model that is tailor made for such question-answering problems. Our model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings. Our model outperforms models previously proposed for these tasks by a large margin."
  },
  "iclr2016_workshop_incorporatingnesterovmomentumintoadam": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Incorporating Nesterov Momentum into Adam",
    "authors": [
      "Timothy Dozat"
    ],
    "page_url": "https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ",
    "pdf_url": "https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ",
    "published": "2016-05",
    "summary": "This work aims to improve upon the recently proposed and rapidly popular- ized optimization algorithm Adam (Kingma & Ba, 2014). Adam has two main components\u2014a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in- ferior to a similar algorithm known as Nesterov\u2019s accelerated gradient (NAG). We show how to modify Adam\u2019s momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod- els. "
  },
  "iclr2016_workshop_variationalinferenceforon-lineanomalydetectioninhigh-dimensionaltimeseries": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series",
    "authors": [
      "Maximilian S\u00f6lch",
      "Justin Bayer",
      "Marvin Ludersdorfer",
      "Patrick van der Smagt"
    ],
    "page_url": "https://openreview.net/forum?id=oVgoWpz5LsrlgPMRsB1v",
    "pdf_url": "https://openreview.net/pdf?id=oVgoWpz5LsrlgPMRsB1v",
    "published": "2016-05",
    "summary": "Approximate variational inference has shown to be a powerful tool for modeling unknown, complex probability distributions. Recent advances in the field allow us to learn probabilistic sequence models. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line."
  },
  "iclr2016_workshop_sequence-to-sequencernnsfortextsummarization": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Sequence-to-Sequence RNNs for Text Summarization",
    "authors": [
      "Ramesh Nallapati",
      "Bing Xiang",
      "Bowen Zhou"
    ],
    "page_url": "https://openreview.net/forum?id=gZ9OMgQWoIAPowrRUAN6",
    "pdf_url": "https://openreview.net/pdf?id=gZ9OMgQWoIAPowrRUAN6",
    "published": "2016-05",
    "summary": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation. Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et. al. (2015), on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance. "
  },
  "iclr2016_workshop_neuralgenerativequestionanswering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Generative Question Answering",
    "authors": [
      "Jun Yin",
      "Xin Jiang",
      "Zhengdong Lu",
      "Lifeng Shang",
      "Hang Li",
      "Xiaoming Li"
    ],
    "page_url": "https://openreview.net/forum?id=OM0vWYM7Eup57ZJjtNql",
    "pdf_url": "https://openreview.net/pdf?id=OM0vWYM7Eup57ZJjtNql",
    "published": "2016-05",
    "summary": "This paper presents an end-to-end neural network model, named Neural Generative Question Answering (genQA), that can generate answers to simple factoid questions, both in natural language. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to access an embedded knowledge-base through an attention-like mechanism. The model is trained on a corpus of question-answer pairs, with their associated triples in the given knowledge-base. Empirical study shows the proposed model can effectively deal with the language variation of the question and generate a right answer by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform the embedding-based QA model as well as the neural dialogue models trained on the same data."
  },
  "iclr2016_workshop_learningdocumentembeddingsbypredictingn-gramsforsentimentclassificationoflongmoviereviews": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews",
    "authors": [
      "Bofang Li",
      "Tao Liu",
      "Xiaoyong Du",
      "Deyuan Zhang",
      "Zhe Zhao"
    ],
    "page_url": "https://openreview.net/forum?id=XL92M93mzhXB8D1RUWBz",
    "pdf_url": "https://openreview.net/pdf?id=XL92M93mzhXB8D1RUWBz",
    "published": "2016-05",
    "summary": "Bag-of-ngram based methods still achieve state-of-the-art results for tasks such as sentiment classification of long movie reviews, though semantic information is partially lost for these methods. Many document embeddings methods have been proposed to capture semantics, but they still can't outperform bag-of-ngram based methods on this task. In this paper, we modify the architecture of the recently proposed Paragraph Vector, allowing it to learn document vectors by predicting not only words, but n-gram features as well. Our model is able to capture both semantics and word order in documents while keeping the expressive power of learned vectors. Experimental results on IMDB movie review dataset show that our model outperforms previous deep learning models and bag-of-ngram based models due to the above advantages."
  },
  "iclr2016_workshop_autoencodingforjointrelationfactorizationanddiscoveryfromtext": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Autoencoding for Joint Relation Factorization and Discovery from Text",
    "authors": [
      "Diego Marcheggiani",
      "Ivan Titov"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOzGWB5hRNGwArs4BJ",
    "pdf_url": "https://openreview.net/pdf?id=L7VOzGWB5hRNGwArs4BJ",
    "published": "2016-05",
    "summary": "We present a method for unsupervised open-domain relation discovery.In contrast to previous (mostly generative and agglomerative clustering) approaches, our model relies on rich contextual features andmakes minimal independence assumptions.The model is composed of two parts: a feature-rich relation extractor, which predicts a semantic relation between two entities, and a factorization model, which reconstructs arguments (i.e., the entities) relying on the predicted relation. We use a variational autoencoding objective and estimate the two components jointly so as to minimize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance."
  },
  "iclr2016_workshop_adaptivenaturalgradientlearningbasedonriemannianmetricofscorematching": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Adaptive Natural Gradient Learning Based on Riemannian Metric of Score Matching",
    "authors": [
      "Ryo Karakida",
      "Masato Okada",
      "Shun-ichi Amari"
    ],
    "page_url": "https://openreview.net/forum?id=lx9lNjDDvU2OVPy8CvGJ",
    "pdf_url": "https://openreview.net/pdf?id=lx9lNjDDvU2OVPy8CvGJ",
    "published": "2016-05",
    "summary": "The natural gradient is a powerful method to improve the transient dynamics of learning by considering the geometric structure of the parameter space. Many natural gradient methods have been developed with regards to Kullback-Leibler (KL) divergence and its Fisher metric, but the framework of natural gradient can be essentially extended to other divergences. In this study, we focus on score matching, which is an alternative to maximum likelihood learning for unnormalized statistical models, and introduce its Riemannian metric. By using the score matching metric, we derive an adaptive natural gradient algorithm that does not require computationally demanding inversion of the metric. Experimental results in a multi-layer neural network model demonstrate that the proposed method avoids the plateau phenomenon and accelerates the convergence of learning compared to the conventional stochastic gradient descent method."
  },
  "iclr2016_workshop_neuralenquirerlearningtoquerytablesinnaturallanguage": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Enquirer: Learning to Query Tables in Natural Language",
    "authors": [
      "Pengcheng Yin",
      "Zhengdong Lu",
      "Hang Li",
      "Ben Kao"
    ],
    "page_url": "https://openreview.net/forum?id=3QxgvRAolhp7y9wltPg8",
    "pdf_url": "https://openreview.net/pdf?id=3QxgvRAolhp7y9wltPg8",
    "published": "2016-05",
    "summary": "We propose Neural Enquirer \u2014 a neural network architecture for answering natural language (NL) questions given a knowledge base (KB) table. Unlike previous work on end-to-end training of semantic parsers, Neural Enquirer is fully \u201cneuralized\u201d: it gives distributed representations of queries and KB tables, and executes queries through a series of differentiable operations. The model can be trained with gradient descent using both end-to-end and step-by-step supervision. During training the representations of queries and the KB table are jointly optimized with the query execution logic. Our experiments show that the model can learn to execute complex NL queries on KB tables with rich structures."
  },
  "iclr2016_workshop_endtoendspeechrecognitioninenglishandmandarin": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "End to end speech recognition in English and Mandarin",
    "authors": [
      "Dario Amodei",
      "Rishita Anubhai",
      "Eric Battenberg",
      "Carl Case",
      "Jared Casper",
      "Bryan Catanzaro",
      "Jingdong Chen",
      "Mike Chrzanowski",
      "Adam Coates",
      "Greg Diamos",
      "Erich Elsen",
      "Jesse Engel",
      "Linxi Fan",
      "Christopher Fougner",
      "Tony Han",
      "Awni Hannun",
      "Billy Jun",
      "Patrick LeGresley",
      "Libby Lin",
      "Sharan Narang",
      "Andrew Ng",
      "Sherjil Ozair",
      "Ryan Prenger",
      "Jonathan Raiman",
      "Sanjeev Satheesh",
      "David Seetapun",
      "Shubho Sengupta",
      "Yi Wang",
      "Zhiqian Wang",
      "Chong Wang",
      "Bo Xiao",
      "Dani Yogatama",
      "Jun Zhan",
      "Zhenyao Zhu"
    ],
    "page_url": "https://openreview.net/forum?id=XL9vPjMAjuXB8D1RUG6L",
    "pdf_url": "https://openreview.net/pdf?id=XL9vPjMAjuXB8D1RUG6L",
    "published": "2016-05",
    "summary": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\u2013two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages.Key to our approach is our application of HPC techniques,enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms.As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale"
  },
  "iclr2016_workshop_lessonsfromtherademachercomplexityfordeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Lessons from the Rademacher Complexity for Deep Learning",
    "authors": [
      "Jure Sokolic",
      "Raja Giryes",
      "Guillermo Sapiro",
      "Miguel R. D. Rodrigues"
    ],
    "page_url": "https://openreview.net/forum?id=P7Vk63koAhKvjNORtJzZ",
    "pdf_url": "https://openreview.net/pdf?id=P7Vk63koAhKvjNORtJzZ",
    "published": "2016-05",
    "summary": "Understanding the generalization properties of deep learning models is critical for successful applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks via the empirical Rademacher complexity and show that it is easier to control the complexity ofconvolutional networks compared to general fully connected networks. In particular, we justify the usage of small convolutional kernels in deep networks as they lead to a better generalization error. Moreover, we propose a representation based regularization method that allows to decrease the generalization errorby controlling the coherence of the representation. Experiments on the MNIST dataset support these foundations."
  },
  "iclr2016_workshop_coverage-basedneuralmachinetranslation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Coverage-based Neural Machine Translation",
    "authors": [
      "Zhaopeng Tu",
      "Zhengdong Lu",
      "Yang Liu",
      "Xiaohua Liu",
      "Hang Li"
    ],
    "page_url": "https://openreview.net/forum?id=jZ9WrEWPmsnlBG2XfGLl",
    "pdf_url": "https://openreview.net/pdf?id=jZ9WrEWPmsnlBG2XfGLl",
    "published": "2016-05",
    "summary": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both translation and alignment qualities over NMT without coverage."
  },
  "iclr2016_workshop_feed-forwardnetworkswithattentioncansolvesomelong-termmemoryproblems": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems",
    "authors": [
      "Colin Raffel",
      "Daniel P. W. Ellis"
    ],
    "page_url": "https://openreview.net/forum?id=81DD7ZNyxI6O2Pl0Ul5j",
    "pdf_url": "https://openreview.net/pdf?id=81DD7ZNyxI6O2Pl0Ul5j",
    "published": "2016-05",
    "summary": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic addition and multiplication long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks."
  },
  "iclr2016_workshop_learningstablerepresentationsinachangingworldwithon-linet-sneproofofconceptinthesongbird": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning stable representations in a changing world withon-line t-SNE: proof of concept in the songbird",
    "authors": [
      "St\u00e9phane Deny",
      "Emily Mackevicius",
      "Tatsuo Okubo",
      "Gordon Berman",
      "Joshua Shaevitz",
      "Michale Fee"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo1jRRDsrlgPMRsBzY",
    "pdf_url": "https://openreview.net/pdf?id=oVgo1jRRDsrlgPMRsBzY",
    "published": "2016-05",
    "summary": "Many real-world time series involve repeated patterns that evolve gradually by following slow underlying trends. The evolution of relevant features prevents conventional learning methods from extracting representations that separate differing patterns while being consistent over the whole time series. Here, we present an unsupervised learning method to finding representations that are consistent over time and which separate patterns in non-stationary time-series. We developed an on-line version of t-Distributed Stochastic Neighbor Embedding (t-SNE). We apply t-SNE to the time series iteratively on a running window, and for each displacement of the window, we choose as the seed of the next embedding the final positions of the points obtained in the previous embedding. This process ensures consistency of the representation of slowly evolving patterns, while ensuring that the embedding at each step is optimally adapted to the current window. We apply this method to the song of the developing zebra finch, and we show that we are able to track multiple distinct syllables that are slowly emerging over multiple days, from babbling to the adult song stage."
  },
  "iclr2016_workshop_mixturesofsparseautoregressivenetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Mixtures of Sparse Autoregressive Networks",
    "authors": [
      "Marc Goessling",
      "Yali Amit"
    ],
    "page_url": "https://openreview.net/forum?id=q7kEZL1W2U8LEkD3tgnV",
    "pdf_url": "https://openreview.net/pdf?id=q7kEZL1W2U8LEkD3tgnV",
    "published": "2016-05",
    "summary": "We consider high-dimensional distribution estimation through autoregressive networks. By combining the concepts of sparsity, mixtures and parameter sharing we obtain a simple model which is fast to train and which achieves state-of-the-art or better results on several standard benchmark datasets. Specifically, we use an L1-penalty to regularize the conditional distributions and introduce a procedure for automatic parameter sharing between mixture components. Moreover, we propose a simple distributed representation which permits exact likelihood evaluations since the latent variables are interleaved with the observable variables and can be easily integrated out. Our model achieves excellent generalization performance and scales well to extremely high dimensions."
  },
  "iclr2016_workshop_comparativestudyofcaffe,neon,theano,andtorchfordeeplearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning",
    "authors": [
      "Soheil Bahrampour",
      "Naveen Ramakrishnan",
      "Lukas Schott",
      "Mohak Shah"
    ],
    "page_url": "https://openreview.net/forum?id=q7kEN7WoXU8LEkD3t7BQ",
    "pdf_url": "https://openreview.net/pdf?id=q7kEN7WoXU8LEkD3t7BQ",
    "published": "2016-05",
    "summary": "Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of four deep learning frameworks, namely Caffe, Neon, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Finally Caffe is the easiest for evaluating the performance of standard deep architectures."
  },
  "iclr2016_workshop_actionrecognitionusingvisualattention": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Action Recognition using Visual Attention",
    "authors": [
      "Shikhar Sharma",
      "Ryan Kiros",
      "Ruslan Salakhutdinov"
    ],
    "page_url": "https://openreview.net/forum?id=3QxX9NwWgIp7y9wlt5L7",
    "pdf_url": "https://openreview.net/pdf?id=3QxX9NwWgIp7y9wlt5L7",
    "published": "2016-05",
    "summary": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed."
  },
  "iclr2016_workshop_improvingperformanceofrecurrentneuralnetworkwithrelunonlinearity": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Improving performance of recurrent neural network with relu nonlinearity",
    "authors": [
      "Sachin S. Talathi",
      "Aniket Vartak"
    ],
    "page_url": "https://openreview.net/forum?id=wVqq536NJiG0qV7mtBNp",
    "pdf_url": "https://openreview.net/pdf?id=wVqq536NJiG0qV7mtBNp",
    "published": "2016-05",
    "summary": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem."
  },
  "iclr2016_workshop_visualizingandunderstandingrecurrentnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Visualizing and Understanding Recurrent Networks",
    "authors": [
      "Andrej Karpathy",
      "Justin Johnson",
      "Li Fei-Fei"
    ],
    "page_url": "https://openreview.net/forum?id=71BmK0m6qfAE8VvKUQWB",
    "pdf_url": "https://openreview.net/pdf?id=71BmK0m6qfAE8VvKUQWB",
    "published": "2016-05",
    "summary": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study."
  },
  "iclr2016_workshop_learningtodecomposeforobjectdetectionandinstancesegmentation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning to Decompose for Object Detection and Instance Segmentation",
    "authors": [
      "Eunbyung Park",
      "Alexander C. Berg"
    ],
    "page_url": "https://openreview.net/forum?id=oVgBRXX9nsrlgPMRsrP4",
    "pdf_url": "https://openreview.net/pdf?id=oVgBRXX9nsrlgPMRsrP4",
    "published": "2016-05",
    "summary": "Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, pre- and post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture, which consists of convolutional and recurrent layers, that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- or post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in theKITTI benchmark dataset.The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection."
  },
  "iclr2016_workshop_learningvisualgroupsfromco-occurrencesinspaceandtime": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning visual groups from co-occurrences in space and time",
    "authors": [
      "Phillip Isola",
      "Daniel Zoran",
      "Dilip Krishnan",
      "Edward H. Adelson"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo4M4RRIrlgPMRsBz5",
    "pdf_url": "https://openreview.net/pdf?id=oVgo4M4RRIrlgPMRsBz5",
    "published": "2016-05",
    "summary": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories."
  },
  "iclr2016_workshop_spatio-temporalvideoautoencoderwithdifferentiablememory": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Spatio-Temporal Video Autoencoder with Differentiable Memory",
    "authors": [
      "Viorica Patraucean",
      "Ankur Handa",
      "Roberto Cipolla"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo4p8O1CrlgPMRsBzE",
    "pdf_url": "https://openreview.net/pdf?id=oVgo4p8O1CrlgPMRsBzE",
    "published": "2016-05",
    "summary": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort."
  },
  "iclr2016_workshop_tasklossestimationforstructuredprediction": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Task Loss Estimation for Structured Prediction",
    "authors": [
      "Dzmitry Bahdanau",
      "Dmiriy Serdyuk",
      "Phil\u00e9mon Brakel",
      "Nan Rosemary Ke",
      "Jan Chorowski",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=GvVJNQZM5f1WDOmRiMKy",
    "pdf_url": "https://openreview.net/pdf?id=GvVJNQZM5f1WDOmRiMKy",
    "published": "2016-05",
    "summary": "Often, the performance on a supervised machine learning task is evaluated with a \\emph{task loss} function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a \\emph{surrogate loss} function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call \\emph{consistency with the task loss}. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error maybe used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this propertyto design specialized surrogate losses forEncoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant ~9\\% relative improvement interms of Character Error Rate (CER) in the case when noextra corpora are used for language modeling. "
  },
  "iclr2016_workshop_conditionalcomputationinneuralnetworksforfastermodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Conditional computation in neural networks for faster models",
    "authors": [
      "Emmanuel Bengio",
      "Pierre-Luc Bacon",
      "Joelle Pineau",
      "Doina Precup"
    ],
    "page_url": "https://openreview.net/forum?id=BNYMo3QRxh7PwR1riEDL",
    "pdf_url": "https://openreview.net/pdf?id=BNYMo3QRxh7PwR1riEDL",
    "published": "2016-05",
    "summary": "Deep learning has become the state-of-art tool in many applications, but the evaluation of expressive deep models can be unfeasible on resource-constrained devices. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. We propose to use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as reinforcement learning. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation."
  },
  "iclr2016_workshop_ametriclearningapproachforgraph-basedlabelpropagation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A metric learning approach for graph-based label propagation",
    "authors": [
      "Pauline Wauquier",
      "Mikaela Keller"
    ],
    "page_url": "https://openreview.net/forum?id=oVgoWgzpZSrlgPMRsB19",
    "pdf_url": "https://openreview.net/pdf?id=oVgoWgzpZSrlgPMRsB19",
    "published": "2016-05",
    "summary": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. We claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently."
  },
  "iclr2016_workshop_bidirectionalhelmholtzmachines": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Bidirectional Helmholtz Machines",
    "authors": [
      "Jorg Bornschein",
      "Samira Shabanian",
      "Asja Fischer",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=91Eo10XnqckRlNvXUVkL",
    "pdf_url": "https://openreview.net/pdf?id=91Eo10XnqckRlNvXUVkL",
    "published": "2016-05",
    "summary": "Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine, involves training a top-down directed generative model together with a bottom-up auxiliary model that is trained to help perform approximate inference. Recent results indicate that better results can be obtained with better approximate inference procedures. Instead of employing more powerful procedures, we here propose to regularize the generative model to stay close to the class of distributions that can be efficiently inverted by the approximate inference model. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. We demonstrate that we can use this approach to fit generative models with many layers of hidden binary stochastic variables to complex training distributions and that this method prefers significantly deeper architectures while it supports orders of magnitude more efficient approximate inference than other approaches. "
  },
  "iclr2016_workshop_acontroller-recognizerframeworkhownecessaryisrecogntionforcontrol?": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Controller-Recognizer Framework: How Necessary is Recogntion for Control?",
    "authors": [
      "Marcin Moczulski",
      "Kelvin Xu",
      "Aaron Courville",
      "Kyunghyun Cho"
    ],
    "page_url": "https://openreview.net/forum?id=5QzkZ8LvJFZgXpo7i3OQ",
    "pdf_url": "https://openreview.net/pdf?id=5QzkZ8LvJFZgXpo7i3OQ",
    "published": "2016-05",
    "summary": "Recently there has been growing interest in building ``active'' visual object recognizers, as opposed to ``passive'' recognizers which classifies a given static image into a predefined set of object categories. In this paper we propose to generalize recent end-to-end active visual recognizers into a controller-recognizer framework. In this framework, the interfaces with an external manipulator, while the recognizer classifies the visual input adjusted by the manipulator. We describe two recently proposed controller-recognizer models-- the recurrent attention model (Mnih et al., 2014) and spatial transformer network (Jaderberg et al., 2015)-- as representative examples of controller-recognizer models. Based on this description we observe that most existing end-to-end controller-recognizers tightly couple the controller and recognizer. We consider whether this tight coupling is necessary, and try to answer this empirically by investigating a decoupled controller and recognizer.Our experiments revealed that it is not always necessary to tightly couple them, and that by decoupling the controller and recognizer, there is a possibility to build a generic controller that is pretrained and works together with any subsequent recognizer. "
  },
  "iclr2016_workshop_onlinebatchselectionforfastertrainingofneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Online Batch Selection for Faster Training of Neural Networks",
    "authors": [
      "Ilya Loshchilov",
      "Frank Hutter"
    ],
    "page_url": "https://openreview.net/forum?id=r8lrkABJ7H8wknpYt5KB",
    "pdf_url": "https://openreview.net/pdf?id=r8lrkABJ7H8wknpYt5KB",
    "published": "2016-05",
    "summary": "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset.While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. As the loss function to be minimized for the whole dataset is an aggregation of loss functions of individualdatapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank.Our experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5."
  },
  "iclr2016_workshop_nonparametriccanonicalcorrelationanalysis": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Nonparametric Canonical Correlation Analysis",
    "authors": [
      "Tomer Michaeli",
      "Weiran Wang",
      "Karen Livescu"
    ],
    "page_url": "https://openreview.net/forum?id=3Qxgwzv3ZCp7y9wltPg2",
    "pdf_url": "https://openreview.net/pdf?id=3Qxgwzv3ZCp7y9wltPg2",
    "published": "2016-05",
    "summary": "Canonical correlation analysis (CCA) is a fundamental technique in multi-view data analysis and representation learning. Several nonlinear extensions of the classical linear CCA method have been proposed, including kernel and deep neural network methods. These approaches restrict attention to certain families of nonlinear projections, which the user must specify (by choosing a kernel or a neural network architecture), and are computationally demanding. Interestingly, the theory of nonlinear CCA without any functional restrictions, has been studied in the population setting by Lancaster already in the 50\u2019s. However, these results, have not inspired practical algorithms. In this paper, we revisit Lancaster\u2019s theory, and use it to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the most correlated nonlinear projections of two random vectors can be expressed in terms of the singular value decomposition of a certain operator associated with their joint density. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without having to compute the inverse of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. PLCCA turns out to have a similar form to the classical linear CCA, but with a nonparametric regression term replacing the linear regression in CCA. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and achieve better performance than kernel CCA and comparable performance to deep CCA."
  },
  "iclr2016_workshop_documentcontextlanguagemodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Document Context Language Models",
    "authors": [
      "Yangfeng Ji",
      "Trevor Cohn",
      "Lingpeng Kong",
      "Chris Dyer",
      "Jacob Eisenstein"
    ],
    "page_url": "https://openreview.net/forum?id=vlpy96kV2C7OYLG5inQw",
    "pdf_url": "https://openreview.net/pdf?id=vlpy96kV2C7OYLG5inQw",
    "published": "2016-05",
    "summary": "Text documents are structured on multiple levels of detail: individual words are related by syntax, and larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLMs), which incorporate contextual information both within and beyond the sentence. In comparison with sentence-level recurrent neural network language models, the DCLMs obtain slightly better predictive likelihoods, and considerably better assessments of document coherence."
  },
  "iclr2016_workshop_unsupervisedlearningofvisualstructureusingpredictivegenerativenetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Unsupervised Learning of Visual Structure using Predictive Generative Networks",
    "authors": [
      "William Lotter",
      "Gabriel Kreiman",
      "David Cox"
    ],
    "page_url": "https://openreview.net/forum?id=BNYAGG8r9F7PwR1riXz8",
    "pdf_url": "https://openreview.net/pdf?id=BNYAGG8r9F7PwR1riXz8",
    "published": "2016-05",
    "summary": "The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features."
  },
  "iclr2016_workshop_parsenetlookingwidertoseebetter": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "ParseNet: Looking Wider to See Better",
    "authors": [
      "Wei Liu",
      "Andrew Rabinovich",
      "Alexander C. Berg"
    ],
    "page_url": "https://openreview.net/forum?id=Qn8x8rGr5CkB2l8pUY8P",
    "pdf_url": "https://openreview.net/pdf?id=Qn8x8rGr5CkB2l8pUY8P",
    "published": "2016-05",
    "summary": "We present a technique for adding global context to fully convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN~\\cite{long2014fully}). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at \\url{https://github.com/weiliu89/caffe/tree/fcn} ."
  },
  "iclr2016_workshop_whyaredeepnetsreversibleasimpletheory,withimplicationsfortraining": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Why are deep nets reversible: A simple theory, with implications for training",
    "authors": [
      "Sanjeev Arora",
      "Yingyu Liang",
      "Tengyu Ma"
    ],
    "page_url": "https://openreview.net/forum?id=q7kqBKMN2U8LEkD3t7Xy",
    "pdf_url": "https://openreview.net/pdf?id=q7kqBKMN2U8LEkD3t7Xy",
    "published": "2016-05",
    "summary": "Generative models fordeep learning are promisingboth to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above.However, there is no accompanying proof of correctness for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input.Furthermore, these models are complicated.The current paper takes a more theoretical tack. It presents a very simple generative model for ReLU deep nets, with the following characteristics: (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.) (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method forrecovering the hidden layer. The generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels andinclude it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training.This extended abstract provides a succinct description of our results while the full paper is available on arXiv."
  },
  "iclr2016_workshop_bindingviareconstructionclustering": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Binding via Reconstruction Clustering",
    "authors": [
      "Klaus Greff",
      "Rupesh Srivastava",
      "J\u00fcrgen Schmidhuber"
    ],
    "page_url": "https://openreview.net/forum?id=gZ9Oq3ZGVCAPowrRUANz",
    "pdf_url": "https://openreview.net/pdf?id=gZ9Oq3ZGVCAPowrRUANz",
    "published": "2016-05",
    "summary": "Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem.We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an algorithm that uses a denoising autoencoder to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process.The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training."
  },
  "iclr2016_workshop_dynamiccapacitynetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Dynamic Capacity Networks",
    "authors": [
      "Amjad Almahairi",
      "Nicolas Ballas",
      "Tim Cooijmans",
      "Yin Zheng",
      "Hugo Larochelle",
      "Aaron Courville"
    ],
    "page_url": "https://openreview.net/forum?id=oVgBVKDQmCrlgPMRsrPQ",
    "pdf_url": "https://openreview.net/pdf?id=oVgBVKDQmCrlgPMRsrPQ",
    "published": "2016-05",
    "summary": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance."
  },
  "iclr2016_workshop_learningrepresentationsofaffectfromspeech": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Representations of Affect from Speech",
    "authors": [
      "Sayan Ghosh",
      "Eugene Laksana",
      "Louis-Philippe Morency",
      "Stefan Scherer"
    ],
    "page_url": "https://openreview.net/forum?id=Jy9kxYV3ziqp6ARvtWXZ",
    "pdf_url": "https://openreview.net/pdf?id=Jy9kxYV3ziqp6ARvtWXZ",
    "published": "2016-05",
    "summary": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes, i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition."
  },
  "iclr2016_workshop_neuralvariationalinferencefortextprocessing": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural Variational Inference for Text Processing",
    "authors": [
      "Yishu Miao",
      "Lei Yu",
      "Phil Blunsom"
    ],
    "page_url": "https://openreview.net/forum?id=L7VOPGYLgCRNGwArs4Bm",
    "pdf_url": "https://openreview.net/pdf?id=L7VOPGYLgCRNGwArs4Bm",
    "published": "2016-05",
    "summary": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks."
  },
  "iclr2016_workshop_recurrentmodelsforauditoryattentioninmulti-microphonedistancespeechrecognition": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition",
    "authors": [
      "Suyoun Kim",
      "Ian Lane"
    ],
    "page_url": "https://openreview.net/forum?id=oVgo9jD93urlgPMRsB1B",
    "pdf_url": "https://openreview.net/pdf?id=oVgo9jD93urlgPMRsB1B",
    "published": "2016-05",
    "summary": "Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multichannel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-to-sequence learning, no attention mechanisms have previously been applied to learn potentially asynchronous and non-stationary multiple inputs. We evaluate our neural attention model on the CHiME-3 challenge task, and show that the model achieves comparable performance to beamforming using a purely data-driven method."
  },
  "iclr2016_workshop_adeepmemory-basedarchitectureforsequence-to-sequencelearning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning",
    "authors": [
      "Fandong Meng",
      "Zhengdong Lu",
      "Zhaopeng Tu",
      "Hang Li",
      "Qun Liu"
    ],
    "page_url": "https://openreview.net/forum?id=GvVr3PmmGC1WDOmRiEo6",
    "pdf_url": "https://openreview.net/pdf?id=GvVr3PmmGC1WDOmRiEo6",
    "published": "2016-05",
    "summary": "We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequence-to-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size."
  },
  "iclr2016_workshop_deconstructingtheladdernetworkarchitecture": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deconstructing the Ladder Network Architecture",
    "authors": [
      "Mohammad Pezeshki",
      "Linxi Fan",
      "Philemon Brakel",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=NL6ggGZ5wI0VOPA8ixm2",
    "pdf_url": "https://openreview.net/pdf?id=NL6ggGZ5wI0VOPA8ixm2",
    "published": "2016-05",
    "summary": "The Ladder Network is a recent new approach to semi-supervised learning that turned out to be very successful. While showing impressive performance, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude thatthe most important contribution is made by the lateral connections, followed by the application of noise, and the choice of what we refer to as the `combinator function'. As the number of labeled training examples increases, the lateral connections and the reconstruction criterion become less important, with most of the generalization improvement coming from the injection of noise in each layer. Finally, we introduce a combinator function that reduces test error rates on Permutation-Invariant MNIST to 0.57\\% for the supervised setting, and to 0.97 % and 1.0 % for semi-supervised settings with 1000 and 100 labeled examples, respectively."
  },
  "iclr2016_workshop_neuralnetwork-basedclusteringusingpairwiseconstraints": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Neural network-based clustering using pairwise constraints",
    "authors": [
      "Yen-Chang Hsu",
      "Zsolt Kira"
    ],
    "page_url": "https://openreview.net/forum?id=0YrNVxKMAUGJ7gK5tR5K",
    "pdf_url": "https://openreview.net/pdf?id=0YrNVxKMAUGJ7gK5tR5K",
    "published": "2016-05",
    "summary": "This paper presents a neural network-based end-to-end clustering framework. We design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data, in addition to learning a feature embedding suitable for such clustering. The network is trained with weak labels, specifically partial pairwise relationships between data instances. The cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data. The framework has the interesting characteristic that no cluster centers need to be explicitly specified, thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined. The experiments show that the proposed approach beats the conventional two-stage method (feature embedding with k-means) by a significant margin. It also compares favorably to the performance of the standard cross entropy loss for classification. Robustness analysis also shows that the method is largely insensitive to the number of clusters. Specifically, we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering. "
  },
  "iclr2016_workshop_lstm-baseddeeplearningmodelsfornon-factoidanswerselection": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "LSTM-based Deep Learning Models for non-factoid answer selection",
    "authors": [
      "Ming Tan",
      "Cicero dos Santos",
      "Bing Xiang",
      "Bowen Zhou"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xwl3PDS5Pk8ELfEzP",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xwl3PDS5Pk8ELfEzP",
    "published": "2016-05",
    "summary": "In this paper, we apply a general deep learning framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and InsuranceQA. Experimental results demonstrate that the proposed models substantially outperform several strong baselines."
  },
  "iclr2016_workshop_usingdeeplearningtopredictdemographicsfrommobilephonemetadata": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Using Deep Learning to Predict Demographics from Mobile Phone Metadata",
    "authors": [
      "Bjarke Felbo",
      "P\u00e5l Sunds\u00f8y",
      "Alex 'Sandy' Pentland",
      "Sune Lehmann",
      "Yves-Alexandre de Montjoye"
    ],
    "page_url": "https://openreview.net/forum?id=91EEnoZX0HkRlNvXUKLA",
    "pdf_url": "https://openreview.net/pdf?id=91EEnoZX0HkRlNvXUKLA",
    "published": "2016-05",
    "summary": "Mobile phone metadata are increasingly used to study human behavior at large-scale. There has recently been a growing interest in predicting demographic information from metadata. Previous approaches relied on hand-engineered features. We here apply, for the first time, deep learning methods to mobile phone metadata using a convolutional network. Our method provides high accuracy on both age and gender prediction. These results show great potential for deep learning approaches for prediction tasks using standard mobile phone metadata."
  },
  "iclr2016_workshop_efficientinferenceinocclusion-awaregenerativemodelsofimages": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Efficient Inference in Occlusion-Aware Generative Models of Images",
    "authors": [
      "Jonathan Huang",
      "Kevin Murphy"
    ],
    "page_url": "https://openreview.net/forum?id=E8Vg037q7f31v0m2iqn3",
    "pdf_url": "https://openreview.net/pdf?id=E8Vg037q7f31v0m2iqn3",
    "published": "2016-05",
    "summary": "We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image \u2014 and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images."
  },
  "iclr2016_workshop_convolutionalmodelsforjointobjectcategorizationandposeestimation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Convolutional Models for Joint Object Categorization and Pose Estimation",
    "authors": [
      "Mohamed Elhoseiny",
      "Tarek El-Gaaly",
      "Amr Bakry",
      "Ahmed Elgammal"
    ],
    "page_url": "https://openreview.net/forum?id=WL9EYwkKyI5zMX2Kf2V2",
    "pdf_url": "https://openreview.net/pdf?id=WL9EYwkKyI5zMX2Kf2V2",
    "published": "2016-05",
    "summary": "In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets."
  },
  "iclr2016_workshop_basiclevelcategorizationfacilitatesvisualobjectrecognition": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Basic Level Categorization Facilitates Visual Object Recognition",
    "authors": [
      "Panqu Wang",
      "Garrison Cottrell"
    ],
    "page_url": "https://openreview.net/forum?id=4QyyQzGjptBYD9yOF80y",
    "pdf_url": "https://openreview.net/pdf?id=4QyyQzGjptBYD9yOF80y",
    "published": "2016-05",
    "summary": "Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children\u2019s visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results. "
  },
  "iclr2016_workshop_learningtorepresentwordsincontextwithmultilingualsupervision": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning to Represent Words in Context with Multilingual Supervision",
    "authors": [
      "Kazuya Kawakami",
      "Chris Dyer"
    ],
    "page_url": "https://openreview.net/forum?id=nx9Av7Am9T7lP3z2ioYK",
    "pdf_url": "https://openreview.net/pdf?id=nx9Av7Am9T7lP3z2ioYK",
    "published": "2016-05",
    "summary": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these."
  },
  "iclr2016_workshop_fine-grainedposeprediction,normalization,andrecognition": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Fine-grained pose prediction, normalization, and recognition",
    "authors": [
      "Ning Zhang",
      "Evan Shelhamer",
      "Yang Gao",
      "Trevor Darrell"
    ],
    "page_url": "https://openreview.net/forum?id=ROVAEyEoPHvnM0J1IpPO",
    "pdf_url": "https://openreview.net/pdf?id=ROVAEyEoPHvnM0J1IpPO",
    "published": "2016-05",
    "summary": "Pose variation and subtle differences in appearance are key challenges to fine- grained classification. While deep networks have markedly improved general recognition, many approaches to fine-grained recognition rely on anchoring net- works to parts for better accuracy. Identifying parts to find correspondence dis- counts pose variation so that features can be tuned to appearance. To this end previous methods have examined how to find parts and extract pose-normalized features. These methods have generally separated fine-grained recognition into stages which first localize parts using hand-engineered and coarsely-localized pro- posal features, and then separately learn deep descriptors centered on inferred part positions. We unify these steps in an end-to-end trainable network supervised by keypoint locations and class labels that localizes parts by a fully convolutional network to focus the learning of feature representations for the fine-grained clas- sification task. Experiments on the popular CUB200 dataset show that our method is state-of-the-art and suggest a continuing role for strong supervision."
  },
  "iclr2016_workshop_scalablegradient-basedtuningofcontinuousregularizationhyperparameters": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters",
    "authors": [
      "Jelena Luketina",
      "Mathias Berglund",
      "Tapani Raiko"
    ],
    "page_url": "https://openreview.net/forum?id=ANYzz8LXgINrwlgXCqGj",
    "pdf_url": "https://openreview.net/pdf?id=ANYzz8LXgINrwlgXCqGj",
    "published": "2016-05",
    "summary": "Hyperparameter selection generally relies on running multiple full training trials, with hyperparameter selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST the resulting regularization levels are within the optimal regions. The method is significantly less computationally demanding compared to similar gradient-based approaches to hyperparameter optimization and consistently finds good hyperparameter values, which makes it a useful tool for training neural network models."
  },
  "iclr2016_workshop_unitaryevolutionrecurrentneuralnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Unitary Evolution Recurrent Neural Networks",
    "authors": [
      "Martin Arjovsky",
      "Amar Shah",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=71BmDZPEluAE8VvKUQ80",
    "pdf_url": "https://openreview.net/pdf?id=71BmDZPEluAE8VvKUQ80",
    "published": "2016-05",
    "summary": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies."
  },
  "iclr2016_workshop_temporalconvolutionalneuralnetworksfordiagnosisfromlabtests": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Temporal Convolutional Neural Networks for Diagnosis from Lab Tests",
    "authors": [
      "Narges Razavian",
      "David Sontag"
    ],
    "page_url": "https://openreview.net/forum?id=ROVmO430RTvnM0J1Ip9z",
    "pdf_url": "https://openreview.net/pdf?id=ROVmO430RTvnM0J1Ip9z",
    "published": "2016-05",
    "summary": "Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases' onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis."
  },
  "iclr2016_workshop_perforatedcnnsaccelerationthrougheliminationofredundantconvolutions": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions",
    "authors": [
      "Michael Figurnov",
      "Dmitry Vetrov",
      "Pushmeet Kohli"
    ],
    "page_url": "https://openreview.net/forum?id=k80x272vDcOYKX7jiYKw",
    "pdf_url": "https://openreview.net/pdf?id=k80x272vDcOYKX7jiYKw",
    "published": "2016-05",
    "summary": "We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. Our method allows to reduce the evaluation time of modern convolutional neural networks by 50% with a small decrease in accuracy."
  },
  "iclr2016_workshop_howfarcanwegowithoutconvolutionimprovingfully-connectednetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "How far can we go without convolution: Improving fully-connected networks",
    "authors": [
      "Zhouhan Lin",
      "Roland Memisevic",
      "Kishore Konda"
    ],
    "page_url": "https://openreview.net/forum?id=1WvovwjA7UMnPB1oinBL",
    "pdf_url": "https://openreview.net/pdf?id=1WvovwjA7UMnPB1oinBL",
    "published": "2016-05",
    "summary": "We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases. We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutation-invariant CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is close to the performance of a decent convolutional network. "
  },
  "iclr2016_workshop_learningdenseconvolutionalembeddingsforsemanticsegmentation": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Learning Dense Convolutional Embeddings for Semantic Segmentation",
    "authors": [
      "Adam W. Harley",
      "Konstantinos G. Derpanis",
      "Iasonas Kokkinos"
    ],
    "page_url": "https://openreview.net/forum?id=mO9mx9y48ij1gPZ3UlOk",
    "pdf_url": "https://openreview.net/pdf?id=mO9mx9y48ij1gPZ3UlOk",
    "published": "2016-05",
    "summary": "This paper proposes a new deep convolutional neural network (DCNN) architecture that learns pixel embeddings, such that pairwise distances between the embeddings can be used to infer whether or not the pixels lie in the same region. That is, for any two pixels on the same object, the embeddings are trained to be similar; for any pair that straddles an object boundary, the embeddings are trained to be dissimilar. Experimental results show that when the embeddings are used in conjunction with a DCNN trained on semantic segmentation, there is a systematic improvement in per-pixel classification accuracy. These contributions are integrated in the popular Caffe deep learning framework, and consist in straightforward modifications to convolution routines. As such, they can be exploited for any task involving convolution layers."
  },
  "iclr2016_workshop_generatingsentencesfromacontinuousspace": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Generating Sentences from a Continuous Space",
    "authors": [
      "Samuel R. Bowman",
      "Luke Vilnis",
      "Oriol Vinyals",
      "Andrew M. Dai",
      "Rafal Jozefowicz",
      "Samy Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=D1VVBv7BKS5jEJ1zfxJg",
    "pdf_url": "https://openreview.net/pdf?id=D1VVBv7BKS5jEJ1zfxJg",
    "published": "2016-05",
    "summary": "The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space."
  },
  "iclr2016_workshop_stackedwhat-whereauto-encoders": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Stacked What-Where Auto-encoders",
    "authors": [
      "Junbo Zhao",
      "Michael Mathieu",
      "Ross Goroshin",
      "Yann LeCun"
    ],
    "page_url": "https://openreview.net/forum?id=1WvkoMpX3FMnPB1oiq8Z",
    "pdf_url": "https://openreview.net/pdf?id=1WvkoMpX3FMnPB1oiq8Z",
    "published": "2016-05",
    "summary": "We present a novel architecture, the stacked what-where auto-encoders (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the what which are fed to the next layer, and its complementary variable where that are fed to the corresponding layer in the generative decoder."
  },
  "iclr2016_workshop_bayesianconvolutionalneuralnetworkswithbernoulliapproximatevariationalinference": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference",
    "authors": [
      "Yarin Gal",
      "Zoubin Ghahramani"
    ],
    "page_url": "https://openreview.net/forum?id=3QxqXoJEyfp7y9wltP11",
    "pdf_url": "https://openreview.net/pdf?id=3QxqXoJEyfp7y9wltP11",
    "published": "2016-05",
    "summary": "Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data \u2013 as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN\u2019s kernels. We approximate our model\u2019s intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10."
  },
  "iclr2016_workshop_blendinglstmsintocnns": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Blending LSTMs into CNNs",
    "authors": [
      "Krzysztof J. Geras",
      "Abdel-rahman Mohamed",
      "Rich Caruana",
      "Gregor Urban",
      "Shengjie Wang",
      "Ozlem Aslan",
      "Matthai Philipose",
      "Matthew Richardson",
      "Charles Sutton"
    ],
    "page_url": "https://openreview.net/forum?id=k80kvBj55IOYKX7ji4V4",
    "pdf_url": "https://openreview.net/pdf?id=k80kvBj55IOYKX7ji4V4",
    "published": "2016-05",
    "summary": "We consider whether deep convolutional networks (CNNs) can represent decision functions with similar accuracy as recurrent networks such as LSTMs. First, we show that a deep CNN with an architecture inspired by the models recently introduced in image recognition can yield better accuracy than previous convolutional and LSTM networks on the standard 309h Switchboard automatic speech recognition task. Then we show that even more accurate CNNs can be trained under the guidance of LSTMs using a variant of model compression, which we call model blending because the teacher and student models are similar in complexity but different in inductive bias. Blending further improves the accuracy of our CNN, yielding a computationally efficient model of accuracy higher than any of the other individual models. Examining the effect of \u201cdark knowledge\u201d in this model compression task, we find that less than 1% of the highest probability labels are needed for accurate model compression."
  },
  "iclr2016_workshop_empiricalperformanceupperboundsforimageandvideocaptioning": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Empirical performance upper bounds for image and video captioning",
    "authors": [
      "Li Yao",
      "Nicolas Ballas",
      "Kyunghyun Cho",
      "John R. Smith",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=vlprXrN35c7OYLG5irDB",
    "pdf_url": "https://openreview.net/pdf?id=vlprXrN35c7OYLG5irDB",
    "published": "2016-05",
    "summary": "The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets."
  },
  "iclr2016_workshop_adversarialautoencoders": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Adversarial Autoencoders",
    "authors": [
      "Alireza Makhzani",
      "Jonathon Shlens",
      "Navdeep Jaitly",
      "Ian Goodfellow"
    ],
    "page_url": "https://openreview.net/forum?id=2xwp4Zwr3TpKBZvXtWoj",
    "pdf_url": "https://openreview.net/pdf?id=2xwp4Zwr3TpKBZvXtWoj",
    "published": "2016-05",
    "summary": "In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named adversarial autoencoder, uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no holes in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets."
  },
  "iclr2016_workshop_deepreinforcementlearningwithanactionspacedefinedbynaturallanguage": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Deep Reinforcement Learning with an Action Space Defined by Natural Language",
    "authors": [
      "Ji He",
      "Jianshu Chen",
      "Xiaodong He",
      "Jianfeng Gao",
      "Lihong Li",
      "Li Deng",
      "Mari Ostendorf"
    ],
    "page_url": "https://openreview.net/forum?id=WL9AjgWvPf5zMX2Kfoj5",
    "pdf_url": "https://openreview.net/pdf?id=WL9AjgWvPf5zMX2Kfoj5",
    "published": "2016-05",
    "summary": "In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, to design a model for handling an action space characterized using natural language with applications to text-based games. For a particular class of games, a user must choose among a number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically what fits the current situation best (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Even with a constrained vocabulary, the action space is very large and sparse, posing challenges for learning. To address this challenge, the DRRN extracts separate high-level embedding vectors from the texts that describe states and actions, respectively, using a general interaction function, such as inner product, bilinear, and DNN interaction, between these embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. "
  },
  "iclr2016_workshop_universumprescriptionregularizationusingunlabeleddata": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Universum Prescription: Regularization using Unlabeled Data",
    "authors": [
      "Xiang Zhang",
      "Yann LeCun"
    ],
    "page_url": "https://openreview.net/forum?id=91EvmZNlwtkRlNvXUKL9",
    "pdf_url": "https://openreview.net/pdf?id=91EvmZNlwtkRlNvXUKL9",
    "published": "2016-05",
    "summary": "This paper shows that simply prescribing none of the above labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100 and STL-10 datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter -- probability of sampling from unlabeled data -- is also studied empirically."
  },
  "iclr2016_workshop_variancereductioninsgdbydistributedimportancesampling": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Variance Reduction in SGD by Distributed Importance Sampling",
    "authors": [
      "Guillaume Alain",
      "Alex Lamb",
      "Chinnadhurai Sankar",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://openreview.net/forum?id=r8lrEqPpYF8wknpYt57j",
    "pdf_url": "https://openreview.net/pdf?id=r8lrEqPpYF8wknpYt57j",
    "published": "2016-05",
    "summary": "Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty.We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling.This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set."
  },
  "iclr2016_workshop_addinggradientnoiseimproveslearningforverydeepnetworks": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Adding Gradient Noise Improves Learning for Very Deep Networks",
    "authors": [
      "Arvind Neelakantan",
      "Luke Vilnis",
      "Quoc V. Le",
      "Ilya Sutskever",
      "Lukasz Kaiser",
      "Karol Kurach",
      "James Martens"
    ],
    "page_url": "https://openreview.net/forum?id=ZY9xxQDMMu5Pk8ELfEz4",
    "pdf_url": "https://openreview.net/pdf?id=ZY9xxQDMMu5Pk8ELfEz4",
    "published": "2016-05",
    "summary": "Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. A major reason for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures. "
  },
  "iclr2016_workshop_blackboxvariationalinferenceforstatespacemodels": {
    "conf_id": "ICLR2016",
    "conf_sub_id": "Workshop",
    "is_workshop": true,
    "conf_name": "ICLR2016_workshop",
    "title": "Black Box Variational Inference for State Space Models",
    "authors": [
      "Evan Archer",
      "Il Memming Park",
      "Lars Buesing",
      "John Cunningham",
      "Liam Paninski"
    ],
    "page_url": "https://openreview.net/forum?id=P7q1lVQQvSKvjNORtJZL",
    "pdf_url": "https://openreview.net/pdf?id=P7q1lVQQvSKvjNORtJZL",
    "published": "2016-05",
    "summary": "Latent variable time-series models are among the most heavily used tools from machine learning and applied statistics. These models have the advantage of learning latent structure both from noisy observations and from the temporal ordering in the data, where it is assumed that meaningful correlation structure exists across time. A few highly-structured models, such as the linear dynamical system with linear-Gaussian observations, have closed-form inference procedures (e.g. the Kalman Filter), but this case is an exception to the general rule that exact posterior inference in more complex generative models is intractable. Consequently, much work in time-series modeling focuses on approximate inference procedures for one particular class of models. Here, we extend recent developments in stochastic variational inference to develop a `black-box' approximate inference technique for latent variable models with latent dynamical structure. We propose a structured Gaussian variational approximate posterior that carries the same intuition as the standard Kalman filter-smoother but, importantly, permits us to use the same inference approach to approximate the posterior of much more general, nonlinear latent variable generative models. We show that our approach recovers accurate estimates in the case of basic models with closed-form posteriors, and more interestingly performs well in comparison to variational approaches that were designed in a bespoke fashion for specific non-conjugate models."
  }
}