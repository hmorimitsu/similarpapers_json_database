{
  "wacv2023_main_3dchangelocalizationandcaptioningfromdynamicscansofindoorscenes": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "3D Change Localization and Captioning From Dynamic Scans of Indoor Scenes",
    "authors": [
      "Yue Qiu",
      "Shintaro Yamamoto",
      "Ryosuke Yamada",
      "Ryota Suzuki",
      "Hirokatsu Kataoka",
      "Kenji Iwata",
      "Yutaka Satoh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Qiu_3D_Change_Localization_and_Captioning_From_Dynamic_Scans_of_Indoor_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Qiu_3D_Change_Localization_and_Captioning_From_Dynamic_Scans_of_Indoor_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Daily indoor scenes often involve constant changes due to human activities. To recognize scene changes, existing change captioning methods focus on describing changes from two images of a scene. However, to accurately perceive and appropriately evaluate physical changes and then identify the geometry of changed objects, recognizing and localizing changes in 3D space is crucial. Therefore, we propose a task to explicitly localize changes in 3D bounding boxes from two point clouds and describe detailed scene changes, including change types, object attributes, and spatial locations. Moreover, we create a simulated dataset with various scenes, allowing generating data without labor costs. We further propose a framework that allows different 3D object detectors to be incorporated in the change detection process, after which captions are generated based on the correlations of different change regions. The proposed framework achieves promising results in both change detection and captioning. Furthermore, we also evaluated on data collected from real scenes. The experiments show that pretraining on the proposed dataset increases the change detection accuracy by +12.8% (mAP0.25) when applied to real-world data. We believe that our proposed dataset and discussion could provide both a new benchmark and insights for future studies in scene change understanding."
  },
  "wacv2023_main_panoptic-awareimage-to-imagetranslation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Panoptic-Aware Image-to-Image Translation",
    "authors": [
      "Liyun Zhang",
      "Photchara Ratsamee",
      "Bowen Wang",
      "Zhaojie Luo",
      "Yuki Uranishi",
      "Manabu Higashida",
      "Haruo Takemura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_Panoptic-Aware_Image-to-Image_Translation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Panoptic-Aware_Image-to-Image_Translation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Despite remarkable progress in image translation, the complex scene with multiple discrepant objects remains a challenging problem. The translated images have low fidelity and tiny objects in fewer details causing unsatisfactory performance in object recognition. Without thorough object perception (i.e., bounding boxes, categories, and masks) of images as prior knowledge, the style transformation of each object will be difficult to track in translation. We propose panoptic-aware generative adversarial networks (PanopticGAN) for image-to-image translation together with a compact panoptic segmentation dataset. The panoptic perception (i.e., foreground instances and background semantics of the image scene) is extracted to achieve alignment between object content codes of the input domain and panoptic-level style codes sampled from the target style space, then refined by a proposed feature masking module for sharping object boundaries. The image-level combination between content and sampled style codes is also merged for higher fidelity image generation. Our proposed method was systematically compared with different competing methods and obtained significant improvement in both image quality and object recognition performance."
  },
  "wacv2023_main_partiallycalibratedsemi-generalizedposefromhybridpointcorrespondences": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Partially Calibrated Semi-Generalized Pose From Hybrid Point Correspondences",
    "authors": [
      "Snehal Bhayani",
      "Torsten Sattler",
      "Viktor Larsson",
      "Janne Heikkil\u00e4",
      "Zuzana Kukelova"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bhayani_Partially_Calibrated_Semi-Generalized_Pose_From_Hybrid_Point_Correspondences_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bhayani_Partially_Calibrated_Semi-Generalized_Pose_From_Hybrid_Point_Correspondences_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper we study the problem of estimating the semi-generalized pose of a partially calibrated camera, i.e., the pose of a perspective camera with unknown focal length w.r.t. a generalized camera, from a hybrid set of 2D-2D and 2D-3D point correspondences. We study all possible camera configurations within the generalized camera system. To derive practical solvers to previously unsolved challenging configurations, we test different parameterizations as well as different solving strategies based on the state-of-the-art methods for generating efficient polynomial solvers. We evaluate the three most promising solvers, i.e., the H51f solver with five 2D-2D correspondences and one 2D-3D correspondence viewed by the same camera inside generalized camera, the H32f solver with three 2D-2D and two 2D-3D correspondences, and the H13f solver with one 2D-2D and three 2D-3D correspondences, on synthetic and real data. We show that in the presence of noise in the 3D points these solvers provide better estimates than the corresponding absolute pose solvers."
  },
  "wacv2023_main_eliminationofnon-novelsegmentsatmulti-scaleforfew-shotsegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Elimination of Non-Novel Segments at Multi-Scale for Few-Shot Segmentation",
    "authors": [
      "Alper Kayaba\u015f\u0131",
      "G\u00fclin T\u00fcfekci",
      "\u0130lkay Ulusoy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kayabasi_Elimination_of_Non-Novel_Segments_at_Multi-Scale_for_Few-Shot_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kayabasi_Elimination_of_Non-Novel_Segments_at_Multi-Scale_for_Few-Shot_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Few-shot segmentation aims to devise a generalizing model that segments query images from unseen classes during training with the guidance of a few support images whose class tally with the class of the query. There exist two domain-specific problems mentioned in the previous works, namely spatial inconsistency and bias towards seen classes. Taking the former problem into account, our method compares the support feature map with the query feature map at multi scales to become scale-agnostic. As a solution to the latter problem, a supervised model, called as base learner, is trained on available classes to accurately identify pixels belonging to seen classes. Hence, subsequent meta learner has a chance to discard areas belonging to seen classes with the help of an ensemble learning model that coordinates meta learner with the base learner. We simultaneously address these two vital problems for the first time and achieve state-of-the-art performances on both PASCAL-5i and COCO-20i datasets."
  },
  "wacv2023_main_continuallearningwithdependencypreservinghypernetworks": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Continual Learning With Dependency Preserving Hypernetworks",
    "authors": [
      "Dupati Srikar Chandra",
      "Sakshi Varshney",
      "P. K. Srijith",
      "Sunil Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chandra_Continual_Learning_With_Dependency_Preserving_Hypernetworks_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chandra_Continual_Learning_With_Dependency_Preserving_Hypernetworks_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Humans learn continually throughout their lifespan by accumulating diverse knowledge and fine-tuning it for future tasks. When presented with a similar goal, neural networks suffer from catastrophic forgetting if data distributions across sequential tasks are not stationary over the course of learning. An effective approach to address such continual learning (CL) problems is to use hypernetworks which generate task dependent weights for a target network. However, the continual learning performance of existing hypernetwork based approaches are affected by the assumption of independence of the weights across the layers in order to maintain parameter efficiency. To address this limitation, we propose a novel approach that uses a dependency preserving hypernetwork to generate weights for the target network while also maintaining the parameter efficiency. We propose to use recurrent neural network (RNN) based hypernetwork that can generate layer weights efficiently while allowing for dependencies across them. In addition, we propose novel regularisation and network growth techniques for the RNN based hypernetwork to further improve the continual learning performance. To demonstrate the effectiveness of the proposed methods, we conducted experiments on several image classification continual learning tasks and settings. We found that the proposed methods based on the RNN hypernetworks outperformed the baselines in all these CL settings and tasks."
  },
  "wacv2023_main_learninghowtomimicusingmodelexplanationstoguidedeeplearningtraining": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Learning How to MIMIC: Using Model Explanations To Guide Deep Learning Training",
    "authors": [
      "Matthew Watson",
      "Bashar Awwad Shiekh Hasan",
      "Noura Al Moubayed"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Watson_Learning_How_to_MIMIC_Using_Model_Explanations_To_Guide_Deep_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Watson_Learning_How_to_MIMIC_Using_Model_Explanations_To_Guide_Deep_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Healthcare is seen as one of the most influential applications of Deep Learning (DL). Increasingly, DL models are applied in healthcare settings with seemingly high levels of performance on-par with medical experts. Yet, very few are deployed into real-life scenarios with variable success rate. One of the main reasons for this is the lack of trust in those models by medical professionals driven by the black-box nature of the deployed models. Numerous explainable techniques have been developed to alleviate this issue by providing a view on how the model reached a given decision. Recent studies have shown that those explanations can expose the models' reliance on areas of the feature space that has no justifiable medical interpretation, widening the gap with the medical experts. In this paper we evaluate the deviation of saliency maps produced by DL classification models from radiologist's eye-gaze while they study the MIMIC-CXR-EGD images, and we propose a novel model architecture that utilises model explanations during training only (i.e. not during inference) to improve the overall plausibility of the model explanations. We substantially improve the similarity between the model's explanations and radiologists' eye-gaze data, reducing Kullback-Leibler Divergence by 90% and increasing Normalised Scanpath Saliency by 216%. We argue that this significant improvement is an important step towards building more robust and interpretable DL solutions in healthcare."
  },
  "wacv2023_main_learningbyhallucinatingvision-languagepre-trainingwithweaksupervision": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Learning by Hallucinating: Vision-Language Pre-Training With Weak Supervision",
    "authors": [
      "Tzu-Jui Julius Wang",
      "Jorma Laaksonen",
      "Tomas Langer",
      "Heikki Arponen",
      "Tom E. Bishop"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wang_Learning_by_Hallucinating_Vision-Language_Pre-Training_With_Weak_Supervision_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wang_Learning_by_Hallucinating_Vision-Language_Pre-Training_With_Weak_Supervision_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Weakly-supervised vision-language (V-L) pre-training (W-VLP) aims at learning cross-modal alignment with little or no paired data, such as aligned images and captions. Recent W-VLP methods, which pair visual features with object tags, help achieve performances comparable with some VLP models trained with aligned pairs in various V-L downstream tasks. This, however, is not the case in cross- modal retrieval (XMR). We argue that the learning of such a W-VLP model is curbed and biased by the object tags of limited semantics. We address the lack of paired V-L data for model supervision with a novel Visual Vocabulary based Feature Hallucinator (WFH), which is trained via weak supervision as a W-VLP model, not requiring images paired with captions. WFH generates visual hallucinations from texts, which are then paired with the originally unpaired texts, allowing more diverse interactions across modalities. Empirically, WFH consistently boosts the prior W-VLP works, e.g. U-VisualBERT (U-VB), over a variety of V-L tasks, i.e. XMR, Visual Question Answering, etc. Notably, benchmarked with recall@ 1,5,10 , it consistently improves U-VB on image-to-text and text-to-image retrieval on two popular datasets Flickr30K and MSCOCO. Meanwhile, it gains by at least 14.5% in cross-dataset generalization tests on these XMR tasks. Moreover, in other V-L downstream tasks considered, our WFH models are on par with models trained with paired V-L data, revealing the utility of unpaired data. These results demonstrate greater generalization of the proposed W-VLP model with WFH."
  },
  "wacv2023_main_self-supervisedrelativeposewithhomographymodel-fittingintheloop": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Supervised Relative Pose With Homography Model-Fitting in the Loop",
    "authors": [
      "Bruce R. Muller",
      "William A. P. Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Muller_Self-Supervised_Relative_Pose_With_Homography_Model-Fitting_in_the_Loop_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Muller_Self-Supervised_Relative_Pose_With_Homography_Model-Fitting_in_the_Loop_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We propose a self-supervised method for relative pose estimation for road scenes. By exploiting the approximate planarity of the local ground plane, we can extract a self-supervision signal via cross-projection between images using a homography derived from estimated ground-relative pose. We augment cross-projected perceptual loss by including classical image alignment in the network training loop. We use pretrained semantic segmentation and optical flow to extract ground plane correspondences between approximately aligned images and RANSAC to find the best fitting homography. By decomposing to ground-relative pose, we obtain pseudo labels that can be used for direct supervision. We show that this extremely simple geometric model is competitive for visual odometry with much more complex self-supervised methods that must learn depth estimation in conjunction with relative pose. Code and result videos: github.com/brucemuller/homographyVO."
  },
  "wacv2023_main_castconditionalattributesubsamplingtoolkitforfine-grainedevaluation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CAST: Conditional Attribute Subsampling Toolkit for Fine-Grained Evaluation",
    "authors": [
      "Wes Robbins",
      "Steven Zhou",
      "Aman Bhatta",
      "Chad Mello",
      "V\u00edtor Albiero",
      "Kevin W. Bowyer",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Robbins_CAST_Conditional_Attribute_Subsampling_Toolkit_for_Fine-Grained_Evaluation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Robbins_CAST_Conditional_Attribute_Subsampling_Toolkit_for_Fine-Grained_Evaluation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Thorough evaluation is critical for developing models that are fair and robust. In this work, we describe the Conditional Attribute Subsampling Toolkit (CAST) for selecting data subsets for fine-grained scientific evaluations. Our toolkit efficiently filters data given an arbitrary number of conditions for metadata attributes. The purpose of the toolkit is to allow researchers to easily to evaluate models on targeted test distributions. The functionality of CAST is demonstrated on the WebFace42M face Recognition dataset. We calculate over 50 attributes for this dataset including race, image quality, facial features, and accessories. Using our toolkit, we create over a hundred test sets conditioned on one or multiple attributes. Results are presented for subsets of various demographics and image quality ranges. Using eleven different subsets, we build a face recognition 1:1 verification benchmark called C11 that exclusively contains pairs that are near the decision threshold. Evaluation on C11 with state-of-the-art methods demonstrates the suitability of the proposed benchmark. The toolkit is publicly available at https://github.com/WesRobbins/CAST."
  },
  "wacv2023_main_seq-upssequentialuncertainty-awarepseudo-labelselectionforsemi-supervisedtextrecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Seq-UPS: Sequential Uncertainty-Aware Pseudo-Label Selection for Semi-Supervised Text Recognition",
    "authors": [
      "Gaurav Patel",
      "Jan P. Allebach",
      "Qiang Qiu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Patel_Seq-UPS_Sequential_Uncertainty-Aware_Pseudo-Label_Selection_for_Semi-Supervised_Text_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Patel_Seq-UPS_Sequential_Uncertainty-Aware_Pseudo-Label_Selection_for_Semi-Supervised_Text_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper looks at semi-supervised learning (SSL) for image-based text recognition. One of the most popular SSL approaches is pseudo-labeling (PL). PL approaches assign labels to unlabeled data before re-training the model with a combination of labeled and pseudo-labeled data. However, PL methods are severely degraded by noise and are prone to over-fitting to noisy labels, due to the inclusion of erroneous high confidence pseudo-labels generated from poorly calibrated models, thus, rendering threshold-based selection ineffective. Moreover, the combinatorial complexity of the hypothesis space and the error accumulation due to multiple incorrect autoregressive steps posit pseudo-labeling challenging for sequential self-training. To this end, we propose a pseudo-label generation and an uncertainty-based data selection framework for semi-supervised text recognition. We first use Beam-Search inference to yield highly probable hypotheses to assign pseudo-labels to the unlabelled examples. Then we adopt an ensemble of models, sampled by applying dropout, to obtain a robust estimate of the uncertainty associated with the prediction, considering both the character-level and word-level predictive distribution to select good quality pseudo-labels. Extensive experiments on several benchmark handwriting and scene-text datasets show that our method outperforms the baseline approaches and the previous state-of-the-art semi-supervised text-recognition methods."
  },
  "wacv2023_main_textandimageguided3davatargenerationandmanipulation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Text and Image Guided 3D Avatar Generation and Manipulation",
    "authors": [
      "Zehranaz Canfes",
      "M. Furkan Atasoy",
      "Alara Dirik",
      "Pinar Yanardag"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Canfes_Text_and_Image_Guided_3D_Avatar_Generation_and_Manipulation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Canfes_Text_and_Image_Guided_3D_Avatar_Generation_and_Manipulation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as 'a young face' or 'a surprised face'. We leverage the power of Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars, and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced, while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons."
  },
  "wacv2023_main_rastrestorablearbitrarystyletransferviamulti-restoration": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "RAST: Restorable Arbitrary Style Transfer via Multi-Restoration",
    "authors": [
      "Yingnan Ma",
      "Chenqiu Zhao",
      "Xudong Li",
      "Anup Basu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ma_RAST_Restorable_Arbitrary_Style_Transfer_via_Multi-Restoration_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ma_RAST_Restorable_Arbitrary_Style_Transfer_via_Multi-Restoration_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Arbitrary style transfer aims at reproducing the target image with provided artistic or photo-realistic styles. Even though existing approaches can successfully transfer style information, arbitrary style transfer still faces many challenges, such as the content leak issue. To be specific, the embedding of artistic style can lead to content changes. In this paper, we solve the content leak problem from the perspective of image restoration. In particular, an iterative architecture is proposed to achieve the restorable arbitrary style transfer (RAST), which can realize the transmission of both content and style information through the multi-restorations. We control the content-style balance in stylized images by the accuracy of image restoration. In order to ensure the effectiveness of the proposed RAST architecture, we design two novel loss functions: multi-restoration loss and style difference loss. In addition, we propose a new quantitative evaluation method to measure content preservation performance and style embedding performance. Comprehensive experiments comparing with state-of-the-art methods demonstrate that our proposed architecture can produce stylized images with superior performance on content preservation and style embedding."
  },
  "wacv2023_main_surfacenormalestimationfromoptimizedanddistributedlightsourcesusingdnn-basedphotometricstereo": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Surface Normal Estimation From Optimized and Distributed Light Sources Using DNN-Based Photometric Stereo",
    "authors": [
      "Takafumi Iwaguchi",
      "Hiroshi Kawasaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Iwaguchi_Surface_Normal_Estimation_From_Optimized_and_Distributed_Light_Sources_Using_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Iwaguchi_Surface_Normal_Estimation_From_Optimized_and_Distributed_Light_Sources_Using_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Photometric stereo (PS) is a major technique to recover surface normal for each pixel. However, since it assumes Lambertian surface and directional light to estimate the value, a large number of images are usually required to avoid the effects of outliers and noise. In this paper, we propose a technique to reduce the number of images by using distributed light sources, where the patterns are optimized by a deep neural network (DNN). In addition, to efficiently realize the distributed light, we use an optical diffuser with a video projector, where the diffuser is illuminated by the projector from behind, the illuminated area on the diffuser works as if an arbitrary-shaped area light. To estimate the surface normal using the distributed light source, we propose a near-light photometric stereo (NLPS) using DNN. Since optimization of the pattern of distributed light is achieved by a differentiable renderer, it is connected with NLPS network, achieving end-to-end learning. The experiments are conducted to show the successful estimation of the surface normal by our method from a small number of images."
  },
  "wacv2023_main_uparunifiedpedestrianattributerecognitionandpersonretrieval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "UPAR: Unified Pedestrian Attribute Recognition and Person Retrieval",
    "authors": [
      "Andreas Specker",
      "Mickael Cormier",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Specker_UPAR_Unified_Pedestrian_Attribute_Recognition_and_Person_Retrieval_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Specker_UPAR_Unified_Pedestrian_Attribute_Recognition_and_Person_Retrieval_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recognizing soft-biometric pedestrian attributes is essential in video surveillance and fashion retrieval. Recent works show promising results on single datasets. Nevertheless, the generalization ability of these methods under different attribute distributions, viewpoints, varying illumination, and low resolutions remains rarely understood due to strong biases and varying attributes in current datasets. To close this gap and support a systematic investigation, we present UPAR, the Unified Person Attribute Recognition Dataset. It is based on four well-known person attribute recognition datasets: PA100K, PETA, RAPv2, and Market1501. We unify those datasets by providing 3,3M additional annotations to harmonize 40 important binary attributes over 12 attribute categories across the datasets. We thus enable research on generalizable pedestrian attribute recognition as well as attribute-based person retrieval for the first time. Due to the vast variance of the image distribution, pedestrian pose, scale, and occlusion, existing approaches are greatly challenged both in terms of accuracy and efficiency. Furthermore, we develop a strong baseline for PAR and attribute-based person retrieval based on a thorough analysis of regularization methods. Our models achieve state-of-the-art performance in cross-domain and specialization settings on PA100k, PETA, RAPv2, Market1501-Attributes, and UPAR. We believe UPAR and our strong baseline will contribute to the artificial intelligence community and promote research on large-scale, generalizable attribute recognition systems."
  },
  "wacv2023_main_instance-dependentnoisylabellearningviagraphicalmodelling": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Instance-Dependent Noisy Label Learning via Graphical Modelling",
    "authors": [
      "Arpit Garg",
      "Cuong Nguyen",
      "Rafael Felix",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Garg_Instance-Dependent_Noisy_Label_Learning_via_Graphical_Modelling_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Garg_Instance-Dependent_Noisy_Label_Learning_via_Graphical_Modelling_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Noisy labels are unavoidable yet troublesome in the ecosystem of deep learning because models can easily overfit them. There are many types of label noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with IDN being the only type that depends on image information. Such dependence on image information makes IDN a critical type of label noise to study, given that labelling mistakes are caused in large part by insufficient or ambiguous information about the visual classes present in images. Aiming to provide an effective technique to address IDN, we present a new graphical modelling approach called InstanceGM, that combines discriminative and generative models. The main contributions of InstanceGM are: i) the use of the continuous Bernoulli distribution to train the generative model, offering significant training advantages, and ii) the exploration of a state-of-the-art noisy-label discriminative classifier to generate clean labels from instance-dependent noisy-label samples. InstanceGM is competitive with current noisy-label learning approaches, particularly in instance-dependent noise benchmarks using synthetic and real-world datasets, where our method shows better accuracy than the competitors in most experiments."
  },
  "wacv2023_main_ontheimportanceofdenoisingwhenlearningtocompressimages": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "On the Importance of Denoising When Learning To Compress Images",
    "authors": [
      "Benoit Brummer",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Brummer_On_the_Importance_of_Denoising_When_Learning_To_Compress_Images_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Brummer_On_the_Importance_of_Denoising_When_Learning_To_Compress_Images_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Image noise is ubiquitous in photography. However, image noise is not compressible nor desirable, thus attempting to convey the noise in compressed image bitstreams yields sub-par results in both rate and distortion. We propose to explicitly learn the image denoising task when training the codec. Therefore, we leverage the Natural Image Noise Dataset, which offers a wide variety of scenes captured with various noise levels. Given this training set, we show that a single model trained based on a mixture of images with variable noise levels appears to yield best-in-class results with both noisy and clean images, achieving better rate-distortion than a compression-only model or even than a pair of denoising-then-compression models with almost one order of magnitude fewer GMac operations."
  },
  "wacv2023_main_adanormadaptivegradientnormcorrectionbasedoptimizerforcnns": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "AdaNorm: Adaptive Gradient Norm Correction Based Optimizer for CNNs",
    "authors": [
      "Shiv Ram Dubey",
      "Satish Kumar Singh",
      "Bidyut Baran Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Dubey_AdaNorm_Adaptive_Gradient_Norm_Correction_Based_Optimizer_for_CNNs_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Dubey_AdaNorm_Adaptive_Gradient_Norm_Correction_Based_Optimizer_for_CNNs_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The stochastic gradient descent (SGD) optimizers are generally used to train the convolutional neural networks (CNNs). In recent years, several adaptive momentum based SGD optimizers have been introduced, such as Adam, diffGrad, Radam and AdaBelief. However, the existing SGD optimizers do not exploit the gradient norm of past iterations and lead to poor convergence and performance. In this paper, we propose a novel AdaNorm based SGD optimizers by correcting the norm of gradient in each iteration based on the adaptive training history of gradient norm. By doing so, the proposed optimizers are able to maintain high and representive gradient throughout the training and solves the low and atypical gradient problems. The proposed concept is generic and can be used with any existing SGD optimizer. We show the efficacy of the proposed AdaNorm with four state-of-the-art optimizers, including Adam, diffGrad, Radam and AdaBelief. We depict the performance improvement due to the proposed optimizers using three CNN models, including VGG16, ResNet18 and ResNet50, on three benchmark object recognition datasets, including CIFAR10, CIFAR100 and TinyImageNet."
  },
  "wacv2023_main_aerialimagedehazingwithattentivedeformabletransformers": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Aerial Image Dehazing With Attentive Deformable Transformers",
    "authors": [
      "Ashutosh Kulkarni",
      "Subrahmanyam Murala"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kulkarni_Aerial_Image_Dehazing_With_Attentive_Deformable_Transformers_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kulkarni_Aerial_Image_Dehazing_With_Attentive_Deformable_Transformers_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Aerial imagery is widely utilized in visual data dependent applications such as military surveillance, earthquake assessment, etc. For these applications, minute texture in the aerial image are essential as any disturbance can cause inaccurate prediction. However, atmospheric haze severely reduces the visibility of the scene to be analysed, and hence takes a toll on accuracy of higher level applications. Existing methods either utilize additional prior while training, or produce sub-optimal outputs on different densities of haze degradation, due to absence of local and global dependencies in the extracted features. Therefore, it is essential to have a texture preserving algorithm for aerial image dehazing. In light of this, we propose a work that introduces a novel deformable multi-head attention with spatially attentive offset extraction based solution for aerial image dehazing. Here, the deformable multi-head attention is introduced to reconstruct fine level texture in the restored image. We also introduce spatially attentive offset extractor in the deformable convolution for focusing on relevant contextual information. Further, edge boosting skip connections are proposed for effectively passing edge features from shallow layers to deeper layers of the network. Thorough experimentation on synthetic as well as real-world data, along with extensive ablation study, demonstrate that the proposed method outperforms the prevailing works on aerial image dehazing. The code is provided at https://github.com/ AshutoshKulkarni4998/AIDTransformer."
  },
  "wacv2023_main_evaluatinggenerativenetworksusinggaussianmixturesofimagefeatures": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Evaluating Generative Networks Using Gaussian Mixtures of Image Features",
    "authors": [
      "Lorenzo Luzi",
      "Carlos Ortiz Marrero",
      "Nile Wynar",
      "Richard G. Baraniuk",
      "Michael J. Henry"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Luzi_Evaluating_Generative_Networks_Using_Gaussian_Mixtures_of_Image_Features_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Luzi_Evaluating_Generative_Networks_Using_Gaussian_Mixtures_of_Image_Features_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We develop a measure for evaluating the performance of generative networks given two sets of images. A popular performance measure currently used to do this is the Frechet Inception Distance (FID). FID assumes that images featurized using the penultimate layer of Inception-v3 follow a Gaussian distribution, an assumption which cannot be violated if we wish to use FID as a metric. However, we show that Inception-v3 features of the ImageNet dataset are not Gaussian; in particular, every single marginal is not Gaussian. To remedy this problem, we model the featurized images using Gaussian mixture models (GMMs) and compute the 2-Wasserstein distance restricted to GMMs. We define a performance measure, which we call WaM, on two sets of images by using Inception-v3 (or another classifier) to featurize the images, estimate two GMMs, and use the restricted 2-Wasserstein distance to compare the GMMs. We experimentally show the advantages of WaM over FID, including how FID is more sensitive than WaM to imperceptible image perturbations. By modelling the non-Gaussian features obtained from Inception-v3 as GMMs and using a GMM metric, we can more accurately evaluate generative network performance."
  },
  "wacv2023_main_sparsityagnosticdepthcompletion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Sparsity Agnostic Depth Completion",
    "authors": [
      "Andrea Conti",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Conti_Sparsity_Agnostic_Depth_Completion_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Conti_Sparsity_Agnostic_Depth_Completion_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present a novel depth completion approach agnostic to the sparsity of depth points, that is very likely to vary in many practical applications. State-of-the-art approaches yield accurate results only when processing a specific density and distribution of input points, i.e. the one observed during training, narrowing their deployment in real use cases. On the contrary, our solution is robust to uneven distributions and extremely low densities never witnessed during training. Experimental results on standard indoor and outdoor benchmarks highlight the robustness of our framework, achieving accuracy comparable to state-of-the-art methods when tested with density and distribution equal to the training one while being much more accurate in the other cases. Our pretrained models and further material are available in our project page."
  },
  "wacv2023_main_movieclipvisualscenerecognitioninmovies": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MovieCLIP: Visual Scene Recognition in Movies",
    "authors": [
      "Digbalay Bose",
      "Rajat Hebbar",
      "Krishna Somandepalli",
      "Haoyang Zhang",
      "Yin Cui",
      "Kree Cole-McLaughlin",
      "Huisheng Wang",
      "Shrikanth Narayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bose_MovieCLIP_Visual_Scene_Recognition_in_Movies_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bose_MovieCLIP_Visual_Scene_Recognition_in_Movies_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Longform media such as movies have complex narrative structures, with events spanning a rich variety of ambient visual scenes. Domain-specific challenges associated with visual scenes in movies include transitions, person coverage, and a wide array of real-life and fictional scenarios. Existing visual scene datasets in movies have limited taxonomies and don't consider the visual scene transition within movie clips. In this work, we address the problem of visual scene recognition in movies by first automatically curating a new and extensive movie-centric taxonomy of 179 scene labels derived from movie scripts and auxiliary web-based video datasets. Instead of manual annotations which can be expensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips based on our proposed taxonomy. We provide baseline visual models trained on the weakly labeled dataset called MovieCLIP and evaluate them on an independent dataset verified by human raters. We show that leveraging features from models pretrained on MovieCLIP benefits downstream tasks such as multi-label scene and genre classification of web videos and movie trailers."
  },
  "wacv2023_main_dynamicre-weightingforlong-tailedsemi-supervisedlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dynamic Re-Weighting for Long-Tailed Semi-Supervised Learning",
    "authors": [
      "Hanyu Peng",
      "Weiguo Pian",
      "Mingming Sun",
      "Ping Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Peng_Dynamic_Re-Weighting_for_Long-Tailed_Semi-Supervised_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Peng_Dynamic_Re-Weighting_for_Long-Tailed_Semi-Supervised_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The high demand for labeled data that characterizes deep learning is very labor-intensive. Semi-supervised Learning (SSL), acting as one of the breakthroughs, allows for the avoidance of this labeling loss thanks to its small amount of labeled data, alongside extracting information from a large amount of unlabeled data. And there is hope that the same performance for SSL can be achieved when compared to supervised learning methods. Regrettably, the research community has often developed SSL regarding the nature of a balanced data set; in contrast, real data is often imbalanced or even long-tailed. The need to study SSL under imbalance is therefore critical. In this paper, we shall essentially extend FixMatch (a SSL method) to the imbalanced case. We find that the unlabeled data is as well highly imbalanced during the training process; in this respect we propose a re-weighting solution based on the effective number. Furthermore, since prediction uncertainty leads to temporal variations in the number of pseudo-labels, we are innovative in proposing a dynamic re-weighting scheme on the unlabeled data. The simplicity and validity of our method are backed up by strong experimental evidence. Especially on CIFAR-10, CIFAR-100, ImageNet127 data sets, our approach provides the strongest results against previous methods across various scales of imbalance."
  },
  "wacv2023_main_groundingscenegraphsonnaturalimagesviavisio-lingualmessagepassing": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing",
    "authors": [
      "Aditay Tripathi",
      "Anand Mishra",
      "Anirban Chakraborty"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Tripathi_Grounding_Scene_Graphs_on_Natural_Images_via_Visio-Lingual_Message_Passing_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Tripathi_Grounding_Scene_Graphs_on_Natural_Images_via_Visio-Lingual_Message_Passing_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper presents a framework for jointly grounding objects that follow certain semantic relationship constraints given in a scene graph. A typical natural scene contains several objects, often exhibiting visual relationships of varied complexities between them. These inter-object relationships provide strong contextual cues towards improving grounding performance compared to a traditional object query-only-based localization task. A scene graph is an efficient and structured way to represent all the objects and their semantic relationships in the image. In an attempt towards bridging these two modalities representing scenes and utilizing contextual information for improving object localization, we rigorously study the problem of grounding scene graphs on natural images. To this end, we propose a novel graph neural network-based approach referred to as Visio-Lingual Message PAssing Graph Neural Network (VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object proposals as nodes and an edge between a pair of nodes representing a plausible relation between them. Then a three-step inter-graph and intra-graph message passing is performed to learn the context-dependent representation of the proposals and query objects. These object representations are used to score the proposals to generate object localization. The proposed method significantly outperforms the baselines on four public datasets."
  },
  "wacv2023_main_improvingmulti-fidelityoptimizationwitharecurringlearningrateforhyperparametertuning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Improving Multi-Fidelity Optimization With a Recurring Learning Rate for Hyperparameter Tuning",
    "authors": [
      "HyunJae Lee",
      "Gihyeon Lee",
      "Junhwan Kim",
      "Sungjun Cho",
      "Dohyun Kim",
      "Donggeun Yoo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_Improving_Multi-Fidelity_Optimization_With_a_Recurring_Learning_Rate_for_Hyperparameter_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_Improving_Multi-Fidelity_Optimization_With_a_Recurring_Learning_Rate_for_Hyperparameter_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Despite the evolution of Convolutional Neural Networks (CNNs), their performance is surprisingly dependent on the choice of hyperparameters. However, it remains challenging to efficiently explore large hyperparameter search space due to the long training times of modern CNNs. Multi-fidelity optimization enables the exploration of more hyperparameter configurations given budget by early termination of unpromising configurations. However, it often results in selecting a sub-optimal configuration as training with the high-performing configuration typically converges slowly in an early phase. In this paper, we propose Multi-fidelity Optimization with a Recurring Learning rate (MORL) which incorporates CNNs' optimization process into multi-fidelity optimization. MORL alleviates the problem of slow-starter and achieves a more precise low-fidelity approximation. Our comprehensive experiments on general image classification, transfer learning, and semi-supervised learning demonstrate the effectiveness of MORL over other multi-fidelity optimization methods such as Successive Halving Algorithm (SHA) and Hyperband. Furthermore, it achieves significant performance improvements over hand-tuned hyperparameter configuration within a practical budget."
  },
  "wacv2023_main_ti2nettemporalidentityinconsistencynetworkfordeepfakedetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "TI2Net: Temporal Identity Inconsistency Network for Deepfake Detection",
    "authors": [
      "Baoping Liu",
      "Bo Liu",
      "Ming Ding",
      "Tianqing Zhu",
      "Xin Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Liu_TI2Net_Temporal_Identity_Inconsistency_Network_for_Deepfake_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Liu_TI2Net_Temporal_Identity_Inconsistency_Network_for_Deepfake_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we propose a Temporal Identity Inconsistency Network (TI2Net), a Deepfake detector that focuses on temporal identity inconsistency. Specifically, TI2Net recognizes fake videos by capturing the dissimilarities of human faces among video frames of the same identity. Therefore, TI2Net is a reference-agnostic detector and can be used on unseen datasets. For a video clip of a given identity, identity information in all frames will first be encoded to identity vectors. TI2Net learns the temporal identity embedding from the temporal difference of the identity vectors. The temporal embedding, representing the identity inconsistency in the video clip, is finally used to determine the authenticity of the video clip. During training, TI2Net incorporates triplet loss to learn more discriminative temporal embeddings. We conduct comprehensive experiments to evaluate the performance of the proposed TI2Net. Experimental results indicate that TI2Net generalizes well to unseen manipulations and datasets with unseen identities. Besides, TI2Net also shows robust performance against compression and additive noise."
  },
  "wacv2023_main_scorenetlearningnon-uniformattentionandaugmentationfortransformer-basedhistopathologicalimageclassification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification",
    "authors": [
      "Thomas Stegm\u00fcller",
      "Behzad Bozorgtabar",
      "Antoine Spahr",
      "Jean-Philippe Thiran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Stegmuller_ScoreNet_Learning_Non-Uniform_Attention_and_Augmentation_for_Transformer-Based_Histopathological_Image_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Stegmuller_ScoreNet_Learning_Non-Uniform_Attention_and_Augmentation_for_Transformer-Based_Histopathological_Image_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning MIL to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and current MIL-based approaches often process images uniformly, discarding the inter-patches interactions. To alleviate these issues, we propose ScoreNet, a new efficient transformer that exploits a differentiable recommendation stage to extract discriminative image regions and dedicate computational resources accordingly. The proposed transformer leverages the local and global attention of a few dynamically recommended high-resolution regions at an efficient computational cost. We further introduce a novel mixing data-augmentation, namely ScoreMix, by leveraging the image's semantic distribution to guide the data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly simple and mitigates the pitfalls of previous augmentations, which assume a uniform semantic distribution and risk mislabeling the samples. Thorough experiments and ablation studies on three breast cancer histology datasets of Haematoxylin & Eosin (H&E) have validated the superiority of our approach over prior arts, including transformer-based models on tumour regions-of-interest TRoIs classification. ScoreNet equipped with proposed ScoreMix augmentation demonstrates better generalization capabilities and achieves new state-of-the-art (SOTA) results with only 50% of the data compared to other mixing augmentation variants. Finally, ScoreNet yields high efficacy and outperforms SOTA efficient transformers, namely TransPath and SwinTransformer, with throughput around 3x and 4x higher than the aforementioned architectures, respectively."
  },
  "wacv2023_main_cross-viewimagesequencegeo-localization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Cross-View Image Sequence Geo-Localization",
    "authors": [
      "Xiaohan Zhang",
      "Waqas Sultani",
      "Safwan Wshah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_Cross-View_Image_Sequence_Geo-Localization_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Cross-View_Image_Sequence_Geo-Localization_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Cross-view geo-localization aims to estimate the GPS location of a query ground-view image by matching it to images from a reference database of geo-tagged aerial images. To address this challenging problem, recent approaches use panoramic ground-view images to increase the range of visibility. Although appealing, panoramic images are not readily available compared to the videos of limited Field-Of-View (FOV) images. In this paper, we present the first cross-view geo-localization method that works on a sequence of limited FOV images. Our model is trained end-to-end to capture the temporal structure that lies within the frames using the attention-based temporal feature aggregation module. To robustly tackle different sequences length and GPS noises during inference, we propose to use a sequential dropout scheme to simulate variant length sequences. To evaluate the proposed approach in realistic settings, we present a new large-scale dataset containing ground-view sequences along with the corresponding aerial-view images. Extensive experiments and comparisons demonstrate the superiority of the proposed approach compared to several competitive baselines."
  },
  "wacv2023_main_cokecontrastivelearningforrobustkeypointdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CoKe: Contrastive Learning for Robust Keypoint Detection",
    "authors": [
      "Yutong Bai",
      "Angtian Wang",
      "Adam Kortylewski",
      "Alan Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bai_CoKe_Contrastive_Learning_for_Robust_Keypoint_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bai_CoKe_Contrastive_Learning_for_Robust_Keypoint_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we introduce a contrastive learning framework for keypoint detection (CoKe). Keypoint detection differs from other visual tasks where contrastive learning has been applied because the input is a set of images in which multiple keypoints are annotated. This requires the contrastive learning to be extended such that the keypoints are represented and detected independently, which enables the contrastive loss to make the keypoint features different from each other and from the background. Our approach has two benefits: It enables us to exploit the power of contrastive learning for keypoint detection, and by detecting each keypoint independently the detection becomes more robust to occlusion compared to holistic methods, such as stacked hourglass networks, which attempt to detect all keypoints jointly. Our CoKe framework introduces several technical innovations. In particular, we introduce: (i) A clutter bank to represent non-keypoint features; (ii) a keypoint bank that stores prototypical representations of keypoints to approximate the contrastive loss between keypoints; and (iii) a cumulative moving average update to learn the keypoint prototypes while training the feature extractor. Our experiments on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D) show that our approach works as well, or better than, alternative methods for keypoint detection, even for human keypoints, for which the literature is vast. Moreover, we observe that CoKe is exceptionally robust to partial occlusion and previously unseen object poses."
  },
  "wacv2023_main_burstabenchmarkforunifyingobjectrecognition,segmentationandtrackinginvideo": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "BURST: A Benchmark for Unifying Object Recognition, Segmentation and Tracking in Video",
    "authors": [
      "Ali Athar",
      "Jonathon Luiten",
      "Paul Voigtlaender",
      "Tarasha Khurana",
      "Achal Dave",
      "Bastian Leibe",
      "Deva Ramanan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Athar_BURST_A_Benchmark_for_Unifying_Object_Recognition_Segmentation_and_Tracking_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Athar_BURST_A_Benchmark_for_Unifying_Object_Recognition_Segmentation_and_Tracking_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multiple existing benchmarks involve tracking and segmenting objects in video e.g., Video Object Segmentation (VOS) and Multi-Object Tracking and Segmentation (MOTS), but there is little interaction between them due to the use of disparate benchmark datasets and metrics (e.g. \\JnF, mAP, sMOTSA). As a result, published works usually target a particular benchmark, and are not easily comparable to each another. We believe that the development of generalized methods that can tackle multiple tasks requires greater cohesion among these research sub-communities. In this paper, we aim to facilitate this by proposing BURST, a dataset which contains thousands of diverse videos with high-quality object masks, and an associated benchmark with six tasks involving object tracking and segmentation in video. All tasks are evaluated using the same data and comparable metrics, which enables researchers to consider them in unison, and hence, more effectively pool knowledge from different methods across different tasks. Additionally, we demonstrate several baselines for all tasks and show that approaches for one task can be applied to another with a quantifiable and explainable performance difference. Dataset annotations are available at: https://github.com/Ali2500/BURST-benchmark."
  },
  "wacv2023_main_collaborativemulti-teacherknowledgedistillationforlearninglowbit-widthdeepneuralnetworks": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Collaborative Multi-Teacher Knowledge Distillation for Learning Low Bit-Width Deep Neural Networks",
    "authors": [
      "Cuong Pham",
      "Tuan Hoang",
      "Thanh-Toan Do"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Pham_Collaborative_Multi-Teacher_Knowledge_Distillation_for_Learning_Low_Bit-Width_Deep_Neural_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Pham_Collaborative_Multi-Teacher_Knowledge_Distillation_for_Learning_Low_Bit-Width_Deep_Neural_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Knowledge distillation which learns a lightweight student model by distilling knowledge from a cumbersome teacher model is an attractive approach for learning compact deep neural networks (DNNs). Recent works further improve student network performance by leveraging multiple teacher networks. However, most of the existing knowledge distillation-based multi-teacher methods use separately pretrained teachers. This limits the collaborative learning between teachers and the mutual learning between teachers and student. Network quantization is another at- tractive approach for learning compact DNNs. However, most existing network quantization methods are developed and evaluated without considering multi-teacher support to enhance the performance of quantized student model. In this paper, we propose a novel framework that leverages both multi-teacher knowledge distillation and network quantization for learning low bit-width DNNs. The proposed method encourages both collaborative learning between quantized teachers and mutual learning between quantized teachers and quantized student. During learning process, at corresponding layers, knowledge from teachers will form an importance-aware shared knowledge which will be used as input for teachers at subsequent layers and also be used to guide student. Our experimental results on CIFAR100 and ImageNet datasets show that the compact quantized student models trained with our method achieve competitive results compared to other state-of-the-art methods, and in some cases, indeed surpass the full precision models."
  },
  "wacv2023_main_knowingwhattolabelforfewshotmicroscopyimagecellsegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Knowing What To Label for Few Shot Microscopy Image Cell Segmentation",
    "authors": [
      "Youssef Dawoud",
      "Arij Bouazizi",
      "Katharina Ernst",
      "Gustavo Carneiro",
      "Vasileios Belagiannis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Dawoud_Knowing_What_To_Label_for_Few_Shot_Microscopy_Image_Cell_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Dawoud_Knowing_What_To_Label_for_Few_Shot_Microscopy_Image_Cell_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In microscopy image cell segmentation, it is common to train a deep neural network on source data, containing different types of microscopy images, and then fine-tune it using a support set comprising a few randomly selected and annotated training target images. In this paper, we argue that the random selection of unlabelled training target images to be annotated and included in the support set may not enable an effective fine-tuning process, so we propose a new approach to optimise this image selection process. Our approach involves a new scoring function to find informative unlabelled target images. In particular, we propose to measure the consistency in the model predictions on target images against specific data augmentations. However, we observe that the model trained with source datasets does not reliably evaluate consistency on target images. To alleviate this problem, we propose novel self-supervised pretext tasks to compute the scores of unlabelled target images. Finally, the top few images with the least consistency scores are added to the support set for oracle (i.e., expert) annotation and later used to fine-tune the model to the target images. In our evaluations that involve the segmentation of five different types of cell images, we demonstrate promising results on several target test sets compared to the random selection approach as well as other selection approaches, such as Shannon's entropy and Monte-Carlo dropout. Our code will be made publicly available."
  },
  "wacv2023_main_ssfe-netself-supervisedfeatureenhancementforultra-fine-grainedfew-shotclassincrementallearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SSFE-Net: Self-Supervised Feature Enhancement for Ultra-Fine-Grained Few-Shot Class Incremental Learning",
    "authors": [
      "Zicheng Pan",
      "Xiaohan Yu",
      "Miaohua Zhang",
      "Yongsheng Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Pan_SSFE-Net_Self-Supervised_Feature_Enhancement_for_Ultra-Fine-Grained_Few-Shot_Class_Incremental_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Pan_SSFE-Net_Self-Supervised_Feature_Enhancement_for_Ultra-Fine-Grained_Few-Shot_Class_Incremental_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Ultra-Fine-Grained Visual Categorization (ultra-FGVC) has become a popular problem due to its great real-world potential for classifying the same or closely related species with very similar layouts. However, there present many challenges for the existing ultra-FGVC methods, firstly there are always not enough samples in the existing ultra-FGVC datasets based on which the models can easily get overfitting. Secondly, in practice, we are likely to find new species that we have not seen before and need to add them to existing models, which is known as incremental learning. The existing methods solve these problems by Few-Shot Class Incremental Learning (FSCIL), but the main challenge of the FSCIL models on ultra-FGVC tasks lies in their inferior discrimination detection ability since they usually use low-capacity networks to extract features, which leads to insufficient discriminative details extraction from ultra-fine-grained images. In this paper, a self-supervised feature enhancement for the few-shot incremental learning network (SSFE-Net) is proposed to solve this problem. Specifically, a self-supervised learning (SSL) and knowledge distillation (KD) framework is developed to enhance the feature extraction of the low-capacity backbone network for ultra-FGVC few-shot class incremental learning tasks. Besides, we for the first time create a series of benchmarks for FSCIL tasks on two public ultra-FGVC datasets and three normal fine-grained datasets, which will facilitate the development of the Ultra-FGVC community. Extensive experimental results on public ultra-FGVC datasets and other state-of-the-art benchmarks consistently demonstrate the effectiveness of the proposed method."
  },
  "wacv2023_main_mevidmulti-viewextendedvideoswithidentitiesforvideopersonre-identification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MEVID: Multi-View Extended Videos With Identities for Video Person Re-Identification",
    "authors": [
      "Daniel Davila",
      "Dawei Du",
      "Bryon Lewis",
      "Christopher Funk",
      "Joseph Van Pelt",
      "Roderic Collins",
      "Kellie Corona",
      "Matt Brown",
      "Scott McCloskey",
      "Anthony Hoogs",
      "Brian Clipp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Davila_MEVID_Multi-View_Extended_Videos_With_Identities_for_Video_Person_Re-Identification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Davila_MEVID_Multi-View_Extended_Videos_With_Identities_for_Video_Person_Re-Identification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we present the Multi-view Extended Videos with Identities (MEVID) dataset for large-scale, video person re-identification (ReID) in the wild. To our knowledge, MEVID represents the most-varied video person ReID dataset, spanning an extensive indoor and outdoor environment across nine unique dates in a 73-day window, various camera viewpoints, and entity clothing changes. Specifically, we label the identities of 158 unique people wearing 598 outfits taken from 8,092 tracklets, average length of about 590 frames, seen in 33 camera views from the very-large-scale MEVA person activities dataset. While other datasets have more unique identities, MEVID emphasizes a richer set of information about each individual, such as: 4 outfits/identity vs. 2 outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5 simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID. Being based on the MEVA video dataset, we also inherit data that is intentionally demographically balanced to the continental United States. To accelerate the annotation process, we developed a semi-automatic annotation framework and GUI that combines state-of-the-art real-time models for object detection, pose estimation, person ReID, and multi-object tracking. We evaluate several state-of-the-art methods on MEVID challenge problems and comprehensively quantify their robustness in terms of changes of outfit, scale, and background location. Our quantitative analysis on the realistic, unique aspects of MEVID shows that there are significant remaining challenges in video person ReID and indicates important directions for future research."
  },
  "wacv2023_main_audio-visualefficientconformerforrobustspeechrecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Audio-Visual Efficient Conformer for Robust Speech Recognition",
    "authors": [
      "Maxime Burchi",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Burchi_Audio-Visual_Efficient_Conformer_for_Robust_Speech_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Burchi_Audio-Visual_Efficient_Conformer_for_Robust_Speech_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "End-to-end Automatic Speech Recognition (ASR) systems based on neural networks have seen large improvements in recent years. The availability of large scale hand-labeled datasets and sufficient computing resources made it possible to train powerful deep neural networks, reaching very low Word Error Rate (WER) on academic benchmarks. However, despite impressive performance on clean audio samples, a drop of performance is often observed on noisy speech. In this work, we propose to improve the noise robustness of the recently proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based architecture by processing both audio and visual modalities. We improve previous lip reading methods using an Efficient Conformer back-end on top of a ResNet-18 visual front-end and by adding intermediate CTC losses between blocks. We condition intermediate block features on early predictions using Inter CTC residual modules to relax the conditional independence assumption of CTC-based models. We also replace the Efficient Conformer grouped attention by a more efficient and simpler attention mechanism that we call patch attention. We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and visual modalities allows to better recognize speech in the presence of environmental noise and significantly accelerate training, reaching lower WER with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC) model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on LRS2 and LRS3 test sets. Code and pretrained models are available at https://github.com/burchim/AVEC."
  },
  "wacv2023_main_digiface-1m1milliondigitalfaceimagesforfacerecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DigiFace-1M: 1 Million Digital Face Images for Face Recognition",
    "authors": [
      "Gwangbin Bae",
      "Martin de La Gorce",
      "Tadas Baltru\u0161aitis",
      "Charlie Hewitt",
      "Dong Chen",
      "Julien Valentin",
      "Roberto Cipolla",
      "Jingjing Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a large-scale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline. We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to SynFace, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images."
  },
  "wacv2023_main_couplformerrethinkingvisiontransformerwithcouplingattention": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Couplformer: Rethinking Vision Transformer With Coupling Attention",
    "authors": [
      "Hai Lan",
      "Xihao Wang",
      "Hao Shen",
      "Peidong Liang",
      "Xian Wei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lan_Couplformer_Rethinking_Vision_Transformer_With_Coupling_Attention_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lan_Couplformer_Rethinking_Vision_Transformer_With_Coupling_Attention_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory consumption hinders the deployment of the Transformer model on the embedded system where the computing resources are limited. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. Our method enables the Transformer model to improve time and memory efficiency while maintaining expressive power. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1K classification task, the Couplformer can significantly decrease 42% memory consumption compared with the regular Transformer. Meanwhile, it accesses sufficient accuracy requirements, which outperforms 0.56% on Top-1 accuracy and occupies the same memory footprint. Besides, the Couplformer achieves state-of-art performance in MS COCO 2017 object detection and instance segmentation tasks. As a result, the Couplformer can serve as an efficient backbone in visual tasks and provide a novel perspective on deploying attention mechanisms for researchers."
  },
  "wacv2023_main_syntheticlatentfingerprintgenerator": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Synthetic Latent Fingerprint Generator",
    "authors": [
      "Andr\u00e9 Brasil Vieira Wyzykowski",
      "Anil K. Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wyzykowski_Synthetic_Latent_Fingerprint_Generator_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wyzykowski_Synthetic_Latent_Fingerprint_Generator_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Given a full fingerprint image (rolled or slap), we present CycleGAN models to generate multiple latent impressions of the same identity as the full print. Our models can control the degree of distortion, noise, blurriness and occlusion in the generated latent print images to obtain Good, Bad and Ugly latent image categories as introduced in the NIST SD27 latent database. The contributions of our work are twofold: (i) demonstrate the similarity of synthetically generated latent fingerprint images to crime scene latents in NIST SD27 and MSP databases as evaluated by the NIST NFIQ 2 quality measure and recognition accuracies obtained by a SOTA fingerprint matcher, and (ii) use of synthetic latents to augment small-size latent training databases in the public domain to improve the performance of DeepPrint, a SOTA fingerprint matcher designed for rolled to rolled fingerprint matching on three latent databases (NIST SD27, NIST SD302, and IIITD-SLF). As an example, with synthetic latent data augmentation, the Rank-1 retrieval performance of DeepPrint is improved from 15.50% to 29.07% on challenging NIST SD27 latent database. Our approach for generating synthetic latent fingerprints can be used to improve the recognition performance of any latent matcher and its individual components (e.g., enhancement, segmentation and feature extraction). https://prip-lab.github.io/Synthetic-Latent-Fingerprint-Generator/"
  },
  "wacv2023_main_accumulatedtrivialattentionmattersinvisiontransformersonsmalldatasets": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets",
    "authors": [
      "Xiangyu Chen",
      "Qinghao Hu",
      "Kaidong Li",
      "Cuncong Zhong",
      "Guanghui Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Accumulated_Trivial_Attention_Matters_in_Vision_Transformers_on_Small_Datasets_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Accumulated_Trivial_Attention_Matters_in_Vision_Transformers_on_Small_Datasets_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Vision Transformers has demonstrated competitive performance on computer vision tasks benefiting from their ability to capture long-range dependencies with multi-head self-attention modules and multi-layer perceptron. However, calculating global attention brings another disadvantage compared with convolutional neural networks, i.e. requiring much more data and computations to converge, which makes it difficult to generalize well on small datasets, which is common in practical applications. Previous works are either focusing on transferring knowledge from large datasets or adjusting the structure for small datasets. After carefully examining the self-attention modules, we discover that the number of trivial attention weights is far greater than the important ones and the accumulated trivial weights are dominating the attention in Vision Transformers due to their large quantity, which is not handled by the attention itself. This will cover useful non-trivial attention and harm the performance when trivial attention includes more noise, e.g. in shallow layers for some backbones. To solve this issue, we proposed to divide attention weights into trivial and non-trivial ones by thresholds, then Suppressing Accumulated Trivial Attention (SATA) weights by proposed Trivial WeIghts Suppression Transformation (TWIST) to reduce attention noise. Extensive experiments on CIFAR-100 and Tiny-ImageNet datasets show that our suppressing method boosts the accuracy of Vision Transformers by up to 2.3%. Code is available at https://github.com/xiangyu8/SATA."
  },
  "wacv2023_main_cross-modalsemanticenhancedinteractionforimage-sentenceretrieval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Cross-Modal Semantic Enhanced Interaction for Image-Sentence Retrieval",
    "authors": [
      "Xuri Ge",
      "Fuhai Chen",
      "Songpei Xu",
      "Fuxiang Tao",
      "Joemon M. Jose"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ge_Cross-Modal_Semantic_Enhanced_Interaction_for_Image-Sentence_Retrieval_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ge_Cross-Modal_Semantic_Enhanced_Interaction_for_Image-Sentence_Retrieval_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Image-sentence retrieval has attracted extensive research attention in multimedia and computer vision due to its promising application. The key issue lies in jointly learning the visual and textual representation to accurately estimate their similarity. To this end, the mainstream schema adopts an object-word based attention to calculate their relevance scores and refine their interactive representations with the attention features, which, however, neglects the context of the object representation on the inter-object relationship that matches the predicates in sentences. In this paper, we propose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for image-sentence retrieval, which correlates the intra- and inter-modal semantics between objects and words. In particular, we first design the intra-modal spatial and semantic graphs based reasoning to enhance the semantic representations of objects guided by the explicit relationships of the objects' spatial positions and their scene graph. Then the visual and textual semantic representations are refined jointly via the inter-modal interactive attention and the cross-modal alignment. To correlate the context of objects with the textual context, we further refine the visual semantic representation via the cross-level object-sentence and word-image based interactive attention. Experimental results on seven standard evaluation metrics show that the proposed CMSEI outperforms the state-of-the-art and the alternative approaches on MS-COCO and Flickr30K benchmarks."
  },
  "wacv2023_main_towardsdiscriminativeandtransferableone-stagefew-shotobjectdetectors": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Towards Discriminative and Transferable One-Stage Few-Shot Object Detectors",
    "authors": [
      "Karim Guirguis",
      "Mohamed Abdelsamad",
      "George Eskandar",
      "Ahmed Hendawy",
      "Matthias Kayser",
      "Bin Yang",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Guirguis_Towards_Discriminative_and_Transferable_One-Stage_Few-Shot_Object_Detectors_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Guirguis_Towards_Discriminative_and_Transferable_One-Stage_Few-Shot_Object_Detectors_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recent object detection models have proved valuable for many robotics and manufacturing tasks, but they require large amounts of annotated data for each new class of objects they are trained for. Few-shot object detection (FSOD) aims to address this problem by learning novel classes given only a few samples of annotated data. While competitive results have been achieved using two-stage FSOD detectors, typically faster one-stage FSODs underperform in comparison. We make the discovery that the large gap in performance between two-stage and one-stage FSODs is mainly due to their weak discriminability, which is explained away by a small post-fusion receptive field and a small number of foreground samples in the loss function. We propose a new one-stage FSOD framework to address these limitations - Few-shot RetinaNet (FSRN). Specifically, we propose: (1) a multi-way support training strategy to augment the number of foreground samples for dense meta-detectors during training, (2) an early multi-level feature fusion providing a wide receptive field that covers the whole anchor area, (3) two augmentation techniques on query and source images to enhance transferability. Extensive experiments demonstrate that the proposed approach addresses the limitations of previous methods and boosts both discriminability and transferability. FSRN is two times faster than twostage FSODs while remaining competitive in accuracy, and it triples the state-of-the-art of one-stage meta-detectors on the competitive 10-shot MS-COCO benchmark. On the PASCAL VOC benchmark, the proposed approach consistently outperforms one-stage meta-detectors and many two-stage FSODs."
  },
  "wacv2023_main_layerdoclayer-wiseextractionofspatialhierarchicalstructureinvisually-richdocuments": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "LayerDoc: Layer-Wise Extraction of Spatial Hierarchical Structure in Visually-Rich Documents",
    "authors": [
      "Puneet Mathur",
      "Rajiv Jain",
      "Ashutosh Mehra",
      "Jiuxiang Gu",
      "Franck Dernoncourt",
      "Anandhavelu N.",
      "Quan Tran",
      "Verena Kaynig-Fittkau",
      "Ani Nenkova",
      "Dinesh Manocha",
      "Vlad I. Morariu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mathur_LayerDoc_Layer-Wise_Extraction_of_Spatial_Hierarchical_Structure_in_Visually-Rich_Documents_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mathur_LayerDoc_Layer-Wise_Extraction_of_Spatial_Hierarchical_Structure_in_Visually-Rich_Documents_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Digital documents often contain images and scanned text. Parsing such visually-rich documents is a core task for workflow automation, but it remains challenging since most documents do not encode explicit layout information, e.g., how characters and words are grouped into boxes and ordered into larger semantic entities. Current state-of-the-art layout extraction methods are challenged on such documents as they rely on word sequences to have correct reading order and do not exploit their hierarchical structure. We propose LayerDoc, an approach that uses visual features, textual semantics, and spatial coordinates along with constraint inference to extract the hierarchical layout structure of documents in a bottom-up layer-wise fashion. LayerDoc recursively groups smaller regions into larger semantic elements in 2D to infer complex nested hierarchies. Experiments show that our approach outperforms competitive baselines by 10-15% on three diverse datasets of forms and mobile app screen layouts for the tasks of spatial region classification, higher-order group identification, layout hierarchy extraction, reading order detection, and word grouping."
  },
  "wacv2023_main_sssdself-supervisedselfdistillation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SSSD: Self-Supervised Self Distillation",
    "authors": [
      "Wei-Chi Chen",
      "Wei-Ta Chu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_SSSD_Self-Supervised_Self_Distillation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_SSSD_Self-Supervised_Self_Distillation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "With labeled data, self distillation (SD) has been proposed to develop compact but effective models without a complex teacher model available in advance. Such approaches need labeled data to guide the self distillation process. Inspired by self-supervised (SS) learning, we propose a self-supervised self distillation (SSSD) approach in this work. Based on an unlabeled image dataset, a model is constructed to learn visual representations in a self-supervised manner. This pre-trained model is then adopted to extract visual representations of the target dataset and generates pseudo labels via clustering. The pseudo labels guide the SD process, and thus enable SD to proceed in an unsupervised way (no data labels are required at all). We verify this idea based on evaluations on the CIFAR-10, CIFAR-100, and ImageNet-1K datasets, and demonstrate the effectiveness of this unsupervised SD approach. Performance outperforming similar frameworks is also shown."
  },
  "wacv2023_main_celltransposefew-shotdomainadaptationforcellularinstancesegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CellTranspose: Few-Shot Domain Adaptation for Cellular Instance Segmentation",
    "authors": [
      "Matthew R. Keaton",
      "Ram J. Zaveri",
      "Gianfranco Doretto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Keaton_CellTranspose_Few-Shot_Domain_Adaptation_for_Cellular_Instance_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Keaton_CellTranspose_Few-Shot_Domain_Adaptation_for_Cellular_Instance_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Automated cellular instance segmentation is a process utilized for accelerating biological research for the past two decades, and recent advancements have produced higher quality results with less effort from the biologist. Most current endeavors focus on completely cutting the researcher out of the picture by generating highly generalized models. However, these models invariably fail when faced with novel data, distributed differently than the ones used for training. Rather than approaching the problem with methods that presume the availability of large amounts of target data and computing power for retraining, in this work we address the even greater challenge of designing an approach that requires minimal amounts of new annotated data as well as training time. We do so by designing specialized contrastive losses that leverage the few annotated samples very efficiently. A large set of results show that 3 to 5 annotations lead to models with accuracy that: 1) significantly mitigate the covariate shift effects; 2) matches or surpasses other adaptation methods; 3) even approaches methods that have been fully retrained on the target distribution. The adaptation training is only a few minutes, paving a path towards a balance between model performance, computing requirements and expert-level annotation needs."
  },
  "wacv2023_main_hardtotrackobjectswithirregularmotionsandsimilarappearances?makeiteasierbybufferingthematchingspace": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Hard To Track Objects With Irregular Motions and Similar Appearances? Make It Easier by Buffering the Matching Space",
    "authors": [
      "Fan Yang",
      "Shigeyuki Odashima",
      "Shoichi Masui",
      "Shan Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We propose a Cascaded Buffered IoU (C-BIoU) tracker to track multiple objects that have irregular motions and indistinguishable appearances. When appearance features are unreliable and geometric features are confused by irregular motions, applying conventional Multiple Object Tracking (MOT) methods may generate unsatisfactory results. To address this issue, our C-BIoU tracker adds buffers to expand the matching space of detections and tracks, which mitigates the effect of irregular motions in two aspects: one is to directly match identical but non-overlapping detections and tracks in adjacent frames, and the other is to compensate for the motion estimation bias in the matching space. In addition, to reduce the risk of overexpansion of the matching space, cascaded matching is employed: first matching alive tracks and detections with a small buffer, and then matching unmatched tracks and detections with a large buffer. Despite its simplicity, our C-BIoU tracker works surprisingly well and achieves state-of-the-art results on MOT datasets that focus on irregular motions and indistinguishable appearances. Moreover, the C-BIoU tracker is the dominant component for our 2nd place solution in the CVPR'22 SoccerNet MOT and the ECCV'22 MOTComplex DanceTrack challenges. Finally, we analyze the limitation of our C-BIoU tracker in ablation studies and discuss its application scope."
  },
  "wacv2023_main_selfsupervisedlowdosecomputedtomographyimagedenoisingusinginvertiblenetworkexploitinginterslicecongruence": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self Supervised Low Dose Computed Tomography Image Denoising Using Invertible Network Exploiting Inter Slice Congruence",
    "authors": [
      "Sutanu Bera",
      "Prabir Kumar Biswas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bera_Self_Supervised_Low_Dose_Computed_Tomography_Image_Denoising_Using_Invertible_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bera_Self_Supervised_Low_Dose_Computed_Tomography_Image_Denoising_Using_Invertible_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The resurgence of deep neural networks has created an alternative pathway for low-dose computed tomography denoising by learning a nonlinear transformation function between low-dose CT (LDCT) and normal-dose CT (NDCT) image pairs. However, those paired LDCT and NDCT images are rarely available in the clinical environment, making deep neural network deployment infeasible. This study proposes a novel method for self-supervised low-dose CT denoising to alleviate the requirement of paired LDCT and NDCT images. Specifically, we have trained an invertible neural network to minimize the pixel-based mean square distance between a noisy slice and the average of its two immediate adjacent noisy slices. We have shown the aforementioned is similar to training a neural network to minimize the distance between clean NDCT and noisy LDCT image pairs. Again, during the reverse mapping of the invertible network, the output image is mapped to the original input image, similar to cycle consistency loss. Finally, the trained invertible network's forward mapping is used for denoising LDCT images. Extensive experiments on two publicly available datasets showed that our method performs favourably against other existing unsupervised methods."
  },
  "wacv2023_main_leveraginglocalpatchdifferencesinmulti-objectscenesforgenerativeadversarialattacks": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Leveraging Local Patch Differences in Multi-Object Scenes for Generative Adversarial Attacks",
    "authors": [
      "Abhishek Aich",
      "Shasha Li",
      "Chengyu Song",
      "M. Salman Asif",
      "Srikanth V. Krishnamurthy",
      "Amit K. Roy-Chowdhury"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Aich_Leveraging_Local_Patch_Differences_in_Multi-Object_Scenes_for_Generative_Adversarial_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Aich_Leveraging_Local_Patch_Differences_in_Multi-Object_Scenes_for_Generative_Adversarial_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "State-of-the-art generative model-based attacks against image classifiers overwhelmingly focus on single-object (ie., single dominant object) images. Different from such settings, we tackle a more practical problem of generating adversarial perturbations using multi-object (ie., multiple dominant objects) images as they are representative of most real-world scenes. Our goal is to design an attack strategy that can learn from such natural scenes by leveraging the local patch differences that occur inherently in such images (eg. difference between the local patch on the object 'person' and the object 'bike' in a traffic scene). Our key idea is to misclassify an adversarial multi-object image by confusing the victim classifier for each local patch in the image. Based on this, we propose a novel generative attack (called Local Patch Difference or LPD-Attack) where a novel contrastive loss function uses the aforesaid local differences in feature space of multi-object scenes to optimize the perturbation generator. Through various experiments across diverse victim convolutional neural networks, we show that our approach outperforms baseline generative attacks with highly transferable perturbations when evaluated under different white-box and black-box settings."
  },
  "wacv2023_main_prnpanopticrefinementnetwork": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "PRN: Panoptic Refinement Network",
    "authors": [
      "Bo Sun",
      "Jason Kuen",
      "Zhe Lin",
      "Philippos Mordohai",
      "Simon Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sun_PRN_Panoptic_Refinement_Network_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sun_PRN_Panoptic_Refinement_Network_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Panoptic segmentation is the task of uniquely assigning every pixel in an image to either a semantic label or an individual object instance, generating a coherent and complete scene description. Many current panoptic segmentation methods, however, predict masks of semantic classes and object instances in separate branches, yielding inconsistent predictions. Moreover, because state-of-the-art panoptic segmentation models rely on box proposals, the instance masks predicted are often of low-resolution. To overcome these limitations, we propose the Panoptic Refinement Network (PRN), which takes masks from base panoptic segmentation models and refines them jointly to produce coherent results. PRN extends the offset map-based architecture of Panoptic-Deeplab with several novel ideas including a foreground mask and instance bounding box offsets, as well as coordinate convolutions for improved spatial prediction. Experimental results on COCO and Cityscapes show that PRN can significantly improve already accurate results from a variety of panoptic segmentation networks."
  },
  "wacv2023_main_controllable3dgenerativeadversarialfacemodelviadisentanglingshapeandappearance": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Controllable 3D Generative Adversarial Face Model via Disentangling Shape and Appearance",
    "authors": [
      "Fariborz Taherkhani",
      "Aashish Rai",
      "Quankai Gao",
      "Shaunak Srivastava",
      "Xuanbai Chen",
      "Fernando de la Torre",
      "Steven Song",
      "Aayush Prakash",
      "Daeil Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Taherkhani_Controllable_3D_Generative_Adversarial_Face_Model_via_Disentangling_Shape_and_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Taherkhani_Controllable_3D_Generative_Adversarial_Face_Model_via_Disentangling_Shape_and_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "3D face modeling has been an active area of research in computer vision and computer graphics, fueling applications ranging from facial expression transfer in virtual avatars to synthetic data generation. Existing 3D deep learning generative models (e.g., VAE, GANs) allow generating compact face representations (both shape and texture) that can model non-linearities in the shape and appearance space (e.g., scatter effects, specularities,..). However, they lack the capability to control the generation of subtle expressions. This paper proposes a new 3D face generative model that can decouple identity and expression and provides granular control over expressions. In particular, we propose using a pair of supervised auto-encoder and generative adversarial networks to produce high-quality 3D faces, both in terms of appearance and shape. Experimental results in the generation of 3D faces learned with holistic expression labels, or Action Unit (AU) labels, show how we can decouple identity and expression; gaining fine-control over expressions while preserving identity."
  },
  "wacv2023_main_self-supervisedmonoculardepthestimationsolvingtheedge-fatteningproblem": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem",
    "authors": [
      "Xingyu Chen",
      "Ruonan Zhang",
      "Ji Jiang",
      "Yan Wang",
      "Ge Li",
      "Thomas H. Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Self-Supervised_Monocular_Depth_Estimation_Solving_the_Edge-Fattening_Problem_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Self-Supervised_Monocular_Depth_Estimation_Solving_the_Edge-Fattening_Problem_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Self-supervised monocular depth estimation (MDE) models universally suffer from the notorious edge-fattening issue. Triplet loss, popular for metric learning, has made a great success in many computer vision tasks. In this paper, we redesign the patch-based triplet loss in MDE to alleviate the ubiquitous edge-fattening issue. We show two drawbacks of the raw triplet loss in MDE and demonstrate our problem-driven redesigns. First, we present a min. operator based strategy applied to all negative samples, to prevent well-performing negatives sheltering the error of edge-fattening negatives. Second, we split the anchor-positive distance and anchor-negative distance from within the original triplet, which directly optimizes the positives without any mutual effect with the negatives. Extensive experiments show the combination of these two small redesigns can achieve unprecedented results: Our powerful and versatile triplet loss not only makes our model outperform all previous SoTA by a large margin, but also provides substantial performance boosts to a large number of existing models, while introducing no extra inference computation at all."
  },
  "wacv2023_main_monodvpsaself-supervisedmonoculardepthestimationapproachtodepth-awarevideopanopticsegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MonoDVPS: A Self-Supervised Monocular Depth Estimation Approach to Depth-Aware Video Panoptic Segmentation",
    "authors": [
      "Andra Petrovai",
      "Sergiu Nedevschi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Petrovai_MonoDVPS_A_Self-Supervised_Monocular_Depth_Estimation_Approach_to_Depth-Aware_Video_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Petrovai_MonoDVPS_A_Self-Supervised_Monocular_Depth_Estimation_Approach_to_Depth-Aware_Video_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Depth-aware video panoptic segmentation tackles the inverse projection problem of restoring panoptic 3D point clouds from video sequences, where the 3D points are augmented with semantic classes and temporally consistent instance identifiers. We propose a novel solution with a multi-task network that performs monocular depth estimation and video panoptic segmentation. Since acquiring ground truth labels for both depth and image segmentation has a relatively large cost, we leverage the power of unlabeled video sequences with self-supervised monocular depth estimation and semi-supervised learning from pseudo-labels for video panoptic segmentation. To further improve the depth prediction, we introduce panoptic-guided depth losses and a novel panoptic masking scheme for moving objects to avoid corrupting the training signal. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that our model with the proposed improvements achieves competitive results and fast inference speed."
  },
  "wacv2023_main_wavelength-aware2dconvolutionsforhyperspectralimaging": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Wavelength-Aware 2D Convolutions for Hyperspectral Imaging",
    "authors": [
      "Leon Amadeus Varga",
      "Martin Messmer",
      "Nuri Benbarka",
      "Andreas Zell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Varga_Wavelength-Aware_2D_Convolutions_for_Hyperspectral_Imaging_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Varga_Wavelength-Aware_2D_Convolutions_for_Hyperspectral_Imaging_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep Learning could drastically boost the classification accuracy for Hyperspectral Imaging (HSI). Still, the training on the mostly small hyperspectral data sets is not trivial. Two key challenges are the large channel dimension of the recordings and the incompatibility between cameras of different manufacturers. By introducing a suitable model bias and continuously defining the channel dimension, we propose a 2D convolution optimized for these challenges of Hyperspectral Imaging. We evaluate the method based on two different hyperspectral applications (inline inspection and remote sensing). Besides the shown superiority of the model, the modification adds additional explanatory power. In addition, the model learns the necessary camera filters in a data-driven manner. Based on these camera filters, an optimal camera can be designed."
  },
  "wacv2023_main_contrastivelossesarenaturalcriteriaforunsupervisedvideosummarization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization",
    "authors": [
      "Zongshang Pang",
      "Yuta Nakashima",
      "Mayu Otani",
      "Hajime Nagahara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Pang_Contrastive_Losses_Are_Natural_Criteria_for_Unsupervised_Video_Summarization_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Pang_Contrastive_Losses_Are_Natural_Criteria_for_Unsupervised_Video_Summarization_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Video summarization aims to select a most informative subset of frames in a video to facilitate efficient video browsing. Unsupervised methods usually rely on heuristic training objectives such as diversity and representativeness. However, such methods need to bootstrap the online-generated summaries to compute the objectives for importance score regression. We consider such a pipeline inefficient and seek to directly quantify the frame-level importance with the help of contrastive losses in the representation learning literature. Leveraging the contrastive losses, we propose three metrics featuring a desirable key frame: local dissimilarity, global consistency, and uniqueness. With features pre-trained on an image classification task, the metrics can already yield high-quality importance scores, demonstrating better or competitive performance compared with past heavily-trained methods. We show that by refining the pre-trained features with contrastive learning, the frame-level importance scores can be further improved, and the model can learn from random videos and generalize to test videos with decent performance."
  },
  "wacv2023_main_spatiallymulti-conditionalimagegeneration": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Spatially Multi-Conditional Image Generation",
    "authors": [
      "Nikola Popovi\u0107",
      "Ritika Chakraborty",
      "Danda Pani Paudel",
      "Thomas Probst",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Popovic_Spatially_Multi-Conditional_Image_Generation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Popovic_Spatially_Multi-Conditional_Image_Generation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In most scenarios, conditional image generation can be thought of as an inversion of the image understanding process. Since generic image understanding involves solving multiple tasks, it is natural to aim at generating images via multi conditioning. However, multi-conditional image generation is a very challenging problem due to the heterogeneity and the sparsity of the (in practice) available conditioning labels. In this work, we propose a novel neural architecture to address the problem of heterogeneity and sparsity of the spatially multi-conditional labels. Our choice of spatial conditioning, such as by semantics and depth, is driven by the promise it holds for better control of the image generation process. The proposed method uses a transformer-like architecture operating pixel-wise, which receives the available labels as input tokens to merge them in a learned homogeneous space of labels. The merged labels are then used for image generation via conditional generative adversarial training. In this process, the sparsity of the labels is handled by simply dropping the input tokens corresponding to the missing labels at the desired locations, thanks to the proposed pixel-wise operating architecture. Our experiments on three benchmark datasets demonstrate the clear superiority of our method over the state-of-the-art and compared baselines."
  },
  "wacv2023_main_towardsonlinedomainadaptiveobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Towards Online Domain Adaptive Object Detection",
    "authors": [
      "Vibashan VS",
      "Poojan Oza",
      "Vishal M. Patel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/VS_Towards_Online_Domain_Adaptive_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/VS_Towards_Online_Domain_Adaptive_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Existing object detection models assume both the training and test data are sampled from the same source domain. This assumption does not hold true when these detectors are deployed in real-world applications, where they encounter new visual domains. Unsupervised Domain Adaptation (UDA) methods are generally employed to mitigate the adverse effects caused by domain shift. Existing UDA methods operate in an offline manner where the model is first adapted toward the target domain and then deployed in real-world applications. However, this offline adaptation strategy is not suitable for real-world applications as the model frequently encounters new domain shifts. Hence, it is critical to develop a feasible UDA method that generalizes to the new domain shifts encountered during deployment time in a continuous online manner. To this end, we propose a novel unified adaptation framework that adapts and improves generalization on the target domain in both offline and online settings. Specifically, we introduce MemXformer - a cross-attention transformer-based memory module where items in the memory take advantage of domain shifts and record prototypical patterns of the target distribution. Further, MemXformer produces strong positive and negative pairs to guide a novel contrastive loss, which enhances target-specific representation learning. Experiments on diverse detection benchmarks show that the proposed strategy producs state-of-the-art performance in both offline and online settings. To the best of our knowledge, this is the first work to address online and offline adaptation settings for object detection. Source code will be released after review."
  },
  "wacv2023_main_whatcanwelearnbypredictingaccuracy?": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "What Can We Learn by Predicting Accuracy?",
    "authors": [
      "Olivier Risser-Maroix",
      "Benjamin Chamand"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Risser-Maroix_What_Can_We_Learn_by_Predicting_Accuracy_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Risser-Maroix_What_Can_We_Learn_by_Predicting_Accuracy_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper seeks to answer the following question: \"What can we learn by predicting accuracy?\". Indeed, classification is one of the most popular tasks in machine learning, and many loss functions have been developed to maximize this non-differentiable objective function. Unlike past work on loss function design, which was guided mainly by intuition and theory before being validated by experimentation, here we propose to approach this problem in the opposite way: we seek to extract knowledge by experimentation. This data-driven approach is similar to that used in physics to discover general laws from data. We used a symbolic regression method to automatically find a mathematical expression highly correlated with a linear classifier's accuracy. The formula discovered on more than 260 datasets of embeddings has a Pearson's correlation of 0.96 and a r2 of 0.93. More interestingly, this formula is highly explainable and confirms insights from various previous papers on loss design. We hope this work will open new perspectives in the search for new heuristics leading to a deeper understanding of machine learning theory."
  },
  "wacv2023_main_nlmvs-netdeepnon-lambertianmulti-viewstereo": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "nLMVS-Net: Deep Non-Lambertian Multi-View Stereo",
    "authors": [
      "Kohei Yamashita",
      "Yuto Enyo",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yamashita_nLMVS-Net_Deep_Non-Lambertian_Multi-View_Stereo_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yamashita_nLMVS-Net_Deep_Non-Lambertian_Multi-View_Stereo_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We introduce a novel multi-view stereo (MVS) method that can simultaneously recover not just per-pixel depth but also surface normals, together with the reflectance of textureless, complex non-Lambertian surfaces captured under known but natural illumination. Our key idea is to formulate MVS as an end-to-end learnable network, which we refer to as nLMVS-Net, that seamlessly integrates radiometric cues to leverage surface normals as view-independent surface features for learned cost volume construction and filtering. It first estimates surface normals as pixel-wise probability densities for each view with a novel shape-from-shading network. These per-pixel surface normal densities and the input multi-view images are then input to a novel cost volume filtering network that learns to recover per-pixel depth and surface normal. The reflectance is also explicitly estimated by alternating with geometry reconstruction. Extensive quantitative evaluations on newly established synthetic and real-world datasets show that nLMVS-Net can robustly and accurately recover the shape and reflectance of complex objects in natural settings."
  },
  "wacv2023_main_ev-nerfeventbasedneuralradiancefield": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Ev-NeRF: Event Based Neural Radiance Field",
    "authors": [
      "Inwoo Hwang",
      "Junho Kim",
      "Young Min Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hwang_Ev-NeRF_Event_Based_Neural_Radiance_Field_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hwang_Ev-NeRF_Event_Based_Neural_Radiance_Field_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present Ev-NeRF, a Neural Radiance Field derived from event data. While event cameras can measure subtle brightness changes in high frame rates, the measurements in low lighting or extreme motion suffer from significant domain discrepancy with complex noise. As a result, the performance of event-based vision tasks does not transfer to challenging environments, where the event cameras are expected to thrive over normal cameras. We find that the multi-view consistency of NeRF provides a powerful self-supervision signal for eliminating spurious measurements and extracting the consistent underlying structure despite highly noisy input. Instead of posed images of the original NeRF, the input to Ev-NeRF is the event measurements accompanied by the movements of the sensors. Using the loss function that reflects the measurement model of the sensor, Ev-NeRF creates an integrated neural volume that summarizes the unstructured and sparse data points captured for about 2-4 seconds. The generated neural volume can also produce intensity images from novel views with reasonable depth estimates, which can serve as a high-quality input to various vision-based tasks. Our results show that Ev-NeRF achieves competitive performance for intensity image reconstruction under extreme noise conditions and high-dynamic-range imaging."
  },
  "wacv2023_main_jointlylearningbandselectionandfilterarraydesignforhyperspectralimaging": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Jointly Learning Band Selection and Filter Array Design for Hyperspectral Imaging",
    "authors": [
      "Ke Li",
      "Dengxin Dai",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Li_Jointly_Learning_Band_Selection_and_Filter_Array_Design_for_Hyperspectral_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Li_Jointly_Learning_Band_Selection_and_Filter_Array_Design_for_Hyperspectral_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "A single-shot multispectral camera equipped with an optimized color filter array (CFA) has the potential to deliver a fast and low-cost hyperspectral (HS) imaging system. Previous solutions are largely restricted to designing demosaicing algorithms for fixed CFAs - be it the Bayer color pattern or evenly-spaced spectral multiplexing patterns. Since sampling and reconstruction are tightly-coupled, the ability to search for an optimal solution is severely constrained by using predefined CFAs. In this work, we simultaneously address the problem of spectral band selection, CFA design, image demosaicing, and spectral image recovery in a joint learning framework for single-shot HS imaging. We propose a reinforcement learning (RL) based method for spectral band selection and a novel neural network for CFA generation, image demosaicing, and HS image recovery. The final spectral reconstruction accuracy is used to supervise the training of the main network to maximize the synergies between those tightly-related tasks. The RL method regards the main network as an agent to collect reward. Our final method delivers a simple setup - as simple as an RGB camera - for HS imaging. Experimental results show that our method outperforms competing methods by a large margin."
  },
  "wacv2023_main_indirectlanguage-guidedzero-shotdeepmetriclearningforimages": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "InDiReCT: Language-Guided Zero-Shot Deep Metric Learning for Images",
    "authors": [
      "Konstantin Kobs",
      "Michael Steininger",
      "Andreas Hotho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kobs_InDiReCT_Language-Guided_Zero-Shot_Deep_Metric_Learning_for_Images_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kobs_InDiReCT_Language-Guided_Zero-Shot_Deep_Metric_Learning_for_Images_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Common Deep Metric Learning (DML) datasets specify only one notion of similarity, e.g., two images in the Cars196 dataset are deemed similar if they show the same car model. We argue that depending on the application, users of image retrieval systems have different and changing similarity notions that should be incorporated as easily as possible. Therefore, we present Language-Guided Zero-Shot Deep Metric Learning (LanZ-DML) as a new DML setting in which users control the aspects that should be important for image representations without training data by only using natural language. To this end, we propose InDiReCT (Image representations using Dimensionality Reduction on CLIP embedded Texts), a model for LanZ-DML on images that exclusively uses a few text prompts for training. InDiReCT utilizes CLIP as a fixed feature extractor for images and texts and transfers the variation in text prompt embeddings to the image embedding space. Extensive experiments on five datasets and overall thirteen similarity notions show that, despite not seeing any images during training, InDiReCT performs better than strong baselines and approaches the performance of fully-supervised models. An analysis reveals that InDiReCT learns to focus on regions of the image that correlate with the desired similarity notion, which makes it a fast to train and easy to use method to create custom embedding spaces only using natural language."
  },
  "wacv2023_main_cut-pasteconsistencylearningforsemi-supervisedlesionsegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Cut-Paste Consistency Learning for Semi-Supervised Lesion Segmentation",
    "authors": [
      "Boon Peng Yap",
      "Beng Koon Ng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yap_Cut-Paste_Consistency_Learning_for_Semi-Supervised_Lesion_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yap_Cut-Paste_Consistency_Learning_for_Semi-Supervised_Lesion_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Semi-supervised learning has the potential to improve the data-efficiency of training data-hungry deep neural networks, which is especially important for medical image analysis tasks where labeled data is scarce. In this work, we present a simple semi-supervised learning method for lesion segmentation tasks based on the ideas of cut-paste augmentation and consistency regularization. By exploiting the mask information available in the labeled data, we synthesize partially labeled samples from the unlabeled images so that the usual supervised learning objective (e.g., binary cross entropy) can be applied. Additionally, we introduce a background consistency term to regularize the training on the unlabeled background regions of the synthetic images. We empirically verify the effectiveness of the proposed method on two public lesion segmentation datasets, including an eye fundus photograph dataset and a brain CT scan dataset. The experiment results indicate that our method achieves consistent and superior performance over other self-training and consistency-based methods without introducing sophisticated network components."
  },
  "wacv2023_main_medicalimagesegmentationviacascadedattentiondecoding": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Medical Image Segmentation via Cascaded Attention Decoding",
    "authors": [
      "Md Mostafijur Rahman",
      "Radu Marculescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Rahman_Medical_Image_Segmentation_via_Cascaded_Attention_Decoding_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Rahman_Medical_Image_Segmentation_via_Cascaded_Attention_Decoding_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Transformers have shown great promise in medical image segmentation due to their ability to capture long-range dependencies through self-attention. However, they lack the ability to learn the local (contextual) relations among pixels. Previous works try to overcome this problem by embedding convolutional layers either in the encoder or decoder modules of transformers thus ending up sometimes with inconsistent features. To address this issue, we propose a novel attention-based decoder, namely CASCaded Attention DEcoder (CASCADE), which leverages the multiscale features of hierarchical vision transformers. CASCADE consists of i) an attention gate which fuses features with skip connections and ii) a convolutional attention module that enhances the long-range and local context by suppressing background information. We use a multi-stage feature and loss aggregation framework due to their faster convergence and better performance. Our experiments demonstrate that transformers with CASCADE significantly outperform state-of-the-art CNN- and transformer-based approaches, obtaining up to 5.07% and 6.16% improvements in DICE and mIoU scores, respectively. CASCADE opens new ways of designing better attention-based decoders."
  },
  "wacv2023_main_visualizingglobalexplanationsofpointclouddnns": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Visualizing Global Explanations of Point Cloud DNNs",
    "authors": [
      "Hanxiao Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Tan_Visualizing_Global_Explanations_of_Point_Cloud_DNNs_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Tan_Visualizing_Global_Explanations_of_Point_Cloud_DNNs_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "So far, few researchers have targeted the explainability of point cloud neural networks. Part of the explainability methods are not directly applicable to those networks due to the structural specifics. In this work, we show that Activation Maximization (AM) with traditional pixel-wise regularizations fails to generate human-perceptible global explanations for point cloud networks. We propose new generative model-based AM approaches to clearly outline the global explanations and enhance their comprehensibility. Additionally, we propose a composite evaluation metric to address the limitations of existing evaluating methods, which simultaneously takes into account activation value, diversity and perceptibility. Extensive experiments demonstrate that our generative-based AM approaches outperform regularization-based ones both qualitatively and quantitatively. To the best of our knowledge, this is the first work investigating global explainability of point cloud networks. Our code is available at: https://github.com/Explain3D/PointCloudAM."
  },
  "wacv2023_main_lcslearningcompressiblesubspacesforefficient,adaptive,real-timenetworkcompressionatinferencetime": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "LCS: Learning Compressible Subspaces for Efficient, Adaptive, Real-Time Network Compression at Inference Time",
    "authors": [
      "Elvis Nunez",
      "Maxwell Horton",
      "Anish Prabhu",
      "Anurag Ranjan",
      "Ali Farhadi",
      "Mohammad Rastegari"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nunez_LCS_Learning_Compressible_Subspaces_for_Efficient_Adaptive_Real-Time_Network_Compression_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nunez_LCS_Learning_Compressible_Subspaces_for_Efficient_Adaptive_Real-Time_Network_Compression_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "When deploying deep neural networks (DNNs) to a device, it is traditionally assumed that available computational resources (compute, memory, and power) remain static. However, real-world computing systems do not always provide stable resource guarantees. Computational resources need to be conserved when load from other processes is high, or available memory is low. In this work, we present a training procedure to produce DNNs that can be compressed in real-time to arbitrary compression levels entirely on-device. This enables the deployment of a single model that can efficiently adapt to its host device's available resources. We formulate this problem as learning an adaptively compressible network subspace, where one end is optimized for accuracy, and the other for efficiency. Our subspace model requires no recalibration nor retraining when changing compression levels. Moreover, our generic training framework is amenable to multiple forms of compression, and we present results for unstructured sparsity, structured sparsity, and quantization on a variety of architectures. We present models that require a single extra copy of network parameters, as well as models that require no extra parameters. Both models allow for operation at any compression level within a wide range (for example, 0% to 90% for structured sparsity with ResNet18 on ImageNet). At each compression level, our models achieve an accuracy comparable to a baseline model optimized for that particular compression level. To our knowledge, our method is the first to enable adaptive on-device network compression with little to no computational overhead."
  },
  "wacv2023_main_fine-contextshadowdetectionusingshadowremoval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fine-Context Shadow Detection Using Shadow Removal",
    "authors": [
      "Jeya Maria Jose Valanarasu",
      "Vishal M. Patel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Valanarasu_Fine-Context_Shadow_Detection_Using_Shadow_Removal_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Valanarasu_Fine-Context_Shadow_Detection_Using_Shadow_Removal_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Current shadow detection methods perform poorly when detecting shadow regions that are small, unclear or have blurry edges. In this work, we attempt to address this problem on two fronts. First, we propose a Fine Context-aware Shadow Detection Network (FCSD-Net), where we constraint the receptive field size and focus on low-level features to learn fine context features better. Second, we propose a new learning strategy, called Restore to Detect (R2D), where we show that when a deep neural network is trained for restoration (shadow removal), it learns meaningful features to delineate the shadow masks as well. To make use of this complementary nature of shadow detection and removal tasks, we train an auxiliary network for shadow removal and propose a complementary feature learning block (CFL) to learn and fuse meaningful features from shadow removal network to the shadow detection network. We train the proposed network, FCSD-Net, using the R2D learning strategy across multiple datasets. Experimental results on three public shadow detection datasets (ISTD, SBU and UCF) show that our method improves the shadow detection performance while being able to detect fine context better compared to the other recent methods. Our proposed learning strategy can also be adopted easily as a useful pipeline in future advances in shadow detection and removal."
  },
  "wacv2023_main_spatialconsistencylossfortrainingmulti-labelclassifiersfromsingle-labelannotations": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Spatial Consistency Loss for Training Multi-Label Classifiers From Single-Label Annotations",
    "authors": [
      "Thomas Verelst",
      "Paul K. Rubenstein",
      "Marcin Eichner",
      "Tinne Tuytelaars",
      "Maxim Berman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Verelst_Spatial_Consistency_Loss_for_Training_Multi-Label_Classifiers_From_Single-Label_Annotations_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Verelst_Spatial_Consistency_Loss_for_Training_Multi-Label_Classifiers_From_Single-Label_Annotations_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multi-label image classification is more applicable 'in the wild' than single-label classification, as natural images usually contain multiple objects. However, exhaustively annotating images with every object of interest is costly and time-consuming. We train multi-label classifiers from datasets where each image is annotated with a single positive label only. As the presence of all other classes is unknown, we propose an Expected Negative loss that builds a set of expected negative labels in addition to the annotated positives. This set is determined based on prediction consistency, by averaging predictions over consecutive training epochs to build robust targets. Moreover, the crop data-augmentation leads to additional label noise by cropping out the single annotated object. Our novel spatial consistency loss improves supervision and ensures consistency of the spatial feature maps by maintaining per-class running-average heatmaps for each training image. We use MS-COCO, Pascal VOC, NUS-WIDE and CUB-Birds datasets to demonstrate the gains of the Expected Negative loss in combination with consistency and spatial consistency losses. We also demonstrate improved multi-label classification mAP on ImageNet-1K using the ReaL multi-label validation set."
  },
  "wacv2023_main_arubaanarchitecture-agnosticbalancedlossforaerialobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ARUBA: An Architecture-Agnostic Balanced Loss for Aerial Object Detection",
    "authors": [
      "Rebbapragada V. C. Sairam",
      "Monish Keswani",
      "Uttaran Sinha",
      "Nishit Shah",
      "Vineeth N. Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sairam_ARUBA_An_Architecture-Agnostic_Balanced_Loss_for_Aerial_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sairam_ARUBA_An_Architecture-Agnostic_Balanced_Loss_for_Aerial_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep neural networks tend to reciprocate the bias of their training dataset. In object detection, the bias exists in the form of various imbalances such as class, background-foreground, and object size. In this paper, we denote size of an object as the number of pixels it covers in an image and size imbalance as the over-representation of certain sizes of objects in a dataset. We aim to address the problem of size imbalance in drone-based aerial image datasets. Existing methods for solving size imbalance are based on architectural changes that utilize multiple scales of images or feature maps for detecting objects of different sizes. We, on the other hand, propose a novel ARchitectUre-agnostic BAlanced Loss (ARUBA) that can be applied as a plugin on top of any object detection model. It follows a neighborhood-driven approach inspired by the ordinality of object size. We evaluate the effectiveness of our approach through comprehensive experiments on aerial datasets such as HRSC2016, DOTAv1.0, DOTAv1.5 and VisDrone and obtain consistent improvement in performance."
  },
  "wacv2023_main_multimodalmulti-headconvolutionalattentionwithvariouskernelsizesformedicalimagesuper-resolution": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Multimodal Multi-Head Convolutional Attention With Various Kernel Sizes for Medical Image Super-Resolution",
    "authors": [
      "Mariana-Iuliana Georgescu",
      "Radu Tudor Ionescu",
      "Andreea-Iuliana Miron",
      "Olivian Savencu",
      "Nicolae-C\u0103t\u0103lin Ristea",
      "Nicolae Verga",
      "Fahad Shahbaz Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Georgescu_Multimodal_Multi-Head_Convolutional_Attention_With_Various_Kernel_Sizes_for_Medical_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Georgescu_Multimodal_Multi-Head_Convolutional_Attention_With_Various_Kernel_Sizes_for_Medical_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques capture several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multimodal multi-head convolutional attention module to super-resolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention, and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for super-resolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads. Our code is freely available at https://github.com/lilygeorgescu/MHCA."
  },
  "wacv2023_main_fusslfuzzyuncertainselfsupervisedlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "FUSSL: Fuzzy Uncertain Self Supervised Learning",
    "authors": [
      "Salman Mohamadi",
      "Gianfranco Doretto",
      "Donald A. Adjeroh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mohamadi_FUSSL_Fuzzy_Uncertain_Self_Supervised_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mohamadi_FUSSL_Fuzzy_Uncertain_Self_Supervised_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. Similar to some other disciplines in deep representation learning, one main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regard- less of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. We performed extensive experiments under multiple settings to show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles."
  },
  "wacv2023_main_ddnerfdepthdistributionneuralradiancefields": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DDNeRF: Depth Distribution Neural Radiance Fields",
    "authors": [
      "David Dadon",
      "Ohad Fried",
      "Yacov Hel-Or"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Dadon_DDNeRF_Depth_Distribution_Neural_Radiance_Fields_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Dadon_DDNeRF_Depth_Distribution_Neural_Radiance_Fields_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The field of implicit neural representation has made significant progress. Models such as neural radiance fields (NeRF), which uses relatively small neural networks, can represent high-quality scenes and achieve state-of-the-art results for novel view synthesis. Training these types of networks, however, is still computationally expensive and the model struggles with real life 360 degree scenes. In this work, we propose the depth distribution neural radiance field (DDNeRF), a new method that significantly increases sampling efficiency along rays during training, while achieving superior results for a given sampling budget. DDNeRF achieves this performance by learning a more accurate representation of the density distribution along rays. More specifically, the proposed framework trains a coarse model to predict the internal distribution of the transparency of an input volume along each ray. This estimated distribution then guides the sampling procedure of the fine model. Our method allows using fewer samples during training while achieving better output quality with the same computational resources."
  },
  "wacv2023_main_deepmodel-basedsuper-resolutionwithnon-uniformblur": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Deep Model-Based Super-Resolution With Non-Uniform Blur",
    "authors": [
      "Charles Laroche",
      "Andr\u00e9s Almansa",
      "Matias Tassano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Laroche_Deep_Model-Based_Super-Resolution_With_Non-Uniform_Blur_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Laroche_Deep_Model-Based_Super-Resolution_With_Non-Uniform_Blur_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We propose a state-of-the-art method for super-resolution with non-uniform blur. Single-image super-resolution methods seek to restore a high-resolution image from blurred, subsampled, and noisy measurements. Despite their impressive performance, existing techniques usually assume a uniform blur kernel. Hence, these techniques do not generalize well to the more general case of non-uniform blur. Instead, in this paper, we address the more realistic and computationally challenging case of spatially-varying blur. To this end, we first propose a fast deep plug-and-play algorithm, based on linearized ADMM splitting techniques, which can solve the super-resolution problem with spatially-varying blur. Second, we unfold our iterative algorithm into a single network and train it end-to-end. In this way, we overcome the intricacy of manually tuning the parameters involved in the optimization scheme. Our algorithm presents remarkable performance and generalizes well after a single training to a large family of spatially-varying blur kernels, noise levels and scale factors."
  },
  "wacv2023_main_progressivevideosummarizationviamultimodalself-supervisedlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Progressive Video Summarization via Multimodal Self-Supervised Learning",
    "authors": [
      "Haopeng Li",
      "Qiuhong Ke",
      "Mingming Gong",
      "Tom Drummond"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Li_Progressive_Video_Summarization_via_Multimodal_Self-Supervised_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Li_Progressive_Video_Summarization_via_Multimodal_Self-Supervised_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Modern video summarization methods are based on deep neural networks that require a large amount of annotated data for training. However, existing datasets for video summarization are small-scale, easily leading to over-fitting of the deep models. Considering that the annotation of large-scale datasets is time-consuming, we propose a multimodal self-supervised learning framework to obtain semantic representations of videos, which benefits the video summarization task. Specifically, the self-supervised learning is conducted by exploring the semantic consistency between the videos and text in both course-grained and fine-grained fashions, as well as recovering masked frames in the videos. The multimodal framework is trained on a newly-collected dataset that consists of video-text pairs. Additionally, we introduce a progressive video summarization method, where the important content in a video is pinpointed progressively to generate better summaries. Extensive experiments have proved the effectiveness and superiority of our method in rank correlation coefficients and F-score compared to the state of the art."
  },
  "wacv2023_main_pushingtheefficiencylimitusingstructuredsparseconvolutions": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Pushing the Efficiency Limit Using Structured Sparse Convolutions",
    "authors": [
      "Vinay Kumar Verma",
      "Nikhil Mehta",
      "Shijing Si",
      "Ricardo Henao",
      "Lawrence Carin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Verma_Pushing_the_Efficiency_Limit_Using_Structured_Sparse_Convolutions_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Verma_Pushing_the_Efficiency_Limit_Using_Structured_Sparse_Convolutions_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Weight pruning is among the most popular approaches for compressing deep convolutional neural networks. Recent work suggests that in a randomly initialized deep neural network, there exist sparse subnetworks that achieve performance comparable to the original network. Unfortunately, finding these subnetworks involves iterative stages of training and pruning, which can be computationally expensive. We propose Structured Sparse Convolution (SSC), that leverages the inherent structure in images to reduce the parameters in the convolutional filter. This leads to improved efficiency of convolutional architectures compared to existing methods that perform pruning at initialization. We show that SSC is a generalization of commonly used layers (depthwise, groupwise, and pointwise convolution) in \"efficient architectures.\" Extensive experiments on well-known CNN models and datasets show the effectiveness of the proposed method. Architectures based on SSC achieve state-of-the-art performance compared to baselines on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks."
  },
  "wacv2023_main_robustreal-worldimageenhancementbasedonmulti-exposureldrimages": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Robust Real-World Image Enhancement Based on Multi-Exposure LDR Images",
    "authors": [
      "Haoyu Ren",
      "Yi Fan",
      "Stephen Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ren_Robust_Real-World_Image_Enhancement_Based_on_Multi-Exposure_LDR_Images_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ren_Robust_Real-World_Image_Enhancement_Based_on_Multi-Exposure_LDR_Images_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Robust real-world image enhancement from multi-exposure low dynamic range (LDR) images is a challenging task due to the unexpected inconsistency among the input images, such as the large motion or various exposures. In this paper, we propose a novel end-to-end image enhancement network to solve this problem. After extracting contextual information from the LDR images, we design a novel matching volume to align them by considering the motion and exposure differences among the input images. A stacked hourglass with dilated convolution is further utilized to aggregate the matched feature maps to the final enhanced image. In addition, we design a weakly-supervised pairwise loss function to evaluate the color consistency in the enhanced image, which further boosts the performance. We show the effectiveness of our methods on high dynamic ranging imaging (HDR) and End-to-End image signal processing (E2E-ISP). Experimental results demonstrate that our model achieves state-of-the-art enhancement performance."
  },
  "wacv2023_main_hootheavyocclusionsinobjecttrackingbenchmark": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HOOT: Heavy Occlusions in Object Tracking Benchmark",
    "authors": [
      "Gozde Sahin",
      "Laurent Itti"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sahin_HOOT_Heavy_Occlusions_in_Object_Tracking_Benchmark_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sahin_HOOT_Heavy_Occlusions_in_Object_Tracking_Benchmark_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we present HOOT, the Heavy Occlusions in Object Tracking Benchmark, a new visual object tracking dataset aimed towards handling high occlusion scenarios for single-object tracking tasks. The benchmark consists of 581 high-quality videos, which have 436K frames densely annotated with rotated bounding boxes for the targets spanning 74 object classes. The dataset is geared for development, evaluation and analysis of visual tracking algorithms that are robust to occlusions. It is comprised of videos with high occlusion levels, where the median percentage of occluded frames per-video is 68%. It also provides critical attributes on occlusions, which include defining a taxonomy for occluders, providing occlusion masks for every bounding box, per-frame partial/full occlusion labels and more. HOOT has been compiled to encourage development of new methods targeting occlusion handling in visual tracking, by providing training and test splits with high occlusion levels. This makes HOOT the first densely-annotated, large dataset designed for single-object tracking under severe occlusion. We evaluate 15 state-of-the-art trackers on this new dataset to act as a baseline for future work focusing on occlusions."
  },
  "wacv2023_main_self-attentivepoolingforefficientdeeplearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Attentive Pooling for Efficient Deep Learning",
    "authors": [
      "Fang Chen",
      "Gourav Datta",
      "Souvik Kundu",
      "Peter A. Beerel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Self-Attentive_Pooling_for_Efficient_Deep_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Self-Attentive_Pooling_for_Efficient_Deep_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Efficient custom pooling techniques that can aggressively trim the dimensions of a feature map for resource-constrained computer vision applications have recently gained significant traction. However, prior pooling works extract only the local context of the activation maps, limiting their effectiveness. In contrast, we propose a novel non-local self-attentive pooling method that can be used as a drop-in replacement to the standard pooling layers, such as max/average pooling or strided convolution. The proposed self-attention module uses patch embedding, multi-head self-attention, and spatial-channel restoration, followed by sigmoid activation and exponential soft-max. This self-attention mechanism efficiently aggregates dependencies between non-local activation patches during down-sampling. Extensive experiments on standard object classification and detection tasks with various convolutional neural network (CNN) architectures demonstrate the superiority of our proposed mechanism over the state-of-the-art (SOTA) pooling techniques. In particular, we surpass the test accuracy of existing pooling techniques on different variants of MobileNet-V2 on ImageNet by an average of 1.2%. With the aggressive down-sampling of the activation maps in the initial layers (providing up to 22x reduction in memory consumption), our approach achieves 1.43% higher test accuracy compared to SOTA techniques with iso-memory footprints. This enables the deployment of our models in memory-constrained devices, such as micro-controllers without losing significant accuracy, because the initial activation maps consume a significant amount of on-chip memory for high-resolution images required for complex vision tasks. Our pooling method also leverages channel pruning to further reduce memory footprints. Codes are available at https://github.com/C-Fun/Non-Local-Pooling."
  },
  "wacv2023_main_self-distilledself-supervisedrepresentationlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Distilled Self-Supervised Representation Learning",
    "authors": [
      "Jiho Jang",
      "Seonhoon Kim",
      "Kiyoon Yoo",
      "Chaerin Kong",
      "Jangho Kim",
      "Nojun Kwak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jang_Self-Distilled_Self-Supervised_Representation_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jang_Self-Distilled_Self-Supervised_Representation_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "State-of-the-art frameworks in self-supervised learning have recently shown that fully utilizing transformer-based models can lead to performance boost compared to conventional CNN models. Striving to maximize the mutual information of two views of an image, existing works apply a contrastive loss to the final representations. Motivated by self-distillation in the supervised regime, we further exploit this by allowing the intermediate representations to learn from the final layer via the contrastive loss. Through self-distillation, the intermediate layers are better suited for instance discrimination, making the performance of an early-exited sub-network not much degraded from that of the full network. This renders the pretext task easier also for the final layer, lead to better representations. Our method, Self-Distilled Self-Supervised Learning (SDSSL), outperforms competitive baselines (SimCLR, BYOL and MoCo v3) using ViT on various tasks and datasets. In the linear evaluation and k-NN protocol, SDSSL not only leads to superior performance in the final layers, but also in most of the lower layers. Furthermore, qualitative and quantative analyses show how representations are formed more effectively along the transformer layers. Code will be available."
  },
  "wacv2023_main_compositelearningforrobustandeffectivedensepredictions": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Composite Learning for Robust and Effective Dense Predictions",
    "authors": [
      "Menelaos Kanakis",
      "Thomas E. Huang",
      "David Br\u00fcggemann",
      "Fisher Yu",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kanakis_Composite_Learning_for_Robust_and_Effective_Dense_Predictions_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kanakis_Composite_Learning_for_Robust_and_Effective_Dense_Predictions_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multi-task learning promises better model generalization on a target task by jointly optimizing it with an auxiliary task. However, the current practice requires additional labeling efforts for the auxiliary task, while not guaranteeing better model performance. In this paper, we find that jointly training a dense prediction (target) task with a self-supervised (auxiliary) task can consistently improve the performance of the target task, while eliminating the need for labeling auxiliary tasks. We refer to this joint training as Composite Learning (CompL). Experiments of CompL on monocular depth estimation, semantic segmentation, and boundary detection show consistent performance improvements in fully and partially labeled datasets. Further analysis on depth estimation reveals that joint training with self-supervision outperforms most labeled auxiliary tasks. We also find that CompL can improve model robustness when the models are evaluated in new domains. These results demonstrate the benefits of self-supervision as an auxiliary task, and establish the design of novel task-specific self-supervised methods as a new axis of investigation for future multi-task learning research."
  },
  "wacv2023_main_efficientskeleton-basedactionrecognitionviajoint-mappingstrategies": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Efficient Skeleton-Based Action Recognition via Joint-Mapping Strategies",
    "authors": [
      "Min-Seok Kang",
      "Dongoh Kang",
      "HanSaem Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kang_Efficient_Skeleton-Based_Action_Recognition_via_Joint-Mapping_Strategies_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kang_Efficient_Skeleton-Based_Action_Recognition_via_Joint-Mapping_Strategies_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Graph convolutional networks (GCNs) have brought remarkable progress in skeleton-based action recognition. However, high computational cost and large model size make models difficult to be applied in real-world embedded system. Specifically, GCN that is applied in automated surveillance system pre-require models such as pedestrian detection and human pose estimation. Therefore, each model should be computationally lightweight and whole process should be operated in real-time. In this paper, we propose two different joint-mapping modules to reduce the number of joint representations, alleviating a total computational cost and model size. Our models achieve better accuracy-latency trade-off compared to previous state-of-the-arts on two datasets, namely NTU RGB+D and NTU RGB+D 120, demonstrating the suitability for practical applications. Furthermore, we measure the latency of the models by using TensorRT framework to compare the models from a practical perspective."
  },
  "wacv2023_main_pointinverterpointcloudreconstructionandeditingviaagenerativemodelwithshapepriors": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "PointInverter: Point Cloud Reconstruction and Editing via a Generative Model With Shape Priors",
    "authors": [
      "Jaeyeon Kim",
      "Binh-Son Hua",
      "Thanh Nguyen",
      "Sai-Kit Yeung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kim_PointInverter_Point_Cloud_Reconstruction_and_Editing_via_a_Generative_Model_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kim_PointInverter_Point_Cloud_Reconstruction_and_Editing_via_a_Generative_Model_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we propose a new method for mapping a 3D point cloud to the latent space of a 3D generative adversarial network. Our generative model for 3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point cloud generator. We derive an efficient way to encode an input 3D point cloud to the latent space of the SP-GAN. Our point cloud encoder can resolve the point ordering issue during inversion, and thus can determine the correspondences between points in the generated 3D point cloud and those in the canonical sphere used by the generator. We show that our method outperforms previous GAN inversion methods for 3D point clouds, achieving the state-of-the-art results both quantitatively and qualitatively. Our code is available upon publication."
  },
  "wacv2023_main_advisil-aclass-incrementallearningadvisor": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "AdvisIL - A Class-Incremental Learning Advisor",
    "authors": [
      "Eva Feillet",
      "Gr\u00e9goire Petit",
      "Adrian Popescu",
      "Marina Reyboz",
      "C\u00e9line Hudelot"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Feillet_AdvisIL_-_A_Class-Incremental_Learning_Advisor_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Feillet_AdvisIL_-_A_Class-Incremental_Learning_Advisor_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recent class-incremental learning methods combine deep neural architectures and learning algorithms to handle streaming data under memory and computational constraints. The performance of existing methods varies depending on the characteristics of the incremental process. To date, there is no other approach than to test all pairs of learning algorithms and neural architectures on the training data available at the start of the learning process to select a suited algorithm-architecture combination. To tackle this problem, in this article, we introduce AdvisIL, a method which takes as input the main characteristics of the incremental process (memory budget for the deep model, initial number of classes, size of incremental steps) and recommends an adapted pair of learning algorithm and neural architecture. The recommendation is based on a similarity between the user-provided settings and a large set of pre-computed experiments. AdvisIL makes class-incremental learning easier, since users do not need to run cumbersome experiments to design their system. We evaluate our method on four datasets under six incremental settings and three deep model sizes. We compare six algorithms and three deep neural architectures. Results show that AdvisIL has better overall performance than any of the individual combinations of a learning algorithm and a neural architecture. AdvisIL's code is available at https://github.com/EvaJF/AdvisIL."
  },
  "wacv2023_main_lablearnableactivationbinarizerforbinaryneuralnetworks": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "LAB: Learnable Activation Binarizer for Binary Neural Networks",
    "authors": [
      "Sieger Falkena",
      "Hadi Jamali-Rad",
      "Jan van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Falkena_LAB_Learnable_Activation_Binarizer_for_Binary_Neural_Networks_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Falkena_LAB_Learnable_Activation_Binarizer_for_Binary_Neural_Networks_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Binary Neural Networks (BNNs) are receiving an upsurge of attention for bringing power-hungry deep learning towards edge devices. The traditional wisdom in this space is to employ sign() for binarizing featuremaps. We argue and illustrate that sign() is a uniqueness bottleneck, limiting information propagation throughout the network. To alleviate this, we propose to dispense sign(), replacing it with a learnable activation binarizer (LAB), allowing the network to learn a fine-grained binarization kernel per layer - as opposed to global thresholding. LAB is a novel universal module that can seamlessly be integrated into existing architectures. To confirm this, we plug it into four seminal BNNs and show a considerable performance boost at the cost of tolerable increase in delay and complexity. Finally, we build an end-to-end BNN (coined as LAB-BNN) around LAB, and demonstrate that it achieves competitive performance on par with the state-of-the-art on ImageNet. Codebase in the supplementary will be made publicly available upon acceptance."
  },
  "wacv2023_main_fine-grainedactivitiesofpeopleworldwide": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fine-Grained Activities of People Worldwide",
    "authors": [
      "Jeffrey Byrne",
      "Gregory Casta\u00f1\u00f3n",
      "Zhongheng Li",
      "Gil Ettinger"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Byrne_Fine-Grained_Activities_of_People_Worldwide_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Byrne_Fine-Grained_Activities_of_People_Worldwide_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Every day, humans perform many closely related activities that involve subtle discriminative motions, such as putting on a shirt vs. putting on a jacket, or shaking hands vs. giving a high five. Activity recognition by ethical visual AI could provide insights into our patterns of daily life, however existing activity recognition datasets do not capture the massive diversity of these human activities around the world. To address this limitation, we introduce Collector, a free mobile app to record video while simultaneously annotating objects and activities of consented subjects. This new data collection platform was used to curate the Consented Activities of People (CAP) dataset, the first large-scale, fine-grained activity dataset of people worldwide. The CAP dataset contains 1.45M video clips of 512 fine grained activity labels of daily life, grouped into 144 coarse activity classes, collected by 780 subjects in 33 countries. We provide activity classification and activity detection benchmarks for this dataset, and analyze baseline results to gain insight into how people around with world perform common activities. The dataset, benchmarks, evaluation tools, public leaderboards and mobile apps are available for use at visym.github.io/cap."
  },
  "wacv2023_main_recur,attendorconvolve?onwhethertemporalmodelingmattersforcross-domainrobustnessinactionrecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition",
    "authors": [
      "Sofia Broom\u00e9",
      "Ernest Pokropek",
      "Boyu Li",
      "Hedvig Kjellstr\u00f6m"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Broome_Recur_Attend_or_Convolve_On_Whether_Temporal_Modeling_Matters_for_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Broome_Recur_Attend_or_Convolve_On_Whether_Temporal_Modeling_Matters_for_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Most action recognition models today are highly parameterized, and evaluated on datasets with appearance-wise distinct classes. It has also been shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward texture rather than shape in still image recognition tasks, in contrast to humans. Taken together, this raises suspicion that large video models partly learn spurious spatial texture correlations rather than to track relevant shapes over time to infer generalizable semantics from their movement. A natural way to avoid parameter explosion when learning visual patterns over time is to make use of recurrence. Biological vision consists of abundant recurrent circuitry, and is superior to computer vision in terms of domain shift generalization. In this article, we empirically study whether the choice of low-level temporal modeling has consequences for texture bias and cross-domain robustness. In order to enable a light-weight and systematic assessment of the ability to capture temporal structure, not revealed from single frames, we provide the Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing for the investigation of spatial texture bias in video models. The combined results of our experiments indicate that sound physical inductive bias such as recurrence in temporal modeling may be advantageous when robustness to domain shift is important for the task."
  },
  "wacv2023_main_rethinkingrotationinself-supervisedcontrastivelearningadaptivepositiveornegativedataaugmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Rethinking Rotation in Self-Supervised Contrastive Learning: Adaptive Positive or Negative Data Augmentation",
    "authors": [
      "Atsuyuki Miyai",
      "Qing Yu",
      "Daiki Ikami",
      "Go Irie",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Miyai_Rethinking_Rotation_in_Self-Supervised_Contrastive_Learning_Adaptive_Positive_or_Negative_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Miyai_Rethinking_Rotation_in_Self-Supervised_Contrastive_Learning_Adaptive_Positive_or_Negative_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Rotation is frequently listed as a candidate for data augmentation in contrastive learning but seldom provides satisfactory improvements. We argue that this is because the rotated image is always treated as either positive or negative. The semantics of an image can be rotation-invariant or rotation-variant, so whether the rotated image is treated as positive or negative should be determined based on the content of the image. Therefore, we propose a novel augmentation strategy, adaptive Positive or Negative Data Augmentation (PNDA), in which an original and its rotated image are a positive pair if they are semantically close and a negative pair if they are semantically different. To achieve PNDA, we first determine whether rotation is positive or negative on an image-by-image basis in an unsupervised way. Then, we apply PNDA to contrastive learning frameworks. Our experiments showed that PNDA improves the performance of contrastive learning."
  },
  "wacv2023_main_far3dettowardsfar-field3ddetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Far3Det: Towards Far-Field 3D Detection",
    "authors": [
      "Shubham Gupta",
      "Jeet Kanjani",
      "Mengtian Li",
      "Francesco Ferroni",
      "James Hays",
      "Deva Ramanan",
      "Shu Kong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Gupta_Far3Det_Towards_Far-Field_3D_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Gupta_Far3Det_Towards_Far-Field_3D_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We focus on the task of far-field 3D detection (Far3Det) of objects beyond a certain distance from an observer, e.g., >50m. Far3Det is particularly important for autonomous vehicles (AVs) operating at highway speeds, which require detections of far-field obstacles to ensure sufficient braking distances. However, contemporary AV benchmarks such as nuScenes underemphasize this problem because they evaluate performance only up to a certain distance (50m). One reason is that obtaining far-field 3D annotations is difficult, particularly for lidar sensors that produce very few point returns for far-away objects. Indeed, we find that almost 50% of far-field objects (beyond 50m) contain zero lidar points. Secondly, current metrics for 3D detection employ a \"one-size-fits-all\" philosophy, using the same tolerance thresholds for near and far objects, inconsistent with tolerances for both human vision and stereo disparities. Both factors lead to an incomplete analysis of the Far3Det task. For example, while conventional wisdom tells us that high-resolution RGB sensors should be vital for the 3D detection of far-away objects, lidar-based methods still rank higher compared to RGB counterparts on the current benchmark leaderboards. As a first step towards a Far3Det benchmark, we develop a method to find well-annotated scenes from the nuScenes dataset and derive a well-annotated far-field validation set. We also propose a Far3Det evaluation protocol and explore various 3D detection methods for Far3Det. Our result convincingly justifies the long held conventional wisdom that high-resolution RGB improves 3D detection in the far-field. We further propose a simple yet effective method that fuses detections from RGB and lidar detectors based on non-maximum suppression, which remarkably outperforms state-of-the-art 3D detectors in the far-field."
  },
  "wacv2023_main_towardsmoocsforlipreadingusingsynthetictalkingheadstotrainhumansinlipreadingatscale": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Towards MOOCs for Lipreading: Using Synthetic Talking Heads To Train Humans in Lipreading at Scale",
    "authors": [
      "Aditya Agarwal",
      "Bipasha Sen",
      "Rudrabha Mukhopadhyay",
      "Vinay P. Namboodiri",
      "C. V. Jawahar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Towards_MOOCs_for_Lipreading_Using_Synthetic_Talking_Heads_To_Train_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Towards_MOOCs_for_Lipreading_Using_Synthetic_Talking_Heads_To_Train_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Many people with some form of hearing loss consider lipreading as their primary mode of day-to-day communication. However, finding resources to learn or improve one's lipreading skills can be challenging. This is further exacerbated in the COVID19 pandemic due to restrictions on direct interactions with peers and speech therapists. Today, online MOOCs platforms like Coursera and Udemy have become the most effective form of training for many types of skill development. However, online lipreading resources are scarce as creating such resources is an extensive process needing months of manual effort to record hired actors. Because of the manual pipeline, such platforms are also limited in vocabulary, supported languages, accents, and speakers and have a high usage cost. In this work, we investigate the possibility of replacing real human talking videos with synthetically generated videos. Synthetic data can easily incorporate larger vocabularies, variations in accent, and even local languages and many speakers. We propose an end-to-end automated pipeline to develop such a platform using state-of-the-art talking head video generator networks, text-to-speech models, and computer vision techniques. We then perform an extensive human evaluation using carefully thought out lipreading exercises to validate the quality of our designed platform against the existing lipreading platforms. Our studies concretely point toward the potential of our approach in developing a large-scale lipreading MOOC platform that can impact millions of people with hearing loss."
  },
  "wacv2023_main_modalitymixerformulti-modalactionrecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Modality Mixer for Multi-Modal Action Recognition",
    "authors": [
      "Sumin Lee",
      "Sangmin Woo",
      "Yeonju Park",
      "Muhammad Adi Nugroho",
      "Changick Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_Modality_Mixer_for_Multi-Modal_Action_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_Modality_Mixer_for_Multi-Modal_Action_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In multi-modal action recognition, it is important to consider not only the complementary nature of different modalities but also global action content. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, to leverage complementary information across modalities and temporal context of an action for multi-modal action recognition. We also introduce a simple yet effective recurrent unit, called Multi-modal Contextualization Unit (MCU), which is a core component of M-Mixer. Our MCU temporally encodes a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth, IR). This process encourages M-Mixer to exploit global action content and also to supplement complementary information of other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NWUCLA datasets. Moreover, we demonstrate the effectiveness of M-Mixer by conducting comprehensive ablation studies."
  },
  "wacv2023_main_relaxingcontrastivenessinmultimodalrepresentationlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Relaxing Contrastiveness in Multimodal Representation Learning",
    "authors": [
      "Zudi Lin",
      "Erhan Bas",
      "Kunwar Yashraj Singh",
      "Gurumurthy Swaminathan",
      "Rahul Bhotika"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multimodal representation learning for images with paired raw texts can improve the usability and generality of the learned semantic concepts while significantly reducing annotation costs. In this paper, we explore the design space of loss functions in visual-linguistic pretraining frameworks and propose a novel Relaxed Contrastive (ReCo) objective, which acts as a drop-in replacement of the widely used InfoNCE loss. The key insight of ReCo is to allow a relaxed negative space by not penalizing unpaired multimodal samples (ie, negative pairs) that are already orthogonal or negatively correlated. Unlike the widely-used InfoNCE, which keeps repelling negative pairs as long as they are not anti-correlated, ReCo by design embraces more diversity and flexibility of the learned embeddings. We conduct extensive experiments using ReCo with state-of-the-art models by pretraining on the MIMIC-CXR dataset that consists of chest radiographs and free-text radiology reports, and evaluating on the CheXpert dataset for multimodal retrieval and disease classification. Our ReCo achieves an absolute improvement of 2.9% over the InfoNCE baseline on the CheXpert Retrieval dataset in average retrieval precision and reports better or comparable performance in the linear evaluation and finetuning for classification. We further show that ReCo outperforms InfoNCE on the Flickr30K dataset by 1.7% in retrieval Recall@1, demonstrating the generalizability of our approach to natural images."
  },
  "wacv2023_main_towardsdisturbance-freevisualmobilemanipulation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Towards Disturbance-Free Visual Mobile Manipulation",
    "authors": [
      "Tianwei Ni",
      "Kiana Ehsani",
      "Luca Weihs",
      "Jordi Salvador"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ni_Towards_Disturbance-Free_Visual_Mobile_Manipulation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ni_Towards_Disturbance-Free_Visual_Mobile_Manipulation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep reinforcement learning has shown promising results on an abundance of robotic tasks in simulation, including visual navigation and manipulation. Prior work generally aims to build embodied agents that solve their assigned tasks as quickly as possible, while largely ignoring the problems caused by collision with objects during interaction. This lack of prioritization is understandable: there is no inherent cost in breaking virtual objects. As a result, \"well-trained\" agents frequently collide with objects before achieving their primary goals, a behavior that would be catastrophic in the real world. In this paper, we study the problem of training agents to complete the task of visual mobile manipulation in the ManipulaTHOR environment while avoiding unnecessary collision (disturbance) with objects. We formulate disturbance avoidance as a penalty term in the reward function, but find that directly training with such penalized rewards often results in agents being unable to escape poor local optima. Instead, we propose a two-stage training curriculum where an agent is first allowed to freely explore and build basic competencies without penalization, after which a disturbance penalty is introduced to refine the agent's behavior. Results on testing scenes show that our curriculum not only avoids these poor local optima, but also leads to 10% absolute gains in success rate without disturbance, compared to our state-of-the-art baselines. Moreover, our curriculum is significantly more performant than a safe RL algorithm that casts collision avoidance as a constraint. Finally, we propose a novel disturbance-prediction auxiliary task that accelerates learning."
  },
  "wacv2023_main_exploitinginstance-basedmixedsamplingviaauxiliarysourcedomainsupervisionfordomain-adaptiveactiondetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Exploiting Instance-Based Mixed Sampling via Auxiliary Source Domain Supervision for Domain-Adaptive Action Detection",
    "authors": [
      "Yifan Lu",
      "Gurkirt Singh",
      "Suman Saha",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lu_Exploiting_Instance-Based_Mixed_Sampling_via_Auxiliary_Source_Domain_Supervision_for_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lu_Exploiting_Instance-Based_Mixed_Sampling_via_Auxiliary_Source_Domain_Supervision_for_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We propose a novel domain adaptive action detection approach and a new adaptation protocol that leverages the recent advancements in image-level unsupervised domain adaptation (UDA) techniques and handle vagaries of instance-level video data. Self-training combined with cross-domain mixed sampling has shown remarkable performance gain in semantic segmentation in UDA (unsupervised domain adaptation) context. Motivated by this fact, we propose an approach for human action detection in videos that transfers knowledge from the source domain (annotated dataset) to the target domain (unannotated dataset) using mixed sampling and pseudo-label-based selftraining. The existing UDA techniques follow a ClassMix algorithm for semantic segmentation. However, simply adopting ClassMix for action detection does not work, mainly because these are two entirely different problems, i.e., pixel-label classification vs. instance-label detection. To tackle this, we propose a novel action instance mixed sampling technique that combines information across domains based on action instances instead of action classes. Moreover, we propose a new UDA training protocol that addresses the long-tail sample distribution and domain shift problem by using supervision from an auxiliary source domain (ASD). For the ASD, we propose a new action detection dataset with dense frame-level annotations. We name our proposed framework as domain-adaptive action instance mixing (DA-AIM). We demonstrate that DA-AIM consistently outperforms prior works on challenging domain adaptation benchmarks. The source code is available at https://github.com/wwwfan628/DA-AIM."
  },
  "wacv2023_main_graph-basedself-learningforrobustpersonre-identification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Graph-Based Self-Learning for Robust Person Re-Identification",
    "authors": [
      "Yuqiao Xian",
      "Jinrui Yang",
      "Fufu Yu",
      "Jun Zhang",
      "Xing Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Xian_Graph-Based_Self-Learning_for_Robust_Person_Re-Identification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Xian_Graph-Based_Self-Learning_for_Robust_Person_Re-Identification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Existing deep learning approaches for person re-identification (Re-ID) mostly rely on large-scale and well-annotated training data. However, human-annotated labels are prone to label noise in real-world applications. Previous person Re-ID works mainly focus on random label noise, which doesn't properly reflect the characteristic of label noise in practical human-annotated process. In this work, we find the visual ambiguity noise is more common and reasonable noise assumption in annotation of person Re-ID. To handle the kind of noise, we propose a simple and effective robust person Re-ID framework, namely Graph-Based Self-Learning (GBSL), to iteratively learn discriminative representation and rectify noisy labels with limited annotated samples for each identity. Meanwhile, considering the practical annotation process in person Re-ID, we further extend the visual ambiguity noise assumption and propose a type of more practical label noise in person Re-ID, namely the tracklet-level label noise (TLN). Without modifying network architecture or loss function, our approach significantly improves the robustness against label noise of the Re-ID system. Our model obtains competitive performance with training data corrupted by various types of label noise and outperforms the existing methods for robust Re-ID on public benchmarks."
  },
  "wacv2023_main_svd-nascouplinglow-rankapproximationandneuralarchitecturesearch": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SVD-NAS: Coupling Low-Rank Approximation and Neural Architecture Search",
    "authors": [
      "Zhewen Yu",
      "Christos-Savvas Bouganis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yu_SVD-NAS_Coupling_Low-Rank_Approximation_and_Neural_Architecture_Search_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yu_SVD-NAS_Coupling_Low-Rank_Approximation_and_Neural_Architecture_Search_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The task of compressing pre-trained Deep Neural Networks has attracted wide interest of the research community due to its great benefits in freeing practitioners from data access requirements. In this domain, low-rank approximation is a promising method, but existing solutions considered a restricted number of design choices and failed to efficiently explore the design space, which lead to severe accuracy degradation and limited compression ratio achieved. To address the above limitations, this work proposes the SVD-NAS framework that couples the domains of low-rank approximation and neural architecture search. SVD-NAS generalises and expands the design choices of previous works by introducing the Low-Rank architecture space, LR-space, which is a more fine-grained design space of low-rank approximation. Afterwards, this work proposes a gradient-descent-based search for efficiently traversing the LR-space. This finer and more thorough exploration of the possible design choices results in improved accuracy as well as reduction in parameters, FLOPS, and latency of a CNN model. Results demonstrate that the SVD-NAS achieves 2.06-12.85pp higher accuracy on ImageNet than state-of-the-art methods under the data-limited problem settings. SVD-NAS is open-sourced at https://github.com/Yu-Zhewen/SVD-NAS."
  },
  "wacv2023_main_multi-levelcontrastivelearningforself-supervisedvisiontransformers": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Multi-Level Contrastive Learning for Self-Supervised Vision Transformers",
    "authors": [
      "Shentong Mo",
      "Zhun Sun",
      "Chao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mo_Multi-Level_Contrastive_Learning_for_Self-Supervised_Vision_Transformers_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mo_Multi-Level_Contrastive_Learning_for_Self-Supervised_Vision_Transformers_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recent studies aim to establish contrastive self-supervised learning (CSL) algorithms specialized for the family of Vision Transformers (ViTs) to make them function normally as ordinary convolutional-based backbones in the training progress. Despite obtaining promising performance on related downstream tasks, one compelling property of the ViTs is ignored in those approaches. As previous studies have demonstrated, vision transformers benefit from the early stage global attention mechanics, obtaining feature representations that contain information from distant patches, even in their shallow layers. Motivated by this, we present a simple yet effective framework to facilitate the self-supervised feature learning of transformer-based vision architectures, namely, Multi-level Contrastive learning for Vision Transformers (MCVT). Specifically, we equip the vision transformers with individual-based (InfoNCE) and prototypical-based (ProtoNCE) contrastive loss in different stages of the architecture to capture low-level invariance and high-level invariance between views of samples, respectively. We conduct extensive experiments to demonstrate the effectiveness of the proposed method, using two well-known vision transformer backbones, on several vision downstream tasks, including linear classification, detection, and semantic segmentation."
  },
  "wacv2023_main_dopre-trainedmodelsbenefitequallyincontinuallearning?": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Do Pre-Trained Models Benefit Equally in Continual Learning?",
    "authors": [
      "Kuan-Ying Lee",
      "Yuanyi Zhong",
      "Yu-Xiong Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_Do_Pre-Trained_Models_Benefit_Equally_in_Continual_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_Do_Pre-Trained_Models_Benefit_Equally_in_Continual_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "A large part of the continual learning (CL) literature focuses on developing algorithms for models trained from scratch. While these algorithms work great with from-sc ratch trained models on widely used CL benchmarks, they show dramatic performance drops on more complex datasets (e.g., Split-CUB200). Pre-trained models, widely used to transfer knowledge to downstream tasks, could enhance these methods to be applicable in more realistic scenarios. However, surprisingly, improvements in CL algorithms from pre-training are inconsistent. For instance, while Incremental Classifier and Representation Learning (iCaRL) underperforms Supervised Contrastive Replay (SCR) when trained from scratch, it outperforms SCR when both are initialized with a pre-trained model. This indicates the paradigm current CL literature follows, where all methods are compared in from-scratch training, is not well reflective of the true CL objective and desired progress. Furthermore, we found 1) CL algorithms that exert less regularization benefit more from a pre-trained model; 2) a model pre-trained with a larger dataset (WebImageText in Contrastive Language-Image Pre-training (CLIP) vs. ImageNet) does not guarantee a better improvement. Based on these findings, we introduced a simple yet effective baseline that employs minimum regularization and leverages the more beneficial pre-trained model, which outperforms state-of-the-art methods when pre-training is applied. Our code is available at https://github.com/eric11220/pretrained-models-in-CL."
  },
  "wacv2023_main_cross-domainvideoanomalydetectionwithouttargetdomainadaptation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Cross-Domain Video Anomaly Detection Without Target Domain Adaptation",
    "authors": [
      "Abhishek Aich",
      "Kuan-Chuan Peng",
      "Amit K. Roy-Chowdhury"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Aich_Cross-Domain_Video_Anomaly_Detection_Without_Target_Domain_Adaptation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Aich_Cross-Domain_Video_Anomaly_Detection_Without_Target_Domain_Adaptation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Most cross-domain unsupervised Video Anomaly Detection (VAD) works assume that at least few task-relevant target domain training data are available for adaptation from the source to the target domain. However, this requires laborious model-tuning by the end-user who may prefer to have a system that works \"out-of-the-box\". To address such practical scenarios, we identify a novel target domain (inference-time) VAD task where no target domain training data are available. To this end, we propose a new 'Zero-shot Cross-domain Video Anomaly Detection (zxvad)' framework that includes a future-frame prediction generative model setup. Different from prior future-frame prediction models, our model uses a novel Normalcy Classifier module to learn the features of normal event videos by learning how such features are different \"relative\" to features in pseudo-abnormal examples. A novel Untrained Convolutional Neural Network based Anomaly Synthesis module crafts these pseudo-abnormal examples by adding foreign objects in normal video frames with no extra training cost. With our novel relative normalcy feature learning strategy, zxvad generalizes and learns to distinguish between normal and abnormal frames in a new target domain without adaptation during inference. Through evaluations on common datasets, we show that zxvad outperforms the state-of-the-art (SOTA), regardless of whether task-relevant (i.e., VAD) source training data are available or not. Lastly, zxvad also beats the SOTA methods in inference-time efficiency metrics including the model size, total parameters, GPU energy consumption, and GMACs."
  },
  "wacv2023_main_testtest-timeself-trainingunderdistributionshift": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "TeST: Test-Time Self-Training Under Distribution Shift",
    "authors": [
      "Samarth Sinha",
      "Peter Gehler",
      "Francesco Locatello",
      "Bernt Schiele"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sinha_TeST_Test-Time_Self-Training_Under_Distribution_Shift_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sinha_TeST_Test-Time_Self-Training_Under_Distribution_Shift_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Despite their recent success, deep neural networks continue to perform poorly when they encounter distribution shifts at test time. Many recently proposed approaches try to counter this by aligning the model to the new distribution prior to inference. With no labels available this requires unsupervised objectives to adapt the model on the observed test data. In this paper, we propose Test-Time Self-Training (TeST): a technique that takes as input a model trained on some source data and a novel data distribution at test time, and learns invariant and robust representations using a student-teacher framework. We find that models adapted using TeST significantly improve over baseline test-time adaptation algorithms. TeST achieves competitive performance to modern domain adaptation algorithms [4,43], while having access to 5-10x less data at time of adaption. We thoroughly evaluate a variety of baselines on two tasks: object detection and image segmentation and find that models adapted with TeST. We find that TeST sets the new state-of-the art for test-time domain adaptation algorithms."
  },
  "wacv2023_main_idd-3dindiandrivingdatasetfor3dunstructuredroadscenes": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "IDD-3D: Indian Driving Dataset for 3D Unstructured Road Scenes",
    "authors": [
      "Shubham Dokania",
      "A. H. Abdul Hafez",
      "Anbumani Subramanian",
      "Manmohan Chandraker",
      "C. V. Jawahar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Dokania_IDD-3D_Indian_Driving_Dataset_for_3D_Unstructured_Road_Scenes_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Dokania_IDD-3D_Indian_Driving_Dataset_for_3D_Unstructured_Road_Scenes_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Autonomous driving and assistance systems rely on annotated data from traffic and road scenarios to model and learn the various object relations in complex real-world scenarios. Preparation and training of deploy-able deep learning architectures require the models to be suited to different traffic scenarios and adapt to different situations. Currently, existing datasets, while large-scale, lack such diversities and are geographically biased towards mainly developed cities. An unstructured and complex driving layout found in several developing countries such as India poses a challenge to these models due to the sheer degree of variations in the object types, densities, and locations. To facilitate better research toward accommodating such scenarios, we build a new dataset,IDD-3D , which consists of multi-modal data from multiple cameras and LiDAR sensors with 12k annotated driving LiDAR frames across various traffic scenarios. We discuss the need for this dataset through statistical comparisons with existing datasets and highlight benchmarks on standard 3D object detection and tracking tasks in complex layouts."
  },
  "wacv2023_main_conmixforsource-freesingleandmulti-targetdomainadaptation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CoNMix for Source-Free Single and Multi-Target Domain Adaptation",
    "authors": [
      "Vikash Kumar",
      "Rohit Lal",
      "Himanshu Patil",
      "Anirban Chakraborty"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kumar_CoNMix_for_Source-Free_Single_and_Multi-Target_Domain_Adaptation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kumar_CoNMix_for_Source-Free_Single_and_Multi-Target_Domain_Adaptation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This work introduces the novel task of Source-free Multi-target Domain Adaptation and proposes adaptation framework comprising of Consistency with Nuclear-Norm Maximization and MixUp knowledge distillation (CoNMix) as a solution to this problem. The main motive of this work is to solve for Single and Multi target Domain Adaptation (SMTDA) for the source-free paradigm, which enforces a constraint where the labeled source data is not available during target adaptation due to various privacy-related restrictions on data sharing. The source-free approach leverages target pseudo labels, which can be noisy, to improve the target adaptation. We introduce consistency between label preserving augmentations and utilize pseudo label refinement methods to reduce noisy pseudo labels. Further, we propose novel MixUp Knowledge Distillation (MKD) for better generalization on multiple target domains using various source-free STDA models. We also show that the Vision Transformer (VT) backbone gives better feature representation with improved domain transferability and class discriminability. Our proposed framework achieves the state-of-the-art (SOTA) results in various paradigms of source-free STDA and MTDA settings on popular domain adaptation datasets like Office-Home, Office-Caltech, and DomainNet. Project Page: https://sites.google.com/view/conmix-vcl"
  },
  "wacv2023_main_temporalfeatureenhancementdilatedconvolutionnetworkforweakly-supervisedtemporalactionlocalization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Temporal Feature Enhancement Dilated Convolution Network for Weakly-Supervised Temporal Action Localization",
    "authors": [
      "Jianxiong Zhou",
      "Ying Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhou_Temporal_Feature_Enhancement_Dilated_Convolution_Network_for_Weakly-Supervised_Temporal_Action_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhou_Temporal_Feature_Enhancement_Dilated_Convolution_Network_for_Weakly-Supervised_Temporal_Action_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Weakly-supervised Temporal Action Localization (WTAL) aims to classify and localize action instances in untrimmed videos with only video-level labels. Existing methods typically use snippet-level RGB and optical flow features extracted from pre-trained extractors directly. Because of two limitations: the short temporal span of snippets and the inappropriate initial features, these WTAL methods suffer from the lack of effective use of temporal information and have limited performance. In this paper, we propose the Temporal Feature Enhancement Dilated Convolution Network (TFE-DCN) to address these two limitations. The proposed TFE-DCN has an enlarged receptive field that covers a long temporal span to observe the full dynamics of action instances, which makes it powerful to capture temporal dependencies between snippets. Furthermore, we propose the Modality Enhancement Module that can enhance RGB features with the help of enhanced optical flow features, making the overall features appropriate for the WTAL task. Experiments conducted on THUMOS'14 and ActivityNet v1.3 datasets show that our proposed approach far outperforms state-of-the-art WTAL methods."
  },
  "wacv2023_main_meta-auxiliarylearningforfuturedepthpredictioninvideos": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Meta-Auxiliary Learning for Future Depth Prediction in Videos",
    "authors": [
      "Huan Liu",
      "Zhixiang Chi",
      "Yuanhao Yu",
      "Yang Wang",
      "Jun Chen",
      "Jin Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Liu_Meta-Auxiliary_Learning_for_Future_Depth_Prediction_in_Videos_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Liu_Meta-Auxiliary_Learning_for_Future_Depth_Prediction_in_Videos_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We consider a new problem of future depth prediction in video. Given a sequence of observed frames, the goal is to predict the depth map of a future frame that has not been observed yet. Depth estimation plays a vital role for scene understanding and decision-making in intelligent systems. Predicting future depth maps can be valuable for autonomous vehicles to anticipate the behaviors of their surrounding objects. Our proposed model for this problem has a two-branch architecture. One branch is for the primary task of future depth estimation. The other branch is for an auxiliary task of image reconstruction. The auxiliary branch can act as a regularization. Inspired by some recent work on test-time adaption, we use the auxiliary task during testing to adapt the model to a specific test video. We also propose a novel meta-auxiliary learning that learn the model specifically for the purpose of effective test-time adaptation. Experimental results demonstrate that our proposed approach significantly outperforms other alternative methods."
  },
  "wacv2023_main_large-to-smallimageresolutionasymmetryindeepmetriclearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Large-to-Small Image Resolution Asymmetry in Deep Metric Learning",
    "authors": [
      "Pavel Suma",
      "Giorgos Tolias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Suma_Large-to-Small_Image_Resolution_Asymmetry_in_Deep_Metric_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Suma_Large-to-Small_Image_Resolution_Asymmetry_in_Deep_Metric_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep metric learning for vision is trained by optimizing a representation network to map (non-)matching image pairs to (non-)similar representations. During testing, which typically corresponds to image retrieval, both database and query examples are processed by the same network to obtain the representation used for similarity estimation and ranking. In this work, we explore an asymmetric setup by light-weight processing of the query at a small image resolution to enable fast representation extraction. The goal is to obtain a network for database examples that is trained to operate on large resolution images and benefits from fine-grained image details, and a second network for query examples that operates on small resolution images but preserves a representation space aligned with that of the database network. We achieve this with a distillation approach that transfers knowledge from a fixed teacher network to a student via a loss that operates per image and solely relies on coupled augmentations without the use of any labels. In contrast to prior work that explores such asymmetry from the point of view of different network architectures, this work uses the same architecture but modifies the image resolution. We conclude that resolution asymmetry is a better way to optimize the performance/efficiency trade-off than architecture asymmetry. Evaluation is performed on three standard deep metric learning benchmarks, namely CUB200, Cars196, and SOP. Code: https://github.com/pavelsuma/raml"
  },
  "wacv2023_main_serftowardsbettertrainingofdeepneuralnetworksusinglog-softpluserroractivationfunction": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SERF: Towards Better Training of Deep Neural Networks Using Log-Softplus ERror Activation Function",
    "authors": [
      "Sayan Nag",
      "Mayukh Bhattacharyya",
      "Anuraag Mukherjee",
      "Rohit Kundu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and nonmonotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multimodal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of preconditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster."
  },
  "wacv2023_main_ave-clipaudioclip-basedmulti-windowtemporaltransformerforaudiovisualeventlocalization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "AVE-CLIP: AudioCLIP-Based Multi-Window Temporal Transformer for Audio Visual Event Localization",
    "authors": [
      "Tanvir Mahmud",
      "Diana Marculescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mahmud_AVE-CLIP_AudioCLIP-Based_Multi-Window_Temporal_Transformer_for_Audio_Visual_Event_Localization_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mahmud_AVE-CLIP_AudioCLIP-Based_Multi-Window_Temporal_Transformer_for_Audio_Visual_Event_Localization_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "An audio-visual event (AVE) is denoted by the correspondence of the visual and auditory signals in a video segment. Precise localization of the AVEs is very challenging since it demands effective multi-modal feature correspondence to ground the short and long range temporal interactions. Existing approaches struggle in capturing the different scales of multi-modal interaction due to ineffective multi-modal training strategies. To overcome this limitation, we introduce AVE-CLIP, a novel framework that integrates the AudioCLIP pre-trained on large-scale audio-visual data with a multi-window temporal transformer to effectively operate on different temporal scales of video frames. Our contributions are three-fold: (1) We introduce a multi-stage training framework to incorporate AudioCLIP pre-trained with audio-image pairs into the AVE localization task on video frames through contrastive fine-tuning, effective mean video feature extraction, and multi-scale training phases. (2) We propose a multi-domain attention mechanism that operates on both temporal and feature domains over varying timescales to fuse the local and global feature variations. (3) We introduce a temporal refining scheme with event-guided attention followed by a simple-yet-effective post processing step to handle significant variations of the background over diverse events. Our method achieves state-of-the-art performance on the publicly available AVE dataset with 5.9% mean accuracy improvement which proves its superiority over existing approaches."
  },
  "wacv2023_main_misclassificationsofcontactlensirispadalgorithmsisitgenderbiasorenvironmentalconditions?": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Misclassifications of Contact Lens Iris PAD Algorithms: Is It Gender Bias or Environmental Conditions?",
    "authors": [
      "Akshay Agarwal",
      "Nalini Ratha",
      "Afzel Noore",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Misclassifications_of_Contact_Lens_Iris_PAD_Algorithms_Is_It_Gender_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Misclassifications_of_Contact_Lens_Iris_PAD_Algorithms_Is_It_Gender_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "One of the critical steps in biometrics pipeline is detection of presentation attacks, a physical adversary. Several presentation (adversary) attack detection (PAD) algorithms, including iris PAD, are proposed and have shown superlative performance. However, a recent study, on a small-scale database, has highlighted that iris PAD may have gender biases. In this research, we present a rigorous study on gender bias in iris presentation attack detection algorithms using a large-scale and gender-balanced database. The paper provides several interesting observations which can help in building future presentation attack detection algorithms with aim of fair treatment of each demography. In addition, we also present a robust iris presentation attack detection algorithm by combining gender-covariate biased classifiers. The proposed robust classifier not only reduces the difference in accuracy between different genders but also improves the overall performance of the PAD system."
  },
  "wacv2023_main_empiricalgeneralizationstudyunsuperviseddomainadaptationvs.domaingeneralizationmethodsforsemanticsegmentationinthewild": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Empirical Generalization Study: Unsupervised Domain Adaptation vs. Domain Generalization Methods for Semantic Segmentation in the Wild",
    "authors": [
      "Fabrizio J. Piva",
      "Daan de Geus",
      "Gijs Dubbelman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Piva_Empirical_Generalization_Study_Unsupervised_Domain_Adaptation_vs._Domain_Generalization_Methods_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Piva_Empirical_Generalization_Study_Unsupervised_Domain_Adaptation_vs._Domain_Generalization_Methods_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "For autonomous vehicles and mobile robots to safely operate in the real world, i.e., the wild, scene understanding models should perform well in the many different scenarios that can be encountered. In reality, these scenarios are not all represented in the model's training data, leading to poor performance. To tackle this, current training strategies attempt to either exploit additional unlabeled data with unsupervised domain adaptation (UDA), or to reduce overfitting using the limited available labeled data with domain generalization (DG). However, it is not clear from current literature which of these methods allows for better generalization to unseen data from the wild. Therefore, in this work, we present an evaluation framework in which the generalization capabilities of state-of-the-art UDA and DG methods can be compared fairly. From this evaluation, we find that UDA methods, which leverage unlabeled data, outperform DG methods in terms of generalization, and can deliver similar performance on unseen data as fully-supervised training methods that require all data to be labeled. We show that semantic segmentation performance can be increased up to 30% for a priori unknown data without using any extra labeled data."
  },
  "wacv2023_main_vlc-bertvisualquestionansweringwithcontextualizedcommonsenseknowledge": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "VLC-BERT: Visual Question Answering With Contextualized Commonsense Knowledge",
    "authors": [
      "Sahithya Ravi",
      "Aditya Chinchure",
      "Leonid Sigal",
      "Renjie Liao",
      "Vered Shwartz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ravi_VLC-BERT_Visual_Question_Answering_With_Contextualized_Commonsense_Knowledge_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ravi_VLC-BERT_Visual_Question_Answering_With_Contextualized_Commonsense_Knowledge_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work, we focus on questions that require commonsense reasoning. In contrast to previous methods which inject knowledge from static knowledge bases, we investigate the incorporation of contextualized knowledge using Commonsense Transformer (COMET), an existing knowledge model trained on human-curated knowledge bases. We propose a method to generate, select, and encode external commonsense knowledge alongside visual and textual cues in a new pre-trained Vision-Language-Commonsense transformer model, VLC-BERT. Through our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets, we show that VLC-BERT is capable of outperforming existing models that utilize static knowledge bases. Furthermore, through a detailed analysis, we explain which questions benefit, and which don't, from contextualized commonsense knowledge from COMET."
  },
  "wacv2023_main_featuredisentanglementlearningwithswitchingandaggregationforvideo-basedpersonre-identification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Feature Disentanglement Learning With Switching and Aggregation for Video-Based Person Re-Identification",
    "authors": [
      "Minjung Kim",
      "MyeongAh Cho",
      "Sangyoun Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kim_Feature_Disentanglement_Learning_With_Switching_and_Aggregation_for_Video-Based_Person_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kim_Feature_Disentanglement_Learning_With_Switching_and_Aggregation_for_Video-Based_Person_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In video person re-identification (Re-ID), the network must consistently extract features of the target person from successive frames. Existing methods tend to focus only on how to use temporal information, which often leads to networks being fooled by similar appearances and same backgrounds. In this paper, we propose a Disentanglement and Switching and Aggregation Network (DSANet), which segregates the features representing identity and features based on camera characteristics, and pays more attention to ID information. We also introduce an auxiliary task that utilizes a new pair of features created through switching and aggregation to increase the network's capability for various camera scenarios. Furthermore, we devise a Target Localization Module (TLM) that extracts robust features against a change in the position of the target according to the frame flow and a Frame Weight Generation (FWG) that reflects temporal information in the final representation. Various loss functions for disentanglement learning are designed so that each component of the network can cooperate while satisfactorily performing its own role. Quantitative and qualitative results from extensive experiments demonstrate the superiority of DSANet over state-of-the-art methods on three benchmark datasets."
  },
  "wacv2023_main_openearthmapabenchmarkdatasetforglobalhigh-resolutionlandcovermapping": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping",
    "authors": [
      "Junshi Xia",
      "Naoto Yokoya",
      "Bruno Adriano",
      "Clifford Broni-Bediako"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Xia_OpenEarthMap_A_Benchmark_Dataset_for_Global_High-Resolution_Land_Cover_Mapping_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Xia_OpenEarthMap_A_Benchmark_Dataset_for_Global_High-Resolution_Land_Cover_Mapping_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We introduce OpenEarthMap, a benchmark dataset, for global high-resolution land cover mapping. OpenEarthMap consists of 2.2 million segments of 5000 aerial and satellite images covering 97 regions from 44 countries across 6 continents, with manually annotated 8-class land cover labels at a 0.25--0.5m ground sampling distance. Semantic segmentation models trained on the OpenEarthMap generalize worldwide and can be used as off-the-shelf models in a variety of applications. We evaluate the performance of state-of-the-art methods for unsupervised domain adaptation and present challenging problem settings suitable for further technical development. We also investigate lightweight models using automated neural architecture search for limited computational resources and fast mapping. The dataset will be made publicly available."
  },
  "wacv2023_main_semanticsguidedcontrastivelearningoftransformersforzero-shottemporalactivitydetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Semantics Guided Contrastive Learning of Transformers for Zero-Shot Temporal Activity Detection",
    "authors": [
      "Sayak Nag",
      "Orpaz Goldstein",
      "Amit K. Roy-Chowdhury"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nag_Semantics_Guided_Contrastive_Learning_of_Transformers_for_Zero-Shot_Temporal_Activity_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nag_Semantics_Guided_Contrastive_Learning_of_Transformers_for_Zero-Shot_Temporal_Activity_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Zero-shot temporal activity detection (ZSTAD) is the problem of simultaneous temporal localization and classification of activity segments that are previously unseen during training. This is achieved by transferring the knowledge learned from semantically-related seen activities. This ability to reason about unseen concepts without supervision makes ZSTAD very promising for applications where the acquisition of annotated training videos is difficult. In this paper, we design a transformer-based framework titled TranZAD, which streamlines the detection of unseen activities by casting ZSTAD as a direct set-prediction problem, removing the need for hand-crafted designs and manual post-processing. We show how a semantic information-guided contrastive learning strategy can effectively train TranZAD for the zero-shot setting, enabling the efficient transfer of knowledge from the seen to the unseen activities. To reduce confusion between unseen activities and unrelated background information in videos, we introduce a more efficient method of computing the background class embedding by dynamically adapting it as part of the end-to-end learning. Additionally, unlike existing work on ZSTAD, we do not assume the knowledge of which classes are unseen during training and use the visual and semantic information of only the seen classes for the knowledge transfer. This makes TranZAD more viable for practical scenarios, which we evaluate by conducting extensive experiments on Thumos'14 and Charades."
  },
  "wacv2023_main_real-timeconcealedweapondetectionon3dradarimagesforwalk-throughscreeningsystem": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Real-Time Concealed Weapon Detection on 3D Radar Images for Walk-Through Screening System",
    "authors": [
      "Nagma S. Khan",
      "Kazumine Ogura",
      "Eric Cosatto",
      "Masayuki Ariyoshi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Khan_Real-Time_Concealed_Weapon_Detection_on_3D_Radar_Images_for_Walk-Through_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Khan_Real-Time_Concealed_Weapon_Detection_on_3D_Radar_Images_for_Walk-Through_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper presents a framework for real-time concealed weapon detection (CWD) on 3D radar images for walk-through screening systems. The walk-through screening system aims to ensure security in crowded areas by performing CWD on walking persons, hence it requires an accurate and real-time detection approach. To ensure accuracy, a weapon needs to be detected irrespective of its 3D orientation, thus we use the 3D radar images as detection input. For achieving real-time, we reformulate classic U-Net based segmentation networks to perform 3D detection tasks. Our 3D segmentation network predicts peak-shaped probability map, instead of voxel-wise masks, to enable position inference by elementary peak detection operation on the predicted map. In the peak-shaped probability map, the peak marks the weapon's position. So, weapon detection task translates to peak detection on the probability map. A Gaussian function is used to model weapons in the probability map. We experimentally validate our approach on realistic 3D radar images obtained from a walk-through weapon screening system prototype. Extensive ablation studies verify the effectiveness of our proposed approach over existing conventional approaches. The experimental results demonstrate that our proposed approach can perform accurate and real-time CWD, thus making it suitable for practical applications of walk-through screening."
  },
  "wacv2023_main_afpsnetmulti-classpartparsingbasedonscaledattentionandfeaturefusion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "AFPSNet: Multi-Class Part Parsing Based on Scaled Attention and Feature Fusion",
    "authors": [
      "Njuod Alsudays",
      "Jing Wu",
      "Yu-Kun Lai",
      "Ze Ji"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Alsudays_AFPSNet_Multi-Class_Part_Parsing_Based_on_Scaled_Attention_and_Feature_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Alsudays_AFPSNet_Multi-Class_Part_Parsing_Based_on_Scaled_Attention_and_Feature_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multi-class part parsing is a dense prediction task that seeks to simultaneously detect multiple objects and the semantic parts within these objects in the scene. This problem is important in providing detailed object understanding, but is challenging due to the existence of both class-level and part-level ambiguities. In this paper, we propose to integrate an attention refinement module and a feature fusion module to tackle the part-level ambiguity. The attention refinement module aims to enhance the feature representations by focusing on important features. The feature fusion module aims to improve the fusion operation for different scales of features. We also propose an object-to-part training strategy to tackle the class-level ambiguity, which improves the localization of parts by exploiting prior knowledge of objects. The experimental results demonstrated the effectiveness of the proposed modules and the training strategy, and showed that our proposed method achieved state-of-the-art performance on the benchmark dataset."
  },
  "wacv2023_main_thechangeyouwanttosee": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "The Change You Want To See",
    "authors": [
      "Ragav Sachdeva",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sachdeva_The_Change_You_Want_To_See_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sachdeva_The_Change_You_Want_To_See_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We live in a dynamic world where things change all the time. Given two images of the same scene, being able to automatically detect the changes in them has practical applications in a variety of domains. In this paper, we tackle the change detection problem with the goal of detecting \"object-level\" changes in an image pair despite differences in their viewpoint and illumination. To this end, we make the following four contributions: (i) we propose a scalable methodology for obtaining a large-scale change detection training dataset by leveraging existing object segmentation benchmarks; (ii) we introduce a co-attention based novel architecture that is able to implicitly determine correspondences between an image pair and find changes in the form of bounding box predictions; (iii) we contribute four evaluation datasets that cover a variety of domains and transformations, including synthetic image changes, real surveillance images of a 3D scene, and synthetic 3D scenes with camera motion; (iv) we evaluate our model on these four datasets and demonstrate zero-shot and beyond training transformation generalization. The code, datasets and pre-trained model can be found at our project page: https://www.robots.ox.ac.uk/ vgg/research/cyws/"
  },
  "wacv2023_main_kernel-awareburstblindsuper-resolution": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Kernel-Aware Burst Blind Super-Resolution",
    "authors": [
      "Wenyi Lian",
      "Shanglian Peng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lian_Kernel-Aware_Burst_Blind_Super-Resolution_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lian_Kernel-Aware_Burst_Blind_Super-Resolution_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Burst super-resolution technique provides a possibility of restoring rich details from low-quality images. However, since real world low-resolution (LR) images in practical applications have multiple complicated and unknown degradations, existing non-blind (e.g., bicubic) designed networks usually suffer severe performance drop in recovering high-resolution (HR) images. In this paper, we address the problem of reconstructing HR images from raw burst sequences acquired from modern handheld devices. The central idea is a kernel-guided strategy which can solve the burst SR problem with two steps: kernel estimation and HR image restoration. The former estimates burst kernels from raw inputs, while the latter predicts the super-resolved image based on the estimated kernels. Furthermore, we introduce a pyramid kernel-aware deformable alignment module which can effectively align the raw images with consideration of the blurry priors. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method can perform favorable state-of-the-art performance in the burst SR problem."
  },
  "wacv2023_main_bi-directionalframeinterpolationforunsupervisedvideoanomalydetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Bi-Directional Frame Interpolation for Unsupervised Video Anomaly Detection",
    "authors": [
      "Hanqiu Deng",
      "Zhaoxiang Zhang",
      "Shihao Zou",
      "Xingyu Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Deng_Bi-Directional_Frame_Interpolation_for_Unsupervised_Video_Anomaly_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Deng_Bi-Directional_Frame_Interpolation_for_Unsupervised_Video_Anomaly_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Anomaly detection in video surveillance aims to detect anomalous frames whose properties significantly differ from normal patterns. Anomalies in videos can occur in both spatial appearance and temporal motion, making unsupervised video anomaly detection challenging. To tackle this problem, we investigate forward and backward motion continuity between adjacent frames and propose a new video anomaly detection paradigm based on bi-directional frame interpolation. The proposed framework consists of an optical flow estimation network and an interpolation network jointly optimized end-to-end to synthesize a middle frame from its nearest two frames. We further introduce a novel dynamic memory mechanism to balance memory sparsity and normality representation diversity, which attenuates abnormal features in frame interpolation without affecting normal prototypes. In inference, interpolation error and dynamic memory error are fused as anomaly scores. The proposed bi-directional interpolation design improves normal frame synthesis, lowering the false alarm rate of anomaly appearance; meanwhile, the implicit \"regular\" motion constraint in our optical flow estimation and the novel dynamic memory mechanism play blocking roles in interpolating abnormal frames, increasing the system's sensitivity to anomalies. Extensive experiments on public benchmarks demonstrate the superiority of the proposed framework over prior arts."
  },
  "wacv2023_main_isyournoisecorrectionnoisy?plsrobustnesstolabelnoisewithtwostagedetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Is Your Noise Correction Noisy? PLS: Robustness To Label Noise With Two Stage Detection",
    "authors": [
      "Paul Albert",
      "Eric Arazo",
      "Tarun Krishna",
      "Noel E. O\u2019Connor",
      "Kevin McGuinness"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Albert_Is_Your_Noise_Correction_Noisy_PLS_Robustness_To_Label_Noise_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Albert_Is_Your_Noise_Correction_Noisy_PLS_Robustness_To_Label_Noise_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Designing robust algorithms capable of training accurate neural networks on uncurated datasets from the web has been the subject of much research as it reduces the need for time consuming human labor. The focus of many previous research contributions has been on the detection of different types of label noise; however, this paper proposes to improve the correction accuracy of noisy samples once they have been detected. In many state-of-the-art contributions, a two phase approach is adopted where the noisy samples are detected before guessing a corrected pseudo-label in a semi-supervised fashion. The guessed pseudo-labels are then used in the supervised objective without ensuring that the label guess is likely to be correct. This can lead to confirmation bias, which reduces the noise robustness. Here we propose the pseudo-loss, a simple metric that we find to be strongly correlated with pseudo-label correctness on noisy samples. Using the pseudo-loss, we dynamically down weight under-confident pseudo-labels throughout training to avoid confirmation bias and improve the network accuracy. We additionally propose to use a confidence guided contrastive objective that learns robust representation on an interpolated objective between class bound (supervised) for confidently corrected samples and unsupervised representation for under-confident label corrections. Experiments demonstrate the state-of-the-art performance of our Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets including curated data synthetically corrupted with in-distribution and out-of-distribution noise, and two real world web noise datasets. Our experiments are fully reproducible github.com/PaulAlbert31/PLS."
  },
  "wacv2023_main_pp4avabenchmarkingdatasetforprivacy-preservingautonomousdriving": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "PP4AV: A Benchmarking Dataset for Privacy-Preserving Autonomous Driving",
    "authors": [
      "Linh Trinh",
      "Phuong Pham",
      "Hoang Trinh",
      "Nguyen Bach",
      "Dung Nguyen",
      "Giang Nguyen",
      "Huy Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Trinh_PP4AV_A_Benchmarking_Dataset_for_Privacy-Preserving_Autonomous_Driving_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Trinh_PP4AV_A_Benchmarking_Dataset_for_Privacy-Preserving_Autonomous_Driving_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Massive data collected on public roads for autonomous driving has become more popular in many locations in the world. More collected data leads to more concerns about data privacy, including but not limited to pedestrian faces and surrounding vehicle license plates, which urges for robust solutions for detecting and anonymizing them in realistic road-driving scenarios. Existing public datasets for both face and license plate detection are either not focused on autonomous driving or only in parking lots. In this paper, we introduce a challenging public dataset for face and license plate detection in autonomous driving domain. The dataset is aggregated from visual data that is available in public domain, to cover scenarios from six European cities, including daytime and nighttime, annotated with both faces and license plates. All of the images feature a variety of poses and sizes for both faces and license plates. Our dataset offers not only a benchmark for evaluating data anonymization models but also data to get more insights about privacy-preserving autonomous driving. The experimental results showed that 1) current generic state-of-the-art face and/or license plate detection models do not perform well on a realistic and diverse road-driving dataset like ours, 2) our model trained with autonomous driving data (even with soft-labeling data) outperformed strong but generic models, and 3) the size of faces and license plates is an important factor for evaluating and optimizing the performance of privacy-preserving autonomous driving. The annotation of dataset as well as baseline model and results are available at our github: https://github.com/khaclinh/pp4av."
  },
  "wacv2023_main_heightfieldsforefficientscenereconstructionforar": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Heightfields for Efficient Scene Reconstruction for AR",
    "authors": [
      "Jamie Watson",
      "Sara Vicente",
      "Oisin Mac Aodha",
      "Cl\u00e9ment Godard",
      "Gabriel Brostow",
      "Michael Firman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "3D scene reconstruction from a sequence of posed RGB images is a cornerstone task for computer vision and augmented reality (AR). While depth-based fusion is the foundation of most real-time approaches for 3D reconstruction, recent learning based methods that operate directly on RGB images can achieve higher quality reconstructions, but at the cost of increased runtime and memory requirements, making them unsuitable for AR applications. We propose an efficient learning-based method that refines the 3D reconstruction obtained by a traditional fusion approach. By leveraging a top-down heightfield representation, our method remains real-time while approaching the quality of other learning-based methods. Despite being a simplification, our heightfield is perfectly appropriate for robotic path planning or augmented reality character placement. We outline several innovations that push the performance beyond existing top-down prediction baselines, and we present an evaluation framework on the challenging ScanNetV2 dataset, targeting AR tasks. Ultimately, we show that our method improves over the baselines for AR applications. Full code and pretrained models will be released on acceptance."
  },
  "wacv2023_main_mffnmulti-viewfeaturefusionnetworkforcamouflagedobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MFFN: Multi-View Feature Fusion Network for Camouflaged Object Detection",
    "authors": [
      "Dehua Zheng",
      "Xiaochen Zheng",
      "Laurence T. Yang",
      "Yuan Gao",
      "Chenlu Zhu",
      "Yiheng Ruan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zheng_MFFN_Multi-View_Feature_Fusion_Network_for_Camouflaged_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zheng_MFFN_Multi-View_Feature_Fusion_Network_for_Camouflaged_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recent research about camouflaged object detection (COD) aims to segment highly concealed objects hidden in complex surroundings. The tiny, fuzzy camouflaged objects result in visually indistinguishable properties. However, current single-view COD detectors are sensitive to background distractors. Therefore, blurred boundaries and variable shapes of the camouflaged objects are challenging to be fully captured with a single-view detector. To overcome these obstacles, we propose a behavior-inspired framework, called Multi-view Feature Fusion Network (MFFN), which mimics the human behaviors of finding indistinct objects in images, i.e., observing from multiple angles, distances, perspectives. Specifically, the key idea behind it is to generate multiple ways of observation (multi-view) by data augmentation and apply them as inputs. MFFN captures critical boundary and semantic information by comparing and fusing extracted multi-view features. In addition, our MFFN exploits the dependence and interaction between views and channels. Specifically, our methods leverage the complementary information between different views through a two-stage attention module called Co-attention of Multi-view (CAMV). And we design a local-overall module called Channel Fusion Unit (CFU) to explore the channel-wise contextual clues of diverse feature maps in an iterative manner. The experiment results show that our method performs favorably against existing state-of-the-art methods via training with the same data. The code will be available at https://github.com/dwardzheng/MFFN_COD."
  },
  "wacv2023_main_federatedlearningforcommercialimagesources": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Federated Learning for Commercial Image Sources",
    "authors": [
      "Shreyansh Jain",
      "Koteswar Rao Jerripothula"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jain_Federated_Learning_for_Commercial_Image_Sources_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jain_Federated_Learning_for_Commercial_Image_Sources_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Federated Learning is a collaborative machine learning paradigm that enables multiple clients to learn a global model without exposing their data to each other. Consequently, it provides a secure learning platform with privacy-preserving capabilities. This paper introduces a new dataset containing 23,326 images collected from eight different commercial sources and classified into 31 categories, similar to the Office-31 dataset. To the best of our knowledge, this is the first image classification dataset specifically designed for Federated Learning. We also propose two new Federated Learning algorithms, namely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from its previous client, updates them through local training, and passes them to the next client, thus forming a cyclic topology. In Fed-Star, a client receives weights from all other clients, updates its local weights through pre-aggregation (to address statistical heterogeneity) and local training, and sends its updated local weights to all other clients, thus forming a star-like topology. Our experiments reveal that both algorithms perform better than existing baselines on our newly introduced dataset."
  },
  "wacv2023_main_adaptivelocal-component-awaregraphconvolutionalnetworkforone-shotskeleton-basedactionrecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Adaptive Local-Component-Aware Graph Convolutional Network for One-Shot Skeleton-Based Action Recognition",
    "authors": [
      "Anqi Zhu",
      "Qiuhong Ke",
      "Mingming Gong",
      "James Bailey"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhu_Adaptive_Local-Component-Aware_Graph_Convolutional_Network_for_One-Shot_Skeleton-Based_Action_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhu_Adaptive_Local-Component-Aware_Graph_Convolutional_Network_for_One-Shot_Skeleton-Based_Action_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Skeleton-based action recognition receives increasing attention because skeleton sequences reduce training complexity by eliminating visual information irrelevant to actions. To further improve sample efficiency, meta-learning-based one-shot learning solutions were developed for skeleton-based action recognition. These methods predict by finding the nearest neighbors according to the similarity between instance-level global embedding. However, such measurement holds unstable representativity due to inadequate generalized learning on the averaged local invariant and noisy features, while intuitively, steady and fine-grained recognition relies on determining key local body movements. To address this limitation, we present the Adaptive Local-Component-aware Graph Convolutional Network, which replaces the comparison metric with a focused sum of similarity measurements on aligned local embedding of action-critical spatial/temporal segments. Comprehensive one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our method provides a stronger representation than the global embedding and helps our model reach state-of-the-art."
  },
  "wacv2023_main_inducingdataamplificationusingauxiliarydatasetsinadversarialtraining": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Inducing Data Amplification Using Auxiliary Datasets in Adversarial Training",
    "authors": [
      "Saehyung Lee",
      "Hyungyu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_Inducing_Data_Amplification_Using_Auxiliary_Datasets_in_Adversarial_Training_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_Inducing_Data_Amplification_Using_Auxiliary_Datasets_in_Adversarial_Training_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Several recent studies have shown that the use of extra in-distribution data can lead to a high level of adversarial robustness. However, there is no guarantee that it will always be possible to obtain sufficient extra data for a selected dataset. In this paper, we propose a biased multi-domain adversarial training (BiaMAT) method that induces training data amplification on a primary dataset using publicly available auxiliary datasets, without requiring the class distribution match between the primary and auxiliary datasets. The proposed method can achieve increased adversarial robustness on a primary dataset by leveraging auxiliary datasets via multi-domain learning. Specifically, data amplification on both robust and non-robust features can be accomplished through the application of BiaMAT as demonstrated through a theoretical and empirical analysis. Moreover, we demonstrate that while existing methods are vulnerable to negative transfer due to the distributional discrepancy between auxiliary and primary data, the proposed method enables neural networks to flexibly leverage diverse image datasets for adversarial training by successfully handling the domain discrepancy through the application of a confidence-based selection strategy. The code and pre-trained models of our study are available at: https://github.com/BiaMAT/BiaMAT_under_review."
  },
  "wacv2023_main_atttrackonlinedeepattentiontransferformulti-objecttracking": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "AttTrack: Online Deep Attention Transfer for Multi-Object Tracking",
    "authors": [
      "Keivan Nalaie",
      "Rong Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nalaie_AttTrack_Online_Deep_Attention_Transfer_for_Multi-Object_Tracking_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nalaie_AttTrack_Online_Deep_Attention_Transfer_for_Multi-Object_Tracking_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multi-object tracking (MOT) is a vital component of intelligent video analytics applications such as surveillance and autonomous driving. The time and storage complexity required to execute deep learning models for visual object tracking hinder their adoption on embedded devices with limited computing power. In this paper, we aim to accelerate MOT by transferring the knowledge from high-level features of a complex network (teacher) to a lightweight network (student) at both training and inference times. The proposed AttTrack framework has three key components: 1) cross-model feature learning to align intermediate representations from the teacher and student models, 2) interleaving the execution of the two models at inference time, and 3) incorporating the updated predictions from the teacher model as prior knowledge to assist the student model. Experiments on pedestrian tracking tasks are conducted on the MOT17 and MOT15 datasets using two different object detection backbones YOLOv5 and DLA34 show that AttTrack can significantly improve student model tracking performance while sacrificing only minor degradation of tracking speed."
  },
  "wacv2023_main_pruning-guidedcurriculumlearningforsemi-supervisedsemanticsegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Pruning-Guided Curriculum Learning for Semi-Supervised Semantic Segmentation",
    "authors": [
      "Heejo Kong",
      "Gun-Hee Lee",
      "Suneung Kim",
      "Seong-Whan Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kong_Pruning-Guided_Curriculum_Learning_for_Semi-Supervised_Semantic_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kong_Pruning-Guided_Curriculum_Learning_for_Semi-Supervised_Semantic_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This study focuses on improving the quality of pseudo-labeling in the context of semi-supervised semantic segmentation. Previous studies have adopted confidence thresholding to reduce erroneous predictions in pseudo-labeled data and to enhance their qualities. However, numerous pseudo-labels with high confidence scores exist in the early training stages even though their predictions are incorrect, and this ambiguity limits confidence thresholding substantially. In this paper, we present a novel method to resolve the ambiguity of confidence scores with the guidance of network pruning. A recent finding showed that network pruning severely impairs the network generalization ability on samples that are not yet well learned or represented. Inspired by this finding, we refine the confidence scores by reflecting the extent to which the predictions are affected by pruning. Furthermore, we adopted a curriculum learning strategy for the confidence score, which enables the network to learn gradually from easy to hard samples. This approach resolves the ambiguity by suppressing the learning of noisy pseudo-labels, the confidence scores of which are difficult to trust owing to insufficient training in the early stages. Extensive experiments on various benchmarks demonstrate the superiority of our framework over state-of-the-art alternatives."
  },
  "wacv2023_main_ml-decoderscalableandversatileclassificationhead": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ML-Decoder: Scalable and Versatile Classification Head",
    "authors": [
      "Tal Ridnik",
      "Gilad Sharir",
      "Avi Ben-Cohen",
      "Emanuel Ben-Baruch",
      "Asaf Noy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ridnik_ML-Decoder_Scalable_and_Versatile_Classification_Head_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ridnik_ML-Decoder_Scalable_and_Versatile_Classification_Head_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we introduce ML-Decoder, a new attention-based classification head. ML-Decoder predicts the existence of class labels via queries, and enables better utilization of spatial data compared to global average pooling. By redesigning the decoder architecture, and using a novel group-decoding scheme, ML-Decoder is highly efficient, and can scale well to thousands of classes. Compared to using a larger backbone, ML-Decoder consistently provides a better speed-accuracy trade-off. ML-Decoder is also versatile - it can be used as a drop-in replacement for various classification heads, and generalize to unseen classes when operated with word queries. Novel query augmentations further improve its generalization ability. Using ML-Decoder, we achieve state-of-the-art results on several classification tasks: on MS-COCO multi-label, we reach 91.1% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP; and on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top score of 80.7%, without extra data or distillation. Public code will be available."
  },
  "wacv2023_main_zero-shotversusmany-shotunsupervisedtextureanomalydetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Zero-Shot Versus Many-Shot: Unsupervised Texture Anomaly Detection",
    "authors": [
      "Toshimichi Aota",
      "Lloyd Teh Tzer Tong",
      "Takayuki Okatani"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Aota_Zero-Shot_Versus_Many-Shot_Unsupervised_Texture_Anomaly_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Aota_Zero-Shot_Versus_Many-Shot_Unsupervised_Texture_Anomaly_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Research on unsupervised anomaly detection (AD) has recently progressed, significantly increasing detection accuracy. This paper focuses on texture images and considers how few normal samples are needed for accurate AD. We first highlight the critical nature of the problem that previous studies have overlooked: accurate detection gets harder for anisotropic textures when image orientations are not aligned between inputs and normal samples. We then propose a zero-shot method, which detects anomalies without using a normal sample. The method is free from the issue of unaligned orientation between input and normal images. It assumes the input texture to be homogeneous, detecting image regions that break the homogeneity as anomalies. We present a quantitative criterion to judge whether this assumption holds for an input texture. Experimental results show the broad applicability of the proposed zero-shot method and its good performance comparable to or even higher than the state-of-the-art methods using hundreds of normal samples. The code and data are available from https://drive.google.com/drive/folders/10OyPzvI3H6llCZBxKxFlKWt1Pw1tkMK1."
  },
  "wacv2023_main_unsupervisedaudio-visuallecturesegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Unsupervised Audio-Visual Lecture Segmentation",
    "authors": [
      "Darshan Singh S.",
      "Anchit Gupta",
      "C. V. Jawahar",
      "Makarand Tapaswi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/S._Unsupervised_Audio-Visual_Lecture_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/S._Unsupervised_Audio-Visual_Lecture_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Over the last decade, online lecture videos have become increasingly popular and have experienced a meteoric rise during the pandemic. However, video-language research has primarily focused on instructional videos or movies, and tools to help students navigate the growing online lectures are lacking. Our first contribution is to facilitate research in the educational domain, by introducing AVLectures, a large-scale dataset consisting of 86 courses with over 2,350 lectures covering various STEM subjects. Each course contains video lectures, transcripts, OCR outputs for lecture frames, and optionally lecture notes, slides, assignments, and related educational content that can inspire a variety of tasks. Our second contribution is introducing video lecture segmentation that splits lectures into bite-sized topics that show promise in improving learner engagement. We formulate lecture segmentation as an unsupervised task that leverages visual, textual, and OCR cues from the lecture, while clip representations are fine-tuned on a pretext self-supervised task of matching the narration with the temporally aligned visual content. We use these representations to generate segments using a temporally consistent 1-nearest neighbor algorithm, TW-FINCH. We evaluate our method on 15 courses and compare it against various visual and textual baselines, outperforming all of them. Our comprehensive ablation studies also identify the key factors driving the success of our approach."
  },
  "wacv2023_main_diffeomorphicimageregistrationwithneuralvelocityfield": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Diffeomorphic Image Registration With Neural Velocity Field",
    "authors": [
      "Kun Han",
      "Shanlin Sun",
      "Xiangyi Yan",
      "Chenyu You",
      "Hao Tang",
      "Junayed Naushad",
      "Haoyu Ma",
      "Deying Kong",
      "Xiaohui Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Han_Diffeomorphic_Image_Registration_With_Neural_Velocity_Field_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Han_Diffeomorphic_Image_Registration_With_Neural_Velocity_Field_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Diffeomorphic image registration, offering smooth transformation and topology preservation, is required in many medical image analysis tasks.Traditional methods impose certain modeling constraints on the space of admissible transformations and use optimization to find the optimal transformation between two images. Specifying the right space of admissible transformations is challenging: the registration quality can be poor if the space is too restrictive, while the optimization can be hard to solve if the space is too general. Recent learning-based methods, utilizing deep neural networks to learn the transformation directly, achieve fast inference, but face challenges in accuracy due to the difficulties in capturing the small local deformations and generalization ability. Here we propose a new optimization-based method named DNVF (Diffeomorphic Image Registration with Neural Velocity Field) which utilizes deep neural network to model the space of admissible transformations. A multilayer perceptron (MLP) with sinusoidal activation function is used to represent the continuous velocity field and assigns a velocity vector to every point in space, providing the flexibility of modeling complex deformations as well as the convenience of optimization. Moreover, we propose a cascaded image registration framework (Cas-DNVF) by combining the benefits of both optimization and learning based methods, where a fully convolutional neural network (FCN) is trained to predict the initial deformation, followed by DNVF for further refinement. Experiments on two large-scale 3D MR brain scan datasets demonstrate that our proposed methods significantly outperform the state-of-the-art registration methods."
  },
  "wacv2023_main_densebutefficientvideoqaforintricatecompositionalreasoning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dense but Efficient VideoQA for Intricate Compositional Reasoning",
    "authors": [
      "Jihyeon Lee",
      "Wooyoung Kang",
      "Eun-Sol Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_Dense_but_Efficient_VideoQA_for_Intricate_Compositional_Reasoning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_Dense_but_Efficient_VideoQA_for_Intricate_Compositional_Reasoning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "It is well known that most of the conventional video question answering (VideoQA) datasets consist of easy questions requiring simple reasoning processes. However, long videos inevitably contain complex and compositional semantic structures along with the spatio-temporal axis, which requires a model to understand the compositional structures inherent in the videos. In this paper, we suggest a new compositional VideoQA method based on transformer architecture with a deformable attention mechanism to address the complex VideoQA tasks. The deformable attentions are introduced to sample a subset of informative visual features from the dense visual feature map to cover a temporally long range of frames efficiently. Furthermore, the dependency structure within the complex question sentences is also combined with the language embeddings to readily understand the relations among question words. Extensive experiments and ablation studies show that the suggested dense but efficient model outperforms other baselines."
  },
  "wacv2023_main_multi-viewphotometricstereorevisited": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Multi-View Photometric Stereo Revisited",
    "authors": [
      "Berk Kaya",
      "Suryansh Kumar",
      "Carlos Oliveira",
      "Vittorio Ferrari",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kaya_Multi-View_Photometric_Stereo_Revisited_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kaya_Multi-View_Photometric_Stereo_Revisited_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Multi-view photometric stereo (MVPS) is a preferred method for detailed and precise 3D acquisition of an object from images. Although popular methods for MVPS can provide outstanding results, they are often complex to execute and limited to isotropic material objects. To address such limitations, we present a simple, practical approach to MVPS, which works well for isotropic as well as other object material types such as anisotropic and glossy. The proposed approach in this paper exploits the benefit of uncertainty modeling in a deep neural network for a reliable fusion of photometric stereo (PS) and multi-view stereo (MVS) network predictions. Yet, contrary to the recently proposed state-of-the-art, we introduce neural volume rendering methodology for a trustworthy fusion of MVS and PS measurements. The advantage of introducing neural volume rendering is that it helps in the reliable modeling of objects with diverse material types, where existing MVS methods, PS methods, or both may fail. Furthermore, it allows us to work on neural 3D shape representation, which has recently shown outstanding results for many geometric processing tasks. Our suggested new loss function aims to fit the zero level set of the implicit neural function using the most certain MVS and PS network predictions coupled with weighted neural volume rendering cost. The proposed approach shows state-of-the-art results when tested extensively on several benchmark datasets."
  },
  "wacv2023_main_k-vqgknowledge-awarevisualquestiongenerationforcommon-senseacquisition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "K-VQG: Knowledge-Aware Visual Question Generation for Common-Sense Acquisition",
    "authors": [
      "Kohei Uehara",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Uehara_K-VQG_Knowledge-Aware_Visual_Question_Generation_for_Common-Sense_Acquisition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Uehara_K-VQG_Knowledge-Aware_Visual_Question_Generation_for_Common-Sense_Acquisition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Visual Question Generation (VQG) is a task to generate questions from images. When humans ask questions about an image, their goal is often to acquire some new knowledge. However, existing studies on VQG have mainly addressed question generation from answers or question categories, overlooking the objectives of knowledge acquisition. To introduce a knowledge acquisition perspective into VQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is the first large, humanly annotated dataset in which questions regarding images are tied to structured knowledge. We also developed a new VQG model that can encode and use knowledge as the target for a question. The experiment results show that our model outperforms existing models on the K-VQG dataset. Our dataset is publicly available at https://uehara-mech.github.io/kvqg."
  },
  "wacv2023_main_virtualhomeactiongenomeasimulatedspatio-temporalscenegraphdatasetwithconsistentrelationshiplabels": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "VirtualHome Action Genome: A Simulated Spatio-Temporal Scene Graph Dataset With Consistent Relationship Labels",
    "authors": [
      "Yue Qiu",
      "Yoshiki Nagasaki",
      "Kensho Hara",
      "Hirokatsu Kataoka",
      "Ryota Suzuki",
      "Kenji Iwata",
      "Yutaka Satoh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Qiu_VirtualHome_Action_Genome_A_Simulated_Spatio-Temporal_Scene_Graph_Dataset_With_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Qiu_VirtualHome_Action_Genome_A_Simulated_Spatio-Temporal_Scene_Graph_Dataset_With_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Spatio-temporal scene graph generation is an essential task in household activity recognition that aims to identify human-object interactions. Constructing a dataset with per-frame object region and consistent relationship annotations requires extremely high labor costs. Existing datasets sparsely annotate frames sampled from videos, resulting in the lack of dense spatio-temporal correlation in videos. Additionally, existing datasets contain inconsistent relationship annotations, leading to the problem of learning ambiguous temporal associations. Moreover, existing datasets mainly discuss relationships that can be inferred from a single frame, ignoring the significance of temporal associations. To resolve those issues, we created a simulated dataset with per-frame consistent annotations and introduced a range of relationships requiring both spatial and temporal context. Most existing methods explore spatial correlations within single images and do not explicitly consider the dynamic changes across frames. Therefore, we proposed a tracking-based approach that explicitly grasps spatio-temporal human-object interactions while simultaneously localizing humans and objects. Our proposed approach achieved state-of-the-art performance on scene graph generation and outperformed existing methods in scene graph localization by large margins on the proposed dataset. Moreover, the experiments show the efficacy of pre-training on the proposed dataset while adapting to a previous benchmark consisting of real daily videos, indicating the potential of the proposed dataset in real-world scenarios."
  },
  "wacv2023_main_towardsinterpretablevideoanomalydetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Towards Interpretable Video Anomaly Detection",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Doshi_Towards_Interpretable_Video_Anomaly_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Doshi_Towards_Interpretable_Video_Anomaly_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Most video anomaly detection approaches are based on data-intensive end-to-end trained neural networks, which extract spatiotemporal features from videos. The extracted feature representations in such approaches are not interpretable, which prevents the automatic identification of anomaly cause. To this end, we propose a novel framework which can explain the detected anomalous event in a surveillance video. In addition to monitoring objects independently, we also monitor the interactions between them to detect anomalous events and explain their root causes. Specifically, we demonstrate that the scene graphs obtained by monitoring the object interactions provide an interpretation for the context of the anomaly while performing competitively with respect to the recent state-of-the-art approaches. Moreover, the proposed interpretable method enables cross-domain adaptability (i.e., transfer learning in another surveillance scene), which is not feasible for most existing end-to-end methods due to the lack of sufficient labeled training data for every surveillance scene. The quick and reliable detection performance of the proposed method is evaluated both theoretically (through an asymptotic optimality proof) and empirically on the popular benchmark datasets."
  },
  "wacv2023_main_weaklysupervisedcell-instancesegmentationwithtwotypesofweaklabelsbysingleinstancepasting": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Weakly Supervised Cell-Instance Segmentation With Two Types of Weak Labels by Single Instance Pasting",
    "authors": [
      "Kazuya Nishimura",
      "Ryoma Bise"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nishimura_Weakly_Supervised_Cell-Instance_Segmentation_With_Two_Types_of_Weak_Labels_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nishimura_Weakly_Supervised_Cell-Instance_Segmentation_With_Two_Types_of_Weak_Labels_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Cell instance segmentation that recognizes each cell boundary is an important task in cell image analysis. While deep learning-based methods have shown promising performances with a certain amount of training data, most of them require full annotations that show the boundary of each cell. Generating the annotation for cell segmentation is time-consuming and human labor. To reduce the annotation cost, we propose a weakly supervised segmentation method using two types of weak labels (one for cell type and one for nuclei position). Unlike general images, these two labels are easily obtained in phase-contrast images. The intercellular boundary, which is necessary for cell instance segmentation, cannot be directly obtained from these two weak labels, so to generate the boundary information, we propose a single instance pasting based on the copy-and-paste technique. First, we locate single-cell regions by counting cells and store them in a pool. Then, we generate the intercellular boundary by pasting the stored single-cell regions to the original image. Finally, we train a boundary estimation network with the generated labels and perform instance segmentation with the network. Our evaluation on a public dataset demonstrated that the proposed method achieves the best performance among the several weakly supervised methods we compared."
  },
  "wacv2023_main_dramajointrisklocalizationandcaptioningindriving": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DRAMA: Joint Risk Localization and Captioning in Driving",
    "authors": [
      "Srikanth Malla",
      "Chiho Choi",
      "Isht Dwivedi",
      "Joon Hee Choi",
      "Jiachen Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Considering the functionality of situational awareness in safety-critical automation systems, the perception of risk in driving scenes and its explainability is of particular importance for autonomous and cooperative driving. Toward this goal, this paper proposes a new research direction of joint risk localization in driving scenes and its risk explanation as a natural language description. Due to the lack of standard benchmarks, we collected a large-scale dataset, DRAMA (Driving Risk Assessment Mechanism with A captioning module), which consists of 17,785 interactive driving scenarios collected in Tokyo, Japan. Our DRAMA dataset accommodates video- and object-level questions on driving risks with associated important objects to achieve the goal of visual captioning as a free-form language description utilizing closed and open-ended responses for multi-level questions, which can be used to evaluate a range of visual captioning capabilities in driving scenarios. We make this data available to the community for further research. Using DRAMA, we explore multiple facets of joint risk localization and captioning in interactive driving scenarios. In particular, we benchmark various multi-task prediction architectures and provide a detailed analysis of joint risk localization and risk captioning. The data set is available at https://usa.honda-ri.com/drama"
  },
  "wacv2023_main_visiontransformerfornerf-basedviewsynthesisfromasingleinputimage": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Vision Transformer for NeRF-Based View Synthesis From a Single Input Image",
    "authors": [
      "Kai-En Lin",
      "Yen-Chen Lin",
      "Wei-Sheng Lai",
      "Tsung-Yi Lin",
      "Yi-Chang Shih",
      "Ravi Ramamoorthi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lin_Vision_Transformer_for_NeRF-Based_View_Synthesis_From_a_Single_Input_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lin_Vision_Transformer_for_NeRF-Based_View_Synthesis_From_a_Single_Input_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Although neural radiance fields (NeRF) have shown impressive advances in novel view synthesis, most methods require multiple input images of the same scene with accurate camera poses. In this work, we seek to substantially reduce the inputs to a single unposed image. Existing approaches using local image features to reconstruct a 3D object often render blurry predictions at viewpoints distant from the source view. To address this, we propose to leverage both the global and local features to form an expressive 3D representation. The global features are learned from a vision transformer, while the local features are extracted from a 2D convolutional network. To synthesize a novel view, we train a multi-layer perceptron (MLP) network conditioned on the learned 3D representation to perform volume rendering. This novel 3D representation allows the network to reconstruct unseen regions without enforcing constraints like symmetry or canonical coordinate systems. Our method renders novel views from just a single input image, and generalizes across multiple object categories using a single model. Quantitative and qualitative evaluations demonstrate that the proposed method achieves state-of-the-art performance and renders richer details than existing approaches."
  },
  "wacv2023_main_dbceasaliencymethodformedicaldeeplearningthroughanatomically-consistentfree-formdeformations": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DBCE: A Saliency Method for Medical Deep Learning Through Anatomically-Consistent Free-Form Deformations",
    "authors": [
      "Joshua Peters",
      "L\u00e9o Lebrat",
      "Rodrigo Santa Cruz",
      "Aaron Nicolson",
      "Gregg Belous",
      "Salamata Konate",
      "Parnesh Raniga",
      "Vincent Dore",
      "Pierrick Bourgeat",
      "Jurgen Mejan-Fripp",
      "Clinton Fookes",
      "Olivier Salvado"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Peters_DBCE_A_Saliency_Method_for_Medical_Deep_Learning_Through_Anatomically-Consistent_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Peters_DBCE_A_Saliency_Method_for_Medical_Deep_Learning_Through_Anatomically-Consistent_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep learning models are powerful tools for addressing challenging medical imaging problems. However, for an ever-growing range of applications, interpreting a model's prediction remains non-trivial. Understanding decisions made by black-box algorithms is critical, and assessing their fairness and susceptibility to bias is a key step towards healthcare deployment. In this paper, we propose DBCE (Deformation Based Counterfactual Explainability). We optimise a diffeomorphic transformation that deforms a given input image to change the prediction of the model. This provides anatomically meaningful saliency maps indicating tissue atrophy and expansion, which can be easily interpreted by clinicians. In our test case, DBCE replicates the transition of a patient from healthy control (HC) to Alzheimer's disease (AD). We benchmark DBCE against three commonly used saliency methods. We show that it provides more meaningful saliency maps when applied to one subject and disease-consistent atrophy patterns when used over a larger cohort. In addition, our method fulfils a recent sanity check and is repeatable for different model initialisations in contrast to classical sensitivity-based methods."
  },
  "wacv2023_main_eventpointself-supervisedinterestpointdetectionanddescriptionforevent-basedcamera": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "EventPoint: Self-Supervised Interest Point Detection and Description for Event-Based Camera",
    "authors": [
      "Ze Huang",
      "Li Sun",
      "Cheng Zhao",
      "Song Li",
      "Songzhi Su"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Huang_EventPoint_Self-Supervised_Interest_Point_Detection_and_Description_for_Event-Based_Camera_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Huang_EventPoint_Self-Supervised_Interest_Point_Detection_and_Description_for_Event-Based_Camera_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper proposes a self-supervised learned local detector and descriptor, called EventPoint, for event stream/camera tracking and registration. Event-based cameras have grown in popularity because of their biological inspiration and low power consumption. Despite this, applying local features directly to the event stream is difficult due to its peculiar data structure. We propose a new time-surface-like event stream representation method called Tencode. The event stream data processed by Tencode can obtain the pixel-level positioning of interest points while also simultaneously extracting descriptors through a neural network. Instead of using costly and unreliable manual annotation, our network leverages the prior knowledge of local feature extraction on color images and conducts self-supervised learning via homographic and spatio-temporal adaptation. To the best of our knowledge, our proposed method is the first research on event-based local features learning using a deep neural network. We provide comprehensive experiments of feature point detection and matching, and three public datasets are used for evaluation (i.e. DSEC, N-Caltech101, and HVGA ATIS Corner Dataset). The experimental findings demonstrate that our method outperforms SOTA in terms of feature point detection and description."
  },
  "wacv2023_main_leveragingoff-the-shelfdiffusionmodelformulti-attributefashionimagemanipulation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Leveraging Off-the-Shelf Diffusion Model for Multi-Attribute Fashion Image Manipulation",
    "authors": [
      "Chaerin Kong",
      "DongHyeon Jeon",
      "Ohjoon Kwon",
      "Nojun Kwak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kong_Leveraging_Off-the-Shelf_Diffusion_Model_for_Multi-Attribute_Fashion_Image_Manipulation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kong_Leveraging_Off-the-Shelf_Diffusion_Model_for_Multi-Attribute_Fashion_Image_Manipulation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Fashion attribute editing is a task that aims to convert the semantic attributes of a given fashion image while preserving the irrelevant regions. Previous works typically employ conditional GANs where the generator explicitly learns the target attributes and directly execute the conversion. These approaches, however, are neither scalable nor generic as they operate only with few limited attributes and a separate generator is required for each dataset or attribute set. Inspired by the recent advancement of diffusion models, we explore the classifier-guided diffusion that leverages the off-the-shelf diffusion model pretrained on general visual semantics such as Imagenet. In order to achieve a generic editing pipeline, we pose this as multi-attribute image manipulation task, where the attribute ranges from item category, fabric, pattern to collar and neckline. We empirically show that conventional methods fail in our challenging setting, and study efficient adaptation scheme that involves recently introduced attention-pooling technique to obtain a multi-attribute classifier guidance. Based on this, we present a mask-free fashion attribute editing framework that leverages the classifier logits and the cross-attention map for manipulation. We empirically demonstrate that our framework achieves convincing sample quality and attribute alignments."
  },
  "wacv2023_main_x-nerfexplicitneuralradiancefieldformulti-scene360deginsufficientrgb-dviews": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360deg Insufficient RGB-D Views",
    "authors": [
      "Haoyi Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhu_X-NeRF_Explicit_Neural_Radiance_Field_for_Multi-Scene_360deg_Insufficient_RGB-D_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhu_X-NeRF_Explicit_Neural_Radiance_Field_for_Multi-Scene_360deg_Insufficient_RGB-D_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Neural Radiance Fields (NeRFs), despite their outstanding performance on novel view synthesis, often need dense input views. Many papers train one model for each scene respectively and few of them explore incorporating multi-modal data into this problem. In this paper, we focus on a rarely discussed but important setting: can we train one model that can represent multiple scenes, with 360deg insufficient views and RGB-D images? We refer insufficient views to few extremely sparse and almost non-overlapping views. To deal with it, X-NeRF, a fully explicit approach which learns a general scene completion process instead of a coordinate-based mapping, is proposed. Given a few insufficient RGB-D input views, X-NeRF first transforms them to a sparse point cloud tensor and then applies a 3D sparse generative Convolutional Neural Network (CNN) to complete it to an explicit radiance field whose volumetric rendering can be conducted fast without running networks during inference. To avoid overfitting, besides common rendering loss, we apply perceptual loss as well as view augmentation through random rotation on point clouds. The proposed methodology significantly out-performs previous implicit methods in our setting, indicating the great potential of proposed problem and approach. Codes and data are available at https://github.com/HaoyiZhu/XNeRF."
  },
  "wacv2023_main_attendwhoisweakpruning-assistedmedicalimagelocalizationundersophisticatedandimplicitimbalances": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Attend Who Is Weak: Pruning-Assisted Medical Image Localization Under Sophisticated and Implicit Imbalances",
    "authors": [
      "Ajay Jaiswal",
      "Tianlong Chen",
      "Justin F. Rousseau",
      "Yifan Peng",
      "Ying Ding",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jaiswal_Attend_Who_Is_Weak_Pruning-Assisted_Medical_Image_Localization_Under_Sophisticated_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jaiswal_Attend_Who_Is_Weak_Pruning-Assisted_Medical_Image_Localization_Under_Sophisticated_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep neural networks (DNNs) have rapidly become a de facto choice to medical image understanding tasks. However, DNNs are notoriously fragile to the class imbalance in image classification. We further point out that such imbalance fragility can be amplified when it comes to more sophisticated tasks such as pathology localization, as imbalances in such problems can have highly complex and often implicit forms of presence. For example, different pathology can have different sizes or colors (w.r.t.the background), different underlying demographic distributions, and in general different difficulty levels to recognize, even in a meticulously curated balanced distribution of training data. In this paper, we propose to use pruning to automatically and adaptively identify hard-to-learn (HTL) training samples, and improve pathology localization by attending them explicitly, during training in supervised, semi-supervised, and weakly-supervised settings. Our main inspiration is drawn from the recent finding that deep classification models have difficult-to-memorize samples and those may be effectively exposed through network pruning - and we extend such observation beyond classification for the first time. We also present interesting demographic analysis which illustrates HTLs ability to capture complex demographic imbalances. Our extensive experiments on the Skin Lesion Localization task in multiple training settings by paying additional attention to HTLs show significant improvement of localization performance by2-3%."
  },
  "wacv2023_main_dynamicneuralportraits": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dynamic Neural Portraits",
    "authors": [
      "Michail Christos Doukas",
      "Stylianos Ploumpis",
      "Stefanos Zafeiriou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Doukas_Dynamic_Neural_Portraits_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Doukas_Dynamic_Neural_Portraits_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present Dynamic Neural Portraits, a novel approach to the problem of full-head reenactment. Our method generates photo-realistic video portraits by explicitly controlling head pose, facial expressions and eye gaze. Our proposed architecture is different from existing methods that rely on GAN-based image-to-image translation networks for transforming renderings of 3D faces into photo-realistic images. Instead, we build our system upon a 2D coordinate-based MLP with controllable dynamics. Our intuition to adopt a 2D-based representation, as opposed to recent 3D NeRF-like systems, stems from the fact that video portraits are captured by monocular stationary cameras, therefore, only a single viewpoint of the scene is available. Primarily, we condition our generative model on expression blendshapes, nonetheless, we show that our system can be successfully driven by audio features as well. Our experiments demonstrate that the proposed method is 270 times faster than recent NeRF-based reenactment methods, with our networks achieving speeds of 24 fps for resolutions up to 1024x1024, while outperforming prior works in terms of visual quality."
  },
  "wacv2023_main_satscale-augmentedtransformerforpersonsearch": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SAT: Scale-Augmented Transformer for Person Search",
    "authors": [
      "Mustansar Fiaz",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Fiaz_SAT_Scale-Augmented_Transformer_for_Person_Search_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Fiaz_SAT_Scale-Augmented_Transformer_for_Person_Search_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Person search is a challenging computer vision problem where the objective is to simultaneously detect and reidentify a target person from the gallery of whole scene images captured from multiple cameras. Here, the challenges related to underlying detection and re-identification tasks need to be addressed along with a joint optimization of these two tasks. In this paper, we propose a three-stage cascaded Scale-Augmented Transformer (SAT) person search framework. In the three-stage design of our SAT framework, the first stage performs person detection whereas the last two stages performs both detection and re-identification. Considering the contradictory nature of detection and identification, in the last two stages, we introduce separate norm feature embeddings for the two tasks to reconcile the relationship between them in a joint person search model. Our SAT framework benefits from the attributes of convolutional neural networks and transformers by introducing a convolutional encoder and a scale modulator within each stage. Here, the convolutional encoder increases the generalization ability of the model whereas the scale modulator performs context aggregation at different granularity levels to aid in handling pose/scale variations within a region of interest. To further improve the performance during occlusion, we apply shifting augmentation operations at each granularity level within the scale modulator. Experimental results on challenging CUHK-SYSU [35] and PRW [47] datasets demonstrate the favorable performance of our method compared to state-of-the-art methods. Our source code and trained models are available at this https URL."
  },
  "wacv2023_main_self-supervisedlearningwithmaskedimagemodelingforteethnumbering,detectionofdentalrestorations,andinstancesegmentationindentalpanoramicradiographs": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Supervised Learning With Masked Image Modeling for Teeth Numbering, Detection of Dental Restorations, and Instance Segmentation in Dental Panoramic Radiographs",
    "authors": [
      "Amani Almalki",
      "Longin Jan Latecki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Almalki_Self-Supervised_Learning_With_Masked_Image_Modeling_for_Teeth_Numbering_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Almalki_Self-Supervised_Learning_With_Masked_Image_Modeling_for_Teeth_Numbering_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The computer-assisted radiologic informative report is currently emerging in dental practice to facilitate dental care and reduce time consumption in manual panoramic radiographic interpretation. However, the amount of dental radiographs for training is very limited, particularly from the point of view of deep learning. This study aims to utilize recent self-supervised learning methods like SimMIM and UM-MAE to increase the model efficiency and understanding of the limited number of dental radiographs. We use the Swin Transformer for teeth numbering, detection of dental restorations, and instance segmentation tasks. To the best of our knowledge, this is the first study that applied self-supervised learning methods to Swin Transformer on dental panoramic radiographs. Our results show that the SimMIM method obtained the highest performance of 90.4% and 88.9% on detecting teeth and dental restorations and instance segmentation, respectively, increasing the average precision by 13.4 and 12.8 over the random initialization baseline. Moreover, we augment and correct the existing dataset of panoramic radiographs. The code and the dataset are available at https://github.com/AmaniHAlmalki/DentalMIM."
  },
  "wacv2023_main_domaininvariantvisiontransformerlearningforfaceanti-spoofing": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Domain Invariant Vision Transformer Learning for Face Anti-Spoofing",
    "authors": [
      "Chen-Hao Liao",
      "Wen-Cheng Chen",
      "Hsuan-Tung Liu",
      "Yi-Ren Yeh",
      "Min-Chun Hu",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Liao_Domain_Invariant_Vision_Transformer_Learning_for_Face_Anti-Spoofing_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Liao_Domain_Invariant_Vision_Transformer_Learning_for_Face_Anti-Spoofing_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Existing face anti-spoofing (FAS) models have achieved high performance on specific datasets. However, for the application of real-world systems, the FAS model should generalize to the data from unknown domains rather than only achieve good results on a single baseline. As vision transformer models have demonstrated astonishing performance and strong capability in learning discriminative information, we investigate applying transformers to distinguish the face presentation attacks over unknown domains. In this work, we propose the Domain-invariant Vision Transformer (DiVT) for FAS, which adopts two losses to improve the generalizability of the vision transformer. First, a concentration loss is employed to learn a domain-invariant representation that aggregates the features of real face data. Second, a separation loss is utilized to union each type of attack from different domains. The experimental results show that our proposed method achieves state-of-the-art performance on the protocols of domain-generalized FAS tasks. Compared to previous domain generalization FAS models, our proposed method is simpler but more effective."
  },
  "wacv2023_main_cnn2graphbuildinggraphsforimageclassification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CNN2Graph: Building Graphs for Image Classification",
    "authors": [
      "Vivek Trivedy",
      "Longin Jan Latecki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Trivedy_CNN2Graph_Building_Graphs_for_Image_Classification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Trivedy_CNN2Graph_Building_Graphs_for_Image_Classification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Neural Network classifiers generally operate via the i.i.d. assumption where examples are passed through independently during training. We propose CNN2GNN and CNN2Transformer which instead leverage inter-example information for classification. We use Graph Neural Networks (GNNs) to build a latent space bipartite graph and compute cross-attention scores between input images and a proxy set. Our approach addresses several challenges of existing methods. Firstly, it is end-to-end differentiable despite the generally discrete nature of graph construction. Secondly, it allows inductive inference at no extra cost. Thirdly, it presents a simple method to construct graphs from arbitrary datasets that captures both example level and class level information. Finally, it addresses the proxy collapse problem by combining contrastive and cross-entropy losses rather than separate clustering algorithms. Our results increase classification performance over baseline experiments and outperform other methods. We also conduct an empirical investigation showing that Transformer style attention scales better than GAT attention with dataset size."
  },
  "wacv2023_main_automatedlinelabellingdatasetforcontourdetectionand3dreconstruction": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Automated Line Labelling: Dataset for Contour Detection and 3D Reconstruction",
    "authors": [
      "Hari Santhanam",
      "Nehal Doiphode",
      "Jianbo Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Santhanam_Automated_Line_Labelling_Dataset_for_Contour_Detection_and_3D_Reconstruction_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Santhanam_Automated_Line_Labelling_Dataset_for_Contour_Detection_and_3D_Reconstruction_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Understanding the finer details of a 3D object, its contours, is the first step toward a physical understanding of an object. Many real-world application domains require adaptable 3D object shape recognition models, usually with little training data. For this purpose, we develop the first automatically generated contour labeled dataset, bypassing manual human labeling. Using this dataset, we study the performance of current state-of-the-art instance segmentation algorithms on detecting and labeling the contours. We produce promising visual results with accurate contour prediction and labeling. We demonstrate that our finely labeled contours can help downstream tasks in computer vision, such as 3D reconstruction from a 2D image."
  },
  "wacv2023_main_himeefficientheadshotimagesuper-resolutionwithmultipleexemplars": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HIME: Efficient Headshot Image Super-Resolution With Multiple Exemplars",
    "authors": [
      "Xiaoyu Xiang",
      "Jon Morton",
      "Fitsum A. Reda",
      "Lucas D. Young",
      "Federico Perazzi",
      "Rakesh Ranjan",
      "Amit Kumar",
      "Andrea Colaco",
      "Jan P. Allebach"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Xiang_HIME_Efficient_Headshot_Image_Super-Resolution_With_Multiple_Exemplars_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Xiang_HIME_Efficient_Headshot_Image_Super-Resolution_With_Multiple_Exemplars_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "A promising direction for recovering the lost information in low-resolution headshot images is utilizing a set of high-resolution exemplars from the same identity. Complementary images in the reference set can improve the generated headshot quality across many different views and poses. However, it is challenging to make the best use of multiple exemplars: the quality and alignment of each exemplar cannot be guaranteed. Using low-quality and mismatched images as references will impair the output results. To overcome these issues, we propose the efficient Headshot Image Super-Resolution with Multiple Exemplars network (HIME) method. Compared with previous methods, our network can effectively handle the misalignment between the input and the reference without requiring facial priors and learn the aggregated reference set representation in an end-to-end manner. Furthermore, to reconstruct more detailed facial features, we propose a correlation loss that provides a rich representation of the local texture in a controllable spatial range. Experimental results demonstrate that the proposed framework not only has significantly fewer computation cost than recent exemplar-guided methods but also achieves better qualitative and quantitative performance."
  },
  "wacv2023_main_frequency-awareself-supervisedmonoculardepthestimation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Frequency-Aware Self-Supervised Monocular Depth Estimation",
    "authors": [
      "Xingyu Chen",
      "Thomas H. Li",
      "Ruonan Zhang",
      "Ge Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Frequency-Aware_Self-Supervised_Monocular_Depth_Estimation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Frequency-Aware_Self-Supervised_Monocular_Depth_Estimation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present two versatile methods to generally enhance self-supervised monocular depth estimation (MDE) models. The high generalizability of our methods is achieved by solving the fundamental and ubiquitous problems in photometric loss function. In particular, from the perspective of spatial frequency, we first propose Ambiguity-Masking to suppress the incorrect supervision under photometric loss at specific object boundaries, the cause of which could be traced to pixel-level ambiguity. Second, we present a novel frequency-adaptive Gaussian low-pass filter, designed to robustify the photometric loss in high-frequency regions. We are the first to propose blurring images to improve depth estimators with an interpretable analysis. Both modules are lightweight, adding no parameters and no need to manually change the network structures. Experiments show that our methods provide performance boosts to a large number of existing models, including those who claimed state-of-the-art, while introducing no extra inference computation at all."
  },
  "wacv2023_main_densepredictionwithattentivefeatureaggregation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dense Prediction With Attentive Feature Aggregation",
    "authors": [
      "Yung-Hsu Yang",
      "Thomas E. Huang",
      "Min Sun",
      "Samuel Rota Bul\u00f2",
      "Peter Kontschieder",
      "Fisher Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yang_Dense_Prediction_With_Attentive_Feature_Aggregation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yang_Dense_Prediction_With_Attentive_Feature_Aggregation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Aggregating information from features across different layers is essential for dense prediction models. Despite its limited expressiveness, vanilla feature concatenation dominates the choice of aggregation operations. In this paper, we introduce Attentive Feature Aggregation (AFA) to fuse different network layers with more expressive non-linear operations. AFA exploits both spatial and channel attention to compute weighted averages of the layer activations. Inspired by neural volume rendering, we further extend AFA with Scale-Space Rendering (SSR) to perform a late fusion of multi-scale predictions. AFA is applicable to a wide range of existing network designs. Our experiments show consistent and significant improvements on challenging semantic segmentation benchmarks, including Cityscapes and BDD100K at negligible computational and parameter overhead. In particular, AFA improves the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on Cityscapes. Our experimental analyses show that AFA learns to progressively refine segmentation maps and improve boundary details, leading to new state-of-the-art results on boundary detection benchmarks on NYUDv2 and BSDS500."
  },
  "wacv2023_main_interactinghand-objectposeestimationviadensemutualattention": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Interacting Hand-Object Pose Estimation via Dense Mutual Attention",
    "authors": [
      "Rong Wang",
      "Wei Mao",
      "Hongdong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "3D hand-object pose estimation is the key to the success of many computer vision applications. The main focus of this task is to effectively model the interaction between the hand and an object. To this end, existing works either rely on interaction constraints in a computationally-expensive iterative optimization, or consider only a sparse correlation between sampled hand and object keypoints. In contrast, we propose a novel dense mutual attention mechanism that is able to model fine-grained dependencies between the hand and the object. Specifically, we first construct the hand and object graphs according to their mesh structures. For each hand node, we aggregate features from every object node by the learned attention and vice versa for each object node. Thanks to such dense mutual attention, our method is able to produce physically plausible poses with high quality and real-time inference speed. Extensive quantitative and qualitative experiments on large benchmark datasets show that our method outperforms state-of-the-art methods. The code is available at https://github.com/rongakowang/DenseMutualAttention.git."
  },
  "wacv2023_main_detectionrecoveryinonlinemulti-objecttrackingwithsparsegraphtracker": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Detection Recovery in Online Multi-Object Tracking With Sparse Graph Tracker",
    "authors": [
      "Jeongseok Hyun",
      "Myunggu Kang",
      "Dongyoon Wee",
      "Dit-Yan Yeung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hyun_Detection_Recovery_in_Online_Multi-Object_Tracking_With_Sparse_Graph_Tracker_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hyun_Detection_Recovery_in_Online_Multi-Object_Tracking_With_Sparse_Graph_Tracker_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In existing joint detection and tracking methods, pairwise relational features are used to match previous tracklets to current detections. However, the features may not be discriminative enough for a tracker to identify a target from a large number of detections. Selecting only high-scored detections for tracking may lead to missed detections whose confidence score is low. Consequently, in the online setting, this results in disconnections of tracklets which cannot be recovered. In this regard, we present Sparse Graph Tracker (SGT), a novel online graph tracker using higher-order relational features which are more discriminative by aggregating the features of neighboring detections and their relations. SGT converts video data into a graph where detections, their connections, and the relational features of two connected nodes are represented by nodes, edges, and edge features, respectively. The strong edge features allow SGT to track targets with tracking candidates selected by top-K scored detections with large K. As a result, even low-scored detections can be tracked, and the missed detections are also recovered. The robustness of K value is shown through the extensive experiments. In the MOT16/17/20 and HiEve Challenge, SGT outperforms the state-of-the-art trackers with real-time inference speed. Especially, a large improvement in MOTA is shown in the MOT20 and HiEve Challenge. Code is available at https://github.com/HYUNJS/SGT."
  },
  "wacv2023_main_analysisofmasterveinattacksonfingerveinrecognitionsystems": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Analysis of Master Vein Attacks on Finger Vein Recognition Systems",
    "authors": [
      "Huy H. Nguyen",
      "Trung-Nghia Le",
      "Junichi Yamagishi",
      "Isao Echizen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nguyen_Analysis_of_Master_Vein_Attacks_on_Finger_Vein_Recognition_Systems_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nguyen_Analysis_of_Master_Vein_Attacks_on_Finger_Vein_Recognition_Systems_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Finger vein recognition (FVR) systems have been commercially used, especially in ATMs, for customer verification. Thus, it is essential to measure their robustness against various attack methods, especially when a hand-crafted FVR system is used without any countermeasure methods. In this paper, we are the first in the literature to introduce master vein attacks in which we craft a vein-looking image so that it can falsely match with as many identities as possible by the FVR systems. We present two methods for generating master veins for use in attacking these systems. The first uses an adaptation of the latent variable evolution algorithm with a proposed generative model (a multi-stage combination of beta-VAE and WGAN-GP models). The second uses an adversarial machine learning attack method to attack a strong surrogate CNN-based recognition system. The two methods can be easily combined to boost their attack ability. Experimental results demonstrated that the proposed methods alone and together achieved false acceptance rates up to 73.29% and 88.79%, respectively, against Miura's hand-crafted FVR system. We also point out that Miura's system is easily compromised by non-vein-looking samples generated by a WGAN-GP model with false acceptance rates up to 94.21%. The results raise the alarm about the robustness of such systems and"
  },
  "wacv2023_main_imagecompletionwithheterogeneouslyfilteredspectralhints": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Image Completion With Heterogeneously Filtered Spectral Hints",
    "authors": [
      "Xingqian Xu",
      "Shant Navasardyan",
      "Vahram Tadevosyan",
      "Andranik Sargsyan",
      "Yadong Mu",
      "Humphrey Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Xu_Image_Completion_With_Heterogeneously_Filtered_Spectral_Hints_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Xu_Image_Completion_With_Heterogeneously_Filtered_Spectral_Hints_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Image completion with large-scale free-form missing regions is one of the most challenging tasks for the computer vision community. While researchers pursue better solutions, drawbacks such as pattern unawareness, blurry textures, and structure distortion remain noticeable, and thus leave space for improvement. To overcome these challenges, we propose a new StyleGAN-based image completion network, Spectral Hint GAN (SH-GAN), inside which a carefully designed spectral processing module, Spectral Hint Unit, is introduced. We also propose two novel 2D spectral processing strategies, Heterogeneous Filtering, and Gaussian Split that well-fit modern deep learning models and may further be extended to other tasks. From our inclusive experiments, we demonstrate that our model can reach FID scores of 3.4134 and 7.0277 on the benchmark datasets FFHQ and Places2, and therefore outperforms prior works and reaches a new state-of-the-art. We also prove the effectiveness of our design via ablation studies, from which one may notice that the aforementioned challenges, i.e. pattern unawareness, blurry textures, and structure distortion, can be noticeably resolved. Our code will be open-sourced at: https://github.com/SHI-Labs/SH-GAN."
  },
  "wacv2023_main_splittolearngradientsplitformulti-taskhumanimageanalysis": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Split To Learn: Gradient Split for Multi-Task Human Image Analysis",
    "authors": [
      "Weijian Deng",
      "Yumin Suh",
      "Xiang Yu",
      "Masoud Faraki",
      "Liang Zheng",
      "Manmohan Chandraker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Deng_Split_To_Learn_Gradient_Split_for_Multi-Task_Human_Image_Analysis_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Deng_Split_To_Learn_Gradient_Split_for_Multi-Task_Human_Image_Analysis_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper presents an approach to train a unified deep network that simultaneously solves multiple human-related tasks. A multi-task framework is favorable for sharing information across tasks under restricted computational resources. However, tasks not only share information but may also compete for resources and conflict with each other, making the optimization of shared parameters difficult and leading to suboptimal performance. We propose a simple but effective training scheme called GradSplit that alleviates this issue by utilizing asymmetric inter-task relations. Specifically, at each convolution module, it splits features into T groups for T tasks and trains each group only using the gradient back-propagated from the task losses with which it does not have conflicts. During training, we apply GradSplit to a series of convolution modules. As a result, each module is trained to generate a set of task-specific features using the shared features from the previous module. This enables a network to use complementary information across tasks while circumventing gradient conflicts. Experimental results show that GradSplit achieves a better accuracy-efficiency trade-off than existing methods. It minimizes accuracy drop caused by task conflicts while significantly saving compute resources in terms of both FLOPs and memory at inference. We further show that GradSplit achieves higher cross-dataset accuracy compared to single-task and other multi-task networks."
  },
  "wacv2023_main_transpillarscoarse-to-fineaggregationformulti-frame3dobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "TransPillars: Coarse-To-Fine Aggregation for Multi-Frame 3D Object Detection",
    "authors": [
      "Zhipeng Luo",
      "Gongjie Zhang",
      "Changqing Zhou",
      "Tianrui Liu",
      "Shijian Lu",
      "Liang Pan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Luo_TransPillars_Coarse-To-Fine_Aggregation_for_Multi-Frame_3D_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Luo_TransPillars_Coarse-To-Fine_Aggregation_for_Multi-Frame_3D_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "3D object detection using point clouds has attracted increasing attention due to its wide applications in autonomous driving and robotics. However, most existing studies focus on single point cloud frames without harnessing the temporal information in point cloud sequences. In this paper, we design TransPillars, a novel transformer-based feature aggregation technique that exploits temporal features of consecutive point cloud frames for multi-frame 3D object detection. TransPillars aggregates spatial-temporal point cloud features from two perspectives. First, it fuses voxel-level features directly from multi-frame feature maps instead of pooled instance features to preserve instance details with contextual information that are essential to accurate object localization. Second, it introduces a hierarchical coarse-to-fine strategy to fuse multi-scale features progressively to effectively capture the motion of moving objects and guide the aggregation of fine features. Besides, a variant of deformable transformer is introduced to improve the effectiveness of cross-frame feature matching. Extensive experiments show that our proposed TransPillars achieves state-of-art performance as compared to existing multi-frame detection approaches."
  },
  "wacv2023_main_bodypart-basedrepresentationlearningforoccludedpersonre-identification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Body Part-Based Representation Learning for Occluded Person Re-Identification",
    "authors": [
      "Vladimir Somers",
      "Christophe De Vleeschouwer",
      "Alexandre Alahi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Somers_Body_Part-Based_Representation_Learning_for_Occluded_Person_Re-Identification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Somers_Body_Part-Based_Representation_Learning_for_Occluded_Person_Re-Identification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Occluded person re-identification (ReID) is a person retrieval task which aims at matching occluded person images with holistic ones. For addressing occluded ReID, part-based methods have been shown beneficial as they offer fine-grained information and are well suited to represent partially visible human bodies. However, training a part-based model is a challenging task for two reasons. Firstly, individual body part appearance is not as discriminative as global appearance (two distinct IDs might have the same local appearance), this means standard ReID training objectives using identity labels are not adapted to local feature learning. Secondly, ReID datasets are not provided with human topographical annotations. In this work, we propose BPBreID, a body part-based ReID model for solving the above issues. We first design two modules for predicting body part attention maps and producing body part-based features of the ReID target. We then propose GiLt, a novel training scheme for learning part-based representations that is robust to occlusions and non-discriminative local appearance. Extensive experiments on popular holistic and occluded datasets show the effectiveness of our proposed method, which outperforms state-of-the-art methods by 0.7% mAP and 5.6% rank-1 accuracy on the challenging Occluded-Duke dataset. Our code is available at https://github.com/VlSomers/bpbreid."
  },
  "wacv2023_main_generativerangeimagingforlearningscenepriorsof3dlidardata": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data",
    "authors": [
      "Kazuto Nakashima",
      "Yumi Iwashita",
      "Ryo Kurazume"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Nakashima_Generative_Range_Imaging_for_Learning_Scene_Priors_of_3D_LiDAR_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Nakashima_Generative_Range_Imaging_for_Learning_Scene_Priors_of_3D_LiDAR_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "3D LiDAR sensors are indispensable for the robust vision of autonomous mobile robots. However, deploying LiDAR-based perception algorithms often fails due to a domain gap from the training environment, such as inconsistent angular resolution and missing properties. Existing studies have tackled the issue by learning inter-domain mapping, while the transferability is constrained by the training configuration and the training is susceptible to peculiar lossy noises called ray-drop. To address the issue, this paper proposes a generative model of LiDAR range images applicable to the data-level domain transfer. Motivated by the fact that LiDAR measurement is based on point-by-point range imaging, we train an implicit image representation-based generative adversarial networks along with a differentiable ray-drop effect. We demonstrate the fidelity and diversity of our model in comparison with the point-based and image-based state-of-the-art generative models. We also showcase upsampling and restoration applications. Furthermore, we introduce a Sim2Real application for LiDAR semantic segmentation. We demonstrate that our method is effective as a realistic ray-drop simulator and outperforms state-of-the-art methods."
  },
  "wacv2023_main_placinghumananimationsinto3dscenesbylearninginteraction-andgeometry-drivenkeyframes": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Placing Human Animations Into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes",
    "authors": [
      "James F. Mullen",
      "Divya Kothandaraman",
      "Aniket Bera",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call \"keyframes.\" These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances of the scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5% compared to POSA. Our project website is available at https://gamma.umd.edu/paak/."
  },
  "wacv2023_main_rebalancinggradienttoimproveself-supervisedco-trainingofdepth,odometryandopticalflowpredictions": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Rebalancing Gradient To Improve Self-Supervised Co-Training of Depth, Odometry and Optical Flow Predictions",
    "authors": [
      "Marwane Hariat",
      "Antoine Manzanera",
      "David Filliat"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hariat_Rebalancing_Gradient_To_Improve_Self-Supervised_Co-Training_of_Depth_Odometry_and_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hariat_Rebalancing_Gradient_To_Improve_Self-Supervised_Co-Training_of_Depth_Odometry_and_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present CoopNet, an approach that improves the cooperation of co-trained networks by dynamically adapting the apportionment of gradient, to ensure equitable learning progress. It is applied to motion-aware self-supervised prediction of depth maps, by introducing a new hybrid loss, based on a distribution model of photo-metric reconstruction errors made by, on the one hand the depth + odometry paired networks, and on the other hand the optical flow network. This model essentially assumes that the pixels from moving objects (that must be discarded for training depth and odometry), correspond to those where the two reconstructions strongly disagree. We justify this model by theoretical considerations and experimental evidences, and show that its implementation improves or is comparable to the state of the art in depth, odometry and optical flow predictions. Our code is available here: https://github.com/mhariat/CoopNet."
  },
  "wacv2023_main_radiantbetterrppgestimationusingsignalembeddingsandtransformer": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "RADIANT: Better rPPG Estimation Using Signal Embeddings and Transformer",
    "authors": [
      "Anup Kumar Gupta",
      "Rupesh Kumar",
      "Lokendra Birla",
      "Puneet Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Gupta_RADIANT_Better_rPPG_Estimation_Using_Signal_Embeddings_and_Transformer_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Gupta_RADIANT_Better_rPPG_Estimation_Using_Signal_Embeddings_and_Transformer_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Remote photoplethysmography can provide non-contact heart rate (HR) estimation by analyzing the skin color variations obtained from face videos. These variations are subtle, imperceptible to human eyes, and easily affected by noise. Existing deep learning-based rPPG estimators are incompetent due to three reasons. Firstly, they suppress the noise by utilizing information from the whole face even though different facial regions contain different noise characteristics. Secondly, local noise characteristics inherently affect the convolutional neural network (CNN) architectures. Lastly, the CNN sequential architectures fail to preserve long temporal dependencies. To address these issues, we propose RADIANT, that is, rPPG estimation using Signal Embeddings and Transformer. Our Transformer utilizes a multi-head attention mechanism that facilitates the feature subspace learning to extract the multiple correlations among the color variations corresponding to the periodic pulse. Also, its global information processing ability helps to suppress local noise characteristics. Apart from Transformer, we propose novel signal embedding to enhance the rPPG feature representation and suppress noise. We have also improved the generalization of our architecture by adding a new training set. To this end, the effectiveness of synthetic temporal signals and data augmentations were explored. Experiments on extensively utilized UBFC-rPPG and COHFACE datasets demonstrate that our architecture outperforms previous well-known architectures. The implementation will be made publicly available upon paper acceptance."
  },
  "wacv2023_main_marker-removalnetworkstocollectprecise3dhanddataforrgb-basedestimationanditsapplicationinpiano": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Marker-Removal Networks To Collect Precise 3D Hand Data for RGB-Based Estimation and Its Application in Piano",
    "authors": [
      "Erwin Wu",
      "Hayato Nishioka",
      "Shinichi Furuya",
      "Hideki Koike"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wu_Marker-Removal_Networks_To_Collect_Precise_3D_Hand_Data_for_RGB-Based_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wu_Marker-Removal_Networks_To_Collect_Precise_3D_Hand_Data_for_RGB-Based_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Hand pose analysis is a key step to understanding dexterous hand performances of many high-level skills, such as playing the piano. Currently, most accurate hand tracking systems are using fabric-/marker-based sensing that potentially disturbs users' performance. On the other hand, markerless computer vision-based methods rely on a precise bare-hand dataset for training, which is difficult to obtain. In this paper, we collect a large-scale high precision 3D hand pose dataset with a small workload using a novel marker-removal network (MR-Net). The proposed MR-Net translates the marked-hand images to realistic bare-hand images, and the corresponding 3D postures are captured by a motion capture system thus few manual annotations are required. A baseline estimation network PiaNet is introduced and we report the accuracy of various metrics together with a blind qualitative test to show the practical effect."
  },
  "wacv2023_main_cross-identityvideomotionretargetingwithjointtransformationandsynthesis": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Cross-Identity Video Motion Retargeting With Joint Transformation and Synthesis",
    "authors": [
      "Haomiao Ni",
      "Yihao Liu",
      "Sharon X. Huang",
      "Yuan Xue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ni_Cross-Identity_Video_Motion_Retargeting_With_Joint_Transformation_and_Synthesis_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ni_Cross-Identity_Video_Motion_Retargeting_With_Joint_Transformation_and_Synthesis_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we propose a novel dual-branch Transformation-Synthesis network (TS-Net), for video motion retargeting. Given one subject video and one driving video, TS-Net can produce a new plausible video with the subject appearance of the subject video and motion pattern of the driving video. TS-Net consists of a warp-based transformation branch and a warp-free synthesis branch. The novel design of dual branches combines the strengths of deformation-grid-based transformation and warp-free generation for better identity preservation and robustness to occlusion in the synthesized videos. A mask-aware similarity module is further introduced to the transformation branch to reduce computational overhead. Experimental results on face and dance datasets show that TS-Net achieves better performance in video motion retargeting than several state-of-the-art models as well as its single-branch variants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet."
  },
  "wacv2023_main_learningacrossdomainsanddevicesstyle-drivensource-freedomainadaptationinclusteredfederatedlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning",
    "authors": [
      "Donald Shenaj",
      "Eros Fan\u00ec",
      "Marco Toldo",
      "Debora Caldarola",
      "Antonio Tavera",
      "Umberto Michieli",
      "Marco Ciccone",
      "Pietro Zanuttigh",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Shenaj_Learning_Across_Domains_and_Devices_Style-Driven_Source-Free_Domain_Adaptation_in_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Shenaj_Learning_Across_Domains_and_Devices_Style-Driven_Source-Free_Domain_Adaptation_in_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Federated Learning (FL) has recently emerged as a possible way to tackle the domain shift in real-world Semantic Segmentation (SS) without compromising the private nature of the collected data. However, most of the existing works on FL unrealistically assume labeled data in the remote clients. Here we propose a novel task (FFREEDA) in which the clients' data is unlabeled and the server accesses a source labeled dataset for pre-training only. To solve FFREEDA, we propose LADD, which leverages the knowledge of the pre-trained model by employing self-supervision with ad-hoc regularization techniques for local training and introducing a novel federated clustered aggregation scheme based on the clients' style. Our experiments show that our algorithm is able to efficiently tackle the new task outperforming existing approaches. The code is available at https://github.com/Erosinho13/LADD."
  },
  "wacv2023_main_learningattentionpropagationforcompositionalzero-shotlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Learning Attention Propagation for Compositional Zero-Shot Learning",
    "authors": [
      "Muhammad Gul Zain Ali Khan",
      "Muhammad Ferjad Naeem",
      "Luc Van Gool",
      "Alain Pagani",
      "Didier Stricker",
      "Muhammad Zeshan Afzal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Khan_Learning_Attention_Propagation_for_Compositional_Zero-Shot_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Khan_Learning_Attention_Propagation_for_Compositional_Zero-Shot_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Compositional zero-shot learning aims to recognize unseen compositions of seen visual primitives of object classes and their states. While all primitives (states and objects) are observable during training in some combination, their complex interaction makes this task especially hard. For example, wet changes the visual appearance of a dog very differently from a bicycle. Furthermore, we argue that relationships between compositions go beyond shared states or objects. A cluttered office can contain a busy table; even though these compositions don't share a state or object, the presence of a busy table can guide the presence of a cluttered office. We propose a novel method called Compositional Attention Propagated Embedding (CAPE) as a solution. The key intuition to our method is that a rich dependency structure exists between compositions arising from complex interactions of primitives in addition to other dependencies between compositions. CAPE learns to identify this structure and propagates knowledge between them to learn class embedding for all seen and unseen compositions. In the challenging generalized compositional zero-shot setting, we show that our method outperforms previous baselines to set a new state-of-the-art on three publicly available benchmarks."
  },
  "wacv2023_main_language-freetrainingforzero-shotvideogrounding": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Language-Free Training for Zero-Shot Video Grounding",
    "authors": [
      "Dahye Kim",
      "Jungin Park",
      "Jiyoung Lee",
      "Seongheon Park",
      "Kwanghoon Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kim_Language-Free_Training_for_Zero-Shot_Video_Grounding_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kim_Language-Free_Training_for_Zero-Shot_Video_Grounding_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Given an untrimmed video and a language query depicting a specific temporal moment in the video, video grounding aims to localize the time interval by understanding the text and video simultaneously. One of the most challenging issues is an extremely time- and cost-consuming annotation collection, including video captions in a natural language form and their corresponding temporal regions. In this paper, we present a simple yet novel training framework for video grounding in the zero-shot setting, which learns a network with only video data without any annotation. Inspired by the recent language-free paradigm, i.e. training without language data, we train the network without compelling the generation of fake (pseudo) text queries into a natural language form. Specifically, we propose a method for learning a video grounding model by selecting a temporal interval as a hypothetical correct answer and considering the visual feature selected by our method in the interval as a language feature, with the help of the well-aligned visual-language space of CLIP. Extensive experiments demonstrate the prominence of our language-free training framework, outperforming the existing zero-shot video grounding method and even several weakly-supervised approaches with large margins on two standard datasets."
  },
  "wacv2023_main_weakly-supervisedpointcloudinstancesegmentationwithgeometricpriors": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Weakly-Supervised Point Cloud Instance Segmentation With Geometric Priors",
    "authors": [
      "Heming Du",
      "Xin Yu",
      "Farookh Hussain",
      "Mohammad Ali Armin",
      "Lars Petersson",
      "Weihao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Du_Weakly-Supervised_Point_Cloud_Instance_Segmentation_With_Geometric_Priors_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Du_Weakly-Supervised_Point_Cloud_Instance_Segmentation_With_Geometric_Priors_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper investigates how to leverage more readily acquired annotations, i.e., 3D bounding boxes instead of dense point-wise labels, for instance segmentation. We propose a Weakly-supervised point cloud Instance Segmentation framework with Geometric Priors (WISGP) that allows segmentation models to be trained with 3D bounding boxes of instances. Considering intersections among bounding boxes in a scene would result in ambiguous labels, we first group points into two sets, i.e., univocal and equivocal sets, indicating the certainty of a 3D point belonging to an instance, respectively. Specifically, 3D points with clear labels belong to the univocal set while the rest are grouped into the equivocal set. To assign reliable labels to points in the equivocal set, we design a Geometry-guided Label Propagation (GLP) scheme that progressively propagates labels to linked points based on geometric structure, e.g., polygon meshes and superpoints. Afterwards, we train an instance segmentation model with the univocal points and equivocal points labeled by GLP, and then employ it to assign pseudo labels for the remainder of the unlabeled points. Lastly, we retrain the model with all the labeled points to achieve better instance segmentation performance. Experiments on large-scale datasets ScanNet-v2 and S3DIS demonstrate that WISGP is superior to competing weakly-supervised algorithms and even on par with a few fully-supervised ones."
  },
  "wacv2023_main_federateddomaingeneralizationforimagerecognitionviacross-clientstyletransfer": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Federated Domain Generalization for Image Recognition via Cross-Client Style Transfer",
    "authors": [
      "Junming Chen",
      "Meirui Jiang",
      "Qi Dou",
      "Qifeng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Federated_Domain_Generalization_for_Image_Recognition_via_Cross-Client_Style_Transfer_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Federated_Domain_Generalization_for_Image_Recognition_via_Cross-Client_Style_Transfer_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Domain generalization (DG) has been a hot topic in image recognition, with a goal to train a general model that can perform well on unseen domains. Recently, federated learning (FL), an emerging machine learning paradigm to train a global model from multiple decentralized clients without compromising data privacy, brings new challenges, also new possibilities, to DG. In the FL scenario, many existing state-of-the-art (SOTA) DG methods become ineffective, because they require the centralization of data from different domains during training. In this paper, we propose a novel domain generalization method for image recognition under federated learning through cross-client style transfer (CCST) without exchanging data samples. Our CCST method can lead to more uniform distributions of source clients, and thus make each local model learn to fit the image styles of all the clients to avoid the different model biases. Two types of style (single image style and overall domain style) with corresponding mechanisms are proposed to be chosen according to different scenarios. Our style representation is exceptionally lightweight and can hardly be used for the reconstruction of the dataset. The level of diversity is also flexible to be controlled with a hyper-parameter. Our method outperforms recent SOTA DG methods on two DG benchmarks (PACS, OfficeHome) and a large-scale medical image dataset (Camelyon17) in the FL setting. Last but not least, our method is orthogonal to many classic DG methods, achieving additive performance by combined utilization."
  },
  "wacv2023_main_bevsegformerbirdseyeviewsemanticsegmentationfromarbitrarycamerarigs": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs",
    "authors": [
      "Lang Peng",
      "Zhirong Chen",
      "Zhangjie Fu",
      "Pengpeng Liang",
      "Erkang Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Peng_BEVSegFormer_Birds_Eye_View_Semantic_Segmentation_From_Arbitrary_Camera_Rigs_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Peng_BEVSegFormer_Birds_Eye_View_Semantic_Segmentation_From_Arbitrary_Camera_Rigs_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Semantic segmentation in bird's eye view (BEV) is an important task for autonomous driving. Though this task has attracted a large amount of research efforts, it is still challenging to flexibly cope with arbitrary (single or multiple) camera sensors equipped on the autonomous vehicle. In this paper, we present BEVSegFormer, an effective transformer-based method for BEV semantic segmentation from arbitrary camera rigs. Specifically, our method first encodes image features from arbitrary cameras with a shared backbone. These image features are then enhanced by a deformable transformer-based encoder. Moreover, we introduce a BEV transformer decoder module to parse BEV semantic segmentation results. An efficient multi-camera deformable attention unit is designed to carry out the BEV-to-image view transformation. Finally, the queries are reshaped according the layout of grids in the BEV, and upsampled to produce the semantic segmentation result in a supervised manner. We evaluate the proposed algorithm on the public nuScenes dataset and a self-collected dataset. Experimental results show that our method achieves promising performance on BEV semantic segmentation from arbitrary camera rigs. We also demonstrate the effectiveness of each component via ablation study."
  },
  "wacv2023_main_self-supervised2d/3dregistrationforx-raytoctimagefusion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Supervised 2D/3D Registration for X-Ray to CT Image Fusion",
    "authors": [
      "Srikrishna Jaganathan",
      "Maximilian Kukla",
      "Jian Wang",
      "Karthik Shetty",
      "Andreas Maier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jaganathan_Self-Supervised_2D3D_Registration_for_X-Ray_to_CT_Image_Fusion_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jaganathan_Self-Supervised_2D3D_Registration_for_X-Ray_to_CT_Image_Fusion_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep Learning-based 2D/3D registration enables fast, robust, and accurate X-ray to CT image fusion when large annotated paired datasets are available for training. However, the need for paired CT volume and X-ray images with ground truth registration limits the applicability in interventional scenarios. An alternative is to use simulated X-ray projections from CT volumes, thus removing the need for paired annotated datasets. Deep Neural Networks trained exclusively on simulated X-ray projections can perform significantly worse on real X-ray images due to the domain gap. We propose a self-supervised 2D/3D registration framework combining simulated training with unsupervised feature and pixel space domain adaptation to overcome the domain gap and eliminate the need for paired annotated datasets. Our framework achieves a registration accuracy of 1.83 +-1.16 mm with a high success ratio of 90.1% on real X-ray images showing a 23.9% increase in success ratio compared to reference annotation-free algorithms."
  },
  "wacv2023_main_trans4maprevisitingholisticbirds-eye-viewmappingfromegocentricimagestoallocentricsemanticswithvisiontransformers": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Trans4Map: Revisiting Holistic Bird's-Eye-View Mapping From Egocentric Images to Allocentric Semantics With Vision Transformers",
    "authors": [
      "Chang Chen",
      "Jiaming Zhang",
      "Kailun Yang",
      "Kunyu Peng",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Trans4Map_Revisiting_Holistic_Birds-Eye-View_Mapping_From_Egocentric_Images_to_Allocentric_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Trans4Map_Revisiting_Holistic_Birds-Eye-View_Mapping_From_Egocentric_Images_to_Allocentric_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Humans have an innate ability to sense their surroundings, as they can extract the spatial representation from the egocentric perception and form an allocentric semantic map via spatial transformation and memory updating. However, endowing mobile agents with such a spatial sensing ability is still a challenge, due to two difficulties: (1) the previous convolutional models are limited by the local receptive field, thus, struggling to capture holistic long-range dependencies during observation; (2) the excessive computational budgets required for success, often lead to a separation of the mapping pipeline into stages, resulting the entire mapping process inefficient. To address these issues, we propose an end-to-end one-stage Transformer-based framework for Mapping, termed Trans4Map. Our egocentric-to-allocentric mapping process includes three steps: (1) the efficient transformer extracts the contextual features from a batch of egocentric images; (2) the proposed Bidirectional Allocentric Memory (BAM) module projects egocentric features into the allocentric memory; (3) the map decoder parses the accumulated memory and predicts the top-down semantic segmentation map. In contrast, Trans4Map achieves state-of-the-art results, reducing 67.2% parameters, yet gaining a +3.25% mIoU and a +4.09% mBF1 improvements on the Matterport3D dataset."
  },
  "wacv2023_main_densevoxelfusionfor3dobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dense Voxel Fusion for 3D Object Detection",
    "authors": [
      "Anas Mahmoud",
      "Jordan S. K. Hu",
      "Steven L. Waslander"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mahmoud_Dense_Voxel_Fusion_for_3D_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mahmoud_Dense_Voxel_Fusion_for_3D_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Camera and LiDAR sensor modalities provide complementary appearance and geometric information useful for detecting 3D objects for autonomous vehicle applications. However, current end-to-end fusion methods are challenging to train and underperform state-of-the-art LiDAR-only detectors. Sequential fusion methods suffer from a limited number of pixel and point correspondences due to point cloud sparsity, or their performance is strictly capped by the detections of one of the modalities. Our proposed solution, Dense Voxel Fusion (DVF) is a sequential fusion method that generates multi-scale dense voxel feature representations, improving expressiveness in low point density regions. To enhance multi-modal learning, we train directly with projected ground truth 3D bounding box labels, avoiding noisy, detector-specific 2D predictions. Both DVF and the multi-modal training approach can be applied to any voxel-based LiDAR backbone. DVF ranks 3rd among published fusion methods on KITTI's 3D car detection benchmark without introducing additional trainable parameters, nor requiring stereo images or dense depth labels. In addition, DVF significantly improves 3D vehicle detection performance of voxel-based methods on the Waymo Open Dataset."
  },
  "wacv2023_main_effectiveinvertiblearbitraryimagerescaling": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Effective Invertible Arbitrary Image Rescaling",
    "authors": [
      "Zhihong Pan",
      "Baopu Li",
      "Dongliang He",
      "Wenhao Wu",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Pan_Effective_Invertible_Arbitrary_Image_Rescaling_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Pan_Effective_Invertible_Arbitrary_Image_Rescaling_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Great successes have been achieved using deep learning techniques for image super-resolution (SR) with fixed scales. To increase its real world applicability, numerous models have also been proposed to restore SR images with arbitrary scale factors, including asymmetric ones where images are resized to different scales along horizontal and vertical directions. Though most models are only optimized for the unidirectional upscaling task while assuming a predefined downscaling kernel for low-resolution (LR) inputs, recent models based on Invertible Neural Networks (INN) are able to increase upscaling accuracy significantly by optimizing the downscaling and upscaling cycle jointly. However, limited by the INN architecture, it is constrained to fixed integer scale factors and requires one model for each scale. Without increasing model complexity, a simple and effective invertible arbitrary rescaling network (IARN) is proposed to achieve arbitrary image rescaling by training only one model in this work. Using innovative components like position-aware scale encoding and preemptive channel splitting, the network is optimized to convert the non-invertible rescaling cycle to an effectively invertible process. It is shown to achieve a state-of-the-art (SOTA) performance in bidirectional arbitrary rescaling without compromising perceptual quality in LR outputs. It is also demonstrated to perform well on tests with asymmetric scales using the same network architecture."
  },
  "wacv2023_main_facedancerpose-andocclusion-awarehighfidelityfaceswapping": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping",
    "authors": [
      "Felix Rosberg",
      "Eren Erdal Aksoy",
      "Fernando Alonso-Fernandez",
      "Cristofer Englund"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Rosberg_FaceDancer_Pose-_and_Occlusion-Aware_High_Fidelity_Face_Swapping_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Rosberg_FaceDancer_Pose-_and_Occlusion-Aware_High_Fidelity_Face_Swapping_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this work, we present a new single-stage method for subject agnostic face swapping and identity transfer, named FaceDancer. We have two major contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature Similarity Regularization (IFSR). The AFFA module is embedded in the decoder and adaptively learns to fuse attribute features and features conditioned on identity information without requiring any additional facial segmentation process. In IFSR, we leverage the intermediate features in an identity encoder to preserve important attributes such as head pose, facial expression, lighting, and occlusion in the target face, while still transferring the identity of the source face with high fidelity. We conduct extensive quantitative and qualitative experiments on various datasets and show that the proposed FaceDancer outperforms other state-of-the-art networks in terms of identity transfer, while having significantly better pose preservation than most of the previous methods."
  },
  "wacv2023_main_few-shotmedicalimagesegmentationwithcycle-resemblanceattention": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Few-Shot Medical Image Segmentation With Cycle-Resemblance Attention",
    "authors": [
      "Hao Ding",
      "Changchang Sun",
      "Hao Tang",
      "Dawen Cai",
      "Yan Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ding_Few-Shot_Medical_Image_Segmentation_With_Cycle-Resemblance_Attention_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ding_Few-Shot_Medical_Image_Segmentation_With_Cycle-Resemblance_Attention_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recently, due to the increasing requirements of medical imaging applications and the professional requirements of annotating medical images, few-shot learning has gained increasing attention in the medical image semantic segmentation field. To perform segmentation with limited number of labeled medical images, most existing studies use Prototypical Networks (PN) and have obtained compelling success. However, these approaches overlook the query image features extracted from the proposed representation network, failing to preserving the spatial connection between query and support images. In this paper, we propose a novel self-supervised few-shot medical image segmentation network and introduce a novel Cycle-Resemblance Attention (CRA) module to fully leverage the pixel-wise relation between query and support medical images. Notably, we first line up multiple attention blocks to refine more abundant relation information. Then, we present CRAPNet by integrating the CRA module with a classic prototype network, where pixel-wise relations between query and support features are well recaptured for segmentation. Extensive experiments on two different medical image datasets, e.g., abdomen MRI and abdomen CT, demonstrate the superiority of our model over existing state-of-the-art methods."
  },
  "wacv2023_main_datasetcondensationwithdistributionmatching": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dataset Condensation With Distribution Matching",
    "authors": [
      "Bo Zhao",
      "Hakan Bilen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhao_Dataset_Condensation_With_Distribution_Matching_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhao_Dataset_Condensation_With_Distribution_Matching_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and second-order derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost. We also show promising practical benefits of our method in continual learning and neural architecture search."
  },
  "wacv2023_main_canshadowsrevealbiometricinformation?": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Can Shadows Reveal Biometric Information?",
    "authors": [
      "Safa C. Medin",
      "Amir Weiss",
      "Fr\u00e9do Durand",
      "William T. Freeman",
      "Gregory W. Wornell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Medin_Can_Shadows_Reveal_Biometric_Information_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Medin_Can_Shadows_Reveal_Biometric_Information_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We study the problem of extracting biometric information of individuals by looking at shadows of objects cast on diffuse surfaces. We show that the biometric information leakage from shadows can be sufficient for reliable identity inference under representative scenarios via a maximum likelihood analysis. We then develop a learning-based method that demonstrates this phenomenon in real settings, exploiting the subtle cues in the shadows that are the source of the leakage without requiring any labeled real data. In particular, our approach relies on building synthetic scenes composed of 3D face models obtained from a single photograph of each identity. We transfer what we learn from the synthetic data to the real data using domain adaptation in a completely unsupervised way. Our model is able to generalize well to the real domain and is robust to several variations in the scenes. We report high classification accuracies in an identity classification task that takes place in a scene with unknown geometry and occluding objects."
  },
  "wacv2023_main_neuralweightsearchforscalabletaskincrementallearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Neural Weight Search for Scalable Task Incremental Learning",
    "authors": [
      "Jian Jiang",
      "Oya Celiktutan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jiang_Neural_Weight_Search_for_Scalable_Task_Incremental_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jiang_Neural_Weight_Search_for_Scalable_Task_Incremental_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Task incremental learning aims to enable a system to maintain its performance on previously learned tasks while learning new tasks, solving the problem of catastrophic forgetting. One promising approach is to build an individual network or sub-network for future tasks. However, this leads to an ever-growing memory due to saving extra weights for new tasks and how to address this issue has remained an open problem in task incremental learning. In this paper, we introduce a novel Neural Weight Search technique that designs a fixed search space where the optimal combinations of frozen weights can be searched to build new models for novel tasks in an end-to-end manner, resulting in a scalable and controllable memory growth. Extensive experiments on two benchmarks, i.e., Split-CIFAR-100 and CUB-to-Sketches, show our method achieves state-of-the-art performance with respect to both average inference accuracy and total memory cost."
  },
  "wacv2023_main_reenfpdetail-preservingfacereconstructionbyencodingfacialpriors": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ReEnFP: Detail-Preserving Face Reconstruction by Encoding Facial Priors",
    "authors": [
      "Yasheng Sun",
      "Jiangke Lin",
      "Hang Zhou",
      "Zhiliang Xu",
      "Dongliang He",
      "Hideki Koike"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sun_ReEnFP_Detail-Preserving_Face_Reconstruction_by_Encoding_Facial_Priors_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sun_ReEnFP_Detail-Preserving_Face_Reconstruction_by_Encoding_Facial_Priors_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We address the problem of face modeling, which is still challenging in achieving high-quality reconstruction results efficiently. Neither previous regression-based nor optimization-based frameworks could well balance between the facial reconstruction fidelity and efficiency. We notice that the large amount of in-the-wild facial images contain diverse appearance information, however, their underlying knowledge is not fully exploited for face modeling. To this end, we propose our Reconstruction by Encoding Facial Priors (ReEnFP) pipeline to exploit the potential of unconstrained facial images for further improvement. Our key is to encode generative priors learned by a style-based texture generator on unconstrained data for fast and detail-preserving face reconstruction. With our texture generator pre-trained using a differentiable renderer, faces could be encoded to its latent space as opposed to the time-consuming optimization-based inversion. Our generative prior encoding is further enhanced with a pyramid fusion block for adaptive integration of input spatial information. Extensive experiments show that our method reconstructs photo-realistic facial textures and geometric details with precise identity recovery."
  },
  "wacv2023_main_certifieddefenseforcontentbasedimageretrieval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Certified Defense for Content Based Image Retrieval",
    "authors": [
      "Kazuya Kakizaki",
      "Kazuto Fukuchi",
      "Jun Sakuma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kakizaki_Certified_Defense_for_Content_Based_Image_Retrieval_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kakizaki_Certified_Defense_for_Content_Based_Image_Retrieval_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper develops a certified defense for deep neural network (DNN) based content based image retrieval (CBIR) against adversarial examples (AXs). Previous works put their effort into certified defense for classification to improve certified robustness, which guarantees that no AX to cause misclassification exists around the sample. Such certified defense, however, could not be applied to CBIR directly because the goals of adversarial attack against classification and CBIR are completely different. To develop the certified defense for CBIR, we first define new certified robustness of CBIR, which guarantees that no AX that changes the ranking of CBIR exists around the query or candidate images. Then, we propose computationally tractable verification algorithms that verify whether the certified robustness of CBIR is achieved by utilizing upper and lower bounds of distances between feature representations of perturbed and non-perturbed images. Finally, we propose new objective functions for training feature extraction DNNs that increases the number of inputs that satisfy the certified robustness of CBIR by tightening the upper and lower bounds. Experimental results show that our objective functions significantly improve the certified robustness of CBIR than existing methods."
  },
  "wacv2023_main_etranefficienttransformerforre-rankinginvisualplacerecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ETR: An Efficient Transformer for Re-Ranking in Visual Place Recognition",
    "authors": [
      "Hao Zhang",
      "Xin Chen",
      "Heming Jing",
      "Yingbin Zheng",
      "Yuan Wu",
      "Cheng Jin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_ETR_An_Efficient_Transformer_for_Re-Ranking_in_Visual_Place_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_ETR_An_Efficient_Transformer_for_Re-Ranking_in_Visual_Place_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Visual place recognition is to estimate the geographical location of a given image, which is usually addressed by recognizing its similar reference images from a database. The reference images are usually retrieved via similarity search using global descriptor, and the local descriptors are used to re-rank the initial retrieved candidates. The local descriptors re-ranking can significantly improve the accuracy of global retrieval but comes at a high computational cost. To achieve a good trade-off between accuracy and efficiency, we propose an Efficient Transformer for Re-ranking (ETR), utilizing both global and local descriptors to re-rank the top candidates in a single shot. In contrast to traditional re-ranking methods, we leverage self-attention to capture relationships between local descriptors in a single image and cross-attention to explore the similarity of the image pairs. We show that the proposed model can be regarded as a general re-ranking algorithm for significantly boosting the performance of other global-only retrieval methods. Extensive experimental results show that our method outperforms state-of-the-arts and is orders of magnitude faster in terms of computational efficiency."
  },
  "wacv2023_main_mt-detrrobustend-to-endmultimodaldetectionwithconfidencefusion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MT-DETR: Robust End-to-End Multimodal Detection With Confidence Fusion",
    "authors": [
      "Shih-Yun Chu",
      "Ming-Sui Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Due to the trending need for autonomous driving, camera-based object detection has recently attracted lots of attention and successful development. However, there are times when unexpected and severe weather occurs in outdoor environments, making the detection tasks less effective and unexpected. In this case, additional sensors like lidar and radar are adopted to help the camera work in bad weather. However, existing multimodal detection methods do not consider the characteristics of different vehicle sensors to complement each other. Therefore, a novel end-to-end multimodal multistage object detection network called MT-DETR is proposed. Unlike the unimodal object detection networks, MT-DETR adds fusion modules and enhancement modules and adopts a hierarchical fusion mechanism. The Residual Fusion Module (RFM) and Confidence Fusion Module (CFM) are designed to fuse camera, lidar, radar, and time features. The Residual Enhancement Module (REM) reinforces each unimodal branch while a multistage loss is introduced to strengthen each branch's effectiveness. The synthesis algorithm for generating camera-lidar data pairs in foggy conditions further boosts the performance in unseen adverse weather. Extensive experiments on various weather conditions of the STF dataset demonstrate that MT-DETR outperforms state-of-the-art methods. The generality of MT-DETR has also been confirmed by replacing the feature extractor in the experiments. The code and pre-trained models are available on https://github.com/Chushihyun/MT-DETR."
  },
  "wacv2023_main_videoobjectmattingviahierarchicalspace-timesemanticguidance": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Video Object Matting via Hierarchical Space-Time Semantic Guidance",
    "authors": [
      "Yumeng Wang",
      "Bo Xu",
      "Ziwen Li",
      "Han Huang",
      "Cheng Lu",
      "Yandong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wang_Video_Object_Matting_via_Hierarchical_Space-Time_Semantic_Guidance_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wang_Video_Object_Matting_via_Hierarchical_Space-Time_Semantic_Guidance_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Different from most existing approaches that require trimap generation for each frame, we reformulate video object matting (VOM) by introducing improved semantic guidance propagation. The proposed approach can achieve a higher degree of temporal coherence between frames with only a single coarse mask as reference. In this paper, we adapt the hierarchical memory matching mechanism into the space-time baseline to build an efficient and robust framework for semantic guidance propagation and alpha prediction. To enhance the temporal smoothness, we also propose a cross-frame attention refinement (CFAR) module that can refine the feature representations across multiple adjacent frames (both historical and current frames) based on the spatio-temporal correlation among the cross-frame pixels. Extensive experiments demonstrate the effectiveness of hierarchical spatio-temporal semantic guidance and the cross-video-frame attention refinement module, and our model outperforms the state-of-the-art VOM methods. We also analyze the significance of different components in our model."
  },
  "wacv2023_main_patchdropouteconomizingvisiontransformersusingpatchdropout": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
    "authors": [
      "Yue Liu",
      "Christos Matsoukas",
      "Fredrik Strand",
      "Hossein Azizpour",
      "Kevin Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Liu_PatchDropout_Economizing_Vision_Transformers_Using_Patch_Dropout_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Liu_PatchDropout_Economizing_Vision_Transformers_Using_Patch_Dropout_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Vision transformers have demonstrated the potential to outperform CNNs in a variety of vision tasks. But the computational and memory requirements of these models prohibit their use in many applications, especially those that depend on high-resolution images, such as medical image classification. Efforts to train ViTs more efficiently are overly complicated, necessitating architectural changes or intricate training schemes. In this work, we show that standard ViT models can be efficiently trained at high resolution by randomly dropping input image patches. This simple approach, PatchDropout, reduces FLOPs and memory by at least 50% in standard natural image datasets such as ImageNet, and those savings only increase with image size. On CSAW, a high-resolution medical dataset, we observe a 5x savings in computation and memory using PatchDropout, along with a boost in performance. For practitioners with a fixed computational or memory budget, PatchDropout makes it possible to choose image resolution, hyperparameters, or model size to get the most performance out of their model."
  },
  "wacv2023_main_reducingannotationeffortbyidentifyingandlabelingcontextuallydiverseclassesforsemanticsegmentationunderdomainshift": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Reducing Annotation Effort by Identifying and Labeling Contextually Diverse Classes for Semantic Segmentation Under Domain Shift",
    "authors": [
      "Sharat Agarwal",
      "Saket Anand",
      "Chetan Arora"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Reducing_Annotation_Effort_by_Identifying_and_Labeling_Contextually_Diverse_Classes_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Reducing_Annotation_Effort_by_Identifying_and_Labeling_Contextually_Diverse_Classes_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In Active Domain Adaptation (ADA), one uses Active Learning (AL) to select target domain frames to annotate for Domain Adaptation (DA). Thus, ADA creates a continuum of cost-performance trade-off models, with unsupervised, and fully supervised DA techniques at the two ends. We observe that in ADA not all regions of a selected frame contribute equally to a model's performance, and there is a strong correlation between annotating certain hard/unique/novel object/stuff instances, and a model's performance. E.g., road regions in a target dataset may look mostly similar to source domain except for certain curved instances, where annotation may be more useful. Based on the observation, we propose Anchor-based and Augmentation-based ADA techniques, which, given a selected frame, determine certain 'hard' semantic regions to be annotated in that frame, such that the selected regions are complementary and diverse in the context of the current labeled set. The proposed techniques carefully avoid the pitfall of region based AL techniques which try to choose most uncertain regions in a frame, but ends up selecting all edge pixels, and similar annotation cost as the whole frame. We show that our approach achieves 66.6 \\miou on \\gta->\\cityscapes dataset with a budget of 4.7% in comparison to 64.9 \\miou by MADA [??]. Our technique can also be used as a decorator for any existing frame-based AL technique. E.g., we report 1.5% performance improvement for CDAL [??] on \\cityscapes using our approach."
  },
  "wacv2023_main_colorrecommendationforvectorgraphicdocumentsbasedonmulti-paletterepresentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Color Recommendation for Vector Graphic Documents Based on Multi-Palette Representation",
    "authors": [
      "Qianru Qiu",
      "Xueting Wang",
      "Mayu Otani",
      "Yuki Iwazaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Qiu_Color_Recommendation_for_Vector_Graphic_Documents_Based_on_Multi-Palette_Representation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Qiu_Color_Recommendation_for_Vector_Graphic_Documents_Based_on_Multi-Palette_Representation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Vector graphic documents present multiple visual elements, such as images, shapes, and texts. Choosing appropriate colors for multiple visual elements is a difficult but crucial task for both amateurs and professional designers. Instead of creating a single color palette for all elements, we extract multiple color palettes from each visual element in a graphic document, and then combine them into a color sequence. We propose a masked color model for color sequence completion and recommend the specified colors based on color context in multi-palette with high probability. We train the model and build a color recommendation system on a large-scale dataset of vector graphic documents. The proposed color recommendation method outperformed other state-of-the-art methods by both quantitative and qualitative evaluations on color prediction and our color recommendation system received positive feedback from professional designers in an interview study."
  },
  "wacv2023_main_interactiveimagemanipulationwithcomplextextinstructions": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Interactive Image Manipulation With Complex Text Instructions",
    "authors": [
      "Ryugo Morita",
      "Zhiqiang Zhang",
      "Man M. Ho",
      "Jinjia Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Morita_Interactive_Image_Manipulation_With_Complex_Text_Instructions_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Morita_Interactive_Image_Manipulation_With_Complex_Text_Instructions_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recently, text-guided image manipulation has received increasing attention in the research field of multimedia processing and computer vision due to its high flexibility and controllability. Its goal is to semantically manipulate parts of an input reference image according to the text descriptions. However, most of the existing works have the following problems: (1) text-irrelevant content cannot always be maintained but randomly changed, (2) the performance of image manipulation still needs to be further improved, (3) only can manipulate descriptive attributes. To solve these problems, we propose a novel image manipulation method that interactively edits an image using complex text instructions. It allows users to not only improve the accuracy of image manipulation but also achieve complex tasks such as enlarging, dwindling, or removing objects and replacing the background with the input image. To make these tasks possible, we apply three strategies. First, the given image is divided into text-relevant content and text-irrelevant content. Only the text-relevant content is manipulated and the text-irrelevant content can be maintained. Second, a super-resolution method is used to enlarge the manipulation region to further improve the operability and to help manipulate the object itself. Third, a user interface is introduced for editing the segmentation map interactively to re-modify the generated image according to the user's desires. Extensive experiments on the Caltech-UCSD Birds-200-2011 (CUB) dataset and Microsoft Common Objects in Context (MS COCO) datasets demonstrate our proposed method can enable interactive, flexible, and accurate image manipulation in real-time. Through qualitative and quantitative evaluations, we show that the proposed model outperforms other state-of-the-art methods."
  },
  "wacv2023_main_dstransdual-streamtransformerforhyperspectralimagerestoration": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DSTrans: Dual-Stream Transformer for Hyperspectral Image Restoration",
    "authors": [
      "Dabing Yu",
      "Qingwu Li",
      "Xiaolin Wang",
      "Zhiliang Zhang",
      "Yixi Qian",
      "Chang Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yu_DSTrans_Dual-Stream_Transformer_for_Hyperspectral_Image_Restoration_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yu_DSTrans_Dual-Stream_Transformer_for_Hyperspectral_Image_Restoration_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Most CNN models exhibit two major flaws in hyperspectral image (HSI) restoration tasks. First, limited high-dimensional HSI training examples exacerbate the difficulty of deep learning methods in learning effective spatial and spectral representations. Second, the existing CNN-based methods model local relations and present limitations in capturing long-range dependencies. In this paper, we customize a novel dual-stream Transformer (DSTrans) for HSI restoration, which mainly consists of the dual-stream attention and the dual-stream feed-forward network. Specifically, we develop the dual-stream attention consisting of Multi-Dconv-head spectral attention (MDSA) and Multi-head Spatial self-attention (MSSA). MDSA and MSSA respectively calculate self-attention along the spectral and spatial dimensions in local windows to capture long-range spectrum dependencies and model global spatial interactions. Meanwhile, the dual-stream feed-forward network is developed to extract global signals and local details in parallel branches. In addition, we exploit a multi-tasking network to train the auxiliary RGB image (RGBI) task and HSI task jointly so that both numerous RGBI samples and limited HSI samples are exploited to learn parameter distribution for DSTrans. Extensive experimental results demonstrate that our method achieves state-of-the-art results on HSI restoration tasks, including HSI super-resolution and denoising. The source code can be obtained at: https://github.com/yudadabing/Dual-Stream-Transformer-for-Hyperspectral-Image-Restoration."
  },
  "wacv2023_main_lineexdataextractionfromscientificlinecharts": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "LineEX: Data Extraction From Scientific Line Charts",
    "authors": [
      "Shivasankaran V. P.",
      "Muhammad Yusuf Hassan",
      "Mayank Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/P._LineEX_Data_Extraction_From_Scientific_Line_Charts_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/P._LineEX_Data_Extraction_From_Scientific_Line_Charts_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we introduce LineEX that extracts data from scientific line charts. We adapt existing vision transformers and pose detection methods and showcase significant performance gains over existing SOTA baselines. We also propose a new loss function and present its effectiveness against existing loss functions. In addition, we synthetically created the largest line chart dataset comprising 430K images. The code and the dataset will be placed in the public domain soon after the acceptance."
  },
  "wacv2023_main_neuralimplicitrepresentationsforphysicalparameterinferencefromasinglevideo": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Neural Implicit Representations for Physical Parameter Inference From a Single Video",
    "authors": [
      "Florian Hofherr",
      "Lukas Koestler",
      "Florian Bernard",
      "Daniel Cremers"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hofherr_Neural_Implicit_Representations_for_Physical_Parameter_Inference_From_a_Single_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hofherr_Neural_Implicit_Representations_for_Physical_Parameter_Inference_From_a_Single_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible."
  },
  "wacv2023_main_mesh-tensiondrivenexpression-basedwrinklesforsyntheticfaces": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Mesh-Tension Driven Expression-Based Wrinkles for Synthetic Faces",
    "authors": [
      "Chirag Raman",
      "Charlie Hewitt",
      "Erroll Wood",
      "Tadas Baltru\u0161aitis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Raman_Mesh-Tension_Driven_Expression-Based_Wrinkles_for_Synthetic_Faces_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Raman_Mesh-Tension_Driven_Expression-Based_Wrinkles_for_Synthetic_Faces_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recent advances in synthesizing realistic faces have shown that synthetic training data can replace real data for various face-related computer vision tasks. A question arises: how important is realism? Is the pursuit of photorealism excessive? In this work, we show otherwise. We boost the realism of our synthetic faces by introducing dynamic skin wrinkles in response to facial expressions, and observe significant performance improvements in downstream computer vision tasks. Previous approaches for producing such wrinkles either required prohibitive artist effort to scale across identities and expressions, or were not capable of reconstructing high-frequency skin details with sufficient fidelity. Our key contribution is an approach that produces realistic wrinkles across a large and diverse population of digital humans. Concretely, we formalize the concept of mesh-tension and use it to aggregate possible wrinkles from high-quality expression scans into albedo and displacement texture maps. At synthesis, we use these maps to produce wrinkles even for expressions not represented in the source scans. Additionally, to provide a more nuanced indicator of model performance under deformations resulting from compressed expressions, we introduce the 300W-winks evaluation subset and the Pexels dataset of closed eyes and winks."
  },
  "wacv2023_main_crt-6dfast6dobjectposeestimationwithcascadedrefinementtransformers": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CRT-6D: Fast 6D Object Pose Estimation With Cascaded Refinement Transformers",
    "authors": [
      "Pedro Castro",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Learning based 6D object pose estimation methods rely on computing large intermediate pose representations and/or iteratively refining an initial estimation with a slow render-compare pipeline. This paper introduces a novel method we call Cascaded Pose Refinement Transformers, or CRT-6D. We replace the commonly used dense intermediate representation with a sparse set of features sampled from the feature pyramid we call OSKFs(Object Surface Keypoint Features) where each element corresponds to an object keypoint. We employ lightweight deformable transformers and chain them together to iteratively refine proposed poses over the sampled OSKFs. We achieve inference runtimes 2x faster than the closest real-time state of the art methods while supporting up to 21 objects on a single model. We demonstrate the effectiveness of CRT-6D by performing extensive experiments on the LM-O and YCBV datasets. Compared to real-time methods, we achieve state of the art on LM-O and YCB-V, falling slightly behind methods with inference runtimes one order of magnitude higher. The source code is available at: https://github.com/PedroCastro/CRT-6D"
  },
  "wacv2023_main_dcvnetdilatedcostvolumenetworksforfastopticalflow": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DCVNet: Dilated Cost Volume Networks for Fast Optical Flow",
    "authors": [
      "Huaizu Jiang",
      "Erik Learned-Miller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jiang_DCVNet_Dilated_Cost_Volume_Networks_for_Fast_Optical_Flow_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jiang_DCVNet_Dilated_Cost_Volume_Networks_for_Fast_Optical_Flow_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, coarse-to-fine or recurrent processing of the cost volume is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. In this paper, we propose an alternative by constructing cost volumes with different dilation factors to capture small and large displacements simultaneously. A U-Net with sikp connections is employed to convert the dilated cost volumes into interpolation weights between all possible captured displacements to get the optical flow. Our proposed model DCVNet only needs to process the cost volume once in a simple feedforward manner and does not rely on the sequential processing strategy. DCVNet obtains comparable accuracy to existing approaches and achieves real-time inference (30 fps on a mid-end 1080ti GPU)."
  },
  "wacv2023_main_out-of-distributiondetectionviafrequency-regularizedgenerativemodels": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Out-of-Distribution Detection via Frequency-Regularized Generative Models",
    "authors": [
      "Mu Cai",
      "Yixuan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Cai_Out-of-Distribution_Detection_via_Frequency-Regularized_Generative_Models_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Cai_Out-of-Distribution_Detection_via_Frequency-Regularized_Generative_Models_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Modern deep generative models can assign high likelihood to inputs drawn from outside the training distribution, posing threats to models in open-world deployments. While much research attention has been placed on defining new test-time measures of OOD uncertainty, these methods do not fundamentally change how deep generative models are regularized and optimized in training. In particular, generative models are shown to overly rely on the background information to estimate the likelihood. To address the issue, we propose a novel frequency-regularized learning (FRL) framework for OOD detection, which incorporates high-frequency information into training and guides the model to focus on semantically relevant features. FRL effectively improves performance on a wide range of generative architectures, including variational auto-encoder, GLOW, and PixelCNN++. On a new large-scale evaluation task, FRL achieves the state-of-the-art performance, outperforming a strong baseline Likelihood Regret by 10.7% (AUROC) while achieving 147x faster inference speed. Extensive ablations show that FRL improves the OOD detection performance while preserving the image generation quality."
  },
  "wacv2023_main_similaritycontrastiveestimationforself-supervisedsoftcontrastivelearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Similarity Contrastive Estimation for Self-Supervised Soft Contrastive Learning",
    "authors": [
      "Julien Denize",
      "Jaonary Rabarisoa",
      "Astrid Orcesi",
      "Romain H\u00e9rault",
      "St\u00e9phane Canu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Denize_Similarity_Contrastive_Estimation_for_Self-Supervised_Soft_Contrastive_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Denize_Similarity_Contrastive_Estimation_for_Self-Supervised_Soft_Contrastive_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations, or semantic similarity, between the instances. Contrastive learning implicitly learns relations but considering all negatives as noise harms the quality of the learned relations. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive learning one. Instead of hard classifying positives and negatives, we estimate from one view of a batch a continuous distribution to push or pull instances based on their semantic similarities. This target similarity distribution is sharpened to eliminate noisy relations. The model predicts for each instance, from another view, the target distribution while contrasting its positive with negatives. Experimental results show that SCE is Top-1 on the ImageNet linear evaluation protocol at 100 pretraining epochs with 72.1% accuracy and is competitive with state-of-the-art algorithms by reaching 75.4% for 200 epochs with multi-crop. We also show that SCE is able to generalize to several tasks. Source code is available here: https://github.com/CEA-LIST/SCE."
  },
  "wacv2023_main_hyperposepdf-hypernetworkspredictingtheprobabilitydistributiononso(3)": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HyperPosePDF - Hypernetworks Predicting the Probability Distribution on SO(3)",
    "authors": [
      "Timon H\u00f6fer",
      "Benjamin Kiefer",
      "Martin Messmer",
      "Andreas Zell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hofer_HyperPosePDF_-_Hypernetworks_Predicting_the_Probability_Distribution_on_SO3_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hofer_HyperPosePDF_-_Hypernetworks_Predicting_the_Probability_Distribution_on_SO3_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Pose estimation of objects in images is an essential problem in virtual and augmented reality and robotics. Traditional solutions use depth cameras, which are expensive, and working solutions require long processing times. This work focuses on the more difficult task when only RGB information is available. To this end, we predict not only the pose of an object but the complete probability density function (pdf) on the rotation manifold. This is the most general way to approach the pose estimation problem and is particularly useful in analysing object symmetries. In this work, we leverage implicit neural representations for the task of pose estimation and show that hypernetworks can be used to predict the rotational pdf. Furthermore, we analyse the Fourier embedding on SO(3) and evaluate the effectiveness of an initial Fourier embedding that proved successful. Our HyperPosePDF outperforms the current SOTA approach on the SYMSOL dataset."
  },
  "wacv2023_main_class-levelconfidencebased3dsemi-supervisedlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Class-Level Confidence Based 3D Semi-Supervised Learning",
    "authors": [
      "Zhimin Chen",
      "Longlong Jing",
      "Liang Yang",
      "Yingwei Li",
      "Bing Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Class-Level_Confidence_Based_3D_Semi-Supervised_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Class-Level_Confidence_Based_3D_Semi-Supervised_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Current pseudo-labeling strategies in 3D semi-supervised learning (SSL) fail to dynamically incorporate the variance of learning status which is affected by each class's learning difficulty and data imbalance. To address this problem, we practically demonstrate that 3D unlabeled data class-level confidence can represent the learning status. Based on this finding, we present a novel class-level confidence based 3D SSL method. Firstly, a dynamic thresholding strategy is proposed to utilize more unlabeled data, especially for low learning status classes. Then, a re-sampling strategy is designed to avoid biasing toward high learning status classes, which dynamically changes the sampling probability of each class. Unlike the latest state-of-the-art SSL method FlexMatch which also utilizes dynamic threshold, our method can be applied to the inherently imbalanced dataset and thus is more general. To show the effectiveness of our method in 3D SSL tasks, we conduct extensive experiments on 3D SSL classification and detection tasks. Our method significantly outperforms state-of-the-art counterparts for both 3D SSL classification and detection tasks in all datasets."
  },
  "wacv2023_main_pidsjointpointinteraction-dimensionsearchfor3dpointcloud": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "PIDS: Joint Point Interaction-Dimension Search for 3D Point Cloud",
    "authors": [
      "Tunhou Zhang",
      "Mingyuan Ma",
      "Feng Yan",
      "Hai Li",
      "Yiran Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_PIDS_Joint_Point_Interaction-Dimension_Search_for_3D_Point_Cloud_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_PIDS_Joint_Point_Interaction-Dimension_Search_for_3D_Point_Cloud_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The interaction and dimension of points are two important axes in designing point operators to serve hierarchical 3D models. Yet, these two axes are heterogeneous and challenging to fully explore. Existing works craft point operator under a single axis and reuse the crafted operator in all parts of 3D models. This overlooks the opportunity to better combine point interactions and dimensions by exploiting varying geometry/density of 3D point clouds. In this work, we establish PIDS, a novel paradigm to jointly explore point interactions and point dimensions to serve semantic segmentation on point cloud data. We establish a large search space to jointly consider versatile point interactions and point dimensions. This supports point operators with various geometry/density considerations. The enlarged search space with heterogeneous search components calls for a better ranking of candidate models. To achieve this, we improve the search space exploration by leveraging predictor-based Neural Architecture Search (NAS), and enhance the quality of prediction by assigning unique encoding to heterogeneous search components based on their priors. We thoroughly evaluate the networks crafted by PIDS on two semantic segmentation benchmarks, showing1% mIOU improvement on SemanticKITTI and S3DIS over state-of-the-art 3D models."
  },
  "wacv2023_main_adaptivefeaturefusionforcooperativeperceptionusinglidarpointclouds": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Adaptive Feature Fusion for Cooperative Perception Using LiDAR Point Clouds",
    "authors": [
      "Donghao Qiao",
      "Farhana Zulkernine"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Qiao_Adaptive_Feature_Fusion_for_Cooperative_Perception_Using_LiDAR_Point_Clouds_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Qiao_Adaptive_Feature_Fusion_for_Cooperative_Perception_Using_LiDAR_Point_Clouds_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Cooperative perception allows a Connected Autonomous Vehicle (CAV) to interact with the other CAVs in the vicinity to enhance perception of surrounding objects to increase safety and reliability. It can compensate for the limitations of the conventional vehicular perception such as blind spots, low resolution, and weather effects. An effective feature fusion model for the intermediate fusion methods of cooperative perception can improve feature selection and information aggregation to further enhance the perception accuracy. We propose adaptive feature fusion models with trainable feature selection modules. One of our proposed models Spatial-wise Adaptive feature Fusion (S-AdaFusion) outperforms all other State-of-the-Arts (SOTAs) on two subsets of the OPV2V dataset: Default CARLA Towns for vehicle detection and the Culver City for domain adaptation. In addition, previous studies have only tested cooperative perception for vehicle detection. A pedestrian, however, is much more likely to be seriously injured in a traffic accident. We evaluate the performance of cooperative perception for both vehicle and pedestrian detection using the CODD dataset. Our architecture achieves higher Average Precision (AP) than other existing models for both vehicle and pedestrian detection on the CODD dataset. The experiments demonstrate that cooperative perception also improves the pedestrian detection accuracy compared to the conventional single vehicle perception process."
  },
  "wacv2023_main_huprabenchmarkforhumanposeestimationusingmillimeterwaveradar": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HuPR: A Benchmark for Human Pose Estimation Using Millimeter Wave Radar",
    "authors": [
      "Shih-Po Lee",
      "Niraj Prakash Kini",
      "Wen-Hsiao Peng",
      "Ching-Wen Ma",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_HuPR_A_Benchmark_for_Human_Pose_Estimation_Using_Millimeter_Wave_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_HuPR_A_Benchmark_for_Human_Pose_Estimation_Using_Millimeter_Wave_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper introduces a novel human pose estimation benchmark, Human Pose with Millimeter Wave Radar (HuPR), that includes synchronized vision and radio signal components. This dataset is created using cross-calibrated mmWave radar sensors and a monocular RGB camera for cross-modality training of radar-based human pose estimation. There are two advantages of using mmWave radar to perform human pose estimation. First, it is robust to dark and low-light conditions. Second, it is not visually perceivable by humans and therefore, can be widely applied to applications with privacy concerns, e.g., surveillance systems in patient rooms. In addition to the benchmark, we propose a cross-modality training framework that leverages the ground-truth 2D keypoints representing human body joints for training, which are systematically generated from the pre-trained 2D pose estimation network based on a monocular camera input image, avoiding laborious manual label annotation efforts. The framework consists of a new radar pre-processing method that better extracts the velocity information from radar data, Cross- and Self-Attention Module (CSAM), to fuse multi-scale radar features, and Pose Refinement Graph Convolutional Networks (PRGCN), to refine the predicted keypoint confidence heatmaps. Our intensive experiments on the HuPR benchmark show that the proposed scheme achieves better human pose estimation performance with only radar data, as compared to traditional pre-processing solutions and previous radio-frequency-based methods. Our proposed scheme further outperforms state-of-the-art pointcloud-based methods."
  },
  "wacv2023_main_geofillreference-basedimageinpaintingwithbettergeometricunderstanding": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "GeoFill: Reference-Based Image Inpainting With Better Geometric Understanding",
    "authors": [
      "Yunhan Zhao",
      "Connelly Barnes",
      "Yuqian Zhou",
      "Eli Shechtman",
      "Sohrab Amirghodsi",
      "Charless Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhao_GeoFill_Reference-Based_Image_Inpainting_With_Better_Geometric_Understanding_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhao_GeoFill_Reference-Based_Image_Inpainting_With_Better_Geometric_Understanding_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Reference-guided image inpainting restores image pixels by leveraging the content from another single reference image. The primary challenge is how to precisely place the pixels from the reference image into the hole region. Therefore, understanding the 3D geometry that relates pixels between two views is a crucial step towards building a better model. Given the complexity of handling various types of reference images, we focus on the scenario where the images are captured by freely moving the same camera around. Compared to the previous work, we propose a principled approach that does not make heuristic assumptions about the planarity of the scene. We leverage a monocular depth estimate and predict relative pose between cameras, then align the reference image to the target by a differentiable 3D reprojection and a joint optimization of relative pose and depth map scale and offset. Our approach achieves state-of-the-art performance on both RealEstate10K and MannequinChallenge dataset with large baselines, complex geometry and extreme camera motions. We experimentally verify our approach is also better at handling large holes."
  },
  "wacv2023_main_boostingvisiontransformersforimageretrieval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Boosting Vision Transformers for Image Retrieval",
    "authors": [
      "Chull Hwan Song",
      "Jooyoung Yoon",
      "Shunghyun Choi",
      "Yannis Avrithis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Song_Boosting_Vision_Transformers_for_Image_Retrieval_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Song_Boosting_Vision_Transformers_for_Image_Retrieval_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The explosive increase in vision transformers studies has shown remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image epresentation. (3) In each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation."
  },
  "wacv2023_main_dels-mvsdeepepipolarlinesearchformulti-viewstereo": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DELS-MVS: Deep Epipolar Line Search for Multi-View Stereo",
    "authors": [
      "Christian Sormann",
      "Emanuele Santellani",
      "Mattia Rossi",
      "Andreas Kuhn",
      "Friedrich Fraundorfer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sormann_DELS-MVS_Deep_Epipolar_Line_Search_for_Multi-View_Stereo_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sormann_DELS-MVS_Deep_Epipolar_Line_Search_for_Multi-View_Stereo_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We propose a novel approach for deep learning-based Multi-View Stereo (MVS). For each pixel in the reference image, our method leverages a deep architecture to search for the corresponding point in the source image directly along the corresponding epipolar line. We denote our method DELS-MVS: Deep Epipolar Line Search Multi-View Stereo. Previous works in deep MVS select a range of interest within the depth space, discretize it, and sample the epipolar line according to the resulting depth values: this can result in an uneven scanning of the epipolar line, hence of the image space. Instead, our method works directly on the epipolar line: this guarantees an even scanning of the image space and avoids both the need to select a depth range of interest, which is often not known a priori and can vary dramatically from scene to scene, and the need for a suitable discretization of the depth space. In fact, our search is iterative, which avoids the building of a cost volume, costly both to store and to process. Finally, our method performs a robust geometry-aware fusion of the estimated depth maps, leveraging a confidence predicted alongside each depth. We test DELS-MVS on the ETH3D, Tanks and Temples and DTU benchmarks and achieve competitive results with respect to state-of-the-art approaches."
  },
  "wacv2023_main_upliftandupsampleefficient3dhumanposeestimationwithupliftingtransformers": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Uplift and Upsample: Efficient 3D Human Pose Estimation With Uplifting Transformers",
    "authors": [
      "Moritz Einfalt",
      "Katja Ludwig",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Einfalt_Uplift_and_Upsample_Efficient_3D_Human_Pose_Estimation_With_Uplifting_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Einfalt_Uplift_and_Upsample_Efficient_3D_Human_Pose_Estimation_With_Uplifting_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The state-of-the-art for monocular 3D human pose estimation in videos is dominated by the paradigm of 2D-to-3D pose uplifting. While the uplifting methods themselves are rather efficient, the true computational complexity depends on the per-frame 2D pose estimation. In this paper, we present a Transformer-based pose uplifting scheme that can operate on temporally sparse 2D pose sequences but still produce temporally dense 3D pose estimates. We show how masked token modeling can be utilized for temporal upsampling within Transformer blocks. This allows to decouple the sampling rate of input 2D poses and the target frame rate of the video and drastically decreases the total computational complexity. Additionally, we explore the option of pre-training on large motion capture archives, which has been largely neglected so far. We evaluate our method on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. With an MPJPE of 45.0 mm and 46.9 mm, respectively, our proposed method can compete with the state-of-the-art while reducing inference time by a factor of 12. This enables real-time throughput with variable consumer hardware in stationary and mobile applications. We release our code and models at https://github.com/goldbricklemon/uplift-upsample-3dhpe"
  },
  "wacv2023_main_boostingneuralvideocodecsbyexploitinghierarchicalredundancy": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Boosting Neural Video Codecs by Exploiting Hierarchical Redundancy",
    "authors": [
      "Reza Pourreza",
      "Hoang Le",
      "Amir Said",
      "Guillaume Sauti\u00e8re",
      "Auke Wiggers"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Pourreza_Boosting_Neural_Video_Codecs_by_Exploiting_Hierarchical_Redundancy_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Pourreza_Boosting_Neural_Video_Codecs_by_Exploiting_Hierarchical_Redundancy_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In video compression, coding efficiency is improved by reusing pixels from previously decoded frames via motion and residual compensation. We define two levels of hierarchical redundancy in video frames: 1) first-order: redundancy in pixel space, i.e, similarities in pixel values across neighboring frames, which is effectively captured using motion and residual compensation, 2) second-order: redundancy in motion and residual maps due to smooth motion in natural videos. While most of the existing neural video coding literature addresses first-order redundancy, we tackle the problem of capturing second-order redundancy in neural video codecs via predictors. We introduce generic motion and residual predictors that learn to extrapolate from previously decoded data. These predictors are lightweight, and can be employed with most neural video codecs in order to improve their rate-distortion performance. Moreover, while RGB is the dominant colorspace in neural video coding literature, we introduce general modifications for neural video codecs to embrace the YUV420 colorspace and report YUV420 results. Our experiments show that using our predictors with a well-known neural video codec leads to 38% and 34% bitrate saving in RGB and YUV420 colorspaces measured on the UVG dataset."
  },
  "wacv2023_main_garsimparticlebasedneuralgarmentsimulator": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "GarSim: Particle Based Neural Garment Simulator",
    "authors": [
      "Lokender Tiwari",
      "Brojeshwar Bhowmick"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Tiwari_GarSim_Particle_Based_Neural_Garment_Simulator_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Tiwari_GarSim_Particle_Based_Neural_Garment_Simulator_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present a particle-based neural garment simulator (dubbed as GarSim) that can simulate template garments on the target arbitrary body poses. Existing learning-based methods majorly work for specific garment type (e.g. t-shirt, skirt, etc) or garment topology, and needs retraining for a new type of garment. Similarly, some methods focus on a particular fabric, body shape, and pose. To circumvent these limitations, our method fundamentally learns the physical dynamics of the garment vertices conditioned on underlying body shape, motion, and fabric properties to generalize across garment types, topology, and fabric along with different body shape and pose. In particular, we represent the garment as a graph, where the nodes represent the physical state of the garment vertices, and the edges represent the relation between the two nodes. The nodes and edges of the garment graph encode various properties of garments and the human body to compute the dynamics of the vertices through a learned message-passing. Learning of such dynamics of the garment vertices conditioned on underlying body motion and fabric properties enables our method to be trained simultaneously for multiple types of garments (e.g., tops, skirts, etc) with arbitrary mesh resolutions, varying topologies, and fabric properties. Our experimental results show that GarSim with less amount of training data not only outperforms the SOTA methods on challenging CLOTH3D dataset both qualitatively and quantitatively, but also works reliably well on the unseen poses obtained from YouTube videos, and give satisfactory results on unseen cloth types which were not present during the training."
  },
  "wacv2023_main_event-basedrgbsensingwithstructuredlight": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Event-Based RGB Sensing With Structured Light",
    "authors": [
      "Seyed Ehsan Marjani Bajestani",
      "Giovanni Beltrame"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bajestani_Event-Based_RGB_Sensing_With_Structured_Light_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bajestani_Event-Based_RGB_Sensing_With_Structured_Light_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Event-based cameras (ECs) are bio-inspired sensors that asynchronously report pixel brightness changes. Due to their high dynamic range, pixel bandwidth, temporal resolution, low power consumption, and computational simplicity, they are beneficial for vision-based projects in challenging lighting conditions and they can detect fast movements with their microsecond response time. The first generation of ECs are monochrome, but color data is very useful and sometimes essential for certain vision-based applications. The latest technology enables manufacturers to build color ECs, trading off the size of the sensor and substantially reducing the resolution compared to monochrome models, despite having the same bandwidth. In addition, ECs only detect changes in light and do not show static or slowly moving objects. We introduce a method to detect full RGB events using a monochrome EC aided by a structured light projector. The projector emits rapidly changing RGB patterns of light beams on the scene, the reflection of which is captured by the EC. We combine the benefits of ECs and projection-based techniques and allow depth and color detection of static or moving objects with a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, paving the way for frameless RGB-D sensing applications. Our code is available publicly: github.com/MISTLab/event_based_rgbd_ros"
  },
  "wacv2023_main_saliencyguidedexperiencepackingforreplayincontinuallearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Saliency Guided Experience Packing for Replay in Continual Learning",
    "authors": [
      "Gobinda Saha",
      "Kaushik Roy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Saha_Saliency_Guided_Experience_Packing_for_Replay_in_Continual_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Saha_Saliency_Guided_Experience_Packing_for_Replay_in_Continual_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Artificial learning systems aspire to mimic human intelligence by continually learning from a stream of tasks without forgetting past knowledge. One way to enable such learning is to store past experiences in the form of input examples in episodic memory and replay them when learning new tasks. However, performance of such method suffers as the size of the memory becomes smaller. In this paper, we propose a new approach for experience replay, where we select the past experiences by looking at the saliency maps which provide visual explanations for the model's decision. Guided by these saliency maps, we pack the memory with only the parts or patches of the input images important for the model's prediction. While learning a new task, we replay these memory patches with appropriate zero-padding to remind the model about its past decisions. We evaluate our algorithm on CIFAR-100, miniImageNet and CUB datasets and report better performance than the state-of-the-art approaches. With qualitative and quantitative analyses we show that our method captures richer summaries of past experiences without any memory increase, and hence performs well with small episodic memory."
  },
  "wacv2023_main_ctrgancycletransformersganforgaittransfer": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CTrGAN: Cycle Transformers GAN for Gait Transfer",
    "authors": [
      "Shahar Mahpod",
      "Noam Gaash",
      "Hay Hoffman",
      "Gil Ben-Artzi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mahpod_CTrGAN_Cycle_Transformers_GAN_for_Gait_Transfer_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mahpod_CTrGAN_Cycle_Transformers_GAN_for_Gait_Transfer_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We introduce a novel approach for gait transfer from unconstrained videos in-the-wild. In contrast to motion transfer, the objective here is not to imitate the source's motions by the target, but rather to replace the walking source with the target, while transferring the target's typical gait. Our approach can be trained only once with multiple sources and is able to transfer the gait of the target from unseen sources, eliminating the need for retraining for each new source independently. Furthermore, we propose a novel metrics for gait transfer based on gait recognition models that enable to quantify the quality of the transferred gait, and show that existing techniques yield a discrepancy that can be easily detected.\\nWe introduce Cycle Transformers GAN (CTrGAN), that consist of a decoder and encoder, both Transformers, where the attention is on the temporal domain between complete images rather than the spatial domain between patches. Using a widely-used gait recognition dataset, we demonstrate that our approach is capable of producing over an order of magnitude more realistic personalized gaits than existing methods, even when used with sources that were not available during training. As part of our solution, we present a detector that determines whether a video is real or generated by our model."
  },
  "wacv2023_main_handlingimageandlabelresolutionmismatchinremotesensing": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Handling Image and Label Resolution Mismatch in Remote Sensing",
    "authors": [
      "Scott Workman",
      "Armin Hadzic",
      "M. Usman Rafique"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Workman_Handling_Image_and_Label_Resolution_Mismatch_in_Remote_Sensing_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Workman_Handling_Image_and_Label_Resolution_Mismatch_in_Remote_Sensing_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Though semantic segmentation has been heavily explored in vision literature, unique challenges remain in the remote sensing domain. One such challenge is how to handle resolution mismatch between overhead imagery and ground-truth label sources, due to differences in ground sample distance. To illustrate this problem, we introduce a new dataset and use it to showcase weaknesses inherent in existing strategies that naively upsample the target label to match the image resolution. Instead, we present a method that is supervised using low-resolution labels (without upsampling), but takes advantage of an exemplar set of high-resolution labels to guide the learning process. Our method incorporates region aggregation, adversarial learning, and self-supervised pretraining to generate fine-grained predictions, without requiring high-resolution annotations. Extensive experiments demonstrate the real-world applicability of our approach."
  },
  "wacv2023_main_aggregatingbilateralattentionforfew-shotinstancelocalization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Aggregating Bilateral Attention for Few-Shot Instance Localization",
    "authors": [
      "He-Yen Hsieh",
      "Ding-Jie Chen",
      "Cheng-Wei Chang",
      "Tyng-Luh Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hsieh_Aggregating_Bilateral_Attention_for_Few-Shot_Instance_Localization_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Aggregating_Bilateral_Attention_for_Few-Shot_Instance_Localization_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Attention filtering under various learning scenarios has proven advantageous in enhancing the performance of many neural network architectures. The mainstream attention mechanism is established upon the non-local block, also known as an essential component of the prominent Transformer networks, to catch long-range correlations. However, such unilateral attention is often hampered by sparse and obscure responses, revealing insufficient dependencies across images/patches, and high computational cost, especially for those employing the multi-head design. To overcome these issues, we introduce a novel mechanism of aggregating bilateral attention (ABA) and validate its usefulness in tackling the task of few-shot instance localization, reflecting the underlying query-support dependency. Specifically, our method facilitates uncovering informative features via assessing: i) an embedding norm for exploring the semantically-related cues; ii) context awareness for correlating the query data and support regions. ABA is then carried out by integrating the affinity relations derived from the two measurements to serve as a lightweight but effective query-support attention mechanism with high localization recall. We evaluate ABA on two localization tasks, namely, few-shot action localization and one-shot object detection. Extensive experiments demonstrate that the proposed ABA achieves superior performances over existing methods."
  },
  "wacv2023_main_anoleafunsupervisedleafdiseasesegmentationviastructurallyrobustgenerativeinpainting": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "AnoLeaf: Unsupervised Leaf Disease Segmentation via Structurally Robust Generative Inpainting",
    "authors": [
      "Swati Bhugra",
      "Vinay Kaushik",
      "Amit Gupta",
      "Brejesh Lall",
      "Santanu Chaudhury"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bhugra_AnoLeaf_Unsupervised_Leaf_Disease_Segmentation_via_Structurally_Robust_Generative_Inpainting_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bhugra_AnoLeaf_Unsupervised_Leaf_Disease_Segmentation_via_Structurally_Robust_Generative_Inpainting_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Plant diseases severely limits agriculture production, necessitating the high-throughput monitoring of plant leaves. Currently, this is formulated as an automatic disease segmentation task addressed via deep learning frameworks. These deep leaning frameworks trained with leaf image data in a supervised paradigm have few limitations, mainly: (1) training datasets are heavily imbalanced towards healthy leaf images, (2) disease region annotation is labour-intensive and (3) due to the heterogeneity of disease symptoms, these frameworks lacks generalisability. In this paper, we reformulate disease segmentation as an anomaly localisation task. Specifically, we introduce a novel unsupervised framework (AnoLeaf) based on an edge-guided inpainting that optimises the learning of contextual attention on only healthy leaf images. The network utilisation on diseased leaf images results in reconstruction of its healthy counterparts, generating an inpainting error. The contextual attention maps reinforce the inpainting error to effectively localise the disease. Thus, AnoLeaf alleviates the acquisition and annotation of rare disease images. Additional experiments on MVTec anomaly detection dataset further demonstrate its generalisability."
  },
  "wacv2023_main_howtopracticevqaonaresource-limitedtargetdomain": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "How To Practice VQA on a Resource-Limited Target Domain",
    "authors": [
      "Mingda Zhang",
      "Rebecca Hwa",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_How_To_Practice_VQA_on_a_Resource-Limited_Target_Domain_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_How_To_Practice_VQA_on_a_Resource-Limited_Target_Domain_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Visual question answering (VQA) is an active research area at the intersection of computer vision and natural language understanding. One major obstacle that keeps VQA models that perform well on benchmarks from being as successful on real-world applications, is the lack of annotated Image-Question-Answer triplets in the task of interest. In this work, we focus on a previously overlooked perspective, which is the disparate effectiveness of transfer learning and domain adaptation methods depending on the amount of labeled/unlabeled data available. We systematically investigated the visual domain gaps and question-defined textual gaps, and compared different knowledge transfer strategies under unsupervised, self-supervised, semi-supervised and fully-supervised adaptation scenarios. We show that different methods have varied sensitivity and requirements for data amount in the target domain. We conclude by sharing the best practice from our exploration regarding transferring VQA models to resource-limited target domains."
  },
  "wacv2023_main_lightweightvideodenoisingusingaggregatedshiftedwindowattention": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Lightweight Video Denoising Using Aggregated Shifted Window Attention",
    "authors": [
      "Lydia Lindner",
      "Alexander Effland",
      "Filip Ilic",
      "Thomas Pock",
      "Erich Kobler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lindner_Lightweight_Video_Denoising_Using_Aggregated_Shifted_Window_Attention_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lindner_Lightweight_Video_Denoising_Using_Aggregated_Shifted_Window_Attention_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Video denoising is a fundamental problem in numerous computer vision applications. State-of-the-art attention-based denoising methods typically yield good results, but require vast amounts of GPU memory and usually suffer from very long computation times. Especially in the field of restoring digitized high-resolution historic films, these techniques are not applicable in practice. To overcome these issues, we introduce a lightweight video denoising network that combines efficient axial-coronal-sagittal (ACS) convolutions with a novel shifted window attention formulation (ASwin), which is based on the memory-efficient aggregation of self- and cross-attention across video frames. We numerically validate the performance and efficiency of our approach on synthetic Gaussian noise. Moreover, we train our network as a general-purpose blind denoising model for real-world videos, using a realistic noise synthesis pipeline to generate clean-noisy video pairs. A user study and non- reference quality assessment prove that our method outperforms the state-of-the-art on real-world historic videos in terms of denoising performance and temporal consistency."
  },
  "wacv2023_main_dynamicmixtureofcounternetworkforlocation-agnosticcrowdcounting": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Dynamic Mixture of Counter Network for Location-Agnostic Crowd Counting",
    "authors": [
      "Mingjie Wang",
      "Hao Cai",
      "Yong Dai",
      "Minglun Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wang_Dynamic_Mixture_of_Counter_Network_for_Location-Agnostic_Crowd_Counting_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wang_Dynamic_Mixture_of_Counter_Network_for_Location-Agnostic_Crowd_Counting_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Crowd counting has attracted increasing attentions in recent years due to its challenges and wide societal applications. Despite persevering efforts made by the research community, most of existing methods require a large amount of location-level annotations. Collecting such type of fine-granularity supervisory signals is extremely time-consuming and labour-intensive, thereby hindering the well generalization of these location-adherent models. To shun this drawback, several pioneering studies open a promising research direction of location-agonistic crowd counting. Albeit the noticeable efforts, they somewhat ignore the merits of diverse learning paradigms and the issue of intractable density shift. To ameliorate these issues, in this paper, a novel Dynamic Mixture of Counter Network (DMCNet) is proposed for location-agnostic crowd counting. Specifically, our DMCNet inherits the hybrid advantages of CNNs (e.g. locality-oriented and pyramidal property) and MLP-based structure (e.g. global receptive fields and light weight). Particularly, the dynamic counter predictor and the mixture of counter heads are delicately designed to hammer at combating huge density shift and overfitting. Extensive experiments demonstrate that our DMCNet attains state-of-the-art performance against existing location-agnostic approaches and performs on par with many conventional location-adherent ones."
  },
  "wacv2023_main_fastonlinevideosuper-resolutionwithdeformableattentionpyramid": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fast Online Video Super-Resolution With Deformable Attention Pyramid",
    "authors": [
      "Dario Fuoli",
      "Martin Danelljan",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Fuoli_Fast_Online_Video_Super-Resolution_With_Deformable_Attention_Pyramid_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Fuoli_Fast_Online_Video_Super-Resolution_With_Deformable_Attention_Pyramid_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Video super-resolution (VSR) has many applications that pose strict causal, real-time, and latency constraints, including video streaming and TV. We address the VSR problem under these settings, which poses additional important challenges since information from future frames is unavailable. Importantly, designing efficient, yet effective frame alignment and fusion modules remain central problems. In this work, we propose a recurrent VSR architecture based on a deformable attention pyramid (DAP). Our DAP aligns and integrates information from the recurrent state into the current frame prediction. To circumvent the computational cost of traditional attention-based methods, we only attend to a limited number of spatial locations, which are dynamically predicted by the DAP. Comprehensive experiments and analysis of the proposed key innovations show the effectiveness of our approach. We significantly reduce processing time and computational complexity in comparison to state-of-the-art methods, while maintaining a high performance. We surpass state-of-the-art method EDVR-M on two standard benchmarks with a speed-up of over 3x."
  },
  "wacv2023_main_perceiver-vlefficientvision-and-languagemodelingwithiterativelatentattention": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Perceiver-VL: Efficient Vision-and-Language Modeling With Iterative Latent Attention",
    "authors": [
      "Zineng Tang",
      "Jaemin Cho",
      "Jie Lei",
      "Mohit Bansal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Tang_Perceiver-VL_Efficient_Vision-and-Language_Modeling_With_Iterative_Latent_Attention_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Tang_Perceiver-VL_Efficient_Vision-and-Language_Modeling_With_Iterative_Latent_Attention_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present Perceiver-VL, a vision-and-language framework that efficiently handles high-dimensional multimodal inputs such as long videos and text. Powered by the iterative latent-cross-attention of Perceiver, our framework scales with linear complexity, in contrast to the quadratic complexity of self-attention used in many state-of-the-art transformer-based models. To further improve the efficiency of our framework, we also study applying LayerDrop on cross-attention layers and introduce a mixed-stream architecture for cross-modal retrieval. We evaluate Perceiver-VL on diverse video-text and image-text benchmarks, where Perceiver-VL achieves the lowest GFLOPs and latency, while maintaining competitive performance. In addition, we also provide comprehensive analyses over various aspects of our framework, including pretraining data, scalability of latent size and input size, dropping cross-attention layers at inference to reduce latency, modality aggregation strategy, positional encoding, and weight initialization strategy."
  },
  "wacv2023_main_self-supervisedpyramidrepresentationlearningformulti-labelvisualanalysisandbeyond": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond",
    "authors": [
      "Cheng-Yen Hsieh",
      "Chih-Jung Chang",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with additional learners to observe and relate inherent semantic information within an image. In particular, we present a cross-scale patch-level correlation learning in SS-PRL, which allows the model to aggregate and associate information learned across patch scales. We show that, with our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection, and instance segmentation."
  },
  "wacv2023_main_nearestneighborsmeetdeepneuralnetworksforpointcloudanalysis": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis",
    "authors": [
      "Renrui Zhang",
      "Liuhui Wang",
      "Ziyu Guo",
      "Jianbo Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_Nearest_Neighbors_Meet_Deep_Neural_Networks_for_Point_Cloud_Analysis_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Nearest_Neighbors_Meet_Deep_Neural_Networks_for_Point_Cloud_Analysis_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Performances on standard 3D point cloud benchmarks have plateaued, resulting in oversized models and complex network design to make a fractional improvement. We present an alternative to enhance existing deep neural networks without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter SN-Adapter. Building on any trained 3D network, we utilize its learned encoding capability to extract features of the training dataset and summarize them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter retrieves k nearest neighbors (k-NN) from the pre-constructed spatial prototypes and linearly interpolates the k-NN prediction with that of the original 3D network. By providing complementary characteristics, the proposed SN-Adapter serves as a plug-and-play module to economically improve performance in a non-parametric manner. More importantly, our SN-Adapter can be effectively generalized to various 3D tasks, including shape classification, part segmentation, and 3D object detection, demonstrating its superiority and robustness. We hope our approach could show a new perspective for point cloud analysis and facilitate future research."
  },
  "wacv2023_main_siunetsparsityinvariantu-netforedge-awaredepthcompletion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SIUNet: Sparsity Invariant U-Net for Edge-Aware Depth Completion",
    "authors": [
      "Avinash Nittur Ramesh",
      "Fabio Giovanneschi",
      "Mar\u00eda A. Gonz\u00e1lez-Huici"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ramesh_SIUNet_Sparsity_Invariant_U-Net_for_Edge-Aware_Depth_Completion_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ramesh_SIUNet_Sparsity_Invariant_U-Net_for_Edge-Aware_Depth_Completion_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Depth completion is the task of generating dense depth images from sparse depth measurements, e.g., LiDARs. Existing unguided approaches fail to recover dense depth images with sharp object boundaries due to depth bleeding, especially from extremely sparse measurements. State-of-the-art guided approaches require additional processing for spatial and temporal alignment of multi-modal inputs, and sophisticated architectures for data fusion, making them non-trivial for customized sensor setup. To address these limitations, we propose an unguided approach based on UNet that is invariant to sparsity of inputs. Boundary consistency in reconstruction is explicitly enforced through auxiliary learning on a synthetic dataset with dense depth and depth contour images as targets, followed by fine-tuning on a real-world dataset. With our network architecture and simple implementation approach, we achieve competitive results among unguided approaches on KITTI benchmark and show that the reconstructed image has sharp boundaries and is robust even towards extremely sparse LiDAR measurements."
  },
  "wacv2023_main_weaklysupervisedfacenamingwithsymmetry-enhancedcontrastiveloss": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Weakly Supervised Face Naming With Symmetry-Enhanced Contrastive Loss",
    "authors": [
      "Tingyu Qu",
      "Tinne Tuytelaars",
      "Marie-Francine Moens"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Qu_Weakly_Supervised_Face_Naming_With_Symmetry-Enhanced_Contrastive_Loss_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Qu_Weakly_Supervised_Face_Naming_With_Symmetry-Enhanced_Contrastive_Loss_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We revisit the weakly supervised cross-modal face-name alignment task; that is, given an image and a caption, we label the faces in the image with the names occurring in the caption. Whereas past approaches have learned the latent alignment between names and faces by uncertainty reasoning over a set of images and their respective captions, in this paper, we rely on appropriate loss functions to learn the alignments in a neural network setting and propose SECLA and SECLA-B. SECLA is a Symmetry-Enhanced Contrastive Learning-based Alignment model that can effectively maximize the similarity scores between corresponding faces and names in a weakly supervised fashion. A variation of the model, SECLA-B, learns to align names and faces as humans do, that is, learning from easy to hard cases to further increase the performance of SECLA. More specifically, SECLA-B applies a two-stage learning framework: (1) Training the model on an easy subset with a few names and faces in each image-caption pair. (2) Leveraging the known pairs of names and faces from the easy cases using a bootstrapping strategy with additional loss to prevent forgetting and learning new alignments at the same time. We achieve state-of-the-art results for both the augmented Labeled Faces in the Wild dataset and the Celebrity Together dataset. In addition, we believe that our methods can be adapted to other multimodal news understanding tasks."
  },
  "wacv2023_main_hoechstganvirtuallymphocytestainingusinggenerativeadversarialnetworks": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial Networks",
    "authors": [
      "Georg W\u00f6lflein",
      "In Hwa Um",
      "David J. Harrison",
      "Ognjen Arandjelovi\u0107"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The presence and density of specific types of immune cells are important to understand a patient's immune response to cancer. However, immunofluorescence staining required to identify T cell subtypes is expensive, timeconsuming, and rarely performed in clinical settings. We present a framework to virtually stain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to identify T cell subtypes in clear cell renal cell carcinoma using generative adversarial networks. Our proposed method jointly learns both staining tasks, incentivising the network to incorporate mutually beneficial information from each task. We devise a novel metric to quantify the virtual staining quality, and use it to evaluate our method."
  },
  "wacv2023_main_switchingtodiscriminativeimagecaptioningbyrelievingabottleneckofreinforcementlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning",
    "authors": [
      "Ukyo Honda",
      "Taro Watanabe",
      "Yuji Matsumoto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Honda_Switching_to_Discriminative_Image_Captioning_by_Relieving_a_Bottleneck_of_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Honda_Switching_to_Discriminative_Image_Captioning_by_Relieving_a_Bottleneck_of_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Discriminativeness is a desirable feature of image captions: captions should describe the characteristic details of input images. However, recent high-performing captioning models, which are trained with reinforcement learning (RL), tend to generate overly generic captions despite their high performance in various other criteria. First, we investigate the cause of the unexpectedly low discriminativeness and show that RL has a deeply rooted side effect of limiting the output words to high-frequency words. The limited vocabulary is a severe bottleneck for discriminativeness as it is difficult for a model to describe the details beyond its vocabulary. This identification of the bottleneck allows us to drastically recast discriminative image captioning as a much simpler task of encouraging low-frequency word generation. Hinted by long-tail classification and debiasing methods, we propose the methods that easily switch off-the-shelf RL models to discriminativeness-aware models with only a single-epoch fine-tuning on the part of the parameters. Extensive experiments demonstrate that our methods significantly enhance the discriminativeness of off-the-shelf RL models and even outperform previous discriminativeness-aware methods with much smaller computational costs. Detailed analysis and human evaluation also verify that our methods boost the discriminativeness without sacrificing the overall quality of captions."
  },
  "wacv2023_main_rancernon-axisalignedanisotropiccertificationwithrandomizedsmoothing": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "RANCER: Non-Axis Aligned Anisotropic Certification With Randomized Smoothing",
    "authors": [
      "Taras Rumezhak",
      "Francisco Girbal Eiras",
      "Philip H.S. Torr",
      "Adel Bibi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Rumezhak_RANCER_Non-Axis_Aligned_Anisotropic_Certification_With_Randomized_Smoothing_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Rumezhak_RANCER_Non-Axis_Aligned_Anisotropic_Certification_With_Randomized_Smoothing_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "As modern networks have been proven to be unprotected from adversarial attacks and are applied in safety-critical applications, defense against them is very crucial. Many works were dedicated to this topic, but randomized smoothing has been recently proven to be an effective approach for the certified defense of deep neural networks and getting robust classifiers. Some prior results were obtained utilizing the techniques of adding extra parameters to extend the limits of the certification regions. In this way, sample-wise optimization was proposed to maximize the certification radius per input. The idea was further extended with the generalized anisotropic counterparts of l1 and l2 certificates which allow achieving larger certified region volume avoiding worst-case certification near potentially larger safe regions. However, anisotropic certification is limited by the aligned axis lacking the freedom to extend in any direction. To mitigate this constraint, in this work, we (i) revisit the anisotropic certification, provide an analysis of its non-axis aligned counterpart and propose its rotation-free extension, (ii) conduct experiments on the CIFAR-10 dataset to report the improved performance."
  },
  "wacv2023_main_handgcnformeranoveltopology-awaretransformernetworkfor3dhandposeestimation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HandGCNFormer: A Novel Topology-Aware Transformer Network for 3D Hand Pose Estimation",
    "authors": [
      "Yintong Wang",
      "LiLi Chen",
      "Jiamao Li",
      "Xiaolin Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wang_HandGCNFormer_A_Novel_Topology-Aware_Transformer_Network_for_3D_Hand_Pose_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wang_HandGCNFormer_A_Novel_Topology-Aware_Transformer_Network_for_3D_Hand_Pose_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Despite the substantial progress in 3D hand pose estimation, inferring plausible and accurate poses in the presence of severe self-occlusion and high self-similarity remains an inherent challenge. To mitigate the ambiguity arising from invisible and similar joints, we propose a novel Topology-aware Transformer network named HandGCNFormer, incorporating the prior knowledge of hand kinematic topology into the network while modeling long-range context information. Specifically, we present a novel Graphformer decoder with an additional node-offset graph convolutional layer (NoffGConv) that optimizes the synergy of Transformer and GCN, capturing long-range dependencies as well as local topology connection between joints. Furthermore, we replace the standard MLP prediction head with a novel Topology-aware head to better utilize local topology constraints for more plausible and accurate poses. Our method achieves state-of-the-art performance on four challenging datasets including Hands2017, NYU, ICVL, and MSRA."
  },
  "wacv2023_main_mixtureoutlierexposuretowardsout-of-distributiondetectioninfine-grainedenvironments": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Mixture Outlier Exposure: Towards Out-of-Distribution Detection in Fine-Grained Environments",
    "authors": [
      "Jingyang Zhang",
      "Nathan Inkawhich",
      "Randolph Linderman",
      "Yiran Chen",
      "Hai Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_Mixture_Outlier_Exposure_Towards_Out-of-Distribution_Detection_in_Fine-Grained_Environments_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Mixture_Outlier_Exposure_Towards_Out-of-Distribution_Detection_in_Fine-Grained_Environments_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Many real-world scenarios in which DNN-based recognition systems are deployed have inherently fine-grained attributes (e.g., bird-species recognition, medical image classification). In addition to achieving reliable accuracy, a critical subtask for these models is to detect Out-of-distribution (OOD) inputs. Given the nature of the deployment environment, one may expect such OOD inputs to also be fine-grained w.r.t. the known classes (e.g., a novel bird species), which are thus extremely difficult to identify. Unfortunately, OOD detection in fine-grained scenarios remains largely underexplored. In this work, we aim to fill this gap by first carefully constructing four large-scale fine-grained test environments, in which existing methods are shown to have difficulties. Particularly, we find that even explicitly incorporating a diverse set of auxiliary outlier data during training does not provide sufficient coverage over the broad region where fine-grained OOD samples locate. We then propose Mixture Outlier Exposure (MixOE), which mixes ID data and training outliers to expand the coverage of different OOD granularities, and trains the model such that the prediction confidence linearly decays as the input transitions from ID to OOD. Extensive experiments and analyses demonstrate the effectiveness of MixOE for building up OOD detector in fine-grained environments. The code is available at https://github.com/zjysteven/MixOE."
  },
  "wacv2023_main_mixvprfeaturemixingforvisualplacerecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MixVPR: Feature Mixing for Visual Place Recognition",
    "authors": [
      "Amar Ali-bey",
      "Brahim Chaib-draa",
      "Philippe Gigu\u00e8re"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ali-bey_MixVPR_Feature_Mixing_for_Visual_Place_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ali-bey_MixVPR_Feature_Mixing_for_Visual_Place_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Visual Place Recognition (VPR) is a crucial part of mobile robotics and autonomous driving as well as other computer vision tasks. It refers to the process of identifying a place depicted in a query image using only computer vision. At large scale, repetitive structures, weather, and illumination changes pose a real challenge, as appearances can drastically change over time. Along with tackling these challenges, an efficient VPR technique must also be practical in real-world scenarios where latency matters. To address this, we introduce MixVPR, a new holistic feature aggregation technique that takes feature maps from pre-trained backbones as a set of global features. Then, it incorporates a global relationship between elements in each feature map in a cascade of feature mixing, eliminating the need for local or pyramidal aggregation as done in NetVLAD or TransVPR. We demonstrate the effectiveness of our technique through extensive experiments on multiple large-scale benchmarks. Our method outperforms all existing techniques by a large margin while having less than half the number of parameters compared to CosPlace and NetVLAD. We achieve a new all-time high recall@1 score of 94.6% on Pitts250k-test, 88.0% on MapillarySLS, and more importantly, 58.4% on Nordland. Finally, our method outperforms two-stage retrieval techniques such as Patch-NetVLAD, TransVPR and SuperGLUE, all while being orders of magnitude faster."
  },
  "wacv2023_main_uncertainty-awareinteractivelidarsamplingfordeepdepthcompletion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Uncertainty-Aware Interactive LiDAR Sampling for Deep Depth Completion",
    "authors": [
      "Kensuke Taguchi",
      "Shogo Morita",
      "Yusuke Hayashi",
      "Wataru Imaeda",
      "Hironobu Fujiyoshi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Taguchi_Uncertainty-Aware_Interactive_LiDAR_Sampling_for_Deep_Depth_Completion_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Taguchi_Uncertainty-Aware_Interactive_LiDAR_Sampling_for_Deep_Depth_Completion_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Programmable scan LiDAR is able to measure arbitrary areas and is expected to be used in various applications. In this paper, we study a LiDAR sampling strategy for deep depth completion of a programmable scan LiDAR with an RGB camera. General data sampling strategies include adaptive approaches such as active learning, in which candidate data are assessed through a task model for data selection and then the selected data pool is updated sequentially. Although it is an effective approach, the adaptive approach requires many iterations involving the inference process to assess the candidate data, which is not suitable for LiDAR systems. Therefore, we propose a novel interactive LiDAR sampling method without each inference process. Our key insights are that we assess sampling candidates by depth estimation uncertainty and virtually update the uncertainty by an approximation of the candidate assessment. This enables us to add interactivity to the model state without requiring each inference process. We demonstrate the effectiveness of our method on the KITTI dataset and the generalization performance on the NYU-Depth-v2 dataset in comparison with a conventional adaptive LiDAR sampling method, and we find superior results in the depth completion task. We also show ablation studies to analyze our approach."
  },
  "wacv2023_main_contrastiveknowledge-augmentedmeta-learningforfew-shotclassification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Contrastive Knowledge-Augmented Meta-Learning for Few-Shot Classification",
    "authors": [
      "Rakshith Subramanyam",
      "Mark Heimann",
      "T.S. Jayram",
      "Rushil Anirudh",
      "Jayaraman J. Thiagarajan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Subramanyam_Contrastive_Knowledge-Augmented_Meta-Learning_for_Few-Shot_Classification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Subramanyam_Contrastive_Knowledge-Augmented_Meta-Learning_for_Few-Shot_Classification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Model agnostic meta-learning algorithms aim to infer priors from several observed tasks that can then be used to adapt to a new task with few examples. Given the inherent diversity of tasks arising in existing benchmarks, recent methods have resorted to task-specific adaptation of the prior. Our goal is to improve generalization of meta learners when the task distribution contains challenging distribution shifts and semantic disparities. To this end, we introduce CAML (Contrastive Knowledge-Augmented Meta Learning), a knowledge-enhanced few-shot learning approach that evolves a knowledge graph to encode historical experience, and employs a contrastive distillation strategy to leverage the encoded knowledge for task-aware modulation of the base learner. In addition to the standard few-shot task adaptation, we also consider the more challenging multi-domain task adaptation and few-shot dataset generalization settings in our evaluation with standard benchmarks. Our empirical study shows that CAML (i) enables simple task encoding schemes; (ii) eliminates the need for knowledge extraction at inference time; and most importantly, (iii) effectively aggregates historical experience thus leading to improved performance in both multi-domain adaptation and dataset generalization."
  },
  "wacv2023_main_rsfoptimizingrigidsceneflowfrom3dpointcloudswithoutlabels": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "RSF: Optimizing Rigid Scene Flow From 3D Point Clouds Without Labels",
    "authors": [
      "David Deng",
      "Avideh Zakhor"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Deng_RSF_Optimizing_Rigid_Scene_Flow_From_3D_Point_Clouds_Without_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Deng_RSF_Optimizing_Rigid_Scene_Flow_From_3D_Point_Clouds_Without_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present a method for optimizing object-level rigid 3D scene flow over two successive point clouds without any annotated labels in autonomous driving settings. Rather than using pointwise flow vectors, our approach represents scene flow as the composition a global ego-motion and a set of bounding boxes with their own rigid motions, exploiting the multi-body rigidity commonly present in dynamic scenes. We jointly optimize these parameters over a novel loss function based on the nearest neighbor distance using a differentiable bounding box formulation. Our approach achieves state-of-the-art accuracy on KITTI Scene Flow and nuScenes without requiring any annotations, outperforming even supervised methods. Additionally, we demonstrate the effectiveness of our approach on motion segmentation and ego-motion estimation. Lastly, we visualize our predictions and validate our loss function design with an ablation study."
  },
  "wacv2023_main_semi-supervisedlearningforlow-lightimagerestorationthroughqualityassistedpseudo-labeling": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Semi-Supervised Learning for Low-Light Image Restoration Through Quality Assisted Pseudo-Labeling",
    "authors": [
      "Sameer Malik",
      "Rajiv Soundararajan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Malik_Semi-Supervised_Learning_for_Low-Light_Image_Restoration_Through_Quality_Assisted_Pseudo-Labeling_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Malik_Semi-Supervised_Learning_for_Low-Light_Image_Restoration_Through_Quality_Assisted_Pseudo-Labeling_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Convolutional neural networks have been successful in restoring images captured under poor illumination conditions addressing multiple challenges such as contrast enhancement, denoising, and color cast removal. Nevertheless, such approaches require a large number of paired low-light and ground truth images for training. Thus, we study the problem of semi-supervised learning for low-light image restoration when limited low-light images have ground truth labels. Our main contributions in this work are twofold. We first deploy an ensemble of low-light restoration networks to restore the unlabeled images and generate a set of potential pseudo-labels. We model the contrast distortions in the labeled set to generate different sets of training data and create the ensemble of networks. We then design a contrastive self-supervised learning based image quality measure to obtain the pseudo-label among the images restored by the ensemble. We show that training the restoration network with the pseudo-labels allows us to achieve excellent restoration performance even with very few labeled pairs. We conduct extensive experiments on three popular low-light image restoration datasets to show the superior performance of our semi-supervised low-light image restoration compared to other approaches."
  },
  "wacv2023_main_select,label,andmixlearningdiscriminativeinvariantfeaturerepresentationsforpartialdomainadaptation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation",
    "authors": [
      "Aadarsh Sahoo",
      "Rameswar Panda",
      "Rogerio Feris",
      "Kate Saenko",
      "Abir Das"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sahoo_Select_Label_and_Mix_Learning_Discriminative_Invariant_Feature_Representations_for_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sahoo_Select_Label_and_Mix_Learning_Discriminative_Invariant_Feature_Representations_for_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability, and domain invariance in the latent space. To alleviate the above issues, we develop a novel 'Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. First, we present an efficient \"select\" module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the \"label\" module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space. Finally, the \"mix\" module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed framework over state-of-the-art methods. Project page: https://cvir.github.io/projects/slm."
  },
  "wacv2023_main_weakly-supervisedopticalflowestimationfortime-of-flight": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Weakly-Supervised Optical Flow Estimation for Time-of-Flight",
    "authors": [
      "Michael Schelling",
      "Pedro Hermosilla",
      "Timo Ropinski"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Schelling_Weakly-Supervised_Optical_Flow_Estimation_for_Time-of-Flight_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Schelling_Weakly-Supervised_Optical_Flow_Estimation_for_Time-of-Flight_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Indirect Time-of-Flight (iToF) cameras are a widespread type of 3D sensor, which perform multiple captures to obtain depth values of the captured scene. While recent approaches to correct iToF depths achieve high performance when removing multi-path-interference and sensor noise, little research has been done to tackle motion artifacts. In this work we propose a training algorithm, which allows to supervise Optical Flow (OF) networks directly on the reconstructed depth, without the need of having ground truth flows. We demonstrate that this approach enables the training of OF networks to align raw iToF measurements and compensate motion artifacts in the iToF depth images. The approach is evaluated for both single- and multi-frequency sensors as well as multi-tap sensors, and is able to outperform other motion compensation techniques."
  },
  "wacv2023_main_thor-netend-to-endgraformer-basedrealistictwohandsandobjectreconstructionwithself-supervision": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "THOR-Net: End-to-End Graformer-Based Realistic Two Hands and Object Reconstruction With Self-Supervision",
    "authors": [
      "Ahmed Tawfik Aboukhadra",
      "Jameel Malik",
      "Ahmed Elhayek",
      "Nadia Robertini",
      "Didier Stricker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Aboukhadra_THOR-Net_End-to-End_Graformer-Based_Realistic_Two_Hands_and_Object_Reconstruction_With_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Aboukhadra_THOR-Net_End-to-End_Graformer-Based_Realistic_Two_Hands_and_Object_Reconstruction_With_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Realistic reconstruction of two hands interacting with objects is a new and challenging problem that is essential for building personalized Virtual and Augmented Reality environments. Graph Convolutional networks (GCNs) allow for the preservation of the topologies of hands poses and shapes by modeling them as a graph. In this work, we propose the THOR-Net which combines the power of GCNs, Transformer, and self-supervision to realistically reconstruct two hands and an object from a single RGB image. Our network comprises two stages; namely the features extraction stage and the reconstruction stage. In the features extraction stage, a Keypoint RCNN is used to extract 2D poses, features maps, heatmaps, and bounding boxes from a monocular RGB image. Thereafter, this 2D information is modeled as two graphs and passed to the two branches of the reconstruction stage. The shape reconstruction branch estimates meshes of two hands and an object using our novel coarse-to-fine GraFormer shape network. The 3D poses of the hands and objects are reconstructed by the other branch using a GraFormer network. Finally, a self-supervised photometric loss is used to directly regress the realistic textured of each vertex in the hands' meshes. Our approach achieves State-of-the-art results in Hand shape estimation on the HO3D dataset (10.0mm) exceeding ArtiBoost (10.8mm). It also surpasses other methods in hand pose estimation on the challenging two hands and object (H2O) dataset by 5mm on the left-hand pose and 1 mm on the right-hand pose. The code base of THOR-Net will be released soon under https://github.com/ATAboukhadra/THOR-Net."
  },
  "wacv2023_main_discretecosintransformerimagemodelingfromfrequencydomain": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Discrete Cosin TransFormer: Image Modeling From Frequency Domain",
    "authors": [
      "Xinyu Li",
      "Yanyi Zhang",
      "Jianbo Yuan",
      "Hanlin Lu",
      "Yibo Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Li_Discrete_Cosin_TransFormer_Image_Modeling_From_Frequency_Domain_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Li_Discrete_Cosin_TransFormer_Image_Modeling_From_Frequency_Domain_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we propose Discrete Cosin TransFormer (DCFormer) that directly learn semantics from DCT-based frequency domain representation. We first show that transformer-based networks are able to learn semantics directly from frequency domain representation based on discrete cosine transform (DCT) without compromising the performance. To achieve the desired efficiency-effectiveness trade-off, we then leverage an input information compression on its frequency domain representation, which highlights the visually significant signals inspired by JPEG compression. We explore different frequency domain down-sampling strategies and show that it is possible to preserve the semantic meaningful information by strategically dropping the high-frequency components. The proposed DCFormer is tested on various downstream tasks including image classification, object detection and instance segmentation, and achieves state-of-the-art comparable performance with less FLOPs, and outperforms the commonly used backbone (e.g. SWIN) at similar FLOPs. Our ablation results also show that the proposed method generalizes well on different transformer backbones."
  },
  "wacv2023_main_intra-sourcestyleaugmentationforimproveddomaingeneralization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Intra-Source Style Augmentation for Improved Domain Generalization",
    "authors": [
      "Yumeng Li",
      "Dan Zhang",
      "Margret Keuper",
      "Anna Khoreva"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Li_Intra-Source_Style_Augmentation_for_Improved_Domain_Generalization_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Li_Intra-Source_Style_Augmentation_for_Improved_Domain_Generalization_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to 12.4% mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by 3% mIoU in Cityscapes to Dark Zurich."
  },
  "wacv2023_main_rnas-merarefinedneuralarchitecturesearchwithhybridspatiotemporaloperationsformicro-expressionrecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "RNAS-MER: A Refined Neural Architecture Search With Hybrid Spatiotemporal Operations for Micro-Expression Recognition",
    "authors": [
      "Monu Verma",
      "Priyanka Lubal",
      "Santosh Kumar Vipparthi",
      "Mohamed Abdel-Mottaleb"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Verma_RNAS-MER_A_Refined_Neural_Architecture_Search_With_Hybrid_Spatiotemporal_Operations_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Verma_RNAS-MER_A_Refined_Neural_Architecture_Search_With_Hybrid_Spatiotemporal_Operations_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Existing NAS methods comprise linear connected convolutional operations and used ample search space to search task-driven convolution neural networks (CNN). These CNN models are computationally expensive and diminish the quality of receptive fields for tasks like micro-expression recognition (MER) with limited training samples. Therefore, we proposed a refined neural architecture search strategy to search a tiny CNN architecture for MER. In addition, we introduced a refined hybrid module (RHM) for innerlevel search space and an optimal path explore network (OPEN) for outer-level search. The RHM focuses on discovering optimal cell structures by incorporating a multilateral hybrid spatiotemporal operation space. Also, spatiotemporal attention blocks are embedded to refine the aggregated cell features. The OPEN search space aims to trace an optimal path between the cells to generate a tiny spatiotemporal CNN architecture instead of covering all possible tracks. The aggregate mix of RHM and OPEN search space availed the NAS method to robustly search and design an effective and efficient framework for MER. Compared with contemporary works, experiments reveal that the RNAS-MER is capable of bridging the gap between NAS algorithms and MER tasks. Further, RNAS-MER achieves new state-of-the-art performances on challenging MER benchmarks, including % on CASME-2, % SMIC, % SAMM, and % on COMPOSITE."
  },
  "wacv2023_main_hiformerhierarchicalmulti-scalerepresentationsusingtransformersformedicalimagesegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "HiFormer: Hierarchical Multi-Scale Representations Using Transformers for Medical Image Segmentation",
    "authors": [
      "Moein Heidari",
      "Amirhossein Kazerouni",
      "Milad Soltany",
      "Reza Azad",
      "Ehsan Khodapanah Aghdam",
      "Julien Cohen-Adad",
      "Dorit Merhof"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Heidari_HiFormer_Hierarchical_Multi-Scale_Representations_Using_Transformers_for_Medical_Image_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Heidari_HiFormer_Hierarchical_Multi-Scale_Representations_Using_Transformers_for_Medical_Image_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Convolutional neural networks (CNNs) have been the consensus for medical image segmentation tasks. However, they inevitably suffer from the limitation in modeling long-range dependencies and spatial correlations due to the nature of convolution operation. Although Transformers were first developed to address this issue, they fail to capture low-level features. In contrast, it is demonstrated that both local and global features are crucial for dense prediction, such as segmenting in challenging contexts. In this paper, we propose HiFormer, a novel method that efficiently bridges a Convolutional neural network and a Transformer for medical image segmentation. Specifically, we design two multi-scale feature representations using the seminal Swin-Transformer module and a CNN-based encoder. To secure a fine fusion of global and local features obtained from the two aforementioned representations, we propose a Double-Level Fusion (DLF) module in the skip connection of the encoder-decoder outline. Extensive experiments on various medical image segmentation datasets demonstrate the effectiveness of HiFormer over other CNN-based, Transformer-based, and hybrid methods in terms of computational complexity, quantitative and qualitative results"
  },
  "wacv2023_main_expert-definedkeywordsimproveinterpretabilityofretinalimagecaptioning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Expert-Defined Keywords Improve Interpretability of Retinal Image Captioning",
    "authors": [
      "Ting-Wei Wu",
      "Jia-Hong Huang",
      "Joseph Lin",
      "Marcel Worring"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Wu_Expert-Defined_Keywords_Improve_Interpretability_of_Retinal_Image_Captioning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Wu_Expert-Defined_Keywords_Improve_Interpretability_of_Retinal_Image_Captioning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Automatic machine learning-based (ML-based) medical report generation systems for retinal images suffer from a relative lack of interpretability. Hence, such ML-based systems are still not widely accepted. The main reason is that trust is one of the important motivating aspects of interpretability and humans do not trust blindly. Precise technical definitions of interpretability still lack consensus. Hence, it is difficult to make a human-comprehensible ML-based medical report generation system. Heat maps/saliency maps, i.e., post-hoc explanation approaches, are widely used to improve the interpretability of ML-based medical systems. However, they are well known to be problematic. From an ML-based medical model's perspective, the highlighted areas of an image are considered important for making a prediction. However, from a doctor's perspective, even the hottest regions of a heat map contain both useful and non-useful information. Simply localizing the region, therefore, does not reveal exactly what it was in that area that the model considered useful. Hence, the post-hoc explanation-based method relies on humans who probably have a biased nature to decide what a given heat map might mean. Interpretability boosters, in particular expert-defined keywords, are effective carriers of expert domain knowledge and they are human-comprehensible. In this work, we propose to exploit such keywords and a specialized attention-based strategy to build a more human-comprehensible medical report generation system for retinal images. Both keywords and the proposed strategy effectively improve the interpretability. The proposed method achieves state-of-the-art performance under commonly used text evaluation metrics BLEU, ROUGE, CIDEr, and METEOR. Project website: https://github.com/Jhhuangkay/Expert-defined-Keywords-Improve-Interpretability-of-Retinal-Image-Captioning."
  },
  "wacv2023_main_morganmeta-learning-basedfew-shotopen-setrecognitionviagenerativeadversarialnetwork": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MORGAN: Meta-Learning-Based Few-Shot Open-Set Recognition via Generative Adversarial Network",
    "authors": [
      "Debabrata Pal",
      "Shirsha Bose",
      "Biplab Banerjee",
      "Yogananda Jeppu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Pal_MORGAN_Meta-Learning-Based_Few-Shot_Open-Set_Recognition_via_Generative_Adversarial_Network_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Pal_MORGAN_Meta-Learning-Based_Few-Shot_Open-Set_Recognition_via_Generative_Adversarial_Network_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment known-unknowns. A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented known-unknowns or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently."
  },
  "wacv2023_main_fine-grainedaffordanceannotationforegocentrichand-objectinteractionvideos": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fine-Grained Affordance Annotation for Egocentric Hand-Object Interaction Videos",
    "authors": [
      "Zecheng Yu",
      "Yifei Huang",
      "Ryosuke Furuta",
      "Takuma Yagi",
      "Yusuke Goutsu",
      "Yoichi Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yu_Fine-Grained_Affordance_Annotation_for_Egocentric_Hand-Object_Interaction_Videos_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yu_Fine-Grained_Affordance_Annotation_for_Egocentric_Hand-Object_Interaction_Videos_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Object affordance is an important concept in hand-object interaction, providing information on action possibilities based on human motor capacity and objects' physical property thus benefiting tasks such as action anticipation and robot imitation learning. However, the definition of affordance in existing datasets often: 1) mix up affordance with object functionality; 2) confuse affordance with goal-related action; and 3) ignore human motor capacity. This paper proposes an efficient annotation scheme to address these issues by combining goal-irrelevant motor actions and grasp types as affordance labels and introducing the concept of mechanical action to represent the action possibilities between two objects. We provide new annotations by applying this scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as affordance recognition, hand-object interaction hotspots prediction, and cross-domain evaluation of affordance. The results show that models trained with our annotation can distinguish affordance from other concepts, predict fine-grained interaction possibilities on objects, and generalize through different domains."
  },
  "wacv2023_main_end-to-endsingle-frameimagesignalprocessingforhighdynamicrangescenes": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "End-to-End Single-Frame Image Signal Processing for High Dynamic Range Scenes",
    "authors": [
      "Khanh Quoc Dinh",
      "Kwang Pyo Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Dinh_End-to-End_Single-Frame_Image_Signal_Processing_for_High_Dynamic_Range_Scenes_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Dinh_End-to-End_Single-Frame_Image_Signal_Processing_for_High_Dynamic_Range_Scenes_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper considers photography of high dynamic range scenes containing mixtures of shadows and highlights on mobile phones. Multi-frame merging constructs a high-quality image at the cost of capturing multiple frames of the same scene. Contrarily, end-to-end optimized image signal processing (E2EISP) produces an enhanced image from a single-frame Bayer array. This paper combines the merits of the two approaches by using labels of high-quality multi-frame merged images to train E2EISP with a novel neural network architecture composed of a multi-head mixture of brightness enhancement for accurately processing shadows/highlights and a multi-head mixture of image processing featured camera settings of white balance and color correction for a proper color generation. We also proposed a combination of supervised, unsupervised, and generative adversarial losses for brightness, edge, and detail enhancement. Experimental results show that the proposed single-frame ISP produces enhanced images and outperforms state-of-the-art methods."
  },
  "wacv2023_main_indirectadversariallossesviaanintermediatedistributionfortraininggans": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Indirect Adversarial Losses via an Intermediate Distribution for Training GANs",
    "authors": [
      "Rui Yang",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yang_Indirect_Adversarial_Losses_via_an_Intermediate_Distribution_for_Training_GANs_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yang_Indirect_Adversarial_Losses_via_an_Intermediate_Distribution_for_Training_GANs_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this study, we consider the weak convergence characteristics of the Integral Probability Metrics (IPM) methods in training Generative Adversarial Networks (GANs). We first concentrate on a successful IPM-based GAN method that employs a repulsive version of the Maximum Mean Discrepancy (MMD) as the discriminator loss (called repulsive MMD-GAN). We reinterpret its repulsive metrics as an indirect discriminator loss function toward an intermediate distribution. This allows us to propose a novel generator loss via such an intermediate distribution based on our reinterpretation. Our indirect adversarial losses use a simple known distribution (i.e., the Normal or Uniform distribution in our experiments) to simulate indirect adversarial learning between three parts -- real, fake, and intermediate distributions. Furthermore, we found the Kernelized Stein Discrepancy (KSD) from the IPM family as the adversarial loss function to avoid randomness from intermediate distribution samples because the target side (intermediate one) is sample-free in KSD. Experiments on several real-world datasets show that our methods can successfully train GANs with the intermediate-distribution-based KSD and MMD and can outperform previous loss metrics."
  },
  "wacv2023_main_gaf-netimprovingtheperformanceofremotesensingimagefusionusingnovelglobalselfandcrossattentionlearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "GAF-Net: Improving the Performance of Remote Sensing Image Fusion Using Novel Global Self and Cross Attention Learning",
    "authors": [
      "Ankit Jha",
      "Shirsha Bose",
      "Biplab Banerjee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Jha_GAF-Net_Improving_the_Performance_of_Remote_Sensing_Image_Fusion_Using_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Jha_GAF-Net_Improving_the_Performance_of_Remote_Sensing_Image_Fusion_Using_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The notion of self and cross-attention learning has been found to substantially boost the performance of remote sensing (RS) image fusion. However, while the self-attention models fail to incorporate the global context due to the limited size of the receptive fields, cross-attention learning may generate ambiguous features as the feature extractors for all the modalities are jointly trained. This results in the generation of redundant multi-modal features, thus limiting the fusion performance. To address these issues, we propose a novel fusion architecture called Global Attention based Fusion Network (GAF-Net), equipped with novel self and cross-attention learning techniques. We introduce the within-modality feature refinement module through global spectral-spatial attention learning using the query-key-value processing where both the global spatial and channel contexts are used to generate two channel attention masks. Since it is non-trivial to generate the cross-attention from within the fusion network, we propose to leverage two auxiliary tasks of modality-specific classification to produce highly discriminative cross-attention masks. Finally, to ensure non-redundancy, we propose to penalize the high correlation between attended modality-specific features. Our extensive experiments on five benchmark datasets, including optical, multispectral (MS), hyperspectral (HSI), light detection and ranging (LiDAR), synthetic aperture radar (SAR), and audio modalities establish the superiority of GAF-Net concerning the literature."
  },
  "wacv2023_main_fromforkstoforcepsanewframeworkforinstancesegmentationofsurgicalinstruments": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments",
    "authors": [
      "Britty Baby",
      "Daksh Thapar",
      "Mustafa Chasmai",
      "Tamajit Banerjee",
      "Kunal Dargan",
      "Ashish Suri",
      "Subhashis Banerjee",
      "Chetan Arora"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Baby_From_Forks_to_Forceps_A_New_Framework_for_Instance_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Baby_From_Forks_to_Forceps_A_New_Framework_for_Instance_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head misclassifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training the proposed classifier module using metric learning with arc loss to handle low inter-class variance of surgical instruments. We conduct exhaustive experiments on the benchmark datasets EndoVis2017 and EndoVis2018. We demonstrate that our method outperforms all (more than 18) SOTA methods compared with and improves the \\sota performance by at least 12 points (20%) on the EndoVis2017 benchmark challenge and generalizes effectively across the datasets. Project page with source code is available at nets-iitd.github.io/s3net."
  },
  "wacv2023_main_harnessingunrecognizablefacesforimprovingfacerecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Harnessing Unrecognizable Faces for Improving Face Recognition",
    "authors": [
      "Siqi Deng",
      "Yuanjun Xiong",
      "Meng Wang",
      "Wei Xia",
      "Stefano Soatto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Deng_Harnessing_Unrecognizable_Faces_for_Improving_Face_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Deng_Harnessing_Unrecognizable_Faces_for_Improving_Face_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The common implementation of face recognition systems as a cascade of a detection stage and a recognition or verification stage can cause problems beyond failures of the detector. When the detector succeeds, it can detect faces that cannot be recognized, no matter how capable the recognition system is. Recognizability, a latent variable, should therefore be factored into the design and implementation of face recognition systems. We propose a measure of recognizability of a face image that leverages a key empirical observation: An embedding of face images, implemented by a deep neural network trained using mostly recognizable identities, induces a partition of the hypersphere whereby unrecognizable identities cluster together. This occurs regardless of the phenomenon that causes a face to be unrecognizable, be it optical or motion blur, partial occlusion, spatial quantization, or poor illumination. Therefore, we use the distance from such an \"unrecognizable identity\" as a measure of recognizability, and incorporate it into the design of the overall system. We show that accounting for recognizability reduces the error rate of single-image face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification benchmark, and reduces the verification error rate by 24% at FAR=1e-5 in set-based recognition on the IJB-C benchmark."
  },
  "wacv2023_main_multi-viewactionrecognitionusingcontrastivelearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Multi-View Action Recognition Using Contrastive Learning",
    "authors": [
      "Ketul Shah",
      "Anshul Shah",
      "Chun Pong Lau",
      "Celso M. de Melo",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Shah_Multi-View_Action_Recognition_Using_Contrastive_Learning_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Shah_Multi-View_Action_Recognition_Using_Contrastive_Learning_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this work, we present a method for RGB-based action recognition using multi-view videos. We present a supervised contrastive learning framework to learn a feature embedding robust to changes in viewpoint, by effectively leveraging multi-view data. We use an improved supervised contrastive loss and augment the positives with those coming from synchronized viewpoints. We also propose a new approach to use classifier probabilities to guide the selection of hard negatives in the contrastive loss, to learn a more discriminative representation. Negative samples from confusing classes based on posterior are weighted higher. We also show that our method leads to better domain generalization compared to the standard supervised training based on synthetic multi-view data. Extensive experiments on real (NTU-60, NTU-120, NUMA) and synthetic (RoCoG) data demonstrate the effectiveness of our approach."
  },
  "wacv2023_main_tvttransferablevisiontransformerforunsuperviseddomainadaptation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation",
    "authors": [
      "Jinyu Yang",
      "Jingjing Liu",
      "Ning Xu",
      "Junzhou Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yang_TVT_Transferable_Vision_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yang_TVT_Transferable_Vision_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViT's intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned transferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods."
  },
  "wacv2023_main_interpolatedselectionconvforsphericalimagesandsurfaces": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Interpolated SelectionConv for Spherical Images and Surfaces",
    "authors": [
      "David Hart",
      "Michael Whitney",
      "Bryan Morse"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hart_Interpolated_SelectionConv_for_Spherical_Images_and_Surfaces_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hart_Interpolated_SelectionConv_for_Spherical_Images_and_Surfaces_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present a new and general framework for convolutional neural network operations on spherical (or omnidirectional) images. Our approach represents the surface as a graph of connected points that doesn't rely on a particular sampling strategy. Additionally, by using an interpolated version of SelectionConv, we can operate on the sphere while using existing 2D CNNs and their weights. Since our method leverages existing graph implementations, it is also fast and can be fine-tuned efficiently. Our method is also general enough to be applied to any surface type, even those that are topologically non-simple. We demonstrate the effectiveness of our technique on the tasks of style transfer and segmentation for spheres as well as stylization for 3D meshes. We provide a thorough ablation study of the performance of various spherical sampling strategies."
  },
  "wacv2023_main_matchcuttingfindingcutswithsmoothvisualtransitions": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Match Cutting: Finding Cuts With Smooth Visual Transitions",
    "authors": [
      "Boris Chen",
      "Amir Ziai",
      "Rebecca S. Tucker",
      "Yuchen Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Match_Cutting_Finding_Cuts_With_Smooth_Visual_Transitions_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Match_Cutting_Finding_Cuts_With_Smooth_Visual_Transitions_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "A match cut is a transition between a pair of shots that uses similar framing, composition, or action to fluidly bring the viewer from one scene to the next. Match cuts are frequently used in film, television, and advertising. However, finding shots that work together is a highly manual and time-consuming process that can take days. We propose a modular and flexible system to efficiently find high-quality match cut candidates starting from millions of shot pairs. We annotate and release a dataset of approximately 20,000 labeled pairs that we use to evaluate our system, using both classification and metric learning approaches that leverage a variety of image, video, audio, and audio-visual feature extractors. In addition, we release code and embeddings for reproducing our experiments at github.com/netflix/matchcut."
  },
  "wacv2023_main_arestraight-throughgradientsandsoft-thresholdingallyouneedforsparsetraining?": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Are Straight-Through Gradients and Soft-Thresholding All You Need for Sparse Training?",
    "authors": [
      "Antoine Vanderschueren",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Vanderschueren_Are_Straight-Through_Gradients_and_Soft-Thresholding_All_You_Need_for_Sparse_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Vanderschueren_Are_Straight-Through_Gradients_and_Soft-Thresholding_All_You_Need_for_Sparse_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Turning the weights to zero when training a neural network helps in reducing the computational complexity at inference. To progressively increase the sparsity ratio in the network without causing sharp weight discontinuities during training, our work combines soft-thresholding and straight-through gradient estimation to update the raw, i.e. non-thresholded, version of zeroed weights. Our method, named ST-3 for straight-through/soft-thresholding/sparse-training, obtains SoA results, both in terms of accuracy/sparsity and accuracy/FLOPS trade-offs, when progressively increasing the sparsity ratio in a single training cycle. In particular, despite its simplicity, ST-3 favorably compares to the most recent methods, adopting differentiable formulations or bio-inspired neuroregeneration principles. This suggests that the key ingredients for effective sparsification primarily lie in the ability to give the weights the freedom to evolve smoothly across the zero state while progressively increasing the sparsity ratio. Source code and weights available at https://github.com/vanderschuea/stthree."
  },
  "wacv2023_main_fastandaccuratevideoenhancementusingsparsedepth": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fast and Accurate: Video Enhancement Using Sparse Depth",
    "authors": [
      "Yu Feng",
      "Patrick Hansen",
      "Paul N. Whatmough",
      "Guoyu Lu",
      "Yuhao Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Feng_Fast_and_Accurate_Video_Enhancement_Using_Sparse_Depth_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Feng_Fast_and_Accurate_Video_Enhancement_Using_Sparse_Depth_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper presents a general framework to build fast and accurate algorithms for video enhancement tasks such as super-resolution, deblurring, and denoising. Essential to our framework is the realization that the accuracy, rather than the density, of pixel flows is what is required for high-quality video enhancement. Most of prior works take the opposite approach: they estimate dense (per-pixel)--but generally less robust--flows, mostly using computationally costly algorithms. Instead, we propose a lightweight flow estimation algorithm; it fuses the sparse point cloud data and (even sparser and less reliable) IMU data available in modern autonomous agents to estimate the flow information. Building on top of the flow estimation, we demonstrate a general framework that integrates the flows in a plug-and-play fashion with different task-specific layers. Algorithms built in our framework achieve 1.78x -- 187.41x speedup while providing a 0.42dB - 6.70 dB quality improvement over competing methods."
  },
  "wacv2023_main_sgpcrsphericalgaussianpointcloudrepresentationanditsapplicationtoobjectregistrationandretrieval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SGPCR: Spherical Gaussian Point Cloud Representation and Its Application To Object Registration and Retrieval",
    "authors": [
      "Driton Salihu",
      "Eckehard Steinbach"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Salihu_SGPCR_Spherical_Gaussian_Point_Cloud_Representation_and_Its_Application_To_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Salihu_SGPCR_Spherical_Gaussian_Point_Cloud_Representation_and_Its_Application_To_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Retrieving and aligning CAD models from databases with scanned real-world point clouds remains an important topic for 3D reconstruction. Due to zero point-to-point correspondences between the sampled CAD model and the scanned real-world object, an information-rich representation of point clouds is needed. We propose SGPCR, a novel method for representing 3D point clouds by Spherical Gaussians for efficient, stable, and rotation-equivariant representation. We also propose a rotation-invariant convolution to improve the representation quality through a trainable optimization process. In addition, we demonstrate the strengths of SGPCR-based point cloud representation using the fundamental challenge of shape retrieval and point cloud registration on point clouds with zero point-to-point correspondences. Under these conditions, our approach improves registration quality by reducing chamfer distance by up to 90% and rotation root mean square error by up to 86% compared to the state of the art. Furthermore, the proposed SGCPR is used for one-shot shape retrieval and registration and improves retrieval precision by up to 58% over comparable methods."
  },
  "wacv2023_main_nesteddeformablemulti-headattentionforfacialimageinpainting": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Nested Deformable Multi-Head Attention for Facial Image Inpainting",
    "authors": [
      "Shruti S. Phutke",
      "Subrahmanyam Murala"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Phutke_Nested_Deformable_Multi-Head_Attention_for_Facial_Image_Inpainting_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Phutke_Nested_Deformable_Multi-Head_Attention_for_Facial_Image_Inpainting_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Extracting adequate contextual information is an important aspect of any image inpainting method. To achieve this, ample image inpainting methods are available that aim to focus on large receptive fields. Recent advancements in the deep learning field with the introduction of transformers for image inpainting paved the way toward plausible results. Stacking multiple transformer blocks in a single layer causes the architecture to become computationally complex. In this context, we propose a novel lightweight architecture with a nested deformable attention based transformer layer for feature fusion. The nested attention helps the network to focus on long-term dependencies from encoder and decoder features. Also, multi head attention consisting of a deformable convolution is proposed to delve into the diverse receptive fields. With the advantage of nested and deformable attention, we propose a lightweight architecture for facial image inpainting. The results comparison on Celeb HQ [25] dataset using known (NVIDIA) and unknown (QD-IMD) masks and Places2 [57] dataset with NVIDIA masks along with extensive ablation study prove the superiority of the proposed approach for image inpainting tasks. The code is available at: https://github.com/shrutiphutke/NDMA_ Facial_Inpainting."
  },
  "wacv2023_main_fashionimageretrievalwithtextfeedbackbyadditiveattentioncompositionallearning": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fashion Image Retrieval With Text Feedback by Additive Attention Compositional Learning",
    "authors": [
      "Yuxin Tian",
      "Shawn Newsam",
      "Kofi Boakye"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Tian_Fashion_Image_Retrieval_With_Text_Feedback_by_Additive_Attention_Compositional_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Tian_Fashion_Image_Retrieval_With_Text_Feedback_by_Additive_Attention_Compositional_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Effective fashion image retrieval with text feedback stands to impact a range of real-world applications, such as e-commerce. Given a source image and text feedback that describes the desired modifications to that image, the goal is to retrieve the target images that resemble the source yet satisfy the given modifications by composing a multi-modal (image-text) query. We propose a novel solution to this problem, Additive Attention Compositional Learning (AACL), that uses a multi-modal transformer-based architecture and effectively models the image-text contexts. Specifically, we propose a novel image-text composition module based on additive attention that can be seamlessly plugged into deep neural networks. We also introduce a new challenging benchmark derived from the Shopping100k dataset. AACL is evaluated on three large-scale datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong baselines. Extensive experiments show that AACL achieves new state-of-the-art results on all three datasets."
  },
  "wacv2023_main_center-awareadversarialaugmentationforsingledomaingeneralization": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Center-Aware Adversarial Augmentation for Single Domain Generalization",
    "authors": [
      "Tianle Chen",
      "Mahsa Baktashmotlagh",
      "Zijian Wang",
      "Mathieu Salzmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Center-Aware_Adversarial_Augmentation_for_Single_Domain_Generalization_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Center-Aware_Adversarial_Augmentation_for_Single_Domain_Generalization_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Domain generalization (DG) aims to learn a model from multiple training (i.e., source) domains that can generalize well to the unseen test (i.e., target) data coming from a different distribution. Single domain generalization (Single-DG) has recently emerged to tackle a more challenging, yet realistic setting, where only one source domain is available at training time. The existing Single-DG approaches typically are based on data augmentation strategies and aim to expand the span of source data by augmenting out-of-domain samples. Generally speaking, they aim to generate hard examples to confuse the classifier. While this may make the classifier robust to small perturbation, the generated samples are typically not diverse enough to mimic a large domain shift, resulting in sub-optimal generalization performance To alleviate this, we propose a center-aware adversarial augmentation technique that expands the source distribution by altering the source samples so as to push them away from the class centers via a novel angular center loss. We conduct extensive experiments to demonstrate the effectiveness of our approach on several benchmark datasets for Single-DG and show that our method outperforms the state-of-the-art in most cases."
  },
  "wacv2023_main_sd-posestructuraldiscrepancyawarecategory-level6dobjectposeestimation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SD-Pose: Structural Discrepancy Aware Category-Level 6D Object Pose Estimation",
    "authors": [
      "Guowei Li",
      "Dongchen Zhu",
      "Guanghui Zhang",
      "Wenjun Shi",
      "Tianyu Zhang",
      "Xiaolin Zhang",
      "Jiamao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Category-level 6D object pose estimation aims to predict the full pose and size information for previously unseen instances from known categories, which is an essential portion of robot grasping and augmented reality. However, the core challenge of this task still is the enormous shape variation within each category. With regard to the challenge, we propose a novel framework SD-Pose, which utilizes the instance-category structural discrepancy and the potential geometric-semantic association to enhance the exploration of the intra-class shape information. Specifically, an information exchange augmentation (IEA) module is introduced to supplement the instance-category structural information by their structural discrepancy, thus facilitating the enhanced geometric information to contain both the character of instance shape and the commonality of category structure. For complementing the deficiencies of structural information adaptively, a semantic dynamic fusion (SDF) module is further designed to fuse semantic and geometric features. Finally, the proposed SD-Pose framework equipped with the IEA and SDF modules hierarchically supplements instance-category structural information in a stacked manner and achieves state-of-the-art performance on the CAMERA25 and REAL275 datasets."
  },
  "wacv2023_main_floatfastlearnableonce-for-alladversarialtrainingfortunabletrade-offbetweenaccuracyandrobustness": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "FLOAT: Fast Learnable Once-for-All Adversarial Training for Tunable Trade-Off Between Accuracy and Robustness",
    "authors": [
      "Souvik Kundu",
      "Sairam Sundaresan",
      "Massoud Pedram",
      "Peter A. Beerel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kundu_FLOAT_Fast_Learnable_Once-for-All_Adversarial_Training_for_Tunable_Trade-Off_Between_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kundu_FLOAT_Fast_Learnable_Once-for-All_Adversarial_Training_for_Tunable_Trade-Off_Between_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Existing models that achieve state-of-the-art (SOTA) performance on both clean and adversarially-perturbed images rely on convolution operations conditioned with feature-wise linear modulation (FiLM) layers. These layers require additional parameters and are hyperparameter sensitive. They significantly increase training time, memory cost, and potential latency which can be costly for resource-limited or real-time applications. In this paper, we present a fast learnable once-for-all adversarial training (FLOAT) algorithm, which instead of the existing FiLM-based conditioning, presents a unique weight conditioned learning that requires no additional layer, thereby incurring no significant increase in parameter count, training time, or network latency compared to standard adversarial training. In particular, we add configurable scaled noise to the weight tensors that enables a trade-off between clean and adversarial performance. Extensive experiments show that FLOAT can yield SOTA performance improving both clean and perturbed image classification by up to6% and10%, respectively. Moreover, real hardware measurement shows that FLOAT can reduce the training time by up to 1.43x with fewer model parameters of up to 1.47x on iso-hyperparameter settings compared to the FiLM-based alternatives. Additionally, to further improve memory efficiency we introduce FLOAT sparse (FLOATS), a form of non-iterative model pruning, and provide detailed empirical analysis in yielding a three-way accuracy-robustness-complexity trade-off for these new class of pruned conditionally trained models."
  },
  "wacv2023_main_spatio-temporalactiondetectionunderlargemotion": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Spatio-Temporal Action Detection Under Large Motion",
    "authors": [
      "Gurkirt Singh",
      "Vasileios Choutas",
      "Suman Saha",
      "Fisher Yu",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Singh_Spatio-Temporal_Action_Detection_Under_Large_Motion_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Singh_Spatio-Temporal_Action_Detection_Under_Large_Motion_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Current methods for spatiotemporal action tube detection often extend a bounding box proposal at a given key-frame into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatiotemporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to the cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset."
  },
  "wacv2023_main_sli-pspinjectingmulti-scalespatiallayoutinpsp": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "SLI-pSp: Injecting Multi-Scale Spatial Layout in pSp",
    "authors": [
      "Aradhya Neeraj Mathur",
      "Anish Madan",
      "Ojaswa Sharma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mathur_SLI-pSp_Injecting_Multi-Scale_Spatial_Layout_in_pSp_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mathur_SLI-pSp_Injecting_Multi-Scale_Spatial_Layout_in_pSp_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We propose SLI-pSp, a general purpose Image-to-Image (I2I) translation model that encodes spatial layout information as well as style in the generator, using pSp as the base architecture. Previous methods like pSp have shown promising results by leveraging StyleGAN as a generator in various I2I tasks but they seem to miss finer or under-represented details in facial images like earrings and caps, and break down on complex datasets due to their solely global approach. To address these shortcomings, we propose a technique termed Spatial Layout Injection (SLI-pSp) that encodes spatial layout information in the input image in the StyleGAN generator along with style. We do so without modifying the style vector injection in the generator through pSp's map2style network, but rather by combining SLI with noise layers in the StyleGAN generator at multiple spatial scales. Such an approach helps preserve global aspects of image generation as well as enhance spatial layout details in the output. We experiment on several challenging datasets and across several I2I tasks that highlight the effectiveness of our approach over previous methods with respect to finer details in the generated image and overall visual quality."
  },
  "wacv2023_main_anomalyclusteringgroupingimagesintocoherentclustersofanomalytypes": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Anomaly Clustering: Grouping Images Into Coherent Clusters of Anomaly Types",
    "authors": [
      "Kihyuk Sohn",
      "Jinsung Yoon",
      "Chun-Liang Li",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Sohn_Anomaly_Clustering_Grouping_Images_Into_Coherent_Clusters_of_Anomaly_Types_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Sohn_Anomaly_Clustering_Grouping_Images_Into_Coherent_Clusters_of_Anomaly_Types_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We study anomaly clustering, grouping data into coherent clusters of anomaly types. This is different from anomaly detection that aims to divide anomalies from normal data. Unlike object-centered image clustering, anomaly clustering is particularly challenging as anomalous patterns are subtle and local. We present a simple yet effective clustering framework using a patch-based pretrained deep embeddings and off-the-shelf clustering methods. We define a distance function between images, each of which is represented as a bag of embeddings, by the Euclidean distance between weighted averaged embeddings. The weight defines the importance of instances (i.e., patch embeddings) in the bag, which may highlight defective regions. We compute weights in an unsupervised way or in a semi-supervised way when labeled normal data is available. Extensive experimental studies show the effectiveness of the proposed clustering framework along with a novel distance function upon exist-ing multiple instance or deep clustering frameworks. Over-all, our framework achieves 0.451 and 0.674 normalized mutual information scores on MVTec object and texture categories and further improve with a few labeled normal data (0.577, 0.669), far exceeding the baselines (0.244, 0.273) or state-of-the-art deep clustering methods (0.176, 0.277)."
  },
  "wacv2023_main_dsagascalabledeepframeworkforaction-conditionedmulti-actorfullbodymotionsynthesis": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "DSAG: A Scalable Deep Framework for Action-Conditioned Multi-Actor Full Body Motion Synthesis",
    "authors": [
      "Debtanu Gupta",
      "Shubh Maheshwari",
      "Sai Shashank Kalakonda",
      "Manasvi Vaidyula",
      "Ravi Kiran Sarvadevabhatla"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Gupta_DSAG_A_Scalable_Deep_Framework_for_Action-Conditioned_Multi-Actor_Full_Body_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Gupta_DSAG_A_Scalable_Deep_Framework_for_Action-Conditioned_Multi-Actor_Full_Body_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We introduce DSAG, a controllable deep neural framework for action-conditioned generation of full body multi-actor variable duration actions. To compensate for incompletely detailed finger joints in existing large-scale datasets, we introduce full body dataset variants with detailed finger joints. To overcome shortcomings in existing generative approaches, we introduce dedicated representations for encoding finger joints. We also introduce novel spatiotemporal transformation blocks with multi-head self-attention and specialized temporal processing. The design choices enable generations for a large range in body joint counts (24 - 52), frame rates (13 - 50), global body movement (in-place, locomotion) and action categories (12 - 120), across multiple datasets (NTU-120, HumanAct12, UESTC, Human3.6M). Our experimental results demonstrate DSAG's significant improvements over state-of-the-art, its suitability for action-conditioned generation at scale."
  },
  "wacv2023_main_riftdisentangledunsupervisedimagetranslationviarestrictedinformationflow": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "RIFT: Disentangled Unsupervised Image Translation via Restricted Information Flow",
    "authors": [
      "Ben Usman",
      "Dina Bashkirova",
      "Kate Saenko"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Usman_RIFT_Disentangled_Unsupervised_Image_Translation_via_Restricted_Information_Flow_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Usman_RIFT_Disentangled_Unsupervised_Image_Translation_via_Restricted_Information_Flow_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Unsupervised image-to-image translation methods aim to map images from one domain into plausible examples from another domain while preserving the structure shared across two domains. In the many-to-many setting, an additional guidance example from the target domain is used to determine the domain-specific factors of variation of the generated image. In the absence of attribute annotations, methods have to infer which factors of variation are specific to each domain from data during training. In this paper, we show that many state-of-the-art architectures implicitly treat textures and colors as always being domain-specific, and thus fail when they are not. We propose a new method called RIFT that does not rely on such inductive architectural biases and instead infers which attributes are domain-specific vs shared directly from data. As a result, RIFT achieves consistently high cross-domain manipulation accuracy across multiple datasets spanning a wide variety of domain-specific and shared factors of variation."
  },
  "wacv2023_main_adversarialrobustnessindiscontinuousspacesviaalternatingsampling&descent": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Adversarial Robustness in Discontinuous Spaces via Alternating Sampling & Descent",
    "authors": [
      "Rahul Venkatesh",
      "Eric Wong",
      "Zico Kolter"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Venkatesh_Adversarial_Robustness_in_Discontinuous_Spaces_via_Alternating_Sampling__Descent_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Venkatesh_Adversarial_Robustness_in_Discontinuous_Spaces_via_Alternating_Sampling__Descent_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Several works have shown that deep learning models are vulnerable to adversarial attacks where seemingly simple label-preserving changes to the input image lead to incorrect predictions. To combat this, gradient based adversarial training is generally employed as a standard defense mechanism. However, in cases where the loss landscape is discontinuous with respect to a given perturbation set, first order methods get stuck in local optima, and fail to defend against threat. This is often a problem for many physically realizable perturbation sets such as 2D affine transformations and 3D scene parameters. To work in such settings, we introduce a new optimization framework that alternates between global zeroth order sampling and local gradient updates to compute strong adversaries that can be used to harden the model against attack. Further, we design a powerful optimization algorithm using this framework, called Alternating Evolutionary Sampling and Descent (ASD), which combines an evolutionary search strategy (viz. covariance matrix adaptation) with gradient descent. We consider two settings with discontinuous/discrete and non-convex loss landscapes to evaluate ASD: a) 3D scene parameters and b) 2D patch attacks, and find that it achieves state-of-the-art results on adversarial robustness."
  },
  "wacv2023_main_backpropinducedfeatureweightingforadversarialdomainadaptationwithiterativelabeldistributionalignment": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Backprop Induced Feature Weighting for Adversarial Domain Adaptation With Iterative Label Distribution Alignment",
    "authors": [
      "Thomas Westfechtel",
      "Hao-Wei Yeh",
      "Qier Meng",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Westfechtel_Backprop_Induced_Feature_Weighting_for_Adversarial_Domain_Adaptation_With_Iterative_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Westfechtel_Backprop_Induced_Feature_Weighting_for_Adversarial_Domain_Adaptation_With_Iterative_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The requirement for large labeled datasets is one of the limiting factors for training accurate deep neural networks. Unsupervised domain adaptation tackles this problem of limited training data by transferring knowledge from one domain, which has many labeled data, to a different domain for which little to no labeled data is available. One common approach is to learn domain-invariant features for example with an adversarial approach. Previous methods often train the domain classifier and label classifier network separately, where both classification networks have little interaction with each other. In this paper, we introduce a classifier-based backprop-induced weighting of the feature space. This approach has two main advantages. Firstly, it lets the domain classifier focus on features that are important for the classification, and, secondly, it couples the classification and adversarial branch more closely. Furthermore, we introduce an iterative label distribution alignment method, that employs results of previous runs to approximate a class-balanced dataloader. We conduct experiments and ablation studies on three benchmarks Office-31, OfficeHome, and DomainNet to show the effectiveness of our proposed algorithm."
  },
  "wacv2023_main_understandingtheroleofmixupinknowledgedistillationanempiricalstudy": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Understanding the Role of Mixup in Knowledge Distillation: An Empirical Study",
    "authors": [
      "Hongjun Choi",
      "Eun Som Jeon",
      "Ankita Shukla",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Choi_Understanding_the_Role_of_Mixup_in_Knowledge_Distillation_An_Empirical_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Choi_Understanding_the_Role_of_Mixup_in_Knowledge_Distillation_An_Empirical_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Mixup is a popular data augmentation technique based on creating new samples by linear interpolation between two given data samples, to improve both the generalization and robustness of the trained model. Knowledge distillation (KD), on the other hand, is widely used for model compression and transfer learning, which involves using a larger network's implicit knowledge to guide the learning of a smaller network. At first glance, these two techniques seem very different, however, we found that \"smoothness\" is the connecting link between the two and is also a crucial attribute in understanding KD's interplay with mixup. Although many mixup variants and distillation methods have been proposed, much remains to be understood regarding the role of a mixup in knowledge distillation. In this paper, we present a detailed empirical study on various important dimensions of compatibility between mixup and knowledge distillation. We also scrutinize the behavior of the networks trained with a mixup in the light of knowledge distillation through extensive analysis, visualizations, and comprehensive experiments on image classification. Finally, based on our findings, we suggest improved strategies to guide the student network to enhance its effectiveness. Additionally, the findings of this study provide insightful suggestions to researchers and practitioners that commonly use techniques from KD. Our code is available at https://github.com/hchoi71/MIX-KD."
  },
  "wacv2023_main_revisitingtraining-freenasmetricsanefficienttraining-basedmethod": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Revisiting Training-Free NAS Metrics: An Efficient Training-Based Method",
    "authors": [
      "Taojiannan Yang",
      "Linjie Yang",
      "Xiaojie Jin",
      "Chen Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yang_Revisiting_Training-Free_NAS_Metrics_An_Efficient_Training-Based_Method_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yang_Revisiting_Training-Free_NAS_Metrics_An_Efficient_Training-Based_Method_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Recent neural architecture search (NAS) works proposed training-free metrics to rank networks which largely reduced the search cost in NAS. In this paper, we revisit these training-free metrics and find that: (1) the number of parameters (#Param), which is the most straightforward training-free metric, is overlooked in previous works but is surprisingly effective, (2) recent training-free metrics largely rely on the #Param information to rank networks. Our experiments show that the performance of recent training-free metrics drops dramatically when the #Param information is not available. Motivated by these observations, we argue that metrics less correlated with the #Param are desired to provide additional information for NAS. We propose a light-weight training-based metric which has a weak correlation with the #Param while achieving better performance than training-free metrics at a lower search cost. Specifically, on DARTS search space, our method completes searching directly on ImageNet in only 2.6 GPU hours and achieves a top-1/top-5 error rate of 24.1%/7.1%, which is competitive among state-of-the-art NAS methods."
  },
  "wacv2023_main_mfcflowamotionfeaturecompensatedmulti-framerecurrentnetworkforopticalflowestimation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "MFCFlow: A Motion Feature Compensated Multi-Frame Recurrent Network for Optical Flow Estimation",
    "authors": [
      "Yonghu Chen",
      "Dongchen Zhu",
      "Wenjun Shi",
      "Guanghui Zhang",
      "Tianyu Zhang",
      "Xiaolin Zhang",
      "Jiamao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_MFCFlow_A_Motion_Feature_Compensated_Multi-Frame_Recurrent_Network_for_Optical_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_MFCFlow_A_Motion_Feature_Compensated_Multi-Frame_Recurrent_Network_for_Optical_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Occlusions have long been a hard nut to crack in optical flow estimation due to ambiguous pixels matching between abutting images. Current methods only take two consecutive images as input, which is challenging to capture temporal coherence and reason about occluded regions. In this paper, we propose a novel optical flow estimation framework, namely MFCFlow, which attempts to compensate for the information of occlusions by mining and transferring motion features between multiple frames. Specifically, we construct a Motion-guided Feature Compensation cell (MFC cell) to enhance the ambiguous motion features according to the correlation of previous features obtained by attention-based structure. Furthermore, a TopK attention strategy is developed and embedded into the MFC cell to improve the subsequent matching quality. Extensive experiments demonstrate that our MFCFlow achieves significant improvements in occluded regions and attains state-of-the-art performances on both Sintel and KITTI benchmarks among other multi-frame optical flow methods."
  },
  "wacv2023_main_mappingdnnembeddingmanifoldsfornetworkgeneralizationprediction": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Mapping DNN Embedding Manifolds for Network Generalization Prediction",
    "authors": [
      "Molly O\u2019Brien",
      "Brett Wolfinger",
      "Julia Bukowski",
      "Mathias Unberath",
      "Aria Pezeshk",
      "Gregory D. Hager"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/OBrien_Mapping_DNN_Embedding_Manifolds_for_Network_Generalization_Prediction_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/OBrien_Mapping_DNN_Embedding_Manifolds_for_Network_Generalization_Prediction_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep Neural Networks(DNN) often fail in surprising ways, and predicting how well a trained DNN will generalize in a new, external operating domain is essential for deploying DNNs in safety critical applications, e.g., perception for self-driving vehicles or medical image analysis. Recently, the task of Network Generalization Prediction (NGP) has been proposed to predict how a DNN will generalize in an external operating domain. Previous NGP approaches have leveraged multiple labeled test sets or labeled metadata. In this study, we propose an embedding map, the first NGP approach that predicts DNN performance based on how unlabeled images from an external operating domain map in the DNN embedding space. We evaluate our proposed Embedding Map and other recently proposed NGP approaches for pedestrian, melanoma, and animal classification tasks. We find that our embedding map has the best average NGP performance, and that our embedding map is effective at modeling complex, non-linear embedding space structures."
  },
  "wacv2023_main_glitrglimpsetransformerswithspatiotemporalconsistencyforonlineactionprediction": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "GliTr: Glimpse Transformers With Spatiotemporal Consistency for Online Action Prediction",
    "authors": [
      "Samrudhdhi B. Rangrej",
      "Kevin J. Liang",
      "Tal Hassner",
      "James J. Clark"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Rangrej_GliTr_Glimpse_Transformers_With_Spatiotemporal_Consistency_for_Online_Action_Prediction_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Rangrej_GliTr_Glimpse_Transformers_With_Spatiotemporal_Consistency_for_Online_Action_Prediction_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Many online action prediction models observe complete frames to locate and attend to informative subregions in the frames called glimpses and recognize an ongoing action based on global and local information. However, in applications with constrained resources, an agent may not be able to observe the complete frame, yet must still locate useful glimpses to predict an incomplete action based on local information only. In this paper, we develop Glimpse Transformers (GliTr), which observe only narrow glimpses at all times, thus predicting an ongoing action and the following most informative glimpse location based on the partial spatiotemporal information collected so far. In the absence of a ground truth for the optimal glimpse locations for action recognition, we train GliTr using a novel spatiotemporal consistency objective: We require GliTr to attend to the glimpses with features similar to the corresponding complete frames (i.e. spatial consistency) and the resultant class logits at time t equivalent to the ones predicted using whole frames up to t (i.e. temporal consistency). Inclusion of our proposed consistency objective yields10% higher accuracy on the Something-Something-v2 (SSv2) dataset than the baseline cross-entropy objective. Overall, despite observing only33% of the total area per frame, GliTr achieves 53.02% and 93.91% accuracy on the SSv2 and Jester datasets, respectively."
  },
  "wacv2023_main_impdetexploringimplicitfieldsfor3dobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ImpDet: Exploring Implicit Fields for 3D Object Detection",
    "authors": [
      "Xuelin Qian",
      "Li Wang",
      "Yi Zhu",
      "Li Zhang",
      "Yanwei Fu",
      "Xiangyang Xue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Qian_ImpDet_Exploring_Implicit_Fields_for_3D_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Qian_ImpDet_Exploring_Implicit_Fields_for_3D_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Conventional 3D object detection approaches concentrate on bounding boxes representation learning with several parameters, i.e., localization, dimension, and orientation. Despite its popularity and universality, such a straightforward paradigm is sensitive to slight numerical deviations, especially in localization. By exploiting the property that point clouds are naturally captured on the surface of objects along with accurate location and intensity information, we introduce a new perspective that views bounding box regression as an implicit function. This leads to our proposed framework, termed Implicit Detection or ImpDet, which leverages implicit field learning for 3D object detection. Our ImpDet assigns specific values to points in different local 3D spaces, thereby high-quality boundaries can be generated by classifying points inside or outside the boundary. To solve the problem of sparsity on the object surface, we further present a simple yet efficient virtual sampling strategy to not only fill the empty region, but also learn rich semantic features to help refine the boundaries. Extensive experimental results on KITTI and Waymo benchmarks demonstrate the effectiveness and robustness of unifying implicit fields into object detection."
  },
  "wacv2023_main_towardsfew-annotationlearningforobjectdetectionaretransformer-basedmodelsmoreefficient?": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Towards Few-Annotation Learning for Object Detection: Are Transformer-Based Models More Efficient?",
    "authors": [
      "Quentin Bouniot",
      "Ang\u00e9lique Loesch",
      "Romaric Audigier",
      "Amaury Habrard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Bouniot_Towards_Few-Annotation_Learning_for_Object_Detection_Are_Transformer-Based_Models_More_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Bouniot_Towards_Few-Annotation_Learning_for_Object_Detection_Are_Transformer-Based_Models_More_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilities to adapt similar object detection methods in this setup as well."
  },
  "wacv2023_main_domainadaptivevideosemanticsegmentationviacross-domainmovingobjectmixing": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Domain Adaptive Video Semantic Segmentation via Cross-Domain Moving Object Mixing",
    "authors": [
      "Kyusik Cho",
      "Suhyeon Lee",
      "Hongje Seong",
      "Euntai Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Cho_Domain_Adaptive_Video_Semantic_Segmentation_via_Cross-Domain_Moving_Object_Mixing_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Cho_Domain_Adaptive_Video_Semantic_Segmentation_via_Cross-Domain_Moving_Object_Mixing_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The network trained for domain adaptation is prone to bias toward the easy-to-transfer classes. Since the ground truth label on the target domain is unavailable during training, the bias problem leads to skewed predictions, forgetting to predict hard-to-transfer classes. To address this problem, we propose Cross-domain Moving Object Mixing (CMOM) that cuts several objects, including hard-to-transfer classes, in the source domain video clip and pastes them into the target domain video clip. Unlike image-level domain adaptation, the temporal context should be maintained to mix moving objects in two different videos. Therefore, we design CMOM to mix with consecutive video frames, so that unrealistic movements are not occurring. We additionally propose Feature Alignment with Temporal Context (FATC) to enhance target domain feature discriminability. FATC exploits the robust source domain features, which are trained with ground truth labels, to learn discriminative target domain features in an unsupervised manner by filtering unreliable predictions with temporal consensus. We demonstrate the effectiveness of the proposed approaches through extensive experiments. In particular, our model reaches mIoU of 53.81% on VIPER -> Cityscapes-Seq benchmark and mIoU of 56.31% on SYNTHIA-Seq -> Cityscapes-Seq benchmark, surpassing the state-of-the-art methods by large margins."
  },
  "wacv2023_main_self-improvingmultiplane-to-layerimagesfornovelviewsynthesis": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Improving Multiplane-To-Layer Images for Novel View Synthesis",
    "authors": [
      "Pavel Solovev",
      "Taras Khakhulin",
      "Denis Korzhenkov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Solovev_Self-Improving_Multiplane-To-Layer_Images_for_Novel_View_Synthesis_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Solovev_Self-Improving_Multiplane-To-Layer_Images_for_Novel_View_Synthesis_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "We present a new method for lightweight novel-view synthesis that generalizes to an arbitrary forward-facing scene. Recent approaches are computationally expensive, require per-scene optimization, or produce a memory-expensive representation. We start by representing the scene with a set of fronto-parallel semitransparent planes and afterwards convert them to deformable layers in an end-to-end manner. Additionally, we employ a feed-forward refinement procedure that corrects the estimated representation by aggregating information from input views. Our method does not require any fine-tuning when a new scene is processed and can handle an arbitrary number of views without any restrictions. Experimental results show that our approach surpasses recent models in terms of both common metrics and human evaluation, with the noticeable advantage in inference speed and compactness of the inferred layered geometry."
  },
  "wacv2023_main_patch-levelgazedistributionpredictionforgazefollowing": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Patch-Level Gaze Distribution Prediction for Gaze Following",
    "authors": [
      "Qiaomu Miao",
      "Minh Hoai",
      "Dimitris Samaras"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Miao_Patch-Level_Gaze_Distribution_Prediction_for_Gaze_Following_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Miao_Patch-Level_Gaze_Distribution_Prediction_for_Gaze_Following_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Gaze following aims to predict where a person is looking in a scene, by predicting the target location, or indicating that the target is located outside the image. Recent works detect the gaze target by training a heatmap regression task with a pixel-wise mean-square error (MSE) loss, while formulating the in/out prediction task as a binary classification task. This training formulation puts a strict, pixel-level constraint in higher resolution on the single annotation available in training, and does not consider annotation variance and the correlation between the two subtasks. To address these issues, we introduce the patch distribution prediction (PDP) method. We replace the in/out prediction branch in previous models with the PDP branch, by predicting a patch-level gaze distribution that also considers the outside cases. Experiments show that our model regularizes the MSE loss by predicting better heatmap distributions on images with larger annotation variances, meanwhile bridging the gap between the target prediction and in/out prediction subtasks, showing a significant improvement in performance on both subtasks on public gaze following datasets."
  },
  "wacv2023_main_self-distillationforunsupervised3ddomainadaptation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Distillation for Unsupervised 3D Domain Adaptation",
    "authors": [
      "Adriano Cardace",
      "Riccardo Spezialetti",
      "Pierluigi Zama Ramirez",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Cardace_Self-Distillation_for_Unsupervised_3D_Domain_Adaptation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Cardace_Self-Distillation_for_Unsupervised_3D_Domain_Adaptation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation."
  },
  "wacv2023_main_multivariateprobabilisticmonocular3dobjectdetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Multivariate Probabilistic Monocular 3D Object Detection",
    "authors": [
      "Xuepeng Shi",
      "Zhixiang Chen",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Shi_Multivariate_Probabilistic_Monocular_3D_Object_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Shi_Multivariate_Probabilistic_Monocular_3D_Object_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In autonomous driving, monocular 3D object detection is an important but challenging task. Towards accurate monocular 3D object detection, some recent methods recover the distance of objects from the physical height and visual height of objects. Such decomposition framework can introduce explicit constraints on the distance prediction, thus improving its accuracy and robustness. However, the inaccurate physical height and visual height prediction still may exacerbate the inaccuracy of the distance prediction. In this paper, we improve the framework by multivariate probabilistic modeling. We explicitly model the joint probability distribution of the physical height and visual height. This is achieved by learning a full covariance matrix of the physical height and visual height during training, with the guide of a multivariate likelihood. Such explicit joint probability distribution modeling not only leads to robust distance prediction when both the predicted physical height and visual height are inaccurate, but also brings learned covariance matrices with expected behaviors. The experimental results on the challenging Waymo Open and KITTI datasets show the effectiveness of our framework."
  },
  "wacv2023_main_pixel-wisepredictionbasedvisualodometryviauncertaintyestimation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Pixel-Wise Prediction Based Visual Odometry via Uncertainty Estimation",
    "authors": [
      "Hao-Wei Chen",
      "Ting-Hsuan Liao",
      "Hsuan-Kung Yang",
      "Chun-Yi Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Chen_Pixel-Wise_Prediction_Based_Visual_Odometry_via_Uncertainty_Estimation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Chen_Pixel-Wise_Prediction_Based_Visual_Odometry_via_Uncertainty_Estimation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "This paper introduces pixel-wise prediction based visual odometry(PWVO), which is a dense prediction task that evaluates the values of translation and rotation for every pixel in its input observations. PWVO employs uncertainty estimation to identify the noisy regions in the input observations, and adopts a selection mechanism to integrate pixel-wise predictions based on the estimated uncertainty maps to derive the final translation and rotation. In order to train PWVO in a comprehensive fashion, we further develop a data generation workflow for generating synthetic training data. The experimental results show that PWVO isable to deliver favorable results. In addition, our analyses validate the effectiveness of the designs adopted in PWVO, and demonstrate that the uncertainty mapsestimated by PWVO is capable of capturing the noises in its input observations."
  },
  "wacv2023_main_relationpreservingtripletminingforstabilisingthetripletlossinre-identificationsystems": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Relation Preserving Triplet Mining for Stabilising the Triplet Loss In re-Identification Systems",
    "authors": [
      "Adhiraj Ghosh",
      "Kuruparan Shanmugalingam",
      "Wen-Yan Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Ghosh_Relation_Preserving_Triplet_Mining_for_Stabilising_the_Triplet_Loss_In_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Ghosh_Relation_Preserving_Triplet_Mining_for_Stabilising_the_Triplet_Loss_In_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Object appearances change dramatically with pose variations. This creates a challenge for embedding schemes that seek to map instances with the same object ID to locations that are as close as possible. This issue becomes significantly heightened in complex computer vision tasks such as re-identification(reID). In this paper, we suggest that these dramatic appearance changes are indications that an object ID is composed of multiple natural groups, and it is counterproductive to forcefully map instances from different groups to a common location. This leads us to introduce Relation Preserving Triplet Mining (RPTM), a feature matching guided triplet mining scheme, that ensures that triplets will respect the natural subgroupings within an object ID. We use this triplet mining mechanism to establish a pose-aware, well-conditioned triplet loss by implicitly enforcing view consistency. This allows a single network to be trained with fixed parameters across datasets while providing state-of-the-art results. Code is available at https: //github.com/adhirajghosh/RPTM_reid."
  },
  "wacv2023_main_improvingsaliencymodelspredictionsofthenextfixationwithhumansintrinsiccostofgazeshifts": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Improving Saliency Models' Predictions of the Next Fixation With Humans' Intrinsic Cost of Gaze Shifts",
    "authors": [
      "Florian Kadner",
      "Tobias Thomas",
      "David Hoppe",
      "Constantin A. Rothkopf"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kadner_Improving_Saliency_Models_Predictions_of_the_Next_Fixation_With_Humans_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kadner_Improving_Saliency_Models_Predictions_of_the_Next_Fixation_With_Humans_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The human prioritization of image regions can be modeled in a time invariant fashion with saliency maps or sequentially with scanpath models. However, while both types of models have steadily improved on several benchmarks and datasets, there is still a considerable gap in predicting human gaze. Here, we leverage two recent developments to reduce this gap: theoretical analyses establishing a principled framework for predicting the next gaze target and the empirical measurement of the human cost for gaze switches independently of image content. We introduce an algorithm in the framework of sequential decision making, which converts any static saliency map into a sequence of dynamic history-dependent value maps, which are recomputed after each gaze shift. These maps are based on 1) a saliency map provided by an arbitrary saliency model, 2) the recently measured human cost function quantifying preferences in magnitude and direction of eye movements, and 3) a sequential exploration bonus, which changes with each subsequent gaze shift. The parameters of the spatial extent and temporal decay of this exploration bonus are estimated from human gaze data. The relative contributions of these three components were optimized on the MIT1003 dataset for the NSS score and are sufficient to significantly outperform predictions of the next gaze target on NSS and AUC scores for five state of the art saliency models on three image data sets."
  },
  "wacv2023_main_guidingvisualquestionansweringwithattentionpriors": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Guiding Visual Question Answering With Attention Priors",
    "authors": [
      "Thao Minh Le",
      "Vuong Le",
      "Sunil Gupta",
      "Svetha Venkatesh",
      "Truyen Tran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Le_Guiding_Visual_Question_Answering_With_Attention_Priors_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Le_Guiding_Visual_Question_Answering_With_Attention_Priors_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The current success of modern visual reasoning systems is arguably attributed to cross-modality attention mechanisms. However, in deliberative reasoning such as in VQA, attention is unconstrained at each step, and thus may serve as a statistical pooling mechanism rather than a semantic operation intended to select information relevant to inference. This is because at training time, attention is only guided by a very sparse signal (i.e. the answer label) at the end of the inference chain. This causes the cross-modality attention weights to deviate from the desired visual-language bindings. To rectify this deviation, we propose to guide the attention mechanism using explicit linguistic-visual grounding. This grounding is derived by connecting structured linguistic concepts in the query to their referents among the visual objects. Here we learn the grounding from the pairing of questions and images alone, without the need for answer annotation or external grounding supervision. This grounding guides the attention mechanism inside VQA models through a duality of mechanisms: pre-training attention weight calculation and directly guiding the weights at inference time on a case-by-case basis. The resultant algorithm is capable of probing attention-based reasoning models, injecting relevant associative knowledge, and regulating the core reasoning process. This scalable enhancement improves the performance of VQA models, fortifies their robustness to limited access to supervised data, and increases interpretability."
  },
  "wacv2023_main_self-pairsynthesizingchangesfromsinglesourceforobjectchangedetectioninremotesensingimagery": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Self-Pair: Synthesizing Changes From Single Source for Object Change Detection in Remote Sensing Imagery",
    "authors": [
      "Minseok Seo",
      "Hakjin Lee",
      "Yongjin Jeon",
      "Junghoon Seo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Seo_Self-Pair_Synthesizing_Changes_From_Single_Source_for_Object_Change_Detection_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Seo_Self-Pair_Synthesizing_Changes_From_Single_Source_for_Object_Change_Detection_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "For change detection in remote sensing, constructing a training dataset for deep learning models is quite difficult due to the requirements of bi-temporal supervision. To overcome this issue, single-temporal supervision which treats change labels as the difference of two semantic masks has been proposed. This novel method trains a change detector using two spatially unrelated images with corresponding semantic labels. However, training with unpaired dataset shows not enough performance compared with other methods based on bi-temporal supervision. We suspect this phenomenon caused by ignorance of meaningful information in the actual bi-temporal pairs.In this paper, we emphasize that the change originates from the source image and show that manipulating the source image as an after-image is crucial to the performance of change detection. Our method achieves state-of-the-art performance in a large gap than existing methods."
  },
  "wacv2023_main_performeranovelppg-to-ecgreconstructiontransformerforadigitalbiomarkerofcardiovasculardiseasedetection": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Performer: A Novel PPG-to-ECG Reconstruction Transformer for a Digital Biomarker of Cardiovascular Disease Detection",
    "authors": [
      "Ella Lan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lan_Performer_A_Novel_PPG-to-ECG_Reconstruction_Transformer_for_a_Digital_Biomarker_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lan_Performer_A_Novel_PPG-to-ECG_Reconstruction_Transformer_for_a_Digital_Biomarker_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Electrocardiography (ECG), an electrical measurement which captures cardiac activities, is the gold standard for diagnosing cardiovascular disease (CVD). However, ECG is infeasible for continuous cardiac monitoring due to its requirement for user participation. By contrast, photoplethysmography (PPG) provides easy-to-collect data, but its limited accuracy constrains its clinical usage. To combine the advantages of both signals, recent studies incorporate various deep learning techniques for the reconstruction of PPG signals to ECG; however, the lack of contextual information as well as the limited abilities to denoise biomedical signals ultimately constrain model performance. In this research, we propose Performer, a novel Transformer-based architecture that reconstructs ECG from PPG and combines the PPG and reconstructed ECG as multiple modalities for CVD detection. This method is the first time that Transformer sequence-to-sequence translation has been performed on biomedical waveform reconstruction, combining the advantages of both PPG and ECG. We also create Shifted Patch-based Attention (SPA), an effective method to encode/decode the biomedical waveforms. Through fetching the various sequence lengths and capturing cross-patch connections, SPA maximizes the signal processing for both local features and global contextual representations. The proposed architecture generates a state-of-the-art performance of 0.29 RMSE for the reconstruction of PPG to ECG on the BIDMC database, surpassing prior studies. We also evaluated this model on the MIMIC-III dataset, achieving a 95.9% accuracy in CVD detection, and on the PPG-BP dataset, achieving 75.9% accuracy in related CVD diabetes detection, indicating its generalizability. As a proof of concept, an earring wearable named PEARL (prototype), was designed to scale up the point-of-care (POC) healthcare system."
  },
  "wacv2023_main_tokenpoolinginvisiontransformersforimageclassification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Token Pooling in Vision Transformers for Image Classification",
    "authors": [
      "Dmitrii Marin",
      "Jen-Hao Rick Chang",
      "Anurag Ranjan",
      "Anish Prabhu",
      "Mohammad Rastegari",
      "Oncel Tuzel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Marin_Token_Pooling_in_Vision_Transformers_for_Image_Classification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Marin_Token_Pooling_in_Vision_Transformers_for_Image_Classification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Pooling is commonly used to improve the computation-accuracy trade-off of convolutional networks. By aggregating neighboring feature values on the image grid, pooling layers downsample feature maps while maintaining accuracy. In transformers, however, tokens are processed individually and do not necessarily lie on regular grids. Utilizing pooling methods designed for image grids (e.g., average pooling) can thus be sub-optimal for transformers, as shown by our experiments. In this paper, we propose Token Pooling to downsample tokens in vision transformers. We take a new perspective --- instead of assuming tokens form a regular grid, we treat them as discrete (and irregular) samples of a continuous signal. Given a target number of tokens, Token Pooling finds the set of tokens that best approximates the underlying continuous signal. We rigorously evaluate the proposed method on the standard transformer architecture (ViT/DeiT), and our experiments show that Token Pooling significantly improves the computation-accuracy trade-off without any further modifications to the architecture. On ImageNet-1k, Token Pooling enables DeiT-Ti to achieve the same top-1 accuracy while using 42% fewer computations."
  },
  "wacv2023_main_lightweightnetworkforvideomotionmagnification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Lightweight Network for Video Motion Magnification",
    "authors": [
      "Jasdeep Singh",
      "Subrahmanyam Murala",
      "G. Sankara Raju Kosuru"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Singh_Lightweight_Network_for_Video_Motion_Magnification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Singh_Lightweight_Network_for_Video_Motion_Magnification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Video motion magnification provides information to understand the subtle changes present in objects for applications like industrial, healthcare, sports, etc. Most state-ofthe-art (SOTA) methods use hand-crafted bandpass filters, which require prior information for the motion magnification, produces ringing artifacts, and small magnification in dynamic scenarios etc. While others use deep-learning based techniques, but their output suffers from artificially induced motion, distortions, blurriness, etc. Further, SOTA methods are computationally complex, which makes them less suitable for real-time applications. To address these problems, we proposed deep learning based simple yet effective solution for motion magnification. The proposed method uses a feature sharing and appearance encoder for better motion magnification with less distortions, artifacts etc. Additionally, for reducing magnification of noise and other unwanted changes, proxy-model based training is proposed. A computationally lightweight model (0.12 M parameters) is proposed along with the base model. The performance of the proposed models is tested qualitatively and quantitatively, with the SOTA methods. Results demonstrate the effectiveness of the proposed lightweight and base model over the existing SOTA methods."
  },
  "wacv2023_main_fantasticstylechannelsandwheretofindthemasubmodularframeworkfordiscoveringdiversedirectionsingans": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fantastic Style Channels and Where To Find Them: A Submodular Framework for Discovering Diverse Directions in GANs",
    "authors": [
      "Enis Simsar",
      "Umut Kocasari",
      "Ezgi G\u00fclperi Er",
      "Pinar Yanardag"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Simsar_Fantastic_Style_Channels_and_Where_To_Find_Them_A_Submodular_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Simsar_Fantastic_Style_Channels_and_Where_To_Find_Them_A_Submodular_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The discovery of interpretable directions in the latent spaces of pre-trained GAN models has recently become a popular topic. In particular, StyleGAN2 has enabled various image generation and manipulation tasks due to its rich and disentangled latent spaces. However, the discovery of such directions is typically made either in a supervised manner, which requires annotated data for each desired manipulation, or in an unsupervised manner, which requires a manual effort to identify the directions. As a result, existing work typically finds only a handful of directions in which controllable edits can be made. In this study, we design a novel submodular framework that finds the most representative and diverse subset of directions in the latent space of StyleGAN2. Our approach takes advantage of the latent space of channel-wise style parameters, so-called stylespace, in which we cluster channels that perform similar manipulations into groups. Our framework promotes diversity by using the notion of clusters and can be efficiently solved with a greedy optimization scheme. We evaluate our framework with qualitative and quantitative experiments and show that our method finds more diverse and disentangled directions."
  },
  "wacv2023_main_improvingpixel-levelcontrastivelearningbyleveragingexogenousdepthinformation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Improving Pixel-Level Contrastive Learning by Leveraging Exogenous Depth Information",
    "authors": [
      "Ahmed Ben Saad",
      "Kristina Prokopetc",
      "Josselin Kherroubi",
      "Axel Davy",
      "Adrien Courtois",
      "Gabriele Facciolo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Saad_Improving_Pixel-Level_Contrastive_Learning_by_Leveraging_Exogenous_Depth_Information_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Saad_Improving_Pixel-Level_Contrastive_Learning_by_Leveraging_Exogenous_Depth_Information_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Self-supervised representation learning based on Contrastive Learning (CL) has been the subject of much attention in recent years. This is due to the excellent results obtained on a variety of subsequent tasks (in particular classification), without requiring a large amount of labeled samples. However, most reference CL algorithms (such as SimCLR and MoCo, but also BYOL and Barlow Twins) are not adapted to pixel-level downstream tasks. One existing solution known as PixPro proposes a pixel-level approach that is based on filtering of pairs of positive/negative image crops of the same image using the distance between the crops in the whole image. We argue that this idea can be further enhanced by incorporating semantic information provided by exogenous data as an additional selection filter, which can be used (at training time) to improve the selection of the pixel-level positive/negative samples. In this paper we will focus on the depth information, which can be obtained by using a depth estimation network or measured from available data (stereovision, parallax motion, lidar, ...). Scene depth can provide meaningful cues to distinguish pixels belonging to different objects based on their depth. We show that using this exogenous information in the contrastive loss leads to improved results and that the learned representations better follow the shapes of objects. In addition, we introduce a multi-scale loss that alleviates the issue of finding the training parameters adapted to different object sizes. We demonstrate the effectiveness of our ideas on the Breakout Segmentation on Borehole Images where we achieve an improvement of 1.9% over PixPro and nearly 5% over the supervised baseline. We further validate our technique on the indoor scene segmentation tasks with ScanNet and outdoor scenes with CityScapes ( 1.6% and 1.1% improvement over PixPro respectively)."
  },
  "wacv2023_main_cooperativeself-trainingformulti-targetadaptivesemanticsegmentation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Cooperative Self-Training for Multi-Target Adaptive Semantic Segmentation",
    "authors": [
      "Yangsong Zhang",
      "Subhankar Roy",
      "Hongtao Lu",
      "Elisa Ricci",
      "St\u00e9phane Lathuili\u00e8re"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Zhang_Cooperative_Self-Training_for_Multi-Target_Adaptive_Semantic_Segmentation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Cooperative_Self-Training_for_Multi-Target_Adaptive_Semantic_Segmentation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this work we address multi-target domain adaptation (MTDA) in semantic segmentation, which consists in adapting a single model from an annotated source dataset to multiple unannotated target datasets that differ in their underlying data distributions. To address MTDA, we propose a self-training strategy that employs pseudo-labels to induce cooperation among multiple domain-specific classifiers. We employ feature stylization as an efficient way to generate image views that forms an integral part of self-training. Additionally, to prevent the network from overfitting to noisy pseudo-labels, we devise a rectification strategy that leverages the predictions from different classifiers to estimate the quality of pseudo-labels. Our extensive experiments on numerous settings, based on four different semantic segmentation datasets, validates the effectiveness of the proposed self-training strategy and shows that our method outperforms state-of-the-art MTDA approaches."
  },
  "wacv2023_main_corlcompositionalrepresentationlearningforfew-shotclassification": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "CORL: Compositional Representation Learning for Few-Shot Classification",
    "authors": [
      "Ju He",
      "Adam Kortylewski",
      "Alan Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/He_CORL_Compositional_Representation_Learning_for_Few-Shot_Classification_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/He_CORL_Compositional_Representation_Learning_for_Few-Shot_Classification_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Few-shot image classification consists of two consecutive learning processes: 1) In the meta-learning stage, the model acquires a knowledge base from a set of training classes. 2) During meta-testing, the acquired knowledge is used to recognize unseen classes from very few examples. Inspired by the compositional representation of objects in humans, we train a neural network architecture that explicitly represents objects as a dictionary of shared components and their spatial composition. In particular, during meta-learning, we train a knowledge base that consists of a dictionary of component representations and a dictionary of component activation maps that encode common spatial activation patterns of components. The elements of both dictionaries are shared among the training classes. During meta-testing, the representation of unseen classes is learned using the component representations and the component activation maps from the knowledge base. Finally, an attention mechanism is used to strengthen those components that are most important for each category. We demonstrate the value of our compositional learning framework for a few-shot classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where we achieve comparable performance."
  },
  "wacv2023_main_unsupervisedvideoobjectsegmentationviaprototypememorynetwork": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Unsupervised Video Object Segmentation via Prototype Memory Network",
    "authors": [
      "Minhyeok Lee",
      "Suhwan Cho",
      "Seunghoon Lee",
      "Chaewon Park",
      "Sangyoun Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Lee_Unsupervised_Video_Object_Segmentation_via_Prototype_Memory_Network_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Lee_Unsupervised_Video_Object_Segmentation_via_Prototype_Memory_Network_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Unsupervised video object segmentation aims to segment a target object in the video without a ground truth mask in the initial frame. This challenging task requires extracting features for the most salient common objects within a video sequence. This difficulty can be solved by using motion information such as optical flow, but using only the information between adjacent frames results in poor connectivity between distant frames and poor performance. To solve this problem, we propose a novel prototype memory network architecture. The proposed model effectively extracts the RGB and motion information by extracting superpixel-based component prototypes from the input RGB images and optical flow maps. In addition, the model scores the usefulness of the component prototypes in each frame based on a self-learning algorithm and adaptively stores the most useful prototypes in memory and discards obsolete prototypes. We use the prototypes in the memory bank to predict the next query frame's mask, which enhances the association between distant frames to help with accurate mask prediction. Our method is evaluated on three datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed model with various ablation studies."
  },
  "wacv2023_main_finegazeredirectionlearningwithgazehardness-awaretransformation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Fine Gaze Redirection Learning With Gaze Hardness-Aware Transformation",
    "authors": [
      "Sangjin Park",
      "Daeha Kim",
      "Byung Cheol Song"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Park_Fine_Gaze_Redirection_Learning_With_Gaze_Hardness-Aware_Transformation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Park_Fine_Gaze_Redirection_Learning_With_Gaze_Hardness-Aware_Transformation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The gaze redirection is a task to adjust the gaze of a given face or eye image toward the desired direction and aims to learn the gaze direction of a face image through a neural network-based generator. Considering that the prior arts have learned coarse gaze directions, learning fine gaze directions is very challenging. In addition, explicit discriminative learning of high-dimensional gaze features has not been reported yet. This paper presents solutions to overcome the above limitations. First, we propose the featurelevel transformation which provides gaze features corresponding to various gaze directions in the latent feature space. Second, we propose a novel loss function for discriminative learning of gaze features. Specifically, features with insignificant or irrelevant effects on gaze (e.g., head pose and appearance) are set as negative pairs, and important gaze features are set as positive pairs, and then pair-wise similarity learning is performed. As a result, the proposed method showed a redirection error of only 2deg for the GazeCapture dataset. This is a 10% better performance than a state-of-the-art method, i.e., STED. Additionally, the rationale for why latent features of various attributes should be discriminated is presented through activation visualization. Code is available at https://github.com/san9569/Gaze-Redir-Learning."
  },
  "wacv2023_main_separatingpartially-polarizeddiffuseandspecularreflectioncomponentsunderunpolarizedlightsources": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Separating Partially-Polarized Diffuse and Specular Reflection Components Under Unpolarized Light Sources",
    "authors": [
      "Soma Kajiyama",
      "Taihe Piao",
      "Ryo Kawahara",
      "Takahiro Okabe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Kajiyama_Separating_Partially-Polarized_Diffuse_and_Specular_Reflection_Components_Under_Unpolarized_Light_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Kajiyama_Separating_Partially-Polarized_Diffuse_and_Specular_Reflection_Components_Under_Unpolarized_Light_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Separating diffuse and specular reflection components observed on an object surface is important for preprocessing of various computer vision techniques. Conventionally, diffuse-specular separation based on the polarimetric and color clues assumes that the diffuse/specular reflection components are unpolarized/partially polarized under unpolarized light sources. However, the diffuse reflection component is partially polarized in fact, because the diffuse reflectance is maximal when the polarization direction is parallel to the outgoing plane. Accordingly, we propose a method for separating partially-polarized diffuse and specular reflection components on the basis of the polarization reflection model and the dichromatic reflection model. In particular, our method enables us not only to achieve diffuse-specular separation but also to estimate the polarimetric properties of the object surface from a single color polarization image. We experimentally confirmed that our method performs better than the method assuming unpolarized diffuse reflection components."
  },
  "wacv2023_main_scannerfascalablebenchmarkforneuralradiancefields": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "ScanNeRF: A Scalable Benchmark for Neural Radiance Fields",
    "authors": [
      "Luca De Luigi",
      "Damiano Bolognini",
      "Federico Domeniconi",
      "Daniele De Gregorio",
      "Matteo Poggi",
      "Luigi Di Stefano"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/De_Luigi_ScanNeRF_A_Scalable_Benchmark_for_Neural_Radiance_Fields_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/De_Luigi_ScanNeRF_A_Scalable_Benchmark_for_Neural_Radiance_Fields_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "In this paper, we propose the first-ever real benchmark thought for evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering (NR) frameworks. We design and implement an effective pipeline for scanning real objects in quantity and effortlessly. Our scan station is built with less than 500 hardware budget and can collect roughly 4000 images of a scanned object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset characterized by several train/val/test splits aimed at benchmarking the performance of modern NeRF methods under different conditions. Accordingly, we evaluate three cutting-edge NeRF variants on it to highlight their strengths and weaknesses. The dataset is available on our project page, together with an online benchmark to foster the development of better and better NeRFs."
  },
  "wacv2023_main_adaptivesampleselectionforrobustlearningunderlabelnoise": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Adaptive Sample Selection for Robust Learning Under Label Noise",
    "authors": [
      "Deep Patel",
      "P. S. Sastry"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Patel_Adaptive_Sample_Selection_for_Robust_Learning_Under_Label_Noise_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Patel_Adaptive_Sample_Selection_for_Robust_Learning_Under_Label_Noise_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization or overfitting in the presence of noisily-labelled data. For the problem of robust learning under such noisy data, several algorithms have been proposed. A prominent class of algorithms rely on sample selection strategies wherein, essentially, a fraction of samples with loss values below a certain threshold are selected for training. These algorithms are sensitive to such thresholds, and it is difficult to fix or learn these thresholds. Often, these algorithms also require information such as label noise rates which are typically unavailable in practice. In this paper, we propose an adaptive sample selection strategy that relies only on batch statistics of a given mini-batch to provide robustness against label noise. The algorithm does not have any additional hyperparameters for sample selection, does not need any information on noise rates and does not need access to separate data with clean labels. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets."
  },
  "wacv2023_main_patch-basedprivacypreservingneuralnetworkforvisiontasks": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Patch-Based Privacy Preserving Neural Network for Vision Tasks",
    "authors": [
      "Mitsuhiro Mabuchi",
      "Tetsuya Ishikawa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Mabuchi_Patch-Based_Privacy_Preserving_Neural_Network_for_Vision_Tasks_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Mabuchi_Patch-Based_Privacy_Preserving_Neural_Network_for_Vision_Tasks_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "As machine learning technology is increasingly adopted into a variety of application domains, the potential risks of data leakage are becoming more serious in certain cases where the data contains highly sensitive information. While some privacy-preserving learning mechanisms for image data, such as SplitNN, enable the training of models without sharing private data on a central server, there exists a trade-off between security and computational cost to a client device. We propose a new mechanism to achieve higher level security and lower computational cost on a client device while maintaining model performance. Our approach, called Patch SplitNN, is based on SplitNN architecture that divides a CNN into two networks, called upper and lower. The difference from that previous work is to input individual image patches into multiple upper models, before concatenating their outputs before the lower model. For further improvement of the upper model training, we introduce an additional network and a loss function into the training process. We demonstrate our Patch SplitNN can classify images as accurately as a ResNet18 on various image classification datasets (CIFAR-10, CIFAR-100, and PCam) under multiple conditions (e.g. patching patterns, dropping patches)."
  },
  "wacv2023_main_image-textpre-trainingforlogorecognition": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Image-Text Pre-Training for Logo Recognition",
    "authors": [
      "Mark Hubenthal",
      "Suren Kumar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Hubenthal_Image-Text_Pre-Training_for_Logo_Recognition_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Hubenthal_Image-Text_Pre-Training_for_Logo_Recognition_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "Open-set logo recognition is commonly solved by first detecting possible logo regions and then matching the detected parts against an ever-evolving dataset of cropped logo images. The matching model, a metric learning problem, is especially challenging for logo recognition due to the mixture of text and symbols in logos. We propose two novel contributions to improve the matching model's performance: (a) using image-text paired samples for pre-training, and (b) an improved metric learning loss function. A standard paradigm of fine-tuning ImageNet pre-trained models fails to discover the text sensitivity necessary to solve the matching problem effectively. This work demonstrates the importance of pre-training on image-text pairs, which significantly improves the performance of a visual embedder trained for the logo retrieval task, especially for more text-dominant classes. We construct a composite public logo dataset combining LogoDet3K, OpenLogo, and FlickrLogos-47 deemed OpenLogoDet3K47. We show that the same vision backbone pre-trained on image-text data, when fine-tuned on OpenLogoDet3K47, achieves 98.6% recall@1, significantly improving performance over pre-training on Imagenet1K (97.6%). We generalize the ProxyNCA++ loss function to propose ProxyNCAHN++ which incorporates class-specific hard negative images. The proposed method sets new state-of-the-art on five public logo datasets considered, with a 3.5% zero-shot recall@1 improvement on LogoDet3K test, 4% on OpenLogo, 6.5% on FlickrLogos-47, 6.2% on Logos In The Wild, and 0.6% on BelgaLogo."
  },
  "wacv2023_main_lra&ldrarethinkingresidualpredictionsforefficientshadowdetectionandremoval": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "LRA&LDRA: Rethinking Residual Predictions for Efficient Shadow Detection and Removal",
    "authors": [
      "Mehmet Kerim Y\u00fccel",
      "Valia Dimaridou",
      "Bruno Manganelli",
      "Mete Ozay",
      "Anastasios Drosou",
      "Albert Sa\u00e0-Garriga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Yucel_LRALDRA_Rethinking_Residual_Predictions_for_Efficient_Shadow_Detection_and_Removal_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Yucel_LRALDRA_Rethinking_Residual_Predictions_for_Efficient_Shadow_Detection_and_Removal_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "The majority of the state-of-the-art shadow removal models (SRMs) reconstruct whole input images, where their capacity is needlessly spent on reconstructing non-shadow regions. SRMs that predict residuals remedy this up to a degree, but fall short of providing an accurate and flexible solution. In this paper, we rethink residual predictions and propose Learnable Residual Attention (LRA) and Learnable Dense Reconstruction Attention (LDRA) modules, which operate over the input and the output of SRMs. These modules guide an SRM to concentrate on shadow region reconstruction, and limit reconstruction of non-shadow regions. The modules improve shadow removal (up to 20%) and detection accuracy across various backbones, and even improve the accuracy of other removal methods (up to 10%). In addition, the modules have minimal overhead (+<1MB memory) and are implemented in a few lines of code. Furthermore, to combat the challenge of training SRMs with small datasets, we present a synthetic dataset generation pipeline. Using our pipeline, we create a dataset called PITSA, which has 10 times more unique shadow-free images than the largest benchmark dataset. Pre-training models on the PITSA significantly improves shadow removal (+2 MAE on shadow regions) and detection accuracy of multiple methods. Our results show that LRA&LDRA, when plugged into a lightweight architecture pre-trained on the PITSA, outperform state-of-the-art shadow removal (+0.7 all-region MAE) and detection (+0.1 BER) methods on the benchmark ISTD and SRD datasets, despite running faster (+5%) and consuming less memory (x150)."
  },
  "wacv2023_main_urbanscenesemanticsegmentationwithlow-costcoarseannotation": {
    "conf_id": "WACV2023",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "WACV2023",
    "title": "Urban Scene Semantic Segmentation With Low-Cost Coarse Annotation",
    "authors": [
      "Anurag Das",
      "Yongqin Xian",
      "Yang He",
      "Zeynep Akata",
      "Bernt Schiele"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2023/html/Das_Urban_Scene_Semantic_Segmentation_With_Low-Cost_Coarse_Annotation_WACV_2023_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2023/papers/Das_Urban_Scene_Semantic_Segmentation_With_Low-Cost_Coarse_Annotation_WACV_2023_paper.pdf",
    "published": "2023-01",
    "summary": "For best performance, today's semantic segmentation methods use large and carefully labeled datasets, requiring expensive annotation budgets. In this work, we show that coarse annotation is a low-cost but highly effective alternative for training semantic segmentation models. Considering the urban scene segmentation scenario, we leverage cheap coarse annotations for real-world captured data, as well as synthetic data to train our model and show competitive performance compared with fully annotated real-world data. Specifically, we propose a coarse-to fine self-training framework that generates pseudo labels for unlabeled regions of the coarsely annotated data, using synthetic data to improve predictions around the boundaries between semantic classes, and using cross-domain data augmentation to increase diversity. Our extensive experimental results on Cityscapes and BDD100k datasets demonstrate that our method achieves a significantly better performance vs annotation cost tradeoff, yielding a comparable performance to fully annotated data with only a small fraction of the annotation budget. Also, when used as pretraining, our framework performs better compared to the standard fully supervised setting."
  }
}