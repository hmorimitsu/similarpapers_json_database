{
  "cvpr2018_w1_disguisedfacesinthewild": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "Disguised Faces in the Wild",
    "authors": [
      "Vineet Kushwaha",
      "Maneet Singh",
      "Richa Singh",
      "Mayank Vatsa",
      "Nalini Ratha",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Kushwaha_Disguised_Faces_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Kushwaha_Disguised_Faces_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Existing research in the field of face recognition with variations due to disguises focuses primarily on images captured in controlled settings. Limited research has been performed on images captured in unconstrained environments, primarily due to the lack of corresponding disguised face datasets. In order to overcome this limitation, this work presents a novel Disguised Faces in the Wild (DFW) dataset, consisting of over 11,000 images for understanding and pushing the current state-of-the-art for disguised face recognition. To the best of our knowledge, DFW is a first-of-a-kind dataset containing images pertaining to both obfuscation and impersonation for understanding the effect of disguise variations. A major portion of the dataset has been collected from the Internet, thereby encompassing a wide variety of disguise accessories and variations across other covariates. As part of CVPR2018, a competition and workshop are organized to facilitate research in this direction. This paper presents a description of the dataset, the baseline protocols and performance, along with the phase-I results of the competition.",
    "code_link": ""
  },
  "cvpr2018_w1_deepfeaturesforrecognizingdisguisedfacesinthewild": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "Deep Features for Recognizing Disguised Faces in the Wild",
    "authors": [
      "Ankan Bansal",
      "Rajeev Ranjan",
      "Carlos D. Castillo",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Bansal_Deep_Features_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Bansal_Deep_Features_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Unconstrained face verification is a challenging problem owing to variations in pose, illumination, resolution of image, age, etc. This problem becomes even more complex when the subjects are actively trying to deceive face verification systems by wearing a disguise. The problem under consideration here is to identify a subject under disguises and reject impostors trying to look like the subject of inter- est.In this paper we present a DCNN-based approach for recognizing people under disguises and picking out impostors. We train two different networks on a large dataset comprising of still images and video frames with L2-softmax loss. We fuse features obtained from the two networks and show that the resulting features are effective for discriminating between disguised faces and impostors in the wild. We present results on the recently introduced Disguised Faces in the Wild challenge dataset.",
    "code_link": ""
  },
  "cvpr2018_w1_faceverificationwithdisguisevariationsviadeepdisguiserecognizer": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "Face Verification With Disguise Variations via Deep Disguise Recognizer",
    "authors": [
      "Naman Kohli",
      "Daksha Yadav",
      "Afzel Noore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Kohli_Face_Verification_With_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Kohli_Face_Verification_With_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The performance of current automatic face recognition algorithms is hindered by different covariates such as facial aging, disguises, and pose variations. Specifically, disguises are employed for intentional or unintentional modifications in the facial appearance for hiding one's own identity or impersonating someone else's identity. In this paper, we utilize deep learning based transfer learning approach for face verification with disguise variations. We employ residual Inception network framework with center loss for learning inherent face representations. The training for the Inception-ResNet model is performed using a large-scale face database which is followed by inductive transfer learning to mitigate the impact of facial disguises. To evaluate the performance of the proposed Deep Disguise Recognizer (DDR) framework, Disguised Faces in the Wild and IIIT-Delhi Disguise Version 1 face databases are used. Experimental evaluation reveals that for the two databases, the proposed DDR framework yields 90.36% and 66.9% face verification accuracy at the false accept rate of 10%.",
    "code_link": ""
  },
  "cvpr2018_w1_disguisenetacontrastiveapproachfordisguisedfaceverificationinthewild": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "DisguiseNet: A Contrastive Approach for Disguised Face Verification in the Wild",
    "authors": [
      "Skand Vishwanath Peri",
      "Abhinav Dhall"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Peri_DisguiseNet_A_Contrastive_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Peri_DisguiseNet_A_Contrastive_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper describes our approach for the Disguised Faces in the Wild (DFW) 2018 challenge. The task here is to verify the identity of a person among disguised and impostors images. Given the importance of the task of face verification it is essential to compare methods across a common platform. Our approach is based on VGG-face architecture paired with contrastive loss based on cosine distance metric. For augmenting the data set, we source more data from the internet. The experiments show the effectiveness of the approach on the DFW data. We show that adding extra data to the DFW dataset with noisy labels also helps in increasing the generalization performance of the network.",
    "code_link": "https://github.com/pvskand/DisguiseNet"
  },
  "cvpr2018_w1_deepdisguisedfacesrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "Deep Disguised Faces Recognition",
    "authors": [
      "Kaipeng Zhang",
      "Ya-Liang Chang",
      "Winston Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Zhang_Deep_Disguised_Faces_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Zhang_Deep_Disguised_Faces_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recently, deep learning based approaches have yielded a significant improvement in face recognition in the wild. However, \"disguised face\" recognition is still a challenging task that needs to be investigated, and the Disguised Faces in the Wild (DFW) competition is designed for this task. In this paper, we propose a two-stage training approach to utilize the small-scale training data provided by the DFW competition. Specifically, in the first stage, we train Deep Convolutional Neural Networks (DCNNs) for generic face recognition. In the second stage, we use Principal Components Analysis (PCA) based on the DFW training set to find the best transformation matrix for identity representation of disguised faces. We evaluate our model on the DFW testing dataset and it shows better performance over the state-of-the-art generic face recognition methods. It also achieves the best results on the DFW competition - Phase 1.",
    "code_link": ""
  },
  "cvpr2018_w1_hardexampleminingwithauxiliaryembeddings": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "Hard Example Mining With Auxiliary Embeddings",
    "authors": [
      "Evgeny Smirnov",
      "Aleksandr Melnikov",
      "Andrei Oleinik",
      "Elizaveta Ivanova",
      "Ilya Kalinovskiy",
      "Eugene Luckyanets"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Smirnov_Hard_Example_Mining_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Smirnov_Hard_Example_Mining_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Hard example mining is an important part of the deep embedding learning. Most methods perform it at the mini-batch level. However, in the large-scale settings there is only a small chance that proper examples will appear in the same mini-batch and will be coupled into the hard example pairs or triplets. Doppelganger mining was previously proposed to increase this chance by means of class-wise similarity. This method ensures that examples of similar classes are sampled into the same mini-batch together. One of the drawbacks of this method is that it operates only at the class level, while there also might be a wayto select appropriate examples within class in a more elaborated way than randomly. In this paper, we propose to use auxiliary embeddings for hard example mining. These embeddings are constructed in such way that similar examples have close embeddings in the cosine similarity sense. With the help of these embeddings it is possible to select new examples for the mini-batch based on their similarity with the already selected examples. We propose several ways to create auxiliary embeddings and use them to increase the number of potentially hard positive and negative examples in each mini-batch. Our experiments on the challenging Disguised Faces in the Wild (DFW) dataset show that hard example mining with auxiliary embeddings improves the discriminative power of learned representations.",
    "code_link": ""
  },
  "cvpr2018_w1_detectingpresentationattacksfrom3dfacemasksundermultispectralimaging": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Disguised Faces in the Wild",
    "title": "Detecting Presentation Attacks From 3D Face Masks Under Multispectral Imaging",
    "authors": [
      "Jun Liu",
      "Ajay Kumar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w1/html/Liu_Detecting_Presentation_Attacks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w1/Liu_Detecting_Presentation_Attacks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Automated detection of sensor level spoof attacks using 3D face masks is critical to protect integrity of face recognition systems deployed for security and surveillance. This paper investigates a multispectral imaging approach to more accurately detect such presentation attacks. Real human faces and spoof face images from 3D face masks are simultaneously acquired under visible and near infrared (multispectral) illumination using two separate sensors. Ranges of convolutional neural network based configurations are investigated to improve the detection accuracy from such presentation attacks. Our experimental results indicate that near-infrared based imaging of 3D face masks offers superior performance as compared to those for the respective real/spoof face images acquired under visible illumination. Combination of simultaneously acquired presentation attack images under multispectral illumination can be used to further improve the accuracy of detecting attacks from more realistic 3D face masks.",
    "code_link": ""
  },
  "cvpr2018_w3_the2018nvidiaaicitychallenge": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "The 2018 NVIDIA AI City Challenge",
    "authors": [
      "Milind Naphade",
      "Ming-Ching Chang",
      "Anuj Sharma",
      "David C. Anastasiu",
      "Vamsi Jagarlamudi",
      "Pranamesh Chakraborty",
      "Tingting Huang",
      "Shuo Wang",
      "Ming-Yu Liu",
      "Rama Chellappa",
      "Jenq-Neng Hwang",
      "Siwei Lyu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Naphade_The_2018_NVIDIA_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Naphade_The_2018_NVIDIA_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The NVIDIA AI City Challenge has been created to accelerate intelligent video analysis that helps make cities smarter and safer. With millions of traffic video cameras acting as sensors around the world, there is significant opportunity for real-time and batch analysis of these videos to provide actionable insights. These insights will benefit a wide variety of agencies from traffic control to public safety. The second edition of the NVIDIA AI City Challenge being organized as a CVPR workshop provided a forum to more than 70 academic and industrial research teams to compete and solve real-world problems using traffic camera video data. The Challenge was launched with three tracks - speed estimation, anomaly detection, and vehicle re-identification. Each track was chosen in consultation with traffic and public safety officials based on the value of potential solutions. With the largest available data set for such tasks, and ground truth for each track the Challenge enabled 22 teams to evaluate their approaches. Given how complex these tasks are, the results are encouraging and reflect increased value addition year over year for the Challenge.",
    "code_link": "https://github.com/VehicleReId/VeRidataset"
  },
  "cvpr2018_w3_videoanalyticsinsmarttransportationfortheaic18challenge": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Video Analytics in Smart Transportation for the AIC'18 Challenge",
    "authors": [
      "Ming-Ching Chang",
      "Yi Wei",
      "Nenghui Song",
      "Siwei Lyu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Chang_Video_Analytics_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Chang_Video_Analytics_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " With the fast advancements of AICity and omnipresent street cameras, smart transportation can benefit greatly from actionable insights derived fromvideo analytics. We participate the NVIDIA AICity Challenge 2018 in all three tracks of challenges.In Track 1 challenge, we demonstrate automatic traffic flow analysisusing the detection and tracking of vehicles with robust speed estimation. In Track 2 challenge, we develop a reliable anomaly detectionpipeline that can recognize abnormal incidences including stalled vehicles and crashes with precise locations and time segments. In Track 3 challenge, we present an early result of vehicle re-identificationusing deep triplet-loss features that matches vehicles across 4 cameras in 15+ hours of videos.All developed methods are evaluated and compared against 30 contesting methods from 70 registered teams on the real-world challenge videos.",
    "code_link": "https://github.com/VehicleReId/VeRidataset"
  },
  "cvpr2018_w3_challengesonlargescalesurveillancevideoanalysis": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Challenges on Large Scale Surveillance Video Analysis",
    "authors": [
      "Weitao Feng",
      "Deyi Ji",
      "Yiru Wang",
      "Shuorong Chang",
      "Hansheng Ren",
      "Weihao Gan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Feng_Challenges_on_Large_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Feng_Challenges_on_Large_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Large scale surveillance video analysis is one of the most important components in the future artificial intelligent city. It is a very challenging but practical system, consists of multiple functionalities such as object detection, tracking, identification and behavior analysis. In this paper, we try to address three tasks hosted in NVIDIA AI City Challenge contest. First, a system that transforming the image coordinate to world coordinate has been proposed, which is useful to estimate the vehicle speed on the road. Second, anomalies like car crash event and stalled vehicles can be found by the proposed anomaly detector framework . Third, multiple camera vehicle re-identification problem has been investigated and a matching algorithm is explained. All these tasks are based on our proposed online single camera multiple object tracking (MOT) system, which has been evaluated on the widely used MOT16 challenge benchmark. We show that it achieves the best performance compared to the state-of-the-art methods. Besides of MOT, we evaluate the proposed vehicle re-identification model on VeRi-776 dataset and it outperforms all other methods with a large margin.",
    "code_link": ""
  },
  "cvpr2018_w3_graph@fitsubmissiontothenvidiaaicitychallenge2018": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Graph@FIT Submission to the NVIDIA AI City Challenge 2018",
    "authors": [
      "Jakub Sochor",
      "Jakub Spanhel",
      "Roman Juranek",
      "Petr Dobes",
      "Adam Herout"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Sochor_GraphFIT_Submission_to_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Sochor_GraphFIT_Submission_to_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In our submission to the NVIDIA AI City Challenge, we address speed measurement of vehicles and vehicle re-identification. For both these tasks, we use a calibration method based on extracted vanishing points. We detect and track vehicles by a CNN-based detector and we construct 3D bounding boxes for all vehicles. For the speed measurement task, we estimate the speed from the movement of the bounding box in the 3D space using the calibration. Our approach to vehicle re-identification is based on extraction of visual features from \"unpacked\" images of the vehicles. The features are aggregated in temporal domain to obtain a single feature descriptor for the whole track. Furthermore, we utilize a validation network to improve the re-identification accuracy.",
    "code_link": ""
  },
  "cvpr2018_w3_aic2018reporttrafficsurveillanceresearch": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "AIC2018 Report: Traffic Surveillance Research",
    "authors": [
      "Tingyu Mao",
      "Wei Zhang",
      "Haoyu He",
      "Yanjun Lin",
      "Vinay Kale",
      "Alexander Stein",
      "Zoran Kostic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Mao_AIC2018_Report_Traffic_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Mao_AIC2018_Report_Traffic_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Traffic surveillance and management technologies are one of the most intriguing aspects of smart city applications. In this paper, we investigate and present the methods for vehicle detections, tracking, speed estimation and anomaly detection in NVIDIA AI City Challenge 2018 (AIC2018). We applied Mask-RCNN and deep-sort for vehicle detection and tracking in track 1, and optical flow based method in track 2. In track 1, we achieve 100% detection rate and 7.97 mile/hour estimation error for speed estimation.",
    "code_link": ""
  },
  "cvpr2018_w3_speedestimationandabnormalitydetectionfromsurveillancecameras": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Speed Estimation and Abnormality Detection From Surveillance Cameras",
    "authors": [
      "Panagiotis Giannakeris",
      "Vagia Kaltsa",
      "Konstantinos Avgerinakis",
      "Alexia Briassouli",
      "Stefanos Vrochidis",
      "Ioannis Kompatsiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Giannakeris_Speed_Estimation_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Giannakeris_Speed_Estimation_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Motivated by the increasing industry trends towards autonomous driving, vehicles, and transportation we focus on developing a traffic analysis framework for the automatic exploitation of a large pool of available data relative to traffic applications. We propose a cooperative detection and tracking algorithm for the retrieval of vehicle trajectories in video surveillance footage based on deep CNN features that is ultimately used for two separate traffic analysis modalities: (a) vehicle speed estimation based on a state of the art fully automatic camera calibration algorithm and (b) the detection of possibly abnormal events in the scene using robust optical flow descriptors of the detected vehicles and Fisher vector representations of spatiotemporal visual volumes. Finally we measure the performance of our proposed methods in the NVIDIA AI CITY challenge evaluation dataset.",
    "code_link": ""
  },
  "cvpr2018_w3_trafficflowanalysiswithmultipleadaptivevehicledetectorsandvelocityestimationwithlandmark-basedscanlines": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Traffic Flow Analysis With Multiple Adaptive Vehicle Detectors and Velocity Estimation With Landmark-Based Scanlines",
    "authors": [
      "Minh-Triet Tran",
      "Tung Dinh-Duy",
      "Thanh-Dat Truong",
      "Vinh Ton-That",
      "Thanh-Nhon Do",
      "Quoc-An Luong",
      "Thanh-An Nguyen",
      "Vinh-Tiep Nguyen",
      "Minh N. Do"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Tran_Traffic_Flow_Analysis_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Tran_Traffic_Flow_Analysis_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose our method for vehicle detection with multiple adaptive vehicle detectors and velocity estimation with landmark-based scanlines. Inspired by the idea for tiny object detection, we use Faster R-CNN with Resnet-101 to create different specialized vehicle detectors corresponding to different levels of details and poses. We propose a heuristic to check the fitness of a particular vehicle detector to a specific region in camera's view by the mean velocity direction and the mean object size. By this way, we can determine an adaptive set of appropriate vehicle detectors for each region in camera's view. Thus our system is expected to detect vehicles with high accuracy, both in precision and recall, even with tiny objects.We exploit the U.S. road rules for the length and distance of broken white lines on roads to propose our method for vehicle's velocity estimation using such landmarks. We determine equally-distributed scanlines, virtual parallel lines that are nearly-perpendicular to the road direction, with reference to the line connecting the corresponding ends of multiple broken white lines. From the timespan for a vehicle to cross two consecutive virtual scanlines, we can calculate the average vehicle's velocity within that road segment. We also refine the speed estimation by detecting when a vehicle stops at a traffic light, and smooth the results with a moving average filter. Experiments on the dataset of Traffic Flow Analysis from NVIDIA AI City Challenge 2018 show that our method achieves the perfect detect rate of 100%, the average velocity difference of 6.9762 mph on freeways, and 8.9144 mph on both freeways and urban roads.",
    "code_link": ""
  },
  "cvpr2018_w3_single-cameraandinter-cameravehicletrackingand3dspeedestimationbasedonfusionofvisualandsemanticfeatures": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Single-Camera and Inter-Camera Vehicle Tracking and 3D Speed Estimation Based on Fusion of Visual and Semantic Features",
    "authors": [
      "Zheng Tang",
      "Gaoang Wang",
      "Hao Xiao",
      "Aotian Zheng",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Tang_Single-Camera_and_Inter-Camera_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Tang_Single-Camera_and_Inter-Camera_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Tracking of vehicles across multiple cameras with non-overlapping views has been a challenging task for the intelligent transportation system (ITS). It is mainly because of high similarity among vehicle models, frequent occlusion, large variation in different viewing perspectives and low video resolution. In this work, we propose a fusion of visual and semantic features for both single-camera tracking (SCT) and inter-camera tracking (ICT). Specifically, a histogram-based adaptive appearance model is introduced to learn long-term history of visual features for each vehicle target. Besides, semantic features including trajectory smoothness, velocity change and temporal information are incorporated into a bottom-up clustering strategy for data association in each single camera view. Across different camera views, we also exploit other information, such as deep learning features, detected license plate features and detected car types, for vehicle re-identification. Additionally, evolutionary optimization is applied to camera calibration for reliable 3D speed estimation. Our algorithm achieves the top performance in both 3D speed estimation and vehicle re-identification at the NVIDIA AI City Challenge 2018.",
    "code_link": ""
  },
  "cvpr2018_w3_geometry-awaretrafficflowanalysisbydetectionandtracking": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Geometry-Aware Traffic Flow Analysis by Detection and Tracking",
    "authors": [
      "Honghui Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Shi_Geometry-Aware_Traffic_Flow_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Shi_Geometry-Aware_Traffic_Flow_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In the second Nvidia AI City Challenge hosted in 2018, the traffic flow analysis challenge proposes an interest task that requires participants to predict the speed of vehicles on road from various traffic camera videos. We propose a simple yet effective method combing both learning based detection and geometric calibration based estimation.We use a learning based method to detect and track vehicles, and use a geometry based camera calibration method to calculate the speed of those vehicles. We achieve a perfect detection rate of target vehicles and a root mean square error (RMSE) of 6.6674 in predicting the vehicle speed, which rank us the third place in the competition.",
    "code_link": ""
  },
  "cvpr2018_w3_vehiclere-identificationwiththespace-timeprior": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Vehicle Re-Identification With the Space-Time Prior",
    "authors": [
      "Chih-Wei Wu",
      "Chih-Ting Liu",
      "Cheng-En Chiang",
      "Wei-Chih Tu",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Wu_Vehicle_Re-Identification_With_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Wu_Vehicle_Re-Identification_With_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Vehicle re-identification (Re-ID) is fundamentally challenging due to the difficulties in data labeling, visual domain mismatch between datasets and diverse appearance of the same vehicle. We propose the adaptive feature learning technique based on the space-time prior to address these issues. The idea is demonstrated effectively in both the human Re-ID and the vehicle Re-ID tasks. We train a vehicle feature extractor in a multi-task learning manner on three existing vehicle datasets and fine-tune the feature extractor with the adaptive feature learning technique on the target domain. We then develop a vehicle Re-ID system based on the learned vehicle feature extractor. Finally, our meticulous system design leads to the second place in the 2018 NVIDIA AI City Challenge Track 3.",
    "code_link": ""
  },
  "cvpr2018_w3_unsupervisedanomalydetectionfortrafficsurveillancebasedonbackgroundmodeling": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Unsupervised Anomaly Detection for Traffic Surveillance Based on Background Modeling",
    "authors": [
      "JiaYi Wei",
      "JianFei Zhao",
      "YanYun Zhao",
      "ZhiCheng Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Wei_Unsupervised_Anomaly_Detection_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Wei_Unsupervised_Anomaly_Detection_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Most state-of-the-art anomaly detection methods are specific to detecting anomaly for pedestrians and cannot work without adequate normal training videos. Recently, there is a growing demand for detecting anomalous vehicles in traffic surveillance videos. However, the biggest challenge in this task is the lack of labeled datasets for training supervised models. By examining the resemblances of anomalous vehicles, we find it reasonable to label a vehicle as anomaly if it stays still in the video for a relatively long time. Utilizing this property, in this paper we introduce a novel unsupervised anomaly detection method for traffic surveilliance based on background modeling, which shows great potentials in handling heterogeneous scenes as well as extremely low resolution videos recordings without the dependence on labeled data. In the proposed system, we first employ background modeling using MOG2 to remove the moving vehicles as foreground while keeping the stopped vehicles as part of the background. Then we use Faster R-CNN to detect vehicles in the extracted background and decide if they are new anomalies under certain conditions. All information is updated on a frame basis until the end of the video which contains the final results. In this way, we make full use of the characteristics that abnormal vehicles stay in the scene for a relatively long time and reduce the difficulty of vehicle anomaly detection. Eventually, we can detect almost every anomaly in the NVIDIA AI CITY CHALLENGE track-2 dataset except for several extremely complex cases with a 81.08% F1-score and 10.2369 RMSE.",
    "code_link": ""
  },
  "cvpr2018_w3_asemi-automatic2dsolutionforvehiclespeedestimationfrommonocularvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "A Semi-Automatic 2D Solution for Vehicle Speed Estimation From Monocular Videos",
    "authors": [
      "Amit Kumar",
      "Pirazh Khorramshahi",
      "Wei-An Lin",
      "Prithviraj Dhar",
      "Jun-Cheng Chen",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Kumar_A_Semi-Automatic_2D_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Kumar_A_Semi-Automatic_2D_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work, we present a novel approach for vehicle speed estimation from monocular videos. The pipeline consists of modules for multi-object detection, robust tracking, and speed estimation. The tracking algorithm has the capability for jointly tracking individual vehicles and estimating velocities in the image domain. However, since camera parameters are often unavailable and extensive variations are present in the scenes, transforming measurements in the image domain to real world is challenging. We propose a simple two-stage algorithm to approximate the transformation. Images are first rectified to restore affine properties, then the scaling factor is compensated for each scene. We show the effectiveness of the proposed method with extensive experiments on the traffic speed analysis dataset in the NVIDIA AI City challenge. We achieve a detection rate of 1.0 in vehicle detection and tracking, and Root Mean Square Error of 9.54 (mph) for the task of vehicle speed estimation in unconstrained traffic videos.",
    "code_link": ""
  },
  "cvpr2018_w3_dual-modevehiclemotionpatternlearningforhighperformanceroadtrafficanomalydetection": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Dual-Mode Vehicle Motion Pattern Learning for High Performance Road Traffic Anomaly Detection",
    "authors": [
      "Yan Xu",
      "Xi Ouyang",
      "Yu Cheng",
      "Shining Yu",
      "Lin Xiong",
      "Choon-Ching Ng",
      "Sugiri Pranata",
      "Shengmei Shen",
      "Junliang Xing"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Xu_Dual-Mode_Vehicle_Motion_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Xu_Dual-Mode_Vehicle_Motion_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Anomaly detection on road traffic is an important task due to its great potential in urban traffic management and road safety. It is also a very challenging task since the abnormal event happens very rarely and exhibits different behaviors. In this work, we present a model to detect anomaly in road traffic by learning from the vehicle motion patterns in two distinctive yet correlated modes, the static mode and the dynamic mode, of the vehicles. The static mode analysis of the vehicles is learned from the background modeling followed by vehicle detection procedure to find the abnormal vehicles that keep still on the road. The dynamic mode analysis of the vehicles is learned from detected and tracked vehicle trajectories to find the abnormal trajectory which is aberrant from the dominant motion patterns. The results from the dual-mode analyses are finally fused together by driven a re-identification model to obtain the final anomaly. Experimental results on the Track 2 testing set of NVIDIA AI CITY CHALLENGE show the effectiveness of the proposed dual-mode learning model and its robustness in different real scenes. Our result ranks the first place on the final Leaderboard of the Track 2.",
    "code_link": ""
  },
  "cvpr2018_w3_vehicletrackingandspeedestimationfromtrafficvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Vehicle Tracking and Speed Estimation From Traffic Videos",
    "authors": [
      "Shuai Hua",
      "Manika Kapoor",
      "David C. Anastasiu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Hua_Vehicle_Tracking_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Hua_Vehicle_Tracking_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The rapid recent advancements in the computation ability of everyday computers have made it possible to widely apply deep learning methods to the analysis of traffic surveillance videos. Traffic flow prediction, anomaly detection, vehicle re-identification, and vehicle tracking are basic components in traffic analysis. Among these applications, traffic flow prediction, or vehicle speed estimation, is one of the most important research topics of recent years. Good solutions to this problem could prevent traffic collisions and help improve road planning by better estimating transit demand. In the 2018 NVIDIA AI City Challenge, we combine modern deep learning models with classic computer vision approaches to propose an efficient way to predict vehicle speed. In this paper, we introduce some state-of-the-art approaches in vehicle speed estimation, vehicle detection, and object tracking, as well as our solution for Track 1 of the Challenge.",
    "code_link": ""
  },
  "cvpr2018_w3_trafficspeedestimationfromsurveillancevideodata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Traffic Speed Estimation From Surveillance Video Data",
    "authors": [
      "Tingting Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Huang_Traffic_Speed_Estimation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Huang_Traffic_Speed_Estimation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Estimating traffic flow condition is a tough but beneficial task. In Intelligent Transportation System (ITS), many applications have been done to collect and analyze traffic data. However, the surveillance video data are still only used for engineer's manual check. To better utilize this data source, traffic flow estimation from surveillance camera should be explored. This study uses Faster Regional Convolutional Neural Network (Faster R-CNN) with ResNet 101 as the backbone to achieve multi-object detection. Then a tracking algorithm based on histogram comparison is applied to link objects across frames. Finally, this study uses warping method to convert vehicle speeds from the pixel domain to the real world. The results show that estimating vehicle speed at intersection is more challenging than in uninterrupted flow.",
    "code_link": "https://github.com/tensorflow/models"
  },
  "cvpr2018_w3_unsupervisedvehiclere-identificationusingtripletnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - NVIDIA AI City Challenge",
    "title": "Unsupervised Vehicle Re-Identification Using Triplet Networks",
    "authors": [
      "Pedro Antonio Marin-Reyes",
      "Andrea Palazzi",
      "Luca Bergamini",
      "Simone Calderara",
      "Javier Lorenzo-Navarro",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w3/html/Marin-Reyes_Unsupervised_Vehicle_Re-Identification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w3/Marin-Reyes_Unsupervised_Vehicle_Re-Identification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Vehicle re-identification plays a major role in modern smart surveillance systems. Specifically, the task requires the capability to predict the identity of a given vehicle, given a dataset of known associations, collected from different views and surveillance cameras. Generally, it can be cast as a ranking problem: given a probe image of a vehicle, the model needs to rank all database images based on their similarities w.r.t the probe image. In line with recent research, we devise a metric learning model that employs a supervision based on local constraints. In particular, we leverage pairwise and triplet constraints for training a network capable of assigning a high degree of similarity to samples sharing the same identity, while keeping different identities distant in feature space. Eventually, we show how vehicle tracking can be exploited to automatically generate a weakly labelled dataset that can be used to train the deep network for the task of vehicle re-identification. Learning and evaluation is carried out on the NVIDIA AI city challenge videos.",
    "code_link": ""
  },
  "cvpr2018_w4_deepglobe2018achallengetoparsetheearththroughsatelliteimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "DeepGlobe 2018: A Challenge to Parse the Earth Through Satellite Images",
    "authors": [
      "Ilke Demir",
      "Krzysztof Koperski",
      "David Lindenbaum",
      "Guan Pang",
      "Jing Huang",
      "Saikat Basu",
      "Forest Hughes",
      "Devis Tuia",
      "Ramesh Raskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Demir_DeepGlobe_2018_A_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Demir_DeepGlobe_2018_A_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which includes three public competitions for segmentation, detection, and classification tasks on satellite images (Figure \\ref fig:datas ). Similar to other challenges in computer vision domain such as DAVIS[??] and COCO[??], DeepGlobe proposes three datasets and corresponding evaluation methodologies, coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2018. We observed that satellite imagery is a rich and structured source of information, yet it is less investigated than everyday images by computer vision researchers. However, bridging modern computer vision with remote sensing data analysis could have critical impact to the way we understand our environment and lead to major breakthroughs in global urban planning or climate change research. Keeping such bridging objective in mind, DeepGlobe aims to bring together researchers from different domains to raise awareness of remote sensing in the computer vision community and vice-versa. We aim to improve and evaluate state-of-the-art satellite image understanding approaches, which can hopefully serve as reference benchmarks for future research in the same topic. In this paper, we analyze characteristics of each dataset, define the evaluation criteria of the competitions, and provide baselines for each task.",
    "code_link": ""
  },
  "cvpr2018_w4_d-linknetlinknetwithpretrainedencoderanddilatedconvolutionforhighresolutionsatelliteimageryroadextraction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "D-LinkNet: LinkNet With Pretrained Encoder and Dilated Convolution for High Resolution Satellite Imagery Road Extraction",
    "authors": [
      "Lichen Zhou",
      "Chuang Zhang",
      "Ming Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Zhou_D-LinkNet_LinkNet_With_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Zhou_D-LinkNet_LinkNet_With_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Road extraction is a fundamental task in the field of remote sensing which has been a hot research topic in the past decade. In this paper, we propose a semantic segmentation neural network, named D-LinkNet, which adopts encoder-decoder structure, dilated convolution and pretrained encoder for road extraction task. The network is built with LinkNet architecture and has dilated convolution layers in its center part. Linknet architecture is efficient in computation and memory. Dilation convolution is a powerful tool that can enlarge the receptive field of feature points without reducing the resolution of the feature maps. In the CVPR DeepGlobe 2018 Road Extraction Challenge, our best IoU scores on the validation set and the test set are 0.6466 and 0.6342 respectively.",
    "code_link": ""
  },
  "cvpr2018_w4_buildingdetectionfromsatelliteimageryusingensembleofsize-specificdetectors": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Building Detection From Satellite Imagery Using Ensemble of Size-Specific Detectors",
    "authors": [
      "Ryuhei Hamaguchi",
      "Shuhei Hikosaka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Hamaguchi_Building_Detection_From_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Hamaguchi_Building_Detection_From_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In recent years, convolutional neural networks (CNNs) show remarkably high performance in building detection tasks. While much progress has been made, there are two aspects that have not been considered well in the past: how to address a wide variation in building size, and how to well incorporate with context information such as roads. To answer these questions, we propose a simple, but effective multi-task model. The model learns multiple detectors each of which is dedicated to a specific size of buildings. Moreover, the model implicitly utilizes context information by simultaneously training road extraction task along with building detection task. The road extractor is trained by distilling knowledge from another pre-trained CNN, requiring no labels for roads in its training. Our experiments show that the proposed model significantly improves the building detection accuracy.",
    "code_link": ""
  },
  "cvpr2018_w4_densefusionclassmatenetworkforlandcoverclassification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Dense Fusion Classmate Network for Land Cover Classification",
    "authors": [
      "Chao Tian",
      "Cong Li",
      "Jianping Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Tian_Dense_Fusion_Classmate_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Tian_Dense_Fusion_Classmate_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recently, FCNs based methods have made great progress in semantic segmentation. Different with ordinary scenes, satellite image owns specific characteristics, which elements always extend to large scope and no regular or clear boundaries. Therefore, effective mid-level structure information extremely missing, precise pixel-level classification becomes tough issues. In this paper, a Dense Fusion Classmate Network (DFCNet) is proposed to adopt in land cover classification. DFCNet is jointly trained with auxiliary road dataset seemed as \"classmate\", which properly compensates the lack of mid-level information. Meanwhile, a dense fusion module is also integrated, which guarantees the precise discrimination of confused pixels and benefits the network optimization from scratch. Score on DeepGlobe land cover classification competition shows that our approach has achieved good performance.",
    "code_link": ""
  },
  "cvpr2018_w4_semanticbinarysegmentationusingconvolutionalnetworkswithoutdecoders": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Semantic Binary Segmentation Using Convolutional Networks Without Decoders",
    "authors": [
      "Shubhra Aich",
      "William van der Kamp",
      "Ian Stavness"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Aich_Semantic_Binary_Segmentation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Aich_Semantic_Binary_Segmentation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose an efficient architecture for semantic image segmentation using the depth-to-space (D2S) operation. Our D2S model is comprised of a standard CNN encoder followed by a depth-to-space reordering of the final convolutional feature maps. Our approach eliminates the decoder portion of traditional encoder-decoder segmentation models and reduces the amount of computation almost by half. As a participant of the DeepGlobe Road Extraction competition, we evaluate our models on the corresponding road segmentation dataset. Our highly efficient D2S models exhibit comparable performance to standard segmentation models with much lower computational cost.",
    "code_link": "https://github.com/littleaich/deepglobe2018"
  },
  "cvpr2018_w4_stackedu-netswithmulti-outputforroadextraction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Stacked U-Nets With Multi-Output for Road Extraction",
    "authors": [
      "Tao Sun",
      "Zehui Chen",
      "Wenxiang Yang",
      "Yin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Sun_Stacked_U-Nets_With_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Sun_Stacked_U-Nets_With_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " With the recent advances of Convolutional Neural Networks(CNN) in computer vision, there have been rapid progresses in extracting roads and other features fromsatellite imagery for mapping and other purposes.In this paper, we propose a new method for road extraction using stacked U-Nets with multiple output.A hybrid loss function is used to address the problem of unbalanced classes of training data.Post-processing methods, including road map vectorization and shortest path search with hierarchical thresholds, help improve recall.The overall improvement of mean IoU compared to the vanilla VGG network is more than 20%.",
    "code_link": ""
  },
  "cvpr2018_w4_fullyconvolutionalnetworkforautomaticroadextractionfromsatelliteimagery": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Fully Convolutional Network for Automatic Road Extraction From Satellite Imagery",
    "authors": [
      "Alexander Buslaev",
      "Selim Seferbekov",
      "Vladimir Iglovikov",
      "Alexey Shvets"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Buslaev_Fully_Convolutional_Network_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Buslaev_Fully_Convolutional_Network_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Analysis of high resolution satellite images has been an important research topic for traffic management, city planning and road monitoring. One of the problem here is automatic and precise road extraction. From an original image it is difficult and computationally expensive to extract roads due to presences of other road-like features with straight edges. In this paper we propose an approach for automatic road extraction based on fully convolutional neural network of U-net family. This network is consisted of ResNet-34 pre-trained on ImageNet and decoder adapted from vanilla U-Net. Based on validation results, leaderboardand our own experience this network shows superior results for the DEEPGLOBE - CVPR 2018 road extraction sub-challenge. Moreover, this network uses moderate memory that allows to use just one GTX 1080 or 1080ti video cards to preform whole training and makes pretty fast predictions.",
    "code_link": ""
  },
  "cvpr2018_w4_roaddetectionwitheosresunetandpostvectorizingalgorithm": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Road Detection With EOSResUNet and Post Vectorizing Algorithm",
    "authors": [
      "Oleksandr Filin",
      "Anton Zapara",
      "Serhii Panchenko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Filin_Road_Detection_With_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Filin_Road_Detection_With_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Object recognition on the satellite images is one of the most relevant and popular topics in the problem of pattern recognition. This was facilitated by many factors, such as a high number of satellites with high-resolution imagery, the significant development of computer vision, especially with a major breakthrough in the field of convolutional neural networks, a wide range of industry verticals for usage and still a quite empty market. Roads are one of the most popular objects for recognition. In this article, we want to present you the combination of work of neural network and postprocessing algorithm, due to which we get not only the coverage mask but also the vectors of all of the individual roads that are present in the image and can be used to address the higher-level tasks in the future. This approach was used to solve the DeepGlobe Road Extraction Challenge",
    "code_link": ""
  },
  "cvpr2018_w4_residualinceptionskipnetworkforbinarysegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Residual Inception Skip Network for Binary Segmentation",
    "authors": [
      "Jigar Doshi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Doshi_Residual_Inception_Skip_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Doshi_Residual_Inception_Skip_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper summarizes our approach to the Deep GlobeRoad Extraction challenge 2018.In this challenge, we are tasked to find road networks from satellite images. First, we explain our U-Net type baseline model for the challenge.Second, we explain a new architecture that takes in the lessons from some of the popular approaches that we call Residual Inception Skip Net. Finally, we outline our cyclic learning rate based ensembling approach which improved the overall single model performance and the final solution for submission.Our final model increases the IoU by 3 points over the baseline.",
    "code_link": ""
  },
  "cvpr2018_w4_roadmapgenerationusingamulti-stageensembleofdeepneuralnetworkswithsmoothing-basedoptimization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Roadmap Generation Using a Multi-Stage Ensemble of Deep Neural Networks With Smoothing-Based Optimization",
    "authors": [
      "Dragos Costea",
      "Alina Marcu",
      "Emil Slusanschi",
      "Marius Leordeanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Costea_Roadmap_Generation_Using_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Costea_Roadmap_Generation_Using_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Road detection from aerial images is a challenging task for humans and machines alike. Occlusion, the lack of visual cues and slim class borders for other road-like structures (such as pathways or private alleys) make the problem inherently ambiguous, requiring logic that goes beyond the input image. We propose a three-stage method for the task of road segmentation - first, an ensemble of multiple U-Net like CNNs generate binary road masks. Second, an optimization algorithm generates road vectors with their corresponding thickness based on the fusion of the road maps from the first stage. Third, missing links are added based on the inferred graph to improve segmentation.",
    "code_link": ""
  },
  "cvpr2018_w4_rotatedrectanglesforsymbolizedbuildingfootprintextraction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Rotated Rectangles for Symbolized Building Footprint Extraction",
    "authors": [
      "Matt Dickenson",
      "Lionel Gueguen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Dickenson_Rotated_Rectangles_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Dickenson_Rotated_Rectangles_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Building footprints (BFP) provide useful visual context for users of digital maps when navigating in space. This paper proposes a method for extracting and symbolizing building footprints from satellite imagery using a convolu- tional neural network (CNN). The CNN architecture out- puts rotated rectangles, providing a symbolized approxi- mation that works well for small buildings. Experiments are conducted on the four cities in the DeepGlobe Chal- lenge dataset (Las Vegas, Paris, Shanghai, Khartoum). Our method performs best on suburbs consisting of individual houses. These experiments show that either large buildings or buildings without clear delineation produce weaker re- sults in terms of precision and recall.",
    "code_link": ""
  },
  "cvpr2018_w4_buildingdetectionfromsatelliteimageryusingacompositelossfunction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Building Detection From Satellite Imagery Using a Composite Loss Function",
    "authors": [
      "Sergey Golovanov",
      "Rauf Kurbanov",
      "Aleksey Artamonov",
      "Alex Davydow",
      "Sergey Nikolenko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Golovanov_Building_Detection_From_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Golovanov_Building_Detection_From_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we present a LinkNet-based architecture with SE-ResNeXt-50 encoder and a novel training strategy that strongly relies on image preprocessing and incorporating distorted network outputs. The architecture combines a pre-trained convolutional encoder and a symmetric expanding path that enables precise localization. We show that such a network can be trained on plain RGB images with a composite loss function and achieves competitive results on the DeepGlobe challenge on building extraction from satellite images.",
    "code_link": ""
  },
  "cvpr2018_w4_ternausnetv2fullyconvolutionalnetworkforinstancesegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "TernausNetV2: Fully Convolutional Network for Instance Segmentation",
    "authors": [
      "Vladimir Iglovikov",
      "Selim Seferbekov",
      "Alexander Buslaev",
      "Alexey Shvets"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Iglovikov_TernausNetV2_Fully_Convolutional_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Iglovikov_TernausNetV2_Fully_Convolutional_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The most common approaches to instance segmentation are complex and use two-stage networks with object proposals, conditional random-fields, template matching or recurrent neural networks. In this work we present TernausNetV2 - a simple fully convolutional network that allows extracting objects from a high-resolution satellite imagery on an instance level. The network has popular encoder-decoder type of architecture with skip connections but has a few essential modifications that allows using for semantic as well as for instance segmentation tasks. This approach is universal and allows to extend any network that has been successfully applied for semantic segmentation to perform instance segmentation task. In addition, we generalize network encoder that was pre-trained for RGB images to use additional input channels. It makes possible to use transfer learning from visual to a wider spectral range. For DeepGlobe-CVPR 2018 building detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods.",
    "code_link": ""
  },
  "cvpr2018_w4_semanticsegmentationbasedbuildingextractionmethodusingmulti-sourcegismapdatasetsandsatelliteimagery": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Semantic Segmentation Based Building Extraction Method Using Multi-Source GIS Map Datasets and Satellite Imagery",
    "authors": [
      "Weijia Li",
      "Conghui He",
      "Jiarui Fang",
      "Haohuan Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Li_Semantic_Segmentation_Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Li_Semantic_Segmentation_Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper describes our proposed building extraction method in DeepGlobe - CVPR 2018 Satellite Challenge. We proposed a semantic segmentation and ensemble learning based building extraction method for high resolution satellite images. Several public GIS map datasets were utilized through combining with the multispectral WorldView-3 satellite image datasets for improving the building extraction results. Our proposed method achieves the overall prediction score of 0.701 on the test dataset in DeepGlobe Building Extraction Challenge.",
    "code_link": ""
  },
  "cvpr2018_w4_cnnsfusionforbuildingdetectioninaerialimagesforthebuildingdetectionchallenge": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "CNNs Fusion for Building Detection in Aerial Images for the Building Detection Challenge",
    "authors": [
      "Remi Delassus",
      "Romain Giot"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Delassus_CNNs_Fusion_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Delassus_CNNs_Fusion_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents our contribution to the DeepGlobe Building Detection Challenge.We enhanced the SpaceNet Challenge winning solution by proposing a new fusion strategy based on a deep combiner using segmentation both results of different CNN and input data to segment.Segmentation results for all cities have been significantly improved (between 1% improvement over the baseline for the smallest one to more than 7% for the biggest one). The separation of adjacent buildingsshould be the next enhancement made to the solution.",
    "code_link": ""
  },
  "cvpr2018_w4_buildingextractionfromsatelliteimagesusingmaskr-cnnwithbuildingboundaryregularization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Building Extraction From Satellite Images Using Mask R-CNN With Building Boundary Regularization",
    "authors": [
      "Kang Zhao",
      "Jungwon Kang",
      "Jaewook Jung",
      "Gunho Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Zhao_Building_Extraction_From_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Zhao_Building_Extraction_From_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The DeepGlobe Building Extraction Challenge poses the problem of localizing all building polygons in the given satellite images. We can create polygons using an existing instance segmentation algorithm based on Mask R-CNN. However, polygons produced from instance segmentation have irregular shapes, which are far different from real building footprint boundaries and therefore cannot be directly applied to many cartographic and engineering applications. Hence, we present a method combining Mask R-CNN with building boundary regularization. Through the experiments, we find that the proposed method and Mask R-CNN achieve almost equivalent performance in terms of accuracy and completeness. However, compared to Mask R-CNN, our method produces better regularized polygons which are beneficial in many applications.",
    "code_link": ""
  },
  "cvpr2018_w4_deepaggregationnetforlandcoverclassification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Deep Aggregation Net for Land Cover Classification",
    "authors": [
      "Tzu-Sheng Kuo",
      "Keng-Sen Tseng",
      "Jia-Wei Yan",
      "Yen-Cheng Liu",
      "Yu-Chiang Frank Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Kuo_Deep_Aggregation_Net_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Kuo_Deep_Aggregation_Net_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Land cover classification aims at classifying each pixel in a satellite image into a particular land cover category, which can be regarded as a multi-class semantic segmentation task. In this paper, we propose a deep aggregation network for solving this task, which extracts and combines multi-layer features during the segmentation process. In particular, we introduce soft semantic labels and graph-based fine tuning in our proposed network for improving the segmentation performance. In our experiments, we demonstrate that our network performs favorably against state-of-the-art models on the dataset of DeepGlobe Satellite Challenge, while our ablation study further verifies the effectiveness of our proposed network architecture.",
    "code_link": ""
  },
  "cvpr2018_w4_stackedu-netsforgroundmaterialsegmentationinremotesensingimagery": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Stacked U-Nets for Ground Material Segmentation in Remote Sensing Imagery",
    "authors": [
      "Arthita Ghosh",
      "Max Ehrlich",
      "Sohil Shah",
      "Larry S. Davis",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Ghosh_Stacked_U-Nets_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Ghosh_Stacked_U-Nets_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a semantic segmentation algorithm for RGB remote sensing images. Our method is based on the Dilated Stacked U-Nets architecture. This state-of-the-art method has been shown to have good performance in other applications. We perform additional post-processing by blending image tiles and degridding the result. Our method gives competitive results on the DeepGlobe dataset.",
    "code_link": ""
  },
  "cvpr2018_w4_landcoverclassificationfromsatelliteimagerywithu-netandlovasz-softmaxloss": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Land Cover Classification From Satellite Imagery With U-Net and Lovasz-Softmax Loss",
    "authors": [
      "Alexander Rakhlin",
      "Alex Davydow",
      "Sergey Nikolenko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Rakhlin_Land_Cover_Classification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Rakhlin_Land_Cover_Classification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The land cover classification task of the DeepGlobe Challenge presents significant obstacles even to state of the art segmentation models due to a small amount of data, incomplete and sometimes incorrect labeling, and highly imbalanced classes. In this work, we show an approach based on the U-Net architecture with the Lovasz-Softmax loss that successfully alleviates these problems; we compare several different convolutional architectures for U-Net encoders.",
    "code_link": ""
  },
  "cvpr2018_w4_nu-netdeepresidualwidefieldofviewconvolutionalneuralnetworkforsemanticsegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "NU-Net: Deep Residual Wide Field of View Convolutional Neural Network for Semantic Segmentation",
    "authors": [
      "Mohamed Samy",
      "Karim Amer",
      "Kareem Eissa",
      "Mahmoud Shaker",
      "Mohamed ElHelw"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Samy_NU-Net_Deep_Residual_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Samy_NU-Net_Deep_Residual_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Semantic Segmentation of satellite images is one of the most challenging problems in computer vision as it requires a model capable of capturing both local and global information at each pixel. Current state of the art methods are based on Fully Convolutional Neural Networks (FCNN) with mostly two main components: an encoder which is a pretrained model on classification that gradually reduces the input spatial size and a decoder that transforms the encoder's feature map into a predicted mask with the original size. We change this conventional architecture to a model that makes use of the full resolution information. NU-Net is a deep FCNN that is able to capture wide field of global information around each pixel while maintaining full resolution information throughout the model. We evaluate our model on the Road Extraction track and Land Cover Classification track in Deep Globe competition.",
    "code_link": ""
  },
  "cvpr2018_w4_featurepyramidnetworkformulti-classlandsegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Feature Pyramid Network for Multi-Class Land Segmentation",
    "authors": [
      "Selim Seferbekov",
      "Vladimir Iglovikov",
      "Alexander Buslaev",
      "Alexey Shvets"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Seferbekov_Feature_Pyramid_Network_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Seferbekov_Feature_Pyramid_Network_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Semantic segmentation is in-demand in satellite imagery processing. Because of the complex environment, automatic categorization and segmentation of land cover is challenging problem solving which can help to overcome many obstacles in urban planning, environmental engineering or natural landscape monitoring. In this paper we propose an approach for automatic multi-class land segmentation based on fully convolutional neural network of feature pyramid network (FPN) family. This network is consisted of pre-trained on ImageNet Resnet50 encoder and neatly developed decoder. Based on validation results, leader-board score and our own experience this network shows reliable results for the DEEPGLOBE - CVPR 2018 land cover classification sub-challenge. Moreover, this network moderately uses memory that allowsto use GTX 1080 or 1080 TI video cards to preform whole training and makes pretty fast predictions.",
    "code_link": ""
  },
  "cvpr2018_w4_uncertaintygatednetworkforlandcoversegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Uncertainty Gated Network for Land Cover Segmentation",
    "authors": [
      "Guillem Pascual",
      "Santi Segui",
      "Jordi Vitria"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Pascual_Uncertainty_Gated_Network_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Pascual_Uncertainty_Gated_Network_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The production of thematic maps depicting land cover is one of the most common applications of remote sensing. To this end, several semantic segmentation approaches, based on deep learning, have been proposed in the literature, but land cover segmentation is stillconsidered an open problem due to some specific problems related to remote sensing imaging. In this paper we propose a novel approach to deal with the problem of modelling multiscale contexts surrounding pixels of different land cover categories. The approach leverages the computation of a heteroscedastic measure of uncertainty when classifying individual pixels in an image. This classification uncertainty measure is used to define a set of memory gates between layers that allow a principled method to select the optimal decision for each pixel.",
    "code_link": ""
  },
  "cvpr2018_w4_landcoverclassificationwithsuperpixelsandjaccardindexpost-optimization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - DeepGlobe: A Challenge for Parsing the Earth through Satellite Images",
    "title": "Land Cover Classification With Superpixels and Jaccard Index Post-Optimization",
    "authors": [
      "Alex Davydow",
      "Sergey Nikolenko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w4/html/Davydow_Land_Cover_Classification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w4/Davydow_Land_Cover_Classification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work, we consider the land cover classification task of the DeepGlobe Challenge. This task features the largest available labeled dataset for satellite imagery segmentation. We propose an approach to this problem where standard neural network image classification models are augmented by superpixel extraction and postprocessing that aims to directly optimize the average Jaccard index.",
    "code_link": ""
  },
  "cvpr2018_w6_adaptationandre-identificationnetworkanunsuperviseddeeptransferlearningapproachtopersonre-identification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Humans in Crowd Scene and Look Into Person Challenge",
    "title": "Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification",
    "authors": [
      "Yu-Jhe Li",
      "Fu-En Yang",
      "Yen-Cheng Liu",
      "Yu-Ying Yeh",
      "Xiaofei Du",
      "Yu-Chiang Frank Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w6/html/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w6/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.",
    "code_link": ""
  },
  "cvpr2018_w6_attentioninmultimodalneuralnetworksforpersonre-identification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Humans in Crowd Scene and Look Into Person Challenge",
    "title": "Attention in Multimodal Neural Networks for Person Re-Identification",
    "authors": [
      "Aske R. Lejbolle",
      "Benjamin Krogh",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w6/html/Lejbolle_Attention_in_Multimodal_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w6/Lejbolle_Attention_in_Multimodal_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In spite of increasing interest from the research community, person re-identification remains an unsolved problem. Correctly deciding on a true match by comparing images of a person, captured by several cameras, requires extraction of discriminative features to counter challenges such as changes in lighting, viewpoint and occlusion. Besides devising novel feature descriptors, the setup can be changed to capture persons from an overhead viewpoint rather than a horizontal. Furthermore, additional modalities can be considered that are not affected by similar environmental changes as RGB images. In this work, we present a Multimodal ATtention network (MAT) based on RGB and depth modalities. We combine a Convolution Neural Network with an attention module to extract local and discriminative features that are fused with globally extracted features. Attention is based on correlation between the two modalities and we finally also fuse RGB and depth features to generate a joint RGB-D feature. Experiments conducted on three datasets captured from an overhead view show the importance of attention, increasing accuracies by 3.43%, 2.01% and 2.13% on OPR, DPI-T and TVPR, respectively.",
    "code_link": ""
  },
  "cvpr2018_w6_poseencodingforrobustskeleton-basedactionrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Humans in Crowd Scene and Look Into Person Challenge",
    "title": "Pose Encoding for Robust Skeleton-Based Action Recognition",
    "authors": [
      "Girum G. Demisse",
      "Konstantinos Papadopoulos",
      "Djamila Aouada",
      "Bjorn Ottersten"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w6/html/Demisse_Pose_Encoding_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w6/Demisse_Pose_Encoding_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Some of the main challenges in skeleton-basedaction recognition systems are redundant and noisy pose transformations. Earlier works in skeleton-based action recognition explored different approaches for filtering linear noise transformations, but neglect to address potential nonlinear transformations. In this paper, we present an unsupervised learning approach for estimating nonlinear noise transformations in pose estimates. Our approach starts by decoupling linear and nonlinear noise transformations. While the linear transformations are modelled explicitly the nonlinear transformations are learned from data. Subsequently, we use an autoencoder with L_ 2 -norm reconstruction error and show that it indeed does capture nonlinear noise transformations, and recover a denoised pose estimate which in turn improves performance significantly. We validate our approach on a publicly available dataset, NW-UCLA.",
    "code_link": ""
  },
  "cvpr2018_w6_anaggregatedmulticolumndilatedconvolutionnetworkforperspective-freecounting": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Humans in Crowd Scene and Look Into Person Challenge",
    "title": "An Aggregated Multicolumn Dilated Convolution Network for Perspective-Free Counting",
    "authors": [
      "Diptodip Deb",
      "Jonathan Ventura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w6/html/Deb_An_Aggregated_Multicolumn_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w6/Deb_An_Aggregated_Multicolumn_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose the use of dilated filters to construct an aggregation module in a multicolumn convolutional neural network for perspective-free counting. Counting is a common problem in computer vision (e.g. traffic on the street or pedestrians in a crowd). Modern approaches to the counting problem involve the production of a density map via regression whose integral is equal to the number of objects in the image. However, objects in the image can occur at different scales (e.g. due to perspective effects) which can make it difficult for a learning agent to learn the proper density map. While the use of multiple columns to extract multiscale information from images has been shown before, our approach aggregates the multiscale information gathered by the multicolumn convolutional neural network to improve performance. Our experiments show that our proposed network outperforms the state-of-the-art on many benchmark datasets, and also that using our aggregation module in combination with a higher number of columns is beneficial for multiscale counting.",
    "code_link": ""
  },
  "cvpr2018_w6_learningtorefinehumanposeestimation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Humans in Crowd Scene and Look Into Person Challenge",
    "title": "Learning to Refine Human Pose Estimation",
    "authors": [
      "Mihai Fieraru",
      "Anna Khoreva",
      "Leonid Pishchulin",
      "Bernt Schiele"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w6/html/Fieraru_Learning_to_Refine_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w6/Fieraru_Learning_to_Refine_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Multi-person pose estimation in images and videos is an important yet challenging task with many applications. Despite the large improvements in human pose estimation enabled by the development of convolutional neural networks, there still exist a lot of difficult cases where even the state-of-the-art models fail to correctly localize all body joints. This motivates the need for an additional refinement step that addresses these challenging cases and can be easily applied on top of any existing method. In this work, we introduce a pose refinement network (PoseRefiner) which takes as input both the image and a given pose estimate and learns to directly predict a refined pose by jointly reasoning about the input-output space. In order for the network to learn to refine incorrect body joint predictions, we employ a novel data augmentation scheme for training, where we model \"hard\" human pose cases. We evaluate our approach on four popular large-scale pose estimation benchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack Pose Estimation, and PoseTrack Pose Tracking, and report systematic improvement over the state of the art.",
    "code_link": ""
  },
  "cvpr2018_w6_crowdactivitychangepointdetectioninvideosviagraphstreammining": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Humans in Crowd Scene and Look Into Person Challenge",
    "title": "Crowd Activity Change Point Detection in Videos via Graph Stream Mining",
    "authors": [
      "Meng Yang",
      "Lida Rashidi",
      "Sutharshan Rajasegarar",
      "Christopher Leckie",
      "Aravinda S. Rao",
      "Marimuthu Palaniswami"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w6/html/Yang_Crowd_Activity_Change_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w6/Yang_Crowd_Activity_Change_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In recent years, there has been a growing interest in detecting anomalous behavioral patterns in video. In this work, we address this task by proposing a novel activity change point detection method to identify crowd movement anomalies for video surveillance. In our proposed novel framework, a hyperspherical clustering algorithm is utilized for the automatic identification of interesting regions, then the density of pedestrian flows between every pair of interesting regions over consecutive time intervals is monitored and represented as a sequence of adjacency matrices where the direction and density of flows are captured through a directed graph. Finally, we use graph edit distance as well as a cumulative sum test to detect change points in the graph sequence. We conduct experiments on four real-world video datasets: Dublin, New Orleans, Abbey Road and MCG Datasets. We observe that our proposed approach achieves a high F-measure, i.e., in the range [0.7, 1], for these datasets. The evaluation reveals that our proposed method can successfully detect the change points in all datasets at both global and local levels. Our results also demonstrate the efficiency and effectiveness of our proposed algorithm for change point detection and segmentation tasks.",
    "code_link": ""
  },
  "cvpr2018_w9_superpointself-supervisedinterestpointdetectionanddescription": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "SuperPoint: Self-Supervised Interest Point Detection and Description",
    "authors": [
      "Daniel DeTone",
      "Tomasz Malisiewicz",
      "Andrew Rabinovich"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.",
    "code_link": ""
  },
  "cvpr2018_w9_globalposeestimationwithanattention-basedrecurrentnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Global Pose Estimation With an Attention-Based Recurrent Network",
    "authors": [
      "Emilio Parisotto",
      "Devendra Singh Chaplot",
      "Jian Zhang",
      "Ruslan Salakhutdinov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Parisotto_Global_Pose_Estimation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Parisotto_Global_Pose_Estimation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The ability for an agent to localize itself within an environment is crucial for many real-world applications. For unknown environments, Simultaneous Localization and Mapping (SLAM) enables incremental and concurrent building of and localizing within a map. We present a new, differentiable architecture, Neural Graph Optimizer, progressing towards a complete neural network solution for SLAM by designing a system composed of a local pose estimation model, a novel pose selection module, and a novel graph optimization process. The entire architecture is trained in an end-to-end fashion, enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering. We demonstrate the effectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom environment.",
    "code_link": ""
  },
  "cvpr2018_w9_visualslamforautomateddrivingexploringtheapplicationsofdeeplearning": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Visual SLAM for Automated Driving: Exploring the Applications of Deep Learning",
    "authors": [
      "Stefan Milz",
      "Georg Arbeiter",
      "Christian Witt",
      "Bassam Abdallah",
      "Senthil Yogamani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Milz_Visual_SLAM_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Milz_Visual_SLAM_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep learning has become the standard model for object detection and recognition. Recently, there is progress on using CNN models for geometric vision tasks like depth estimation, optical flow prediction or motion segmentation. However, Visual SLAM remains to be one of the areas of automated driving where CNNs are not mature for deployment in commercial automated driving systems. In this paper, we explore how deep learning can be used to replace parts of the classical Visual SLAM pipeline. Firstly, we describe the building blocks of Visual SLAM pipeline composed of standard geometric vision tasks. Then we provide an overview of Visual SLAM use cases for automated driving based on the authors' experience in commercial deployment. Finally, we discuss the opportunities of using Deep Learning to improve upon state-of-the-art classical methods.",
    "code_link": ""
  },
  "cvpr2018_w9_mask-slamrobustfeature-basedmonocularslambymaskingusingsemanticsegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Mask-SLAM: Robust Feature-Based Monocular SLAM by Masking Using Semantic Segmentation",
    "authors": [
      "Masaya Kaneko",
      "Kazuya Iwami",
      "Toru Ogawa",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Kaneko_Mask-SLAM_Robust_Feature-Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Kaneko_Mask-SLAM_Robust_Feature-Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose a novel method that combines monocular visual simultaneous localization and mapping (vSLAM) and deep-learning-based semantic segmentation. For stable operation, vSLAM requires feature points on static objects. In conventional vSLAM, random sample consensus (RANSAC)is used to select those feature points. However, if a major portion of the view is occupied by moving objects, many feature points become inappropriate and RANSAC does not perform well. Based on our empirical studies, feature points in the sky and on cars often cause errors in vSLAM. We propose a new framework to exclude feature points using a mask produced by semantic segmentation. Excluding feature points in masked areas enables vSLAM to stably estimate camera motion. We apply ORB-SLAM in our framework, which is a state-of-the-art implementation of monocular vSLAM. For our experiments, we created vSLAM evaluation datasets by using the CARLA simulator under various conditions. Compared to state-of-the-art methods, our method can achieve significantly higher accuracy.",
    "code_link": ""
  },
  "cvpr2018_w9_geometricconsistencyforself-supervisedend-to-endvisualodometry": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Geometric Consistency for Self-Supervised End-to-End Visual Odometry",
    "authors": [
      "Ganesh Iyer",
      "J. Krishna Murthy",
      "Gunshi Gupta",
      "Madhava Krishna",
      "Liam Paull"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Iyer_Geometric_Consistency_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Iyer_Geometric_Consistency_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose \"Composite Transformation Constraints (CTCs)\", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.",
    "code_link": ""
  },
  "cvpr2018_w9_learningdescriptor,confidence,anddepthestimationinmulti-viewstereo": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Learning Descriptor, Confidence, and Depth Estimation in Multi-View Stereo",
    "authors": [
      "Sungil Choi",
      "Seungryong Kim",
      "Kihong Park",
      "Kwanghoon Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Choi_Learning_Descriptor_Confidence_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Choi_Learning_Descriptor_Confidence_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Depth estimation from multi-view stereo images is one of the most fundamental and essential tasks in understanding a scene imaginary. In this paper, we propose a machine learning technique based on deep convolutional neural networks (CNNs) for multi-view stereo matching. The proposed method measures the matching cost to extract depth values between two-view stereo images among multi-view stereo images using a deep architecture. Moreover, we present the confidence estimation network for incorporating the cost volumes along the depth hypothesis in multi-view stereo. Experiments show that our estimated depth map from multiple views shows the better performance than the other matching similarity measure on DTU dataset.",
    "code_link": ""
  },
  "cvpr2018_w9_depthnetarecurrentneuralnetworkarchitectureformonoculardepthprediction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "DepthNet: A Recurrent Neural Network Architecture for Monocular Depth Prediction",
    "authors": [
      "Arun CS Kumar",
      "Suchendra M. Bhandarkar",
      "Mukta Prasad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Kumar_DepthNet_A_Recurrent_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Kumar_DepthNet_A_Recurrent_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Predicting the depth map of a scene is often a vital component of monocular SLAM pipelines. Depth prediction is fundamentally ill-posed due to the inherent ambiguity in the scene formation process. In recent times, convolutional neural networks (CNNs) that exploit scene geometric constraints have been explored extensively for supervised single-view depth prediction and semi-supervised 2-view depth prediction. In this paper we explore whether recurrent neural networks (RNNs) can learn spatio-temporally accurate monocular depth prediction from video sequences, even without explicit definition of the inter-frame geometric consistency or pose supervision. To this end, we propose a novel convolutional LSTM (ConvLSTM)-based network architecture for depth prediction from a monocular video sequence. In the proposed ConvLSTM network architecture, we harness the ability of long short-term memory (LSTM)-based RNNs to reason sequentially and predict the depth map for an image frame as a function of the appearances of scene objects in the image frame as well as image frames in its temporal neighborhood. In addition, the proposed ConvLSTM network is also shown to be able to make depth predictions for future or unseen image frame(s). We demonstrate the depth prediction performance of the proposed ConvLSTM network on the KITTI dataset and show that it gives results that are superior in terms of accuracy to those obtained via depth-supervised and self-supervised methods and comparable to those generated by state-of-the-art pose-supervised methods.",
    "code_link": ""
  },
  "cvpr2018_w9_towardscnnmaprepresentationandcompressionforcamerarelocalisation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Towards CNN Map Representation and Compression for Camera Relocalisation",
    "authors": [
      "Luis Contreras",
      "Walterio Mayol-Cuevas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Contreras_Towards_CNN_Map_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Contreras_Towards_CNN_Map_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents a study on the use of Convolutional Neural Networks for camera relocalisation and its application to map compression. We follow state of the art visual relocalisation results and evaluate the response to different data inputs. We use a CNN map representation and introduce the notion of map compression under this paradigm by using smaller CNN architectures without sacrificing relocalisation performance. We evaluate this approach in a series of publicly available datasets over a number of CNN architectures with different sizes, both in complexity and number of layers. This formulation allows us to improve relocalisation accuracy by increasing the number of training trajectories while maintaining a constant-size CNN.",
    "code_link": ""
  },
  "cvpr2018_w9_monoculardepthpredictionusinggenerativeadversarialnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Monocular Depth Prediction Using Generative Adversarial Networks",
    "authors": [
      "Arun CS Kumar",
      "Suchendra M. Bhandarkar",
      "Mukta Prasad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Kumar_Monocular_Depth_Prediction_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Kumar_Monocular_Depth_Prediction_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a technique for monocular reconstruction, i.e. depth map and pose prediction from input monocular video sequences, using adversarial learning. We extend current geometry-aware neural network architectures that learn from photoconsistency-based reconstruction loss functions defined over spatially and temporally adjacent images by leveraging recent advances in adversarial learning. We propose a generative adversarial network (GAN) that can learn improved reconstruction models, with flexible loss functions that are less susceptible to adversarial examples, using generic semi-supervised or unsupervised datasets. The generator function in the proposed GAN learns to synthesize neighbouring images to predict a depth map and relative object pose, while the discriminator function learns the distribution of monocular images to correctly classify the authenticity of the synthesized images. A typical photoconsistency-based reconstruction loss function is used to assist the generator function to train well and compete against the discriminator function. We demonstrate the performance of our method on the KITTI dataset in both, depth-supervised and unsupervised settings. Thedepth prediction results of the proposed GAN are shown to compare favorably with state-of-the-art techniques for monocular reconstruction.",
    "code_link": ""
  },
  "cvpr2018_w9_learning3dscenesemanticsandstructurefromasingledepthimage": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "Learning 3D Scene Semantics and Structure From a Single Depth Image",
    "authors": [
      "Bo Yang",
      "Zihang Lai",
      "Xiaoxuan Lu",
      "Shuyu Lin",
      "Hongkai Wen",
      "Andrew Markham",
      "Niki Trigoni"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Yang_Learning_3D_Scene_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Yang_Learning_3D_Scene_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we aim to understand the semantics and 3D structure of a scene from a single depth image. Recent deep neural networks based methods aim to simultaneously learn object class labels and infer the 3D shape of a scene represented by a large voxel grid. However, individual objects within the scene are usually only represented by a few voxels leading to a loss of geometric detail. In addition, significant computational and memory resources are required to process the large scale voxel grid of a whole scene. To address this, we propose an efficient and holistic pipeline, 3D-Depth, to simultaneously learn the semantics and structure of a scene from a single depth image. Our key idea is to deeply fuse an efficient 3D shape estimator with existing recognition (e.g., ResNets) and segmentation (e.g., Mask R-CNN) techniques. Object level semantics and latent feature maps are extracted and then fed to a shape estimator to extract the 3D shape. Extensive experiments are conducted on large-scale synthesized indoor scene datasets, quantitatively and qualitatively demonstrating the merits and superior performance of 3R-Depth.",
    "code_link": ""
  },
  "cvpr2018_w9_quadricslamdualquadricsasslamlandmarks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Deep Learning for Visual SLAM",
    "title": "QuadricSLAM: Dual Quadrics As SLAM Landmarks",
    "authors": [
      "Lachlan Nicholson",
      "Michael Milford",
      "Niko Sunderhauf"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w9/html/Nicholson_QuadricSLAM_Dual_Quadrics_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w9/Nicholson_QuadricSLAM_Dual_Quadrics_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Research in Simultaneous Localization And Mapping (SLAM) is increasingly moving towards richer world representations involving objects and high level features that enable a semantic model of the world for robots, potentially leading to a more meaningful set of robot-world interactions. Many of these advances are grounded in state-of-the-art computer vision techniques primarily developed in the context of image-based benchmark datasets, leaving several challenges to be addressed in adapting them for use in robotics. In this paper, we derive a SLAM formulation that uses dual quadrics as 3D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2D bounding boxes (such as those typically obtained from visual object detection systems) can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for deep-learned object detectors that addresses the challenge of partial object detections often encountered in robotics applications, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.",
    "code_link": ""
  },
  "cvpr2018_w10_theriemanniangeometryofdeepgenerativemodels": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "The Riemannian Geometry of Deep Generative Models",
    "authors": [
      "Hang Shao",
      "Abhishek Kumar",
      "P. Thomas Fletcher"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Shao_The_Riemannian_Geometry_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Shao_The_Riemannian_Geometry_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep generative models learn a mapping from a low-dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation canbe used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold.",
    "code_link": ""
  },
  "cvpr2018_w10_elastichandlingofpredictorphaseinfunctionalregressionmodels": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Elastic Handling of Predictor Phase in Functional Regression Models",
    "authors": [
      "Kyungmin Ahn",
      "J. Derek Tucker",
      "Wei Wu",
      "Anuj Srivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Ahn_Elastic_Handling_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Ahn_Elastic_Handling_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Functional variables serve important roles as predictors in a variety of pattern recognition and vision applications. Focusing on a specific subproblem, termed scalar-on-function regression, most current approaches adopt the standard L2 inner product to form a link between functional predictors and scalar responses. These methods may perform poorly when predictor functions contain nuisance phase variability, i.e., predictors are temporally misaligned due to noise. While a simple solution could be to pre-align predictors as a pre-processing step, before applying a regression model, this alignment is seldom optimal from the perspective of regression. We propose a new approach, termed elastic functional regression, where alignment is included in the regression model itself, and is performed in conjunction with the estimation of other model parameters. This model is based on a norm preserving warping of predictors, not the standard time warping of functions, and provides better prediction in situations where the shape or the amplitude of the predictor is more useful than its phase. We demonstrate the effectiveness of this framework using simulated and stock market data.",
    "code_link": ""
  },
  "cvpr2018_w10_geodesicdiscriminantanalysisformanifold-valueddata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Geodesic Discriminant Analysis for Manifold-Valued Data",
    "authors": [
      "Maxime Louis",
      "Benjamin Charlier",
      "Stanley Durrleman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Louis_Geodesic_Discriminant_Analysis_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Louis_Geodesic_Discriminant_Analysis_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In many statistical settings, it is assumed that high-dimensional data actually lies on a low-dimensional manifold. In this perspective, there is a need to generalize statistical methods to nonlinear spaces. To that end, we propose generalizations of the Linear Discriminant Analysis (LDA) to manifolds. First, we generalize the reduced rank LDA solution by constructing a geodesic subspace which optimizes a criterion equivalent to Fisher's discriminant in the linear case. Second, we generalize the LDA formulated as a restricted Gaussian classifier. The generalizations of those two methods, which are equivalent in the linear case, are in general different in the manifold case. We illustrate the first generalization on the 2-sphere. Then, we propose applications using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, in which we rephrase the second generalization. We perform dimension reduction and classification on the kimia-216 dataset and on a set of 3D brain structures segmented from Alzheimer's disease and control subjects, recovering state-of-the-art performances.",
    "code_link": ""
  },
  "cvpr2018_w10_amixturemodelforaggregationofmultiplepre-trainedweakclassifiers": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Mixture Model for Aggregation of Multiple Pre-Trained Weak Classifiers",
    "authors": [
      "Rudrasis Chakraborty",
      "Chun-Hao Yang",
      "Baba C. Vemuri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Chakraborty_A_Mixture_Model_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Chakraborty_A_Mixture_Model_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep networks have gained immense popularity in Computer Vision and other fields in the past few years due to their remarkable performance on recognition/classification tasks surpassing the state-of-the art. One of the keys to their success lies in the richness of the automatically learned features. In order to get very good accuracy, one popular option is to increase the depth of the network. Training such a deep network is however infeasible or impractical with moderate computational resources and budget. The other alternative to increase the performance is to learn multiple weak classifiers and boost their performance using a boosting algorithm or a variant thereof. But, one of the problems with boosting algorithms is that they require a re-training of the networks based on the misclassified samples. Motivated by these problems, in this work we propose an aggregation technique which combines the output of multiple weak classifiers. We formulate the aggregation problem using a mixture model fitted to the trained classifier outputs. Our model does not require any re-training of the \"weak\" networks and is computationally very fast (takes < 30 seconds to run in our experiments). Thus, using a less expensive training stage and without doing any re-training of networks, we experimentally demonstrate that it is possible to boost the performance by 12%. Furthermore, we present experiments using hand-crafted features and improved the classification performance using the proposed aggregation technique. One of the major advantages of our framework is that our framework allows one to combine features that are very likely to be of distinct dimensions since they are extracted using different networks/algorithms. Our experimental results demonstrate a significant performance gain from the use of our aggregation technique at a very small computational cost.",
    "code_link": ""
  },
  "cvpr2018_w10_temporalalignmentimprovesfeaturequalityanexperimentonactivityrecognitionwithaccelerometerdata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Temporal Alignment Improves Feature Quality: An Experiment on Activity Recognition With Accelerometer Data",
    "authors": [
      "Hongjun Choi",
      "Qiao Wang",
      "Meynard Toledo",
      "Pavan Turaga",
      "Matthew Buman",
      "Anuj Srivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Choi_Temporal_Alignment_Improves_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Choi_Temporal_Alignment_Improves_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Activity recognition has been receiving significant attention from a variety of research areas such as human performance enhancement, health promotion, and human computer interaction. However, recognizing activities from accelerometer data still remains a challenging problem due to sensitivity to sampling rates, misalignment of data, and increased variability in activities among clinically relevant populations. In order to solve these issues, we adopt methods from functional analysis, which consider non-elastic rate variations in movement. The overall framework factors out temporal variability within activity classes, before leveraging robust machine learning pipelines for a given end-use. The proposed approach has been evaluated on 7 classes of everyday activities with 50 subjects. The results indicate that proposed approach achieves improved performance with the improvements observed in separating similar classes that differ in temporal rates, and also demonstrate higher robustness to change in window lengths. These results suggest that temporal alignment should be considered a core part of activity recognition pipelines.",
    "code_link": ""
  },
  "cvpr2018_w10_locally-weightedelasticcomparisonofplanarshapes": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Locally-Weighted Elastic Comparison of Planar Shapes",
    "authors": [
      "Justin Strait",
      "Sebastian Kurtek",
      "Steven MacEachern"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Strait_Locally-Weighted_Elastic_Comparison_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Strait_Locally-Weighted_Elastic_Comparison_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Registration of curves is a necessary component of statistical shape analysis. The goal of registration is to align collections of shapes so that common features are appropriately matched for further comparison and subsequent analyses. Traditional methods for registration typically rely on optimizing an energy functional over a set of appropriate shape-preserving transformations (i.e., rotations and re-parameterizations). These functionals typically rely on the standard L^2 metric. In certain applications, it may make sense to use a more flexible metric which can align shapes most preferably with respect to a local shape feature (i.e., a certain curve segment selected from the overall shape). In this work, we define a weighted shape metric which allows for emphasis on local shape features. Registration can be performed with respect to this metric. We demonstrate the registration procedure using simulated curves as well as real data, and show the dependence of the optimal rotation and re-parameterization on the specified weights, as well as the resulting deformation path from one shape to another.",
    "code_link": ""
  },
  "cvpr2018_w10_covariancepoolingforfacialexpressionrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Covariance Pooling for Facial Expression Recognition",
    "authors": [
      "Dinesh Acharya",
      "Zhiwu Huang",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Acharya_Covariance_Pooling_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Acharya_Covariance_Pooling_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Classifying facial expressions into different categories requires capturing regional distortions of facial landmarks. We believe that second-order statistics such as covariance is better able to capture such distortions in regional facial features. In this work, we explore the benefits of using a manifold network structure for covariance pooling to improve facial expression recognition. In particular, we first employ such kind of manifold networks in conjunction with traditional convolutional networks for spatial pooling within individual image feature maps in an end-to-end deep learning manner. By doing so, we are able to achieve a recognition accuracy of 58.14% on the validation set of Static Facial Expressions in the Wild (SFEW 2.0) and 87.0% on the validation set of Real-World Affective Faces (RAF) Database. Both of these results are the best results we are aware of. Besides, we leverage covariance pooling to capture the temporal evolution of per-frame features for video-based facial expression recognition. Our reported results demonstrate the advantage of pooling image-set features temporally by stacking the designed manifold network of covariance pooling on top of convolutional network layers.",
    "code_link": "https://github.com/d-acharya/CovPoolFER"
  },
  "cvpr2018_w10_imagesegmentationbydeeplearningofdisjunctivenormalshapemodelshaperepresentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Image Segmentation by Deep Learning of Disjunctive Normal Shape Model Shape Representation",
    "authors": [
      "Mehran Javanmardi",
      "Ricardo Bigolin Lanfredi",
      "Mujdat Cetin",
      "Tolga Tasdizen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Javanmardi_Image_Segmentation_by_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Javanmardi_Image_Segmentation_by_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Segmenting images with low-quality, low signal to noise ratio has been a challenging task in computer vision.It has been shown that statistical prior information about the shape of the object to be segmented can be used to significantly mitigate this problem. However estimating the probability densities of the object shapes in the space of shapes can be difficult. This problem becomes more difficult when there is limited amount of training data or the testing images contain missing data. Most shape model based segmentation approaches tend to minimize an energy functional to segment the object. In this paper we propose a shape-based segmentation algorithm that utilizes convolutional neural networks to learn a posterior distribution of disjunction of conjunctions of half spaces to segment the object. This approach shows promising results on noisy and occluded data where it is able to accurately segment the objects. We show visual and quantitative results on datasets from several applications, demonstrating the effectiveness of the proposed approach. We should also note that inference with a CNN is computationally more efficient than density estimation and sampling approaches.",
    "code_link": ""
  },
  "cvpr2018_w10_predictingdynamicalevolutionofhumanactivitiesfromasingleimage": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Predicting Dynamical Evolution of Human Activities From a Single Image",
    "authors": [
      "Suhas Lohit",
      "Ankan Bansal",
      "Nitesh Shroff",
      "Jaishanker Pillai",
      "Pavan Turaga",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Lohit_Predicting_Dynamical_Evolution_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Lohit_Predicting_Dynamical_Evolution_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " A human pose often conveys not only the configuration of the body parts, but also implicit predictive information about the ensuing motion. This dynamic information can benefit vision applications which lack explicit motion cues. The human visual system can easily perceive the dynamic information in still images. However, computational algorithms to inferand utilize it in computer vision applications are limited. In this paper, we propose a probabilistic framework to infer the dynamic information associated with a human pose. The inference problem is posed as a non-parametric density estimation problem on a non-Euclidean manifold of linear dynamical models. Since direct modeling is intractable, we develop a data driven approach, estimating the density for the test sample under consideration. Statistical inference on the estimated density provides us with quantities of interest like the most probable future motion of the human and the amount of motion information conveyed by a pose. Our experiments demonstrate that the extracted motion information benefits numerous applications in computer vision. In particular, the predicted future motion is useful for activity recognition, human trajectory synthesis, and motion prediction.",
    "code_link": ""
  },
  "cvpr2018_w10_covariancematricesencodingbasedonthelog-euclideanandaffineinvariantriemannianmetrics": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Covariance Matrices Encoding Based on the Log-Euclidean and Affine Invariant Riemannian Metrics",
    "authors": [
      "Ioana Ilea",
      "Lionel Bombrun",
      "Salem Said",
      "Yannick Berthoumieu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Ilea_Covariance_Matrices_Encoding_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Ilea_Covariance_Matrices_Encoding_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents coding methods used to encode a set of covariance matrices. Starting from a Gaussian mixture model adapted to the log-Euclidean or affine invariant Riemannian metric, we propose a Fisher Vector (FV) descriptor adapted to each of these metrics: the log Euclidean FV (LE FV) and the Riemannian Fisher Vector (RFV). An experiment is conducted on four conventional texture databases to compare these two metrics and to illustrate the potential of these FV based descriptors compared to state-of-the-art BoW and VLAD based descriptors. A focus is also done to illustrate the advantage of using the Fisher information matrix during the derivation of the FV.",
    "code_link": ""
  },
  "cvpr2018_w10_principalcurvatureguidedsurfacegeometryawareglobalshaperepresentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Diff-CVML: Differential Geometry in Computer Vision and Machine Learning",
    "title": "Principal Curvature Guided Surface Geometry Aware Global Shape Representation",
    "authors": [
      "Somenath Das",
      "Suchendra M. Bhandarkar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w10/html/Das_Principal_Curvature_Guided_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w10/Das_Principal_Curvature_Guided_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " A surface principal curvature preserving local geometry aware global shape representation for 3D shapes is proposed. The shape representation computes the shortest quasi-geodesic path between all possible pairs of points on the shape manifold that enforces minimal variation of geodesic curvature along the path. The normal component of the principal curvature along the quasi-geodesic paths is dominant and shown to preserve the local shape geometry. The eigenspectrum of the proposed representation is exploited to characterize self-symmetry. The commutative property between shape spectra is exploited to compute region-based correspondence between isometric 3D shapes without requiring an initial correspondence map to be specified a priori. The results of the region-based correspondence are extended to characterize the compatibility of the commutative eigen-spectrum in order to address the problem of shape deformation transfer. Eigenspectrum-based characterization metrics are proposed to quantify the performance of the proposed 3D shape descriptor for self-symmetry detection and correspondence determination. The proposed shape descriptor spectrum-based optimization criterion is observed to yield competitive performance compared to relevant state-of-the-art correspondence determination techniques.",
    "code_link": ""
  },
  "cvpr2018_w11_towardmorerealisticfacerecognitionevaluationprotocolsfortheyoutubefacesdatabase": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Toward More Realistic Face Recognition Evaluation Protocols for the YouTube Faces Database",
    "authors": [
      "Yoanna Martinez-Diaz",
      "Heydi Mendez-Vazquez",
      "Leyanis Lopez-Avila",
      "Leonardo Chang",
      "L. Enrique Sucar",
      "Massimo Tistarelli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Martinez-Diaz_Toward_More_Realistic_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Martinez-Diaz_Toward_More_Realistic_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " One of the key factors to measure the progress of a research problem is the design of appropriate evaluation protocols defined on suitable databases.Recently, the introduction of comprehensive databases and benchmarks of face videos has had a great impact on the development of new face recognition techniques. However, most of the protocols provided for these datasets are limited and do not capture requirements of unconstrained scenarios. That is why sometimes the performance of face recognition methods on current benchmarks seems to be saturated. To address this lack, the tendency is to collect new datasets, which is more expensive and sometimes the main the problem is not the data but the protocols. In this work, we propose new relevant evaluation protocols for the YouTube Faces database (REP-YTF) supporting face verification and open/closed-set identification. The proposal better fits realistic face recognition scenarios and allows us to test existing algorithms at relevant assessment points, under different openness values and taking both videos and images as the gallery. We provide an extensive experimental evaluation, by combining several well-established feature representations with three different metric learning algorithms. The obtained results show that by using the proposed evaluation protocols, there is room for improvement in the recognition performance on the YouTube Faces database.",
    "code_link": ""
  },
  "cvpr2018_w11_dictlayerastructureddictionarylayer": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Dict Layer: A Structured Dictionary Layer",
    "authors": [
      "Yefei Chen",
      "Jianbo Su"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Chen_Dict_Layer_A_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Chen_Dict_Layer_A_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Dictionary learning and deep learning both have played important roles in computer vision. Dictionary learning strives to learn best representative atoms to reconstruct the source images or signals. Alternating iterative methods are developed to solve such problems. On the other hand, deep learning performs in an end-to-end mode, where both feature extraction and classification are achieved simultaneously. Obviously, the scheme of deep learning is different from that of traditional dictionary learning which is shallow and focuses more on data reconstruction. However, studies on building a deep layer-stacked model imply that there could be a relationship between them. In this paper, the relationship between dictionary learning and deep learning is studied. Dictionary learning can be viewed as a special full connection layer (FC Layer) of deep learning. According to the relationship, we try to introduce those mature improvements from dictionary learning to deep learning. Hence, a new kind of layer named as Dict Layer is introduced in this paper, where the idea of structured dictionary is adopted. In Dict Layer, neural units (coefficients) are class specified, which means the activated neural units are encouraged to be the same class. The proposed method is evaluated on MNIST, CIFAR-10 and SVHN as an improvement of FC Layer. Experiments on AR and Extended YaleB are conducted where Dict Layer is viewed as a special form of dictionary learning method. Results show that the outputs of Dict Layer are more discriminative and class specific than that of the traditional FC Layer.",
    "code_link": ""
  },
  "cvpr2018_w11_hierarchicaldictionarylearningandsparsecodingforstaticsignatureverification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Hierarchical Dictionary Learning and Sparse Coding for Static Signature Verification",
    "authors": [
      "Elias N. Zois",
      "Marianna Papagiannopoulou",
      "Dimitrios Tsourounis",
      "George Economou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Zois_Hierarchical_Dictionary_Learning_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Zois_Hierarchical_Dictionary_Learning_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " An assortment of review papers as well as newly quoted literature clearly indicates that the modeling and subsequent verification of static or offline signatures still poses an active field of scientific interest. Usually, the most important link in the chain of designing signature verification systems (SV's) is the feature extraction one, in which any static signature image is projected and measured onto a feature space. Feature extraction methods are divided in two main categories. The first one, inspired by different computer vision applications, includes handcrafted features. That is features manually engineered by scientists to be optimal for certain type of information extraction-summarization from signature images. Typical examples of this kind include global-local and/or grid-texture oriented features. The second feature category addresses signature modeling and verification with the use of dedicated features, usually learned directly from raw signature image data. Typical representatives include Deep Learning (DL) as well as Bag of Visual Words (BoW) or Histogram of Templates (HOT). Quite recently also sparse representation (SR) methods which include dictionary learning and coding have been introduced for signature modeling and verification with promising results. In this paper, we propose an extension of the classic SR conceptual framework by introducing the idea of embedding the atoms of a dictionary in a directed tree. For the purpose of this work, this is demonstrated by employing an l0 tree-structured sparse regularization norm which has been found to be useful in in a number of cases. We examine the efficiency of the proposed method by conducting experiments with two popular datasets namely the CEDAR and MCYT-75. The verification results of the proposed method are considered at least comparable with those provided with other state-of-the-art approaches.",
    "code_link": ""
  },
  "cvpr2018_w11_realtimequalityassessmentofirisbiometricsundervisiblelight": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Realtime Quality Assessment of Iris Biometrics Under Visible Light",
    "authors": [
      "Mohsen Jenadeleh",
      "Marius Pedersen",
      "Dietmar Saupe"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Jenadeleh_Realtime_Quality_Assessment_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Jenadeleh_Realtime_Quality_Assessment_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Ensuring sufficient quality of iris images acquired by handheld imaging devices in visible light poses many challenges to iris recognition systems. Many distortions affect the input iris images, and the source and types of these distortions are unknown in uncontrolled environments. We propose a fast no-reference image quality assessment measure for predicting iris image quality to handle severely degraded iris images. The proposed differential sign-magnitude statistics index (DSMI) is based on statistical features of the local difference sign-magnitude transform, which are computed by comparing the local mean with the central pixel of the patch and considering the noticeable variations. The experiments, conducted with a reference iris recognition system and three visible light datasets, showed that the quality of iris images strongly affects the recognition performance. Using the proposed method as a quality filtering step improved the performance of the iris recognition system by rejecting poor quality iris samples.",
    "code_link": ""
  },
  "cvpr2018_w11_multi-framesuperresolutionforocularbiometrics": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Multi-Frame Super Resolution for Ocular Biometrics",
    "authors": [
      "Narsi Reddy",
      "Dewan Fahim Noor",
      "Zhu Li",
      "Reza Derakhshani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Reddy_Multi-Frame_Super_Resolution_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Reddy_Multi-Frame_Super_Resolution_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Some biometrics methods, especially ocular, may use fine spatial information akin to level-3 features. Examples include fine vascular patterns visible in the white of the eyes in green and blue channels, iridial patterns in near infrared, or minute periocular features in visible light. In some mobile applications, NIR or RGB camera is used to capture these ocular images in a \"selfie\" like manner. However, most of such ocular images captured under unconstrained environments are of lower quality due to spatial resolution, noise, and motion blur, affecting the performance of the ensuing biometric authentication. Here we propose a multi-frame super resolution (MFSR) pipeline to mitigate the problem, where a higher resolution image is generated from multiple lower resolution, noisy and blurry images. We show that the proposed MFSR method at 2X upscaling can improve the equal error rate (EER) by 9.85% compared to single frame bicubic upscaling in RGB ocular matching while being 8.5X faster than comparable state-of-the-art MFSR method.",
    "code_link": ""
  },
  "cvpr2018_w11_facetemplateprotectionusingdeepconvolutionalneuralnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Face Template Protection Using Deep Convolutional Neural Network",
    "authors": [
      "Arun Kumar Jindal",
      "Srinivas Chalamala",
      "Santosh Kumar Jami"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Jindal_Face_Template_Protection_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Jindal_Face_Template_Protection_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The growing use of biometrics has led to rising concerns about the security and privacy of the biometric data (template) since it is unique to each individual and cannot be replaced.To address this problem, many biometric template protection algorithms have been reported but most have a trade-off between matching performance and template security. In this work, we propose a method for face template protection, which improves upon existing face template protection algorithm, to provide better matching performance. The proposed method uses deep Convolutional Neural Network (CNN), with one-shot and multi-shot enrollment, to learn a robust mapping from face images of the users to the unique binary codes (assigned to the users during enrollment phase). The cryptographic hash (like SHA-3 512) of the user's binary code represents the protected face template. The deep CNN is trained to minimize the intra-class variations and maximize the inter-class variations. Duringverification, given an input face image of a user, deep CNN predicts the binary code assigned to the user. The hash of the predicted binary code is matched with the hash of the actual binary code assigned to the user during enrollment. Three face datasets, namely CMU-PIE, FEI and Color FERET are used for evaluation. The proposed method improves the matching performance by6% and reduces Equal Error Rate by about 4 times when compared to related work, while providing high template security.",
    "code_link": ""
  },
  "cvpr2018_w11_incorporatingtouchbiometricstomobileone-timepasswordsexplorationofdigits": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Incorporating Touch Biometrics to Mobile One-Time Passwords: Exploration of Digits",
    "authors": [
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Julian Fierrez",
      "Javier Ortega-Garcia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Tolosana_Incorporating_Touch_Biometrics_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Tolosana_Incorporating_Touch_Biometrics_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This work evaluates the advantages and potential of incorporating touch biometrics to mobile one-time passwords (OTP). The new e-BioDigit database, which comprises on-line handwritten numerical digits from 0 to 9, has been acquired using the finger touch as input to a mobile device. This database is used in the experiments reported in this work and it is publicly available to the research community. An analysis of the OTP scenario using handwritten digits is carried out regarding which are the most discriminative handwritten digits and how robust the system is when increasing the number of them in the user password. Additionally, the best features for each handwritten numerical digit are studied in order to enhance our proposed biometric system. Our proposed approach achieves remarkable results with EERs ca. 5.0% when using skilled forgeries, outperforming other traditional biometric verification traits such as the handwritten signature or graphical passwords on similar mobile scenarios.",
    "code_link": ""
  },
  "cvpr2018_w11_identityawaresynthesisforcrossresolutionfacerecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Identity Aware Synthesis for Cross Resolution Face Recognition",
    "authors": [
      "Maneet Singh",
      "Shruti Nagpal",
      "Mayank Vatsa",
      "Richa Singh",
      "Angshul Majumdar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Singh_Identity_Aware_Synthesis_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Singh_Identity_Aware_Synthesis_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Enhancing low resolution images via super-resolution or image synthesis for cross-resolution face recognition has been well studied. Several image processing and machine learning paradigms have been explored for addressing the same. In this research, we propose Synthesis via Hierarchical Sparse Representation (SHSR) algorithm for synthesizing a high resolution face image from a low resolution input image. The proposed algorithm learns multi-level sparse representation for both high and low resolution gallery images, along with an identity aware dictionary and a transformation function between the two representations for face identification scenarios. With low resolution test data as input, the high resolution test image is synthesized using the identity aware dictionary and transformation, which is then used for face recognition. The performance of the proposed SHSR algorithm is evaluated on four databases, including one real world dataset. Experimental results and comparison with seven existing algorithms demonstrate the efficacy of the proposed algorithm in terms of both face identification and image quality measures.",
    "code_link": ""
  },
  "cvpr2018_w11_genlr-netdeepframeworkforverylowresolutionfaceandobjectrecognitionwithgeneralizationtounseencategories": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "GenLR-Net: Deep Framework for Very Low Resolution Face and Object Recognition With Generalization to Unseen Categories",
    "authors": [
      "Sivaram Prasad Mudunuri",
      "Soubhik Sanyal",
      "Soma Biswas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Mudunuri_GenLR-Net_Deep_Framework_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Mudunuri_GenLR-Net_Deep_Framework_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Matching very low resolution images of faces and objects with high resolution images in the database has important applications in surveillance scenarios, street-to-shop matching for general objects, etc. Matching across huge resolution difference along with variations in illumination, view-point, etc. makes the problem quite challenging. The problem becomes even more difficult if the testing objects have not been seen during training. In this work, we propose a novel deep convolutional neural network architecture to address these problems. We systematically introduce different kinds of constraints at different stages of the architecture so that the approach can recognize low resolution images as well as generalize well to images of unseen categories. The reason behind each additional step along with its effect on the overall performance is thoroughly analyzed. Extensive experiments are conducted on two face and object datasets which justifies the effectiveness of the proposed approach for handling these real-life challenging scenarios.",
    "code_link": ""
  },
  "cvpr2018_w11_attribute-centeredlossforsoft-biometricsguidedfacesketch-photorecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Attribute-Centered Loss for Soft-Biometrics Guided Face Sketch-Photo Recognition",
    "authors": [
      "Hadi Kazemi",
      "Sobhan Soleymani",
      "Ali Dabouei",
      "Mehdi Iranmanesh",
      "Nasser M. Nasrabadi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Kazemi_Attribute-Centered_Loss_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Kazemi_Attribute-Centered_Loss_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": "Face sketches are able to capture the spatial topology of a face while lacking some facial attributes such as race, skin, or hair color. Existing sketch-photo recognition approaches have mostly ignored the importance of facial attributes. In this paper, we propose a new loss function, called attribute-centered loss, to train a Deep Coupled Convolutional Neural Network (DCCNN) for facial attribute guided sketch to photo matching. Specifically, an attribute-centered loss is proposed which learns several distinct centers, in a shared embedding space, for photos and sketches with different combinations of attributes. The DCCNN simultaneously is trained to map photos and pairs of testified attributes and corresponding forensic sketches around their associated centers, while preserving the spatial topology information. Importantly, the centers learn to keep a relative distance from each other, related to their number of contradictory attributes. Extensive experiments are performed on composite (E-PRIP) and semi-forensic (IIIT-D Semi-forensic) databases. The proposed method significantly outperforms the state-of-the-art.",
    "code_link": ""
  },
  "cvpr2018_w11_latentfingerprintimagequalityassessmentusingdeeplearning": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Latent Fingerprint Image Quality Assessment Using Deep Learning",
    "authors": [
      "Jude Ezeobiejesi",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Ezeobiejesi_Latent_Fingerprint_Image_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Ezeobiejesi_Latent_Fingerprint_Image_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Latent fingerprints are fingerprint impressions unintentionally left on surfaces at a crime scene. They are crucial in crime scene investigations for making identifications or exclusions of suspects. Determining the quality of latent fingerprint images is crucial to the effectiveness and reliability of matching algorithms. To alleviate the inconsistency and subjectivity inherent in feature markups by latent fingerprint examiners, automatic processing of latent fingerprints is imperative. We propose a deep neural network that predicts the quality of image patches extracted from a latent fingerprint and knits them together to predict the quality of a given latent fingerprint. The proposed approach eliminates the need for manual ROI markup and manual feature markup by latent examiners. Experimental results on NIST SD27 show the effectiveness of our technique in latent fingerprint quality prediction.",
    "code_link": ""
  },
  "cvpr2018_w11_unconstrainedfingerphotodatabase": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Unconstrained Fingerphoto Database",
    "authors": [
      "Shaan Chopra",
      "Aakarsh Malhotra",
      "Mayank Vatsa",
      "Richa Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Chopra_Unconstrained_Fingerphoto_Database_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Chopra_Unconstrained_Fingerphoto_Database_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Biometrics based user authentication for mobile devices is now popular with face and fingerprints being the primary modalities. Fingerphoto, an image of a person's finger captured using inbuilt smartphone camera, based user authentication is an attractive and cost-effective alternative. Existing research focuses on constrained or semi-constrained environment; whereas, challenges such as user cooperation, number of fingers, background, orientation, and deformation are important to address before fingerphoto authentication becomes usable. This paper presents the first publicly available fingerphoto database, termed as UNconstrained FIngerphoTo (UNFIT) database, which contains fingerphoto images acquired in unconstrained environments. We also present baseline results with deep learning based segmentation, and CompCode and ResNet50 representation based matching approaches. We assert that the availability of the proposed database can encourage research in this important domain.",
    "code_link": "https://github.com/pdollar/toolbox"
  },
  "cvpr2018_w11_hybriduser-independentanduser-dependentofflinesignatureverificationwithatwo-channelcnn": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Hybrid User-Independent and User-Dependent Offline Signature Verification With a Two-Channel CNN",
    "authors": [
      "Mustafa Berkay Yilmaz",
      "Kagan Ozturk"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Yilmaz_Hybrid_User-Independent_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Yilmaz_Hybrid_User-Independent_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Signature verification task needs relevant signature representations to achieve low error rates. Many signature representations have been proposed so far. In this work we propose a hybrid user-independent/dependent offline signature verification technique with a two-channel convolutional neural network (CNN) both for verification and feature extraction. Signature pairs are input to the CNN as two channels of one image, where the first channel always represents a reference signature and the second channel represents a query signature. We decrease the image size through the network by keeping the convolution stride parameter large enough. Global average pooling is applied to decrease the dimensionality to 200 at the end of locally connected layers.We utilize the CNN as a feature extractor and report 4.13% equal error rate (EER) considering 12 reference signatures with the proposed 200-dimensional representation, compared to 3.66% of a recently proposed technique with 2048-dimensional representation using the same experimental protocol. When the two methods are combined at score level, more than 50% improvement (1.76% EER) is achieved demonstrating the complementarity of them. Sensitivity of the model to gray-level and binary images is investigated in detail. One model is trained using gray-level images and the other is trained using binary images. It is shown that the availability of gray-level information in train and test data decreases the EER e.g. from 11.86% to 4.13%.",
    "code_link": ""
  },
  "cvpr2018_w11_ittakestwototangocascadingoff-the-shelffacedetectors": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "It Takes Two to Tango: Cascading Off-the-Shelf Face Detectors",
    "authors": [
      "Siqi Yang",
      "Arnold Wiliem",
      "Brian C. Lovell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Yang_It_Takes_Two_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Yang_It_Takes_Two_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recent face detection methods have achieved high detection rates in unconstrained environments. However, as they still generate excessive false positives, any method for reducing false positives is highly desirable. This work aims to massively reduce false positives of existing face detection methods whilst maintaining the true detection rate. In addition, the proposed method also aims to sidestep the detector retraining task which generally requires enormous effort. To this end, we propose a two-stage framework which cascades two off-the-shelf face detectors.Not all face detectors can be cascaded and achieve good performance. Thus, we study three properties that allow us to determine the best pair of detectors. These three properties are: (1) correlation of true positives; (2) diversity of false positives and (3) detector runtime. Experimental results on recent large benchmark datasets such as FDDB and WIDER FACE support our findings that the false positives of a face detector could be potentially reduced by 90% whilst still maintaining high true positive detection rate. In addition, with a slight decrease in true positives, we found a pair of face detector that achieves significantly lower false positives, while being five times faster than the current state-of-the-art detector.",
    "code_link": ""
  },
  "cvpr2018_w11_timeanalysisofpulse-basedfaceanti-spoofinginvisibleandnir": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Time Analysis of Pulse-Based Face Anti-Spoofing in Visible and NIR",
    "authors": [
      "Javier Hernandez-Ortega",
      "Julian Fierrez",
      "Aythami Morales",
      "Pedro Tome"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Hernandez-Ortega_Time_Analysis_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Hernandez-Ortega_Time_Analysis_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper we study Presentation Attack Detection (PAD) in face recognition systems against realistic arti- facts such as 3D masks or good quality of photo attacks. In recent works, pulse detection based on remote photo- plethysmography (rPPG) has shown to be a effective coun- termeasure in concrete setups, but still there is a need for a deeper understanding of when and how this kind of PAD works in various practical conditions. Related works ana- lyze full video sequences (usually over 60 seconds) to dis- tinguish between attacks and legitimate accesses. However, existing approaches may not be as effective as it has been claimed in the literature in time variable scenarios. In this paper we evaluate the performance of an existent state-of- the-art PAD scheme based on rPPG when analyzing short- time video sequences extracted from a longer video. Results are reported using the 3D Mask Attack Database (3DMAD), and a self-collected dataset called Heart Rate Database (HR), including different video durations, spec- trum bands, resolutions and frame rates. Several conclusions can be drawn from this work: a) PAD performance based on rPPG varies significantly with the length of the analyzed video, b) rPPG information ex- tracted from short-time sequences (over 5 seconds) can be discriminant enough for performing the PAD task, c) in gen- eral, videos using the NIR band perform better than those using the RGB band, and d) the temporal resolution is more valuable for rPPG signal extraction than the spatial resolu- tion.",
    "code_link": ""
  },
  "cvpr2018_w11_adeepfaceidentificationnetworkenhancedbyfacialattributesprediction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "A Deep Face Identification Network Enhanced by Facial Attributes Prediction",
    "authors": [
      "Fariborz Taherkhani",
      "Nasser M. Nasrabadi",
      "Jeremy Dawson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Taherkhani_A_Deep_Face_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Taherkhani_A_Deep_Face_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose a new deep framework which predicts facial attributes and leverage it as a soft modality to improve face identification performance. Our model is an end to end framework which consists of a convolutional neural network (CNN) whose output is fanned out into two separate branches; the first branch predicts facial attributes while the second branch identifies face images. Contrary to the existing multi-task methods which only use a shared CNN feature space to train these two tasks jointly, we fuse the predicted attributes with the features from the face modality in order to improve the face identification performance. Experimental results show that our model brings benefits to both face identification as well as facial attribute prediction performance, especially in the case of identity facial attributes such as gender prediction. We tested our model on two standard datasets annotated by identities and face attributes. Experimental results indicate that the proposed model outperforms most of the current existing face identification and attribute prediction methods.",
    "code_link": ""
  },
  "cvpr2018_w11_gaitrecognitionbydeformableregistration": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Gait Recognition by Deformable Registration",
    "authors": [
      "Yasushi Makihara",
      "Daisuke Adachi",
      "Chi Xu",
      "Yasushi Yagi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Makihara_Gait_Recognition_by_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Makihara_Gait_Recognition_by_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper describes a method of gait recognition robust against intra-subject posture changes. A person sometimes walks with changing his/her posture when looking down at a smartphone or carrying a heavy object, which makes intra-subject variation large and consequently makes gait recognition difficult. We therefore introduce a deformable registration model to mitigate the intra-subject posture changes. More specifically, we represent a deformation field by a set of deformation vectors on lattice-type control points allocated on an image, i.e., by free-form deformation (FFD) framework. Given a pair of a probe and a gallery, we compute the deformation field so as to minimize the difference between a probe morphed by the deformation field and the gallery, as well as to ensure the spatial smoothness of the deformation field. We then learn the intra-subject eigen deformation modes from a training set of the same subjects' pairs (e.g., bending the upper body forward and swinging arms more), which are relatively different from inter-subject deformation modes (e.g., body shape spread and stride change). Moreover, because the deformable registration is responsible for a preprocessing part before matching, it can be combined with any types of matching algorithms for gait recognition. Experiments with 1,334 subjects show that the proposed method improves the gait recognition accuracy in both cases without and with a state-of-the-art deep learning-based matcher, respectively.",
    "code_link": ""
  },
  "cvpr2018_w11_fusionofhandcraftedanddeeplearningfeaturesforlarge-scalemultipleirispresentationattackdetection": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Fusion of Handcrafted and Deep Learning Features for Large-Scale Multiple Iris Presentation Attack Detection",
    "authors": [
      "Daksha Yadav",
      "Naman Kohli",
      "Akshay Agarwal",
      "Mayank Vatsa",
      "Richa Singh",
      "Afzel Noore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Yadav_Fusion_of_Handcrafted_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Yadav_Fusion_of_Handcrafted_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Typical iris recognition systems may be vulnerable to presentation attacks such as textured contact lenses, print attacks, and synthetic iris images. Increasing applications of iris recognition have raised the importance of efficient presentation attack detection algorithms. In this paper, we propose a novel algorithm for detecting iris presentation attacks using a combination of handcrafted and deep learning based features. The proposed algorithm combines local and global Haralick texture features in multi-level Redundant Discrete Wavelet Transform domain with VGG features to encode the textural variations between real and attacked iris images. The proposed algorithm is extensively tested on a large iris dataset comprising more than 270,000 real and attacked iris images and yields a total error of 1.01%. The experimental evaluation demonstrates the superior presentation attack detection performance of the proposed algorithm as compared to state-of-the-art algorithms.",
    "code_link": ""
  },
  "cvpr2018_w11_hierarchicalnetworkforfacialpalsydetection": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Biometrics",
    "title": "Hierarchical Network for Facial Palsy Detection",
    "authors": [
      "Gee-Sern Jison Hsu",
      "Wen-Fong Huang",
      "Jiunn-Horng Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w11/html/Hsu_Hierarchical_Network_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w11/Hsu_Hierarchical_Network_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose the Hierarchical Detection Network (HDN) for the detection of facial palsy syndrome. This can be the first deep-learning based approach for the facial palsy detection. The proposed HDN consists of three component networks, the first detects faces, the second detects the landmarks on the detected faces, and the third detects the local palsy regions. The first and the third component networks are built on the Darknet framework, but with fewer layers of convolutions for shorter processing speed. The second component network employs the latest 3D face alignment network for locating the landmarks. The first component network employs a Na X Na grid over the overall input image, while the third component network employs a Nb X Nb grid over each detected face, making the HDN capable of efficiently locating the affected palsy regions. As previous approaches were evaluated on proprietary databases, we have collected 32 videos from YouTube and made the first public database for facial palsy study. To enhance the robustness against expression variations, we include the CK+ facial expression database in the training and testing phases. We show that the HDN does not just detect the local palsy regions, but also captures the frequency of the intensity, enabling the video-to-description diagnosis of the syndrome. Experiments show that the proposed approach offers an accurate and efficient solution for facial palsy detection/diagnosis",
    "code_link": ""
  },
  "cvpr2018_w12_acomparativestudyofreal-timesemanticsegmentationforautonomousdriving": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "A Comparative Study of Real-Time Semantic Segmentation for Autonomous Driving",
    "authors": [
      "Mennatullah Siam",
      "Mostafa Gamal",
      "Moemen Abdel-Razek",
      "Senthil Yogamani",
      "Martin Jagersand",
      "Hong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Siam_A_Comparative_Study_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Siam_A_Comparative_Study_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Semantic segmentation is a critical module in robotics related applications, especially autonomous driving. Most of the research on semantic segmentation is focused on improving the accuracy with less attention paid to computationally efficient solutions. Majority of the efficient semantic segmentation algorithms have customized optimizations without scalability and there is no systematic way to compare them. In this paper, we present a real-time segmentation benchmarking framework and study various segmentation algorithms for autonomous driving. We implemented a generic meta-architecture via a decoupled design where different types of encoders and decoders can be plugged in independently. We provide several example encoders including VGG16, Resnet18, MobileNet, and ShuffleNet and decoders including SkipNet, UNet and Dilation Frontend. The framework is scalable for addition of new encoders and decoders developed in the community for other vision tasks. We performed detailed experimental analysis on cityscapes dataset for various combinations of encoder and decoder. The modular framework enabled rapid prototyping of a custom efficient architecture which provides x143 GFLOPs reduction compared to SegNet and runs real-time at 15 fps on NVIDIA Jetson TX2. The source code of the framework is publicly available.",
    "code_link": "https://github.com/MSiam/TFSegmentation"
  },
  "cvpr2018_w12_efficientsemanticsegmentationusinggradualgrouping": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Efficient Semantic Segmentation Using Gradual Grouping",
    "authors": [
      "Nikitha Vallurupalli",
      "Sriharsha Annamaneni",
      "Girish Varma",
      "C.V. Jawahar",
      "Manu Mathew",
      "Soyeb Nagori"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Vallurupalli_Efficient_Semantic_Segmentation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Vallurupalli_Efficient_Semantic_Segmentation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep CNNs for semantic segmentation have high mem- ory and run time requirements. Various approaches have been proposed to make CNNs efficient like grouped, shuf- fled, depth-wise separable convolutions. We study the ef- fectiveness of these techniques on a real-time semantic segmentation architecture like ERFNet for improving run- time by over 5X. We apply these techniques to CNN lay- ers partially or fully and evaluate the testing accuracies on Cityscapes dataset. We obtain accuracy vs parame- ters/FLOPs trade offs, giving accuracy scores for models that can run under specified runtime budgets. We further propose a novel training procedure which starts out with a dense convolution but gradually evolves towards a grouped convolution. We show that our proposed training method and efficient architecture design can im- prove accuracies by over 8% with depthwise separable con- volutions applied on the encoder of ERFNet and attaching a light weight decoder. This results in a model which has a 5X improvement in FLOPs while only suffering a 4% degra- dation in accuracy with respect to ERFNet.",
    "code_link": ""
  },
  "cvpr2018_w12_ifq-netintegratedfixed-pointquantizationnetworksforembeddedvision": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "IFQ-Net: Integrated Fixed-Point Quantization Networks for Embedded Vision",
    "authors": [
      "Hongxing Gao",
      "Wei Tao",
      "Dongchao Wen",
      "Tse-Wei Chen",
      "Kinya Osa",
      "Masami Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Gao_IFQ-Net_Integrated_Fixed-Point_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Gao_IFQ-Net_Integrated_Fixed-Point_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deploying deep models on embedded devices has been a challenging problem since the great success of deep learning based networks. Fixed-point networks, which represent their data with low bits fixed-point and thus give remarkable savings on memory usage, are generally preferred.Even though current fixed-point networks employ relative low bits (e.g. 8-bits), the memory saving is far away from enough for the embedded devices. On the other hand, quantization deep networks, for example XNOR-Net and HWGQ-Net, quantize the data into 1 or 2 bits resulting in more significant memory savings but still contain lots of floating-point data. In this paper, we propose a fixed-point network for embedded vision tasks through converting the floating-point data in a quantization network into fixed-point.Furthermore, to overcome the data loss caused by the conversion, we propose to compose floating-point data operations across multiple layers (e.g. convolution, batch normalization and quantization layers) and convert them into fixed-point. We name the fixed-point network obtained through such integrated conversion as Integrated Fixed-point Quantization Networks (IFQ-Net). We demonstrate that our IFQ-Net gives 2.16xand 18xmore savings on model size and runtime feature map memory respectively with similar accuracy on ImageNet. Furthermore, based on YOLOv2, we design IFQ-Tinier-YOLO face detector which is a fixed-point network with 256x reduction in model size (246k Bytes) than Tiny-YOLO. We illustrate the promising performance of our face detector in terms of detection rate on Face Detection Data Set and Benchmark (FDDB) and qualitative results of detecting small faces of Wider Face dataset.",
    "code_link": ""
  },
  "cvpr2018_w12_interpolation-basedobjectdetectionusingmotionvectorsforembeddedreal-timetrackingsystems": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Interpolation-Based Object Detection Using Motion Vectors for Embedded Real-Time Tracking Systems",
    "authors": [
      "Takayuki Ujiie",
      "Masayuki Hiromoto",
      "Takashi Sato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Ujiie_Interpolation-Based_Object_Detection_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Ujiie_Interpolation-Based_Object_Detection_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep convolutional neural networks (CNNs) have achieved outstanding performance in object detection, a crucial task in computer vision. With the computational intensiveness due to repeated convolutions, they consume large amount of power, making them difficult to apply in power-constrained embedded platforms. In this work, we present MVint, a power-efficient detection and tracking framework. MVint combines motion-vector-based interpolator and CNN-based detector to simultaneously achieve high accuracy and energy efficiency by utilizing motion vectors obtained inexpensively in the environments wherein encoding is conducted at the cameras. Through evaluations using MOT16 benchmark that evaluates multiple object tracking, we show MVint maintains 88% MOTA while reducing detection frequency down to 1/12. An implemention of MVint as a system prototype on Xilinx Zynq UltraScale+ MPSoC ZCU102 confirmed that MVint achieves an ideal 12x FPS compared with a vanilla detection approach.",
    "code_link": ""
  },
  "cvpr2018_w12_onboardstereovisionfordronepursuitorsenseandavoid": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Onboard Stereo Vision for Drone Pursuit or Sense and Avoid",
    "authors": [
      "Cevahir Cigla",
      "Rohan Thakker",
      "Larry Matthies"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Cigla_Onboard_Stereo_Vision_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Cigla_Onboard_Stereo_Vision_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We describe a new, on-board, short range perception system that enables micro aerial vehicles (MAVs) to detect, track, and follow or avoid nearby drones (within 2-20 meters) in GPS-denied environments. Each vehicle is able to sense its neighborhood and adapt its motion accordingly without use of centralized reasoning or inter-vehicle communication. To enable a lightweight, low power solution, on-board stereo cameras are used for detection and tracking with depth maps, while a downward-looking camera and an inertial measurement unit are used to estimate the position of the observer without use of GPS. We illustrate the robustness and accuracy of this approach through real-time, outdoor leader-follower experiments with three different quadrotors. Our experiments show that state-of-art trackers are far less robust in detection against cluttered background. This demonstrates that stereo vision is a highly effective approach to perception for safe navigation of multiple MAVs in close proximity.",
    "code_link": ""
  },
  "cvpr2018_w12_lightfielddepthestimationonoff-the-shelfmobilegpu": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Light Field Depth Estimation on Off-the-Shelf Mobile GPU",
    "authors": [
      "Andre Ivan",
      "Williem",
      "In Kyu Park"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Ivan_Light_Field_Depth_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Ivan_Light_Field_Depth_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " While novel light processing algorithms have been continuously introduced, it is still challenging to perform light field processing on a mobile device with limited computation resource due to the high dimensionality of light field data. Recently, the performance of mobile graphics processing unit (GPU) increases rapidly and GPGPU on mobile GPU utilizes massive parallel computation to solve various computer vision problems with high computational complexity. To show the potential capability of light field processing on mobile GPU, we parallelize and optimize the state-of-the-art light field depth estimation which is essential to many light field applications. We employ both algorithm and kernel-based optimization to enable light field processing on mobile GPU. Light field processing involves independent pixel processing with intensive floating-point operations that can be vectorized to match single instruction multiple data (SIMD) style of GPU architecture. We design efficient memory access, caching, and prefetching to exploit light field properties. The experimental result shows that the light field depth estimation on mobile GPU obtains comparable performance as on the desktop CPU. The proposed optimization method gains up to 25 times speedup compared to the naive baseline method.",
    "code_link": ""
  },
  "cvpr2018_w12_pseudo-labelsforsupervisedlearningondynamicvisionsensordata,appliedtoobjectdetectionunderego-motion": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Pseudo-Labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection Under Ego-Motion",
    "authors": [
      "Nicholas F. Y. Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Chen_Pseudo-Labels_for_Supervised_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Chen_Pseudo-Labels_for_Supervised_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In recent years, dynamic vision sensors (DVS), also known as event-based cameras or neuromorphic sensors, have seen increased use due to various advantages over conventional frame-based cameras. Using principles inspired by the retina, its high temporal resolution overcomes motion blurring, its high dynamic range overcomes extreme illumination conditions and its low power consumption makes it ideal for embedded systems on platforms such as drones and self-driving cars. However, event-based data sets are scarce and labels are even rarer for tasks such as object detection. We transferred discriminative knowledge from a state-of-the-art frame-based convolutional neural network (CNN) to the event-based modality via intermediate pseudo-labels, which are used as targets for supervised learning. We show, for the first time, event-based car detection under ego-motion in a real environment at 100 frames per second with a test average precision of 40.3% relative to our annotated ground truth. The event-based car detector handles motion blur and poor illumination conditions despite not explicitly trained to do so, and even complements frame-based CNN detectors, suggesting that it has learnt generalized visual representations.",
    "code_link": ""
  },
  "cvpr2018_w12_gpubasedvideoobjecttrackingonptzcameras": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "GPU Based Video Object Tracking on PTZ Cameras",
    "authors": [
      "Cevahir Cigla",
      "Kemal Emrecan Sahin",
      "Fikret Alim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Cigla_GPU_Based_Video_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Cigla_GPU_Based_Video_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this study, an embedded Pan-Tilt-Zoom (PTZ) tracker system design is proposed that is based on NVIDIA Tegra TK1-TX1 mobile GPU platform. For this purpose, state-of-the-art correlation filter (CF) based video object tracking (VOT) algorithms are exploited regarding their high performance. Each algorithmic step is carefully implemented on GPU that further increases the efficiency and decreases execution times. The PTZ control is designed to track human targets by centralizing within the image coordinates where the targets have limited speed but obvious appearance changes. Incorporating on-board decode and encode capability of Tegra platform as well as angular position control, the presented approach enables 50-100 fps target tracking for HD (1920x1080) videos on TK1 and TX1 correspondingly. This is to our best knowledge the first efficient implementation of CF trackers on a mobile GPU platform with use of multiple features, scale and background adaptation. This study extends the scope of accuracy focused VOT research to platform optimized efficient implementations for real-time high resolution video tracking.",
    "code_link": ""
  },
  "cvpr2018_w12_analysisofefficientcnndesigntechniquesforsemanticsegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Analysis of Efficient CNN Design Techniques for Semantic Segmentation",
    "authors": [
      "Alexandre Briot",
      "Prashanth Viswanath",
      "Senthil Yogamani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Briot_Analysis_of_Efficient_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Briot_Analysis_of_Efficient_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Majority of CNN architecture design is aimed at achieving high accuracy in public benchmarks by increasing the complexity. Typically, they are over-specified by a large margin and can be optimized by a factor of 10-100x with only a small reduction in accuracy. In spite of the increase in computational power of embedded systems, these networks are still not suitable for embedded deployment. There is a large need to optimize for hardware and reduce the size of the network by orders of magnitude for computer vision applications. This has led to a growing community which is focused on designing efficient networks. However, CNN architectures are evolving rapidly and efficient architectures seem to lag behind. There is also a gap in understanding the hardware architecture details and incorporating it into the network design. The motivation of this paper is to systematically summarize efficient design techniques and provide guidelines for an application developer. We also perform a case study by benchmarking various semantic segmentation algorithms for autonomous driving.",
    "code_link": ""
  },
  "cvpr2018_w12_designofareconfigurable3dpixel-parallelneuromorphicarchitectureforsmartimagesensor": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "Design of a Reconfigurable 3D Pixel-Parallel Neuromorphic Architecture for Smart Image Sensor",
    "authors": [
      "Pankaj Bhowmik",
      "Md Jubaer Hossain Pantho",
      "Marjan Asadinia",
      "Christophe Bobda"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Bhowmik_Design_of_a_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Bhowmik_Design_of_a_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Power reduction and speed-up of image processing algorithms remain of high interest as image resolutions continue to increase. Neuromorphic-circuits are inspired by the nervous system aiming to reduce power consumption and speed-up. This paper presents a neuromorphic smart image sensor designed by the pixel-parallel 3D hierarchical architecture with an on-chip attention module. The module dynamically detects regions with relevant information and produces a feedback path to sample those regions at high speed. On the other hand, by sampling non-relevant regions with a low-speed, the sensor can reduce redundancy and enable high-performance computing by ensuring low-power operation. The image sensor is comprised of several hierarchical planes and each plane has small and independent reconfigurable computational units (XPU). In each plane, all XPUs operate in parallel with different operating speed which gives a pixel-parallel architecture. When the raw image passes through the hierarchical planes, necessary image processing algorithms are performed in parallel on different planes at a variable clock rate for saving power and reducing redundancy. The goal of this work is to prototype the focal plane image sensor which emulates the brain features. The results show that the prototype achieves remarkable power savings and speed-up at different stages.",
    "code_link": ""
  },
  "cvpr2018_w12_kcnnextremely-efficienthardwarekeypointdetectionwithacompactconvolutionalneuralnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Embedded Vision",
    "title": "KCNN: Extremely-Efficient Hardware Keypoint Detection With a Compact Convolutional Neural Network",
    "authors": [
      "Paolo Di Febbo",
      "Carlo Dal Mutto",
      "Kinh Tieu",
      "Stefano Mattoccia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w12/html/Di_Febbo_KCNN_Extremely-Efficient_Hardware_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w12/Di_Febbo_KCNN_Extremely-Efficient_Hardware_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Keypoint detection algorithms are typically based on handcrafted combinations of derivative operations implemented with standard image filtering approaches. The early layers of Convolutional Neural Networks (CNNs) for image classification, whose implementation is nowadays often available within optimized hardware units, are characterized by a similar architecture. Therefore, the exploration of CNNs for keypoint detection is a promising avenue to obtain a low-latency implementation, also enabling to effectively move the computational cost of the detection to dedicated Neural Network processing units. This paper proposes a methodology for effective keypoint detection by means of an efficient CNN characterized by a compact three-layer architecture. A novel training procedure is proposed for learning values of the network parameters which allow for an approximation of the response of handcrafted detectors, showing that the proposed architecture is able to obtain results comparable with the state of the art. The capability of emulating different detectors allows to deploy a variety of algorithms to dedicated hardware by simply retraining the network. A sensor-based FPGA implementation of the introduced CNN architecture is presented, allowing latency smaller than 1[ms].",
    "code_link": ""
  },
  "cvpr2018_w13_wespeweaklysupervisedphotoenhancerfordigitalcameras": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "WESPE: Weakly Supervised Photo Enhancer for Digital Cameras",
    "authors": [
      "Andrey Ignatov",
      "Nikolay Kobyshev",
      "Radu Timofte",
      "Kenneth Vanhoey",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Ignatov_WESPE_Weakly_Supervised_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Ignatov_WESPE_Weakly_Supervised_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Low-end and compact mobile cameras demonstrate limited photo quality mainly due to space, hardware and budget constraints. In this work, we propose a deep learning solution that translates photos taken by cameras with limited capabilities into DSLR-quality photos automatically. We tackle this problem by introducing a weakly supervised photo enhancer (WESPE) - a novel image-to-image Generative Adversarial Network-based architecture. The proposed model is trained by under weak supervision: unlike previous works, there is no need for strong supervision in the form of a large annotated dataset of aligned original/enhanced photo pairs. The sole requirement is two distinct datasets: one from the source camera, and one composed of arbitrary high-quality images that can be generally crawled from the Internet - the visual content they exhibit may be unrelated. In this work, we emphasize on extensive evaluation of obtained results. Besides standard objective metrics and subjective user study, we train a virtual rater in the form of a separate CNN that mimics human raters on Flickr data and use this network to get reference scores for both original and enhanced photos. Our experiments on the DPED, KITTI and Cityscapes datasets as well as pictures from several generations of smartphones demonstrate that WESPE produces comparable or improved qualitative results with state-of-the-art strongly supervised methods.",
    "code_link": ""
  },
  "cvpr2018_w13_unsupervisedimagesuper-resolutionusingcycle-in-cyclegenerativeadversarialnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unsupervised Image Super-Resolution Using Cycle-in-Cycle Generative Adversarial Networks",
    "authors": [
      "Yuan Yuan",
      "Siyuan Liu",
      "Jiawei Zhang",
      "Yongbing Zhang",
      "Chao Dong",
      "Liang Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Yuan_Unsupervised_Image_Super-Resolution_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Yuan_Unsupervised_Image_Super-Resolution_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We consider the single image super-resolution problem in a more general case that the low-/high-resolution pairs and the down-sampling process are unavailable. Different from traditional super-resolution formulation, the low-resolution input is further degraded by noises and blurring. This complicated setting makes supervised learning and accurate kernel estimation impossible. To solve this problem, we resort to unsupervised learning without paired data, inspired by the recent successful image-to-image translation applications. With generative adversarial networks (GAN) as the basic component, we propose a Cycle-in-Cycle network structure to tackle the problem within three steps. First, the noisy and blurry input is mapped to a noise-free low-resolution space. Then the intermediate image is up-sampled with a pre-trained deep model. Finally, we fine-tune the two modules in an end-to-end manner to get the high-resolution output. Experiments on NTIRE2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the state-of-the-art supervised models.",
    "code_link": "https://github.com/thstkdgus35/EDSR-PyTorch"
  },
  "cvpr2018_w13_dpw-sdnetdualpixel-waveletdomaindeepcnnsforsoftdecodingofjpeg-compressedimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DPW-SDNet: Dual Pixel-Wavelet Domain Deep CNNs for Soft Decoding of JPEG-Compressed Images",
    "authors": [
      "Honggang Chen",
      "Xiaohai He",
      "Linbo Qing",
      "Shuhua Xiong",
      "Truong Q. Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Chen_DPW-SDNet_Dual_Pixel-Wavelet_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Chen_DPW-SDNet_Dual_Pixel-Wavelet_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " JPEG is one of the widely used lossy compression methods. JPEG-compressed images usually suffer from compression artifacts including blocking and blurring, especially at low bit-rates. Soft decoding is an effective solution to improve the quality of compressed images without changing codec or introducing extra coding bits. Inspired by the excellent performance of the deep convolutional neural networks (CNNs) on both low-level and high-level computer vision problems, we develop a dual pixel-wavelet do\u0002main deep CNNs-based soft decoding network for JPEG\u0002compressed images, namely DPW-SDNet. The pixel do\u0002main deep network takes the four downsampled versions of the compressed image to form a 4-channel input and out\u0002puts a pixel domain prediction, while the wavelet domain deep network uses the 1-level discrete wavelet transformation (DWT) coefficients to form a 4-channel input to produce a DWT domain prediction. The pixel domain and wavelet domain estimates are combined to generate the final soft decoded result. Experimental results demonstrate the superiority of the proposed DPW-SDNet over several state-of-the-art compression artifacts reduction algorithms.",
    "code_link": ""
  },
  "cvpr2018_w13_attributeaugmentedconvolutionalneuralnetworkforfacehallucination": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Attribute Augmented Convolutional Neural Network for Face Hallucination",
    "authors": [
      "Cheng-Han Lee",
      "Kaipeng Zhang",
      "Hu-Cheng Lee",
      "Chia-Wen Cheng",
      "Winston Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Lee_Attribute_Augmented_Convolutional_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Lee_Attribute_Augmented_Convolutional_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Though existing face hallucination methods achieve great performance on the global region evaluation, most of them cannot recover local attributes accurately, especially when super-resolving a very low-resolution face image from 14X12 pixels to its 8X larger one. In this paper, we propose a brand new Attribute Augmented Convolutional Neural Network (AACNN) to assist face hallucination by exploiting facial attributes. The goal is to augment face hallucination, particularly the local regions, with informative attribute description. More specifically, our method fuses the advantages of both image domain and attribute domain, which significantly assists facial attributes recovery. Extensive experiments demonstrate that our proposed method achieves superior visual quality of hallucination on both local region and global region against the state-of-the-art methods. In addition, our AACNN still improves the performance of hallucination adaptively with partial attribute input.",
    "code_link": ""
  },
  "cvpr2018_w13_recursivedeepresiduallearningforsingleimagedehazing": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Recursive Deep Residual Learning for Single Image Dehazing",
    "authors": [
      "Yixin Du",
      "Xin Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Du_Recursive_Deep_Residual_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Du_Recursive_Deep_Residual_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " There have been a flurry of works on deep learning based image dehazing in recent years. However, most of them have only used deep neural networks to estimate the transmission map (or its variant); while the module of generating dehazed image is still model-based. Inspired by the analogy between image dehazing and image denoising, we propose to reformulate dehazing as a problem of learning structural residue (instead of white Gaussian noise) and remove haze from a single image by a deep residue learning (DRL) network. Such novel reformulation enables us to directly estimate a nonlinear mapping from input hazy images to output dehazed ones (i.e., bypassing the unnecessary step of transmission map estimation). The dehazing-denoising analogy also motivates us to leverage the strategy of iterative regularization from denoising to dehazing - i.e., we propose to recursively feed the dehazed image back to the input of DRL network. Such recursive extension can be interpreted as a nonlinear optimization of DRL whose convergence can be rigorously analyzed using fixed-point theory. We have conducted extensive experimental studies on both synthetic and real-world hazy image data. Our experimental results have verified the effectiveness of the proposed recursive DRL approach and shown that our technique outperforms other competing methods in terms of both subjective and objective visual qualities of dehazed images.",
    "code_link": ""
  },
  "cvpr2018_w13_synthesizedtexturequalityassessmentviamulti-scalespatialandstatisticaltextureattributesofimageandgradientmagnitudecoefficients": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Synthesized Texture Quality Assessment via Multi-Scale Spatial and Statistical Texture Attributes of Image and Gradient Magnitude Coefficients",
    "authors": [
      "Alireza Golestaneh",
      "Lina J. Karam"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Golestaneh_Synthesized_Texture_Quality_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Golestaneh_Synthesized_Texture_Quality_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Perceptual quality assessment for synthesized textures is a challenging task. In this paper, we propose a training-free reduced-reference(RR) objective quality assessment method that quantifies the perceived quality of synthesized textures.The proposed reduced-reference synthesized texture quality assessment metric is based on measuring the spatial and statistical attributes of the texture image using both image- and gradient-based wavelet coefficients at multiple scales. Performance evaluations on two synthesized texture databases demonstrate that our proposed RR synthesized texture quality metric significantly outperforms both full-reference and RR state-of-the-art quality metrics in predicting the perceived visual quality of the synthesized textures.The source code of our proposed method and the evaluation results will publicly be available online at the authors' website.",
    "code_link": ""
  },
  "cvpr2018_w13_learningfacedeblurringfastandwide": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Learning Face Deblurring Fast and Wide",
    "authors": [
      "Meiguang Jin",
      "Michael Hirsch",
      "Paolo Favaro"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Jin_Learning_Face_Deblurring_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Jin_Learning_Face_Deblurring_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Portrait images and photos containing faces are ubiquitous on the web and the predominant subject of images shared via social media. Especially selfie images taken with lightweight smartphone cameras are susceptible to camera shake. Despite significant progress in the field of image deblurring over the last decade,the performance of state-of-the-art deblurring methods on blurry faceimages is still limited. In this work, we present a novel deep learning architecture that is designed to be computationally fast and exploits avery wide receptive field to return sharp face images evenin challenging scenarios.Our network features an effective resampling convolution operationthat ensures a wide receptive field from the very first layers,while at the same time being highly computationally efficient. We also show that batch normalization prevents networks from yielding high-quality image results and introduce instance normalization instead.We demonstrate our architecture on face deblurring as well as other more general scenes. Extensive experiments with state-of-the-art methods demonstrate the effectiveness of our proposed network, in terms of run-time, accuracy, and robustness to ISO levels as well as gamma correction.",
    "code_link": ""
  },
  "cvpr2018_w13_o-hazeadehazingbenchmarkwithrealhazyandhaze-freeoutdoorimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "O-HAZE: A Dehazing Benchmark With Real Hazy and Haze-Free Outdoor Images",
    "authors": [
      "Codruta O. Ancuti",
      "Cosmin Ancuti",
      "Radu Timofte",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Ancuti_O-HAZE_A_Dehazing_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Ancuti_O-HAZE_A_Dehazing_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Haze removal or dehazing is a challenging ill-posed problem that has drawn a significant attention in the last few years. Despite this growing interest, the scientific community is still lacking a reference dataset to evaluate objectively and quantitatively the performance of proposed dehazing methods. The few datasets that are currently considered, both for assessment and training of learning-based dehazing techniques, exclusively rely on synthetic hazy images. To address this limitation, we introduce the first outdoor scenes database (named O-HAZE) composed of pairs of real hazy and corresponding haze-free images. In practice, hazy images have been captured in presence of real haze, generated by professional haze machines, and OHAZE contains 45 different outdoor scenes depicting the same visual content recorded in haze-free and hazy conditions, under the same illumination parameters. To illustrate its usefulness, O-HAZE is used to compare a representative set of state-of-the-art dehazing techniques, using traditional image quality metrics such as PSNR, SSIM and CIEDE2000. This reveals the limitations of current techniques, and questions some of their underlying assumptions.",
    "code_link": ""
  },
  "cvpr2018_w13_largereceptivefieldnetworksforhigh-scaleimagesuper-resolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Large Receptive Field Networks for High-Scale Image Super-Resolution",
    "authors": [
      "George Seif",
      "Dimitrios Androutsos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Seif_Large_Receptive_Field_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Seif_Large_Receptive_Field_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Convolutional Neural Networks have been the backbone of recent rapid progress in Single-Image Super-Resolution. However, existing networks are very deep with many network parameters, thus having a large memory footprint and being challenging to train. We propose Large Receptive Field Networks which strive to directly expand the receptive field of Super-Resolution networks without increasing depth or parameter count. In particular, we use two different methods to expand the network receptive field: 1-D separable kernels and atrous convolutions. We conduct considerable experiments to study the performance of various arrangement schemes of the 1-D separable kernels and atrous convolution in terms of accuracy (PSNR / SSIM), parameter count, and speed, while focusing on the more challenging high upscaling factors. Extensive benchmark evaluations demonstrate the effectiveness of our approach.",
    "code_link": ""
  },
  "cvpr2018_w13_multi-levelwavelet-cnnforimagerestoration": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Level Wavelet-CNN for Image Restoration",
    "authors": [
      "Pengju Liu",
      "Hongzhi Zhang",
      "Kai Zhang",
      "Liang Lin",
      "Wangmeng Zuo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Liu_Multi-Level_Wavelet-CNN_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Liu_Multi-Level_Wavelet-CNN_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The tradeoff between receptive field size and efficiency is a crucial issue in low level vision. Plain convolutional networks (CNNs) generally enlarge the receptive field at the expense of computational cost. Recently, dilated filtering has been adopted to address this issue. But it suffers from gridding effect, and the resulting receptive field is only a sparse sampling of input image with checkerboard patterns. In this paper, we present a novel multi-level wavelet CNN (MWCNN) model for better tradeoff between receptive field size and computational efficiency. With the modified U-Net architecture, wavelet transform is introduced to reduce the size of feature maps in the contracting subnetwork. Furthermore, another convolutional layer is further used to decrease the channels of feature maps. In the expanding subnetwork, inverse wavelet transform is then deployed to reconstruct the high resolution feature maps. Our MWCNN can also be explained as the generalization of dilated filtering and subsampling, and can be applied to many image restoration tasks. The experimental results clearly show the effectiveness of MWCNN for image denoising, single image super-resolution, and JPEG image artifacts removal.",
    "code_link": "https://github.com/lpj0/MWCNN"
  },
  "cvpr2018_w13_comboganunrestrainedscalabilityforimagedomaintranslation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "ComboGAN: Unrestrained Scalability for Image Domain Translation",
    "authors": [
      "Asha Anoosheh",
      "Eirikur Agustsson",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Anoosheh_ComboGAN_Unrestrained_Scalability_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Anoosheh_ComboGAN_Unrestrained_Scalability_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The past year alone has seen unprecedented leaps in the area of learning-based image translation, namely CycleGAN, by Zhu et al. But experiments so far have been tailored to merely two domains at a time, and scaling them to more would require an quadratic number of models to be trained. And with two-domain models taking days to train on current hardware, the number of domains quickly becomes limited by the time and resources required to process them. In this paper, we propose a multi-component image translation model and training scheme which scales linearly - both in resource consumption and time required - with the number of domains. We demonstrate its capabilities on a dataset of paintings by 14 different artists and on images of the four different seasons in the Alps. Note that 14 data groups would need (14 choose 2) = 91 different CycleGAN models: a total of 182 generator/discriminator pairs; whereas our model requires only 14 generator/discriminator pairs.",
    "code_link": "https://github.com/AAnoosheh/ComboGAN"
  },
  "cvpr2018_w13_imagesuper-resolutionviaprogressivecascadingresidualnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Image Super-Resolution via Progressive Cascading Residual Network",
    "authors": [
      "Namhyuk Ahn",
      "Byungkon Kang",
      "Kyung-Ah Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Ahn_Image_Super-Resolution_via_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Ahn_Image_Super-Resolution_via_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The problem of enhancing the resolution of a single low-resolution image has been popularly addressed by recent deep learning techniques. However, many deep learning approaches still fail to deal with extreme super-resolution scenarios because of the instability of training. In this paper, we address this issue by adapting a progressive learning scheme to the deep convolutional neural network. In detail, the overall training proceeds in multiple stages so that the model gradually increases the output image resolution. In our experiments, we show that this property yields a large performance gain compared to the non-progressive learning methods.",
    "code_link": ""
  },
  "cvpr2018_w13_deepresidualnetworkwithenhancedupscalingmoduleforsuper-resolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Residual Network With Enhanced Upscaling Module for Super-Resolution",
    "authors": [
      "Jun-Hyuk Kim",
      "Jong-Seok Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Kim_Deep_Residual_Network_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Kim_Deep_Residual_Network_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Single image super-resolution (SR) have recently shown great performance thanks to the advances in deep learning. In the middle of the deep networks for SR, a part that increases image resolution is required, for which a sub-pixel convolution layer is known as an efficient way. However, we argue that the method has room for improvement, and propose an enhanced upscaling module (EUM), which achieves improvement by utilizing nonlinear operations and skip connections. Employing our proposed EUM, we propose a novel deep residual network for SR, called EUSR. Our proposed EUSR was ranked in the 9th place among 24 teams in terms of SSIM in track 1 of the NTIRE 2018 SR Challenge. In addition, we experimentally show that the EUSR has comparable performance on x2 and x4 SR for four benchmark datasets to the state-of-the-art methods, and outperforms them on a large scaling factor (x8).",
    "code_link": ""
  },
  "cvpr2018_w13_persistentmemoryresidualnetworkforsingleimagesuperresolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Persistent Memory Residual Network for Single Image Super Resolution",
    "authors": [
      "Rong Chen",
      "Yanyun Qu",
      "Kun Zeng",
      "Jinkang Guo",
      "Cuihua Li",
      "Yuan Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Chen_Persistent_Memory_Residual_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Chen_Persistent_Memory_Residual_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Progresses has been witnessed in single image super-resolution in which the low-resolution images are simulated by bicubic downsampling. However, for the complex image degradation in the wild such as downsampling, blurring, noises, and geometric deformation, the existing super-resolution methods do not work well. Inspired by a persistent memory network which has been proven to be effective in image restoration, we implement the core idea of human memory on the deep residual convolutional neural network. Two types of memory blocks are designed for the NTIRE2018 challenge. We embed the two types of memory blocks in the framework of enhanced super resolution network (EDSR), which is the NTIRE2017 champion method. The residual blocks of EDSR is replaced by two types of memory blocks. The first type of memory block is a residual module, and one memory block contains four residual modules with four residual blocks followed by a gate unit, which adaptively selects the features needed to store. The second type of memory block is a residual dilated convolutional block, which contains seven dilated convolution layers linked to a gate unit. The two proposed models not only improve the super-resolution performance but also mitigate the image degradation of noises and blurring. Experimental results on the DIV2K demonstrate our models achieve better performance than EDSR.",
    "code_link": ""
  },
  "cvpr2018_w13_fullyend-to-endlearningbasedconditionalboundaryequilibriumganwithreceptivefieldsizesenlargedforsingleultra-highresolutionimagedehazing": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Fully End-to-End Learning Based Conditional Boundary Equilibrium GAN With Receptive Field Sizes Enlarged for Single Ultra-High Resolution Image Dehazing",
    "authors": [
      "Sehwan Ki",
      "Hyeonjun Sim",
      "Jae-Seok Choi",
      "Saehun Kim",
      "Munchurl Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Ki_Fully_End-to-End_Learning_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Ki_Fully_End-to-End_Learning_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " A receptive field is defined as the region in an input image space that an output image pixel is looking at. Thus, the receptive field size influences the learning of deep convolution neural networks. Especially, in single image dehazing problems, larger receptive fields often show more effective dehazying by considering the brightness and color of the entire input hazy image without additional information (e.g. scene transmission map, depth map, and atmospheric light). The conventional generative adversarial network (GAN) with small-sized receptive fields cannot be effective for hazy images of ultra-high resolution. Thus, we proposed a fully end-to-end learning based conditional boundary equilibrium generative adversarial network (BEGAN) with the receptive field sizes enlarged for single image dehazing. In our conditional BEGAN, its discriminator is trained ultra-high resolution conditioned on downscale input hazy images, so that the haze can effectively be removed with the original structures of images stably preserved. From this, we can obtain the high PSNR performance (Track 1 - Indoor: top 4th-ranked) and fast computation speeds. Also, we combine an L1 loss, a perceptual loss and a GAN loss as the generator's loss of the proposed conditional BEGAN, which allows to obtain stable dehazing results for various hazy images.",
    "code_link": ""
  },
  "cvpr2018_w13_cycle-dehazeenhancedcycleganforsingleimagedehazing": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing",
    "authors": [
      "Deniz Engin",
      "Anil Genc",
      "Hazim Kemal Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Engin_Cycle-Dehaze_Enhanced_CycleGAN_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Engin_Cycle-Dehaze_Enhanced_CycleGAN_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we present an end-to-end network, called Cycle-Dehaze, for single image dehazing problem, which does not require pairs of hazy and corresponding ground truth images for training. That is, we train the network by feeding clean and hazy images in an unpaired manner. Moreover, the proposed approach does not rely on estimation of the atmospheric scattering model parameters. Our method enhances CycleGAN formulation by combining cycle-consistency and perceptual losses in order to improve the quality of textural information recovery and generate visually better haze-free images. Typically, deep learning models for dehazing take low resolution images as input and produce low resolution outputs. However, in the NTIRE 2018 challenge on single image dehazing, high resolution images were provided. Therefore, we apply bicubic downscaling. After obtaining low-resolution outputs from the network, we utilize the Laplacian pyramid to upscale the output images to the original resolution. We conduct experiments on NYU-Depth, I-HAZE, and O-HAZE datasets. Extensive experiments demonstrate that the proposed approach improves CycleGAN method both quantitatively and qualitatively.",
    "code_link": "https://github.com/engindeniz/Cycle-Dehaze"
  },
  "cvpr2018_w13_irgunimprovedresiduebasedgradualup-scalingnetworkforsingleimagesuperresolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "IRGUN: Improved Residue Based Gradual Up-Scaling Network for Single Image Super Resolution",
    "authors": [
      "Manoj Sharma",
      "Rudrabha Mukhopadhyay",
      "Avinash Upadhyay",
      "Sriharsha Koundinya",
      "Ankit Shukla",
      "Santanu Chaudhury"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Sharma_IRGUN_Improved_Residue_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Sharma_IRGUN_Improved_Residue_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Convolutional neural network based architectures have achieved decent perceptual quality super resolution on natural images for small scaling factors (2X and 4X). However, image super-resolution for large magnification factors (8X) is an extremely challenging problem for the computer vision community. In this paper, we propose a novel Improved Residual based Gradual Up-Scaling Network (IRGUN) to improve the quality of the super-resolved image for a large magnification factor. IRGUN has a Gradual Upsampling and Residue-based Enhancment Network (GUREN) which comprises of series of Up-scaling and Enhancement blocks (UEB) connected end-to-end and fine-tuned together to give a gradual magnification and enhancement. Due to the perceptual importance of the luminance in super-resolution, the model is trained on luminance(Y) channel of the YCbCr image. Whereas, the chrominance components (Cb and Cr) channel is up-scaled using bicubic interpolation and combined with super-resolved Y channel of the image which is then converted to RGB. A cascaded 3D-RED architecture trained on RGB images is utilized to incorporate its inter-channel correlation. In addition to this, the training methodology is also presented in the paper. In the training procedure, the weights of the previous UEB are used in the next immediate UEB for faster and better convergence. Each UEB is trained on its respective scale by taking the output image of the previous UEB as input and corresponding HR image of the same scale as ground truth to the successive UEB. All the UEBs are then connected end-to-end and fine tuned. The IRGUN recovers fine details effectively at large (8X) magnification factors. The efficiency of IRGUN is presented on various benchmark datasets and at different magnification scales.",
    "code_link": ""
  },
  "cvpr2018_w13_2d-3dcnnbasedarchitecturesforspectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "2D-3D CNN Based Architectures for Spectral Reconstruction From RGB Images",
    "authors": [
      "Sriharsha Koundinya",
      "Himanshu Sharma",
      "Manoj Sharma",
      "Avinash Upadhyay",
      "Raunak Manekar",
      "Rudrabha Mukhopadhyay",
      "Abhijit Karmakar",
      "Santanu Chaudhury"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Koundinya_2D-3D_CNN_Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Koundinya_2D-3D_CNN_Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Hyperspectral cameras are used to preserve fine spectral details of scenes that are not captured by traditional RGB cameras, due to the gross quantization of radiance in RGB images. Spectral details provide additional information that improves the performance of numerous image based analytic applications, but due to high hyperspectral hardware cost and associated physical constraints, hyperspectral images are not easily available for further processing. Motivated by the success of deep learning for various computer vision applications, we propose a 2D convolution neural network and a 3D convolution neural network based approaches for hyper-spectral image reconstruction from RGB images. A 2D-CNN model primarily focuses on extracting spectral data by considering only spatial correlation of the channels in the image, while in 3D-CNN model the inter-channel co-relation is also exploited to refine the extraction of spectral data. Our 3D-CNN based architecture achieves state-of-the-art performance in terms of MRAE and RMSE. In contrast of 3D-CNN, our 2D-CNN based architecture also achieves performance near by state-of-the-art with very less computational complexity.",
    "code_link": ""
  },
  "cvpr2018_w13_ntire2018challengeonsingleimagesuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results",
    "authors": [
      "Radu Timofte",
      "Shuhang Gu",
      "Jiqing Wu",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Timofte_NTIRE_2018_Challenge_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Timofte_NTIRE_2018_Challenge_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper reviews the 2nd NTIRE challenge on single image super-resolution (restoration of rich details in a low resolution image) with focus on proposed solutions and results. The challenge had 4 tracks. Track 1 employed the standard bicubic downscaling setup, while Tracks 2, 3 and 4 had realistic unknown downgrading operators simulating camera image acquisition pipeline. The operators were learnable through provided pairs of low and high resolution train images. The tracks had 145, 114, 101, and 113 registered participants, resp., and 31 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.",
    "code_link": ""
  },
  "cvpr2018_w13_afullyprogressiveapproachtosingle-imagesuper-resolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Fully Progressive Approach to Single-Image Super-Resolution",
    "authors": [
      "Yifan Wang",
      "Federico Perazzi",
      "Brian McWilliams",
      "Alexander Sorkine-Hornung",
      "Olga Sorkine-Hornung",
      "Christopher Schroers"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Wang_A_Fully_Progressive_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Wang_A_Fully_Progressive_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recent deep learning approaches to single image super-resolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method (ProSR) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network (GAN), named ProGanSR, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8 x) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular ProSR ranks 2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge [??]. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster.",
    "code_link": ""
  },
  "cvpr2018_w13_newtechniquesforpreservingglobalstructureanddenoisingwithlowinformationlossinsingle-imagesuper-resolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "New Techniques for Preserving Global Structure and Denoising With Low Information Loss in Single-Image Super-Resolution",
    "authors": [
      "Yijie Bei",
      "Alexandru Damian",
      "Shijia Hu",
      "Sachit Menon",
      "Nikhil Ravi",
      "Cynthia Rudin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Bei_New_Techniques_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Bei_New_Techniques_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This work identifies and addresses two important technical challenges in single-image super-resolution: (1) how to upsample an image without magnifying noise and (2) how to preserve large scale structure when upsampling. We summarize the techniques we developed for our second place entry in Track 1 (Bicubic Downsampling), seventh place entry in Track 2 (Realistic Adverse Conditions), and seventh place entry in Track 3 (Realistic difficult) in the 2018 NTIRE Super-Resolution Challenge. Furthermore, we present new neural network architectures that specifically address the two challenges listed above: denoising and preservation of large-scale structure.",
    "code_link": "https://github.com/nikhilvravi/DukeSR"
  },
  "cvpr2018_w13_efficientmodulebasedsingleimagesuperresolutionformultipleproblems": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Efficient Module Based Single Image Super Resolution for Multiple Problems",
    "authors": [
      "Dongwon Park",
      "Kwanyoung Kim",
      "Se Young Chun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Park_Efficient_Module_Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Park_Efficient_Module_Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Example based single image super resolution (SR) is a fundamental task in computer vision. It is challenging, but recently, there have been significant performance improvements using deep learning approaches. In this article, we propose efficient module based single image SR networks (EMBSR) and tackle multiple SR problems in NTIRE 2018 SR challenge by recycling trained networks. Our proposed EMBSR allowed us to reduce training time with effectively deeper networks, to use modular ensemble for improved performance, and to separate subproblems for better performance. We also proposed EDSR-PP, an improved version of previous ESDR by incorporating pyramid pooling so that global as well as local context information can be utilized. Lastly, we proposed a novel denoising / deblurring residual convolutional network (DnResNet) using residual block and batch normalization. Our proposed EMBSR with DnResNet and EDSR-PP demonstrated that multiple SR problems can be tackled efficiently and effectively by winning the 2nd place for Track 2 (x4 SR with mild adverse condition) and the 3rd place for Track 3 (x4 SR with diffi- cult adverse condition). Our proposed method with EDSR-PP also achieved the ninth place for Track 1 (x8 SR) with the fastest run time among top nine teams.",
    "code_link": ""
  },
  "cvpr2018_w13_ntire2018challengeonimagedehazingmethodsandresults": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2018 Challenge on Image Dehazing: Methods and Results",
    "authors": [
      "Cosmin Ancuti",
      "Codruta O. Ancuti",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Ancuti_NTIRE_2018_Challenge_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Ancuti_NTIRE_2018_Challenge_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper reviews the first challenge on image dehazing (restoration of rich details in hazy image) with focus on proposed solutions and results. The challenge had 2 tracks. Track 1 employed the indoor images (using I-HAZE dataset), while Track 2 outdoor images (using O-HAZE dataset). The hazy images have been captured in presence of real haze, generated by professional haze machines. I-HAZE dataset contains 35 scenes that correspond to indoor domestic environments, with objects with different colors and specularities. O-HAZE contains 45 different outdoor scenes depicting the same visual content recorded in haze-free and hazy conditions, under the same illumination parameters. The dehazing process was learnable through provided pairs of haze-free and hazy train images. Each track had120 registered participants and 21 teams competed in the final testing phase. They gauge the state-of-the-art in image dehazing.",
    "code_link": ""
  },
  "cvpr2018_w13_multi-scalesingleimagedehazingusingperceptualpyramiddeepnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Scale Single Image Dehazing Using Perceptual Pyramid Deep Network",
    "authors": [
      "He Zhang",
      "Vishwanath Sindagi",
      "Vishal M. Patel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Zhang_Multi-Scale_Single_Image_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Zhang_Multi-Scale_Single_Image_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Haze adversely degrades quality of an image thereby affecting its aesthetic appeal and visibility in outdoor scenes. Single image dehazing is particularly challenging due to its ill-posed nature. Most existing work, including therecent convolutional neural network (CNN) based methods, rely on the classical mathematical formulation where the hazy image is modeled as the superposition of attenuated scene radiance and the atmospheric light. In this work, we explore CNNs to directly learn a non-linear function between hazy images and the corresponding clear images.We present a multi-scale image dehazing method using Perceptual Pyramid Deep Network based on the recently popular dense blocks and residual blocks. The proposed method involves an encoder-decoder structure with a pyramid pooling module in the decoder to incorporate contextual information of the scene while decoding. The network is learned by minimizing the mean squared error and perceptual losses. Multi-scale patches are used during training and inference process to further improve the performance. Experiments on the recently released NTIRE2018-Dehazing dataset demonstrates the superior performance of the proposed method over recent state-of-the-art approaches. Additionally, the proposed method is ranked among top-3 methods in terms of quantitative performance in the recently conducted NTIRE2018-Dehazing challenge.",
    "code_link": "https://github.com/hezhangsprinter/NTIRE2018-Dehazing-Challenge"
  },
  "cvpr2018_w13_high-resolutionimagedehazingwithrespecttotraininglossesandreceptivefieldsizes": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "High-Resolution Image Dehazing With Respect to Training Losses and Receptive Field Sizes",
    "authors": [
      "Hyeonjun Sim",
      "Sehwan Ki",
      "Jae-Seok Choi",
      "Soomin Seo",
      "Saehun Kim",
      "Munchurl Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Sim_High-Resolution_Image_Dehazing_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Sim_High-Resolution_Image_Dehazing_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Haze removal is one of the essential image enhancement processes that makes degraded images visually pleasing. Since haze in images often appears differently depending on the surroundings, haze removal requires the use of spatial information to effectively remove the haze according to the types of image region characteristics. However, in the conventional training, the small-sized training image patches could not provide spatial information to the training networks when they are relatively very small compared to the original training image resolutions. In this paper, we propose a simple but effective network for high-resolution image dehazing using a conditional generative adversarial network (CGAN), which is called DeHazing GAN (DHGAN), where the hazy patches of scale-reduced training input images are applied to the generator network of DHGAN. By doing so, DHGAN can capture more global features of the haziness in the training image patches, thus leading to improved dehazing performance. Also, DHGAN is trained based on the largest binary cross entropy loss among the multiple outputs so that the generator network of DHGAN can favorably be trained in accordance with perceptual quality. From extensive training and test, our proposed DHGAN was ranked in the second place for the NTIRE2018 Image Dehazing Challenge Track2: Outdoor.",
    "code_link": ""
  },
  "cvpr2018_w13_imagedehazingbyjointestimationoftransmittanceandairlightusingbi-directionalconsistencylossminimizedfcn": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Image Dehazing by Joint Estimation of Transmittance and Airlight Using Bi-Directional Consistency Loss Minimized FCN",
    "authors": [
      "Ranjan Mondal",
      "Sanchayan Santra",
      "Bhabatosh Chanda"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Mondal_Image_Dehazing_by_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Mondal_Image_Dehazing_by_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Very few of the existing image dehazing methods have laid stress on the accurate restoration of color from hazy images, although it is crucial for proper removal of haze. In this paper, we are proposing a Fully Convolutional Neural Network (FCN) based image dehazing method. We have designed a network that jointly estimates scene transmittance and airlight. The network is trained using a custom designed loss, called bi-directional consistency loss, that tries to minimize the error to reconstruct the hazy image from clear image and the clear image from hazy image. Apart from that, to minimize the dependence of the network on the scale of the training data, we have proposed to do both the training and inference in multiple levels. Quantitative and qualitative evaluations show, that the method works comparably with state-of-the-art image dehazing methods.",
    "code_link": ""
  },
  "cvpr2018_w13_ntire2018challengeonspectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2018 Challenge on Spectral Reconstruction From RGB Images",
    "authors": [
      "Boaz Arad",
      "Ohad Ben-Shahar",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Arad_NTIRE_2018_Challenge_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Arad_NTIRE_2018_Challenge_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper reviews the first challenge on spectral image reconstruction from RGB images, i.e., the recovery of whole-scene hyperspectral (HS) information from a 3-channel RGB image.The challenge was divided into 2 tracks: the \"Clean\" track sought HS recovery from noiseless RGB images obtained from a known response function (representing spectrally-calibrated camera) while the \"Real World\" track challenged participants to recover HS cubes from JPEG-compressed RGB images generated by an unknown response function. To facilitate the challenge, the BGU Hyperspectral Image Database was extended to provide participants with 256 natural HS training images, and 5+10 additional images for validation and testing, respectively.The \"Clean\" and \"Real World\" tracks had 73 and 63 registered participants respectively, with 12 teams competing in the final testing phase. Proposed methods and their corresponding results are reported in this review.",
    "code_link": ""
  },
  "cvpr2018_w13_hscnn+advancedcnn-basedhyperspectralrecoveryfromrgbimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "HSCNN+: Advanced CNN-Based Hyperspectral Recovery From RGB Images",
    "authors": [
      "Zhan Shi",
      "Chang Chen",
      "Zhiwei Xiong",
      "Dong Liu",
      "Feng Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Shi_HSCNN_Advanced_CNN-Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Shi_HSCNN_Advanced_CNN-Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Hyperspectral recovery from a single RGB image has seen a great improvement with the development of deep convolutional neural networks (CNNs). In this paper, we propose two advanced CNNs for the hyperspectral reconstruction task, collectively called HSCNN+. We first develop a deep residual network named HSCNN-R, which comprises a number of residual blocks. The superior performance of this model comes from the modern architecture and optimization by removing the hand-crafted upsampling in HSCNN. Based on the promising results of HSCNN-R, we propose another distinct architecture that replaces the residual block by the dense block with a novel fusion scheme, leading to a new network named HSCNN-D. This model substantially deepens the network structure for a more accurate solution. Experimental results demonstrate that our proposed models significantly advance the state-of-the-art. In the NTIRE 2018 Spectral Reconstruction Challenge, our entries rank the 1st (HSCNN-D) and 2nd (HSCNN-R) places on both the \"Clean\" and \"Real World\" tracks.",
    "code_link": ""
  },
  "cvpr2018_w13_reconstructingspectralimagesfromrgb-imagesusingaconvolutionalneuralnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Reconstructing Spectral Images From RGB-Images Using a Convolutional Neural Network",
    "authors": [
      "Tarek Stiebel",
      "Simon Koppers",
      "Philipp Seltsam",
      "Dorit Merhof"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w13/html/Stiebel_Reconstructing_Spectral_Images_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w13/Stiebel_Reconstructing_Spectral_Images_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recovering high-dimensional spectral images taken with spectrally low-dimensional camera systems, in the extreme case RGB-images, has been of great interest for a variety of applications. An accurate spectral reconstruction is typically required to either achieve a better color accuracy or to improve object recognition/classification tasks. Almost all published work to date aims at performing a mapping from individual camera signals towards the corresponding spectrum. However, it might be beneficial to consider not only single pixels, but also contextual information. Here, we propose a convolutional neural network architecture that learns a mapping from RGB- to spectral images. We trained the network on the largest hyper-spectral data set available to date and analyzed the influence of different error metrics as loss functions. An objective evaluation of the performance in comparison to state of the art spectral reconstruction techniques is given by participating in the NTIRE 2018 challenge on spectral reconstruction.",
    "code_link": ""
  },
  "cvpr2018_w14_theapolloscapedatasetforautonomousdriving": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "The ApolloScape Dataset for Autonomous Driving",
    "authors": [
      "Xinyu Huang",
      "Xinjing Cheng",
      "Qichuan Geng",
      "Binbin Cao",
      "Dingfu Zhou",
      "Peng Wang",
      "Yuanqing Lin",
      "Ruigang Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Huang_The_ApolloScape_Dataset_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Huang_The_ApolloScape_Dataset_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Scene parsing aims to assign a class (semantic) label for each pixel in an image. It is a comprehensive analysis of an image. Given the rise of autonomous driving, pixel-accurate environmental perception is expected to be a key enabling technical piece. However, providing a large scale dataset for the design and evaluation of scene parsing algorithms, in particular for outdoor scenes, has been difficult. The per-pixel labelling process is prohibitively expensive, limiting the scale of existing ones. In this paper, we present a large-scale open dataset, ApolloScape, that consists of RGB videos and corresponding dense 3D point clouds. Comparing with existing datasets, our dataset has the following unique properties. The first is its scale, our initial release contains over 140K images - each with its per-pixel semantic mask, up to 1M is scheduled. The second is its complexity. Captured in various traffic conditions, the number of moving objects averages from tens to over one hundred. And the third is the 3D attribute, each image is tagged with high-accuracy pose information at cm accuracy and the static background point cloud has mm relative accuracy. We are able to label these many images by an interactive and efficient labelling pipeline that utilizes the high-quality 3D point cloud. Moreover, our dataset also contains different lane markings based on the lane colors and styles. We expect our new dataset can deeply benefit various autonomous driving related applications that include but not limited to 2D/3D scene understanding, localization, transfer learning, and driving simulation.",
    "code_link": ""
  },
  "cvpr2018_w14_sceneunderstandingnetworksforautonomousdrivingbasedonaroundviewmonitoringsystem": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Scene Understanding Networks for Autonomous Driving Based on Around View Monitoring System",
    "authors": [
      "Jeong Yeol Baek",
      "Ioana Veronica Chelu",
      "Livia Iordache",
      "Vlad Paunescu",
      "HyunJoo Ryu",
      "Alexandru Ghiuta",
      "Andrei Petreanu",
      "YunSung Soh",
      "Andrei Leica",
      "ByeongMoon Jeon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Baek_Scene_Understanding_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Baek_Scene_Understanding_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Modern driver assistance systems rely on a wide range of sensors (RADAR, LIDAR, ultrasound and cameras) for scene understanding and prediction. These sensors are typically used for detecting traffic participants and scene elements required for navigation. In this paper we argue that relying on camera based systems, specifically Around View Monitoring (AVM) system has great potential to achieve these goals in both parking and driving modes with decreased costs. The contributions of this paper are as follows: we present a new end-to-end solution for delimiting the safe drivable area for a whole frame by means of identifying the closest obstacle in each direction from a driving vehicle, we use this approach to calculate the distance to the nearest obstacles and we incorporate this bottom obstacle prediction into a unified end-to-end architecture capable of joint object detection, curb detection and safe drivable area detection. Furthermore, we describe the family of networks for both a high accuracy solution and a low complexity solution. We also introduce further augmentation of the base architecture with 3D object detection.",
    "code_link": ""
  },
  "cvpr2018_w14_trainingdeepnetworkswithsyntheticdatabridgingtherealitygapbydomainrandomization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Training Deep Networks With Synthetic Data: Bridging the Reality Gap by Domain Randomization",
    "authors": [
      "Jonathan Tremblay",
      "Aayush Prakash",
      "David Acuna",
      "Mark Brophy",
      "Varun Jampani",
      "Cem Anil",
      "Thang To",
      "Eric Cameracci",
      "Shaad Boochoon",
      "Stan Birchfield"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Tremblay_Training_Deep_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Tremblay_Training_Deep_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator - such as lighting, pose, object textures, etc. - are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds - both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.",
    "code_link": ""
  },
  "cvpr2018_w14_ontheiterativerefinementofdenselyconnectedrepresentationlevelsforsemanticsegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "On the Iterative Refinement of Densely Connected Representation Levels for Semantic Segmentation",
    "authors": [
      "Arantxa Casanova",
      "Guillem Cucurull",
      "Michal Drozdzal",
      "Adriana Romero",
      "Yoshua Bengio"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Casanova_On_the_Iterative_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Casanova_On_the_Iterative_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " State-of-the-art semantic segmentation approaches increase the receptive field of their models by using either a downsampling path composed of poolings/strided convolutions or successive dilated convolutions. However, it is not clear which operation leads to best results. In this paper, we systematically study the differences introduced by distinct receptive field enlargement methods and their impact on the performance of a novel architecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a densely connected backbone composed of residual networks. Following standard image segmentation architectures, receptive field enlargement operations that change the representation level are interleaved among residual networks. This allows the model to exploit the benefits of both residual and dense connectivity patterns, namely: gradient flow, iterative refinement of representations, multi-scale feature combination and deep supervision. In order to highlight the potential of our model, we test it on the challenging CamVid urban scene understanding benchmark and make the following observations: 1) downsampling operations outperform dilations when the model is trained from scratch, 2) dilations are useful during the finetuning step of the model, 3) coarser representations require less refinement steps, and 4) ResNets (by model construction) are good regularizers, since they can reduce the model capacity when needed. Finally, we compare our architecture to alternative methods and report state-of-the-art result on the Camvid dataset, with at least twice fewer parameters.",
    "code_link": ""
  },
  "cvpr2018_w14_minimizingsupervisionforfree-spacesegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Minimizing Supervision for Free-Space Segmentation",
    "authors": [
      "Satoshi Tsutsui",
      "Tommi Kerola",
      "Shunta Saito",
      "David J. Crandall"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Tsutsui_Minimizing_Supervision_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Tsutsui_Minimizing_Supervision_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Identifying \"free-space,\" or safely driveable regions in the scene ahead, is a fundamental task for autonomous navigation. While this task can be addressed using semantic segmentation, the manual labor involved in creating pixel-wise annotations to train the segmentation model is very costly. Although weakly supervised segmentation addresses this issue, most methods are not designed for free-space. In this paper, we observe that homogeneous texture and location are two key characteristics of free-space, and develop a novel, practical framework for free-space segmentation with minimal human supervision. Our experiments show that our framework performs better than other weakly supervised methods while using less supervision. Our work demonstrates the potential for performing free-space segmentation without tedious and costly manual annotation, which will be important for adapting autonomous driving systems to different types of vehicles and environments.",
    "code_link": ""
  },
  "cvpr2018_w14_errorcorrectionfordensesemanticimagelabeling": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Error Correction for Dense Semantic Image Labeling",
    "authors": [
      "Yu-Hui Huang",
      "Xu Jia",
      "Stamatios Georgoulis",
      "Tinne Tuytelaars",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Huang_Error_Correction_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Huang_Error_Correction_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Pixel-wise semantic image labeling is an important, yet challenging task with many applications. Especially in autonomous driving systems, it allows for a full understanding of the system's surroundings, which is crucial for trajectory planning. Typical approaches to tackle this problem involve either the training of deep networks on vast amounts of images to directly infer the labels or the use of probabilistic graphical models to jointly model the dependencies of the input (i.e. images) and output (i.e. labels). Yet, the former approaches do not capture the structure of the output labels, which is crucial for the performance of dense labeling, and the latter rely on carefully hand-designed priors that require costly parameter tuning via optimization techniques, which in turn leads to long inference times. To alleviate these restrictions, we explore how to arrive at dense semantic pixel labels given both the input image and an initial estimate of the output labels. We propose a parallel architecture that: 1) exploits the context information through a LabelPropagation network to propagate correct labels from nearby pixels to improve the object boundaries, 2) uses a LabelReplacement network to directly replace possibly erroneous, initial labels with new ones, and 3) combines the different intermediate results via a Fusion network to obtain the final per-pixel label. We experimentally validate our approach on two different datasets for semantic segmentation, where we show improvements over the state-of-the-art. We also provide both a quantitative and qualitative analysis of the generated results.",
    "code_link": ""
  },
  "cvpr2018_w14_ontheimportanceofstereoforaccuratedepthestimationanefficientsemi-superviseddeepneuralnetworkapproach": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach",
    "authors": [
      "Nikolai Smolyanskiy",
      "Alexey Kamenev",
      "Stan Birchfield"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Smolyanskiy_On_the_Importance_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Smolyanskiy_On_the_Importance_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We revisit the problem of visual depth estimation in the context of autonomous vehicles. Despite the progress on monocular depth estimation in recent years, we show that the gap between monocular and stereo depth accuracy remains large---a particularly relevant result due to the prevalent reliance upon monocular cameras by vehicles that are expected to be self-driving.We argue that the challenges of removing this gap are significant, owing to fundamental limitations of monocular vision.As a result, we focus our efforts on depth estimation by stereo.We propose a novel semi-supervised learning approach to training a deep stereo neural network, along with a novel architecture containing a machine-learned argmax layer and a custom runtime that enables a smaller version of our stereo DNN to run on an embedded GPU.Competitive results are shown on the KITTI 2015 stereo dataset.We also evaluate the recent progress of stereo algorithms by measuring the impact upon accuracy of various design criteria.",
    "code_link": ""
  },
  "cvpr2018_w14_accuratedeepdirectgeo-localizationfromgroundimageryandphone-gradegps": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Accurate Deep Direct Geo-Localization From Ground Imagery and Phone-Grade GPS",
    "authors": [
      "Shaohui Sun",
      "Ramesh Sarukkai",
      "Jack Kwok",
      "Vinay Shet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Sun_Accurate_Deep_Direct_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Sun_Accurate_Deep_Direct_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " One of the most critical topics in autonomous driving or ride-sharing technology is to accurately localize vehicles in the world frame. In addition to common multi-view camera systems, it usually also relies on industrial grade sensors, such as LiDAR, differential GPS, high precision IMU, and etc. In this paper, we develop an approach to provide an effective solution to this problem. We propose a method to train a geo-spatial deep neural network (CNN+LSTM) to predict accurate geo-locations (latitude and longitude) using only ordinary ground imagery and low accuracy phone-grade GPS. We evaluate our approach on the open dataset released during ACM Multimedia 2017 Grand Challenge.Having ground truth locations for training, we are able to reach nearly lane-level accuracy. We also evaluate the proposed method on our own collected images in San Francisco downtown area often described as \"downtown canyon\" where consumer GPS signals are extremely inaccurate. The results show the model can predict quality locations that suffice in real business applications, such as ride-sharing, only using phone-grade GPS. Unlike classic visual localization or recent PoseNet-like methods that may work well in indoor environments or small-scale outdoor environments, we avoid using a map or an SFM (structure-from-motion) model at all. More importantly, the proposed method can be scaled up without concerns over the potential failure of 3D reconstruction.",
    "code_link": ""
  },
  "cvpr2018_w14_efficientandsafevehiclenavigationbasedondriverbehaviorclassification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Efficient and Safe Vehicle Navigation Based on Driver Behavior Classification",
    "authors": [
      "Ernest Cheung",
      "Aniket Bera",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Cheung_Efficient_and_Safe_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Cheung_Efficient_and_Safe_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present an autonomous driving planning algorithm that takes into account neighboring drivers' behaviors and achieves safer and more efficient navigation.Our approach leverages the advantages of a data-driven mapping that is used to characterize the behavior of other drivers on the road.Our formulation also takes into account pedestrians and cyclists and uses psychology-based models to perform safe navigation.We demonstrate our benefits over previous methods: safer behavior in avoiding dangerous neighboring drivers, pedestrians and cyclists, and efficient navigation around careful drivers.",
    "code_link": ""
  },
  "cvpr2018_w14_detectionofdistracteddriverusingconvolutionalneuralnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Detection of Distracted Driver Using Convolutional Neural Network",
    "authors": [
      "Bhakti Baheti",
      "Suhas Gajre",
      "Sanjay Talbar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Baheti_Detection_of_Distracted_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Baheti_Detection_of_Distracted_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Number of road accidents is continuously increasing in last few years worldwide. As per the survey of National Highway Traffic Safety Administrator, nearly one in five motor vehicle crashes are caused by distracted driver. We attempt to develop an accurate and robust system for detecting distracted driver and warn him against it. Motivated by the performance of Convolutional Neural Networks in computer vision, we present a CNN based system that not only detects the distracted driver but also identifies the cause of distraction. VGG-16 architecture is modified for this particular task and various regularization techniques are implied in order to improve the performance. Experimental results show that our system outperforms earlier methods in literature achieving an accuracy of 96.31% and processes 42 images per second on GPU. We also study the effect of dropout, L2 regularization and batch normalisation on the performance of the system. Next, we present a modified version of our architecture that achieves 95.54% classification accuracy with the number of parameters reduced from 140M in original VGG-16 to 15M only.",
    "code_link": ""
  },
  "cvpr2018_w14_classifyinggroupemotionsforsocially-awareautonomousvehiclenavigation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Classifying Group Emotions for Socially-Aware Autonomous Vehicle Navigation",
    "authors": [
      "Aniket Bera",
      "Tanmay Randhavane",
      "Austin Wang",
      "Dinesh Manocha",
      "Emily Kubin",
      "Kurt Gray"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Bera_Classifying_Group_Emotions_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Bera_Classifying_Group_Emotions_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a real-time, data-driven algorithm to enhance the social-invisibility of autonomous robot navigation within crowds. Our approach is based on prior psychological research, which reveals that people notice and--importantly--react negatively to groups of social actors when they have negative group emotions or entitativity, moving in a tight group with similar appearances and trajectories. In order to evaluate that behavior, we performed a user study to develop navigational algorithms that minimize emotional reactions. This study establishes a mapping between emotional reactions and multi-robot trajectories and appearances and further generalizes the finding across various environmental conditions. We demonstrate the applicability of our approach for trajectory computation for active navigation and dynamic intervention in simulated autonomous robot-human interaction scenarios. Our approach empirically shows that various levels of emotional autonomous robots can be used to both avoid and influence pedestrians while not eliciting strong emotional reactions, giving multi-robot systems socially-invisibility.",
    "code_link": ""
  },
  "cvpr2018_w14_autonovi-simautonomousvehiclesimulationplatformwithweather,sensing,andtrafficcontrol": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "AutonoVi-Sim: Autonomous Vehicle Simulation Platform With Weather, Sensing, and Traffic Control",
    "authors": [
      "Andrew Best",
      "Sahil Narang",
      "Lucas Pasqualin",
      "Daniel Barber",
      "Dinesh Manocha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Best_AutonoVi-Sim_Autonomous_Vehicle_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Best_AutonoVi-Sim_Autonomous_Vehicle_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present AutonoVi-Sim, a novel high-fidelity simulation platform for autonomous driving data generation and driving strategy testing. AutonoVi-Sim is a collection of high-level extensible modules which allows the rapid development and testing of vehicle configurations and facilitates construction of complex traffic scenarios.Autonovi-Sim supports multiple vehicles with unique steering or acceleration limits, as well as unique tire parameters and dynamics profiles.Engineers can specify the specific vehicle sensor systems and vary time of day and weather conditions to generate robust data and gain insight into how conditions affect the performance of a particular algorithm. In addition, AutonoVi-Sim supports navigation for non-vehicle traffic participants such as cyclists and pedestrians, allowing engineers to specify routes for these actors, or to create scripted scenarios which place the vehicle in dangerous reactive situations.Autonovi-Sim facilitates training of deep-learning algorithms by enabling data export from the vehicle's sensors, including camera data, LIDAR, relative positions of traffic participants, and detection and classification results.Thus, AutonoVi-Sim allows for the rapid prototyping, development and testing of autonomous driving algorithms under varying vehicle, road, traffic, and weather conditions.In this paper, we detail the simulator and provide specific performance and data benchmarks.",
    "code_link": ""
  },
  "cvpr2018_w14_learninghierarchicalmodelsforclass-specificreconstructionfromnaturaldata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Learning Hierarchical Models for Class-Specific Reconstruction From Natural Data",
    "authors": [
      "Arun CS Kumar",
      "Suchendra M. Bhandarkar",
      "Mukta Prasad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Kumar_Learning_Hierarchical_Models_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Kumar_Learning_Hierarchical_Models_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose a novel method for class-specific, single-view, object detection, pose estimation and deformable 3D reconstruction, wherea two-pronged(sparse semantic and dense shape) representation is learnedfrom natural image data automatically. Then, given a new image, it can estimate camera pose and deformable reconstruction using an effective, incremental optimization. Our method extracts a continuous, scaled-orthographic pose (without resorting to regression and/or discretized 1D azimuth-based representations). The method reconstructs a full free-form shape (rather than retrieving the closest 3D CAD shape proxy, typical in state-of-the-art). We learn our two-pronged model purely from natural image data, as automatically and faithfully as possible, reducing the human effort and bias typical to this problem. The pipeline combines data-driven deep learning based semantic part learning with principled modelling and effective optimization of the problem's physics, shape deformation, pose and occlusion. The underlying sparse (part-based) representation of the object is computationally efficient for purposes like detection and discriminative tasks, whereas the overlaid dense (skin like) representation, models and realistically renders comprehensive 3D structure including natural deformation, occlusion. The results for the car class are visually pleasing, and importantly, outperform the state-of-the-art quantitatively too. Our contribution to visual scene understanding through the two-pronged object representation shows promise for more accurate 3D scene understanding for real world applications on virtual/mixed reality, autonomous navigation, to cite a few.",
    "code_link": ""
  },
  "cvpr2018_w14_subsetreplaybasedcontinuallearningforscalableimprovementofautonomoussystems": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Autonomous Driving",
    "title": "Subset Replay Based Continual Learning for Scalable Improvement of Autonomous Systems",
    "authors": [
      "Pratik Prabhanjan Brahma",
      "Adrienne Othon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w14/html/Brahma_Subset_Replay_Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w14/Brahma_Subset_Replay_Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " While machine learning techniques have come a long way in showing astounding performance on various vision problems, the conventional way of training is not applicable for learning from a sequence of new data or tasks. For most real life applications like perception for autonomous vehicles, multiple stages of data collection are necessary to improve the performance of machine learning models over time. The newer observations may have a different distribution than the older ones and thus a simply fine-tuned model often overfits while forgetting the knowledge from past experiences. Recently, few lifelong or continual learning approaches have shown promising results towards overcoming this problem of catastrophic forgetting. In this work, we show that carefully choosing a small subset of the older data with the objective of promoting representativeness and diversity can also help in learning continuously. For large scale cloud based training, this can help in significantly reducing the amount of storage required along with lessening the computation and time for each retraining session.",
    "code_link": ""
  },
  "cvpr2018_w17_monocularrgbhandposeinferencefromunsupervisedrefinablenets": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Human Pose, Motion, Activities and Shape in 3D",
    "title": "Monocular RGB Hand Pose Inference From Unsupervised Refinable Nets",
    "authors": [
      "Endri Dibra",
      "Silvan Melchior",
      "Ali Balkis",
      "Thomas Wolf",
      "Cengiz Oztireli",
      "Markus Gross"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w17/html/Dibra_Monocular_RGB_Hand_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w17/Dibra_Monocular_RGB_Hand_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " 3D hand pose inference from monocular RGB data is a challenging problem. CNN-based approaches have shown great promise in tackling this problem. However, such approaches are data-hungry, and obtaining real labeled training hand data is very hard. To overcome this, in this work, we propose a new, large, realistically rendered hand dataset and a neural network trained on it, with the ability to refine itself unsupervised on real unlabeled RGB images, given corresponding depth images. We benchmark and validate our method on existing and captured datasets, demonstrating that we strongly compare to or outperform state-of-the-art methods for various tasks ranging from 3D pose estimation to hand gesture recognition.",
    "code_link": ""
  },
  "cvpr2018_w17_unsupervisedfeaturesforfacialexpressionintensityestimationovertime": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Human Pose, Motion, Activities and Shape in 3D",
    "title": "Unsupervised Features for Facial Expression Intensity Estimation Over Time",
    "authors": [
      "Maren Awiszus",
      "Stella Grasshof",
      "Felix Kuhnke",
      "Jorn Ostermann"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w17/html/Awiszus_Unsupervised_Features_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w17/Awiszus_Unsupervised_Features_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The diversity of facial shapes and motions among persons is one of the greatest challenges for automatic analysis of facial expressions. In this paper, we propose a feature describing expression intensity over time, while being invariant to person and the type of performed expression. Our feature is a weighted combination of the dynamics of multiple points adapted to the overall expression trajectory. We evaluate our method on several tasks all related to temporal analysis of facial expression. The proposed feature is compared to a state-of-the-art method for expression intensity estimation, which it outperforms. We use our proposed feature to temporally align multiple sequences of recorded 3D facial expressions. Furthermore, we show how our feature can be used to reveal person-specific differences in performances of facial expressions. Additionally, we apply our feature to identify the local changes in face video sequences based on action unit labels. For all the experiments our feature proves to be robust against noise and outliers, making it applicable to a variety of applications for analysis of facial movements.",
    "code_link": ""
  },
  "cvpr2018_w17_deeplearningwholebodypointcloudscansfromasingledepthmap": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Human Pose, Motion, Activities and Shape in 3D",
    "title": "Deep Learning Whole Body Point Cloud Scans From a Single Depth Map",
    "authors": [
      "Nolan Lunscher",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w17/html/Lunscher_Deep_Learning_Whole_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w17/Lunscher_Deep_Learning_Whole_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Personalized knowledge about body shape has numerous applications in fashion and clothing, as well as in health monitoring. Whole body 3D scanning presents a relatively simple mechanism for individuals to obtain this information about themselves without needing much knowledge of anthropometry. With current implementations however, scanning devices are large, complex and expensive. In order to make such systems as accessible and widespread as possible, it is necessary to simplify the process and reduce their hardware requirements. Deep learning models have emerged as the leading method of tackling visual tasks, including various aspects of 3D reconstruction. In this paper we demonstrate that by leveraging deep learning it is possible to create very simple whole body scanners that only require a single input depth map to operate. We show that our presented model is able to produce whole body point clouds with an accuracy of 5.19 mm.",
    "code_link": ""
  },
  "cvpr2018_w17_handynetaone-stopsolutiontodetect,segment,localize&analyzedriverhands": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W17",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Human Pose, Motion, Activities and Shape in 3D",
    "title": "HandyNet: A One-Stop Solution to Detect, Segment, Localize & Analyze Driver Hands",
    "authors": [
      "Akshay Rangesh",
      "Mohan M. Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w17/html/Rangesh_HandyNet_A_One-Stop_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w17/Rangesh_HandyNet_A_One-Stop_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Tasks related to human hands have long been part of the computer vision community. Hands being the primary actuators for humans, convey a lot about activities and intents, in addition to being an alternative form of communication/interaction with other humans and machines. In this study, we focus on training a single feedforward convolutional neural network (CNN) capable of executing many hand related tasks that may be of use in autonomous and semi-autonomous vehicles of the future. The resulting network, which we refer to as HandyNet, is capable of detecting, segmenting and localizing (in 3D) driver hands inside a vehicle cabin. The network is additionally trained to identify handheld objects that the driver may be interacting with. To meet the data requirements to train such a network, we propose a method for cheap annotation based on chroma-keying, thereby bypassing weeks of human effort required to label such data. This process can generate thousands of labeled training samples in an efficient manner, and may be replicated in new environments with relative ease.",
    "code_link": ""
  },
  "cvpr2018_w19_temporalreasoninginvideosusingconvolutionalgatedrecurrentunits": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Brave New Ideas for Video Understanding",
    "title": "Temporal Reasoning in Videos Using Convolutional Gated Recurrent Units",
    "authors": [
      "Debidatta Dwibedi",
      "Pierre Sermanet",
      "Jonathan Tompson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w19/html/Dwibedi_Temporal_Reasoning_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w19/Dwibedi_Temporal_Reasoning_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recently, deep learning based models have pushed state-of-the-art performance for the task of action recognition in videos. Yet, for many action recognition datasets like Kinetics and UCF101, the correct temporal order of frames doesn't seem to be essential to solving the task. We find that the temporal order matters more for the recently introduced 20BN Something-Something dataset where the task of fine-grained action recognition necessitates the model to do temporal reasoning. We show that when temporal order matters, recurrent models can provide a significant boost in performance. Using qualitative methods, we show that when the task of action recognition requires temporal reasoning, the hidden states of the recurrent units encode meaningful state transitions.",
    "code_link": ""
  },
  "cvpr2018_w19_temporal3dconvnetsusingtemporaltransitionlayer": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Brave New Ideas for Video Understanding",
    "title": "Temporal 3D ConvNets Using Temporal Transition Layer",
    "authors": [
      "Ali Diba",
      "Mohsen Fayyaz",
      "Vivek Sharma",
      "A. Hossein Karami",
      "M. Mahdi Arzani",
      "Rahman Yousefzadeh",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w19/html/Diba_Temporal_3D_ConvNets_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w19/Diba_Temporal_3D_ConvNets_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification, and for human action recognition in particular? Thus far, the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths. We introduce a new temporal layer that models variable temporal convolution kernel depths. We embed this new temporal layer in our proposed 3D CNN. We extend the DenseNet architecture - which normally is 2D - with 3D filters and pooling kernels. We name our proposed video convolutional network: Temporal 3D ConvNet (T3D) and its new temporal layer Temporal Transition Layer (TTL). Our experiments show that T3D outperforms the current state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets.",
    "code_link": ""
  },
  "cvpr2018_w19_contextvpfullycontext-awarevideoprediction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Brave New Ideas for Video Understanding",
    "title": "ContextVP: Fully Context-Aware Video Prediction",
    "authors": [
      "Wonmin Byeon",
      "Qin Wang",
      "Rupesh Kumar Srivastava",
      "Petros Koumoutsakos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w19/html/Byeon_ContextVP_Fully_Context-Aware_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w19/Byeon_ContextVP_Fully_Context-Aware_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Video prediction models based on convolutional networks, recurrent networks, and their combinations often result in blurry predictions. We identify an important contributing factor for imprecise predictions that has not been studied adequately in the literature: blind spots, i.e., lack of access to all relevant past information for accurately predicting the future. To address this issue, we introduce a fully context-aware architecture that captures the entire available past context for each pixel using Parallel Multi-Dimensional LSTM units and aggregates it using blending units. Our model outperforms a strong baseline network of 20 recurrent convolutional layers and yields state-of-the-art performance for next step prediction. Moreover, it does so with fewer parameters than several recently proposed models, and does not rely on deep convolutional networks, multi-scale architectures, separation of background and foreground modeling, motion flow learning, or adversarial training. These results highlight that full awareness of past context is of crucial importance for video prediction.",
    "code_link": ""
  },
  "cvpr2018_w19_towardsanunequivocalrepresentationofactions": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Brave New Ideas for Video Understanding",
    "title": "Towards an Unequivocal Representation of Actions",
    "authors": [
      "Michael Wray",
      "Davide Moltisanti",
      "Dima Damen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w19/html/Wray_Towards_an_Unequivocal_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w19/Wray_Towards_an_Unequivocal_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This work introduces verb-only representations for actions and interactions; the problem of describing similar motions (e.g. `open door', `open cupboard'), and distinguish differing ones (e.g. `open door' vs `open bottle') using verb-only labels. Current approaches neglect legitimate semantic ambiguities and class overlaps between verbs (Fig. 1), relying on the objects to disambiguate interactions.We deviate from single-verb labels and introduce a mapping between observations and multiple verb labels -- in order to create an Unequivocal Representation of Actions. The new representation benefits from increased vocabulary and a soft assignment to an enriched space of verb labels. We learn these representations as multi-output regression, using a two-stream fusion CNN. The proposed approach outperforms conventional single-verb labels (also known as majority voting) on three egocentric datasets for both recognition and retrieval.",
    "code_link": ""
  },
  "cvpr2018_w19_unsuperviseddeeprepresentationsforlearningaudiencefacialbehaviors": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Brave New Ideas for Video Understanding",
    "title": "Unsupervised Deep Representations for Learning Audience Facial Behaviors",
    "authors": [
      "Suman Saha",
      "Rajitha Navarathna",
      "Leonhard Helminger",
      "Romann M. Weber"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w19/html/Saha_Unsupervised_Deep_Representations_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w19/Saha_Unsupervised_Deep_Representations_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we present an unsupervised learning approach for analyzing facial behavior based on a deep generative model combined with a convolutional neural network (CNN). We jointly train a variational auto-encoder (VAE) and a generative adversarial network (GAN) to learn a powerful latent representation from footage of audiences viewing feature-length movies. We show that the learned latent representation successfully encodes meaningful signatures of behaviors related to audience engagement (smiling & laughing) and disengagement (yawning). Our results provide a proof of concept for a more general methodology for annotating hard-to-label multimedia data featuring sparse examples of signals of interest.",
    "code_link": ""
  },
  "cvpr2018_w19_ihaveseenenoughateacherstudentnetworkforvideoclassificationusingfewerframes": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W19",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Brave New Ideas for Video Understanding",
    "title": "I Have Seen Enough: A Teacher Student Network for Video Classification Using Fewer Frames",
    "authors": [
      "Shweta Bhardwaj",
      "Mitesh M. Khapra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w19/html/Bhardwaj_I_Have_Seen_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w19/Bhardwaj_I_Have_Seen_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention. Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence. However, for longer videos this is very time consuming. In this paper, we focus on the task of video classification and aim to reduce the computational time by using the idea of distillation. Specifically, we first train a teacher network which looks at all the frames in a video and computes a representation for the video. We then train a student network whose objective is to process only a small fraction of the frames in the video and still produce a representation which is very close to the representation computed by the teacher network. This smaller student network involving fewer computations can then be employed at inference time for video classification. We experiment with the YouTube-8M dataset and show that the proposed student network can reduce the inference time by upto 30% with a very small drop in the performance.",
    "code_link": ""
  },
  "cvpr2018_w21_generatingvisiblespectrumimagesfromthermalinfrared": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Generating Visible Spectrum Images From Thermal Infrared",
    "authors": [
      "Amanda Berg",
      "Jorgen Ahlberg",
      "Michael Felsberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Berg_Generating_Visible_Spectrum_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Berg_Generating_Visible_Spectrum_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Transformation of thermal infrared (TIR) images into visual, i.e. perceptually realistic color (RGB) images, is a challenging problem. TIR cameras have the ability to see in scenarios where vision is severely impaired, for example in total darkness or fog, and they are commonly used, e.g., for surveillance and automotive applications. However, interpretation of TIR images is difficult, especially for untrained operators. Enhancing the TIR image display by transforming it into a plausible, visual, perceptually realistic RGB image presumably facilitates interpretation. Existing grayscale to RGB, so called, colorization methods cannot be applied to TIR images directly since those methods only estimate the chrominance and not the luminance.In the absence of conventional colorization methods, we propose two fully automatic TIR to visual color image transformation methods, a two-step and an integrated approach, based on Convolutional Neural Networks. The methods require neither pre- nor postprocessing, do not require any user input, and are robust to image pair misalignments. We show that the methods do indeed produce perceptually realistic results on publicly available data, which is assessed both qualitatively and quantitatively.",
    "code_link": ""
  },
  "cvpr2018_w21_ir2vienhancednightenvironmentalperceptionbyunsupervisedthermalimagetranslation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation",
    "authors": [
      "Shuo Liu",
      "Vijay John",
      "Erik Blasch",
      "Zheng Liu",
      "Ying Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Liu_IR2VI_Enhanced_Night_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Liu_IR2VI_Enhanced_Night_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Context enhancement is critical for night vision (NV) applications, especially for the dark night situation without any artificial lights. In this paper, we present the infrared-to-visual (IR2VI) algorithm, a novel unsupervised thermal-to-visible image translation framework based on generative adversarial networks (GANs). IR2VI is able to learn the intrinsic characteristics from VI images and integrate them into IR images. Since the existing unsupervised GAN-based image translation approaches face several challenges, such as incorrect mapping and lack of fine details, we propose a structure connection module and a region-of-interest (ROI) focal loss method to address the current limitations. Experimental results show the superiority of the IR2VI algorithm over baseline methods.",
    "code_link": ""
  },
  "cvpr2018_w21_pathorthogonalmatchingpursuitforsparsereconstructionanddenoisingofswirmaritimeimagery": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Path Orthogonal Matching Pursuit for Sparse Reconstruction and Denoising of SWIR Maritime Imagery",
    "authors": [
      "Timothy Doster",
      "Tegan Emerson",
      "Colin Olson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Doster_Path_Orthogonal_Matching_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Doster_Path_Orthogonal_Matching_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We introduce an extension that may be used to augment algorithms used for the sparse decomposition of signals into a linear combination of atoms drawn from a dictionary such as those used in support of, for example, compressive sensing, k-sparse representation, and denoising. Our augmentation may be applied to any reconstruction algorithm that relies on the selection and sorting of high-correlation atoms during an analysis or identification phase by generating a \"path\" between the two highest-correlation atoms. Here we investigate two types of path: a linear combination (Euclidean geodesic) and a construction relying on an optimal transport map (2-Wasserstein geodesic). We test our extension by performing image denoising and k-sparse representation using atoms from a learned overcomplete kSVD dictionary.We study the application of our techniques on SWIR imagery of maritime vessels and show that our methods outperform orthogonal matching pursuit. We conclude that these methods, having shown success in our two tested problem domains, will also be useful for reducing \"basis mismatch\" error that arises in the recovery of compressively sampled images.",
    "code_link": ""
  },
  "cvpr2018_w21_deeplearningbasedsingleimagedehazing": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Deep Learning Based Single Image Dehazing",
    "authors": [
      "Patricia L. Suarez",
      "Angel D. Sappa",
      "Boris X. Vintimilla",
      "Riad I. Hammoud"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Suarez_Deep_Learning_Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Suarez_Deep_Learning_Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper proposes a novel approach to remove haze degradations in RGB images using a stacked conditional Generative Adversarial Network (GAN). It employs a triplet of GAN to remove the haze on each color channel independently. A multiple loss functions scheme, applied over a conditional probabilistic model, is proposed. The proposed GAN architecture learns to remove the haze, using as conditioned entrance, the images with haze from which the clear images will be obtained. Such formulation ensures a fast model training convergence and a homogeneous model generalization. Experiments showed that the proposed method generates high-quality clear images.",
    "code_link": ""
  },
  "cvpr2018_w21_generativeadversarialnetworksfordepthmapestimationfromrgbvideo": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Generative Adversarial Networks for Depth Map Estimation From RGB Video",
    "authors": [
      "Kin Gwn Lore",
      "Kishore Reddy",
      "Michael Giering",
      "Edgar A. Bernal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Lore_Generative_Adversarial_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Lore_Generative_Adversarial_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Depth cues are essential to achieving high-level scene understanding, and in particular to determining geometric relations between objects. The ability to reason about depth information in scene analysis tasks can often result in improved decision-making capabilities. Unfortunately, depth-capable sensors are not as ubiquitous as traditional RGB cameras, which limits the availability of depth-related cues. In this work, we investigate data-driven approaches for depth estimation from images or videos captured with monocular cameras. We propose three different approaches and demonstrate their efficacy through extensive experimental validation.The proposed methods rely on processing of (i) a single 3-channel RGB image frame, (ii) a sequence of RGB frames, and (iii) a single RGB frame plus the optical flow field computed between the frame and a neighboring frame in the video stream, and map the respective inputs to an estimated depth map representation. In contrast to existing literature, the input-output mapping is not directly regressed; rather, it is learned through adversarial techniques that leverage conditional generative adversarial networks (cGANs).",
    "code_link": ""
  },
  "cvpr2018_w21_ontheimpactofparallaxfreecolourandinfraredimageco-registrationtofusedilluminationinvariantadaptivebackgroundmodelling": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "On the Impact of Parallax Free Colour and Infrared Image Co-Registration to Fused Illumination Invariant Adaptive Background Modelling",
    "authors": [
      "Michael Loveday",
      "Toby P. Breckon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Loveday_On_the_Impact_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Loveday_On_the_Impact_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Contrary to other visible-band (colour, RGB) and infrared-band (T) cross-modal work in the field, we present a practical approach to parallax-free RGB-T image formation using a combination of optical engineering (beam-splitter) and visual geometry. We use background to foreground object separation, a task inherently susceptible to multi-view parallax issues, to illustrate our approach. We evaluate the complementary nature of visible and far infrared (thermal, long-wave) information through three fusion schemes which physically combine visible-band (colour, RGB) and infrared-band (T) imagery into a co-registered, parallax free RGB-T image model. The performance of this combined RGB-T image model is assessed against standalone colour and thermal imagery for object detection within an adaptive background modelling framework. Illumination invariant background models, incorporating additional infrared information, increase the accuracy and precision of foreground object detection by over 10% on average when compared to standalone visible-band and over 5% for standalone infrared. Furthermore, the use of combined colour and infrared within adaptive background modelling provides superior results under conditions when either visible or infrared band performance is notably degraded. Evaluation is performed over a range of challenging conditions, over which the combined use of infrared and illumination invariant colour emerges as a more robust background modelling approach.",
    "code_link": ""
  },
  "cvpr2018_w21_integratedlearningandfeatureselectionfordeepneuralnetworksinmultispectralimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Integrated Learning and Feature Selection for Deep Neural Networks in Multispectral Images",
    "authors": [
      "Anthony Ortiz",
      "Alonso Granados",
      "Olac Fuentes",
      "Christopher Kiekintveld",
      "Dalton Rosario",
      "Zachary Bell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Ortiz_Integrated_Learning_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Ortiz_Integrated_Learning_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The curse of dimensionality is a well-known phenomenon that arises when applying machine learning algorithms to highly-dimensional data; it degrades performance as a function of increasing dimension. Due to the high data dimensionality of multispectral and hyperspectral imagery, classifiers trained on limited samples with many spectral bands tend to overfit, leading to weak generalization capability. In this work, we propose an end-to-end framework to effectively integrate input feature selection into the training procedure of a deep neural network for dimensionality reduction. We show that Integrated Learning and Feature Selection (ILFS) significantly improves performance on neural networks for multispectral imagery applications. We also evaluate the proposed methodology as a potential defense against adversarial examples, which are malicious inputs carefully designed to fool a machine learning system. Our experimental results show that methods for generating adversarial examples designed for RGB space are also effective for multispectral imagery and that ILFS significantly mitigates their effect.",
    "code_link": ""
  },
  "cvpr2018_w21_acomprehensivesolutionfordeep-learningbasedcargoinspectiontodiscriminategoodsincontainers": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Comprehensive Solution for Deep-Learning Based Cargo Inspection to Discriminate Goods in Containers",
    "authors": [
      "Jiahang Che",
      "Yuxiang Xing",
      "Li Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Che_A_Comprehensive_Solution_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Che_A_Comprehensive_Solution_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work, we attempt to classify commodities in containers with HS(harmonized system) codes, which is a challenging task due to the large number of categories in HS codes and its hierarchical structure based on a product's composition and economic activity. To tackle this problem, in this paper we propose an ensemble model which incorporates fine-grained image categorization, data analysis on cargo manifests, and human-in-the-loop paradigm. By employing deep learning, we train a triplet network for fine-grained image categorization. Then, by investigating massive information from cargo manifests, unreasonable predictions can be filtered out. With human-in-the-loop embedded, human intelligence is integrated to justify the resulted HS codes. Moreover, a HS code semantic tree is built to trade off specificity and accuracy.",
    "code_link": ""
  },
  "cvpr2018_w21_cross-domainhallucinationnetworkforfine-grainedobjectrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Cross-Domain Hallucination Network for Fine-Grained Object Recognition",
    "authors": [
      "Jin-Fu Lin",
      "Yen-Liang Lin",
      "Erh-Kan King",
      "Hung-Ting Su",
      "Winston H. Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Lin_Cross-Domain_Hallucination_Network_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Lin_Cross-Domain_Hallucination_Network_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Existing fine-grained object recognition methods often require high-resolution images to better discriminate the subordinate classes.However, this assumption does not always hold in current surveillance systems, where the distinguished parts may not be clearly presented.Besides, data insufficiency and class imbalance make the problem even more challenging. In this paper, we leverage high-resolution images collected from Internet to improve the vehicle recognition in the surveillance environments. A cross-domain hallucination network is proposed to minimize the domain discrepancy and enhance the quality of low-resolution surveillance images.To better align the cross-domain features and boost the recognition performance, we extend the original framework to part-based hallucination networks,where the parts are automatically extracted based on the maximum responses from the convolution filters.Whole and part-based hallucination networks are fused in a late fusion scheme to improve the final performance. We evaluate our method on a public surveillance vehicle dataset (BoxCars21k).Experimental results demonstrate that our approach outperforms the state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2018_w21_deepconvolutionalneuralnetworkswithintegratedquadraticcorrelationfiltersforautomatictargetrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Deep Convolutional Neural Networks With Integrated Quadratic Correlation Filters for Automatic Target Recognition",
    "authors": [
      "Brian Millikan",
      "Hassan Foroosh",
      "Qiyu Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Millikan_Deep_Convolutional_Neural_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Millikan_Deep_Convolutional_Neural_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Automatic target recognition involves detecting and recognizing potential targets automatically, which is widely used in civilian and military applications today. Quadratic correlation filters were introduced as two-class recognition classifiers for quickly detecting targets in cluttered scene environments. In this paper, we introduce two methods that integrate the discrimination capability of quadratic correlation filters with the multi-class recognition ability of multilayer neural networks. For mid-wave infrared imagery, the proposed methods aredemonstrated to be multi-class target recognition classifiers with very high accuracy.",
    "code_link": ""
  },
  "cvpr2018_w21_anonlineandflexiblemulti-objecttrackingframeworkusinglongshort-termmemory": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "An Online and Flexible Multi-Object Tracking Framework Using Long Short-Term Memory",
    "authors": [
      "Xingyu Wan",
      "Jinjun Wang",
      "Sanping Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Wan_An_Online_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Wan_An_Online_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The capacity to model temporal dependency by Recurrent Neural Networks (RNNs) makes it a plausible selection for the multi-object tracking (MOT) problem. Due to the non-linear transformations and the unique memory mechanism, Long Short-Term Memory (LSTM) can consider a window of history when learning discriminative features, which suggests that the LSTM is suitable for state estimation of target objects as they move around. This paper focuses on association based MOT, and we propose a novel Siamese LSTM Network to interpret both temporal and spatial components nonlinearly by learning the feature of trajectories, and outputs the similarity score of two trajectories for data association. In addition, we also introduce an online metric learning scheme to update the state estimation of each trajectory dynamically. Experimental evaluation on MOT16 benchmark shows that the proposed method achieves competitive performance compared with other state-of-the-art works.",
    "code_link": ""
  },
  "cvpr2018_w21_polarimetricsynthetic-aperture-radarchange-typeclassificationwithahyperparameter-freeopen-setclassifier": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "Polarimetric Synthetic-Aperture-Radar Change-Type Classification With a Hyperparameter-Free Open-Set Classifier",
    "authors": [
      "Mark W. Koch",
      "R. Derek West",
      "Robert Riley",
      "Tu-Thach Quach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Koch_Polarimetric_Synthetic-Aperture-Radar_Change-Type_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Koch_Polarimetric_Synthetic-Aperture-Radar_Change-Type_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Synthetic aperture radar (SAR) is a remote sensing technology that can truly operate 24/7. It's an all-weather system that can operate at any time except in the most extreme conditions. Coherent change detection (CCD) in SAR can identify minute changes such as vehicle tracks that occur between images taken at different times. From polarimetric SAR capabilities, researchers have developed decompositions that allow one to automatically classify the scattering type in a single polarimetric SAR (PolSAR) image set. We extend that work to CCD in PolSAR images to identify the type change. Such as change caused by no return regions, trees, or ground. This work could then be used as a preprocessor for algorithms to automatically detect tracks.",
    "code_link": ""
  },
  "cvpr2018_w21_pol-lwirvehicledetectionconvolutionalneuralnetworksmeetpolarisedinfraredsensors": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W21",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Perception Beyond the Visible Spectrum",
    "title": "POL-LWIR Vehicle Detection: Convolutional Neural Networks Meet Polarised Infrared Sensors",
    "authors": [
      "Marcel Sheeny",
      "Andrew Wallace",
      "Mehryar Emambakhsh",
      "Sen Wang",
      "Barry Connor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w21/html/Sheeny_POL-LWIR_Vehicle_Detection_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w21/Sheeny_POL-LWIR_Vehicle_Detection_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " For vehicle autonomy, driver assistance and situational awareness, it is necessary to operate at day and night, and in all weather conditions. In particular, long wave infrared (LWIR) sensors that receive predominantly emitted radiation have the capability to operate at night as well as during the day. In this work, we employ a polarised LWIR (POL-LWIR) camera to acquire data from a mobile vehicle, to compare and contrast four different convolutional neural network (CNN) configurations to detect other vehicles in video sequences. We evaluate two distinct and promising approaches, two-stage detection (Faster-RCNN) and one-stage detection (SSD), in four different configurations. We also employ two different image decompositions: the first based on the polarisation ellipse and the second on the Stokes parameters themselves. To evaluate our approach, the experimental trials were quantified by mean average precision (mAP) and processing time, showing a clear trade-off between the two factors. For example, the best mAP result of 80.94 % was achieved using Faster-RCNN, but at a frame rate of 6.4 fps. In contrast, MobileNet SSD achieved only 64.51 % mAP, but at 53.4 fps.",
    "code_link": ""
  },
  "cvpr2018_w27_localgroupinvarianceforheartrateestimationfromfacevideosinthewild": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Local Group Invariance for Heart Rate Estimation From Face Videos in the Wild",
    "authors": [
      "Christian S. Pilz",
      "Sebastian Zaunseder",
      "Jarek Krajewski",
      "Vladimir Blazek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Pilz_Local_Group_Invariance_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Pilz_Local_Group_Invariance_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We study the impact of prior knowledge about invariance for the task of heart rate estimation from face videos in the wild (e.g. in presence of disturbing factors like rigid head motion, talking, facial expressions and natural illumination conditions under different scenarios). We introduce features invariant with respect to the action of a differentiable local group of local transformations. As result, the energy of the blood volume signal is re-arranged in vector space with a more concentrated distribution. The uncertainty in the feature distribution is incorporated with a model that leverages the local invariance of the heart rate. During experiments the method achieved strong estimation performance of heart rate from face videos in the wild. To demonstrate the potential of the approach it is compared against recent algorithms on data collected to study the impact of the mentioned nuisance attributes. To facilitate future comparisons, we made the code and data for reproducing the results publicly available.",
    "code_link": ""
  },
  "cvpr2018_w27_advertisementeffectivenessestimationbasedoncrowdsourcedmultimodalaffectiveresponses": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Advertisement Effectiveness Estimation Based on Crowdsourced Multimodal Affective Responses",
    "authors": [
      "Genki Okada",
      "Kenta Masui",
      "Norimichi Tsumura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Okada_Advertisement_Effectiveness_Estimation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Okada_Advertisement_Effectiveness_Estimation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we estimate the effectiveness of an advertisement using online data collection and the remote measurement of facial expressions and physiological responses. Recently, the online advertisement market has expanded, and the measurement of advertisement effectiveness has become very important. We collected a significant number of videos of Japanese faces watching video advertisements in the same scenario in which media is normally used via the Internet. Facial expression and physiological responses such as heart rate and gaze were remotely measured by analyzing facial videos. By combining the measured responses into multimodal features and using machine learning, we show that ad liking can be predicted (ROC AUC = 0.93) better than when only single-mode features are used. Furthermore, intent to purchase can be estimated well (ROC AUC = 0.91) using multimodal features.",
    "code_link": ""
  },
  "cvpr2018_w27_sparseppgtowardsdrivermonitoringusingcamera-basedvitalsignsestimationinnear-infrared": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "SparsePPG: Towards Driver Monitoring Using Camera-Based Vital Signs Estimation in Near-Infrared",
    "authors": [
      "Ewa Magdalena Nowara",
      "Tim K. Marks",
      "Hassan Mansour",
      "Ashok Veeraraghavan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Nowara_SparsePPG_Towards_Driver_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Nowara_SparsePPG_Towards_Driver_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Camera-based measurement of the heartbeat signal from minute changes in the appearance of a person's skin is known as remote photoplethysmography (rPPG). Methods for rPPG have improved considerably in recent years, making possible its integration into applications such as telemedicine. Driver monitoring using in-car cameras is another potential application of this emerging technology. Unfortunately, there are several challenges unique to the driver monitoring context that must be overcome. First, there are drastic illumination changes on the driver's face, both during the day (as sun filters in and out of overhead trees, etc.) and at night (from streetlamps and oncoming headlights), which current rPPG algorithms cannot account for. We argue that these variations are significantly reduced by narrow-bandwidth near-infrared (NIR) active illumination at 940 nm, with matching bandpass filter on the camera. Second, the amount of motion during driving is significant. We perform a preliminary analysis of the motion magnitude and argue that any in-car solution must provide better robustness to motion artifacts. Third, low signal-to-noise ratio (SNR) and false peaks due to motion have the potential to confound the rPPG signal. To address these challenges, we develop a novel rPPG signal tracking and denoising algorithm (sparsePPG) based on Robust Principal Components Analysis and sparse frequency spectrum estimation. We release a new dataset of face videos collected simultaneously in RGB and NIR. We demonstrate that in each of these frequency ranges, our new method performs as well as or better than current state-of-the-art rPPG algorithms. Overall, our preliminary study indicates that while driver vital signs monitoring using cameras is promising, much work needs to be done in terms of improving robustness to motion artifacts before it becomes practical.",
    "code_link": ""
  },
  "cvpr2018_w27_novelalgorithmstomonitorcontinuouscardiacactivitywithavideocamera": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Novel Algorithms to Monitor Continuous Cardiac Activity With a Video Camera",
    "authors": [
      "Gregory F. Lewis",
      "Maria I. Davila",
      "Stephen W. Porges"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Lewis_Novel_Algorithms_to_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Lewis_Novel_Algorithms_to_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recent advances in computer vision methods have made physiological signal extraction from imaging sensors feasible. There is a demand to translate current post-hoc methods into real-time physiological monitoring techniques. Algorithms that function on a single frame of data meet the requirements for continuous, real-time measurement. If these algorithms are computationally efficient they may serve as the basis for an embedded system design that can be integrated within the vision hardware, turning the camera into a physiological monitor. Compelling results are presented derived from an appropriate algorithm for extracting cardiac pulse from sequential, single frames of a color video camera. Results are discussed with respect to physiologically relevant features of variability in beat-to-beat heart rate.",
    "code_link": ""
  },
  "cvpr2018_w27_measurementofcapillaryrefilltime(crt)inhealthysubjectsusingarobotichand": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Measurement of Capillary Refill Time (CRT) in Healthy Subjects Using a Robotic Hand",
    "authors": [
      "Emmett Kerr",
      "Sonya Coleman",
      "Martin McGinnity",
      "Andrea Shepherd"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Kerr_Measurement_of_Capillary_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Kerr_Measurement_of_Capillary_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " A human's CRT is a key indicator of their current health status. Being able to accurately assess a human's cardiovascular system peripherally by assessing their CRT in an emergency or search and rescue situation could, in critical scenarios, mean the difference between life and death. This paper presents a novel algorithm that enables a Shadow Robot Hand equipped with BioTAC biomimetic tactile fingertip sensors and a red, green, blue (RGB) camera to measure the CRT of humans by making contact with their forehead, regardless of their skin tone. The method presented replicates, to some extent, the methods carried out by medical professionals when measuring CRT and could be used to equip a first responder robot. Furthermore, the algorithms determine whether a person has a healthy cardiovascular system or whether the blood supply has been cut off from the skin indicating various issues such as shock or severe dehydration. The method presented in this work allows for a more accurate measurement of CRT than that of a medical professional.",
    "code_link": ""
  },
  "cvpr2018_w27_anovelframeworkforremotephotoplethysmographypulseextractiononcompressedvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "A Novel Framework for Remote Photoplethysmography Pulse Extraction on Compressed Videos",
    "authors": [
      "Changchen Zhao",
      "Chun-Liang Lin",
      "Weihai Chen",
      "Zhengguo Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Zhao_A_Novel_Framework_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Zhao_A_Novel_Framework_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Remote photoplethysmography (rPPG) has recently attracted much attention due to its non-contact measurement convenience and great potential in health care and computer vision applications. However, almost all the existing rPPG methods are based on uncompressed video data, which greatly limits its application to the scenarios that require long-distance video transmission. This paper proposes a novel framework as a first attempt to address the rPPG pulse extraction in presence of video compression artifacts. Based on the analysis of the impact of various compression methods on rPPG measurements, the problem is cast as single-channel signal separation. The framework consists of three major steps to extract the pulse waveform and heart rate by exploiting frequency structure of the rPPG signal. A benchmark dataset which contains stationary and motion videos has been built. The results show that the proposed algorithm significantly improves the SNR and heart rate precision of state-of-the-art rPPG algorithms on stationary videos and has a positive effect on motion videos at low bitrates.",
    "code_link": ""
  },
  "cvpr2018_w27_non-contactheartratemonitoringbycombiningconvolutionalneuralnetworkskindetectionandremotephotoplethysmographyviaalow-costcamera": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Non-Contact Heart Rate Monitoring by Combining Convolutional Neural Network Skin Detection and Remote Photoplethysmography via a Low-Cost Camera",
    "authors": [
      "Chuanxiang Tang",
      "Jiwu Lu",
      "Jie Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Tang_Non-Contact_Heart_Rate_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Tang_Non-Contact_Heart_Rate_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we present a versatile methodology to flexibly accomplish the non-contact monitoring of heart rate signals in various environments, by combining the convolutional neural network (CNN) skin detection and the camera-based remote photoplethysmography (rPPG) method. Compared to the widely-used three-step skin detection method (i.e., face detection, face tracking, and skin classification), the CNN method used here could significantly enhances the monitoring robustness by achieving the skin detection in one single step. The proposed CNN-rPPG method has been demonstrated in an unconstrained environment (e.g., office conditions) to directly verify its applicability. Combined with the subsequent rPPG heart rate monitoring based on a low-cost camera, the method presented here is of practical interests for the large-scale deployment of the non-contact heart rate monitoring technologies.",
    "code_link": ""
  },
  "cvpr2018_w27_exploringthefeasibilityoffacevideobasedinstantaneousheart-rateformicro-expressionspotting": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Exploring the Feasibility of Face Video Based Instantaneous Heart-Rate for Micro-Expression Spotting",
    "authors": [
      "Puneet Gupta",
      "Brojeshwar Bhowmick",
      "Arpan Pal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Gupta_Exploring_the_Feasibility_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Gupta_Exploring_the_Feasibility_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Facial micro-expressions (ME) are manifested by human reflexive behavior and thus they are useful to disclose the genuine human emotions. Their analysis plays a pivotal role in many real-world applications encompassing affective computing, biometrics and psychotherapy. The first and foremost step for ME analysis is ME spotting which refers to detection of ME affected frames from a video. ME spotting is a highly challenging research problem and even human experts cannot correctly perform it because MEs are manifested using subtle face deformations and that too for a short duration. It is well established that changes in the human emotions, not only manifest ME but it also introduces changes in instantaneous heart rate. Thus, the manifestation of ME and changes in the instantaneous heart rate are related to the change in human expressions and both of them are estimated using temporal deformations of the face. This provides the motivation of this paper that aims to explore the feasibility of variations in the instantaneous heart rate for performing the correct the ME spotting. Experimental results conducted on a publicly available spontaneous ME spotting dataset, reveal that the variations in instantaneous heart rate can be utilized to improve the ME spotting.",
    "code_link": ""
  },
  "cvpr2018_w27_videobasedmeasurementofheartrateandheartratevariabilityspectrogramfromestimatedhemoglobininformation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Video Based Measurement of Heart Rate and Heart Rate Variability Spectrogram From Estimated Hemoglobin Information",
    "authors": [
      "Munenori Fukunishi",
      "Kouki Kurita",
      "Shoji Yamamoto",
      "Norimichi Tsumura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Fukunishi_Video_Based_Measurement_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Fukunishi_Video_Based_Measurement_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose an accurate remote observation of the heart rate (HR) and heart rate variability (HRV) based on extracted hemoglobin information which is based on detail skin optics model. We perform experiments to measure subjects at rest and under cognitive stress with the proposed method putting a polarized filter in front of camera to evaluate the principal of the framework. From the results of the experiments, the proposed method shows a high correlation with the electrocardiograph (ECG) which is assumed as the ground truth. We also evaluated the robustness against illumination change in simulation. We confirmed that the proposed method could obtain accurate BVP detection compared with other conventional methods since the proposed method eliminates the shading component through the process of the extraction of hemoglobin component.",
    "code_link": ""
  },
  "cvpr2018_w27_periodicvariancemaximizationusinggeneralizedeigenvaluedecompositionappliedtoremotephotoplethysmographyestimation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Periodic Variance Maximization Using Generalized Eigenvalue Decomposition Applied to Remote Photoplethysmography Estimation",
    "authors": [
      "Richard Macwan",
      "Serge Bobbia",
      "Yannick Benezeth",
      "Julien Dubois",
      "Alamin Mansouri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Macwan_Periodic_Variance_Maximization_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Macwan_Periodic_Variance_Maximization_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " A generic periodic variance maximization algorithm to extract periodic or quasi-periodic signals of unknown periods embedded into multi-channel temporal signal recordings is described in this paper. The algorithm combines the notion of maximizing a periodicity metric combined with the global optimization scheme to estimate the source periodic signal of an unknown period. The periodicity maximization is performed using Generalized Eigenvalue Decomposition (GEVD) and the global optimization is performed using tabu search. A case study of remote photoplethysmography signal estimation has been utilized to assess the performance of the method using videos from public databases UBFC-RPPG [??] and MMSE-HR [??]. The results confirm the improved performance over existing state of the art methods and the feasibility of the use of the method in a live scenario owing to its small execution time.",
    "code_link": ""
  },
  "cvpr2018_w27_real-timetemporalsuperpixelsforunsupervisedremotephotoplethysmography": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Real-Time Temporal Superpixels for Unsupervised Remote Photoplethysmography",
    "authors": [
      "Serge Bobbia",
      "Duncan Luguern",
      "Yannick Benezeth",
      "Keisuke Nakamura",
      "Randy Gomez",
      "Julien Dubois"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Bobbia_Real-Time_Temporal_Superpixels_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Bobbia_Real-Time_Temporal_Superpixels_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Segmentation is a critical step for many computer vision applications. Among them, the remote photoplethysmography technique is significantly impacted by the quality of region of interest segmentation. With the heart-rate estimation accuracy, the processing time is obviously a key issue for the real-time monitoring. Recent face detection algorithms can perform real-time processing, however for unsupervised algorithms, without any subject detection based on supervised learning, existing methods are not able to achieve real-time on regular platform. In this paper, we propose a new method to perform real-time unsupervised remote photoplethysmograhy. This technique is based on efficient temporally propagated superpixels segmentation. The proposed method performs the segmentation step by implicitly identifying the superpixel boundaries. Hence, only a fraction of the image is used to performed the segmentation which reduced greatly the computational burden of the process. The segmentation quality remains comparable to state of the art methods while computational time is divided by a factor up to 8 times. The efficiency of the superpixel segmentation allow us to propose a real-time unsupervised rPPG algorithm considering frames of 640x480, RGB, at 25 frames per second on a single core platform. We obtained real-time processing for 93% of precision at 2.5 beat per minute using our inhouse video database.",
    "code_link": ""
  },
  "cvpr2018_w27_fully-automaticcamera-basedpulse-oximetryduringsleep": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Fully-Automatic Camera-Based Pulse-Oximetry During Sleep",
    "authors": [
      "Tom Vogels",
      "Mark van Gastel",
      "Wenjin Wang",
      "Gerard de Haan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Vogels_Fully-Automatic_Camera-Based_Pulse-Oximetry_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Vogels_Fully-Automatic_Camera-Based_Pulse-Oximetry_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Current routines for the monitoring of sleep require many sensors attached to the patient during a nocturnal observational study, limiting mobility and causing stress and discomfort. Cameras have shown promise in the remote monitoring of pulse rate, respiration and oxygen saturation, which potentially allows a reduction in the number of sensors. Applying these techniques in a sleep setting is challenging, as it is unknown upfront which portion of the skin will be visible, there is no unique skin-color outside the visible range, and the pulsatility is low in infrared. We present a fully-automatic living tissue detection method to enable continuous monitoring of pulse rate and oxygen saturation during sleep. The system is validated on a dataset where various typical sleep scenarios have been simulated. Results show the proposed method to outperform the current state-of-the-art, especially for the estimation of oxygen saturation.",
    "code_link": ""
  },
  "cvpr2018_w27_impairingfactorsinremote-ppgpulsetransittimemeasurementsontheface": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Impairing Factors in Remote-PPG Pulse Transit Time Measurements on the Face",
    "authors": [
      "Andreia Moco",
      "Sander Stuijk",
      "Mark van Gastel",
      "Gerard de Haan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Moco_Impairing_Factors_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Moco_Impairing_Factors_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The time it takes for a pulse wave to propagate between two arterial sites--i.e., the pulse transit time (PTT)--has received considerable attention as marker of aortic stiffness and as surrogate for blood pressure. However, obtrusiveness and manual intervention requirements render conventional PTT measurement methods inappropriate for ubiquitous monitoring. In this regard, high-speed camera systems are interesting alternatives. Recognizably,a technical breakthrough would be estimating PTT with a relatively inexpensive RGB camera pointed at the face only. A simple means to do this is determining the phase shift (PS) betweenphotoplethysmographic (PPG) signalsextracted at collocated skin pixels. In this paper, we show that the validity of this approach is threatened by skin variability. We analysed simultaneous video recordings of the neck and face in 21 subjects (ages, 33 +-11 yrs). These were used to extract PPG signals at the face and skin motion (sMOT) signals at the vicinity of the carotid artery. Using sMOT as reference signal, we show that the pressure wave undergoes delay and frequency leakage as it propagates across the arterial tree; the extent of propagation distortion is subject-dependent and place PPG-based estimations at a disadvantage in comparison with PTTs measured at the arterial level. Awareness is further raised for the site-dependency of PS outcomes by the provision of facial PPG-phase maps and collocated PPG signals. Lastly, impairments due to waveform dissimilarity are demonstrated under exercise-induced PTT changes. In conclusion, PS is unsuitable for PTT measurements at the face.",
    "code_link": ""
  },
  "cvpr2018_w27_deepsuperresolutionforrecoveringphysiologicalinformationfromvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Deep Super Resolution for Recovering Physiological Information From Videos",
    "authors": [
      "Daniel McDuff"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/McDuff_Deep_Super_Resolution_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/McDuff_Deep_Super_Resolution_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Imaging photoplethysmography (iPPG) allows for remote measurement of vital signs from the human skin. In some applications the skin region of interest may only occupy a small number of pixels (e.g., if an individual is a large distance from the imager.) We present a novel pipeline for iPPG using an image super-resolution preprocessing step that can reduce the mean absolute error in heart rate prediction by over 30%. Furthermore, deep learning-based image super-resolution outperforms standard interpolation methods.Our method can be used in conjunction with any existing iPPG algorithm to estimate physiological parameters. It is particularly promising for analysis of low resolution and spatially compressed videos, where otherwise the pulse signal would be too weak.",
    "code_link": ""
  },
  "cvpr2018_w27_direct-globalseparationforimprovedimagingphotoplethysmography": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W27",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Physiological Measurement",
    "title": "Direct-Global Separation for Improved Imaging Photoplethysmography",
    "authors": [
      "Jaehee Park",
      "Ashutosh Sabharwal",
      "Ashok Veeraraghavan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w27/html/Park_Direct-Global_Separation_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w27/Park_Direct-Global_Separation_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Camera-based estimation of vital signs has made significant progress in last few years. Despite of the significant algorithmic advances, the low signal-to-backgroundratio in video-based photoplethysmography continues to be a performance bottleneck. One of the main challenges is that much of the light returning to the camera from the subject is surface reflection from the skin and other dermal layers, and hence does not contain any pulsatile blood perfusion information to estimate photoplesthysmogram (PPG). In this paper, we show that direct-global separation techniques designed to reject much of the surface reflection photons can improve the signal-to-background ratio in the raw captured video signal. We study two techniques for the suppression of direct surface reflection (a) cross-polarization and (b) structured illumination. Using a dataset from 28 participants, our results show an average SNR improvement in estimating PPG from the use of structured illumination is 1.42 dB compared to the brightfield illumination. The use of cross-polarizers leads to an average SNR increase of 1.49 dB compared tobrightfield illumination. And the combined structured illumination and polarizer method increases the SNR on the average by 1.90 dB compared to thebrightfield illumination. The key result is that local PPG estimate SNR can increase to more than 5.63dB, enabling very large gains on regions with large specular component. The RMSE decreased 55% and the range of error reduced by 12.9% with the use of polarizer and structured illumination.",
    "code_link": ""
  },
  "cvpr2018_w28_automatedanalysisofmarinevideowithlimiteddata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "Automated Analysis of Marine Video With Limited Data",
    "authors": [
      "Deborah Levy",
      "Yuval Belfer",
      "Elad Osherov",
      "Eyal Bigal",
      "Aviad P. Scheinin",
      "Hagai Nativ",
      "Dan Tchernov",
      "Tali Treibitz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w28/html/Levy_Automated_Analysis_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w28/Levy_Automated_Analysis_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Monitoring of the marine environment requires large amounts of data, simply due to its vast size. Therefore, underwater autonomous vehicles and drones are increasingly deployed to acquire numerous photographs. However, ecological conclusions from them are lagging as the data requires expert annotation and thus realistically cannot bemanually processed.This calls for developing automatic classification algorithms dedicated for this type of data. Current out-of-the-box solutions struggle to provide optimal results in these scenarios as the marine data is very different from everyday data. Images taken under water display low contrast levels and reduced visibility range thus making objects harder to localize and classify.Scale varies dramatically because of the complex 3 dimensionality of the scenes. In addition, the scarcity of labeled marine data prevents training these dedicated networks from scratch. In this work, we demonstrate how transfer learning can be utilized to achieve high quality results for both detection and classification in the marine environment. We also demonstrate tracking in videos that enables counting and measuring the organisms. We demonstrate the suggested method on two very different marine datasets, an aerial dataset and an underwater one.",
    "code_link": ""
  },
  "cvpr2018_w28_acomparisonofdeeplearningmethodsforsemanticsegmentationofcoralreefsurveyimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "A Comparison of Deep Learning Methods for Semantic Segmentation of Coral Reef Survey Images",
    "authors": [
      "Andrew King",
      "Suchendra M. Bhandarkar",
      "Brian M. Hopkinson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w28/html/King_A_Comparison_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w28/King_A_Comparison_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Two major deep learning methods for semantic segmentation, i.e., patch-based convolutional neural network (CNN) approaches and fully convolutional neural network (FCNN) models, are studied in the context of classification of regions in underwater images of coral reef ecosystems into biologically meaningful categories. For the patch-based CNN approaches, we use image data extracted from underwater video accompanied by individual point-wise ground truth annotations. We show that patch-based CNN methods can outperform a previously proposed approach that uses support vector machine (SVM)-based classifiers in conjunction with texture-based features. We compare the results of five different CNN architectures in our formulation of patch-based CNN methods. The Resnet152 CNN architecture is observed to perform the best on our annotated dataset of underwater coral reef images. We also examine and compare the results of four different FCNN models for semantic segmentation of coral reef images. We develop a tool for fast generation of segmentation maps to serve as ground truth segmentations for our FCNN models. The FCNN architecture Deeplab v2 is observed to yield the best results for semantic segmentation of underwater coral reef images.",
    "code_link": ""
  },
  "cvpr2018_w28_stingraydetectionofaerialimagesusingaugmentedtrainingimagesgeneratedbyaconditionalgenerativemodel": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "Stingray Detection of Aerial Images Using Augmented Training Images Generated by a Conditional Generative Model",
    "authors": [
      "Yi-Min Chou",
      "Chien-Hung Chen",
      "Keng-Hao Liu",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w28/html/Chou_Stingray_Detection_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w28/Chou_Stingray_Detection_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we present an object detection method that tackles the stingray detection problem based on aerial images. In this problem, the images are aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle (UAV), and the stingrays swimming under (but close to) the sea surface are the target we want to detect and locate. To this end, we use a deep object detection method, faster RCNN, to train a stingray detector based on a limited training set of images. To boost the performance, we develop a new generative approach, conditional GLO, to increase the training samples of stingray, which is an extension of the Generative Latent Optimization (GLO) approach. Unlike traditional data augmentation methods that generate new data only for image classification, our proposed method that mixes foreground and background together can generate new data for an object detection task, and thus improve the training efficacy of a CNN detector. Experimental results show that satisfiable performance can be obtained by using our approach on stingray detection in aerial images.",
    "code_link": ""
  },
  "cvpr2018_w28_cameracalibrationforunderwater3dreconstructionbasedonraytracingusingsnellslaw": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W28",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Automated Analysis of Marine Video for Environmental Monitoring",
    "title": "Camera Calibration for Underwater 3D Reconstruction Based on Ray Tracing Using Snell's Law",
    "authors": [
      "Malte Pedersen",
      "Stefan Hein Bengtson",
      "Rikke Gade",
      "Niels Madsen",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w28/html/Pedersen_Camera_Calibration_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w28/Pedersen_Camera_Calibration_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Accurately estimating the 3D position of underwater objects is of great interest when doing research on marine animals. An inherent problem of 3D reconstruction of underwater positions is the presence of refraction which invalidates the assumption of a single viewpoint. Three ways of performing 3D reconstruction on underwater objects are compared in this work: an approach relying solely on in-air camera calibration, an approach with the camera calibration performed under water and an approach based on ray tracing with Snell's law. As expected, the in-air camera calibration showed to be the most inaccurate as it does not take refraction into account. The precision of the estimated 3D positions based on the underwater camera calibration and the ray tracing based approach were, on the other hand, almost identical. However, the ray tracing based approach is found to be advantageous as it is far more flexible in terms of the calibration procedure due to the decoupling of the intrinsic and extrinsic camera parameters.",
    "code_link": ""
  },
  "cvpr2018_w29_hp-ganprobabilistic3dhumanmotionpredictionviagan": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Joint Detection, Tracking, and Prediction in the Wild",
    "title": "HP-GAN: Probabilistic 3D Human Motion Prediction via GAN",
    "authors": [
      "Emad Barsoum",
      "John Kender",
      "Zicheng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w29/html/Barsoum_HP-GAN_Probabilistic_3D_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w29/Barsoum_HP-GAN_Probabilistic_3D_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Predicting and understanding human motion dynamics has many applications, such as motion synthesis, augmented reality, security, and autonomous vehicles. Due to the recent success of generative adversarial networks (GAN), there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms. We propose a novel sequence-to-sequence model for probabilistic human motion prediction, trained with a modified version of improved Wasserstein generative adversarial networks (WGAN-GP), in which we use a custom loss function designed for human motion prediction. Our model, which we call HP-GAN, learns a probability density function of future human poses conditioned on previous poses. It predicts multiple sequences of possible future human poses, each from the same input sequence but a different vector z drawn from a random distribution. Furthermore, to quantify the quality of the non-deterministic predictions, we simultaneously train a motion-quality-assessment model that learns the probability that a given skeleton sequence is a real human motion. We test our algorithm on two of the largest skeleton datasets: NTURGB-D and Human3.6M.We train our model on both single and multiple action types. Its predictive power for long-term motion estimation is demonstrated by generating multiple plausible futures of more than 30 frames from just 10 frames of input. We show that most sequences generated from the same input have more than 50% probabilities of being judged as a real human sequence. We will release all the code used in this paper to Github.",
    "code_link": "https://github.com/ebarsoum/hpgan"
  },
  "cvpr2018_w29_fusionofheadandfull-bodydetectorsformulti-objecttracking": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Joint Detection, Tracking, and Prediction in the Wild",
    "title": "Fusion of Head and Full-Body Detectors for Multi-Object Tracking",
    "authors": [
      "Roberto Henschel",
      "Laura Leal-Taixe",
      "Daniel Cremers",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w29/html/Henschel_Fusion_of_Head_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w29/Henschel_Fusion_of_Head_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In order to track all persons in a scene, the tracking-by-detection paradigm has proven to be a very effective approach. Yet, relying solely on a single detector is also a major limitation, as useful image information might be ignored. Consequently, this work demonstrates how to fuse two detectors into a tracking system. To obtain the trajectories, we propose to formulate tracking as a weighted graph labeling problem, resulting in a binary quadratic program. As such problems are NP-hard, the solution can only be approximated. Based on the Frank-Wolfe algorithm, we present a new solver that is crucial to handle such difficult problems. Evaluation on pedestrian tracking is provided for multiple scenarios, showing superior results over single detector tracking and standard QP-solvers. Finally, our tracker ranks 2nd on the MOT16 benchmark and 1st on the new MOT17 benchmark, outperforming over 90 trackers.",
    "code_link": ""
  },
  "cvpr2018_w29_re-identificationforonlinepersontrackingbymodelingspace-timecontinuum": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Joint Detection, Tracking, and Prediction in the Wild",
    "title": "Re-Identification for Online Person Tracking by Modeling Space-Time Continuum",
    "authors": [
      "Neeti Narayan",
      "Nishant Sankaran",
      "Srirangaraj Setlur",
      "Venu Govindaraju"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w29/html/Narayan_Re-Identification_for_Online_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w29/Narayan_Re-Identification_for_Online_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a novel approach to multi-person multi-camera tracking based on learning the space-time continuum of a camera network. Some challenges involved in tracking multiple people in real scenarios include a) ensuring reliable continuous association of all persons, and b) accounting for presence of blind-spots or entry/exit points. Most of the existing methods design sophisticated models that require heavy tuning of parameters and it is a non-trivial task for deep learning approaches as they cannot be applied directly to address the above challenges. Here, we deal with the above points in a coherent way by proposing a discriminative spatio-temporal learning approach for tracking based on person re-identification using LSTM networks. This approach is more robust when no a-priori information about the aspect of an individual or the number of individuals is known. The idea is to identify detections as belonging to the same individual by continuous association and recovering from past errors in associating different individuals to a particular trajectory. We exploit LSTM's ability to infuse temporal information to predict the likelihood that new detections belong to the same tracked entity by jointly incorporating visual appearance features and location information. The proposed approach gives a 50% improvement in the error rate compared to the previous state-of-the-art method on the CamNeT dataset and 18% improvement as compared to the baseline approach on DukeMTMC dataset.",
    "code_link": ""
  },
  "cvpr2018_w29_diyhumanactiondatasetgeneration": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Joint Detection, Tracking, and Prediction in the Wild",
    "title": "DIY Human Action Dataset Generation",
    "authors": [
      "Mehran Khodabandeh",
      "Hamid Reza Vaezi Joze",
      "Ilya Zharkov",
      "Vivek Pradeep"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w29/html/Khodabandeh_DIY_Human_Action_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w29/Khodabandeh_DIY_Human_Action_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The recent successes in applying deep learning techniques to solve standard computer vision problems has aspired researchers to propose new computer vision problems in different domains. As previously established in the field, training data itself plays a significant role in the machine learning process, especially deep learning approaches which are data hungry. In order to solve each new problem and get a decent performance, a large amount of data needs to be captured which may in many cases pose logistical difficulties. Therefore, the ability to generate de novo data or expand an existing dataset, however small, in order to satisfy data requirement of current networks may be invaluable. Herein, we introduce a novel way to partition an action video clip into action, subject and context. Each part is manipulated separately and reassembled with our proposed video generation technique. Furthermore, our novel human skeleton trajectory generation along with our proposed video generation technique, enables us to generate unlimited action recognition training data. These techniques enables us to generate video action clips from an small set without costly and time-consuming data acquisition. Lastly, we prove through extensive set of experiments on two small human action recognition datasets, that this new data generation technique can improve the performance of current action recognition neural nets.",
    "code_link": ""
  },
  "cvpr2018_w29_jointdetectionandonlinemulti-objecttracking": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Joint Detection, Tracking, and Prediction in the Wild",
    "title": "Joint Detection and Online Multi-Object Tracking",
    "authors": [
      "Hilke Kieritz",
      "Wolfgang Hubner",
      "Michael Arens"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w29/html/Kieritz_Joint_Detection_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w29/Kieritz_Joint_Detection_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Most multiple object tracking methods rely on object detection methods in order to initialize new tracks and to update existing tracks. Although strongly interconnected, tracking and detection are usually addressed as separate building blocks. However both parts can benefit from each other, e.g. the affinity model from the tracking method can reuse appearance features already calculated by the detector, and the detector can use object information from past in order to avoid missed detection. Towards this end, we propose a multiple object tracking method that jointly performs detection and tracking in a single neural network architecture. By training both parts together, we can use optimized parameters instead of heuristic decisions over the track lifetime. We adapt the Single Shot MultiBox Detector (SSD) to serve single frame detection to a recurrent neural network (RNN), which combines detections into tracks.We show initial prove of concept on the DETRAC benchmark with competitive results, illustrating the feasibility of learnable track management. We conclude with a discussion of open problems on the MOT16 benchmark.",
    "code_link": ""
  },
  "cvpr2018_w29_convolutionalsocialpoolingforvehicletrajectoryprediction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W29",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Joint Detection, Tracking, and Prediction in the Wild",
    "title": "Convolutional Social Pooling for Vehicle Trajectory Prediction",
    "authors": [
      "Nachiket Deo",
      "Mohan M. Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w29/html/Deo_Convolutional_Social_Pooling_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w29/Deo_Convolutional_Social_Pooling_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Forecasting the motion of surrounding vehicles is a critical ability for an autonomous vehicle deployed in complex traffic. Motion of all vehicles in a scene is governed by the traffic context, i.e., the motion and relative spatial configuration of neighboring vehicles. In this paper we propose an LSTM encoder-decoder model that uses convolutional social pooling as an improvement to social pooling layers for robustly learning inter-dependencies in vehicle motion. Additionally, our model outputs a multi-modal predictive distribution over future trajectories based on maneuver classes. We evaluate our model using the publicly available NGSIM US-101 and I-80 datasets. Our results show improvement over the state of the art in terms of RMS values of prediction error and negative log-likelihoods of true future trajectories under the model's predictive distribution. We also present a qualitative analysis of the model's predicted distributions for various traffic scenarios.",
    "code_link": ""
  },
  "cvpr2018_w30_drone-viewbuildingidentificationbycross-viewvisuallearningandrelativespatialestimation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Drone-View Building Identification by Cross-View Visual Learning and Relative Spatial Estimation",
    "authors": [
      "Chun-Wei Chen",
      "Yin-Hsi Kuo",
      "Tang Lee",
      "Cheng-Han Lee",
      "Winston Hsu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Chen_Drone-View_Building_Identification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Chen_Drone-View_Building_Identification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Drones become popular recently and equip more sensors than traditional cameras, which bring emerging applications and research. To enable drone-based applications, providing related information (e.g., building) to understand the environment around the drone is essential. We frame this drone-view building identification as building retrieval problem: given a building (multimodal query) with its images, geolocation and drone's current location, we aim to retrieve the most likely proposal (building candidate) on a drone-view image. Despite few annotated drone-view images to date, there are many images of other views from the Web, like ground-level, street-view and aerial images. Thus, we propose a cross-view triplet neural network to learn visual similarity between drone-view and other views, further design relative spatial estimation of each proposal and the drone, and collect new drone-view datasets for the task. Our method outperforms triplet neural network by 0.12 mAP. (i.e., 22.9 to 35.0, +53% in a sub-dataset [LA])",
    "code_link": ""
  },
  "cvpr2018_w30_integrationofabsoluteorientationmeasurementsinthekinectfusionreconstructionpipeline": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Integration of Absolute Orientation Measurements in the KinectFusion Reconstruction Pipeline",
    "authors": [
      "Silvio Giancola",
      "Jens Schneider",
      "Peter Wonka",
      "Bernard S. Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Giancola_Integration_of_Absolute_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Giancola_Integration_of_Absolute_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we show how absolute orientation measurements provided by low-cost but high-fidelity IMU sensors can be integrated into the KinectFusion pipeline. We show that integration improves both runtime, robustness and quality of the 3D reconstruction. In particular, we use this orientation data to seed and regularize the ICP registration technique. We also present a technique to filter the pairs of 3D matched points based on the distribution of their distances. This filter is implemented efficiently on the GPU. Estimating the distribution of the distances helpscontrol the number of iterations necessary for the convergence of the ICP algorithm. Finally, we show experimental results that highlight improvements in robustness, a speed-up of almost 12%, and a gain in tracking quality of 53% for the ATE metric on the Freiburg benchmark.",
    "code_link": ""
  },
  "cvpr2018_w30_optimallinearattitudeestimatorforalignmentofpointclouds": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Optimal Linear Attitude Estimator for Alignment of Point Clouds",
    "authors": [
      "Xue Iuan Wong",
      "Taewook Lee",
      "Puneet Singla",
      "Manoranjan Majji"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Wong_Optimal_Linear_Attitude_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Wong_Optimal_Linear_Attitude_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents an approach to estimate the rigid transformation between two point clouds using a linear least squares solution termed as the optimal linear attitude estimator (OLAE). It is shown that by parameterizing the relative orientation between point clouds of interest using the Classical Rodrigues Parameters (CRP), the OLAE approach transforms the nonlinear attitude estimation problem into a linear problem. These linear equations are solved efficiently with closed form solution without any expensive matrix decomposition or inversion. This paper also shows that the 3 degrees of freedom (DOF) special case of OLAE that is of interest for aligning point clouds sensed by road vehicles in self-driving car applications can be effectively solved as a linear function with only 1 unknown variable. This formulation enables the 1D RANSAC that can effectively remove outliers in the measurement.",
    "code_link": ""
  },
  "cvpr2018_w30_multi-scalevoxelhashingandefficient3drepresentationformobileaugmentedreality": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Multi-Scale Voxel Hashing and Efficient 3D Representation for Mobile Augmented Reality",
    "authors": [
      "Yi Xu",
      "Yuzhang Wu",
      "Hui Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Xu_Multi-Scale_Voxel_Hashing_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Xu_Multi-Scale_Voxel_Hashing_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In recent years, Visual-Inertial Odometry (VIO) technologies have been making great strides in both research community and industry. With the development of ARKit and ARCore, mobile Augmented Reality (AR) applications have become popular. However, collision detection and avoidance is largely un-addressed with these applications. In this paper, we present an efficient multi-scale voxel hashing algorithm for representing a 3D environment using a set of multi-scale voxels. The input to our algorithm is the 3D point cloud generated by a VIO system (e.g., ARKit). We show that our method can process the 3D points and convert them into multi-scale 3D representation in real time, while maintaining a small memory footprint. The 3D representation can be used to efficiently detect collision between digital objects and real objects in an environment in AR applications.",
    "code_link": ""
  },
  "cvpr2018_w30_adeepcnn-basedframeworkforenhancedaerialimageryregistrationwithapplicationstouavgeolocalization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "A Deep CNN-Based Framework for Enhanced Aerial Imagery Registration With Applications to UAV Geolocalization",
    "authors": [
      "Ahmed Nassar",
      "Karim Amer",
      "Reda ElHakim",
      "Mohamed ElHelw"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Nassar_A_Deep_CNN-Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Nassar_A_Deep_CNN-Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper we present a novel framework for geolocalizing Unmanned Aerial Vehicles (UAVs) using only their onboard camera. The framework exploits the abundance of satellite imagery, along with established computer vision and deep learning methods, to locate the UAV in a satellite imagery map. It utilizes the contextual information extracted from the scene to attain increased geolocalization accuracy and enable navigation without the use of a Global Positioning System (GPS), which is advantageous in GPS-denied environments and provides additional enhancement to existing GPS-based systems. The framework inputs two images at a time, one captured using a UAV-mounted down-looking camera, and the other synthetically generated from the satellite map based on the UAV location within the map. Local features are extracted and used to register both images, a process that is performed recurrently to relate UAV motion to its actual map position, hence performing preliminary localization. A semantic shape matching algorithm is subsequently applied to extract and match meaningful shape information from both images, and use this information to improve localization accuracy.The framework is evaluated on two different datasets representing different geographical regions. Obtained results demonstrate the viability of proposed method and that the utilization of visual information can offer a promising approach for unconstrained UAV navigation and enable the aerial platform to be self-aware of its surroundings thus opening up new application domains or enhancing existing ones.",
    "code_link": ""
  },
  "cvpr2018_w30_automatedvirtualnavigationandmonocularlocalizationofindoorspacesfromvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Automated Virtual Navigation and Monocular Localization of Indoor Spaces From Videos",
    "authors": [
      "Qiong Wu",
      "Ambrose Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Wu_Automated_Virtual_Navigation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Wu_Automated_Virtual_Navigation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " 3D virtual navigation and localization in large indoor spaces (i.e., shopping malls and offices) are usually two separate studied problems. In this paper, we propose an automated framework to publish both 3D virtual navigation and monocular localization services that only require videos (or burst of images) of the environment as input. The framework can unify two problems as one because the collected data are highly utilized for both problems, 3D visual model reconstruction and training data for monocular localization. The power of our approach is that it does not need any human label data and instead automates the process of two separate services based on raw video (or burst of images) data captured by a common mobile device. We build a prototype system that publishes both virtual navigation and localization services for a shopping mall using raw video (or burst of images) data as inputs. Two web applications are developed utilizing two services. One allows navigation in 3D following the original video traces, and user can also stop at any time to explore in 3D space. One allows a user to acquire his/her location by uploading an image of the venue. Because of low barrier of data acquirement, this makes our system widely applicable to a variety of domains and significantly reduces service cost for potential customers.",
    "code_link": ""
  },
  "cvpr2018_w30_deepvisualteachandrepeatonpathnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Deep Visual Teach and Repeat on Path Networks",
    "authors": [
      "Tristan Swedish",
      "Ramesh Raskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Swedish_Deep_Visual_Teach_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Swedish_Deep_Visual_Teach_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose an approach for solving Visual Teach and Repeat tasks for routes that consist of discrete directions along path networks using deep learning. Visual paths are specified by a single monocular image sequence and our approach does not query frames or image features during inference, but instead is composed of classifiers trained on each path.Our method is efficient for both storing or following paths and enables sharing of visual path specifications between parties without sharing visual data explicitly.We evaluate our approach in a simulated environment, and present qualitative results on real data captured with a smartphone.",
    "code_link": "https://github.com/avisingh599/mono-vo"
  },
  "cvpr2018_w30_semanticmetric3dreconstructionforconcreteinspection": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W30",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues",
    "title": "Semantic Metric 3D Reconstruction for Concrete Inspection",
    "authors": [
      "Liang Yang",
      "Bing Li",
      "Wei Li",
      "Biao Jiang",
      "Jizhong Xiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w30/html/Yang_Semantic_Metric_3D_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w30/Yang_Semantic_Metric_3D_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we exploit the concrete surface flaw inspection through the fusion of visual positioning and semantic segmentation approach. The fused inspection result is represented by a 3D metric map with a spatial area, width, and depth information, which shows the advantage over general inspection in image space without metric info. We also relieve the human labor with an automatic labeling approach. The system is composed of three hybrid parts: visual positioning to enable pose association, crack/spalling inspection using a deep neural network (pixel level), and a 3D random field filter for fusion to achieve a global 3D metric map. To improve the infrastructure inspection, we released a new data set for concrete crack and spalling segmentation which is built on CSSC dataset [27]. To leverage the effectiveness of the large-scale SLAM aided semantic inspection, we performed three field tests and one baseline test. Experimental results show that our proposed approach significantly improves the capability of 3D metric concrete inspection via deploying visual SLAM. Furthermore, we achieve an 82.4 % MaxF1 score for crack detection and 88.64% MaxF1 score for spalling detection on the relabeled dataset.",
    "code_link": "https://github.com/ccny-ros-pkg/inspectionNet"
  },
  "cvpr2018_w32_discretecosinetransformresidualfeaturebasedfilteringforgeryandsplicingdetectioninjpegimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Discrete Cosine Transform Residual Feature Based Filtering Forgery and Splicing Detection in JPEG Images",
    "authors": [
      "Aniket Roy",
      "Diangarti Bhalang Tariang",
      "Rajat Subhra Chakraborty",
      "Ruchira Naskar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Roy_Discrete_Cosine_Transform_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Roy_Discrete_Cosine_Transform_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Digital images are one of the primary modern media for information interchange. However, digital images are vulnerable to interception and manipulation due to the wide availability of image editing software tools. Filtering forgery detection and splicing detection are two of the most important problems in digital image forensics. In particular, the primary challenge for the filtering forgery detection problem is that typically the techniques effective for nonlinear filtering (e.g. median filtering) detection are quite ineffective for linear filtering detection, and vice versa. In this paper, we have used Discrete Cosine Transform Residual features to train a Support Vector Machine classifier, and have demonstrated its effectiveness for both linear and non-linear filtering (specifically, Median Filtering) detection and filter classification, as well as re-compression based splicing detection in JPEG images. We have also theoretically justified the choice of the abovementioned feature set for both type of forgeries. Our technique outperforms the state-of-the-art forensic techniques for filtering detection, filter classification and re-compression based splicing detection, when applied on a set of standard benchmark images.",
    "code_link": ""
  },
  "cvpr2018_w32_forgerydetectionin3d-sensorimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Forgery Detection in 3D-Sensor Images",
    "authors": [
      "Noa Privman-Horesh",
      "Azmi Haider",
      "Hagit Hel-Or"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Privman-Horesh_Forgery_Detection_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Privman-Horesh_Forgery_Detection_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The field of Image Forensic, and with it the notion of image forgery and its detection, is widely studied in 2D images and videos. Since3D cameras (cameras with depth sensors) are becoming increasingly commonplace, it is of importance to introduce the notion of forgery detection in depth-images. In this paper, we present an introductorystudy of forgery detection in depth-images. Specifically, we show that noise statistics in depth-images can be exploited for camera source identification, image forgery detection and even depth reconstruction fromnoise.",
    "code_link": ""
  },
  "cvpr2018_w32_vgan-basedimagerepresentationlearningforprivacy-preservingfacialexpressionrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "VGAN-Based Image Representation Learning for Privacy-Preserving Facial Expression Recognition",
    "authors": [
      "Jiawei Chen",
      "Janusz Konrad",
      "Prakash Ishwar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Chen_VGAN-Based_Image_Representation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Chen_VGAN-Based_Image_Representation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion.",
    "code_link": ""
  },
  "cvpr2018_w32_privacy-preservingindoorlocalizationviaactivesceneillumination": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Privacy-Preserving Indoor Localization via Active Scene Illumination",
    "authors": [
      "Jinyuan Zhao",
      "Natalia Frumkin",
      "Janusz Konrad",
      "Prakash Ishwar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Zhao_Privacy-Preserving_Indoor_Localization_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Zhao_Privacy-Preserving_Indoor_Localization_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Traditional camera-based indoor localization systems use visual information to resolve the position of an object or person. This approach, however, may not be acceptable in privacy-sensitive scenarios since high-resolution images may reveal room and occupant details to eavesdroppers. In this paper, we address privacy concerns by replacing cameras with a small network of extremely low resolution color sensors. To make the system robust to ambient lighting fluctuations, we modulate an array of LED light sources to actively control the illumination while recording the light received by the sensors. We quantitatively validate the performance of our localization approach through simulations and real testbed experiments. We quantify the impact of sensor noise and changes in ambient illumination on localization accuracy. Finally, we demonstrate the superior performance of localization via active illumination compared to passive illumination where LEDs produce constant light.",
    "code_link": ""
  },
  "cvpr2018_w32_humanperceptionsofsensitivecontentinphotos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Human Perceptions of Sensitive Content in Photos",
    "authors": [
      "Yifang Li",
      "Wyatt Troutman",
      "Bart P. Knijnenburg",
      "Kelly Caine"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Li_Human_Perceptions_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Li_Human_Perceptions_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Before we can obfuscate portions of an image to enhance privacy, we must know what portions are considered sensitive. In this paper, we report results from a study aimed at identifying sensitive content in photos from a human-centered perspective. We collected sensitive photos and/or descriptions of sensitive photos from participants and asked them to identify which elements of the photo made each photo sensitive. Using this information, we propose an initial two-level taxonomy of sensitive content categories. This taxonomy may be useful to privacy researchers, online social network designers, policy makers, computer vision researchers and anyone wishing to identify potentially sensitive content in photos. We conclude by providing insights about how these results may be used to enhance computer vision approaches to protecting image privacy.",
    "code_link": ""
  },
  "cvpr2018_w32_onvisibleadversarialperturbations&digitalwatermarking": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "On Visible Adversarial Perturbations & Digital Watermarking",
    "authors": [
      "Jamie Hayes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Hayes_On_Visible_Adversarial_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Hayes_On_Visible_Adversarial_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Given a machine learning model, adversarial perturbations transform images such that the model's output is classified as an attacker chosen class. Most research in this area has focused on adversarial perturbations that are imperceptible to the human eye. However, recent work has considered attacks that are perceptible but localized to a small region of the image. Under this threat model, we discuss both defenses that remove such adversarial perturbations, and attacks that can bypass these defenses.",
    "code_link": ""
  },
  "cvpr2018_w32_onthesuitabilityoflp-normsforcreatingandpreventingadversarialexamples": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "On the Suitability of Lp-Norms for Creating and Preventing Adversarial Examples",
    "authors": [
      "Mahmood Sharif",
      "Lujo Bauer",
      "Michael K. Reiter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Sharif_On_the_Suitability_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Sharif_On_the_Suitability_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Much research effort has been devoted to better understanding adversarial examples, which are specially crafted inputs to machine-learning models that are perceptually similar to benign inputs, but are classified differently (i.e., misclassified). Both algorithms that create adversarial examples and strategies for defending against them typically use L_p-norms to measure the perceptual similarity between an adversarial input and its benign original. Prior work has already shown, however, that two images need not be close to each other as measured by an L_p-norm to be perceptually similar. In this work, we show that nearness according to an L_p-norm is not just unnecessary for perceptual similarity, but is also insufficient. Specifically, focusing on datasets (CIFAR10 and MNIST), L_p-norms, and thresholds used in prior work, we show through 299-participant online user studies that \"adversarial examples\" that are closer to their benign counterparts than required by commonly used L_p-norm thresholds can nevertheless be perceptually different to humans from the corresponding benign examples. Namely, the perceptual distance between two images that are \"near\" each other according to an L_p-norm can be high enough that participants frequently classify the two images as representing different objects or digits. Combined with prior work, we thus demonstrate that nearness of inputs as measured by L_p-norms is neither necessary nor sufficient for perceptual similarity, which has implications for both creating and defending against adversarial examples. We propose and discuss alternative similarity metrics to stimulate future research in the area.",
    "code_link": ""
  },
  "cvpr2018_w32_semanticadversarialexamples": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Semantic Adversarial Examples",
    "authors": [
      "Hossein Hosseini",
      "Radha Poovendran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Hosseini_Semantic_Adversarial_Examples_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Hosseini_Semantic_Adversarial_Examples_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep neural networks are known to be vulnerable to adversarial examples, i.e., images that are maliciously perturbed to fool the model. Generating adversarial examples has been mostly limited to finding small perturbations that maximize the model prediction error. Such images, however, contain artificial perturbations that make them somewhat distinguishable from natural images. This property is used by several defense methods to counter adversarial examples by applying denoising filters or training the model to be robust to small perturbations. In this paper, we introduce a new class of adversarial examples, namely \"Semantic Adversarial Examples,\" as images that are arbitrarily perturbed to fool the model, but in such a way that the modified image semantically represents the same object as the original image.We formulate the problem of generating such images as a constrained optimization problem and develop an adversarial transformation based on the shape bias property of human cognitive system. In our method, we generate adversarial images by first converting the RGB image into the HSV (Hue, Saturation and Value) color space and then randomly shifting the Hue and Saturation components, while keeping the Value component the same. Our experimental results on CIFAR10 dataset show that the accuracy of VGG16 network on adversarial color-shifted images is 5.7%.",
    "code_link": ""
  },
  "cvpr2018_w32_convolutionalneuralnetworksforirispresentationattackdetectiontowardcross-datasetandcross-sensorgeneralization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W32",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security",
    "title": "Convolutional Neural Networks for Iris Presentation Attack Detection: Toward Cross-Dataset and Cross-Sensor Generalization",
    "authors": [
      "Steven Hoffman",
      "Renu Sharma",
      "Arun Ross"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w32/html/Hoffman_Convolutional_Neural_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w32/Hoffman_Convolutional_Neural_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Iris recognition systems are vulnerable to presentation attacks where an adversary employs artifacts such as 2D prints of the eye, plastic eyes, and cosmetic contact lenses to obfuscate their own identity or to spoof the identity of another subject. In this work, we design a Convolutional Neural Network (CNN) architecture for presentation attack detection, that is observed to have good cross-dataset generalization capability. The salient features of the proposed approach include:(a) the use of the pre-normalized iris rather than the normalized iris, thereby avoiding spatial information loss; (b) the tessellation of the iris region into overlapping patches to enable data augmentation as well as to learn features that are location agnostic; (c) fusion of information across patches to enhance detection accuracy; (d) incorporating a \"segmentation mask\" in order to automatically learn the relative importance of the pupil and iris regions;(e) generation of a \"heat map\" that displays patch-wise presentation attack information, thereby accounting for artifacts that may impact only a small portion of the iris region. Experiments confirm the efficacy of the proposed approach.",
    "code_link": ""
  },
  "cvpr2018_w33_eyeintheskyreal-timedronesurveillancesystem(dss)forviolentindividualsidentificationusingscatternethybriddeeplearningnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Eye in the Sky: Real-Time Drone Surveillance System (DSS) for Violent Individuals Identification Using ScatterNet Hybrid Deep Learning Network",
    "authors": [
      "Amarjot Singh",
      "Devendra Patil",
      "SN Omkar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Singh_Eye_in_the_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Singh_Eye_in_the_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Drone systems have been deployed by various law enforcement agencies to monitor hostiles, spy on foreign drug cartels, conduct border control operations, etc. This paper introduces a real-time drone surveillance system to identify violent individuals in public areas. The system first uses the Feature Pyramid Network to detect humans from aerial images. The image region with the human is used by the proposed ScatterNet Hybrid Deep Learning (SHDL) network for human pose estimation. The orientations between the limbs of the estimated pose are next used to identify the violent individuals. The proposed deep network can learn meaningful representations quickly using ScatterNet and structural priors with relatively fewer labeled examples. The system detects the violent individuals in real-time by processing the drone images in the cloud. This research also introduces the aerial violent individual dataset used for training the deep network which hopefully may encourage researchers interested in using deep learning for aerial surveillance. The pose estimation and violent individuals identification performance is compared with the state-of-the-art techniques.",
    "code_link": ""
  },
  "cvpr2018_w33_squeezenexthardware-awareneuralnetworkdesign": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "SqueezeNext: Hardware-Aware Neural Network Design",
    "authors": [
      "Amir Gholami",
      "Kiseok Kwon",
      "Bichen Wu",
      "Zizheng Tai",
      "Xiangyu Yue",
      "Peter Jin",
      "Sicheng Zhao",
      "Kurt Keutzer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " One of the main barriers for deploying neural networks on embedded systems has been large memory and power consumption of existing neural network architectures. In this work, we introduce SqueezeNext, a new family of neural network architectures whose design was guided by considering previous architectures such as SqueezeNet, as well as by simulation results on a neural network accelerator. This new neural network architecture is able to match AlexNet's accuracy on the ImageNet benchmark with 112xfewer parameters, and one of its deeper variants is able to achieve VGG-19 accuracy with only 4.4 Million parameters, (31xsmaller than VGG-19). SqueezeNext also achieves better top-5 classification accuracy with 1.3xfewer parameters as compared to MobileNet, but avoids using depthwise-separable convolutions that are inefficient on some mobile processor platforms. This wide range of accuracy gives the user the ability to make speed-accuracy tradeoffs, depending on the available resources on the target hardware. Using hardware simulation results for power and inference speed on a embedded system has guided us to design variations of the baseline model that achieved up to 20% better hardware utilization with minimaldifference in accuracy.",
    "code_link": ""
  },
  "cvpr2018_w33_recurrentsegmentationforvariablecomputationalbudgets": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Recurrent Segmentation for Variable Computational Budgets",
    "authors": [
      "Lane McIntosh",
      "Niru Maheswaranathan",
      "David Sussillo",
      "Jonathon Shlens"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/McIntosh_Recurrent_Segmentation_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/McIntosh_Recurrent_Segmentation_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " State-of-the-art systems for semantic image segmentation use feed-forward pipelines with fixed computational costs. Building an image segmentation system that works across a range of computational budgets is challenging and time-intensive as new architectures must be designed and trained for every computational setting. To address this problem we develop a recurrent neural network that successively improves prediction quality with each iteration. Importantly, the RNN may be deployed across a range of computational budgets by merely running the model for a variable number of iterations. We find that this architecture is uniquely suited for efficiently segmenting videos. By exploiting the segmentation of past frames, the RNN can perform video segmentation at similar quality but reduced computational cost compared to state-of-the-art image segmentation methods. When applied to static images in the PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a speed-accuracy curve that saturates near the performance of state-of-the-art segmentation methods.",
    "code_link": ""
  },
  "cvpr2018_w33_highwaynetworkblockwithgatesconstraintsfortrainingverydeepnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Highway Network Block With Gates Constraints for Training Very Deep Networks",
    "authors": [
      "Oyebade K. Oyedotun",
      "Abd El Rahman Shabayek",
      "Djamila Aouada",
      "Bjorn Ottersten"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Oyedotun_Highway_Network_Block_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Oyedotun_Highway_Network_Block_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose to reformulate the learning of the highway network block to realize both early optimization and improved generalization of very deep networks while preserving the network depth. Gate constraints are duly employed to improve optimization, latent representations and parameterization usage in order to efficiently learn hierarchical feature transformations which are crucial for the success of any deep network. One of the earliest very deep models with over 30 layers that was successfully trained relied on highway network blocks. Although, highway blocks suffice for alleviating optimization problem via improved information flow, we show for the first time that further in training such highway blocks may result into learning mostly untransformed features and therefore a reduction in the effective depth of the model; this could negatively impact model generalization performance. Using the proposed approach, 15-layer and 20-layer models are successfully trained with one gate and a 32-layer model using three gates. This leads to a drastic reduction of model parameters as compared to the original highway network. Extensive experiments on CIFAR-10, CIFAR-100, Fashion-MNIST and USPS datasets are performed to validate the effectiveness of the proposed approach. Particularly, we outperform the original highway network and many state-of-the-art results. To the best our knowledge, on the Fashion-MNIST and USPS datasets, the achieved results are the best reported in literature.",
    "code_link": "https://github.com/zalandoresearch/fashion-mnist"
  },
  "cvpr2018_w33_munetmacrounit-basedconvolutionalneuralnetworkformobiledevices": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "MUNet: Macro Unit-Based Convolutional Neural Network for Mobile Devices",
    "authors": [
      "Dae Ha Kim",
      "Seung Hyun Lee",
      "Byung Cheol Song"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Kim_MUNet_Macro_Unit-Based_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Kim_MUNet_Macro_Unit-Based_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep neural networks perform better than traditional machine learning methods on various classification problems by producing good quality feature maps through successive convolution operation(s). However, when implementing a deep neural network in an embedded system or SoC for mobile devices, its large parameter size can be a significant burden on the internal memory design. In this paper, we propose a new deep neural network that reduces computation and the number of model parameters but maintains reasonable performance. The configuration of the proposed network is as follows: First, we propose a macro unit (MU) to reduce heavy computations and to learn various feature maps. Second, we employ asymmetric convolution of the well-known Inception network to further efficiently manipulate feature maps within the MU. Third, all the feature maps produced from MU(s) of each layer are concatenated and then the grouped feature map is distributed to all the MUs of the next layer for transferring richer information. Experimental results show that the proposed network achieves about 10% higher performance than DenseNet-BC in case of extremely small parameter size for CIFAR-100. The proposed network also has very few learning parameters and smaller floating point operations per second (FLOPS) than the other networks optimized for mobile devices such as MobileNet.",
    "code_link": ""
  },
  "cvpr2018_w33_ultrapower-efficientcnndomainspecificacceleratorwith9.3tops/wattformobileandembeddedapplications": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Ultra Power-Efficient CNN Domain Specific Accelerator With 9.3TOPS/Watt for Mobile and Embedded Applications",
    "authors": [
      "Baohua Sun",
      "Lin Yang",
      "Patrick Dong",
      "Wenhan Zhang",
      "Jason Dong",
      "Charles Young"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Sun_Ultra_Power-Efficient_CNN_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Sun_Ultra_Power-Efficient_CNN_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Computer vision performances have been significantly improved in recent years by Convolutional Neural Networks (CNN). Currently, applications using CNN algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs or FPGAs. However, power consumption, speed, accuracy, memory footprint, and die size should all be taken into consideration for mobile and embedded applications. Domain Specific Architecture (DSA) for CNN is the efficient and practical solution for CNN deployment and implementation. We designed and produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra power-efficient performance of 9.3TOPS/Watt and with all processing done in the internal memory instead of external DRAM. It classifies 224x224 RGB image inputs at more than 140fps with peak power consumption at less than 300mW and an accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is reconfigurable to support CNN model coefficients of various layer sizes and layer types, including convolution, depth-wise convolution, short-cut connections, max pooling, and ReLU. Furthermore, in order to better support real-world deployment for various application scenarios, especially with low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also designed algorithms to fully utilize the CNN-DSA accelerator efficiently by reducing the dependency on external accelerator computation resources, including implementation of Fully-Connected (FC) layers within the accelerator and compression of extracted features from the CNN-DSA accelerator. Live demos with our CNN-DSA accelerator on mobile and embedded systems show its capabilities to be widely and practically applied in the real world.",
    "code_link": ""
  },
  "cvpr2018_w33_mergingdeepneuralnetworksformobiledevices": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Merging Deep Neural Networks for Mobile Devices",
    "authors": [
      "Yi-Min Chou",
      "Yi-Ming Chan",
      "Jia-Hong Lee",
      "Chih-Yi Chiu",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Chou_Merging_Deep_Neural_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Chou_Merging_Deep_Neural_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, a novel method to merge convolutional neural networks for the inference stage is introduced. When two feed-forward networks already trained for handling different tasks are given, our method can align the layers of these networks and merge them into a unified model by sharing the representative weights. The performance of the merged model can be restored or improved via re-training. Without needing high-performance hardware, the proposed method effectively produces a compact model to run the original tasks simultaneously on resource-limited devices. The system development time, as well as training overhead, is substantially reduced because our method leverages the co-used weights and preserves the general architectures of the well-trained networks. The merged model is jointly compressed and can be implemented faster than the original models with a comparable accuracy. When combining VGG-Avg and ZF-Net models, our approach can achieve higher than 12 and 2.5 times of compression and speedup ratios compared to the original whole models, respectively, while the accuracy remains approximately the same.",
    "code_link": "https://github.com/ivclab/NeuralMerger"
  },
  "cvpr2018_w33_efficientdeeplearninginferencebasedonmodelcompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Efficient Deep Learning Inference Based on Model Compression",
    "authors": [
      "Qing Zhang",
      "Mengru Zhang",
      "Mengdi Wang",
      "Wanchen Sui",
      "Chen Meng",
      "Jun Yang",
      "Weidan Kong",
      "Xiaoyuan Cui",
      "Wei Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Zhang_Efficient_Deep_Learning_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Zhang_Efficient_Deep_Learning_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep neural networks (DNNs) have evolved remarkably over the last decade and achieved great success in many machine learning tasks. Along the evolution of deep learning (DL) methods, computational complexity and resource consumption of DL models continue to increase, this makes efficient deployment challenging, especially in devices with low memory resources or in applications with strict latency requirements. In this paper, we will introduce a DL inference optimization pipeline, which consists of a series of model compression methods, including Tensor Decomposition (TD), Graph Adaptive Pruning (GAP), Intrinsic Sparse Structures (ISS) in Long Short-Term Memory (LSTM), Knowledge Distillation (KD) and low-bit model quantization. We use different modeling scenarios to test our inference optimization pipeline with above mentioned methods, and it shows promising results to make inference more efficient with marginal loss of model accuracy.",
    "code_link": ""
  },
  "cvpr2018_w33_learningnetworkarchitecturesofdeepcnnsunderresourceconstraints": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W33",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Learning Network Architectures of Deep CNNs Under Resource Constraints",
    "authors": [
      "Michael Chan",
      "Daniel Scarafoni",
      "Ronald Duarte",
      "Jason Thornton",
      "Luke Skelly"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w33/html/Chan_Learning_Network_Architectures_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w33/Chan_Learning_Network_Architectures_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recent works in deep learning have been driven broadly by the desire to attain high accuracy on certain challenge problems. The network architecture and other hyper-parameters of many published models are typically chosen by trial-and-error experiments with little considerations paid to resource constraints at deployment time. We propose a fully automated model learning approach that (1) treats architecture selection as part of the learning process, (2) uses a blend of broad-based random sampling and adaptive iterative refinement to explore the solution space, (3) performs optimization subject to given memory and computational constraints imposed by target deployment scenarios, and (4) is scalable and can use only a practically small number of GPUs for training. We present results that show graceful model degradation under strict resource constraints for object classification problems using CIFAR-10 in our experiments. We also discuss future work in further extending the approach.",
    "code_link": ""
  },
  "cvpr2018_w34_soccernetascalabledatasetforactionspottinginsoccervideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos",
    "authors": [
      "Silvio Giancola",
      "Mohieddine Amine",
      "Tarek Dghaily",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Giancola_SoccerNet_A_Scalable_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Giancola_SoccerNet_A_Scalable_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we introduce SoccerNet, a benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017, and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). As such, the dataset is easily scalable. These annotations are manually refined to a one second resolution by anchoring them at a single timestamp following well-defined soccer rules. With an average of one event every 6.9 minutes, this dataset focuses on the problem of localizing very sparse events within long videos. We define the task of spotting as finding the anchors of soccer events in a video. Making use of recent developments in the realm of generic action recognition and detection in video, we provide strong baselines for detecting soccer events. We show that our best model for classifying temporal segments of length one minute reaches a mean Average Precision (mAP) of 67.8%. For the spotting task, our baseline reaches an Average-mAP of 49.7% for tolerances \\delta ranging from 5 to 60 seconds. Our dataset and models are available at https://silviogiancola.github.io/SoccerNet.",
    "code_link": ""
  },
  "cvpr2018_w34_deepdecisiontreesfordiscriminativedictionarylearningwithadversarialmulti-agenttrajectories": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Deep Decision Trees for Discriminative Dictionary Learning With Adversarial Multi-Agent Trajectories",
    "authors": [
      "Tharindu Fernando",
      "Sridha Sridharan",
      "Clinton Fookes",
      "Simon Denman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Fernando_Deep_Decision_Trees_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Fernando_Deep_Decision_Trees_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " With the explosion in the availability of spatio-temporal tracking data in modern sports, there is an enormous opportunity to better analyse, learn and predict important events in adversarial group environments. In this paper, we propose a deep decision tree architecture for discriminative dictionary learning from adversarial multi-agent trajectories. We first build up a hierarchy for the tree structure by adding each layer and performing feature weight based clustering in the forward pass. We then fine tune the player role weights using back propagation. The hierarchical architecture ensures the interpretability and the integrity of the group representation. The resulting architecture is a decision tree, with leaf-nodes capturing a dictionary of multi-agent group interactions. Due to the ample volume of data available, we focus on soccer tracking data, although our approach can be used in any adversarial multi-agent domain. We present applications of proposed method for simulating soccer games as well as evaluating and quantifying team strategies.",
    "code_link": ""
  },
  "cvpr2018_w34_part-basedplayeridentificationusingdeepconvolutionalrepresentationandmulti-scalepooling": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Part-Based Player Identification Using Deep Convolutional Representation and Multi-Scale Pooling",
    "authors": [
      "Arda Senocak",
      "Tae-Hyun Oh",
      "Junsik Kim",
      "In So Kweon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Senocak_Part-Based_Player_Identification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Senocak_Part-Based_Player_Identification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper addresses the problem of automatic player identification in broadcast sports videos filmed with a single side-view medium distance camera. Player identification in this setting is a challenging task because visual cues such as faces and jersey numbers are not clearly visible. Thus, this task requires sophisticated approaches to capture distinctive features from players to distinguish them. To this end, we use Convolutional Neural Networks (CNN) features extracted at multiple scales and encode them with an advanced pooling, called Fisher vector. We leverage it for exploring representations that have sufficient discriminatory power and ability to magnify subtle differences. We also analyze the distinguishing parts of the players and present a part based pooling approach to use these distinctive feature points. The resulting player representation is able to identify players even in difficult scenes. It achieves state-of-the-art results up to 96% on NBA basketball clips.",
    "code_link": ""
  },
  "cvpr2018_w34_fine-grainedactivityrecognitioninbaseballvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Fine-Grained Activity Recognition in Baseball Videos",
    "authors": [
      "AJ Piergiovanni",
      "Michael S. Ryoo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Piergiovanni_Fine-Grained_Activity_Recognition_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Piergiovanni_Fine-Grained_Activity_Recognition_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition.",
    "code_link": ""
  },
  "cvpr2018_w34_soccerwhohastheball?generatingvisualanalyticsandplayerstatistics": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Soccer: Who Has the Ball? Generating Visual Analytics and Player Statistics",
    "authors": [
      "Rajkumar Theagarajan",
      "Federico Pala",
      "Xiu Zhang",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Theagarajan_Soccer_Who_Has_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Theagarajan_Soccer_Who_Has_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The world of sports intrinsically involves fast and complex events that aredifficult for coaches, trainers and players to analyze, and also for audiences to follow. In sports, talent identification and selection are imperative for the development of future elite level performers. Current scenarios involve word-of-mouth, coaches and recruiters scouring through hours of videos and many times manual annotation of these videos. In this paper, we propose an approach that automatically generates visual analytics from videos specifically for soccer to help coaches and recruiters identify the most promising talents. We use (a) Convolutional Neural Networks (CNNs) to localize soccer players in a video and identify players controlling the ball, (b) Deep Convolutional Generative Adversarial Networks (DCGAN) for data augmentation, (c) a histogram based matching to identify teams and (d) frame-by-frame prediction and verification analyses to generate visual analytics. We compare our approach with state-of-the-art approaches and achieve an accuracy of 86.59% on identifying players controlling the ball and an accuracy of 84.73% in generating the game analytics and player statistics.",
    "code_link": "https://github.com/pytorch/pytorch"
  },
  "cvpr2018_w34_convolutionalneuralnetworksbasedballdetectionintennisgames": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Convolutional Neural Networks Based Ball Detection in Tennis Games",
    "authors": [
      "Vito Reno",
      "Nicola Mosca",
      "Roberto Marani",
      "Massimiliano Nitti",
      "Tiziana D'Orazio",
      "Ettore Stella"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Reno_Convolutional_Neural_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Reno_Convolutional_Neural_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In recent years sport video research has gained a steady interest among the scientific community. The large amount of video data available from broadcast transmissions and from dedicated camera setups, and the need of extracting meaningful information from data, pose significant research challenges. Hence, computer vision and machine learning are essential for enabling automated or semi-automated processing of big data in sports. Although sports are diverse enough to present unique challenges on their own, most of them share the need to identify active entities such as ball or players. In this paper, an innovative deep learning approach to the identification of the ball in tennis context is presented. The work exploits the potential of a convolutional neural network classifier to decide whether a ball is being observed in a single frame, overcoming the typical issues that can occur dealing with classical approaches on long video sequences (e.g. illumination changes and flickering issues). Experiments on real data confirm the validity of the proposed approach that achieves 98.77 % accuracy and suggest its implementation and integration at a larger scale in more complex vision systems.",
    "code_link": ""
  },
  "cvpr2018_w34_abottom-upapproachbasedonsemanticsfortheinterpretationofthemaincamerastreaminsoccergames": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "A Bottom-Up Approach Based on Semantics for the Interpretation of the Main Camera Stream in Soccer Games",
    "authors": [
      "Anthony Cioppa",
      "Adrien Deliege",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Cioppa_A_Bottom-Up_Approach_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Cioppa_A_Bottom-Up_Approach_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Automatic interpretation of sports games is a major challenge, especially when these sports feature complex players organizations and game phases. This paper describes a bottom-up approach based on the extraction of semantic features from the video stream of the main camera in the particular case of soccer using scene-specific techniques. In our approach, all the features, ranging from the pixel level to the game event level, have a semantic meaning. First, we design our own scene-specific deep learning semantic segmentation network and hue histogram analysis to extract pixel-level semantics for the field, players, and lines. These pixel-level semantics are then processed to compute interpretative semantic features which represent characteristics of the game in the video stream that are exploited to interpret soccer. For example, they correspond to how players are distributed in the image or the part of the field that is filmed. Finally, we show how these interpretative semantic features can be used to set up and train a semantic-based decision tree classifier for major game events with a restricted amount of training data. The main advantages of our semantic approach are that it only requires the video feed of the main camera to extract the semantic features, with no need for camera calibration, field homography, player tracking, or ball position estimation. While the automatic interpretation of sports games remains challenging, our approach allows us to achieve promising results for the semantic feature extraction and for the classification between major soccer game events such as attack, goal or goal opportunity, defense, and middle game.",
    "code_link": ""
  },
  "cvpr2018_w34_humanposeascalibrationpattern;3dhumanposeestimationwithmultipleunsynchronizedanduncalibratedcameras": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Human Pose As Calibration Pattern; 3D Human Pose Estimation With Multiple Unsynchronized and Uncalibrated Cameras",
    "authors": [
      "Kosuke Takahashi",
      "Dan Mikami",
      "Mariko Isogawa",
      "Hideaki Kimata"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Takahashi_Human_Pose_As_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Takahashi_Human_Pose_As_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper proposes a novel algorithm of estimating 3D human pose from multi-view videos captured by unsynchronized and uncalibrated cameras.In a such configuration, the conventional vision-based approaches utilize detected 2D features of common 3D points for synchronization and camera pose estimation, however, they sometimes suffer from difficulties of feature correspondences in case of wide baselines.For such cases, the proposed method focuses on that the projections of human joints can be associated each other robustly even in wide baseline videos and utilizes them as the common reference points. To utilize the projections of joint as the corresponding points, they should be detected in the images, however, these 2D joint sometimes include detection errors which make the estimation unstable. For dealing with such errors, the proposed method introduces two ideas. The first idea is to relax the reprojection errors for avoiding optimizing to noised observations. The second idea is to introduce an geometric constraint on the prior knowledge that the reference points consists of human joints. We demonstrate the performance of the proposed algorithm of synchronization and pose estimation with qualitative and quantitative evaluations using synthesized and real data.",
    "code_link": ""
  },
  "cvpr2018_w34_jerseynumberrecognitionwithsemi-supervisedspatialtransformernetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Jersey Number Recognition With Semi-Supervised Spatial Transformer Network",
    "authors": [
      "Gen Li",
      "Shikun Xu",
      "Xiang Liu",
      "Lei Li",
      "Changhu Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Li_Jersey_Number_Recognition_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Li_Jersey_Number_Recognition_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " It is still a challenging task to recognize the jersey number of players on the court in soccer match videos, as the jersey numbers are very small in the object detection task and annotated data are not easy to collect. Based on the object detection results of all the players on the court, a CNN model is first introduced to classify these numbers on the deteced players' images. To localize the jersey number more precisely without involving another digit detector and extra consumption, we then improve the former network to an end-to-end framework by fusing with the spatial transformer network (STN). To further improve the accuracy, we bring extra supervision to STN and upgrade the model to a semi-supervised multi-task learning system, by labeling a small portion of the number areas in the data set by quadrangle. Extensive experiments illustrate the effectiveness of the proposed framework.",
    "code_link": ""
  },
  "cvpr2018_w34_kinematicposerectificationforperformanceanalysisandretrievalinsports": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Kinematic Pose Rectification for Performance Analysis and Retrieval in Sports",
    "authors": [
      "Dan Zecha",
      "Moritz Einfalt",
      "Christian Eggert",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Zecha_Kinematic_Pose_Rectification_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Zecha_Kinematic_Pose_Rectification_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The automated extraction of kinematic parameters from athletes in video footage allows for direct training feedback and continuous quantitative assessment of an athlete's performance. Recent developments in the field of deep learning enable the measurement of kinematic coefficients directly from human pose estimates. However, the detection quality decreases while errors and noise increase with the complexity of the scene. In aquatic training scenarios, for instance, continuous pose estimation suffers from several orthogonal errors like switched joint predictions between the left and right sides of the body. In this paper, we analyze different error modes and present a rectification pipeline for improving the pose predictions using merely joint coordinates. We show experimentally that joint rectification equally improves the detection of key-poses, which are essential for a continuous qualitative performance assessment and pose retrieval, as well as posture visualization for quantitative training feedback.",
    "code_link": ""
  },
  "cvpr2018_w34_automaticcrickethighlightgenerationusingevent-drivenandexcitement-basedfeatures": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Automatic Cricket Highlight Generation Using Event-Driven and Excitement-Based Features",
    "authors": [
      "Pushkar Shukla",
      "Hemant Sadana",
      "Apaar Bansal",
      "Deepak Verma",
      "Carlos Elmadjian",
      "Balasubramanian Raman",
      "Matthew Turk"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Shukla_Automatic_Cricket_Highlight_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Shukla_Automatic_Cricket_Highlight_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Producing sports highlights is a labor-intensive work that requires some degree of specialization. We propose a model capable of automatically generating sports highlights with a focus on cricket. Cricket is a sport with a complex set of rules and is played for a longer time than most other sports. In this paper we propose a model that considers both event-based and excitement-based features to recognize and clip important events in a cricket match. Replays, audio intensity, player celebration, and playfield scenarios are examples of cues used to capture such events. To evaluate our framework, we conducted a set of experiments ranging from user studies to a comparison analysis between our highlights and the ones provided by the official broadcasters. The general approval by users and the significant overlap between both kinds of highlights indicate the usefulness of our model in real-life scenarios",
    "code_link": ""
  },
  "cvpr2018_w34_estimationofcenterofmassforsportssceneusingweightedvisualhull": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Estimation of Center of Mass for Sports Scene Using Weighted Visual Hull",
    "authors": [
      "Tomoya Kaichi",
      "Shohei Mori",
      "Hideo Saito",
      "Kosuke Takahashi",
      "Dan Mikami",
      "Mariko Isogawa",
      "Hideaki Kimata"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Kaichi_Estimation_of_Center_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Kaichi_Estimation_of_Center_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents a method to estimate the 3D position of a center of mass (CoM) of a human body from a set of multi-view images. As a well-known fact, in sports, collections of CoM are important for analyzing the athletes' performance. Most conventional studies in CoM estimation require installing a measuring system (e.g., a force plate or optical motion capture system) or attaching sensors to the athlete. While such systems reliably estimate CoM, casual settings are preferable for simplifying preparations. To address this issue, the proposed method takes a vision-based approach that does not require specialized hardware and wearable devices. Our method calculates subject's CoM using voxels with body parts dependent weighting. This individual voxel reconstruction and voxel-wise weighting reflects the differences in each body shape, and are expected to contribute to higher performance in analysis. The results using real data demonstrated the performance of the proposed method were compared to force plate data, and provided a 3D CoM visualization in a dynamic scene.",
    "code_link": ""
  },
  "cvpr2018_w34_adirectedsparsegraphicalmodelformulti-targettracking": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "A Directed Sparse Graphical Model for Multi-Target Tracking",
    "authors": [
      "Mohib Ullah",
      "Faouzi Alaya Cheikh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Ullah_A_Directed_Sparse_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Ullah_A_Directed_Sparse_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose a Directed Sparse Graphical Model (DSGM) for multi-target tracking. In the category of global optimization for multi-target tracking, traditional approaches have two main drawbacks. First, a cost function is defined in terms of the linear combination of the spatial and appearance constraints of the targets which results a highly non-convex function. And second, a very dense graph is constructed to capture the global attribute of the targets. In such a graph, It is impossible to find reliable tracks in polynomial time unless some relaxation and heuristics are used. To address these limitations, we proposed DSGM which finds a set of reliable tracks for the targets without any heuristics or relaxation and keeps the computational complexity very low through the design of the graph. Irrespective of traditional approaches where spatial and appearance constraints are added up linearly with a given weight factor, we incorporated these constraints in a cascaded fashion. First, we exploited a Hidden Markov Model (HMM) for the spatial constraints of the target and obtain most probable locations of the targets in a segment of video. Afterwards, a deep feature based appearance model is used to generate the sparse graph. The track for each target is found through dynamic programming. Experiments are performed on 3 challenging sports datasets (football, basketball and sprint) and promising results are achieved.",
    "code_link": ""
  },
  "cvpr2018_w34_estimatingthenumberofsoccerplayersusingsimulation-basedocclusionhandling": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W34",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision in Sports",
    "title": "Estimating the Number of Soccer Players Using Simulation-Based Occlusion Handling",
    "authors": [
      "Noor Ul Huda",
      "Kasper H. Jensen",
      "Rikke Gade",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w34/html/Huda_Estimating_the_Number_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w34/Huda_Estimating_the_Number_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Estimating the number of soccer players is crucial information for occupancy analysis and other monitoring activities in sports analysis. It depends on player detection in the field that should be independent of the environment and light conditions. Thermal cameras are therefore a better option over normal RGB cameras. Detection of non-occluded players is doable but precise estimation of number of the players in groups is hard to achieve. Here we propose a novel method for estimating number of the players in groups using computer graphics and virtual simulations. Occlusion conditions are first classified by using distinctive set of features trained by a bagged tree classifier. Estimation of the number of players is then performed by maximum likelihood of probability density based approach to further classify the occluded players. The results show that the implemented strategy is capable of providing precise results even during occlusion conditions.",
    "code_link": ""
  },
  "cvpr2018_w35_space-time-brightnesssamplingusinganadaptivepixel-wisecodedexposure": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computational Cameras and Displays",
    "title": "Space-Time-Brightness Sampling Using an Adaptive Pixel-Wise Coded Exposure",
    "authors": [
      "Hajime Nagahara",
      "Toshiki Sonoda",
      "Dengyu Liu",
      "Jinwei Gu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w35/html/Nagahara_Space-Time-Brightness_Sampling_Using_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w35/Nagahara_Space-Time-Brightness_Sampling_Using_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Most conventional digital video cameras face a fundamental trade-off between spatial resolution, temporal resolution and dynamic range (i.e., brightness resolution) because of a limited bandwidth for data transmission.A few recent studies have shown that with non-uniform space-time sampling, such as that implemented with pixel-wise coded exposure, one can go beyond this trade-off and achieve high efficiency for scene capture.However, in these studies, the sampling schemes were pre-defined and independent of the target scene content.In this paper, we propose an adaptive space-time-brightness sampling method to further improve the efficiency of video capture.The proposed method adaptively updates a pixel-wise coded exposure pattern using the information analyzed from previously captured frames.We built a prototype camera that enables adaptive coding of patterns online to show the feasibility of the proposed adaptive coded exposure method. Simulation and experimental results show that the adaptive space-time-brightness sampling scheme achieves more accurate video reconstruction results and high dynamic range with less computational cost, than previous method. To the best of our knowledge, our prototype is the first implementation of an adaptive pixel-wise coded exposure camera.",
    "code_link": ""
  },
  "cvpr2018_w35_multi-capturedynamiccalibrationofmulti-camerasystems": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computational Cameras and Displays",
    "title": "Multi-Capture Dynamic Calibration of Multi-Camera Systems",
    "authors": [
      "Avinash Kumar",
      "Manjula Gururaj",
      "Kalpana Seshadrinathan",
      "Ramkumar Narayanswamy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w35/html/Kumar_Multi-Capture_Dynamic_Calibration_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w35/Kumar_Multi-Capture_Dynamic_Calibration_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Multi-camera systems have seen an emergence in various consumer devices enabling many applications e.g. bokeh (Apple IPhone), 3D measurement (Dell Venue 8) etc. An accurately calibrated multi-camera system is essential for proper functioning of these applications. Usually, a onetime factory calibration with technical targets is done to accurately calibrate such systems. Although accurate, factory calibration does not hold over the life time of the device as normal wear and tear, thermal effects, device usage etc. can cause calibration parameters to change. Thus, a dynamic or self-calibration based on multi-view image features is required to refine calibration parameters. One of the important factors governing the accuracy of dynamic calibration is the number and distribution of feature points in the captured scene. A dense feature distribution enables better sampling of the 3D scene, while avoiding degenerate situations (e.g. all features on one plane), thus sufficiently modeling the forward imaging process for calibration. But, single real life images with dense feature distribution are difficult or nearly impossible to capture e.g. texture-less indoor or occluded scenes. In this paper, we propose a new multi-capture paradigm for multi-camera dynamic calibration where multiple multi-view images of different 3D scenes (thus varying feature point distribution) are jointly used to calibrate the multi-camera system. We present a new optimality criteria to select the best set of candidate images from a pool of multi-view images, along with their order, to use for multi-capture dynamic calibration. We also propose a methodology to jointly model calibration parameters of multiple multi-view images. Finally, we show improved performance of multi-capture dynamic calibration over single-capture dynamic calibration in terms of lower epipolar rectification and 3D measurement error.",
    "code_link": ""
  },
  "cvpr2018_w35_jitteredexposuresforimagesuper-resolution": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W35",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computational Cameras and Displays",
    "title": "Jittered Exposures for Image Super-Resolution",
    "authors": [
      "Nianyi Li",
      "Scott McCloskey",
      "Jingyi Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w35/html/Li_Jittered_Exposures_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w35/Li_Jittered_Exposures_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Camera design involves tradeoffs between spatial and temporal resolution. For instance, traditional cameras provide either high spatial resolution (e.g., DSLRs) or high frame rate, but not both. Our approach exploits the optical stabilization hardware already present in commercial cameras and increasingly available in smartphones. Whereas single image super-resolution (SR) methods can produce convincing-looking images and have recently been shown to improve the performance of certain vision tasks, they are still limited in their ability to fully recover information lost due to under-sampling. In this paper, we present a new imaging technique that efficiently trades temporal resolution for spatial resolution in excess of the sensor's pixel count without attenuating light or adding additional hardware. On the hardware front, we demonstrate that the consumer-grade optical stabilization hardware provides enough precision in its static position to enable physically-correct SR. On the algorithm front, we elaborately design the Image Stabilization (IS) lens position pattern so that the SR can be efficiently conducted via image deconvolution. Compared with state-of-the-art solutions, our approach significantly reduces the computation complexity of the processing step while still providing the same level of optical fidelity, especially on quantifiable performance metrics from optical character recognition (OCR) and barcode decoding.",
    "code_link": ""
  },
  "cvpr2018_w36_wicv2018thefourthwomenincomputervisionworkshop": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "WiCV 2018: The Fourth Women in Computer Vision Workshop",
    "authors": [
      "Ilke Demir",
      "Dena Bazazian",
      "Adriana Romero",
      "Viktoriia Sharmanska",
      "Lyne P. Tchapmi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Demir_WiCV_2018_The_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Demir_WiCV_2018_The_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present WiCV 2018 - Women in Computer Vision Workshop to increase the visibility and inclusion of women researchers in computer vision field, organized in conjunction with CVPR 2018. Computer vision and machine learning have made incredible progress over the past years, yet the number of female researchers is still low both in academia and industry. WiCV is organized to raise visibility of female researchers, to increase the collaboration, and to provide mentorship and give opportunities to female-identifying junior researchers in the field. In its fourth year, we are proud to present the changes and improvements over the past years, summary of statistics for presenters and attendees, followed by expectations from future generations.",
    "code_link": ""
  },
  "cvpr2018_w36_autonomousdetectionofdisruptionsintheintensivecareunitusingdeepmaskr-cnn": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Autonomous Detection of Disruptions in the Intensive Care Unit Using Deep Mask R-CNN",
    "authors": [
      "Kumar Rohit Malhotra",
      "Anis Davoudi",
      "Scott Siegel",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Malhotra_Autonomous_Detection_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Malhotra_Autonomous_Detection_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Patients staying in the Intensive Care Unit (ICU) have a severely disrupted circadian rhythm. Due to patients' critical medical condition, ICU physicians and nurses have to provide round-the-clock clinical care, further disrupting patients' circadian rhythm. Mistimed family visits during rest-time can also disrupt patients' circadian rhythm. Currently, such effects are only reported based on hospital visitation policies rather than the actual number of visitors and care providers in the room. To quantify visitation disruptions, we used a deep Mask R-CNN model, a deep learning framework for object instance segmentation to detect and quantify the number of individuals in the ICU unit. This study represents the first effort to automatically quantify visitations in an ICU room, which could have implications in terms of policy adjustment, as well as circadian rhythm investigation. Our model achieved precision of 0.97 and recall of 0.67, with F1 score of 0.79 for detecting disruptions in the ICU units.",
    "code_link": ""
  },
  "cvpr2018_w36_encapsulatingtheimpactoftransferlearning,domainknowledgeandtrainingstrategiesindeep-learningbasedarchitectureabiometricbasedcasestudy": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Encapsulating the Impact of Transfer Learning, Domain Knowledge and Training Strategies in Deep-Learning Based Architecture: A Biometric Based Case Study",
    "authors": [
      "Avantika Singh",
      "Aditya Nigam"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Singh_Encapsulating_the_Impact_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Singh_Encapsulating_the_Impact_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, efforts have been made to analyze the impact of training strategies, transfer learning and domain knowledge on two biometric-based problems namely: three class oculus classification and fingerprint sensor classification. For analyzing these problems we have considered deep-learning based architecture and evaluated our results on benchmark contact-lens datasets like IIIT-D, ND, IIT-K (our model is publicly available) and on fingerprint datasets like FVC-2002, FVC-2004, FVC-2006, IIITD-MOLF. In-depth feature analysis of various proposed deep-learning models has been done in order to infer that indeed training in different ways along with transfer learning and domain knowledge plays a vital role in deciding the learning ability of any network.",
    "code_link": ""
  },
  "cvpr2018_w36_cross-domainfashionimageretrieval": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Cross-Domain Fashion Image Retrieval",
    "authors": [
      "Bojana Gajic",
      "Ramon Baldrich"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Gajic_Cross-Domain_Fashion_Image_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Gajic_Cross-Domain_Fashion_Image_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Cross domain image retrieval is a challenging task that implies matching images from one domain to their pairs from another domain. In this paper we focus on fashion image retrieval, which involves matching an image of a fashion item taken by users, to the images of the same item taken in controlled condition, usually by professional photographer. When facing this problem, we have different products in train and test time, and we use triplet loss to train the network. We stress the importance of proper training of simple architecture, as well as adapting general models to the specific task.",
    "code_link": ""
  },
  "cvpr2018_w36_wordspottinginsceneimagesbasedoncharacterrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Word Spotting in Scene Images Based on Character Recognition",
    "authors": [
      "Dena Bazazian",
      "Dimosthenis Karatzas",
      "Andrew D. Bagdanov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Bazazian_Word_Spotting_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Bazazian_Word_Spotting_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper we address the problem of unconstrained Word Spotting in scene images. We train a Fully Convolutional Network to produce heatmaps of all the character classes. Then, we employ the Text Proposals approach and, via a rectangle classifier, detect the most likely rectangle for each query word based on the character attribute maps. We evaluate the proposed method on ICDAR2015 and show that it is capable of identifying and recognizing query words in natural scene images.",
    "code_link": ""
  },
  "cvpr2018_w36_aholisticframeworkforaddressingtheworldusingmachinelearning": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "A Holistic Framework for Addressing the World Using Machine Learning",
    "authors": [
      "Ilke Demir"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Demir_A_Holistic_Framework_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Demir_A_Holistic_Framework_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Millions of people are disconnected from basic services due to lack of adequate addressing. We propose an automatic generative algorithm to create street addresses from satellite imagery. Our addressing scheme is coherent with the street topology, linear and hierarchical to follow human perception, and universal to be used as a unified geocoding system. Our algorithm starts with extracting road segments using deep learning and partitions the road network into regions. Then regions, streets, and address cells are named using proximity computations. We also extend our addressing scheme to cover inaccessible areas, to be flexible for changes, and to lead as a pioneer for a unified geodatabase.",
    "code_link": "https://github.com/facebookresearch/street-addresses"
  },
  "cvpr2018_w36_iknowhowyoufeelemotionrecognitionwithfaciallandmarks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "I Know How You Feel: Emotion Recognition With Facial Landmarks",
    "authors": [
      "Ivona Tautkute",
      "Tomasz Trzcinski",
      "Adam Bielski"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Tautkute_I_Know_How_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Tautkute_I_Know_How_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Classification of human emotions remains an important and challenging task for many computer vision algorithms, especially in the era of humanoid robots which coexist with humans in their everyday life. Currently proposed methods for emotion recognition solve this task using multi-layered convolutional networks that do not explicitly infer any facial features in the classification phase. In this work, we postulate a fundamentally different approach to solve emotion recognition task that relies on incorporating facial landmarks as a part of the classification loss function. To that end, we extend a recently proposed Deep Alignment Network (DAN), that achieves state-of-the-art results in the recent facial landmark recognition challenge, with a term related to facial features. Thanks to this simple modification, our model called EmotionalDAN is able to outperform state-of-the-art emotion classification methods on two challenging benchmark dataset by up to 5%.",
    "code_link": "https://github.com/co60ca/EmotionNet"
  },
  "cvpr2018_w36_earlydiagnosisofalzheimersdiseaseaneuroimagingstudywithdeeplearningarchitectures": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Early Diagnosis of Alzheimer's Disease: A Neuroimaging Study With Deep Learning Architectures",
    "authors": [
      "Jyoti Islam",
      "Yanqing Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Islam_Early_Diagnosis_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Islam_Early_Diagnosis_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Alzheimer's Disease is an incurable, progressive neurological brain disorder. Early diagnosis of Alzheimer's Disease can help with proper treatment and prevent brain tissue damage. Several statistical and machine learning models have been exploited by researchers for Alzheimer's Disease diagnosis. Detection of Alzheimer's Disease is exacting due to the similarity in Alzheimer's Disease Magnetic Resonance Imaging (MRI) data and standard healthy MRI data of older people. Recently, advanced deep learning techniques have successfully demonstrated human-level performance in numerous fields including medical image analysis. We propose a deep convolutional neural network for Alzheimer's Disease diagnosis using brain MRI data analysis. We have conducted ample experiments to demonstrate that our proposed model outperforms comparative baselines on the Open Access Series of Imaging Studies (OASIS) dataset.",
    "code_link": ""
  },
  "cvpr2018_w36_cosmeticfeaturesextractionbyasingleimagemakeupdecomposition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Cosmetic Features Extraction by a Single Image Makeup Decomposition",
    "authors": [
      "Kanami Yamagishi",
      "Shintaro Yamamoto",
      "Takuya Kato",
      "Shigeo Morishima"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Yamagishi_Cosmetic_Features_Extraction_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Yamagishi_Cosmetic_Features_Extraction_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In recent years, a large number of makeup images have been shared on social media. Most of these images lack information about the cosmetics used, such as color, glitter or etc., while they are difficult to infer due to the diversity of skin color or lighting conditions. In this paper, our goal is to estimate cosmetic features only from a single makeup image. Previous work has measured the material parameters of cosmetic products from pairs of images showing the face with and without makeup, but such comparison images are not always available. Furthermore, this method cannot represent local effects such as pearl or glitter since they adapted physically-based reflectance models. We propose a novel image-based method to extract cosmetic features considering both color and local effects by decomposing the target image into makeup and skin color using Difference of Gaussian (DoG). Our method can be applied for single, standalone makeup images, and considers both local effects and color. In addition, our method is robust to the skin color difference due to the decomposition separating makeup from skin. The experimental results demonstrate that our method is more robust to skin color difference and captures characteristics of each cosmetic product.",
    "code_link": ""
  },
  "cvpr2018_w36_automaticlarge-scale3dbuildingshaperefinementusingconditionalgenerativeadversarialnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Automatic Large-Scale 3D Building Shape Refinement Using Conditional Generative Adversarial Networks",
    "authors": [
      "Ksenia Bittner",
      "Marco Korner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Bittner_Automatic_Large-Scale_3D_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Bittner_Automatic_Large-Scale_3D_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Three-dimensional realistic representations of buildings in urban environments have been increasingly applied as data sources in a growing number of remote sensing fields such as urban planning and city management, navigation, environmental simulation (i.e. flood, earthquake, air pollution), 3D change detection after events like natural disasters or conflicts, etc. With recent technological developments, it becomes possible to acquire high-quality 3D input data. There are two main ways to obtain elevation information: from active remote sensing systems, such as light detection and ranging (LIDAR), and from passive remote sensing systems, such as optical images, which allow the acquisition of stereo images for automatic digital surface models (DSMs) generation. Although airborne laser scanning provides very accurate DSMs, it is a costly method. On the other hand, the DSMs from stereo satellite imagery show a large coverage and lower costs. However, they are not as accurate as LIDAR DSMs. With respect to automatic 3D information extraction, the availability of accurate and detailed DSMs is a crucial issue for automatic 3D building model reconstruction. We present a novel methodology for generating a better-quality stereo DSM with refined buildings shapes using a deep learning framework. To this end, a conditional generative adversarial network (cGAN) is trained to generate accurate LIDAR DSM-like height images from noisy stereo DSMs.",
    "code_link": ""
  },
  "cvpr2018_w36_sampushingthelimitsofsaliencypredictionmodels": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "SAM: Pushing the Limits of Saliency Prediction Models",
    "authors": [
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Giuseppe Serra",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Cornia_SAM_Pushing_the_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Cornia_SAM_Pushing_the_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The prediction of human eye fixations has been recently gaining a lot of attention thanks to the improvements shown by deep architectures. In our work, we go beyond classical feed-forward networks to predict saliency maps and propose a Saliency Attentive Model which incorporates neural attention mechanisms to iteratively refine predictions. Experiments demonstrate that the proposed strategy overcomes by a considerable margin the state of the art on the largest dataset available for saliency prediction. Here, we provide experimental results on other popular saliency datasets to confirm the effectiveness and the generalization capabilities of our model, which enable us to reach the state of the art on all considered datasets.",
    "code_link": ""
  },
  "cvpr2018_w36_rpifieldanewdatasetfortemporallyevaluatingpersonre-identification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "RPIfield: A New Dataset for Temporally Evaluating Person Re-Identification",
    "authors": [
      "Meng Zheng",
      "Srikrishna Karanam",
      "Richard J. Radke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Zheng_RPIfield_A_New_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Zheng_RPIfield_A_New_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The operational aspects of real-world human re-identification are typically oversimplified in academic research. Specifically, re-id algorithms are evaluated by matching probe images to candidates from a fixed gallery collected at the end of a video, ignoring the arrival time of each candidate. However, in real-world applications like crime prevention, a re-id system would likely operate in real time, and might be in continuous operation for several days.It would be natural to provide the user of such a system with instantaneous ranked lists from the current gallery candidates rather than waiting for a collective list after processing the whole video sequence.Re-id algorithms thus need to be evaluated based on their temporal performance on a dynamic gallery populated by an increasing number of candidates (some of whom may return several times over a long duration).This aspect of the problem is difficult to study with current benchmarking re-id datasets since they lack time-stamp information. In this paper, we introduce a new multi-shot re-id dataset, called RPIfield, which provides explicit time-stamp information for each candidate. The RPIfield dataset is comprised of 12 outdoor camera videos, with 112 known actors walking along pre-specified paths among about 4000 distractors. Each actor in RPIfield has multiple reappearances in one or more camera views, which allows the study of re-id algorithms in a more general context, especially with respect to temporal aspects.",
    "code_link": ""
  },
  "cvpr2018_w36_large-scaleecologicalanalysesofanimalsinthewildusingcomputervision": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Large-Scale Ecological Analyses of Animals in the Wild Using Computer Vision",
    "authors": [
      "Mikayla Timm",
      "Subhransu Maji",
      "Todd Fuller"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Timm_Large-Scale_Ecological_Analyses_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Timm_Large-Scale_Ecological_Analyses_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Camera traps are increasingly being deployed by ecologists and citizen-scientists as a cost-effective way of obtaining large amounts of animal images in the wild. In order to analyze this data, the images are labeled manually by ecologists, where they identify species of animals and more fine-grained details, such as animal sex or age, or even individual animal identities. However, with the number of camera trap images quickly outgrowing the capacity of the labelers, ecologists are unable to keep up with the wealth of data they are obtaining. Using computer vision, we can automatically generate labels for new camera trap images at the rate that they are being obtained, allowing ecologists to uncover ecological and biological information at a scale previously not possible. In this paper, we explore computer vision approaches for species identification in camera trap images and for individual jaguar identification, both of which show promising results. We make this novel dataset publicly available for future research directions and further exploration.",
    "code_link": ""
  },
  "cvpr2018_w36_discoveringstyletrendsthroughdeepvisuallyawarelatentitemembeddings": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Discovering Style Trends Through Deep Visually Aware Latent Item Embeddings",
    "authors": [
      "Murium Iqbal",
      "Adair Kovac",
      "Kamelia Aryafar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Iqbal_Discovering_Style_Trends_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Iqbal_Discovering_Style_Trends_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we explore Latent Dirichlet Allocation (LDA) and Polylingual Latent Dirichlet Allocation (PolyLDA), as a means to discover trending styles in Overstock from deep visual semantic features transferred from a pretrained convolutional neural network and text-based item attributes. To utilize deep visual semantic features in conjunction with LDA, we develop a method for creating a bag of words representation of unrolled image vectors. By viewing the channels within the convolutional layers of a Resnet-50 as being representative of a word, we can index these activations to create visual documents.We then train LDA over these documents to discover the latent style in the images. We also incorporate text-based data with PolyLDA, where each representation is viewed as an independent language attempting to describe the same style. The resulting topics are shown to be excellent indicators of visual style across our platform.",
    "code_link": ""
  },
  "cvpr2018_w36_towardsmoreaccurateradiotelescopeimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "Towards More Accurate Radio Telescope Images",
    "authors": [
      "Nezihe Merve Gurel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Gurel_Towards_More_Accurate_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Gurel_Towards_More_Accurate_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Radio interferometry usually compensates for high levels of noise in sensor/antenna electronics by throwing data and energy at the problem: observe longer, then store and process it all. We propose instead a method to remove the noise explicitly before imaging. To this end, we developed an algorithm that first decomposes the instances of antenna correlation matrix, the so-called visibility matrix, into additive components using Singular Spectrum Analysis and then cluster these components using graph Laplacian matrix. We show through simulation the potential for radio astronomy, in particular, illustrating the benefit for LOFAR, the low frequency array in Netherlands. Least-squares images are estimated with far higher accuracy with low computation cost without the need for long observation time.",
    "code_link": ""
  },
  "cvpr2018_w36_arcadversarialrobustcutsforsemi-supervisedandmulti-labelclassification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W36",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Women in Computer Vision",
    "title": "ARC: Adversarial Robust Cuts for Semi-Supervised and Multi-Label Classification",
    "authors": [
      "Sima Behpour"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w36/html/Behpour_ARC_Adversarial_Robust_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w36/Behpour_ARC_Adversarial_Robust_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Many structured prediction tasks arising in computer vision and natural language processing tractably reduce to makingminimumcostcutsingraphswithedgeweights learned using maximum margin methods. Unfortunately, the hinge loss used to construct these methods often provides aparticularlylooseboundonthelossfunctionofinter-est (e.g., the Hamming loss). We develop Adversarial Ro-bust Cuts (ARC), an approach that poses the learning task as a minimax game between predictor and \"label approximator\" based on minimum cost graph cuts. Unlike maximum margin methods, this game-theoretic perspective always provides meaningful bounds on the Hamming loss. We conduct multi-label and semi-supervised binary prediction experiments that demonstrate the benefits of our approach.",
    "code_link": ""
  },
  "cvpr2018_w39_vis-hudusingvisualsaliencytoimprovehumandetectionwithconvolutionalneuralnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "ViS-HuD: Using Visual Saliency to Improve Human Detection With Convolutional Neural Networks",
    "authors": [
      "Vandit Gajjar",
      "Yash Khandhediya",
      "Ayesha Gurnani",
      "Viraj Mavani",
      "Mehul S. Raval"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Gajjar_ViS-HuD_Using_Visual_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Gajjar_ViS-HuD_Using_Visual_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The paper presents a technique to improve human detection in still images using deep learning. Our novel method, ViS-HuD, computes visual saliency map from the image. Then the input image is multiplied by the map and product is fed to the Convolutional Neural Network (CNN) which detects humans in the image. A visual saliency map is generated using ML-Net and human detection is carried out using DetectNet. ML-Net is pre-trained on SALICON while, DetectNet is pre-trained on ImageNet database for visual saliency detection and image classification respectively. The CNNs of ViS-HuD were trained on two challenging databases - Penn Fudan and TUD-Brussels Benchmark. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on Penn Fudan Dataset with 91.4% human detection accuracy and it achieves average miss-rate of 53% on the TUD-Brussels benchmark.",
    "code_link": ""
  },
  "cvpr2018_w39_learningbiomimeticperceptionforhumansensorimotorcontrol": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Learning Biomimetic Perception for Human Sensorimotor Control",
    "authors": [
      "Masaki Nakada",
      "Honglin Chen",
      "Demetri Terzopoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Nakada_Learning_Biomimetic_Perception_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Nakada_Learning_Biomimetic_Perception_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We introduce a biomimetic simulation framework for human perception and sensorimotor control. Our framework features a biomechanically simulated musculoskeletal human model actuated by numerous skeletal muscles, with two human-like eyes whose retinas contain spatially nonuniform distributions of photoreceptors. Its prototype sensorimotor system comprises a set of 20 automatically-trained deep neural networks (DNNs), half of which comprise the neuromuscular motor control subsystem, whereas the other half are devoted to the visual perception subsystem. Directly from the photoreceptor responses, 2 perception DNNs control eye and head movements, while 8 DNNs extract the perceptual information needed to control the arms and legs. Thus, driven exclusively by its egocentric, active visual perception, our virtual human is capable of learning efficient, online visuomotor control of its eyes, head, and four limbs to perform a nontrivial task involving the foveation and visual persuit of a moving target object coupled with visuallyguided reaching actions to intercept the incoming target.",
    "code_link": ""
  },
  "cvpr2018_w39_assessingshapebiaspropertyofconvolutionalneuralnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Assessing Shape Bias Property of Convolutional Neural Networks",
    "authors": [
      "Hossein Hosseini",
      "Baicen Xiao",
      "Mayoore Jaiswal",
      "Radha Poovendran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Hosseini_Assessing_Shape_Bias_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Hosseini_Assessing_Shape_Bias_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " It is known that humans display \"shape bias\" when classifying new items, i.e., they prefer to categorize objects based on their shape rather than color. Convolutional Neural Networks (CNNs) are also designed to take into account the spatial structure of image data. In fact, experiments on image datasets, consisting of triples of a probe image, a shape-match and a color-match, have shown that one-shot learning models display shape bias as well.In this paper, we examine the shape bias property of CNNs. In order to conduct large scale experiments, we propose using the model accuracy on images with reversed brightness as a metric to evaluate the shape bias property. Such images, called negative images, contain objects that have the same shape as original images, but with different colors. Through extensive systematic experiments, we investigate the role of different factors, such as training data, model architecture, initialization and regularization techniques, on the shape bias property of CNNs. We show that it is possible to design different CNNs that achieve similar accuracy on original images, but perform significantly different on negative images, suggesting that CNNs do not intrinsically display shape bias. We then show that CNNs are able to learn and generalize the structures, when the model is properly initialized or data is properly augmented, and if batch normalization is used.",
    "code_link": ""
  },
  "cvpr2018_w39_deep-bcndeepnetworksmeetbiasedcompetitiontocreateabrain-inspiredmodelofattentioncontrol": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Deep-BCN: Deep Networks Meet Biased Competition to Create a Brain-Inspired Model of Attention Control",
    "authors": [
      "Hossein Adeli",
      "Gregory Zelinsky"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Adeli_Deep-BCN_Deep_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Adeli_Deep-BCN_Deep_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The mechanism of attention control is best described by biased-competition theory (BCT), which suggests that a top-down goal state biases a competition among object representations for the selective routing of a visual input for classification. Our work advances this theory by making it computationally explicit as a deep neural network (DNN) model, thereby enabling predictions of goal-directed attention control using real-world stimuli. This model, which we call Deep-BCN, is built on top of an 8-layer DNN pre-trained for object classification, but has layers mapped to early visual (V1, V2/V3, V4), ventral (PIT, AIT), and frontal (PFC) brain areas that have their functional connectivity informed by BCT. Deep-BCN also has a superior colliculus and a frontal-eye field, and can therefore make eye movements. We compared Deep-BCN's eye movements to those made from 15 people performing a categorical search for one of 25 target object categories, and found that it predicted both the number of fixations during search and the saccade-distance travelled before search termination. With Deep-BCN a DNN implementation of BCT now exists, which can be used to predict the neural and behavioral responses of an attention control mechanism as it mediates a goal-directed behavior--in our study the eye movements made in search of a target goal.",
    "code_link": ""
  },
  "cvpr2018_w39_imagecaptiongenerationwithhierarchicalcontextualvisualspatialattention": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Image Caption Generation With Hierarchical Contextual Visual Spatial Attention",
    "authors": [
      "Mahmoud Khademi",
      "Oliver Schulte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Khademi_Image_Caption_Generation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Khademi_Image_Caption_Generation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a novel context-aware attention-based deep architecture for image caption generation. Our architecture employs a Bidirectional Grid LSTM, which takes visual features of an image as input and learns complex spatial patterns based on two-dimensional context, by selecting or ignoring its input. The Grid LSTM has not been applied to image caption generation task before. Another novel aspect is that we leverage a set of local region-grounded texts obtained by transfer learning. The region-grounded texts often describe the properties of the objects and their relationships in an image. To generate a global caption for the image, we integrate the spatial features from the Grid LSTM with the local region-grounded texts, using a two-layer Bidirectional LSTM. The first layer models the global scene context such as object presence. The second layer utilizes a novel dynamic spatial attention mechanism, based on another Grid LSTM, to generate the global caption word-by-word, while considering the caption context around a word in both directions. Unlike recent models that use a soft attention mechanism, our dynamic spatial attention mechanism considers the spatial context of the image regions. Experimental results on MS-COCO dataset show that our architecture outperforms the state-of-the-art.",
    "code_link": ""
  },
  "cvpr2018_w39_estimatingattentionoffacesduetoitsgrowinglevelofemotions": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Estimating Attention of Faces Due to Its Growing Level of Emotions",
    "authors": [
      "Ravi Kant Kumar",
      "Jogendra Garain",
      "Dakshina Ranjan Kisku",
      "Goutam Sanyal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Kumar_Estimating_Attention_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Kumar_Estimating_Attention_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": "In the task of attending faces in the disciplined assembly (Like in examination hall or Silent public places), our gaze automatically goes towards those persons who exhibits their expression other than the normal expression. It happens due to finding of dissimilar expression among the gathering of normal. In order to modeling this concept in the intelligent vision of computer system, hardly some effective researches have been succeeded. Therefore, in this proposal we have tried to come out with a solution for handling such challenging task of computer vision. Actually, this problem is related to cognitive aspect of visual attention. In the literature of visual saliency authors have dealt with expressionless objects but it has not been addressed with object like face which exploits expressions. Visual saliency is a term which differentiates \"appealing\" visual substance from others, based on their feature differences. In this paper, in the set of multiple faces, 'Salient face' has been explored based on 'emotion deviation' from the normal. In the first phase of the experiment, face detection task has been accomplished using Viola Jones face detector. The concept of deep convolution neural network (CNN) has been applied for training and classification of different facial expression of emotions. Moreover, saliency score of every face of the input image have been computed by measuring their 'emotion score' which depends upon the deviation from the 'normal expression' scores. This proposed approach exhibits fairly good result which may give a new dimension to the researchers towards the modeling of an intelligent vision system which can be useful in the task of visual security and surveillance.",
    "code_link": ""
  },
  "cvpr2018_w39_totallylookslike-howhumanscompare,comparedtomachines": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Totally Looks Like - How Humans Compare, Compared to Machines",
    "authors": [
      "Amir Rosenfeld",
      "Markus D. Solbach",
      "John K. Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Rosenfeld_Totally_Looks_Like_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Rosenfeld_Totally_Looks_Like_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Perceptual judgment of image similarity by humans relies on rich internal representations ranging from low-level features to high-level concepts, scene properties and even cultural associations. Existing methods and datasets attempting to explain perceived similarity use stimuli which arguably do not cover the full breadth of factors that affect human similarity judgments, even those geared toward this goal. We introduce a new dataset dubbed Totally-Looks-Like (TLL) after a popular entertainment website, which contains images paired by humans as being visually similar. The dataset contains 6016 image-pairs from the wild, shedding light upon a rich and diverse set of criteria employed by human beings. We conduct experiments to try to reproduce the pairings via features extracted from state-of-the-art deep convolutional neural networks, as well as additional human experiments to verify the consistency of the collected data. Though we create conditions to artificially make the matching task increasingly easier, we show that machine-extracted representations perform very poorly in terms of reproducing the matching selected by humans. The results suggest future directions for improvement of learned image representations.",
    "code_link": "https://github.com/ageitgey/face_recognition"
  },
  "cvpr2018_w39_fusingvisualsaliencyformaterialrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Fusing Visual Saliency for Material Recognition",
    "authors": [
      "Lin Qi",
      "Ying Xu",
      "Xiaowei Shang",
      "Junyu Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Qi_Fusing_Visual_Saliency_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Qi_Fusing_Visual_Saliency_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Material recognition is researched in both computer vision and vision science fields. In this paper, we investigated how humans observe material images and found the eye fixation information improves the performance of material image classification models. We first collected eye-tracking data from human observers and used it to fine-tune a generative adversarial network for saliency prediction (SalGAN). We then fused the predicted saliency map with material images, and fed them to CNN models for material classification. The experiment results show that the classification accuracy is improved than those using original images. This indicates that human's visual cues could benefit computational models as priors.",
    "code_link": ""
  },
  "cvpr2018_w39_increasingvideosaliencymodelgeneralizabilitybytrainingforsmoothpursuitprediction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Increasing Video Saliency Model Generalizability by Training for Smooth Pursuit Prediction",
    "authors": [
      "Mikhail Startsev",
      "Michael Dorr"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Startsev_Increasing_Video_Saliency_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Startsev_Increasing_Video_Saliency_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Saliency prediction even for videos is traditionally associated with fixation prediction. Unlike images, however, videos also induce smooth pursuit eye movements, for example when a salient object is moving and is tracked across the video surface. Nevertheless, current saliency data sets and models mostly ignore pursuit, either by combining it with fixations, or discarding the respective samples. In this work, we utilize a state-of-the-art smooth pursuit detector and a Slicing Convolutional Neural Network (S-CNN) to train two saliency models, one targeting fixation prediction and the other targeting smooth pursuit. We hypothesize that pursuit-salient video parts would generalize better, since the motion patterns should be relatively similar across data sets. To test this, we consider an independent video saliency data set, where no pursuit-fixation differentiation is performed. In our experiments, the pursuit-targeting model outperforms several state-of-the-art saliency algorithms on both the test part of our main data set and the additionally considered data set.",
    "code_link": ""
  },
  "cvpr2018_w39_representationofcategoriesinfiltersofdeepneuralnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Representation of Categories in Filters of Deep Neural Networks",
    "authors": [
      "Katerina Malakhova"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Malakhova_Representation_of_Categories_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Malakhova_Representation_of_Categories_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Transparency in decision-making is an essential aspect of the secure and unbiased application of deep learning for classification problems. Neural networks pre-trained on one dataset can serve as feature extractors to solve various tasks. In this work, I study how categories are represented in latent space of neural networks using an example of face recognition by a network trained without an explicit category for the human person. I propose a semantic-based approach to determine if a model has pre-trained filters for a given set of classes of interest and which layer is better suited for feature extraction. The method is similar to category-selectivity measures used in neuroscience to estimate tuning curves of neurons in high-level areas of the visual cortex.",
    "code_link": ""
  },
  "cvpr2018_w39_usingpsychophysicalmethodstounderstandmechanismsoffaceidentificationinadeepneuralnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Using Psychophysical Methods to Understand Mechanisms of Face Identification in a Deep Neural Network",
    "authors": [
      "Tian Xu",
      "Oliver Garrod",
      "Steven H. Scholte",
      "Robin Ince",
      "Philippe G. Schyns"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Xu_Using_Psychophysical_Methods_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Xu_Using_Psychophysical_Methods_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep Convolutional Neural Networks (CNNs) have been one of the most influential recent developments in computer vision, particularly for categorization. The promise of CNNs is at least two-fold.First, they represent the best engineering solution to successfully tackle the foundational task of visual categorization with a performance level that even exceeds that of humans. Second, for computational neuroscience, CNNs provide a testable modelling platform for visual categorizations inspired by the multi-layered organization of visual cortex. Here, we used a 3D generative model to control the variance of information learned to identify 2,000 face identities in one CNN architecture (10-layer ResNet). We generated 25M face images to train the network by randomly sampling intrinsic (i.e. face morphology, gender, age, expression and ethnicity) and extrinsic factors of face variance (i.e. 3D pose, illumination, scale and 2D translation). At testing, the network performed with 99% generalization accuracy for face identity across variations of intrinsic and extrinsic factors. State-of-the-art information mapping techniques from psychophysics (i.e. Representational Similarity Analysis and Bubbles) revealed respectively the network layer at which factors of variance are resolved and the face features that are used for identity. By explicitly controlling the generative factors of face information, we provide an alternative framework based on human psychophysics to understand information processing in CNNs.",
    "code_link": ""
  },
  "cvpr2018_w39_relatingdeepneuralnetworkrepresentationstoeeg-fmrispatiotemporaldynamicsinaperceptualdecision-makingtask": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Relating Deep Neural Network Representations to EEG-fMRI Spatiotemporal Dynamics in a Perceptual Decision-Making Task",
    "authors": [
      "Tao Tu",
      "Jonathan Koss",
      "Paul Sajda"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Tu_Relating_Deep_Neural_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Tu_Relating_Deep_Neural_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The hierarchical architecture of deep convolutional neural networks (CNN) resembles the multi-level processing stages of the human visual system during object recognition. Converging evidence suggests that this hierarchical organization is key to the CNN achieving human-level performance in object categorization. In this paper, we leverage the hierarchical organization of the CNN to investigate the spatiotemporal dynamics of rapid visual processing in the human brain. Specifically we focus on perceptual decisions associated with different levels of visual ambiguity. Using simultaneous EEG-fMRI, we demonstrate the temporal and spatial hierarchical correspondences between the multi-stage processing in CNN and the activity observed in the EEG and fMRI. The hierarchical correspondence suggests a processing pathway during rapid visual decision-making that involves the interplay between sensory regions, the default mode network (DMN) and the frontal-parietal control network (FPCN).",
    "code_link": ""
  },
  "cvpr2018_w39_scenegrammarinhumanandmachinerecognitionofobjectsandscenes": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Scene Grammar in Human and Machine Recognition of Objects and Scenes",
    "authors": [
      "Akram Bayat",
      "Do Hyong Koh",
      "Anubhaw Kumar Nand",
      "Marta Pereira",
      "Marc Pomplun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Bayat_Scene_Grammar_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Bayat_Scene_Grammar_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we study the effects of violating the high level scene syntactic and semantic rules on human eye-movement behavior and deep neural scene and object recognition networks. An eye-movement experimental study was conducted with twenty human subjects to view scenes from the SCEGRAM image database and determine whether there is an inconsistent object or not. We examine the contribution of multiple types of features that influence eye movements while searching for an inconsistent object in a scene (e.g., size and location of an object) by evaluating the consistency prediction power of the trained classifiers on fixation features. The results of the eye movement analysis and inconsistency prediction reveal that: 1) inconsistent objects are fixated significantly more than consistent objects in a scene, 2) the distribution of fixations is the main factor that is influenced by the inconsistency condition of a scene which is reflected in the ground truth fixation maps. It is also observed that the performance of deep object and scene recognition networks drops due to the violations of scene grammar.The class-specific visual saliency maps are created from the high-level representation of the convolutional layers of a deep network during the scene and object recognition process. We discuss whether the scene inconsistencies are represented in those saliency maps by evaluating their prediction powers using multiple well-known metrics including AUC, SIM, and KL. The results suggest that an inconsistent object in a scene causes significant variations in the prediction power of saliency maps.",
    "code_link": ""
  },
  "cvpr2018_w39_audio-visualtemporalsaliencymodelingvalidatedbyfmridata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Audio-Visual Temporal Saliency Modeling Validated by fMRI Data",
    "authors": [
      "Petros Koutras",
      "Georgia Panagiotaropoulou",
      "Antigoni Tsiami",
      "Petros Maragos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Koutras_Audio-Visual_Temporal_Saliency_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Koutras_Audio-Visual_Temporal_Saliency_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work we propose an audio-visual model for pre- dicting temporal saliency in videos, that we validate and evaluate in an alternative way by employing fMRI data. We intend to bridge the gap between the large improve- ments achieved during the last years in computational mod- eling, especially in deep learning, and the neurobiological and behavioral research regarding human vision. The pro- posed audio-visual model incorporates both state-of-the-art deep architectures for visual saliency, which were trained on eye-tracking data, and behavioral findings concerning audio-visual integration in multimedia stimuli. A new fMRI database has been collected for evaluation purposes, that includes various videos and subjects. This dataset may prove useful not only for saliency but for other computer vision problems as well. The evaluation of our model us- ing the new fMRI database under a mixed-effect analysis shows that the proposed saliency model has strong cor- relation with both the visual and audio brain areas, that confirms its effectiveness and appropriateness in predicting audio-visual saliency for dynamic stimuli.",
    "code_link": ""
  },
  "cvpr2018_w39_primingneuralnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W39",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Mutual Benefits of Cognitive and Computer Vision: How Can We Use One to Understand the Other?",
    "title": "Priming Neural Networks",
    "authors": [
      "Amir Rosenfeld",
      "Mahdi Biparva",
      "John K. Tsotsos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w39/html/Rosenfeld_Priming_Neural_Networks_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w39/Rosenfeld_Priming_Neural_Networks_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Visual priming is known to affect the human visual system to allow detection of scene elements, even those that may have been near unnoticeable before, such as the presence of camouflaged animals. This process has been shown to be an effect of top-down signaling in the visual system triggered by the said cue. In this paper, we propose a mechanism to mimic the process of priming in the context of object detection and segmentation. We view priming as having a modulatory, cue dependent effect on layers of features within a network. Our results show how such a process can be complementary to, and at times more effective than simple post-processing applied to the output of the network, notably so in cases where the object is hard to detect such as in severe noise, small size or atypical appearance. Moreover, we find the effects of priming are sometimes stronger when early visual layers are affected. Overall, our experiments confirm that top-down signals can go a long way in improving object detection and segmentation.",
    "code_link": ""
  },
  "cvpr2018_w40_visdaasynthetic-to-realbenchmarkforvisualdomainadaptation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "VisDA: A Synthetic-to-Real Benchmark for Visual Domain Adaptation",
    "authors": [
      "Xingchao Peng",
      "Ben Usman",
      "Neela Kaushik",
      "Dequan Wang",
      "Judy Hoffman",
      "Kate Saenko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Peng_VisDA_A_Synthetic-to-Real_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Peng_VisDA_A_Synthetic-to-Real_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present the Synthetic-to-Real Visual Domain Adaptation (VisDA) Benchmark, a large-scale testbed for unsupervised domain adaptation across visual domains. The VisDA dataset is focused on the simulation-to-reality shift and has two associated tasks: image classification and image segmentation. The goal in both tracks is to first train a model on simulated, synthetic data in the source domain and then adapt it to perform well on real image data in the unlabeled test domain. Our dataset is the largest one to date for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation and testing domains. The image segmentation dataset is also large-scale with over 30K images across 18 categories in the three domains. We compare VisDA to existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.",
    "code_link": ""
  },
  "cvpr2018_w40_paris-lille-3dapointclouddatasetforurbanscenesegmentationandclassification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Paris-Lille-3D: A Point Cloud Dataset for Urban Scene Segmentation and Classification",
    "authors": [
      "Xavier Roynard",
      "Jean-Emmanuel Deschaud",
      "Francois Goulette"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Roynard_Paris-Lille-3D_A_Point_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Roynard_Paris-Lille-3D_A_Point_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This article presents a dataset called Paris-Lille-3D. This dataset is composed of several point clouds of outdoor scenes in Paris and Lille, France, with a total of more than 140 million hand labeled and classified points with more than 50 classes (e.g., the ground, cars and benches). This dataset is large enough and of high enough quality to further research on techniques regarding the automatic classification of urban point clouds. The fields to which that research may be applied are vast, as it provides the ability to increase productivity in regards to the management of urban infrastructures. Moreover, this type of data has the potential to be crucial in the field of autonomous vehicles.",
    "code_link": ""
  },
  "cvpr2018_w40_newmetricsandexperimentalparadigmsforcontinuallearning": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "New Metrics and Experimental Paradigms for Continual Learning",
    "authors": [
      "Tyler L. Hayes",
      "Ronald Kemker",
      "Nathan D. Cahill",
      "Christopher Kanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Hayes_New_Metrics_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In order for a robotic agent to learn successfully in an uncontrolled environment, it must be able to immediately alter its behavior. Deep neural networks are the dominant approach for classification tasks in computer vision, but typical algorithms and architectures are incapable of immediately learning new tasks without catastrophically forgetting previously acquired knowledge. There has been renewed interest in solving this problem, but there are limitations to existing solutions, including poor performance compared to offline models, large memory footprints, and learning slowly. In this abstract, we formalize the continual learning paradigm and propose new benchmarks for assessing continual learning agents.",
    "code_link": ""
  },
  "cvpr2018_w40_action-conditionedconvolutionalfutureregressionmodelsforrobotimitationlearning": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Action-Conditioned Convolutional Future Regression Models for Robot Imitation Learning",
    "authors": [
      "Alan Wu",
      "AJ Piergiovanni",
      "Michael S. Ryoo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Wu_Action-Conditioned_Convolutional_Future_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Wu_Action-Conditioned_Convolutional_Future_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents convolutional neural network (CNN) architectures for robot action policy learning, and compares them for the robot imitation learning from unlabeled example videos. These not only include standard behavioral cloning but also action learning models with explicit future frame/representation regression. Our objective is to make the robot learn to visually imagine the future consequences of taking an action from a number of example videos, and take advantage of it to learn the optimal behavior. We introduce the approach of decomposing an action model into a convolutional encoder-decoder as well as an action-conditioned future regressor, and present an approach to train them jointly. Our real-time experiments with a ground mobility robot explicitly compare different CNN models for imitation learning, and the results confirm that the use of the action-conditioned future regression benefits the robot. We show both the qualitative results of future frame regression and the quantitative evaluation of robot actions.",
    "code_link": ""
  },
  "cvpr2018_w40_fallingthingsasyntheticdatasetfor3dobjectdetectionandposeestimation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation",
    "authors": [
      "Jonathan Tremblay",
      "Thang To",
      "Stan Birchfield"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Tremblay_Falling_Things_A_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Tremblay_Falling_Things_A_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a new dataset, called Falling Things (FAT), for advancing the state-of-the-art in object detection and 3D pose estimation in the context of robotics. By synthetically combining object models and backgrounds of complex composition and high graphical quality, we are able to generate photorealistic images with accurate 3D pose annotations for all objects in all images. Our dataset contains 60k annotated photos of 21 household objects taken from the YCB dataset. For each image, we provide the 3D poses, per-pixel class segmentation, and 2D/3D bounding box co- ordinates for all objects. To facilitate testing different input modalities, we provide mono and stereo RGB images, along with registered dense depth images. We describe in detail the generation process and statistical analysis of the data.",
    "code_link": ""
  },
  "cvpr2018_w40_learninginstancesegmentationbyinteraction": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Learning Instance Segmentation by Interaction",
    "authors": [
      "Deepak Pathak",
      "Yide Shentu",
      "Dian Chen",
      "Pulkit Agrawal",
      "Trevor Darrell",
      "Sergey Levine",
      "Jitendra Malik"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Pathak_Learning_Instance_Segmentation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Pathak_Learning_Instance_Segmentation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present an approach for building an active agent that learns to segment its visual observations into individual objects by interacting with its environment in a completely self-supervised manner. The agent uses its current segmentation model to infer pixels that constitute objects and refines the segmentation model by interacting with these pixels. The model learned from over 50K interactions generalizes to novel objects and backgrounds. Data collection by interaction is natural and a noisy source of information. We propose a robust set loss to deal with noisy training signal and provide a benchmark dataset comprising robot interactions with few human labeled examples for future research to build upon. We provide evidence that re-organization of visual observations into objects is a powerful representation for downstream vision-based control tasks. Our system is capable of rearranging multiple objects into target configurations from visual inputs alone. Full paper available at https://pathak22.github.io",
    "code_link": ""
  },
  "cvpr2018_w40_activevisiondatasetbenchmark": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Active Vision Dataset Benchmark",
    "authors": [
      "Phil Ammirato",
      "Alexander C. Berg",
      "Jana Kosecka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Ammirato_Active_Vision_Dataset_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Ammirato_Active_Vision_Dataset_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Several recent efforts in computer vision indicate a trend toward studying and understanding problems in larger scale environments, beyond single images, and focus on connections to tasks in navigation, mobile manipulation, and visual question answering.A common goal of these tasks is the capability of moving in the environment, acquiring novel views during perception and while performing a task. This capability comes easily in synthetic environments, however achieving the same effect with real images is much more laborious. We propose using the existing Active Vision Dataset to form a benchmark for such problems in a real-world settings with real images. The dataset is well suited for evaluating tasks of multiview active recognition, target driven navigation, and target search, and also can be effective for studying the transfer of strategies learned in simulation to real settings.",
    "code_link": ""
  },
  "cvpr2018_w40_zero-shotvisualimitation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Zero-Shot Visual Imitation",
    "authors": [
      "Deepak Pathak",
      "Parsa Mahmoudieh",
      "Guanghao Luo",
      "Pulkit Agrawal",
      "Dian Chen",
      "Yide Shentu",
      "Evan Shelhamer",
      "Jitendra Malik",
      "Alexei A. Efros",
      "Trevor Darrell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Pathak_Zero-Shot_Visual_Imitation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Pathak_Zero-Shot_Visual_Imitation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/",
    "code_link": ""
  },
  "cvpr2018_w40_embodiedquestionanswering": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W40",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Real World Challenges and New Benchmarks for Deep Learning in Robotic Vision",
    "title": "Embodied Question Answering",
    "authors": [
      "Abhishek Das",
      "Samyak Datta",
      "Georgia Gkioxari",
      "Stefan Lee",
      "Devi Parikh",
      "Dhruv Batra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w40/html/Das_Embodied_Question_Answering_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w40/Das_Embodied_Question_Answering_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (\"What color is the car?\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question (\"orange\").EmbodiedQA requires a range of AI skills -- language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments, evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning.",
    "code_link": ""
  },
  "cvpr2018_w41_recognizingamericansignlanguagegesturesfromwithincontinuousvideos": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Recognizing American Sign Language Gestures From Within Continuous Videos",
    "authors": [
      "Yuancheng Ye",
      "Yingli Tian",
      "Matt Huenerfauth",
      "Jingya Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Ye_Recognizing_American_Sign_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Ye_Recognizing_American_Sign_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose a novel hybrid model, 3D recurrent convolutional neural networks (3DRCNN), to recognize American Sign Language (ASL) gestures and localize their temporal boundaries within continuous videos, by fusing multi-modality features. Our proposed 3DRCNN model integrates 3D convolutional neural network (3DCNN) and enhanced fully connected recurrent neural network (FC-RNN), where 3DCNN learns multi-modality features from RGB, motion, and depth channels, and FC-RNN captures the temporal information among short video clips divided from the original video. Consecutive clips with the same semantic meaning are singled out by applying the sliding window approach to segment the clips on the entire video sequence. To evaluate our method, we collected a new ASL dataset which contains two types of videos: Sequence videos (in which a human performs a list of specific ASL words) and Sentence videos (in which a human performs ASL sentences, containing multiple ASL words). The dataset is fully annotated for each semantic region (i.e. the time duration of each word that the human signer performs) and contains multiple input channels. Our proposed method achieves 69.2% accuracy on the Sequence videos for 27 ASL words, which demonstrates its effectiveness of detecting ASL gestures from continuous videos.",
    "code_link": ""
  },
  "cvpr2018_w41_fine-grainedheadposeestimationwithoutkeypoints": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Fine-Grained Head Pose Estimation Without Keypoints",
    "authors": [
      "Nataniel Ruiz",
      "Eunji Chong",
      "James M. Rehg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Estimating the head pose of a person is a crucial prob- lem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the tar- get face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detec- tion performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to deter- mine pose by training a multi-loss convolutional neural net- work on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) di- rectly from image intensities through joint binned pose clas- sification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose meth- ods. We open-source our training and testing code as well as release our pre-trained models.",
    "code_link": "https://github.com/natanielruiz/deep-head-pose"
  },
  "cvpr2018_w41_generativeadversarialstyletransfernetworksforfaceaging": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Generative Adversarial Style Transfer Networks for Face Aging",
    "authors": [
      "Sveinn Palsson",
      "Eirikur Agustsson",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Palsson_Generative_Adversarial_Style_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Palsson_Generative_Adversarial_Style_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " How somebody looked like when younger? What could a person look like when 10 years older? In this paper we look at the problem of face aging, which relates to processing an image of a face to change its apparent age. This task involves synthesizing images and modeling the aging process, which both are problems that have recently enjoyed much research interest in the field of face and gesture recognition.We propose to look at the problem from the perspective of image style transfer, where we consider the age of the person as the underlying style of the image. We show that for large age differences, convincing face aging can be achieved by formulating the problem with a pairwise training of Cycle-consistent Generative Adversarial Networks (CycleGAN) over age groups. Furthermore, we propose a variant of CycleGAN which directly incorporates a pre-trained age prediction model, which performs better when the desired age difference is smaller.The proposed approaches are complementary in strengths and their fusion performs well for any desired level of aging effect. We quantitatively evaluate our proposed method through a user study and show that it outperforms prior state-of-the-art techniques for face aging.",
    "code_link": ""
  },
  "cvpr2018_w41_empiricallyanalyzingtheeffectofdatasetbiasesondeepfacerecognitionsystems": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems",
    "authors": [
      "Adam Kortylewski",
      "Bernhard Egger",
      "Andreas Schneider",
      "Thomas Gerig",
      "Andreas Morel-Forster",
      "Thomas Vetter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Kortylewski_Empirically_Analyzing_the_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Kortylewski_Empirically_Analyzing_the_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " It is unknown what kind of biases modern in the wild face datasets have because of their lack of annotation. A direct consequence of this is that total recognition rates alone only provide limited insight about the generalization ability of a Deep Convolutional Neural Networks (DCNNs). We propose to empirically study the effect of different types of dataset biases on the generalization ability of DCNNs. Using synthetically generated face images, we study the face recognition rate as a function of interpretable parameters such as face pose and light. The proposed method allows valuable details about the generalization performance of different DCNN architectures to be observed and compared. In our experiments, we find that: 1) Indeed, dataset bias has a significant influence on the generalization performance of DCNNs. 2) DCNNs can generalize surprisingly well to unseen illumination conditions and large sampling gaps in the pose variation. 3) Using the presented methodology we reveal that the VGG-16 architecture outperforms the AlexNet architecture at face recognition tasks because it can much better generalize to unseen face poses, although it has significantly more parameters. 4) We uncover a main limitation of current DCNN architectures, which is the difficulty to generalize when different identities to not share the same pose variation. 5) We demonstrate that our findings on synthetic data also apply when learning from real-world data. Our face image generator is publicly available to enable the community to benchmark other DCNN architectures.",
    "code_link": ""
  },
  "cvpr2018_w41_motionfusedframesdatalevelfusionstrategyforhandgesturerecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture Recognition",
    "authors": [
      "Okan Kopuklu",
      "Neslihan Kose",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Kopuklu_Motion_Fused_Frames_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Kopuklu_Motion_Fused_Frames_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Acquiring spatio-temporal states of an action is the most crucial step for action classification. In this paper, we propose a data level fusion strategy, Motion Fused Frames (MFFs), designed to fuse motion information into static images as better representatives of spatio-temporal states of an action. MFFs can be used as input to any deep learning architecture with very little modification on the network. We evaluate MFFs on hand gesture recognition tasks using three video datasets - Jester, ChaLearn LAP IsoGD and NVIDIA Dynamic Hand Gesture Datasets - which require capturing long-term temporal relations of hand movements. Our approach obtains very competitive performance on Jester and ChaLearn benchmarks with the classification accuracies of 96.28% and 57.4%, respectively, while achieving state-of-the-art performance with 84.7% accuracy on NVIDIA benchmark.",
    "code_link": "https://github.com/okankop/MFF-pytorch"
  },
  "cvpr2018_w41_clothingchangeawarepersonidentification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Clothing Change Aware Person Identification",
    "authors": [
      "Jia Xue",
      "Zibo Meng",
      "Karthik Katipally",
      "Haibo Wang",
      "Kees van Zon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Xue_Clothing_Change_Aware_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Xue_Clothing_Change_Aware_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We develop a person identification approach - Clothing Change Aware Network (CCAN) for the task of clothing assisted person identification. CCAN concerns approaches that go beyond face recognition and particularly tackles the role of clothing to identification. Person identification is a rather challenging task when clothing appears changed under complex background information. With a pair of two person images as input, CCAN simultaneously performs a verification task to detect change in clothing and an identification task to predict person identity. When clothing from the pair of input images is detected to be different, CCAN automatically understates clothing information while emphasizing face, and vice versa. In practice, CCAN outperforms the way of equally stacking face and full body context features, and shows leading results on the People in Photos Album (PIPA) dataset.",
    "code_link": ""
  },
  "cvpr2018_w41_acompactdeeplearningmodelforrobustfacialexpressionrecognition": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "A Compact Deep Learning Model for Robust Facial Expression Recognition",
    "authors": [
      "Chieh-Ming Kuo",
      "Shang-Hong Lai",
      "Michel Sarkis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Kuo_A_Compact_Deep_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Kuo_A_Compact_Deep_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we propose a compact frame-based facial expression recognition framework for facial expression recognition which achieves very competitive performance with respect to state-of-the-art methods while using much less parameters. The proposed framework is extended to a frame-to-sequence approach by exploiting temporal information with gated recurrent units. In addition, we develop an illumination augmentation scheme to alleviate the overfitting problem when training the deep networks with hybrid data sources. Finally, we demonstrate the performance improvement by using the proposed technique on some public datasets.",
    "code_link": ""
  },
  "cvpr2018_w41_facscapspose-independentfacialactioncodingwithcapsules": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "FACSCaps: Pose-Independent Facial Action Coding With Capsules",
    "authors": [
      "Itir Onal Ertugrul",
      "Laszlo A. Jeni",
      "Jeffrey F. Cohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Ertugrul_FACSCaps_Pose-Independent_Facial_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Ertugrul_FACSCaps_Pose-Independent_Facial_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Most automated facial expression analysis methods treat the face as a 2D object, flat like a sheet of paper. That works well provided images are frontal or nearly so. In real-world conditions, moderate to large head rotation is common and system performance to recognize expression degrades. Multi-view Convolutional Neural Networks (CNNs) have been proposed to increase robustness to pose, but they require greater model sizes and may generalize poorly across views that are not included in the training set. We propose FACSCaps architecture to handle multi-view and multi-label facial action unit (AU) detection within a single model that can generalize to novel views. Additionally, FACSCaps's ability to synthesize faces enables insights into what is leaned by the model. FACSCaps models video frames using matrix capsules, where hierarchical pose relationships between face parts are built into internal representations. The model is trained by jointly optimizing a multi-label loss and the reconstruction accuracy.FACSCaps was evaluated using the FERA 2017 facial expression dataset that includes spontaneous facial expressions in a wide range of head orientations. FACSCaps outperformed both state-of-the-art CNNs and their temporal extensions.",
    "code_link": ""
  },
  "cvpr2018_w41_unravelinghumanperceptionoffacialagingusingeyegaze": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Unraveling Human Perception of Facial Aging Using Eye Gaze",
    "authors": [
      "Daksha Yadav",
      "Naman Kohli",
      "Ekampreet Kalsi",
      "Mayank Vatsa",
      "Richa Singh",
      "Afzel Noore"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Yadav_Unraveling_Human_Perception_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Yadav_Unraveling_Human_Perception_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Continual efforts are being made to understand human perception network with the purpose of developing enhanced computational models for vision-based tasks. In this paper, we utilize eye gaze as a medium to unravel the cues utilized by humans for the perception of facial aging. Specifically, we explore the tasks of face age estimation and age-separate face verification and analyze the eye gaze patterns of participants to understand the strategy followed by human participants. To facilitate this, eye gaze data from 50 participants is acquired using two different eye gaze trackers: Eye Tribe and GazePoint GP3. Comprehensive analysis of various eye movement metrics is performed with respect to different face parts to illustrate their relevance for age estimation and age-separated face verification tasks.",
    "code_link": ""
  },
  "cvpr2018_w41_improvingvisemerecognitionusinggan-basedfrontalviewmapping": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Improving Viseme Recognition Using GAN-Based Frontal View Mapping",
    "authors": [
      "Dario Augusto Borges Oliveira",
      "Andrea Britto Mattos",
      "Edmilson da Silva Morais"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Oliveira_Improving_Viseme_Recognition_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Oliveira_Improving_Viseme_Recognition_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep learning methods have become the standard for Visual Speech Recognition problems due to their high accuracy results reported in the literature. However, while successful works have been reported for words and sentences, recognizing shorter segments of speech, like phones, has proven to be much more challenging due to the lack of temporal and contextual information. Also, head-pose variation remains a known issue for facial analysis with direct impact in this problem. In this context, we propose a novel methodology to tackle the problem of recognizing visemes - the visual equivalent of phonemes - using a GAN to artificially lock the face view into a perfect frontal view, reducing the view angle variability and simplifying the recognition task performed by our classification CNN. The GAN is trained using a large-scale synthetic 2D dataset based on realistic 3D facial models, automatically labelledfor different visemes, mapping a slightly random view to a perfect frontal view. We evaluate our method using the GRID corpus, which was processed to extract viseme images and their corresponding synthetic frontal views to be further classified by our CNN model. Our results demonstrate that the additional synthetic frontal view is able to improve accuracy in 5.9% when compared with classification using the original image only.",
    "code_link": ""
  },
  "cvpr2018_w41_light-weightheadposeinvariantgazetracking": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Light-Weight Head Pose Invariant Gaze Tracking",
    "authors": [
      "Rajeev Ranjan",
      "Shalini De Mello",
      "Jan Kautz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Ranjan_Light-Weight_Head_Pose_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Ranjan_Light-Weight_Head_Pose_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor.",
    "code_link": ""
  },
  "cvpr2018_w41_implementingarobustexplanatorybiasinapersonre-identificationnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Implementing a Robust Explanatory Bias in a Person Re-Identification Network",
    "authors": [
      "Esube Bekele",
      "Wallace E. Lawson",
      "Zachary Horne",
      "Sangeet Khemlani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Bekele_Implementing_a_Robust_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Bekele_Implementing_a_Robust_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep learning improved attributes recognition significantly in recent years. However, many of these networks remain \"black boxes\" and providing a meaningful explanation of their decisions is a major challenge. When these networks misidentify a person, they should be able to explain this mistake.The ability to generate explanations compelling enough to serve as useful accounts of the system's operations at a very high human-level is still in its infancy. In this paper, we utilize person re-identification (re-ID) networks as a platform to generate explanations. We propose and implement a framework that can be used to explain person re-ID using soft-biometric attributes. In particular, the resulting framework embodies a cognitively validated explanatory bias: people prefer and produce explanations that concern inherent properties instead of extrinsic influences. This bias is pervasive in that it affects the fitness of explanations across a broad swath of contexts, particularly those that concern conflicting or anomalous observations. To explain person re-ID, we developed a multi-attribute residual network that treats a subset of its features as either inherent or extrinsic.Using these attributes, the system generates explanations based on inherent properties when the similarity of two input images is low, and it generates explanations based on extrinsic properties when the similarity is high. We argue that such a framework provides a blueprint for how to make the decisions of deep networks comprehensible to human operators. As an intermediate step, we demonstrate state-of-the-art attribute recognition performance on two pedestrian datasets (PETA and PA100K) and a face-based attribute dataset (CelebA). The VIPeR dataset is then used to generate explanations for re-ID with a network trained on PETA attributes.",
    "code_link": ""
  },
  "cvpr2018_w41_ondetectingdomesticabuseviafaces": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W41",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "On Detecting Domestic Abuse via Faces",
    "authors": [
      "Puspita Majumdar",
      "Saheb Chhabra",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w41/html/Majumdar_On_Detecting_Domestic_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w41/Majumdar_On_Detecting_Domestic_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Domestic violence is considered a major social problem worldwide. Different countries have enacted the law to contain and protect the victims of domestic violence. In order to understand the nature of domestic violence, medical professionals and researchers have performed manual analysis of facial injuries. The aim of these studies is to find commonly affected facial regions, to determine the types of maxillofacial trauma associated with domestic violence, and to distinguish the injuries of domestic violence from accidents. Analysis of these injuries assist the service providers in providing proper treatment to the victims as well as facilitate law enforcement investigation. This paper automates the process of analyzing the facial injuries to distinguish the victims of domestic abuse from others. For this purpose, Domestic Violence Face database of 450 subjects with two classes namely, Domestic Violence and Non-Domestic Violence, is prepared. The paper also presents a novel framework using activation maps of deep learning features for determining whether an image belongs to domestic violence class or not. The results on the proposed database show that deep learning based framework is effective in detecting domestic injuries.",
    "code_link": ""
  },
  "cvpr2018_w42_markovchainneuralnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Vision With Biased or Scarce Data",
    "title": "Markov Chain Neural Networks",
    "authors": [
      "Maren Awiszus",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w42/html/Awiszus_Markov_Chain_Neural_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w42/Awiszus_Markov_Chain_Neural_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work we present a modified neural network model which is capable to simulate Markov Chains. We show how to express and train such a network, how to ensure given statistical properties reflected in the training data and we demonstrate several applications where the network produces non-deterministic outcomes. One example is a random walker model, e.g. useful for simulation of brownian motions or a natural Tic-Tac-Toe network which ensures non-deterministic game behavior.",
    "code_link": ""
  },
  "cvpr2018_w42_agenerativemodelforzeroshotlearningusingconditionalvariationalautoencoders": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Vision With Biased or Scarce Data",
    "title": "A Generative Model for Zero Shot Learning Using Conditional Variational Autoencoders",
    "authors": [
      "Ashish Mishra",
      "Shiva Krishna Reddy",
      "Anurag Mittal",
      "Hema A. Murthy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w42/html/Mishra_A_Generative_Model_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w42/Mishra_A_Generative_Model_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training. While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes. By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.",
    "code_link": ""
  },
  "cvpr2018_w42_endoscopenavigationand3dreconstructionoforalcavitybyvisualslamwithmitigateddatascarcity": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W42",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Vision With Biased or Scarce Data",
    "title": "Endoscope Navigation and 3D Reconstruction of Oral Cavity by Visual SLAM With Mitigated Data Scarcity",
    "authors": [
      "Liang Qiu",
      "Hongliang Ren"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w42/html/Qiu_Endoscope_Navigation_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w42/Qiu_Endoscope_Navigation_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Nowadays computer-assisted surgery (CAS) technologies have been widely used in many aspects of the medical field such as Minimally Invasive Surgery (MIS) or operation focusing on a small surgical site, which has provided significant benefits to patients. However, it is hard for surgeons to determine the accurate poses and surrounding circumstances of the endoscope, due to some restrictions such as narrow field of view (FOV) and misregistration. In this paper, we propose to apply ORBSLAM with a low-cost endoscope to estimate the location of endoscope and create a 3D map for the oral surgery scene, which imposes considerable challenges compared to other human tissue environments, because of the irregular shape, texture-less surface and non-rigid characteristics of the oral cavity. In general, it is very difficult to detect sufficient and effective data for Visual SLAM to realize accurate localization and 3D dense map mainly due to the scarce feature points extracted from tissues and the rare correct matches. In order to reconstruct a denser map for a texture-less oral cavity, laser light markers are used for generating more features, which can mitigate the problem of data scarcity. Besides, we have validated this approach with some experiments on a silicone model of human head. Comparisons between the trajectory/map obtained from ORBSLAM and the ground truth are also provided.",
    "code_link": ""
  },
  "cvpr2018_w44_cellimagesegmentationbyintegratingmultiplecnns": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Cell Image Segmentation by Integrating Multiple CNNs",
    "authors": [
      "Yuki Hiramatsu",
      "Kazuhiro Hotta",
      "Ayako Imanishi",
      "Michiyuki Matsuda",
      "Kenta Terai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Hiramatsu_Cell_Image_Segmentation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Hiramatsu_Cell_Image_Segmentation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Convolutional Neural Network is valid for segmentation of objects in an image. In recent years, it is beginning to be applied to the field of medicine and cell biology. In semantic segmentation, the accuracy has been improved by using deeper single deep neural network. However, the accuracy is saturated for difficult segmentation tasks. In this paper, we propose a semantic segmentation method by integrating multiple CNNs adaptively. This method consists of a gating network and multiple expert networks. Expert network outputs a segmentation result for an input image. Gating network automatically divides the input image into several sub-problems and assigns them to expert networks. Thus, each expert network solves only the specific problem, and our proposed method is possible to learn more efficiently than single deep neural network. We evaluate the proposed method on segmentation problem of cell membrane and nucleus. The proposed method improved the segmentation accuracy in comparison with single deep neural network.",
    "code_link": ""
  },
  "cvpr2018_w44_largekernelrefinefusionnetforneuronmembranesegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Large Kernel Refine Fusion Net for Neuron Membrane Segmentation",
    "authors": [
      "Dongnan Liu",
      "Donghao Zhang",
      "Yang Song",
      "Chaoyi Zhang",
      "Heng Huang",
      "Mei Chen",
      "Weidong Cai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Liu_Large_Kernel_Refine_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Liu_Large_Kernel_Refine_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " 2D neuron membrane segmentation for Electron Microscopy (EM) images is a key step in the 3D neuron reconstruction task. Compared with the semantic segmentation tasks for general images, the boundary segmentation in EM images is more challenging. In EM segmentation tasks, we need not only to segment the ambiguous membrane boundaries from bubble-like noise in the images, but also to remove shadow-like intracellular structure. In order to address these problems, we propose a Large Kernel Refine Fusion Net, an encoder-decoder architecture with fusion of features at multiple resolution levels. We incorporate large convolutional blocks to ensure the valid receptive fields for the feature maps are large enough, which can reduce information loss. Our model can also process the background together with the membrane boundary by using residual cascade pooling blocks. In addition, the postprocessing method in our work is simple but effective for a final refinement of the output probability map. Our method was evaluated and achieved competitive performances on two EM membrane segmentation tasks: ISBI2012 EM segmentation challenge and mouse piriform cortex segmentation task.",
    "code_link": ""
  },
  "cvpr2018_w44_threedimensionalfluorescencemicroscopyimagesynthesisandsegmentation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Three Dimensional Fluorescence Microscopy Image Synthesis and Segmentation",
    "authors": [
      "Chichen Fu",
      "Soonam Lee",
      "David Joon Ho",
      "Shuo Han",
      "Paul Salama",
      "Kenneth W. Dunn",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Fu_Three_Dimensional_Fluorescence_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Fu_Three_Dimensional_Fluorescence_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Advances in fluorescence microscopy enable acquisition of 3D image volumes with better image quality and deeper penetration into tissue. Segmentation is a required step to characterize and analyze biological structures in the images and recent 3D segmentation using deep learning has achieved promising results. One issue is that deep learning techniques require a large set of groundtruth data which is impractical to annotate manually for large 3D microscopy volumes. This paper describes a 3D deep learning nuclei segmentation method using synthetic 3D volumes for training. A set of synthetic volumes and the corresponding groundtruth are generated using spatially constrained cycle-consistent adversarial networks. Segmentation results demonstrate that our proposed method is capable of segmenting nuclei successfully for various data sets.",
    "code_link": ""
  },
  "cvpr2018_w44_improvedextractionofobjectsfromurinemicroscopyimageswithunsupervisedthresholdingandsupervisedu-nettechniques": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Improved Extraction of Objects From Urine Microscopy Images With Unsupervised Thresholding and Supervised U-Net Techniques",
    "authors": [
      "Abdul Aziz",
      "Harshit Pande",
      "Bharath Cheluvaraju",
      "Tathagato Rai Dastidar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Aziz_Improved_Extraction_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Aziz_Improved_Extraction_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose a novel unsupervised method for extracting objects from urine microscopy images and also applied U-net for extracting these objects. We fused these proposed methods with a known edge thresholding technique from an existing work on segmentation of urine microscopic images. Comparison between our proposed methods and the existing work showed that for certain object types the proposed unsupervised method with or without edge thresholding outperforms the other methods, while in other cases the U-net methodwith or without edge thresholding outperforms the other methods. Overall the proposed unsupervised method along with edge thresholding worked the best by extracting maximum number of objects and minimum number of artifacts. On a test dataset, the artifact to object ratio for the proposed unsupervised method was 0.71, which is significantly better than that of 1.26 for the existing work. The proposed unsupervised method along with edge thresholding extracted 3208 objects as compared to 1608 by the existing work. To the best of our knowledge this is the first application of Deep Learning for extraction of clinically significant objects in urine microscopy images.",
    "code_link": ""
  },
  "cvpr2018_w44_multilayerencoder-decodernetworkfor3dnuclearsegmentationinspheroidmodelsofhumanmammaryepithelialcelllines": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multilayer Encoder-Decoder Network for 3D Nuclear Segmentation in Spheroid Models of Human Mammary Epithelial Cell Lines",
    "authors": [
      "Mina Khoshdeli",
      "Garrett Winkelmaier",
      "Bahram Parvin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Khoshdeli_Multilayer_Encoder-Decoder_Network_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Khoshdeli_Multilayer_Encoder-Decoder_Network_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Nuclear segmentation is an important step in quantitative profiling of colony organization in 3D cell culture models. However, complexities arise from technical variations and biological heterogeneities. We proposed a new 3D segmentation model based on convolutional neural networks for 3D nuclear segmentation, which overcome the complexities associated with non-uniform staining, aberrations in cellular morphologies, and cells being in different states. The uniqueness of the method originates from (i) volumetric operations to capture all the three-dimensional features, and (ii) the encoder-decoder architecture, which enables segmentation of the spheroid models in one forward pass. The method is validated with four human mammary epithelial cell (HMEC) lines--each with a unique genetic makeup. The performance of the proposed method is compared with the previous methods and is shown that the deep learning model has a superior pixel-based segmentation, and an F1-score of 0.95 is reported.",
    "code_link": ""
  },
  "cvpr2018_w44_resolution-enhancedlenslesscolorshadowimagingmicroscopybasedonlargefield-of-viewsubmicron-pixelimagingsensors": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Resolution-Enhanced Lensless Color Shadow Imaging Microscopy Based on Large Field-of-View Submicron-Pixel Imaging Sensors",
    "authors": [
      "Cheng Yang",
      "Haowen Ma",
      "Xu Cao",
      "Xia Hua",
      "Xiaofeng Bu",
      "Limin Zhang",
      "Tao Yue",
      "Feng Yan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Yang_Resolution-Enhanced_Lensless_Color_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Yang_Resolution-Enhanced_Lensless_Color_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We report a resolution-enhanced lensless color shadow imaging microscopy (RELCSIM) system based on large field-of-view (FOV) submicron-pixel imaging sensors. The physical pixel size of our custom made imaging chip is 0.95mm x 0.95mm, and the pixel-count is 25 millions (5120H x 5120V ). By directly recording the shadow of the samples without any postprocssing, we have realized a microscope with a half-pitch resolution of 1mm and a FOV of 25mm^2 simutaneously. To verify the resolution of our system, the grating samples coated on the surface of the chip are imaged. We further demonstrate the monochromatic and color shadow imaging of muscle tissue specimens with the prototype, which show the potential for applications such as diagnostic pathology.",
    "code_link": ""
  },
  "cvpr2018_w44_sequentialmodelingofdeepfeaturesforbreastcancerhistopathologicalimageclassification": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Sequential Modeling of Deep Features for Breast Cancer Histopathological Image Classification",
    "authors": [
      "Vibha Gupta",
      "Arnav Bhavsar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Gupta_Sequential_Modeling_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Gupta_Sequential_Modeling_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Computerized approaches for automated classification of histopathology images can help in reducing the manual observational workload of pathologists. In recent years, like in other areas, deep networks have also attracted attention for histopathology image analysis. However, existing approaches have paid little attention in exploring multi-layer features for improving the classification.We believe that considering multi-layered features is important as different regions in the images, which are in turn at different magnifications may contain useful discriminative information at different levels of hierarchy.Considering thedependency exists among the layers in deep learning, we propose sequential framework which utilizes multi-layered deep features that are extracted from fine-tuned DenseNet. A decision is made by layer for a sample only if it passes a pre-defined cut-off confidence for that layer otherwise, the sample is passed on to next layers. Various experiments on publicly available BreaKHis dataset, demonstrate the proposed framework yields better performance, in most cases, than typically used highest layer features. We also compare results with the framework where each layer is treated independently. This indicates that low-mid-level features also carry useful discriminative information, when explicitly considered. We also demonstrate an improved performance over various state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2018_w44_comparisonofdeeptransferlearningstrategiesfordigitalpathology": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Comparison of Deep Transfer Learning Strategies for Digital Pathology",
    "authors": [
      "Romain Mormont",
      "Pierre Geurts",
      "Raphael Maree"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Mormont_Comparison_of_Deep_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Mormont_Comparison_of_Deep_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we study deep transfer learning as a way of overcoming object recognition challenges encountered in the field of digital pathology. Through several experiments, we investigate various uses of pre-trained neural network architectures and different combination schemes with random forests for feature selection. Our experiments on eight classification datasets show that densely connected and residual networks consistently yield best performances across strategies. It also appears that network fine-tuning and using inner layers features are the best performing strategies, with the former yielding slightly superior results.",
    "code_link": "https://github.com/keras-team/keras"
  },
  "cvpr2018_w44_3dcellnuclearmorphologymicroscopyimagingdatasetandvoxel-basedmorphometryclassificationresults": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "3D Cell Nuclear Morphology: Microscopy Imaging Dataset and Voxel-Based Morphometry Classification Results",
    "authors": [
      "Alexandr A. Kalinin",
      "Ari Allyn-Feuer",
      "Alex Ade",
      "Gordon-Victor Fon",
      "Walter Meixner",
      "David Dilworth",
      "Jeffrey R. de Wet",
      "Gerald A. Higgins",
      "Gen Zheng",
      "Amy Creekmore",
      "John W. Wiley",
      "James E. Verdone",
      "Robert W. Veltri",
      "Kenneth J. Pienta",
      "Donald S. Coffey",
      "Brian D. Athey",
      "Ivo D. Dinov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Kalinin_3D_Cell_Nuclear_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Kalinin_3D_Cell_Nuclear_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Cell deformation is regulated by complex underlying biological mechanisms associated with spatial and temporal morphological changes in the nucleus that are related to cell differentiation, development, proliferation, and disease. Thus, quantitative analysis of changes in size and shape of nuclear structures in 3D microscopic images is important not only for investigating nuclear organization, but also for detecting and treating pathological conditions such as cancer. While many efforts have been made to develop cell and nuclear shape characteristics in 2D or pseudo-3D, several studies have suggested that 3D morphometric measures provide better results for nuclear shape description and discrimination. A few methods have been proposed to classify cell and nuclear morphological phenotypes in 3D, however, there is a lack of publicly available 3D data for the evaluation and comparison of such algorithms. This limitation becomes of great importance when the ability to evaluate different approaches on benchmark data is needed for better dissemination of the current state of the art methods for bioimage analysis. To address this problem, we present a dataset containing two different cell collections, including original 3D microscopic images of cell nuclei and nucleoli. In addition, we perform a baseline evaluation of a number of popular classification algorithms using 2D and 3D voxel-based morphometric measures. To account for batch effects, while enabling calculations of AUROC and AUPR performance metrics, we propose a specific cross-validation scheme that we compare with commonly used k-fold cross-validation. Original and derived imaging data are made publicly available on the project web-page: http://www.socr.umich.edu/projects/3d-cell-morphometry/data.html.",
    "code_link": ""
  },
  "cvpr2018_w44_fastsmefasterandsmoothermanifoldextractionfrom3dstack": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "FastSME: Faster and Smoother Manifold Extraction From 3D Stack",
    "authors": [
      "Sreetama Basu",
      "Elton Rexhepaj",
      "Nathalie Spassky",
      "Auguste Genovesio",
      "Rasmus Reinhold Paulsen",
      "ASM Shihavuddin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Basu_FastSME_Faster_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Basu_FastSME_Faster_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " 3D image stacks are routinely acquired to capture data that lie on undulating 3D manifolds yet processed in 2D by biologists. Algorithms to reconstruct the specimen morphology into a 2D representation from the 3D image volume are employed in such scenarios. In this paper, we present FastSME, which offers several improvements on the baseline SME algorithm which enables accurate 2D representation of data on a manifold from 3D volumes, however is computationally expensive. The improvements are achieved in terms of processing speed (3X-10X speed-up depending on image size), minimizing sensitivity to initialization, and also increases local smoothness of the recovered manifold resulting in better reconstructed 2D composite image. We compare the proposed FastSME against the baseline SME as well as other accessible state-of-the-art tools on synthetic and real microscopy data. Our evaluation on multiple metrics demonstrates the efficiency of the presented method in maintaining fidelity of manifold shape and hence specimen morphology.",
    "code_link": "https://github.com/Shihav/FastSME"
  },
  "cvpr2018_w44_localizationandtrackingin4dfluorescencemicroscopyimagery": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Localization and Tracking in 4D Fluorescence Microscopy Imagery",
    "authors": [
      "Shahira Abousamra",
      "Shai Adar",
      "Natalie Elia",
      "Roy Shilkrot"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Abousamra_Localization_and_Tracking_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Abousamra_Localization_and_Tracking_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " 3D fluorescence microscopy continues to pose challenging tasks with more experiments leading to identifying new physiological patterns in cells' life cycle and activity. It then falls on the hands of biologists to annotate this imagery which is laborious and time-consuming, especially with noisy images and hard to see and track patterns. Modeling of automation tasks that can handle depth-varying light conditions and noise, and other challenges inherent in 3D fluorescence microscopy often becomes complex and requires high processing power and memory. This paper presents an efficient methodology for the localization, classification, and tracking in fluorescence microscopy imagery by taking advantage of time sequential images in 4D data. We show the application of our proposed method on the challenging task of localizing and tracking microtubule fibers' bridge formation during the cell division of zebrafish embryos where we achieve 98% accuracy and 0.94 F1- score.",
    "code_link": ""
  },
  "cvpr2018_w44_estimationofspermconcentrationandtotalmotilityfrommicroscopicvideosofhumansemensamples": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W44",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Estimation of Sperm Concentration and Total Motility From Microscopic Videos of Human Semen Samples",
    "authors": [
      "Karan Dewan",
      "Tathagato Rai Dastidar",
      "Maroof Ahmad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w44/html/Dewan_Estimation_of_Sperm_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w44/Dewan_Estimation_of_Sperm_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a method for automated analysis of human semen quality using microscopic video sequences of live semen samples. The videos are captured through an automated microscope at 400xmagnification. In each video frame, objects of interest are extracted using image processing techniques. A deep convolution neural network (CNN) is used to distinguish between sperms and non-sperm objects. The frame-wise count of sperm cells is used to estimate the concentration of sperms in unit volume of semen. In each video, individual sperm cells are tracked across the frames using a predictive approach which handles collisions and occlusions well. Based on their computed trajectories, sperms are classified into progressively motile, non-progressively motile and immotile types as per the WHO manual. In certain samples, due to various reasons, all visible objects drift in a certain direction, hence we also present a method for identifying and compensating for that drift. Experimental results are presented on a set of more than 100 semen samples collected from a clinical laboratory. The results compare well with existing accepted standard, SQA-V Gold for sperm concentration as well as motility parameters.",
    "code_link": ""
  },
  "cvpr2018_w47_scalinghandwrittenstudentassessmentswithadocumentimageworkflowsystem": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W47",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computational Models for Learning Systems and Educational Assessment",
    "title": "Scaling Handwritten Student Assessments With a Document Image Workflow System",
    "authors": [
      "Vijay Rowtula",
      "Varun Bhargavan",
      "Mohan Kumar",
      "C.V. Jawahar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w47/html/Rowtula_Scaling_Handwritten_Student_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w47/Rowtula_Scaling_Handwritten_Student_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " With the increase in the number of students enrolled in the university system, regular assessment of student performance has become challenging. This is specially true in case of summative assessments, where one expects the student to write down an answer on paper, rather than selecting a correct answer from multiple choices. In this paper, we present a document image workflow system that helps in scaling the handwritten student assessments in a typical university setting. We argue that this improves the efficiency since the book keeping time as well as physical paper movement is minimized. An electronic workflow can make the anonymization easy, alleviating the fear of biases in many cases. Also, parallel and distributed assessment by multiple instructors is straightforward in an electronic workflow system. At the heart of our solution, we have (i) a distributed image capture module with a mobile phone (ii) image processing algorithms that improve the quality and readability (iii) image annotation module that process the evaluations/feedbacks as a separate layer. Our system also acts as a platform for modern image analysis which can be adapted to the domain of student assessments. This include (i) Handwriting recognition and word spotting (ii) Measure of document similarity (iii) Aesthetic analysis of handwriting (iv)Identity of the writer etc. With the handwriting assessment workflow system, all these recent advances in computer vision can become practical and applicable in evaluating student assessments.",
    "code_link": ""
  },
  "cvpr2018_w47_teachersperceptionintheclassroom": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W47",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Computational Models for Learning Systems and Educational Assessment",
    "title": "Teachers' Perception in the Classroom",
    "authors": [
      "Omer Sumer",
      "Patricia Goldberg",
      "Kathleen Sturmer",
      "Tina Seidel",
      "Peter Gerjets",
      "Ulrich Trautwein",
      "Enkelejda Kasneci"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w47/html/Sumer_Teachers_Perception_in_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w47/Sumer_Teachers_Perception_in_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The ability for a teacher to engage all students in active learning processes in classroom constitutes a crucial prerequisite for enhancing students achievement. Teachers' attentional processes provide important insights into teachers' ability to focus their attention on relevant information in the complexity of classroom interaction and distribute their attention across students in order to recognize the relevant needs for learning. In this context, mobile eye tracking is an innovative approach within teaching effectiveness research to capture teachers' attentional processes while teaching. However, analyzing mobile eye-tracking data by hand is time consuming and still limited. In this paper, we introduce a new approach to enhance the impact of mobile eye tracking by connecting it with computer vision. In mobile eye tracking videos from an educational study using a standardized small group situation, we apply a state-of-the-art face detector, create face tracklets, and introduce a novel method to cluster faces into the number of identity. Subsequently, teachers' attentional focus is calculated per student during a teaching unit by associating eye tracking fixations and face tracklets. To the best of our knowledge, this is the first work to combine computer vision and mobile eye tracking to model teachers' attention while instructing.",
    "code_link": ""
  },
  "cvpr2018_w48_humanactionadverbrecognitionadhadatasetandathree-streamhybridmodel": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "Human Action Adverb Recognition: ADHA Dataset and a Three-Stream Hybrid Model",
    "authors": [
      "Bo Pang",
      "Kaiwen Zha",
      "Cewu Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Pang_Human_Action_Adverb_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Pang_Human_Action_Adverb_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We introduce the first benchmark for a new problem -- recognizing human action adverbs (HAA): \"Adverbs Describing Human Actions\" (ADHA). We demonstrate some key features of ADHA: a semantically complete set of adverbs describing human actions, a set of common, describable human actions, and an exhaustive labelling of simultaneously emerging actions in each video. We commit an in-depth analysis on the implementation of current effective models in action recognition and image captioning on adverb recognition, and the results reveal that such methods are unsatisfactory. Furthermore, we propose a novel three-stream hybrid model to tackle the HAA problem, which achieves better performances and receives relatively promising results.",
    "code_link": ""
  },
  "cvpr2018_w48_payattentiontoviralityunderstandingpopularityofsocialmediavideoswiththeattentionmechanism": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "Pay Attention to Virality: Understanding Popularity of Social Media Videos With the Attention Mechanism",
    "authors": [
      "Adam Bielski",
      "Tomasz Trzcinski"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Bielski_Pay_Attention_to_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Bielski_Pay_Attention_to_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Predicting popularity of social media videos before they are published is a challenging task, mainly due to the complexity of content distribution network as well as the number of factors that play part in this process. As solving this task provides tremendous help for media content creators, many successful methods were proposed to solve this problem with machine learning. In this work, we change the viewpoint and postulate that it is not only the predicted popularity that matters, but also, maybe even more importantly, understanding of how individual parts influence the final popularity score. To that end, we propose to combine the Grad-CAM visualization method with a soft attention mechanism. Our preliminary results show that this approach allows for more intuitive interpretation of the content impact on video popularity, while achieving competitive results in terms of prediction accuracy.",
    "code_link": ""
  },
  "cvpr2018_w48_learningfashionbysimulatedhumansupervision": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "Learning Fashion by Simulated Human Supervision",
    "authors": [
      "Eli Alshan",
      "Sharon Alpert",
      "Assaf Neuberger",
      "Nathaniel Bubis",
      "Eduard Oks"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Alshan_Learning_Fashion_by_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Alshan_Learning_Fashion_by_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We consider the task of predicting subjective fashion traits from images using neural networks. Specifically, we are interested in training a network for ranking outfits according to how well they fit the user. In order to capture the variability induced by human subjective considerations, each training example is annotated by a panel of fashion experts. Similarly to previous works on subjective data, the panel votes are converted to a classification or regression problem and the corresponding network is trained and evaluated using standard objective metrics. The question is which objective metric, if any, is most suitable to measure the performance of a network trained for subjective tasks? In this paper, we conducted human approval tests for outfit ranking networks trained using various objective metrics. We show that these metrics do not adequately estimate the human approval of subjective tasks. Instead, we introduce a supervising network that unlike objective metrics, is designed to capture the variability induced by human subjectivity. We use it to supervise our outfit ranking network and we demonstrate empirically, that training our outfit ranking network with the suggested supervising network achieves greater approval ratings from human subjects.",
    "code_link": ""
  },
  "cvpr2018_w48_findingyourlookalikemeasuringfacesimilarityratherthanfaceidentity": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "Finding Your Lookalike: Measuring Face Similarity Rather Than Face Identity",
    "authors": [
      "Amir Sadovnik",
      "Wassim Gharbi",
      "Thanh Vu",
      "Andrew Gallagher"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task.",
    "code_link": ""
  },
  "cvpr2018_w48_behaviorandpersonalityanalysisinanonsocialcontextdataset": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "Behavior and Personality Analysis in a Nonsocial Context Dataset",
    "authors": [
      "Dario Dotti",
      "Mirela Popa",
      "Stylianos Asteriadis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Dotti_Behavior_and_Personality_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Dotti_Behavior_and_Personality_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Personality recognition using nonverbal behavioral cues is a challenging task in the Affective Computing field. The majority of existing methods investigate personality assessment in social contexts, such as crowded places or social events, but ignore the role of behaviors as well as personality in nonsocial situations (i.e. during individual activities). In this paper we introduce a novel dataset for behavior understanding and personality recognition in a nonsocial context. Forty-six participants were recorded in an unconstrained indoor space, related to a smart home environment, performing six tasks resembling Activities of Daily Living (ADL). During the experiment, personality scores were collected using self-assessment questionnaires. Furthermore, a temporal framework using a Long-Short Term Memory (LSTM) network is proposed to map nonverbal behavioral features to participants' personality labels. Our experiments showed that nonverbal behaviors are important predictors of personality, confirming theories from the personality psychology field.",
    "code_link": ""
  },
  "cvpr2018_w48_ambianceinsocialmediavenuesvisualcueinterpretationbymachinesandcrowds": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "Ambiance in Social Media Venues: Visual Cue Interpretation by Machines and Crowds",
    "authors": [
      "Gulcan Can",
      "Yassir Benkhedda",
      "Daniel Gatica-Perez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Can_Ambiance_in_Social_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Can_Ambiance_in_Social_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We study the perception of ambiance of places captured in social media images by both machines and crowdworkers.This task is challenging due to the subjective nature of the ambiance construct as well as the large variety in layout, style, and visual characteristics of venues. For machine recognition of ambiance, we use Residual Deep Convolutional Neural Networks (ResNets), followed by gradient- weighted class activation mapping (Grad-CAM) visualizations. This form of visual explanation obtained from the trained ResNet-50 models were assessed by crowdworkers based on a carefully designed crowdsourcing task, in which both visual ambiance cues of venues and subjective assessment of Grad-CAM results were collected and analyzed. The results show that paintings, photos, and decorative items are strong cues for artsy ambiance, whereas type of utensils, type of lamps and presence of flowers may indicate formal ambiance. Layout and design-related cues such as type of chairs, type of tables/tablecloth and type of windows are noted to have impact for both ambiances. Overall, the ambiance visual cue recognition results are promising, and the crowd-based assessment approach may motivate other studies on subjective perception of place attributes.",
    "code_link": ""
  },
  "cvpr2018_w48_fromapparenttorealagegender,age,ethnic,makeup,andexpressionbiasanalysisinrealageestimation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W48",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Visual Understanding of Subjective Attributes of Data",
    "title": "From Apparent to Real Age: Gender, Age, Ethnic, Makeup, and Expression Bias Analysis in Real Age Estimation",
    "authors": [
      "Albert Clapes",
      "Ozan Bilici",
      "Dariia Temirova",
      "Egils Avots",
      "Gholamreza Anbarjafari",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w48/html/Clapes_From_Apparent_to_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w48/Clapes_From_Apparent_to_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Real age estimation in still images of faces is an active area of research in the computer vision community. However, very few works attempted to analyse the apparent age as perceived by observers. Apparent age estimation is a subjective task, which is affected by many factors present in the image as well as by observer's characteristics. In this work, we enhance the APPA-REAL dataset, containing around 8K images with real and apparent ages, with new annotated attributes, namely gender, ethnic, makeup, and expression. Age and gender from a subset of guessers is also provided. We show there exists some consistent bias for a subset of these attributes when relating apparent to real age. In addition we run simple experiments with a basic Convolutional Neural Network (CNN) showing that considering apparent labels for training improves real age estimation rather than training with real ages. We also perform bias correction on CNN predictions, showing that it further enhance final age recognition performance.",
    "code_link": ""
  },
  "cvpr2018_w49_learningtoseparateobjectsoundsbywatchingunlabeledvideo": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Learning to Separate Object Sounds by Watching Unlabeled Video",
    "authors": [
      "Ruohan Gao",
      "Rogerio S. Feris",
      "Kristen Grauman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Gao_Learning_to_Separate_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Gao_Learning_to_Separate_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Perceiving a scene most fully requires all the senses. Yet modeling how objects look and sound is challenging: most natural scenes and events contain multiple objects, and the audio track mixes all the sound sources together.We propose to learn audio-visual object models from unlabeled video, then exploit the visual context to perform audio source separation in novel videos.Our approach relies on a deep multi-instance multi-label learning framework to disentangle the audio frequency bases that map to individual visual objects, even without observing/hearing those objects in isolation. We show how the recovered disentangled bases can be used to guide audio source separation to obtain better-separated, object-level sounds. Our work is the first to study audio source separation in large-scale general \"in the wild\" videos. We obtain state-of-the-art results on visually-aided audio source separation and audio denoising.",
    "code_link": ""
  },
  "cvpr2018_w49_visualtosoundgeneratingnaturalsoundforvideosinthewild": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Visual to Sound: Generating Natural Sound for Videos in the Wild",
    "authors": [
      "Yipin Zhou",
      "Zhaowen Wang",
      "Chen Fang",
      "Trung Bui",
      "Tamara L. Berg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Zhou_Visual_to_Sound_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Zhou_Visual_to_Sound_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Specifically, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.",
    "code_link": ""
  },
  "cvpr2018_w49_fastforwardingegocentricvideosbylisteningandwatching": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Fast Forwarding Egocentric Videos by Listening and Watching",
    "authors": [
      "Vinicius S",
      "Furlan",
      "Ruzena Bajcsy",
      "Erickson R. Nascimento"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/S_Fast_Forwarding_Egocentric_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/S_Fast_Forwarding_Egocentric_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The remarkable technological advance in well-equipped wearable devices is pushing an increasing production of long first-person videos. However, since most of these videos have long and tedious parts, they are forgotten or never seen. Despite a large number of techniques proposed to fast-forward these videos by highlighting relevant moments, most of them are image based only. Most of these techniques disregard other relevant sensors present in the current devices such as high-definition microphones. In this work, we propose a new approach to fast-forward videos using psychoacoustic metrics extracted from the soundtrack. These metrics can be used to estimate the annoyance of a segment allowing our method to emphasize moments of sound pleasantness. The efficiency of our method is demonstrated through qualitative results and quantitative results as far as of speed-up and instability are concerned.",
    "code_link": ""
  },
  "cvpr2018_w49_onlearningassociationofsoundsourceandvisualscenes": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "On Learning Association of Sound Source and Visual Scenes",
    "authors": [
      "Arda Senocak",
      "Tae-Hyun Oh",
      "Junsik Kim",
      "Ming-Hsuan Yang",
      "In So Kweon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Senocak_On_Learning_Association_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Senocak_On_Learning_Association_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The sight (vision) and hearing (audition) senses are the most important sources that humans use to understand their surroundings. Visual events are typically associated with sounds and they are combined. Naturally, videos and their corresponding sounds also come together in a synchronized way. Given a plenty of video and sound clip pairs, can a machine model learn to associate the sound with visual scene to reveal the sound source location without any supervision in a way similar to human perception to localize sound sources in visual scenes? In this paper, we are interested in exploring whether computational models can learn the spatial correspondence between visual and audio information by leveraging the correlation between visuals and sound based on simply watching and listening to videos in unsupervised way.",
    "code_link": ""
  },
  "cvpr2018_w49_imagegenerationassociatedwithmusicdata": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Image Generation Associated With Music Data",
    "authors": [
      "Yue Qiu",
      "Hirokatsu Kataoka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Qiu_Image_Generation_Associated_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Qiu_Image_Generation_Associated_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Recently, the development of music appreciation device has made it possible to listen to various kinds of music regardless of location. On the other hand, if it is possible to express visual contents that best matches music, we can expect a more expressive music appreciation experience by not only \"listening to\" the music but also \"watching\" the music.In this paper, we address the problems below: (1) learning a correlation between music data and images; (2) generating images from music data automatically. The experiments show that our proposed method can effectively generate proper images from music data.",
    "code_link": ""
  },
  "cvpr2018_w49_semanticspeechretrievalwithavisuallygroundedmodelofuntranscribedspeech": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Semantic Speech Retrieval With a Visually Grounded Model of Untranscribed Speech",
    "authors": [
      "Herman Kamper",
      "Gregory Shakhnarovich",
      "Karen Livescu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Kamper_Semantic_Speech_Retrieval_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Kamper_Semantic_Speech_Retrieval_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " There is growing interest in speech models that can learn from unlabelled speech paired with visual context. Here we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics.We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to (semantic) keyword labels.We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals.Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches.",
    "code_link": ""
  },
  "cvpr2018_w49_weaklysupervisedrepresentationlearningforunsynchronizedaudio-visualevents": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Weakly Supervised Representation Learning for Unsynchronized Audio-Visual Events",
    "authors": [
      "Sanjeel Parekh",
      "Slim Essid",
      "Alexey Ozerov",
      "Ngoc Q. K. Duong",
      "Patrick Perez",
      "Gael Richard"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Parekh_Weakly_Supervised_Representation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Parekh_Weakly_Supervised_Representation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Audio-visual representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. We show that the learnt representations are useful for classifying events and localizing their characteristic audio-visual elements. The system is trained using only video-level event labels without any timing information. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We achieve state-of-the-art results on a large-scale dataset of weakly-labeled audio event videos. Visualizations of localizedvisual regions and audio segments substantiate our system's efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.",
    "code_link": ""
  },
  "cvpr2018_w49_theexcitementofsportsautomatichighlightsusingaudio/visualcues": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "The Excitement of Sports: Automatic Highlights Using Audio/Visual Cues",
    "authors": [
      "Michele Merler",
      "Dhiraj Joshi",
      "Khoi-Nguyen C. Mac",
      "Quoc-Bao Nguyen",
      "Stephen Hammer",
      "John Kent",
      "Jinjun Xiong",
      "Minh N. Do",
      "John R. Smith",
      "Rogerio S. Feris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Merler_The_Excitement_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Merler_The_Excitement_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " The production of sports highlight packages summarizing a game's most exciting moments is an essential task for broadcast media. Yet, it requires labor-intensive video editing. We propose a novel approach for auto-curating sports highlights, and demonstrate it to create a first of a kind, real-world system for the editorial aid of golf and tennis highlight reels. Our method fuses information from the players' reactions (action recognition such as high-fives and fist pumps), players' expressions (aggressive, tense, smiling and neutral), spectators (crowd cheering), commentator (tone of the voice and word analysis) and game analytics to determine the most interesting moments of a game.",
    "code_link": ""
  },
  "cvpr2018_w49_amultimodalapproachtomappingsoundscapes": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "A Multimodal Approach to Mapping Soundscapes",
    "authors": [
      "Tawfiq Salem",
      "Menghua Zhai",
      "Scott Workman",
      "Nathan Jacobs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Salem_A_Multimodal_Approach_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Salem_A_Multimodal_Approach_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We explore the problem of mapping soundscapes, that is, predicting the types of sounds that are likely to be heard at a given geographic location. Using a novel dataset, which includes geo-tagged audio and overhead imagery, we develop an approach for constructing an aural atlas, which captures the geospatial distribution of soundscapes. We build on previous work relating sound to ground-level imagery but incorporate overhead imagery to overcome the limitations of sparsely distributed geo-tagged audio. In the end, all that we require to construct an aural atlas is overhead imagery of the region of interest. Weshow examples of aural atlases at multiple spatial scales, from block-level to country.",
    "code_link": ""
  },
  "cvpr2018_w49_multimodalattentionforfusionofaudioandspatiotemporalfeaturesforvideodescription": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Multimodal Attention for Fusion of Audio and Spatiotemporal Features for Video Description",
    "authors": [
      "Chiori Hori",
      "Takaaki Hori",
      "Gordon Wichern",
      "Jue Wang",
      "Teng-Yok Lee",
      "Anoop Cherian",
      "Tim K. Marks"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Hori_Multimodal_Attention_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Hori_Multimodal_Attention_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We incorporate audio features, in addition to image and motion features,for video description based on encoder-decoder recurrent neural networks (RNNs).To fuse these modalities, we introduce a multimodal attention model that can selectively utilize features from different modalities for each word in the output description. Weapply our new framework for video description using state-of-the-art audio features such as SoundNet and Audio set VGGish, and state-of-the-art image and spatiotemporal features such as I3D. Results confirm that our attention-based multi-modal fusion of audio features with visual features outperforms conventional video description approaches on three datasets.",
    "code_link": ""
  },
  "cvpr2018_w49_visualrhythmandbeat": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Visual Rhythm and Beat",
    "authors": [
      "Abe Davis",
      "Maneesh Agrawala"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Davis_Visual_Rhythm_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Davis_Visual_Rhythm_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a visual analogue for musical rhythm derived from an analysis of motion in video, and show that alignment of visual rhythm with its musical counterpart results in the appearance of dance. Central to our work is the concept of visual beats --- patterns of motion that can be shifted in time to control visual rhythm. By warping visual beats into alignment with musical beats, we can create or manipulate the appearance of dance in video. Using this approach we demonstrate a variety of retargeting applications that control musical synchronization of audio and video: we can change what song performers are dancing to, warp irregular motion into alignment with music so that it appears to be dancing, or search collections of video for moments of accidentally dance-like motion that can be used to synthesize musical performances. (This paper is a workshop preview of Davis et al., SIGGRAPH 2018.)",
    "code_link": ""
  },
  "cvpr2018_w49_invertingaudio-visualsimulationforshapeandmaterialperception": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W49",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Sight and Sound",
    "title": "Inverting Audio-Visual Simulation for Shape and Material Perception",
    "authors": [
      "Zhoutong Zhang",
      "Jiajun Wu",
      "Qiujia Li",
      "Zhengjia Huang",
      "Joshua B. Tenenbaum",
      "William T. Freeman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w49/html/Zhang_Inverting_Audio-Visual_Simulation_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w49/Zhang_Inverting_Audio-Visual_Simulation_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Humans perceive objects through both their visual appearance and the sounds they make. Given a short audio clip of objects interacting, humans can recover rich information about the materials, surface smoothness, and the quantity of objects involved. Although visual information provides cues for some of these questions, others can only be assessed with sound. For example, objects with different masses and Young's moduli may have almost identical appearance, but they make different sounds when impacted, and vice versa.",
    "code_link": ""
  },
  "cvpr2018_w50_anautoencoder-basedlearnedimagecompressordescriptionofchallengeproposalbynctu": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "An Autoencoder-based Learned Image Compressor: Description of Challenge Proposal by NCTU",
    "authors": [
      "David Alexandre",
      "Chih-Peng Chang",
      "Wen-Hsiao Peng",
      "Hsueh-Ming Hang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Alexandre_An_Autoencoder-based_Learned_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Alexandre_An_Autoencoder-based_Learned_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose a lossy image compression system using the deep-learning autoencoder structure to participate in the Challenge on Learned Image Compression (CLIC) 2018. Our autoencoder uses the residual blocks with skip connections to reduce the correlation among image pixels and condense the input image into a set of feature maps, a compact representation of the original image. The bit allocation and bitrate control are implemented by using the importance maps and quantizer. The importance maps are generated by a separate neural net in the encoder. The autoencoder and the importance net are trained jointly based on minimizing a weighted sum of mean squared error, MS-SSIM, and a rate estimate. Our aim is to produce reconstructed images with good subjective quality subject to the 0.15 bits per-pixel constraint.",
    "code_link": ""
  },
  "cvpr2018_w50_animplementationofpicturecompressionwithacnn-basedauto-encoder": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "An Implementation of Picture Compression with A CNN-based Auto-encoder",
    "authors": [
      "Ming Li",
      "Jianhua Hu",
      "Changsheng Xia",
      "Yundong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Ming_Li_An_Implementation_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Ming_Li_An_Implementation_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We mainly use the importance-map CNN method introduced by Mu.Li[??] to compress the CLIC2018 validation and test pictures. The framework is an autoencoder, with the bottleneck containing a 4-bit importance map and a 1/8 scale-down feature maps(FMs) of 64-channel and 1-bit contents. We re-implemented this model in the Tensorflow/python enviroment. Different from the original work, we modify the network a little to ge better performance and creatively replace the entropy-coding scheme with a much simpler reorder and run-length coding method. We also share some techniques and experiences for model training and fine tuning the encoder for the CLIC2018 test pictures. Method of controlling the final bit rate is also mentioned.",
    "code_link": ""
  },
  "cvpr2018_w50_autoencoderswithvariablesizedlatentvectorforimagecompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Autoencoders with Variable Sized Latent Vector for Image Compression",
    "authors": [
      "Alekh Karkada Ashok",
      "Nagaraju Palani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Ashok_Autoencoders_with_Variable_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Ashok_Autoencoders_with_Variable_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Learning to compress images is an interesting and challenging task. Autoencoders have long been used to compress images into a code of small but fixed size. As different images need different sized code based on their complexity, we propose an autoencoder architecture with a variable sized latent vector. We propose an attention based model which attends over the image and summarizes it into a small code. This summarization is repeated many times depending on the complexity of the image, producing a new code each time to encode new information so as to get a better reconstruction. These small codes then form sub-units of the final code. Our approach is quality progressive and has flexible quality setting which are desirable properties in compression. We show that the proposed model shows better performance compared to JPEG.",
    "code_link": ""
  },
  "cvpr2018_w50_block-optimizedvariablebitrateneuralimagecompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Block-optimized Variable Bit Rate Neural Image Compression",
    "authors": [
      "Caglar Aytekin",
      "Xingyang Ni",
      "Francesco Cricri",
      "Jani Lainema",
      "Emre Aksu",
      "Miska Hannuksela"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Aytekin_Block-optimized_Variable_Bit_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Aytekin_Block-optimized_Variable_Bit_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work, we propose an end-to-end block-based auto-encoder system for image compression. We introduce novel contributions to neural-network based image compression, mainly in achieving binarization simulation, variable bit rates with multiple networks, entropyfriendly representations, inference-stage code optimization and performance-improving normalization layers in the auto-encoder. We evaluate and show the incremental performance increase of each of our contributions.",
    "code_link": ""
  },
  "cvpr2018_w50_blockcnnadeepnetworkforartifactremovalandimagecompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "BlockCNN: A Deep Network for Artifact Removal and Image Compression",
    "authors": [
      "Danial Maleki",
      "Soheila Nadalian",
      "Mohammad Mahdi Derakhshani",
      "Mohammad Amin Sadeghi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Maleki_BlockCNN_A_Deep_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Maleki_BlockCNN_A_Deep_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a general technique that performs both artifact removal and image compression. For artifact removal, we input a JPEG image and try to remove its compression artifacts. For compression, we input an image and process its 8 x 8 blocks in a sequence. For each block, we first try to predict its intensities based on previous blocks; then, we store a residual with respect to the input image. Our technique reuses JPEG's legacy compression and decompression routines. Both our artifact removal and our image compression techniques use the same deep network, but with different training weights. Our technique is simple and fast and it significantly improves the performance of artifact removal and image compression.",
    "code_link": ""
  },
  "cvpr2018_w50_cnn-optimizedimagecompressionwithuncertaintybasedresourceallocation": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "CNN-Optimized Image Compression with Uncertainty based Resource Allocation",
    "authors": [
      "Zhenzhong Chen",
      "Yiming Li",
      "Feiyang Liu",
      "Zizheng Liu",
      "Xiang Pan",
      "Wanjie Sun",
      "Yingbin Wang",
      "Yan Zhou",
      "Han Zhu",
      "Shan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Chen_CNN-Optimized_Image_Compression_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Chen_CNN-Optimized_Image_Compression_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we provide the description of our approach designed for participating the CVPR 2018 Challenge on Learned Image Compression (CLIC). Our approach is a hybrid image coder based on CNN-optimized in-loop filter and mode coding, with uncertainty based resource allocation for compressing the task images. Two solutions were submitted, i.e., \"iipTiramisu\" and its speedup version \"iip-TiramisuS\", resulting in 32.14 dB and 32.06 dB in PSNR, respectively. These two results have been ranked No. 1 and 2 on the leaderboard.",
    "code_link": ""
  },
  "cvpr2018_w50_combinetraditionalcompressionmethodwithconvolutionalneuralnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Combine Traditional Compression Method With Convolutional Neural Networks",
    "authors": [
      "Jianhua Hu",
      "Ming Li",
      "Changsheng Xia",
      "Yundong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Jianhua_Hu_Combine_Traditional_Compression_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Jianhua_Hu_Combine_Traditional_Compression_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Deep learning, e.g., convolutional neural networks (CNNs), has achieved great success in image processing and computer vision tasks like classification, detection and image compression. We propose a method by combining convolution neural networks and traditional compression method. The prepositive compression comes from the SVAC2(which is drafted and maintained by VimicroAI and China's Ministry of Public Security) video codec. We further improve the SVAC2 by adopting a recovering CNN network after the reconstruction. Our approach outperforms JPEG/JPEG2000/WebP standards, and is equivalent to BPG.",
    "code_link": ""
  },
  "cvpr2018_w50_compressionartifactremovalusingmulti-scalereshufflingconvolutionalnetwork": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Compression artifact removal using multi-scale reshuffling convolutional network",
    "authors": [
      "Zhimin Tang",
      "Linkai Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Tang_Compression_artifact_removal_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Tang_Compression_artifact_removal_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this work, we aim to build a high efficient deep network to remove artifact of compressed image. The degeneracy and small receptive field problem might be caused by reducing the computational cost of convolutional network via general approaches, such as pooling features to low resolution space, reducing the width of network and using small size convolutional kernel. For the reasons, we propose a multi-scale reshuffling network to efficiently reduce the compression artifact of compressed images without degeneracy. We firstly present a reshuffling network which includes a downscaling and a upscaling reshuffling operation. The downscaling reshuffling periodically rearranges high resolution to low resolution space without any information loss. The upscaling reshuffling is the reverse transformation of downscaling reshuffling, which allows us reconstructing high resolution image from the low resolution features. A densely connected structure is applied to efficiently extract features without degeneracy. The low resolution representations is gradually recovered to the higher resolution spaces which leads to a multi-scale structure. Results show the effectiveness of the proposed method.",
    "code_link": ""
  },
  "cvpr2018_w50_decodersideimagequalityenhancementexploitinginter-channelcorrelationina3-stagecnnsubmissiontoclic2018": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Decoder Side Image Quality Enhancement exploiting Inter-channel Correlation in a 3-stage CNN: Submission to CLIC 2018",
    "authors": [
      "Kai Cui",
      "Eckehard Steinbach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Cui_Decoder_Side_Image_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Cui_Decoder_Side_Image_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper, we describe our submission to the workshop and challenge on learned image compression (CLIC) hosted at CVPR 2018. Lossy compressed images usually suffer from unpleasant artifacts, especially when the bit-rate is low. In order to improve the image quality without spending extra bit-rate, decoder side quality enhancement becomes necessary. Most approaches focus on spatial information exploration, in which the quality enhancement is usually only performed on the luminance component or the gray-scale images which makes the inter-channel correlation is neglected. Motivated by the characteristics of compressed images, a 3-stage CNN based approach is proposed in this paper, which can exploit most of the inter-channel correlation to enhance the image quality at the decoder side. Both objective and subjective evaluations show the noticeable quality improvements compared to Better Portable Graphics (BPG), the state-of-the-art image codec.",
    "code_link": ""
  },
  "cvpr2018_w50_deepimagecompressionviaend-to-endlearning": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Deep Image Compression via End-to-End Learning",
    "authors": [
      "Haojie Liu",
      "Tong Chen",
      "Qiu Shen",
      "Tao Yue",
      "Zhan Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Liu_Deep_Image_Compression_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Liu_Deep_Image_Compression_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present a lossy image compression method based on deep convolutional neural networks (CNNs), which outperforms the existing BPG, WebP, JPEG2000 and JPEG as measured via multi-scale structural similarity (MS-SSIM), at the same bit rate. Currently, most of the CNNs based approaches train the network using a l-2 loss between the reconstructions and the ground-truths in the pixel domain, which leads to over-smoothing results and visual quality degradation especially at a very low bit rate. Therefore, we improve the subjective quality with the combination of a perception loss and an adversarial loss additionally. To achieve better rate-distortion optimization (RDO), we also introduce an easy-to-hard transfer learning when adding quantization error and rate constraint. Finally, we evaluate our method on public Kodak and the Test Dataset P/M released by the Computer Vision Lab of ETH Zurich, resulting in averaged 7.81% and 19.1% BD-rate reduction over BPG, respectively.",
    "code_link": ""
  },
  "cvpr2018_w50_deepvqadeepnetworkarchitectureforvectorquantization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "DeepVQ: A Deep Network Architecture for Vector Quantization",
    "authors": [
      "Dang-Khoa Le Tan",
      "Huu Le",
      "Tuan Hoang",
      "Thanh-Toan Do",
      "Ngai-Man Cheung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Le_Tan_DeepVQ_A_Deep_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Le_Tan_DeepVQ_A_Deep_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Vector quantization (VQ) is a classic problem in signal processing, source coding and information theory. Leveraging recent advances in deep neural networks (DNN), this paper bridges the gap between a classic quantization problem and DNN. We introduce -- for the first time -- a deep network architecture for vector quantization (DeepVQ).Applying recent binary optimization theory, we propose a training algorithm to tackle binary constraints. Notably, our network outputsbinary codes directly.As a result, DeepVQ can perform quantization of vectors with a simple forward pass, and this overcomes the exponential complexity issue of previous VQ approaches. Experiments show that our network is able to achieve encouraging results andoutperforms recent deep learning-based clustering approaches that have been modified for VQ. Importantly, our network serves as a generic framework which can be applied for other networks in which binary constraints are required.",
    "code_link": ""
  },
  "cvpr2018_w50_deformationawareimagecompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Deformation Aware Image Compression",
    "authors": [
      "Tamar Rott Shaham",
      "Tomer Michaeli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Shaham_Deformation_Aware_Image_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Shaham_Deformation_Aware_Image_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Lossy compression algorithms aim to compactly encode images in a way which enables to restore them with minimal error. We show that a key limitation of existing algorithms is that they rely on error measures that are extremely sensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder to invest many bits in describing the exact geometry of every fine detail in the image, which is obviously wasteful, because the human visual system is indifferent to small local translations. Motivated by this observation, we propose a deformation-insensitive error measure that can be easily incorporated into any existing compression scheme. As we show, optimal compression under our criterion involves slightly deforming the input image such that it becomes more \"compressible\". Surprisingly, while these small deformations are barely noticeable, they enable the CODEC to preserve details that are otherwise completely lost. Our technique uses the CODEC as a \"black box\", thus allowing simple integration with arbitrary compression methods. Extensive experiments, including user studies, confirm that our approach significantly improves the visual quality of many CODECs. These include JPEG, JPEG 2000, WebP, BPG, and a recent deep-net method.",
    "code_link": ""
  },
  "cvpr2018_w50_extremelearnedimagecompressionwithgans": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Extreme Learned Image Compression with GANs",
    "authors": [
      "Eirikur Agustsson",
      "Michael Tschannen",
      "Fabian Mentzer",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Agustsson_Extreme_Learned_Image_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Agustsson_Extreme_Learned_Image_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods. This is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the full-resolution image and is trained in combination with a multi-scale discriminator. Additionally, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from a semantic label map extracted from the original image, therefore only requiring the storage of the preserved region and the semantic label map. A user study confirms that for low bitrates, our approach significantly outperforms state-of-the-art methods, saving up to 67% compared to the next-best method BPG.",
    "code_link": ""
  },
  "cvpr2018_w50_fullyconvolutionalmodelforvariablebitlengthandlossyhighdensitycompressionofmammograms": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Fully Convolutional Model for Variable Bit Length and Lossy High Density Compression of Mammograms",
    "authors": [
      "Aupendu Kar",
      "Sri Phani Krishna Karri",
      "Nirmalya Ghosh",
      "Ramanathan Sethuraman",
      "Debdoot Sheet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Kar_Fully_Convolutional_Model_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Kar_Fully_Convolutional_Model_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Early works on medical image compression date to the 1980's with the impetus on deployment of teleradiology systems for high-resolution digital X-ray detectors. Commercially deployed systems during the period could compress 4,096 x 4,096 sized images at 12 bpp to 2 bpp using lossless arithmetic coding, and over the years JPEG and JPEG2000 were imbibed reaching upto 0.1 bpp. Inspired by the reprise of deep learning based compression for natural images over the last two years, we propose a fully convolutional autoencoder for diagnostically relevant feature preserving lossy compression. This is followed by leveraging arithmetic coding for encapsulating high redundancy of features for further high-density code packing leading to variable bit length. We demonstrate performance on two different publicly available digital mammography datasets using peak signal-to-noise ratio (pSNR), structural similarity (SSIM) index and domain adaptability tests between datasets. At high density compression factors of >300x ( 0.04 bpp), our approach rivals JPEG and JPEG2000 as evaluated through a Radiologist's visual Turing test.",
    "code_link": ""
  },
  "cvpr2018_w50_imagecompressionwithxvc": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Image compression with xvc",
    "authors": [
      "Jonatan Samuelsson",
      "Per Hermansson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Samuelsson_Image_compression_with_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Samuelsson_Image_compression_with_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper describes xvc - a format for efficient compression of visual data - originally developed for compression of video sequences, but in the context of this paper applied to still images. The xvc codec is a block based lossy codec using a traditional approach forprediction, residual representation and entropy coding. There are no elements of Machine Learning or Artificial Neural Networks in the xvc encoder or decoder. The xvc codec offers support for tuning towards PSNR or perceptual quality. The images submitted for the CLIC challenge and the descriptions included in this paper are based on the perceptually tuned setting.",
    "code_link": "https://github.com/divideon/xvc"
  },
  "cvpr2018_w50_jointdenoisinganddecompressionusingcnnregularization": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Joint denoising and decompression using CNN regularization",
    "authors": [
      "Mario Gonzalez",
      "Javier Preciozzi",
      "Pablo Muse",
      "Andres Almansa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Gonzalez_Joint_denoising_and_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Gonzalez_Joint_denoising_and_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Wavelet compression schemes such as JPEG2000 may lead to very specific visual artifacts due to quantization of noisy wavelet coefficients. These artifacts have highly spatially-correlated structure, making it difficult to be re- moved with standard denoising algorithms. In this work, we propose a joint denoising and decompression method that combines a data-fitting term, which takes into account the quantization process, and an implicit prior learnt using a state-of-the-art denoising CNN.",
    "code_link": ""
  },
  "cvpr2018_w50_learnedcompressionartifactremovalbydeepresidualnetworks": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learned Compression Artifact Removal by Deep Residual Networks",
    "authors": [
      "Ogun Kirmemis",
      "Gonca Bakar",
      "A. Murat Tekalp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Kirmemis_Learned_Compression_Artifact_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Kirmemis_Learned_Compression_Artifact_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We propose a method for learned compression artifact removal by post-processing of BPG compressed images. We trained three networks of different sizes. We encoded input images using BPG with different QP values. We submitted the best combination of test images, encoded with different QP and post-processed by one of three networks, which satisfy the file size and decode time constraints imposed by the Challenge. The selection of the best combination is posed as an integer programming problem. Although the visual improvements in image quality is impressive, the average PSNR improvement for the results is about 0.5 dB.",
    "code_link": ""
  },
  "cvpr2018_w50_learningcompressible360degvideoisomers": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Learning Compressible 360deg Video Isomers",
    "authors": [
      "Yu-Chuan Su",
      "Kristen Grauman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Su_Learning_Compressible_360deg_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Su_Learning_Compressible_360deg_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Standard video encoders developed for conventional narrow field-of-view video are widely applied to 360deg video as well, with reasonable results. However, while this approach commits arbitrarily to a projection of the spherical frames, we observe that some orientations of a 360deg video, once projected, are more compressible than others. We introduce an approach to predict the sphere rotation that will yield the maximal compression rate. Given video clips in their original encoding, a convolutional neural network learns the association between a clip's visual content and its compressibility at different rotations of a cubemap projection. We validate our idea on thousands of video clips and multiple popular video codecs. The results show that this untapped dimension of 360deg video compression has substantial potential--\"good\" rotations are typically 8-10% more compressible than bad ones, and our learning approach can predict them reliably 82% of the time. The full report is published in the CVPR main conference.",
    "code_link": ""
  },
  "cvpr2018_w50_perceptuallyoptimizedlowbit-rateimageencoding": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Perceptually optimized low bit-rate image encoding",
    "authors": [
      "Eli Ben-David",
      "Sharon Carmel",
      "Boris Filippov",
      "Dror Gill",
      "Alexey Martemyanov",
      "Tamar Shoham",
      "Nikolay Terterov",
      "Pavel Tiktov",
      "Tom Vaughan",
      "Alexander Zheludkov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Ben-David_Perceptually_optimized_low_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Ben-David_Perceptually_optimized_low_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " In this paper we describe a system for high quality encoding of a given image set to a pre-determined, target average Bit-Per-Pixel (BPP). The proposed system uses our proprietary, patent protected, perceptual quality measure to determine the optimal allocation of bits among the images in the image set, and encodes each image using the HEVC/H.265 video encoder with a per image optimal encoding configuration and optional pre- and post-process. We employ learning methodologies both within the quality measure, and to ascertain optimal per image encoding configurations.",
    "code_link": ""
  },
  "cvpr2018_w50_performancecomparisonofconvolutionalautoencoders,generativeadversarialnetworksandsuper-resolutionforimagecompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Performance Comparison of Convolutional AutoEncoders, Generative Adversarial Networks and Super-Resolution for Image Compression",
    "authors": [
      "Zhengxue Cheng",
      "Heming Sun",
      "Masaru Takeuchi",
      "and Jiro Katto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Cheng_Performance_Comparison_of_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Cheng_Performance_Comparison_of_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " Image compression has been investigated for many decades. Recently, deep learning approaches have achieved a great success in many computer vision tasks, and are gradually used in image compression. In this paper, we develop three overall compression architectures based on convolutional autoencoders (CAEs), generative adversarial networks (GANs) as well as super-resolution (SR), and present a comprehensive performance comparison. According to experimental results, CAEs achieve better coding efficiency than JPEG by extracting compact features. GANs show potential advantages on large compression ratio and high subjective quality reconstruction. Super-resolution achieves the best rate-distortion (RD) performance among them, which is comparable to BPG.",
    "code_link": "https://github.com/lucastheis/rangecoder"
  },
  "cvpr2018_w50_variationalautoencoderforlowbit-rateimagecompression": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Variational Autoencoder for Low Bit-rate Image Compression",
    "authors": [
      "Lei Zhou",
      "Chunlei Cai",
      "Yue Gao",
      "Sanbao Su",
      "Junmin Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Zhou_Variational_Autoencoder_for_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Zhou_Variational_Autoencoder_for_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We present an end-to-end trainable image compression frameworkforlowbit-rateimagecompression. Ourmethod is based on variational autoencoder, which consists of a nonlinear encoder transformation, a uniform quantizer, a nonlinear decoder transformation and a post-processing module. The prior probability of compressed representation is modeled by a Laplacian distribution using a hyperprior autoencoder and it is trained jointly with the transformation autoencoder. In order to remove the compression artifacts and blurs for low bit-rate images, an effective convolution based post-processing module is proposed. Finally,aratecontrolalgorithmisappliedtoallocatethebits adaptively for each image, considering the bits constraint of the challenge. Across the experimental results on validation and test sets, the optimized framework trained by perceptual loss generates the best performance in terms of MS-SSIM. The results also indicate that the proposed postprocessing module can improve compression performance for both deep learning based and traditional methods, with the highest PSNR as 32.09 at the bit-rate of 0.15",
    "code_link": ""
  },
  "cvpr2018_w50_wide-activateddeepresidualnetworksbasedrestorationforbpg-compressedimages": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "Wide-activated Deep Residual Networks based Restoration for BPG-compressed Images",
    "authors": [
      "Yuchen Fan",
      "Jiahui Yu",
      "Thomas S. Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Fan_Wide-activated_Deep_Residual_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Fan_Wide-activated_Deep_Residual_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " We investigate a simple pipeline to achieve high-quality image compression under very low bit-rate. The pipeline is a stack of BPG image compression and deep network based restoration. Wide-activated deep residual networks from recent advances in image super-resolution are adopted for image restoration. Experiments demonstrate that the pipeline significantly reduces the quantity loss and remove visual artifacts for compressed images.",
    "code_link": ""
  },
  "cvpr2018_w50_yaso": {
    "conf_id": "CVPR2018",
    "conf_sub_id": "W50",
    "is_workshop": true,
    "conf_name": "CVPR2018_workshops - Workshop and Challenge on Learned Image Compression",
    "title": "YASO",
    "authors": [
      "Dong Wei",
      "Mei Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/w50/html/Dong_Wei_YASO_CVPR_2018_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2018_workshops/../content_cvpr_2018_workshops/papers/w50/Dong_Wei_YASO_CVPR_2018_paper.pdf",
    "published": "2018-06",
    "summary": " This paper presents a lossy image compression methodbased on neural network. Our architecture just consists ofa recurrent neural network (RNN)-based encoder and decoder,a binarizer. This paper makes contributions in thefollowing two aspects: 1) Preprocess the input images sothat the encoder could work on images with arbitrary size;2) Optimize the number of output channels of the binarizer,and our method ensures that the compressed image usesless than 0.15 bpp; 3) Our network is suitable for highresolutionimages thanks to a sub-pixel architecture. As aconsequence, we find that the optimized method generallyexhibits better rate-distortion performance than standardJPEG compression methods on the Kodak dataset.",
    "code_link": ""
  }
}