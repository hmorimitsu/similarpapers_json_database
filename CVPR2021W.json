{
  "cvpr2021_saiad_simulationdrivendesignandtestforsafetyofaibasedautonomousvehicles": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Simulation Driven Design and Test for Safety of AI Based Autonomous Vehicles",
    "authors": [
      "Vasu Singh",
      "Siva Kumar Sastry Hari",
      "Timothy Tsai",
      "Mandar Pitale"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Singh_Simulation_Driven_Design_and_Test_for_Safety_of_AI_Based_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Singh_Simulation_Driven_Design_and_Test_for_Safety_of_AI_Based_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " An autonomous vehicle (AV) integrates sophisticated perception and localization components to create a model of the world around it, which is then used to navigate the vehicle safely. Machine learning (ML) based models are pervasively used in these components to extract object in-formation from noisy sensor data. The requirements for these components are primarily set to achieve as high accuracy as possible. With modern AVs deploying many sensors(cameras, radars, and LiDARs), processing all the data in real-time leads to engineers making trade-offs which might result in a sub-optimal system in certain driving situations. Due to the lack of precise requirements on individual components, modular testing and validation also becomes challenging. In this paper, we formulate how to leverage top level driving scenario simulations based on AV safety goals to derive abstract world model accuracy requirements. Since the world model can contain many objects with several at-tributes and an AV extracts world model every timestep during a simulation, deriving the requirements is a computationally intensive task. We describe approaches to efficiently address the problem and derive component-level requirements, which open up new research directions to improve AV design and test methodologies. ",
    "code_link": ""
  },
  "cvpr2021_saiad_reevaluatingthesafetyimpactofinherentinterpretabilityondeepneuralnetworksforpedestriandetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Reevaluating the Safety Impact of Inherent Interpretability on Deep Neural Networks for Pedestrian Detection",
    "authors": [
      "Patrick Feifel",
      "Frank Bonarens",
      "Frank Koster"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Feifel_Reevaluating_the_Safety_Impact_of_Inherent_Interpretability_on_Deep_Neural_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Feifel_Reevaluating_the_Safety_Impact_of_Inherent_Interpretability_on_Deep_Neural_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " AI-based perception is a key factor towards the automation of driving systems. A conclusive safety argumentation must provide evidence for safe functioning. Existing safety standards are not suitable to deal with non-interpretable deep neural networks (DNN) learning from unstructured data. This work provides a proof of concept for a comprehensible requirements analysis based on an interpretable DNN. Recent work on interpretability motivates to rethink software considerations of safety standards. We describe the application of established considerations to DNNs by integrating interpretability and identifying artifacts. DNN artifacts result from a meaningful decomposition of requirements and adaptions of the perception pipeline. To prove our concept, we propose an interpretable method for the center, scale and prototype prediction (CSPP) that learns an explicitly structured latent space. The interpretability-based requirements analysis of CSPP is completed by tracing artifacts and source code to decomposed requirements. Finally, qualitative post-hoc evaluations provide evidence for the fulfillment of defined requirements for the latent space. ",
    "code_link": ""
  },
  "cvpr2021_saiad_out-of-distributiondetectionandgenerationusingsoftbrownianoffsetsamplingandautoencoders": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Out-of-Distribution Detection and Generation Using Soft Brownian Offset Sampling and Autoencoders",
    "authors": [
      "Felix Moller",
      "Diego Botache",
      "Denis Huseljic",
      "Florian Heidecker",
      "Maarten Bieshaar",
      "Bernhard Sick"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Moller_Out-of-Distribution_Detection_and_Generation_Using_Soft_Brownian_Offset_Sampling_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Moller_Out-of-Distribution_Detection_and_Generation_Using_Soft_Brownian_Offset_Sampling_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep neural networks often suffer from overconfidence which can be partly remedied by improved out-of-distribution detection. For this purpose, we propose a novel approach that allows for the generation of out-of-distribution datasets based on a given in-distribution dataset. This new dataset can then be used to improve out-of-distribution detection for the given dataset and machine learning task at hand. The samples in this dataset are with respect to the feature space close to the in-distribution dataset and therefore realistic and plausible. Hence, this dataset can also be used to safeguard neural networks, i.e., to validate the generalization performance. Our approach first generates suitable representations of an in-distribution dataset using an autoencoder and then transforms them using our novel proposed Soft Brownian Offset method. After transformation, the decoder part of the autoencoder allows for the generation of these implicit out-of-distribution samples. This newly generated dataset then allows for mixing with other datasets and thus improved training of an out-of-distribution classifier, increasing its performance. Experimentally, we show that our approach is promising for time series using synthetic data. Using our new method, we also show in a quantitative case study that we can improve the out-of-distribution detection for the MNIST dataset. Finally, we provide another case study on the synthetic generation of out-of-distribution trajectories, which can be used to validate trajectory prediction algorithms for automated driving. ",
    "code_link": ""
  },
  "cvpr2021_saiad_anunsupervisedtemporalconsistency(tc)losstoimprovetheperformanceofsemanticsegmentationnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "An Unsupervised Temporal Consistency (TC) Loss To Improve the Performance of Semantic Segmentation Networks",
    "authors": [
      "Serin Varghese",
      "Sharat Gujamagadi",
      "Marvin Klingner",
      "Nikhil Kapoor",
      "Andreas Bar",
      "Jan David Schneider",
      "Kira Maag",
      "Peter Schlicht",
      "Fabian Huger",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Varghese_An_Unsupervised_Temporal_Consistency_TC_Loss_To_Improve_the_Performance_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Varghese_An_Unsupervised_Temporal_Consistency_TC_Loss_To_Improve_the_Performance_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep neural networks (DNNs) for highly automated driving are often trained on a large and diverse dataset, and evaluation metrics are reported usually on a per-frame basis. However, when evaluated on video sequences, the predictions are often unstable between consecutive frames. As such unstable predictions over time can lead to severe safety consequences, there is a growing need to understand, evaluate, and improve the temporal consistency of DNNs. In this paper, we explore such a temporal characteristic and propose a novel unsupervised temporal consistency (TC) loss that penalizes unstable semantic segmentation predictions. This loss function is used in a two-stage training scheme to jointly optimize for both, the accuracy of semantic segmentation predictions, and its temporal consistency based on video sequences. We demonstrate that our training strategy helps in improving the temporal consistency of two state-of-the-art semantic segmentation networks on two different road-scenes datasets. We report an absolute 4.25% improvement in the mean temporal consistency (mTC) of the HRNetV2 network and an absolute 2.78% improvement on the DeepLabv3+ network, both evaluated on the Cityscapes dataset, with only a slight decrease in accuracy. When evaluating on the same video sequences using a synthetic dataset Sim KI-A, we show absolute improvements in both, accuracy (2.19% mIoU) and temporal consistency (0.21% mTC) for the DeepLabv3+ network. We confirm similar improvements for the HRNetV2 network. ",
    "code_link": ""
  },
  "cvpr2021_saiad_improvingonlineperformancepredictionforsemanticsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Improving Online Performance Prediction for Semantic Segmentation",
    "authors": [
      "Marvin Klingner",
      "Andreas Bar",
      "Marcel Mross",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Klingner_Improving_Online_Performance_Prediction_for_Semantic_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Klingner_Improving_Online_Performance_Prediction_for_Semantic_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work we address the task of observing the performance of a semantic segmentation deep neural network (DNN) during online operation, i.e., during inference, which is of high importance in safety-critical applications such as autonomous driving. Here, many high-level decisions rely on such DNNs, which are usually evaluated offline, while their performance in online operation remains unknown. To solve this problem, we propose an improved online performance prediction scheme, building on a recently proposed concept of predicting the primary semantic segmentation task's performance. This can be achieved by evaluating the auxiliary task of monocular depth estimation with a measurement supplied by a LiDAR sensor and a subsequent regression to the semantic segmentation performance. In particular, we propose (i) sequential training methods for both tasks in a multi-task training setup, (ii) to share the encoder as well as parts of the decoder between both task's networks for improved efficiency, and (iii) a temporal statistics aggregation method, which significantly reduces the performance prediction error at the cost of a small algorithmic latency. Evaluation on the KITTI dataset shows that all three aspects improve the performance prediction compared to previous approaches. ",
    "code_link": ""
  },
  "cvpr2021_saiad_plantsdontwalkonthestreetcommon-sensereasoningforreliablesemanticsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Plants Don't Walk on the Street: Common-Sense Reasoning for Reliable Semantic Segmentation",
    "authors": [
      "Linara Adilova",
      "Elena Schulz",
      "Maram Akila",
      "Sebastian Houben",
      "Jan David Schneider",
      "Fabian Huger",
      "Tim Wirtz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Adilova_Plants_Dont_Walk_on_the_Street_Common-Sense_Reasoning_for_Reliable_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Adilova_Plants_Dont_Walk_on_the_Street_Common-Sense_Reasoning_for_Reliable_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Data-driven sensor interpretation in autonomous driving can lead to highly implausible predictions as can most of the time be verified with common-sense knowledge. However, learning common knowledge only from data is hard and approaches for knowledge integration are an active research area. We propose to use a partly human-designed, partly learned set of rules to describe relations between objects of a traffic scene on a high level of abstraction. In doing so, we improve and robustify existing deep neural networks consuming low-level sensor information. We present an initial study adapting the well-established Probabilistic Soft Logic (PSL) framework to validate and improve on the problem of semantic segmentation. We describe in detail how we integrate common knowledge into the segmentation pipeline using PSL and verify our approach in a set of experiments demonstrating the increase in robustness against several severe image distortions applied to the A2D21 autonomous driving data set. ",
    "code_link": "https://github.com/nianticlabs/monodepth2"
  },
  "cvpr2021_saiad_boostingadversarialrobustnessusingfeaturelevelstochasticsmoothing": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Boosting Adversarial Robustness Using Feature Level Stochastic Smoothing",
    "authors": [
      "Sravanti Addepalli",
      "Samyak Jain",
      "Gaurang Sriramanan",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Addepalli_Boosting_Adversarial_Robustness_Using_Feature_Level_Stochastic_Smoothing_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Addepalli_Boosting_Adversarial_Robustness_Using_Feature_Level_Stochastic_Smoothing_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Advances in adversarial defenses have led to a significant improvement in the robustness of Deep Neural Networks. However, the robust accuracy of present state-of-the-art defenses is far from the requirements in critical applications such as robotics and autonomous navigation systems. Further, in practical use cases, network prediction alone might not suffice, and assignment of a confidence value for the prediction can prove crucial. In this work, we propose a generic method for introducing stochasticity in the network predictions, and utilize this for smoothing decision boundaries and rejecting low confidence predictions, thereby boosting the robustness on accepted samples. The proposed Feature Level Stochastic Smoothing based classification also results in a boost in robustness without rejection over existing adversarial training methods. Finally, we combine the proposed method with adversarial detection methods, to achieve the benefits of both approaches. ",
    "code_link": "https://github.com/val-iisc/FLSS"
  },
  "cvpr2021_saiad_detectinganomaliesinsemanticsegmentationwithprototypes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Detecting Anomalies in Semantic Segmentation With Prototypes",
    "authors": [
      "Dario Fontanel",
      "Fabio Cermelli",
      "Massimiliano Mancini",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Fontanel_Detecting_Anomalies_in_Semantic_Segmentation_With_Prototypes_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Fontanel_Detecting_Anomalies_in_Semantic_Segmentation_With_Prototypes_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Traditional semantic segmentation methods can recognize at test time only the classes that are present in the training set. This is a significant limitation, especially for semantic segmentation algorithms mounted on intelligent autonomous systems, deployed in realistic settings. Regardless of how many classes the system has seen at training time, it is inevitable that unexpected, unknown objects will appear at test time. The failure in identifying such anomalies may lead to incorrect, even dangerous behaviors of the autonomous agent equipped with such segmentation model when deployed in the real world. Current state of the art of anomaly segmentation uses generative models, exploiting their incapability to reconstruct patterns unseen during training. However, training these models is expensive, and their generated artifacts may create false anomalies. In this paper we take a different route and we propose to address anomaly segmentation through prototype learning. Our intuition is that anomalous pixels are those that are dissimilar to all class prototypes known by the model. We extract class prototypes from the training data in a lightweight manner using a cosine similarity-based classifier. Experiments on StreetHazards show that our approach achieves the new state of the art, with a significant margin over previous works, despite the reduced computational overhead. Code is available at https://github.com/DarioFontanel/PAnS. ",
    "code_link": "https://github.com/DarioFontanel/PAnS"
  },
  "cvpr2021_saiad_developmentmethodologiesforsafetycriticalmachinelearningapplicationsintheautomotivedomainasurvey": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Development Methodologies for Safety Critical Machine Learning Applications in the Automotive Domain: A Survey",
    "authors": [
      "Martin Rabe",
      "Stefan Milz",
      "Patrick Mader"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Rabe_Development_Methodologies_for_Safety_Critical_Machine_Learning_Applications_in_the_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Rabe_Development_Methodologies_for_Safety_Critical_Machine_Learning_Applications_in_the_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Enabled by recent advances in the field of machine learning, the automotive industry pushes towards automated driving. The development of traditional safety-critical automotive software is subject to rigorous processes, ensuring its dependability while decreasing the probability of failures. However, the development and training of machine learning applications substantially differs from traditional software development. The processes and methodologies traditionally prescribed are unfit to account for specifics like, e.g., the importance of datasets for a development. We perform a systematic mapping study surveying methodologies proposed for the development of machine learning applications in the automotive domain. We map the identified primary publications to a general machine learning-based development process and preliminary assess their maturity. The reviews's goal is providing a holistic view of current and previous research contributing to ML-aware development processes and identifying challenges that need more attention. Additionally, we list methods, network architectures, and datasets used within these publications. Our meta-study identifies that model training and model V&V received by far the most research attention accompanied by the most mature evaluations. The remaining development phases, concerning domain specification, data management, and model integration, appear underrepresented and in need of more thorough research. Additionally, we identify and aggregate typically methods applied when developing automated driving applications like models, datasets and simulators showing the state of practice in this field. ",
    "code_link": "https://github.com/RimEl-Ballouli/NGSIM-US-101-trajectory-datasetsmoothing"
  },
  "cvpr2021_saiad_towardsblack-boxexplainabilitywithgaussiandiscriminantknowledgedistillation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Towards Black-Box Explainability With Gaussian Discriminant Knowledge Distillation",
    "authors": [
      "Anselm Haselhoff",
      "Jan Kronenberger",
      "Fabian Kuppers",
      "Jonas Schneider"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Haselhoff_Towards_Black-Box_Explainability_With_Gaussian_Discriminant_Knowledge_Distillation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Haselhoff_Towards_Black-Box_Explainability_With_Gaussian_Discriminant_Knowledge_Distillation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a method for post-hoc explainability of black-box models. The key component of the semantic and quantitative local explanation is a knowledge distillation (KD) process which is used to mimic the teacher's behavior by means of an explainable generative model. Therefore, we introduce a Concept Probability Density Encoder (CPDE) in conjunction with a Gaussian Discriminant Decoder (GDD) to describe the contribution of high-level concepts (e.g. object parts, color, shape). We argue that our objective function encourages both, an explanation given by a set of likelihood ratios and a measure to describe how far the explainer deviates from the training data distribution of the concepts. The method can leverage any pre-trained concept classifier that admits concept scores (e.g. logits) or probabilities. We demonstrate the effectiveness of the proposed method in the context of object detection utilizing the DensePose dataset. ",
    "code_link": ""
  },
  "cvpr2021_saiad_patchshortcutsinterpretableproxymodelsefficientlyfindblack-boxvulnerabilities": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box Vulnerabilities",
    "authors": [
      "Julia Rosenzweig",
      "Joachim Sicking",
      "Sebastian Houben",
      "Michael Mock",
      "Maram Akila"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Rosenzweig_Patch_Shortcuts_Interpretable_Proxy_Models_Efficiently_Find_Black-Box_Vulnerabilities_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Rosenzweig_Patch_Shortcuts_Interpretable_Proxy_Models_Efficiently_Find_Black-Box_Vulnerabilities_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " An important pillar for safe machine learning (ML) is the systematic mitigation of weaknesses in neural networks to afford their deployment in critical applications. An ubiquitous class of safety risks are learned shortcuts, i.e. spurious correlations a network exploits for its decisions that have no semantic connection to the actual task. Networks relying on such shortcuts bear the risk of not generalizing well to unseen inputs. Explainability methods help to uncover such network vulnerabilities. However, many of these techniques are not directly applicable if access to the network is constrained, in so-called black-box setups. These setups are prevalent when using third-party ML components. To address this constraint, we present an approach to detect learned shortcuts using an interpretable-by-design network as a proxy to the black-box model of interest. Leveraging the proxy's guarantees on introspection we automatically extract candidates for learned shortcuts. Their transferability to the black box is validated in a systematic fashion. Concretely, as proxy model we choose a BagNet, which bases its decisions purely on local image patches. We demonstrate on the autonomous driving dataset A2D2 that extracted patch shortcuts significantly influence the black box model. By efficiently identifying such patch-based vulnerabilities, we contribute to safer ML models. ",
    "code_link": ""
  },
  "cvpr2021_saiad_safesointerpretableandexplainabledeeplearningapproachforseatoccupancyclassificationinvehicleinterior": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "SafeSO: Interpretable and Explainable Deep Learning Approach for Seat Occupancy Classification in Vehicle Interior",
    "authors": [
      "Joanna Jaworek-Korjakowska",
      "Aleksander Kostuch",
      "Pawel Skruch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Jaworek-Korjakowska_SafeSO_Interpretable_and_Explainable_Deep_Learning_Approach_for_Seat_Occupancy_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Jaworek-Korjakowska_SafeSO_Interpretable_and_Explainable_Deep_Learning_Approach_for_Seat_Occupancy_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Classification of seat occupancy in in-vehicle interior remains a significant challenge and is a promising area in the functionality of new generation cars. As majority of accidents are related to the driver errors the consequences of not wearing, or improperly wearing, a seat belt are clear. The NHTSA reports that 47% of the 22,215 passenger vehicle occupants killed in 2019 were not wearing seat belts. To address this problem we propose a deep learning based framework to classify seat occupancy into seven most important categories. In this study, we present an interpretable and explainable AI approach that takes advantage of pre-trained networks including ResNet152V2, DenseNet121 and the most recent EfficientNetB0-B5-B7 to calculate the feature vectors followed by an adjusted densely-connected classifier. Our model provides an interpretation of its results through the identification of object parts without direct supervision and their contribution towards classification. We explore and propose two new statistical metrics including HGD_ score and HGDA_score which are based on the multivariate Gaussian distribution for assessing heatmaps without using human-annotated object parts to quantify the interpretability of our network. We demonstrate that the calculated statistical metrics lead to an interpretable model that correlates with the framework accuracy and can flexibly analyze heatmaps at any resolution for different user needs. Furthermore, extensive experiments have been performed on the SVIRO database including 7,500 sceneries for BMW X5 model which confirm the ability of the developed framework based on the EfficientNetB5 architecture to classify seat occupancy into seven main categories with 79.87% overall accuracy as well as 95.92% recall and 90.32% specificity for empty seats recognition, which is a state-of-the-art result in this domain. ",
    "code_link": ""
  },
  "cvpr2021_saiad_fromevaluationtoverificationtowardstask-orientedrelevancemetricsforpedestriandetectioninsafety-criticaldomains": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "From Evaluation to Verification: Towards Task-Oriented Relevance Metrics for Pedestrian Detection in Safety-Critical Domains",
    "authors": [
      "Maria Lyssenko",
      "Christoph Gladisch",
      "Christian Heinzemann",
      "Matthias Woehrle",
      "Rudolph Triebel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Lyssenko_From_Evaluation_to_Verification_Towards_Task-Oriented_Relevance_Metrics_for_Pedestrian_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Lyssenko_From_Evaluation_to_Verification_Towards_Task-Oriented_Relevance_Metrics_for_Pedestrian_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Whenever a visual perception system is employed in safety-critical applications such as automated driving, a thorough, task-oriented experimental evaluation is necessary to guarantee safe system behavior. While most standard evaluation methods in computer vision provide a good comparability on benchmarks, they tend to fall short on assessing the system performance that is actually relevant for the given task. In our work, we consider pedestrian detection as a highly relevant perception task, and we argue that standard measures such as Intersection over Union (IoU) give insufficient results, mainly because they are insensitive to important physical cues including distance, speed, and direction of motion. Therefore, we investigate so-called relevance metrics, where specific domain knowledge is exploited to obtain a task-oriented performance measure focusing on distance in this initial work. Our experimental setup is based on the CARLA simulator and allows a controlled evaluation of the impact of that domain knowledge. Our first results indicate a linear decrease of the IoU related to the pedestrians' distance, leading to the proposal of a first relevance metric that is also conditioned on the distance. ",
    "code_link": "https://github.com/jfzhang95/pytorch"
  },
  "cvpr2021_saiad_adversarialrobustmodelcompressionusingin-trainpruning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Adversarial Robust Model Compression Using In-Train Pruning",
    "authors": [
      "Manoj-Rohit Vemparala",
      "Nael Fasfous",
      "Alexander Frickenstein",
      "Sreetama Sarkar",
      "Qi Zhao",
      "Sabine Kuhn",
      "Lukas Frickenstein",
      "Anmol Singh",
      "Christian Unger",
      "Naveen-Shankar Nagaraja",
      "Christian Wressnegger",
      "Walter Stechele"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Vemparala_Adversarial_Robust_Model_Compression_Using_In-Train_Pruning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Vemparala_Adversarial_Robust_Model_Compression_Using_In-Train_Pruning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Efficiently deploying learning-based systems on embedded hardware is challenging for various reasons, two of which are considered in this paper: The model's size andits robustness against attacks. Both need to be addressed even-handedly. We combine adversarial training and model pruning in a joint formulation of the fundamental learning objective during training. Unlike existing post train pruning approaches, our method does not use heuristics and eliminates the need for a pre-trained model. This allows for a classifier which is robust against attacks and enables better compression of the model, reducing its computational effort. In comparison to prior work, our approach yields 6.21pp higher accuracy for an 85 % reduction in parameters for ResNet20 on the CIFAR-10 dataset. ",
    "code_link": ""
  },
  "cvpr2021_saiad_sparseactivationmapsforinterpreting3dobjectdetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SAIAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Sparse Activation Maps for Interpreting 3D Object Detection",
    "authors": [
      "Qiuxiao Chen",
      "Pengfei Li",
      "Meng Xu",
      "Xiaojun Qi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/html/Chen_Sparse_Activation_Maps_for_Interpreting_3D_Object_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Chen_Sparse_Activation_Maps_for_Interpreting_3D_Object_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a technique to generate \"visual explanations\" for interpretability of volumetric-based 3D object detection networks. Specifically, we use the average pooling of weights to produce a Sparse Activation Map (SAM) which highlights the important regions of the 3D point cloud data. The SAMs is applicable to any volumetric-based models (model agnostic) to provide intuitive intermediate results at different layers to understand the complexity of network structures. The SAMs at the 3D feature map layer and the 2D feature map layer help to understand the effectiveness of neurons to capture the object information. The SAMs at the classification layer for each object class helps to understand the true positives and false positives of each network. The experimental results on the KITTI dataset demonstrate the visual observations of the SAM match the detection results of three volumetric-based models. ",
    "code_link": ""
  },
  "cvpr2021_ntire_multi-scaleself-calibratednetworkforimagelightsourcetransfer": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Scale Self-Calibrated Network for Image Light Source Transfer",
    "authors": [
      "Yuanzhi Wang",
      "Tao Lu",
      "Yanduo Zhang",
      "Yuntao Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Wang_Multi-Scale_Self-Calibrated_Network_for_Image_Light_Source_Transfer_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Wang_Multi-Scale_Self-Calibrated_Network_for_Image_Light_Source_Transfer_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image light source transfer (LLST), as the most challenging task in the domain of image relighting, has attracted extensive attention in recent years. In the latest research, LLST is decomposed three sub-tasks: scene reconversion, shadow estimation, and image re-rendering, which provides a new paradigm for image relighting. However, many problems for scene reconversion and shadow estimation tasks, including uncalibrated feature information and poor semantic information, are still unresolved, thereby resulting in insufficient feature representation. In this paper, we propose novel down-sampling feature self-calibrated block (DFSB) and up-sampling feature self-calibrated block (UFSB) as the basic blocks of feature encoder and decoder to calibrate feature representation iteratively because the LLST is similar to the recalibration of image light source. In addition, we fuse the multi-scale features of the decoder in scene reconversion task to further explore and exploit more semantic information, thereby providing more accurate primary scene structure for image re-rendering. Experimental results in the VIDIT dataset show that the proposed approach significantly improves the performance for LLST. Codes have been released at https://github.com/mdswyz/MCN-light-source-transfer. ",
    "code_link": ""
  },
  "cvpr2021_ntire_physicallyinspireddensefusionnetworksforrelighting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Physically Inspired Dense Fusion Networks for Relighting",
    "authors": [
      "Amirsaeed Yazdani",
      "Tiantong Guo",
      "Vishal Monga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yazdani_Physically_Inspired_Dense_Fusion_Networks_for_Relighting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yazdani_Physically_Inspired_Dense_Fusion_Networks_for_Relighting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image relighting has emerged as a problem of significant research interest inspired by augmented reality applications. Physics-based traditional methods, as well as black box deep learning models, have been developed. The existing deep networks have exploited training to achieve a new state of the art; however, they may perform poorly when training is limited or does not represent problem phenomenology, such as the addition or removal of dense shadows. We propose a model which enriches neural networks with physical insight. More precisely, our method generates the relighted image with new illumination settings via two different strategies and subsequently fuses them using a weight map (w). In the first strategy, our model predicts the material reflectance parameters (albedo) and illumination/geometry parameters of the scene (shading) for the relit image (we refer to this strategy as intrinsic image decomposition (IID)). The second strategy is solely based on the black box approach, where the model optimizes its weights based on the ground-truth images and the loss terms in the training stage and generates the relit output directly (we refer to this strategy as direct). While our proposed method applies to both one-to-one and any-to-any relighting problems, for each case we introduce problem-specific components that enrich the model performance: 1) For one-to-one relighting we incorporate normal vectors of the surfaces in the scene to adjust gloss and shadows accordingly in the image. 2) For any-to-any relighting, we propose an additional multiscale block to the architecture to enhance feature extraction. Experimental results on the VIDIT 2020 and the VIDIT 2021 dataset (used in the NTIRE 2021 relighting challenge) reveals that our proposal can outperform many state-of-the-art methods in terms of well-known fidelity metrics and perceptual loss. ",
    "code_link": ""
  },
  "cvpr2021_ntire_noiseconditionalflowmodelforlearningthesuper-resolutionspace": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Noise Conditional Flow Model for Learning the Super-Resolution Space",
    "authors": [
      "Younggeun Kim",
      "Donghee Son"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Kim_Noise_Conditional_Flow_Model_for_Learning_the_Super-Resolution_Space_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Kim_Noise_Conditional_Flow_Model_for_Learning_the_Super-Resolution_Space_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Fundamentally, super-resolution is ill-posed problem because a low-resolution image can be obtained from many high-resolution images. Recent studies for super-resolution cannot create diverse super-resolution images. Although SRFlow tried to account for ill-posed nature of the super-resolution by predicting multiple high-resolution images given a low-resolution image, there is room to improve the diversity and visual quality. In this paper, we propose Noise Conditional flow model for Super-Resolution, NCSR, which increases the visual quality and diversity of images through noise conditional layer. To learn more diverse data distribution, we add noise to training data. However, low-quality images are resulted from adding noise. We propose the noise conditional layer to overcome this phenomenon. The noise conditional layer makes our model generate more diverse images with higher visual quality than other works. Furthermore, we show that this layer can overcome data distribution mismatch, a problem that arises in normalizing flow models. With these benefits, NCSR outperforms baseline in diversity and visual quality and achieves better visual quality than traditional GAN-based models. We also get outperformed scores at NTIRE 2021 challenge. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengeonimagedeblurring": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on Image Deblurring",
    "authors": [
      "Seungjun Nah",
      "Sanghyun Son",
      "Suyoung Lee",
      "Radu Timofte",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Motion blur is a common photography artifact in dynamic environments that typically comes jointly with the other types of degradation. This paper reviews the NTIRE 2021 Challenge on Image Deblurring. In this challenge report, we describe the challenge specifics and the evaluation results from the 2 competition tracks with the proposed solutions. While both the tracks aim to recover a high-quality clean image from a blurry image, different artifacts are jointly involved. In track 1, the blurry images are in a low resolution while track 2 images are compressed in JPEG format. In each competition, there were 338 and 238 registered participants and in the final testing phase, 18 and 17 teams competed. The winning methods demonstrate the state-of-the-art performance on the image deblurring task with the jointly combined artifacts. ",
    "code_link": ""
  },
  "cvpr2021_ntire_long-tailedrecognitionofsaraerialviewobjectsbycascadingandparallelingexperts": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Long-Tailed Recognition of SAR Aerial View Objects by Cascading and Paralleling Experts",
    "authors": [
      "Cheng-Yen Yang",
      "Hung-Min Hsu",
      "Jiarui Cai",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yang_Long-Tailed_Recognition_of_SAR_Aerial_View_Objects_by_Cascading_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yang_Long-Tailed_Recognition_of_SAR_Aerial_View_Objects_by_Cascading_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Aerial View Object Classification (AVOC) has started to adopt deep learning approaches with significant success in recent years but limited to optical data. On the other hand, Synthetic Aperture Radar (SAR) has wild aerial view related applications in the remote sensing field. However, SAR has received far less attention due to the special characteristics of the SAR data, which is the long-tailed distribution of the aerial view objects that increase the difficulty of classification. In this paper, we present a two-branch framework, including the cascading expert branch and paralleling expert branch, to tackle the long-tailed distribution of the dataset. Our proposed multi-expert architecture achieves 24.675% and 26.029% in the development phase and testing phase, respectively, in the NTIRE 2021 Multi-modal Aerial View Object Classification Challenge Track 1. The proposed method is proved to possess the effectiveness (top-tier performance among 157 participants) and efficiency (i.e., a lightweight architecture) for the AVOC task. ",
    "code_link": ""
  },
  "cvpr2021_ntire_kernelnetablindsuper-resolutionkernelestimationnetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "KernelNet: A Blind Super-Resolution Kernel Estimation Network",
    "authors": [
      "Mehmet Yamac",
      "Baran Ataman",
      "Aakif Nawaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yamac_KernelNet_A_Blind_Super-Resolution_Kernel_Estimation_Network_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yamac_KernelNet_A_Blind_Super-Resolution_Kernel_Estimation_Network_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recently developed deep neural network methods have achieved remarkable performance in the Super Resolution (SR) problem when applied to Low Resolution (LR) images that are obtained from High Resolution (HR) images with ideal and predefined downsampling processing, i.e., convolution with a known blurring kernel that is followed by subsampling (e.g., Bicubic). However, when these algorithms are applied to real-world images whose downsampling pattern is unknown, unlike synthetically generated LR-HR image pairs, their performance drops drastically. Blind SR problem can be defined as real-world image SR when the downsampling blurring kernel (SR kernel) is unknown. The recent SR kernel estimation techniques like KernelGAN have shown promising results in this direction despite their limited recovery performance, and their high computational complexity makes them unsuitable for real time usage like in mobile cameras. This paper proposes a modular and interpretable neural network structure, KernelNet, for the blind SR kernel estimation problem. The proposed model outperforms the state of the art SR kernel estimator, KernelGAN, by a significant margin in SR kernel reconstruction accuracy. Moreover, to the best of our knowledge, the proposed algorithm is the first one that can estimate SR kernel in real-time by performing O(1k) times faster than KernelGAN. ",
    "code_link": ""
  },
  "cvpr2021_ntire_edgeguidedprogressivelygenerativeimageoutpainting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Edge Guided Progressively Generative Image Outpainting",
    "authors": [
      "Han Lin",
      "Maurice Pagnucco",
      "Yang Song"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Lin_Edge_Guided_Progressively_Generative_Image_Outpainting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Lin_Edge_Guided_Progressively_Generative_Image_Outpainting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep-learning based generative models are proven to be capable for achieving excellent results in numerous image processing tasks with a wide range of applications. One significant improvement of deep-learning approaches compared to traditional approaches is their ability to regenerate semantically coherent images by only relying on an input with limited information. This advantage becomes even more crucial when the input size is only a very minor proportion of the output size. Such image expansion tasks can be more challenging as the missing area may originally contain many semantic features that are critical in judging the quality of an image. In this paper we propose an edge-guided generative network model for producing semantically consistent output from a small image input. Our experiments show the proposed network is able to regenerate high quality images even when some structural features are missing in the input. ",
    "code_link": "https://github.com/jimgoo/caffe-oxford102"
  },
  "cvpr2021_ntire_robustimage-to-imagecolortransferusingoptimalinliermaximization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Robust Image-to-Image Color Transfer Using Optimal Inlier Maximization",
    "authors": [
      "Magnus Oskarsson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Oskarsson_Robust_Image-to-Image_Color_Transfer_Using_Optimal_Inlier_Maximization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Oskarsson_Robust_Image-to-Image_Color_Transfer_Using_Optimal_Inlier_Maximization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper we target the color transfer estimation problem, when we have pixel-to-pixel correspondences. We present a feature-based method, that robustly fits color transforms to data containing gross outliers. Our solution is based on an optimal inlier maximization algorithm that maximizes the number of inliers in polynomial time. We introduce a simple feature detector and descriptor based on the structure tensor that gives the means for reliable matching of the color distributions in two images. Using combinatorial methods from optimization theory and a number of new minimal solvers, we can enumerate all possible stationary points to the inlier maximization problem. In order for our method to be tractable we use a decoupling of the intensity and color direction for a given RGB-vector. This enables the intensity transformation and the color direction transformation to be handled separately. Our method gives results comparable to state-of-the-art methods in the presence of little outliers, and large improvement for moderate or large amounts of outliers in the data. The proposed method has been tested in a number of imaging applications. ",
    "code_link": "https://github.com/hamburgerlady/antifeature-color-transform"
  },
  "cvpr2021_ntire_ebsrfeatureenhancedburstsuper-resolutionwithdeformablealignment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "EBSR: Feature Enhanced Burst Super-Resolution With Deformable Alignment",
    "authors": [
      "Ziwei Luo",
      "Lei Yu",
      "Xuan Mo",
      "Youwei Li",
      "Lanpeng Jia",
      "Haoqiang Fan",
      "Jian Sun",
      "Shuaicheng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Luo_EBSR_Feature_Enhanced_Burst_Super-Resolution_With_Deformable_Alignment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Luo_EBSR_Feature_Enhanced_Burst_Super-Resolution_With_Deformable_Alignment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a novel architecture to handle the problem of multi-frame super-resolution (MFSR). The proposed framework is known as Enhanced Burst Super-Resolution (EBSR), which divides the MFSR problem into three parts: alignment, fusion, and reconstruction. We propose a Feature Enhanced Pyramid Cascading and Deformable convolution (FEPCD) module to align multiple low-resolution burst images in the feature level. And then the aligned features are fused by a Cross Non-Local Fusion (CNLF) module. Finally, the SR image is reconstructed by the Long Range Concatenation Network (LRCN). In addition, we build a cascading residual pathway structure (CR) to improve the performance. We conduct several experiments to analyze and demonstrate these modules. Our EBSR model won the champion in the real track and second place in the synthetic track in the NTIRE21 Burst Super-Resolution Challenge. ",
    "code_link": "https://github.com/Algolzw/EBSR"
  },
  "cvpr2021_ntire_learningacascadednon-localresidualnetworkforsuper-resolvingblurryimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Learning a Cascaded Non-Local Residual Network for Super-Resolving Blurry Images",
    "authors": [
      "Haoran Bai",
      "Songsheng Cheng",
      "Jinhui Tang",
      "Jinshan Pan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Bai_Learning_a_Cascaded_Non-Local_Residual_Network_for_Super-Resolving_Blurry_Images_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Bai_Learning_a_Cascaded_Non-Local_Residual_Network_for_Super-Resolving_Blurry_Images_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deblurring low-resolution images is quite challenging as blur exists in the images and the resolution of the images is low. Existing deblurring methods usually require high-resolution input while the super-resolution methods usually assume that the blur is known or small. Simply applying the deblurring and super-resolution does not solve this problem well. In this paper, we develop an effective cascaded non-local residual network which cascades the deblurring module and super-resolution module to estimate latent high-resolution images from blurry low-resolution ones. The network first uses the deblurring module to generate intermediate clear features and then develops a non-local residual network (NLRN) as the super-resolution module to generate clear high-resolution images from the intermediate clear features. To better constrain the network and reduce the training difficulty, we develop an effective constraint based on image gradients for edge preservation and adopt the progressive upsampling mechanism. We train the proposed network in an end-to-end manner. Both quantitative and qualitative results on the benchmarks demonstrate the effectiveness of the proposed method. Moreover, the proposed method achieves top-3 performance on the low-resolution track of the NTIRE 2021 Image Deblurring Challenge. ",
    "code_link": "https://github.com/csbhr/CNLRN"
  },
  "cvpr2021_ntire_ntire2021learningthesuper-resolutionspacechallenge": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Learning the Super-Resolution Space Challenge",
    "authors": [
      "Andreas Lugmayr",
      "Martin Danelljan",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Lugmayr_NTIRE_2021_Learning_the_Super-Resolution_Space_Challenge_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Lugmayr_NTIRE_2021_Learning_the_Super-Resolution_Space_Challenge_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper reviews the NTIRE 2021 challenge on learning the super-Resolution space. It focuses on the participating methods and final results. The challenge addresses the problem of learning a model capable of predicting the space of plausible super-resolution (SR) images, from a single low-resolution image. The model must thus be capable of sampling diverse outputs, rather than just generating a single SR image. The goal of the challenge is to spur research into developing learning formulations and models better suited for the highly ill-posed SR problem. And thereby advance the state-of-the-art in the broader SR field. In order to evaluate the quality of the predicted SR space, we propose a new evaluation metric and perform a comprehensive analysis of the participating methods. The challenge contains two tracks: 4x and 8x scale factor. In total, 11 teams competed in the final testing phase. ",
    "code_link": ""
  },
  "cvpr2021_ntire_(asna)anattention-basedsiamese-differenceneuralnetworkwithsurrogaterankinglossfunctionforperceptualimagequalityassessment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "(ASNA) An Attention-Based Siamese-Difference Neural Network With Surrogate Ranking Loss Function for Perceptual Image Quality Assessment",
    "authors": [
      "Seyed Mehdi Ayyoubzadeh",
      "Ali Royat"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Ayyoubzadeh_ASNA_An_Attention-Based_Siamese-Difference_Neural_Network_With_Surrogate_Ranking_Loss_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Ayyoubzadeh_ASNA_An_Attention-Based_Siamese-Difference_Neural_Network_With_Surrogate_Ranking_Loss_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recently, deep convolutional neural networks (DCNN) that leverage the adversarial training framework for image restoration and enhancement have significantly improved the processed images' sharpness. Surprisingly, although these DCNNs produced crispier images than other methods visually, they may get a lower quality score when popular measures are employed for evaluating them. Therefore it is necessary to develop a quantitative metric to reflect their performances, which is well-aligned with the perceived quality of an image. Famous quantitative metrics such as Peak signal-to-noise ratio (PSNR), The structural similarity index measure (SSIM), and Perceptual Index (PI) are not well-correlated with the mean opinion score (MOS) for an image, especially for the neural networks trained with adversarial loss functions. This paper has proposed a convolutional neural network using an extension architecture of the traditional Siamese network so-called Siamese-Difference neural network. We have equipped this architecture with the spatial and channel-wise attention mechanism to increase our method's performance. Finally, we employed an auxiliary loss function to train our model. The suggested additional cost function surrogates ranking loss to increase Spearman's rank correlation coefficient while it is differentiable concerning the neural network parameters. Our method achieved superior performance in NTIRE 2021 Perceptual Image Quality Assessment Challenge. The implementations of our proposed method are publicly available. ",
    "code_link": ""
  },
  "cvpr2021_ntire_deeplearning-baseddistortionsensitivitypredictionforfull-referenceimagequalityassessment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Learning-Based Distortion Sensitivity Prediction for Full-Reference Image Quality Assessment",
    "authors": [
      "Sewoong Ahn",
      "Yeji Choi",
      "Kwangjin Yoon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Ahn_Deep_Learning-Based_Distortion_Sensitivity_Prediction_for_Full-Reference_Image_Quality_Assessment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Ahn_Deep_Learning-Based_Distortion_Sensitivity_Prediction_for_Full-Reference_Image_Quality_Assessment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Previous full-reference image quality assessment methods aim to evaluate the quality of images impaired by traditional distortions such as JPEG, white noise, Gaussian blur, and so on. However, there is a lack of research predicting the quality of images generated by various image processing algorithms, including super-resolution, denoising, restoration, etc. Motivated by the previous model that predicts the distortion sensitivity maps, we use the DeepQA as a baseline model on a challenge database that includes various distortions. We have further improved the baseline model by dividing it into three parts and modifying each: 1) distortion encoding network, 2) sensitivity generation network, and 3) score regression. Through rigorous experiments, the proposed model achieves better prediction accuracy on the challenge database than other methods which predict the quality of images. Also, the proposed method shows better visualization results compared to the baseline model. We submitted our model in NTIRE 2021 perceptual image quality assessment challenge and won 12th in the main score. ",
    "code_link": ""
  },
  "cvpr2021_ntire_beyondjointdemosaickinganddenoisinganimageprocessingpipelineforapixel-binimagesensor": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Beyond Joint Demosaicking and Denoising: An Image Processing Pipeline for a Pixel-Bin Image Sensor",
    "authors": [
      "S M A Sharif",
      "Rizwan Ali Naqvi",
      "Mithun Biswas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Sharif_Beyond_Joint_Demosaicking_and_Denoising_An_Image_Processing_Pipeline_for_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Sharif_Beyond_Joint_Demosaicking_and_Denoising_An_Image_Processing_Pipeline_for_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Pixel binning is considered one of the most prominent solutions to tackle the hardware limitation of smartphone cameras. Despite numerous advantages, such an image sensor has to appropriate an artefact-prone non-Bayer colour filter array (CFA) to enable the binning capability. Contrarily, performing essential image signal processing (ISP) tasks like demosaicking and denoising, explicitly with such CFA patterns, makes the reconstruction process notably complicated. In this paper, we tackle the challenges of joint demosaicing and denoising (JDD) on such an image sensor by introducing a novel learning-based method. The proposed method leverages the depth and spatial attention in a deep network. The proposed network is guided by a multiterm objective function, including two novel perceptual losses to produce visually plausible images. On top of that, we stretch the proposed image processing pipeline to comprehensively reconstruct and enhance the images captured with a smartphone camera, which uses pixel binning techniques. The experimental results illustrate that the proposed method can outperform the existing methods by a noticeable margin in qualitative and quantitative comparisons. Code available: https://github.com/sharifapu/BJDD_CVPR21. ",
    "code_link": "https://github.com/sharifapu/BJDD_CVPR21"
  },
  "cvpr2021_ntire_srktdnapplyingsuperresolutionmethodtodehazingtask": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "SRKTDN: Applying Super Resolution Method to Dehazing Task",
    "authors": [
      "Tianyi Chen",
      "Jiahui Fu",
      "Wentao Jiang",
      "Chen Gao",
      "Si Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_SRKTDN_Applying_Super_Resolution_Method_to_Dehazing_Task_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Chen_SRKTDN_Applying_Super_Resolution_Method_to_Dehazing_Task_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Nonhomogeneous haze removal is a challenging problem, which does not follow the physical scattering model of haze. Numerous existing methods focus on homogeneous haze removal by generating transmission map of the image, which is not suitable for nonhomogeneous dehazing tasks. Some methods use end-to-end model but are also designed for homogeneous haze. Inspired by Knowledge Transfer Dehazing Network and Trident Dehazing Network, we propose a model with super-resolution method and knowledge transfer method. Our model consists of a teacher network, a dehaze network and a super-resolution network. The teacher network provides the dehaze network with reliable prior, the dehaze network focuses primarily on haze removal, and the super-resolution network is used to capture details in the hazy image. Ablation study shows that the super-resolution network has significant benefit to image quality. And comparison shows that our model outperforms previous state-of-the-art methods in terms of perceptual quality on NTIRE2021 NonHomogeneous Dehazing Challenge dataset, and also performs well on other datasets. ",
    "code_link": ""
  },
  "cvpr2021_ntire_shadowremovalwithpairedandunpairedlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Shadow Removal With Paired and Unpaired Learning",
    "authors": [
      "Florin-Alexandru Vasluianu",
      "Andres Romero",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Vasluianu_Shadow_Removal_With_Paired_and_Unpaired_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Vasluianu_Shadow_Removal_With_Paired_and_Unpaired_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Shadow removal is an important computer vision task aiming at the detection and successful removal of the shadow produced by an occluded light source and a photo-realistic restoration of the image contents. Decades of research produced a multitude of hand-crafted restoration techniques and, more recently, learned solutions from shadowed and shadow-free training image pairs. In this work, we propose a single image shadow removal solution via self-supervised learning by using a conditioned mask. We rely on self-supervision and jointly learn deep models to remove and add shadows to images. We derive two variants for learning from paired images and unpaired images, respectively. Our validation on the recently introduced ISTD and USR datasets demonstrate large quantitative and qualitative improvements over the state-of-the-art for both paired and unpaired learning settings. ",
    "code_link": ""
  },
  "cvpr2021_ntire_hdrunetsingleimagehdrreconstructionwithdenoisinganddequantization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "HDRUNet: Single Image HDR Reconstruction With Denoising and Dequantization",
    "authors": [
      "Xiangyu Chen",
      "Yihao Liu",
      "Zhengwen Zhang",
      "Yu Qiao",
      "Chao Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HDRUNet_Single_Image_HDR_Reconstruction_With_Denoising_and_Dequantization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Chen_HDRUNet_Single_Image_HDR_Reconstruction_With_Denoising_and_Dequantization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Most consumer-grade digital cameras can only capture a limited range of luminance in real-world scenes due to sensor constraints. Besides, noise and quantization errors are often introduced in the imaging process. In order to obtain high dynamic range (HDR) images with excellent visual quality, the most common solution is to combine multiple images with different exposures. However, it is not always feasible to obtain multiple images of the same scene and most HDR reconstruction methods ignore the noise and quantization loss. In this work, we propose a novel learning-based approach using a spatially dynamic encoder-decoder network, HDRUNet, to learn an end-to-end mapping for single image HDR reconstruction with denoising and dequantization. The network consists of a UNet-style base network to make full use of the hierarchical multi-scale information, a condition network to perform pattern-specific modulation and a weighting network for selectively retaining information. Moreover, we propose a Tanh_L1 loss function to balance the impact of over-exposed values and well-exposed values on the network learning. Our method achieves the state-of-the-art performance in quantitative comparisons and visual quality. The proposed HDRUNet model won the second place in the single frame track of NITRE2021 High Dynamic Range Challenge. The code is available at https://github.com/chxy95/HDRUNet. ",
    "code_link": ""
  },
  "cvpr2021_ntire_s3netasinglestreamstructurefordepthguidedimagerelighting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "S3Net: A Single Stream Structure for Depth Guided Image Relighting",
    "authors": [
      "Hao-Hsiang Yang",
      "Wei-Ting Chen",
      "Sy-Yen Kuo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yang_S3Net_A_Single_Stream_Structure_for_Depth_Guided_Image_Relighting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yang_S3Net_A_Single_Stream_Structure_for_Depth_Guided_Image_Relighting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Depth guided any-to-any image relighting aims to generate a relit image from the original image and corresponding depth maps to match the illumination setting of the given guided image and its depth map. To the best of our knowledge, this task is a new challenge that has not been addressed in the previous literature. To address this issue, we propose a deep learning-based neural Single Stream Structure network called S3Net for depth guided image relighting. This network is an encoder-decoder model. We concatenate all images and corresponding depth maps as the input and feed them into the model. The decoder part contains the attention module and the enhanced module to focus on the relighting-related regions in the guided images. Experiments performed on challenging benchmark show that the proposed model achieves the 3^ rdhighest SSIM in the NTIRE 2021 Depth Guided Any-to-any Relighting Challenge. ",
    "code_link": "https://github.com/dectrfov/NTIRE-2021-Depth-Guided-Image-Any-to-Any-relighting"
  },
  "cvpr2021_ntire_region-adaptivedeformablenetworkforimagequalityassessment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Region-Adaptive Deformable Network for Image Quality Assessment",
    "authors": [
      "Shuwei Shi",
      "Qingyan Bai",
      "Mingdeng Cao",
      "Weihao Xia",
      "Jiahao Wang",
      "Yifan Chen",
      "Yujiu Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Shi_Region-Adaptive_Deformable_Network_for_Image_Quality_Assessment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Shi_Region-Adaptive_Deformable_Network_for_Image_Quality_Assessment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image quality assessment (IQA) aims to assess the perceptual quality of images. The outputs of the IQA algorithms are expected to be consistent with human subjective perception. In image restoration and enhancement tasks, images generated by generative adversarial networks (GAN) can achieve better visual performance than traditional CNN-generated images, although they have spatial shift and texture noise. Unfortunately, the existing IQA methods have unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment. To this end, we propose the reference-oriented deformable convolution, which can improve the performance of an IQA network on GAN-based distortion by adaptively considering this misalignment. We further propose a patch-level attention module to enhance the interaction among different patch regions, which are processed independently in previous patch-based methods. The modified residual block is also proposed by applying modifications to the classic residual block to construct a patch-region-based baseline called WResNet. Equipping this baseline with the two proposed modules, we further propose Region-Adaptive Deformable Network (RADN). The experiment results on the NTIRE 2021 Perceptual Image Quality Assessment Challenge dataset show the superior performance of RADN, and the ensemble approach won fourth place in the final testing phase of the challenge. ",
    "code_link": ""
  },
  "cvpr2021_ntire_singleimagedehazingusingboundedchanneldifferenceprior": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Single Image Dehazing Using Bounded Channel Difference Prior",
    "authors": [
      "Xuan Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Zhao_Single_Image_Dehazing_Using_Bounded_Channel_Difference_Prior_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Zhao_Single_Image_Dehazing_Using_Bounded_Channel_Difference_Prior_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The single image dehazing task has made significant progress recently, aiming to recover the contrast and color of the scattered image. Many patch prior based dehazing methods have been explored, and this paper proposes another single image dehazing method by analyzing the prior information of local dehazed patches. With our observation, when the estimated transmission value varies from the ground-truth transmission value to 1, the output value of a metric function decrease correspondingly, which is defined based on the difference maps among three RGB channels of local dehazed patches normalized using global atmospheric light. Under additional bounding, the local transmission value can be estimated accurately. To reduce computation time, the whole image is divided into many small patches, and within each patch, we estimate a transmission value accurately. We further use weighted interpolation and guided filtering to refine the edges and details of the rough transmission map. Finally, we evaluate the proposed method using Fattal's synthetic haze images, SOTS dataset, and a wide variety of real-world haze images. Experiments show that our method outperforms other state-of-the-art dehazing algorithms by a large margin, especially on synthetic noisy haze images. ",
    "code_link": ""
  },
  "cvpr2021_ntire_deepobjstyledeepobject-basedphotostyletransfer": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DeepObjStyle: Deep Object-Based Photo Style Transfer",
    "authors": [
      "Indra Deep Mastan",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Mastan_DeepObjStyle_Deep_Object-Based_Photo_Style_Transfer_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Mastan_DeepObjStyle_Deep_Object-Based_Photo_Style_Transfer_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " One of the major challenges of style transfer is the appropriate image features supervision between the output image and the input images (style and content). An efficient strategy would be to define an object map between the objects of the style and the content images. However, such a mapping is not well established when there are semantic objects of different types and numbers in the style and the content images. It also leads to content mismatch in the style transfer output, which could reduce the visual quality of the results. We propose an object-based style transfer approach, called DeepObjStyle, for the style supervision in the training data-independent framework. DeepObjStyle preserves the semantics of the objects and achieves better style transfer in the challenging scenario when the style and the content images have a mismatch of image features. We also perform style transfer of images containing a word cloud to demonstrate that DeepObjStyle enables an appropriate image features supervision. We validate the results using quantitative comparisons and user studies. ",
    "code_link": ""
  },
  "cvpr2021_ntire_restorationofvideoframesfromasingleblurredimagewithmotionunderstanding": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Restoration of Video Frames From a Single Blurred Image With Motion Understanding",
    "authors": [
      "Dawit Mureja Argaw",
      "Junsik Kim",
      "Francois Rameau",
      "Chaoning Zhang",
      "In So Kweon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Argaw_Restoration_of_Video_Frames_From_a_Single_Blurred_Image_With_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Argaw_Restoration_of_Video_Frames_From_a_Single_Blurred_Image_With_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a novel framework to generate clean video frames from a single motion-blurred image. While a broad range of literature focuses on recovering a single image from a blurred image, in this work, we tackle a more challenging task i.e. video restoration from a blurred image. We formulate video restoration from a single blurred image as an inverse problem by setting clean image sequence and their respective motion as latent factors, and the blurred image as an observation. Our framework is based on an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its underlying motion in an end-to-end manner. We design a loss function and regularizers with complementary properties to stabilize the training and analyze variant models of the proposed network. The effectiveness and transferability of our network are highlighted through a large set of experiments on two different types of datasets: camera rotation blurs generated from panorama scenes and dynamic motion blurs in high speed videos. ",
    "code_link": ""
  },
  "cvpr2021_ntire_iqmanetworkimagequalitymulti-scaleassessmentnetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "IQMA Network: Image Quality Multi-Scale Assessment Network",
    "authors": [
      "Haiyang Guo",
      "Yi Bin",
      "Yuqing Hou",
      "Qing Zhang",
      "Hengliang Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Guo_IQMA_Network_Image_Quality_Multi-Scale_Assessment_Network_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Guo_IQMA_Network_Image_Quality_Multi-Scale_Assessment_Network_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image Quality Assessment (IQA), which aims to provide computational models for automatically predicting perceptual image quality, is an important computer vision task with many applications. In recent years, a variety of IQA methods have been proposed based on different metric designs, which measure the quality of images affected by various types of distortion. However, with the rapid development of Generative Adversarial Networks (GAN), a new challenge has been brought to the IQA community. Especially, the GAN-based Image Reconstruction (IR) methods overfit the traditional PSNR-based IQA methods by generating images with sharper edges and texture-like noises, leading the outputs to be similar to the reference image in appearance but with loss of details. In this paper, we propose a bilateral-branch multi-scale image quality estimation network, named IQMA network. The two branches are designed with Feature Pyramid Network (FPN)-like architecture, extracting multi-scale features for patches of the reference image and corresponding patches of the distorted image separately. Then features of the same scale from both branches are sent into several scale-specific feature fusion modules. Each module performs feature fusion and a novelly designed pooling operation for corresponding features. Then several score regression modules are used to learn a quality score for each scale. Finally, image scores for different scales are fused as the quality score of the image. IQMA network has achieved 1st place on the NTIRE 21 IQA public leaderboard and 2nd place on the NTIRE 21 IQA private leaderboard, and consistently outperforms existing state-of-the-art (SOTA) methods on LIVE and TID2013. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ltnetlighttransfernetworkfordepthguidedimagerelighting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "LTNet: Light Transfer Network for Depth Guided Image Relighting",
    "authors": [
      "Yu Zhu",
      "Bosong Ding",
      "Chenghua Li",
      "Wanli Qian",
      "Fangya Li",
      "Yiheng Yao",
      "Ruipeng Gang",
      "Chunjie Zhang",
      "Jian Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Zhu_LTNet_Light_Transfer_Network_for_Depth_Guided_Image_Relighting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Zhu_LTNet_Light_Transfer_Network_for_Depth_Guided_Image_Relighting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Relighting is an interesting yet challenging low-level vision problem, which aims to re-render the scene with new light sources. In this paper, we introduce LTNet, a novel framework for image relighting. Unlike previous methods, we propose to solve this challenging problem by decoupling the enhancement process. Specifically, we propose to train a network that focuses on learning light variations. Our key insight is that light variations are the critical information to be learned because the scene stays unchanged during the light transfer process. To this end, we employ a global residual connection and corresponding residual loss for capturing light variations. Experimental results show that the proposed method achieves better visual quality on the VIDIT dataset in the NTIRE2021 relighting challenge. ",
    "code_link": "https://github.com/lchia/relighting_track1_ntire2021"
  },
  "cvpr2021_ntire_pixel-guideddual-branchattentionnetworkforjointimagedeblurringandsuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Pixel-Guided Dual-Branch Attention Network for Joint Image Deblurring and Super-Resolution",
    "authors": [
      "Si Xi",
      "Jia Wei",
      "Weidong Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Xi_Pixel-Guided_Dual-Branch_Attention_Network_for_Joint_Image_Deblurring_and_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Xi_Pixel-Guided_Dual-Branch_Attention_Network_for_Joint_Image_Deblurring_and_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image deblurring and super-resolution (SR) are computer vision tasks aiming to restore image detail and spatial scale, respectively. Besides, only a few recent literatures contribute to this task, as conventional methods deal with SR or deblurring separately. We focus on designing a novel Pixel-Guided dual-branch attention network (PDAN) that handles both tasks jointly to address this issue. Then, we propose a novel loss function better focus on large and medium range errors. Extensive experiments demonstrated that the proposed PDAN with the novel loss function not only generates remarkably clear HR images and achieves compelling results for joint image deblurring and SR tasks. In addition, our method achieves second place in NTIRE 2021 Challenge on track 1 of Image Deblurring Challenge. ",
    "code_link": ""
  },
  "cvpr2021_ntire_unifyingguidedandunguidedoutdoorimagesynthesis": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unifying Guided and Unguided Outdoor Image Synthesis",
    "authors": [
      "Muhammad Usman Rafique",
      "Yu Zhang",
      "Benjamin Brodie",
      "Nathan Jacobs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Rafique_Unifying_Guided_and_Unguided_Outdoor_Image_Synthesis_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Rafique_Unifying_Guided_and_Unguided_Outdoor_Image_Synthesis_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Given a source image, our goal is to synthesize novel images of the same scene under different conditions, which could include changes in the time of day, season, or weather conditions. We consider two variants, unguided and guided synthesis, both of which require a way to generate diverse output images that cover the range of possible conditions. For the former task, the layout of the output image should match the source image and the conditions should appear realistic. For the latter task, the conditions should match those of a provided auxiliary guidance image. We address both tasks simultaneously using a probabilistic formulation, with separate distributions for each task, and use an end-to-end training method. We draw samples from these distributions to synthesize plausible images of the source scene. We prepare a new large-scale dataset and propose three benchmark tasks. The dataset, benchmarks, and evaluation code are available at https://mvrl.github.io/un_guided. ",
    "code_link": ""
  },
  "cvpr2021_ntire_efficientspace-timevideosuperresolutionusinglow-resolutionflowandmaskupsampling": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Efficient Space-Time Video Super Resolution Using Low-Resolution Flow and Mask Upsampling",
    "authors": [
      "Saikat Dutta",
      "Nisarg A. Shah",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Dutta_Efficient_Space-Time_Video_Super_Resolution_Using_Low-Resolution_Flow_and_Mask_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Dutta_Efficient_Space-Time_Video_Super_Resolution_Using_Low-Resolution_Flow_and_Mask_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set. ",
    "code_link": ""
  },
  "cvpr2021_ntire_atwo-stagedeepnetworkforhighdynamicrangeimagereconstruction": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Two-Stage Deep Network for High Dynamic Range Image Reconstruction",
    "authors": [
      "S M A Sharif",
      "Rizwan Ali Naqvi",
      "Mithun Biswas",
      "Sungjun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Sharif_A_Two-Stage_Deep_Network_for_High_Dynamic_Range_Image_Reconstruction_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Sharif_A_Two-Stage_Deep_Network_for_High_Dynamic_Range_Image_Reconstruction_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Mapping a single exposure low dynamic range (LDR) image into a high dynamic range (HDR) is considered among the most strenuous image to image translation tasks due to exposure-related missing information. This study tackles the challenges of single-shot LDR to HDR mapping by proposing a novel two-stage deep network. Notably, our proposed method aims to reconstruct an HDR image without knowing hardware information, including camera response function (CRF) and exposure settings. Therefore, we aim to perform image enhancement task like denoising, exposure correction, etc., in the first stage. Additionally, the second stage of our deep network learns tone mapping and bit-expansion from a convex set of data samples. The qualitative and quantitative comparisons demonstrate that the proposed method can outperform the existing LDR to HDR works with a marginal difference. Apart from that, we collected an LDR image dataset incorporating different camera systems. The evaluation with our collected real-world LDR images illustrates that the proposed method can reconstruct plausible HDR images without presenting any visual artefacts. Code available : https://github.com/sharif-apu/twostageHDR_NTIRE21. ",
    "code_link": ""
  },
  "cvpr2021_ntire_egbimagequalityassessmentbasedonensembleofgradientboosting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "EGB: Image Quality Assessment Based on Ensemble of Gradient Boosting",
    "authors": [
      "Dounia Hammou",
      "Sid Ahmed Fezza",
      "Wassim Hamidouche"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Hammou_EGB_Image_Quality_Assessment_Based_on_Ensemble_of_Gradient_Boosting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Hammou_EGB_Image_Quality_Assessment_Based_on_Ensemble_of_Gradient_Boosting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multimedia services are constantly trying to deliver better image quality to users. To meet this need, they must have an effective and reliable tool to assess the perceptual image quality. This is particularly true for image restoration (IR) algorithms, where the image quality assessment (IQA) metric plays a key role in the development of these latter. For instance, the recent advances in IR algorithms, which are mainly due to the adoption of generative adversarial network (GAN)-based methods, have clearly shown the need for a reliable IQA metric highly correlated with human judgment. In this paper, we propose an ensemble of gradient boosting (EGB) metric based on selected features similarity and ensemble learning. First, we analyzed the capability of features extracted by different layers of deep convolutional neural network (CNN) to characterize the perceptual quality distance between the reference and distorted/processed images. We observed that a subset of these layers is more relevant to the IQA task. Accordingly, we exploited these selected layers to compute the features similarity, which are then used as input to a regression network to predict the image quality score. The regression network consists of three gradient boosting regression models that are combined to derive the final quality score. Experiments were performed on the perceptual image processing algorithms (PIPAL) dataset, which has been used in the NTIRE 2021 perceptual image quality assessment challenge. The results show that the proposed metric significantly outperforms the state-of-the-art methods for IQA task. The source code is available at: https://github.com/Dounia18/EGB. ",
    "code_link": "https://github.com/Dounia18/EGB"
  },
  "cvpr2021_ntire_improvednoise2noisedenoisingwithlimiteddata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Improved Noise2Noise Denoising With Limited Data",
    "authors": [
      "Adria Font Calvarons"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Calvarons_Improved_Noise2Noise_Denoising_With_Limited_Data_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Calvarons_Improved_Noise2Noise_Denoising_With_Limited_Data_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep learning methods have proven to be very effective for the task of image denoising even when clean reference images are not available. In particular, Noise2Noise, which requires pairs of noisy images during the training phase, has been shown to yield results as good as approaches using pairs of noisy and clean images (Noise2Clean). However, the performance of Noise2Noise drops when the amount of training data is reduced, limiting its capability in practical scenarios. In this work, an analysis of the Noise2Noise learning strategy is done using real noise and synthetic datasets. This paper demonstrates, using diverse network architectures and loss functions, that the duplicity of information in the noisy pairs can be exploited to reach increased denoising performance of Noise2Noise. Additionally, the issue of overfitting in Noise2Noise is analyzed, given its relevance when training with limited data, and an interpretable early termination criterion is proposed. ",
    "code_link": ""
  },
  "cvpr2021_ntire_variationalautoencoderforreferencebasedimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Variational AutoEncoder for Reference Based Image Super-Resolution",
    "authors": [
      "Zhi-Song Liu",
      "Wan-Chi Siu",
      "Li-Wen Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Liu_Variational_AutoEncoder_for_Reference_Based_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Liu_Variational_AutoEncoder_for_Reference_Based_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a novel reference based image super-resolution approach via Variational AutoEncoder. Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8x. We propose a reference based image super-resolution, for which any arbitrary image can act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed RefVAE can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate different versions of super-resolved images from a hidden super-resolution space. Besides using different datasets for some standard evaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space challenge and have provided results of the randomness evaluation of our approach. Compared to other state-of-the-art methods, our approach achieves higher diverse scores. ",
    "code_link": "https://github.com/Holmes-Alan/RefVAE"
  },
  "cvpr2021_ntire_self-supervisedmulti-taskpretrainingimprovesimageaestheticassessment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Self-Supervised Multi-Task Pretraining Improves Image Aesthetic Assessment",
    "authors": [
      "Jan Pfister",
      "Konstantin Kobs",
      "Andreas Hotho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Pfister_Self-Supervised_Multi-Task_Pretraining_Improves_Image_Aesthetic_Assessment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Pfister_Self-Supervised_Multi-Task_Pretraining_Improves_Image_Aesthetic_Assessment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural networks for Image Aesthetic Assessment are usually initialized with weights of pretrained ImageNet models and then trained using a labeled image aesthetics dataset. We argue that the ImageNet classification task is not well-suited for pretraining, since content based classification is designed to make the model invariant to features that strongly influence the image's aesthetics, e.g. style-based features such as brightness or contrast. We propose to use self-supervised aesthetic-aware pretext tasks that let the network learn aesthetically relevant features, based on the observation that distorting aesthetic images with image filters usually reduces their appeal. To ensure that images are not accidentally improved when filters are applied, we introduce a large dataset comprised of highly aesthetic images as the starting point for the distortions. The network is then trained to rank less distorted images higher than their more distorted counterparts. To exploit effects of multiple different objectives, we also embed this task into a multi-task setting by adding either a self-supervised classification or regression task. In our experiments, we show that our pretraining improves performance over the ImageNet initialization and reduces the number of epochs until convergence by up to 47%. Additionally, we can match the performance of an ImageNet-initialized model while reducing the labeled training data by 20%. We make our code, data, and pretrained models available. ",
    "code_link": ""
  },
  "cvpr2021_ntire_overparametrizationofhypernetworksatfixedflop-countenablesfastneuralimageenhancement": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast Neural Image Enhancement",
    "authors": [
      "Lorenz K. Muller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Muller_Overparametrization_of_HyperNetworks_at_Fixed_FLOP-Count_Enables_Fast_Neural_Image_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Muller_Overparametrization_of_HyperNetworks_at_Fixed_FLOP-Count_Enables_Fast_Neural_Image_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep convolutional neural networks can enhance images taken with small mobile camera sensors and excel at tasks like demoisaicing, denoising and super-resolution. However, for practical use on mobile devices these networks often require too many FLOPs and reducing the FLOPs of a convolution layer, also reduces its parameter count. This is problematic in view of the recent finding that heavily over-parameterized neural networks are often the ones that generalize best. In this paper we propose to use HyperNetworks to break the fixed ratio of FLOPs to parameters of standard convolutions. This allows us to exceed previous state-of-the-art architectures in SSIM and MS-SSIM on the Zurich RAW-to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we further observe generalization curves consistent with 'double-descent' behavior at fixed FLOP-count, in the large image limit. Finally we demonstrate the same technique can be applied to an existing network (VDN) to reduce its computational cost while maintaining fidelity on the Smartphone Image Denoising Dataset (SIDD). Code for key functions is given in the supplemental. ",
    "code_link": ""
  },
  "cvpr2021_ntire_edpnenhanceddeeppyramidnetworkforblurryimagerestoration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration",
    "authors": [
      "Ruikang Xu",
      "Zeyu Xiao",
      "Jie Huang",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Xu_EDPN_Enhanced_Deep_Pyramid_Network_for_Blurry_Image_Restoration_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Xu_EDPN_Enhanced_Deep_Pyramid_Network_for_Blurry_Image_Restoration_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image deblurring has seen a great improvement with the development of deep neural networks. In practice, however, blurry images often suffer from additional degradations such as downscaling and compression. To address these challenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image restoration from multiple degradations, by fully exploiting the self- and cross-scale similarities in the degraded image. Specifically, we design two pyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and the pyramid self-attention (PSA) module, as the main components of the proposed network. By taking several replicated blurry images as inputs, the PPT module transfers both self- and cross-scale similarity information from the same degraded image in a progressive manner. Then, the PSA module fuses the above transferred features for subsequent restoration using self- and spatial-attention mechanisms. Experimental results demonstrate that our method significantly outperforms existing solutions for blurry image super-resolution and blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN achieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the best SSIM/LPIPS scores in Track 2 (JPEG Artifacts). ",
    "code_link": "https://github.com/zeyuxiao1997/EDPN"
  },
  "cvpr2021_ntire_ntire2021challengeonburstsuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results",
    "authors": [
      "Goutam Bhat",
      "Martin Danelljan",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Bhat_NTIRE_2021_Challenge_on_Burst_Super-Resolution_Methods_and_Results_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Bhat_NTIRE_2021_Challenge_on_Burst_Super-Resolution_Methods_and_Results_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a RAW noisy burst as input, the task in the challenge was to generate a clean RGB image with 4 times higher resolution. The challenge contained two tracks; Track 1 evaluating on synthetically generated data, and Track 2 using real-world bursts from mobile camera. In the final testing phase, 6 teams submitted results using a diverse set of solutions. The top-performing methods set a new state-of-the-art for the burst super-resolution task. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021nonhomogeneousdehazingchallengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 NonHomogeneous Dehazing Challenge Report",
    "authors": [
      "Codruta O. Ancuti",
      "Cosmin Ancuti",
      "Florin-Alexandru Vasluianu",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Ancuti_NTIRE_2021_NonHomogeneous_Dehazing_Challenge_Report_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Ancuti_NTIRE_2021_NonHomogeneous_Dehazing_Challenge_Report_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " NTIRE 2021 NonHomogeneous Dehazing Challenge Report ",
    "code_link": ""
  },
  "cvpr2021_ntire_guidancenetworkwithstagedlearningforimageenhancement": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Guidance Network With Staged Learning for Image Enhancement",
    "authors": [
      "Luming Liang",
      "Ilya Zharkov",
      "Faezeh Amjadi",
      "Hamid Reza Vaezi Joze",
      "Vivek Pradeep"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Liang_Guidance_Network_With_Staged_Learning_for_Image_Enhancement_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Liang_Guidance_Network_With_Staged_Learning_for_Image_Enhancement_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Many important yet not fully resolved problems in computational photography and image enhancement, e.g. generating well-lit images from their low-light counterparts or producing RGB images from their RAW camera inputs share a common nature: discovering a color mapping between input pixels to output pixels based on both global information and local details. We propose a novel deep neural network architecture to learn the RAW to RGB mapping based on this common nature. This architecture consists of both global and local sub-networks, where the first sub-network focuses on determining illumination and color mapping, the second sub-network deals with recovering image details. The result of the global network serves as a guidance to the local network to form the final RGB images. Our method outperforms state-of-the-art with a significantly smaller size of network features on various image enhancement tasks. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengefordefocusdeblurringusingdual-pixelimagesmethodsandresults": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge for Defocus Deblurring Using Dual-Pixel Images: Methods and Results",
    "authors": [
      "Abdullah Abuolaim",
      "Radu Timofte",
      "Michael S. Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Abuolaim_NTIRE_2021_Challenge_for_Defocus_Deblurring_Using_Dual-Pixel_Images_Methods_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Abuolaim_NTIRE_2021_Challenge_for_Defocus_Deblurring_Using_Dual-Pixel_Images_Methods_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper provides a review of the NTIRE 2021 challenge targeting defocus deblurring using dual-pixel (DP) data. The goal of this single-track challenge was to reduce spatially varying defocus blur present in images captured with a shallow depth of field. The images used in this challenge were obtained using a DP sensor that provided a pair of DP views per captured image. Submitted solutions were evaluated using conventional signal processing metrics, namely peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). Out of 185 registered participants, nine teams provided methods and competed in the final stage. The paper describes the methods proposed by the participating teams and their results. The winning teams represent the state-of-the-art in terms of defocus deblurring using DP images. ",
    "code_link": ""
  },
  "cvpr2021_ntire_towardinteractivemodulationforphoto-realisticimagerestoration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Toward Interactive Modulation for Photo-Realistic Image Restoration",
    "authors": [
      "Haoming Cai",
      "Jingwen He",
      "Yu Qiao",
      "Chao Dong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Cai_Toward_Interactive_Modulation_for_Photo-Realistic_Image_Restoration_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Cai_Toward_Interactive_Modulation_for_Photo-Realistic_Image_Restoration_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Modulating image restoration level aims to generate a restored image by altering a factor that represents the restoration strength. Previous works mainly focused on optimizing the mean squared reconstruction error, which brings high reconstruction accuracy but lacks finer texture details. This paper presents a Controllable Unet Generative Adversarial Network (CUGAN) to generate high-frequency textures in the modulation tasks. CUGAN consists of two modules - base networks and condition networks. The base networks comprise a generator and a discriminator. In the generator, we realize the interactive control of restoration levels by tuning the weights of different features from different scales in the Unet architecture. Moreover, we adaptively modulate the intermediate features in the discriminator according to the severity of degradations. The condition networks accept the condition vector (encoded degradation information) as input, then generate modulation parameters for both the generator and the discriminator. During testing, users can control the output effects by tweaking the condition vector. We also provide a smooth transition between GAN and MSE effects by a simple transition method. Extensive experiments demonstrate that the proposed CUGAN achieves excellent performance on image restoration modulation tasks. ",
    "code_link": ""
  },
  "cvpr2021_ntire_genericimagerestorationwithflowbasedpriors": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Generic Image Restoration With Flow Based Priors",
    "authors": [
      "Leonhard Helminger",
      "Michael Bernasconi",
      "Abdelaziz Djelouah",
      "Markus Gross",
      "Christopher Schroers"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Helminger_Generic_Image_Restoration_With_Flow_Based_Priors_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Helminger_Generic_Image_Restoration_With_Flow_Based_Priors_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image restoration has seen great progress in the last years thanks to the advances in deep neural networks. Most of these existing techniques are trained using full supervision with suitable image pairs to tackle a specific degradation. However, in a generic setting with unknown degradations this is not possible and a good prior remains crucial. Recently, neural network based approaches have been proposed to model such priors by leveraging either denoising autoencoders or the implicit regularization captured by the neural network structure itself. In contrast to this, we propose using normalizing flows to model the distribution of the target content and to use this as a prior in a maximum a posteriori (MAP) formulation. By expressing the MAP optimization process in the latent space through the learned bijective mapping, we are able to obtain solutions through gradient descent. To the best of our knowledge, this is the first work that explores normalizing flows as prior in generic image enhancement problems. Furthermore, we present experimental results for a number of different degradations on data sets varying in complexity and show competitive results when comparing with the deep image prior approach. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengeonqualityenhancementofcompressedvideodatasetandstudy": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset and Study",
    "authors": [
      "Ren Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yang_NTIRE_2021_Challenge_on_Quality_Enhancement_of_Compressed_Video_Dataset_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yang_NTIRE_2021_Challenge_on_Quality_Enhancement_of_Compressed_Video_Dataset_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces a novel dataset for video enhancement and studies the state-of-the-art methods of the NTIRE 2021 challenge on quality enhancement of compressed video. The challenge is the first NTIRE challenge in this direction, with three competitions, hundreds of participants and tens of proposed solutions. Our newly collected Large-scale Diverse Video (LDV) dataset is employed in the challenge. In our study, we analyze the solutions of the challenges and several representative methods from previous literature on the proposed LDV dataset. We find that the NTIRE 2021 challenge advances the state-of-the-art in the performance of enhancing compressed video. The proposed LDV dataset is publicly available at the homepage of the challenge: https://github.com/RenYang-home/NTIRE21_VEnh ",
    "code_link": ""
  },
  "cvpr2021_ntire_adnetattention-guideddeformableconvolutionalnetworkforhighdynamicrangeimaging": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "ADNet: Attention-Guided Deformable Convolutional Network for High Dynamic Range Imaging",
    "authors": [
      "Zhen Liu",
      "Wenjie Lin",
      "Xinpeng Li",
      "Qing Rao",
      "Ting Jiang",
      "Mingyan Han",
      "Haoqiang Fan",
      "Jian Sun",
      "Shuaicheng Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Liu_ADNet_Attention-Guided_Deformable_Convolutional_Network_for_High_Dynamic_Range_Imaging_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Liu_ADNet_Attention-Guided_Deformable_Convolutional_Network_for_High_Dynamic_Range_Imaging_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we present an attention-guided deformable convolutional network for hand-held multi-frame high dynamic range (HDR) imaging, namely ADNet. This problem comprises two intractable challenges of how to handle saturation and noise properly and how to tackle misalignments caused by object motion or camera jittering. To address the former, we adopt a spatial attention module to adaptively select the most appropriate regions of various exposure low dynamic range (LDR) images for fusion. For the latter one, we propose to align the gamma-corrected images in the feature-level with a Pyramid, Cascading and Deformable (PCD) alignment module. The proposed ADNet shows state-of-the-art performance compared with previous methods, achieving a PSNR-l of 39.4471 and a PSNR-u of 37.6359 in NTIRE 2021 Multi-Frame HDR Challenge. ",
    "code_link": "https://github.com/Pea-Shooter/ADNet"
  },
  "cvpr2021_ntire_adaptivespatial-temporalfusionofmulti-objectivenetworksforcompressedvideoperceptualenhancement": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Adaptive Spatial-Temporal Fusion of Multi-Objective Networks for Compressed Video Perceptual Enhancement",
    "authors": [
      "He Zheng",
      "Xin Li",
      "Fanglong Liu",
      "Lielin Jiang",
      "Qi Zhang",
      "Fu Li",
      "Qingqing Dang",
      "Dongliang He"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Zheng_Adaptive_Spatial-Temporal_Fusion_of_Multi-Objective_Networks_for_Compressed_Video_Perceptual_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Zheng_Adaptive_Spatial-Temporal_Fusion_of_Multi-Objective_Networks_for_Compressed_Video_Perceptual_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Perceptual quality enhancement of heavily compressed videos is a difficult, unsolved problem because there still not exists a suitable perceptual similarity loss function between two video pairs. Motivated by the fact that it is hard to design unified training objectives which are perceptual-friendly for enhancing regions with smooth content and regions with rich textures simultaneously, in this paper, we propose a simple yet effective novel solution dubbed \"Adaptive Spatial-Temporal Fusion of Two-Stage Multi-Objective Networks\" (ASTF) to adaptive fuse the enhancement results from networks trained with two different optimization objectives. Specifically, the proposed ASTF takes an enhancement frame along with its neighboring frames as input to jointly predict a mask to indicate regions with high-frequency textual details. Then we use the mask to fuse two enhancement results which can retain both smooth content and rich textures. Extensive experiments show that our method achieves a promising performance of compressed video perceptual quality enhancement. ",
    "code_link": "https://github.com/mseitzer/pytorch-fid"
  },
  "cvpr2021_ntire_atwo-branchneuralnetworkfornon-homogeneousdehazingviaensemblelearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Two-Branch Neural Network for Non-Homogeneous Dehazing via Ensemble Learning",
    "authors": [
      "Yankun Yu",
      "Huan Liu",
      "Minghan Fu",
      "Jun Chen",
      "Xiyao Wang",
      "Keyan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yu_A_Two-Branch_Neural_Network_for_Non-Homogeneous_Dehazing_via_Ensemble_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yu_A_Two-Branch_Neural_Network_for_Non-Homogeneous_Dehazing_via_Ensemble_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recently, there has been rapid and significant progress on image dehazing. Many deep learning based methods have shown their superb performance in handling homogeneous dehazing problems. However, we observe that even if a carefully designed convolutional neural network (CNN) can perform well on large-scaled dehazing benchmarks, the network usually fails on the non-homogeneous dehazing datasets introduced by NTIRE challenges. The reasons are mainly in two folds. Firstly, due to its non-homogeneous nature, the non-uniformly distributed haze is harder to be removed than the homogeneous haze. Secondly, the research challenge only provides limited data (there are only 25 training pairs in NH-Haze 2021 dataset). Thus, learning the mapping from the domain of hazy images to that of clear ones based on very limited data is extremely hard. To this end, we propose a simple but effective approach for non-homogeneous dehazing via ensemble learning. To be specific, we introduce a two-branch neural network to separately deal with the aforementioned problems and then map their distinct features by a learnable fusion tail. We show extensive experimental results to illustrate the effectiveness of our proposed method. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021depthguidedimagerelightingchallenge": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Depth Guided Image Relighting Challenge",
    "authors": [
      "Majed El Helou",
      "Ruofan Zhou",
      "Sabine Susstrunk",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Helou_NTIRE_2021_Depth_Guided_Image_Relighting_Challenge_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Helou_NTIRE_2021_Depth_Guided_Image_Relighting_Challenge_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image relighting is attracting increasing interest due to its various applications. From a research perspective, image relighting can be exploited to conduct both image normalization for domain adaptation, and also for data augmentation. It also has multiple direct uses for photo montage and aesthetic enhancement. In this paper, we review the NTIRE 2021 depth guided image relighting challenge. We rely on the VIDIT dataset for each of our two challenge tracks, including depth information. The first track is on one-to-one relighting where the goal is to transform the illumination setup of an input image (color temperature and light source position) to the target illumination setup. In the second track, the any-to-any relighting challenge, the objective is to transform the illumination settings of the input image to match those of another guide image, similar to style transfer. In both tracks, participants were given depth information about the captured scenes. We had nearly 250 registered participants, leading to 18 confirmed team submissions in the final competition stage. The competitions, methods, and final results are presented in this paper. ",
    "code_link": "https://github.com/majedelhelou/VIDIT"
  },
  "cvpr2021_ntire_weightedmulti-kernelpredictionnetworkforburstimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Weighted Multi-Kernel Prediction Network for Burst Image Super-Resolution",
    "authors": [
      "Wooyeong Cho",
      "Sanghyeok Son",
      "Dae-Shik Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Cho_Weighted_Multi-Kernel_Prediction_Network_for_Burst_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Cho_Weighted_Multi-Kernel_Prediction_Network_for_Burst_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Burst image super-resolution is an ill-posed problem that aims to restore a high-resolution (HR) image from a sequence of low-resolution (LR) burst images. To restore a photo-realistic HR image using their abundant information, it is essential to align each burst of frames containing random hand-held motion. Some kernel prediction networks (KPNs) that are operated without external motion compensation such as optical flow estimation have been applied to burst image processing as implicit image alignment modules. However, the existing methods do not consider the interdependencies among the kernels of different sizes that have a significant effect on each pixel. In this paper, we propose a novel weighted multi-kernel prediction network (WMKPN) that can learn the discriminative features on each pixel for burst image super-resolution. Our experimental results demonstrate that WMKPN improves the visual quality of super-resolved images. To the best of our knowledge, it outperforms the state-of-the-art within kernel prediction methods and multiple frame super-resolution (MFSR) on both the Zurich RAW to RGB and BurstSR datasets. ",
    "code_link": ""
  },
  "cvpr2021_ntire_crossmodalityknowledgedistillationformulti-modalaerialviewobjectclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Cross Modality Knowledge Distillation for Multi-Modal Aerial View Object Classification",
    "authors": [
      "Lehan Yang",
      "Kele Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yang_Cross_Modality_Knowledge_Distillation_for_Multi-Modal_Aerial_View_Object_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yang_Cross_Modality_Knowledge_Distillation_for_Multi-Modal_Aerial_View_Object_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In the case of bad weather or low lighting conditions, a single sensor may not be able to capture enough information for object identification. Compared with the traditional optical image, synthetic aperture radar (SAR) imaging has greater advantages, such as the ability to penetrate through fog and smoke. However, SAR images are of low resolution and contaminated by high-level speckle noise. As a result, it is of great difficulty to extract powerful and robust features from the SAR images. In this paper, we explored whether multiple imaging modalities can improve the object detection performance. Here, we propose a Cross Modality Knowledge Distillation (CMKD) paradigm, and explore two different network structures named CMKD-s and CMKD-m for the object classification task. Specifically, CMKD-s transfers the information captured by the two sensors using the online knowledge distillation, which can achieve cross-modal knowledge sharing and enhance the robustness of the aerial view object classification model. Moreover, leveraging the semi-supervised enhanced training, we proposed a novel method named CMKD-m, which strengthens the model for mutual knowledge transfer. Through quantitative comparison, we found that CMKD-s and CMKD-m outperform the method without knowledge transfer, on the NTIRE2021 SAR-EO challenge dataset. ",
    "code_link": ""
  },
  "cvpr2021_ntire_dualcontrastivelearningforunsupervisedimage-to-imagetranslation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Dual Contrastive Learning for Unsupervised Image-to-Image Translation",
    "authors": [
      "Junlin Han",
      "Mehrdad Shoeiby",
      "Lars Petersson",
      "Mohammad Ali Armin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Han_Dual_Contrastive_Learning_for_Unsupervised_Image-to-Image_Translation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Han_Dual_Contrastive_Learning_for_Unsupervised_Image-to-Image_Translation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Unsupervised image-to-image translation tasks aim to find a mapping between a source domain X and a target domain Y from unpaired training data. Contrastive learning for Unpaired image-to-image Translation (CUT) yields state-of-the-art results in modeling unsupervised image-to-image translation by maximizing mutual information between input and output patches using only one encoder for both domains. In this paper, we propose a novel method based on contrastive learning and a dual learning setting (exploiting two encoders) to infer an efficient mapping between unpaired data. Additionally, while CUT suffers from mode collapse, a variant of our method efficiently addresses this issue. We further demonstrate the advantage of our approach through extensive ablation studies demonstrating superior performance comparing to recent approaches in multiple challenging image translation tasks. Lastly, we demonstrate that the gap between unsupervised methods and supervised methods can be efficiently closed. ",
    "code_link": ""
  },
  "cvpr2021_ntire_singleimagehdrsynthesisusingadenselyconnecteddilatedconvnet": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Single Image HDR Synthesis Using a Densely Connected Dilated ConvNet",
    "authors": [
      "Akhil K. A.",
      "Jiji C. V."
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/A._Single_Image_HDR_Synthesis_Using_a_Densely_Connected_Dilated_ConvNet_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/A._Single_Image_HDR_Synthesis_Using_a_Densely_Connected_Dilated_ConvNet_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Visual representations using high dynamic range (HDR) images have become increasingly popular because of their high quality and expressive ability. HDR images are expected to be used in a broad range of applications, including digital cinema, photography, and broadcast. The generation of a HDR image from a single exposure Low Dynamic Range (LDR) image is a challenging task where one must make up for missing data due to underexposure or overexposure and color quantization. In this paper, we propose a deep convolutional neural network (CNN) model with a stack of dilated convolutional blocks for reconstructing a HDR image from a single LDR image. Within each dilation block, the dilation rate of the convolution layer is three and progressively decreases to one. Multiple dilation convolution blocks are further connected densely to improve the representation capacity of the network. As the network is trained in a supervised manner, the additional information is reconstructed from learned features. Our experimental results show that the model effectively captures missing information that was lost from the original image. ",
    "code_link": ""
  },
  "cvpr2021_ntire_dw-ganadiscretewavelettransformganfornonhomogeneousdehazing": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DW-GAN: A Discrete Wavelet Transform GAN for NonHomogeneous Dehazing",
    "authors": [
      "Minghan Fu",
      "Huan Liu",
      "Yankun Yu",
      "Jun Chen",
      "Keyan Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Fu_DW-GAN_A_Discrete_Wavelet_Transform_GAN_for_NonHomogeneous_Dehazing_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Fu_DW-GAN_A_Discrete_Wavelet_Transform_GAN_for_NonHomogeneous_Dehazing_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Hazy images are often subject to color distortion, blurring, and other visible quality degradation. Some existing CNN-based methods have great performance on removing homogeneous haze, but they are not robust in non-homogeneous case. The reasons are mainly in two folds. Firstly, due to the complicated haze distribution, texture details are easy to be lost during the dehazing process. Secondly, since the training pairs are hard to be collected, training on limited data can easily lead to over-fitting problem. To tackle these two issues, we introduce a novel dehazing network using 2D discrete wavelet transform, namely DW-GAN. Specifically, we propose a two-branch network to deal with the aforementioned problems. By utilizing wavelet transform in DWT branch, our proposed method can retain more high frequency knowledge in feature maps. In order to prevent over-fitting, ImageNet pre-trained Res2Net is adopted in the knowledge adaptation branch. Owing to the robust feature representations of ImageNet pre-training, the generalization ability of our network is improved dramatically. Finally, a patch-based discriminator is used to reduce artifacts of the restored images. Extensive experimental results demonstrate that the proposed method outperforms the state-of-the-arts quantitatively and qualitatively. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengeonvideosuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on Video Super-Resolution",
    "authors": [
      "Sanghyun Son",
      "Suyoung Lee",
      "Seungjun Nah",
      "Radu Timofte",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Son_NTIRE_2021_Challenge_on_Video_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Son_NTIRE_2021_Challenge_on_Video_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Super-Resolution (SR) is a fundamental computer vision task that aims to obtain a high-resolution clean image from the given low-resolution counterpart. This paper reviews the NTIRE 2021 Challenge on Video Super-Resolution. We present evaluation results from two competition tracks as well as the proposed solutions. Track 1 aims to develop conventional video SR methods focusing on the restoration quality. Track 2 assumes a more challenging environment with lower frame rates, casting spatio-temporal SR problem. In each competition, 247 and 223 participants have registered, respectively. During the final testing phase, 14 teams competed in each track to achieve state-of-the-art performance on video SR tasks. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengeonqualityenhancementofcompressedvideomethodsandresults": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Methods and Results",
    "authors": [
      "Ren Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yang_NTIRE_2021_Challenge_on_Quality_Enhancement_of_Compressed_Video_Methods_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yang_NTIRE_2021_Challenge_on_Quality_Enhancement_of_Compressed_Video_Methods_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper reviews the first NTIRE challenge on quality enhancement of compressed video, with focus on proposed solutions and results. In this challenge, the new Large-scale Diverse Video (LDV) dataset is employed. The challenge has three tracks. Tracks 1 and 2 aim at enhancing the videos compressed by HEVC at a fixed QP, while Track 3 is designed for enhancing the videos compressed by x265 at a fixed bit-rate. Besides, the quality enhancement of Tracks 1 and 3 targets at improving the fidelity (PSNR), and Track 2 targets at enhancing the perceptual quality. The three tracks totally attract 482 registrations. In the test phase, 12 teams, 8 teams and 11 teams submitted the final results of Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of video quality enhancement. The homepage of the challenge: https://github.com/RenYang-home/NTIRE21_VEnh ",
    "code_link": ""
  },
  "cvpr2021_ntire_pngmicro-structuredprune-and-grownetworksforflexibleimagerestoration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "PNG: Micro-Structured Prune-and-Grow Networks for Flexible Image Restoration",
    "authors": [
      "Wei Jiang",
      "Wei Wang",
      "Shan Liu",
      "Songnan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Jiang_PNG_Micro-Structured_Prune-and-Grow_Networks_for_Flexible_Image_Restoration_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Jiang_PNG_Micro-Structured_Prune-and-Grow_Networks_for_Flexible_Image_Restoration_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper addresses one major issue of DNN-based image restoration: the difficulty of using one model to fit multiple reconstruction requirements, such as supporting different compression ratios in neural image compression (NIC) or different zooming scales in single image super-resolution (SISR). Instead of training an independent model instance for each requirement as an individual task, we develop a practical solution that uses one model instance to support multiple requirements. We propose a general multi-task learning framework based on a novel prune-and-grow (PnG) process, where each task corresponds to each of the requirements. Different from traditional multi-task networks that use fully shared or task-specific layers, we enable in-layer partial parameter sharing to obtain both common and task-specific features at various abstraction levels. This encourages adequate sharing to improve the overall multi-task performance. The parameters are shared at a micro-structured level to both maintan the task performance and reduce inference computation. The sharing structure is automatically learned, where a model instance trained for previous tasks is progressively pruned and regrown to perform more tasks. The framework is task-generic and model-structure-agnostic. Using NIC and SISR as two example applications, extensive experiments show that the multi-task PnG network can largely reduce the overall model size and inference computation, with almost no degradation of the reconstruction performance. ",
    "code_link": "https://github.com/InterDigitalInc/CompressAI"
  },
  "cvpr2021_ntire_hinethalfinstancenormalizationnetworkforimagerestoration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "HINet: Half Instance Normalization Network for Image Restoration",
    "authors": [
      "Liangyu Chen",
      "Xin Lu",
      "Jie Zhang",
      "Xiaojie Chu",
      "Chengpeng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we explore the role of Instance Normalization in low-level vision tasks. Specifically, we present a novel block: Half Instance Normalization Block (HIN Block), to boost the performance of image restoration networks. Based on HIN Block, we design a simple and powerful multi-stage network named HINet, which consists of two subnetworks. With the help of HIN Block, HINet surpasses the state-of-the-art (SOTA) on various image restoration tasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD dataset, with only 7.5% and 30% of its multiplier-accumulator operations (MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we get comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS and GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the average result of multiple datasets with 1.4 times speedup. With HINet, we won 1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG Artifacts, with a PSNR of 29.70. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021multi-modalaerialviewobjectclassificationchallenge": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Multi-Modal Aerial View Object Classification Challenge",
    "authors": [
      "Jerrick Liu",
      "Nathan Inkawhich",
      "Oliver Nina",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Liu_NTIRE_2021_Multi-Modal_Aerial_View_Object_Classification_Challenge_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Liu_NTIRE_2021_Multi-Modal_Aerial_View_Object_Classification_Challenge_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we introduce the first Challenge on Multimodal Aerial View Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at CVPR. This challenge is composed of two different tracks using EO and SAR imagery. Both EO and SAR sensors possess different advantages and drawbacks. The purpose of this competition is to analyze how to use both sets of sensory information in complementary ways. We discuss the top methods submitted for this competition and evaluate their results on our blind test set. Our challenge results show significant improvement of more than 15% accuracy from our current baselines for each track of the competition. ",
    "code_link": ""
  },
  "cvpr2021_ntire_threegapsforquantisationinlearnedimagecompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Three Gaps for Quantisation in Learned Image Compression",
    "authors": [
      "Shi Pan",
      "Chris Finlay",
      "Chri Besenbruch",
      "William Knottenbelt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Pan_Three_Gaps_for_Quantisation_in_Learned_Image_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Pan_Three_Gaps_for_Quantisation_in_Learned_Image_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Learned lossy image compression has demonstrated impressive progress via end-to-end neural network training. However, this end-to-end training belies the fact that lossy compression is inherently not differentiable, due to the necessity of quantisation. To overcome this difficulty in training, researchers have used various approximations to the quantisation step. However, little work has studied the mechanism of quantisation approximation itself. We address this issue, identifying three gaps arising in the quantisation approximation problem. These gaps are visualised, and show the effect of applying different quantisation approximation methods. Following this analysis, we propose a Soft-STE quantisation approximation method, which closes these gaps and demonstrates better performance than other quantisation approaches on the Kodak dataset. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengeonhighdynamicrangeimagingdataset,methodsandresults": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on High Dynamic Range Imaging: Dataset, Methods and Results",
    "authors": [
      "Eduardo Perez-Pellitero",
      "Sibi Catley-Chandar",
      "Ales Leonardis",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Perez-Pellitero_NTIRE_2021_Challenge_on_High_Dynamic_Range_Imaging_Dataset_Methods_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Perez-Pellitero_NTIRE_2021_Challenge_on_High_Dynamic_Range_Imaging_Dataset_Methods_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper reviews the first challenge on high-dynamic range (HDR) imaging that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2021. This manuscript focuses on the newly introduced dataset, the proposed methods and their results. The challenge aims at estimating a HDR image from one or multiple respective low-dynamic range (LDR) observations, which might suffer from under- or over-exposed regions and different sources of noise. The challenge is composed by two tracks: In Track 1 only a single LDR image is provided as input, whereas in Track 2 three differently-exposed LDR images with inter-frame motion are available. In both tracks, the ultimate goal is to achieve the best objective HDR reconstruction in terms of PSNR with respect to a ground-truth image, evaluated both directly and with a canonical tonemapping operation. ",
    "code_link": ""
  },
  "cvpr2021_ntire_vspsrexplorablesuper-resolutionviavariationalsparserepresentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "VSpSR: Explorable Super-Resolution via Variational Sparse Representation",
    "authors": [
      "Hangqi Zhou",
      "Chao Huang",
      "Shangqi Gao",
      "Xiahai Zhuang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Zhou_VSpSR_Explorable_Super-Resolution_via_Variational_Sparse_Representation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Zhou_VSpSR_Explorable_Super-Resolution_via_Variational_Sparse_Representation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Super-resolution (SR) is an ill-posed problem, which means that infinitely many high-resolution (HR) images can be degraded to the same low-resolution (LR) image. To study the one-to-many stochastic SR mapping, we implicitly represent the non-local self-similarity of natural images and develop a Variational Sparse framework for Super-Resolution (VSpSR) via neural networks. Since every small patch of a HR image can be well approximated by the sparse representation of atoms in an over-complete dictionary, we design a two-branch module, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM extracts patch-level basis from the LR input, and the other branch infers pixel-wise variational distributions with respect to the sparse coefficients. By repeatedly sampling coefficients, we could obtain infinite sparse representations, and thus generate diverse HR images. According to the preliminary results of NTIRE 2021 challenge on learning SR space, our team (FudanZmic21) ranks 7-th in terms of released scores. The implementation of VSpSR is released at https://zmiclab.github.io/. ",
    "code_link": "https://github.com/andreas128/NTIRE21"
  },
  "cvpr2021_ntire_symmetricparallaxattentionforstereoimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Symmetric Parallax Attention for Stereo Image Super-Resolution",
    "authors": [
      "Yingqian Wang",
      "Xinyi Ying",
      "Longguang Wang",
      "Jungang Yang",
      "Wei An",
      "Yulan Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Wang_Symmetric_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Wang_Symmetric_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Although recent years have witnessed the great advances in stereo image super-resolution (SR), the beneficial information provided by binocular systems has not been fully used. Since stereo images are highly symmetric under epipolar constraint, in this paper, we improve the performance of stereo image SR by exploiting symmetry cues in stereo image pairs. Specifically, we propose a symmetric bi-directional parallax attention module (biPAM) and an inline occlusion handling scheme to effectively interact cross-view information. Then, we design a Siamese network equipped with a biPAM to super-resolve both sides of views in a highly symmetric manner. Finally, we design several illuminance-robust losses to enhance stereo consistency. Experiments on four public datasets demonstrate the superior performance of our method. Source code is available at https://github.com/YingqianWang/iPASSR. ",
    "code_link": "https://github.com/YingqianWang/iPASSR"
  },
  "cvpr2021_ntire_multi-modalbifurcatednetworkfordepthguidedimagerelighting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Modal Bifurcated Network for Depth Guided Image Relighting",
    "authors": [
      "Hao-Hsiang Yang",
      "Wei-Ting Chen",
      "Hao-Lun Luo",
      "Sy-Yen Kuo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Yang_Multi-Modal_Bifurcated_Network_for_Depth_Guided_Image_Relighting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Yang_Multi-Modal_Bifurcated_Network_for_Depth_Guided_Image_Relighting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image relighting aims to recalibrate the illumination setting in an image. In this paper, we propose a deep learning-based method called multi-modal bifurcated network (MBNet) for depth guided image relighting. That is, given an image and the corresponding depth maps, a new image with the given illuminant angle and color temperature is generated by our network. This model extracts the image and the depth features by the bifurcated network in the encoder. To use the two features effectively, we adopt the dynamic dilated pyramid modules in the decoder. Moreover, to increase the variety of training data, we propose a novel data process pipeline to increase the number of the training data. Experiments conducted on the VIDIT dataset show that the proposed solution obtains the 1st place in terms of SSIM and PMS in the NTIRE 2021 Depth Guide One-to-one Relighting Challenge. ",
    "code_link": "https://github.com/weitingchen83/NTIRE2021-Depth-Guided-Image-Relighting-MBNet"
  },
  "cvpr2021_ntire_efficientcnnarchitectureformulti-modalaerialviewobjectclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Efficient CNN Architecture for Multi-Modal Aerial View Object Classification",
    "authors": [
      "Casian Miron",
      "Alexandru Pasarica",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Miron_Efficient_CNN_Architecture_for_Multi-Modal_Aerial_View_Object_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Miron_Efficient_CNN_Architecture_for_Multi-Modal_Aerial_View_Object_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The NTIRE 2021 workshop features a Multi-modal Aerial View Object Classification Challenge. Its focus is on multi-sensor imagery classification in order to improve the performance of automatic target recognition (ATR) systems. In this paper we describe our entry in this challenge, a method focused on efficiency and low computational time, while maintaining a high level of accuracy. The method is a convolutional neural network with 11 convolutions, 1 max pooling layers and 3 residual blocks which has a total of 373.130 parameters. The method ranks 3rd in the Track 2 (SAR+EO) of the challenge. ",
    "code_link": ""
  },
  "cvpr2021_ntire_attention!stayfocus!": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Attention! Stay Focus!",
    "authors": [
      "Tu Vo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Vo_Attention_Stay_Focus_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Vo_Attention_Stay_Focus_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We develop a deep convolutional neural networks (CNNs) to deal with the blurry artifacts caused by the defocus of the camera using dual-pixel images. Specifically, we develop a double attention network which consists of attentional encoders, triple locals and global local modules to effectively extract useful information from each image in the dual-pixels and select the useful information from each image and synthesize the final output image. We demonstrate the effectiveness of the proposed deblurring algorithm in terms of both qualitative and quantitative aspects by evaluating on the test set in the NTIRE 2021 Defocus Deblurring using Dual-pixel Images Challenge. The code, and trained models are available at https://github.com/tuvovan/ATTSF. ",
    "code_link": "https://github.com/tuvovan/ATTSF"
  },
  "cvpr2021_ntire_multi-scaleselectiveresiduallearningfornon-homogeneousdehazing": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Scale Selective Residual Learning for Non-Homogeneous Dehazing",
    "authors": [
      "Eunsung Jo",
      "Jae-Young Sim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Jo_Multi-Scale_Selective_Residual_Learning_for_Non-Homogeneous_Dehazing_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Jo_Multi-Scale_Selective_Residual_Learning_for_Non-Homogeneous_Dehazing_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As the particles in hazy medium cause the absorption and scattering of light, the images captured under such environment suffer from quality degradation such as low contrast and color distortion. While numerous single image dehazing methods have been proposed to reconstruct clean images from hazy images, non-homogeneous dehazing has been rarely studied. In this paper, we design an end-to-end network to remove non-homogeneous dense haze. We employ the selective residual blocks to adaptively improve the visibility of resulting images, where the input feature and the residual feature are combined with fully trainable weights. Experimental results including the ablation study demonstrate that the proposed method is a promising tool for non-homogeneous dehazing that enhances the contrast of hazy images effectively while restoring colorful appearance faithfully. ",
    "code_link": ""
  },
  "cvpr2021_ntire_ntire2021challengeonperceptualimagequalityassessment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2021 Challenge on Perceptual Image Quality Assessment",
    "authors": [
      "Jinjin Gu",
      "Haoming Cai",
      "Chao Dong",
      "Jimmy S. Ren",
      "Yu Qiao",
      "Shuhang Gu",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Gu_NTIRE_2021_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Gu_NTIRE_2021_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper reports on the NTIRE 2021 challenge on perceptual image quality assessment (IQA), held in conjunction with the New Trends in Image Restoration and Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image processing technology, perceptual image processing algorithms based on Generative Adversarial Networks (GAN) have produced images with more realistic textures. These output images have completely different characteristics from traditional distortions, thus pose a new challenge for IQA methods to evaluate their visual quality. In comparison with previous IQA challenges, the training and testing datasets in this challenge include the outputs of perceptual image processing algorithms and the corresponding subjective scores. Thus they can be used to develop and evaluate IQA methods on GAN-based distortions. The challenge has 270 registered participants in total. In the final testing stage, 13 participating teams submitted their models and fact sheets. Almost all of them have achieved much better results than existing IQA methods, while the winning method can demonstrate state-of-the-art performance. ",
    "code_link": ""
  },
  "cvpr2021_ntire_instagramfilterremovalonfashionableimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Instagram Filter Removal on Fashionable Images",
    "authors": [
      "Furkan Kinli",
      "Baris Ozcan",
      "Furkan Kirac"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Kinli_Instagram_Filter_Removal_on_Fashionable_Images_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Kinli_Instagram_Filter_Removal_on_Fashionable_Images_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Social media images are generally transformed by filtering to obtain aesthetically more pleasing appearances. However, CNNs generally fail to interpret both the image and its filtered version as the same in the visual analysis of social media images. We introduce Instagram Filter Removal Network (IFRNet) to mitigate the effects of image filters for social media analysis applications. To achieve this, we assume any filter applied to an image substantially injects a piece of additional style information to it, and we consider this problem as a reverse style transfer problem. The visual effects of filtering can be directly removed by adaptively normalizing external style information in each level of the encoder. Experiments demonstrate that IFRNet outperforms all compared methods in quantitative and qualitative comparisons, and has the ability to remove the visual effects to a great extent. Additionally, we present the filter classification performance of our proposed model, and analyze the dominant color estimation on the images unfiltered by all compared methods. ",
    "code_link": ""
  },
  "cvpr2021_ntire_boostingtheperformanceofvideocompressionartifactreductionwithreferenceframeproposalsandfrequencydomaininformation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Boosting the Performance of Video Compression Artifact Reduction With Reference Frame Proposals and Frequency Domain Information",
    "authors": [
      "Yi Xu",
      "Minyi Zhao",
      "Jing Liu",
      "Xinjian Zhang",
      "Longwen Gao",
      "Shuigeng Zhou",
      "Huyang Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Xu_Boosting_the_Performance_of_Video_Compression_Artifact_Reduction_With_Reference_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Xu_Boosting_the_Performance_of_Video_Compression_Artifact_Reduction_With_Reference_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Many deep learning based video compression artifact removal algorithms have been proposed to recover high-quality videos from low-quality compressed videos. Recently, methods were proposed to mine spatiotemporal information via utilizing multiple neighboring frames as reference frames. However, these post-processing methods take advantage of adjacent frames directly but neglect the information of the video itself, which can be exploited. In this paper, we propose an effective reference frame proposal strategy to boost the performance of the existing multi-frame approaches. Besides, we introduce a loss based on fast Fourier transformation (FFT) to further improve the effectiveness of restoration. Experimental results show that our method achieves better fidelity and perceptual performance on MFQE 2.0 dataset than the state-of-the-art methods. And our method won Track 1 and Track 2, and was ranked the 2nd in Track 3 of NTIRE 2021 Quality enhancement of heavily compressed videos Challenge. ",
    "code_link": ""
  },
  "cvpr2021_ntire_single-imagehdrreconstructionwithtask-specificnetworkbasedonchanneladaptiverdn": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Single-Image HDR Reconstruction With Task-Specific Network Based on Channel Adaptive RDN",
    "authors": [
      "Guannan Chen",
      "Lijie Zhang",
      "Mengdi Sun",
      "Yan Gao",
      "Pablo Navarrete Michelini",
      "YanHong Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_Single-Image_HDR_Reconstruction_With_Task-Specific_Network_Based_on_Channel_Adaptive_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Chen_Single-Image_HDR_Reconstruction_With_Task-Specific_Network_Based_on_Channel_Adaptive_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We describe our solution for the NTIRE--2021 High Dynamic Range Challenge with Single Frame Track where we achieved the 3rd place in terms of muPSNR and the 1st place in terms of PSNR. Aiming at this challenge we introduce the Task-specific Network based on Channel Adaptive RDN(TCRDN) that achieves good performance on the similarity with the ground truth. The network is divided into three subnets: Image Reconstruction(IR), Detail Restoration(DR) and Local Contrast Enhancement(LCE). Each subnet processes its own task, and results are fused to produce the HDR output. We carefully design these subnets so that they are properly trained for their intended purpose: detail restoration in the IR subnet and contrast enhancement in the LCE subnet. The Channel Adaptive RDN is a novel network working as the subnet backbone that combines the classic Residual Dense Network(RDN) and the Gate Channel Transformation layer. The L1 loss is used for training the network and the final model can balance the trade--off between PSNR and muPSNR for high performance in the competition's task. ",
    "code_link": ""
  },
  "cvpr2021_ntire_srflow-dasuper-resolutionusingnormalizingflowwithdeepconvolutionalblock": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "SRFlow-DA: Super-Resolution Using Normalizing Flow With Deep Convolutional Block",
    "authors": [
      "Younghyun Jo",
      "Sejong Yang",
      "Seon Joo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Jo_SRFlow-DA_Super-Resolution_Using_Normalizing_Flow_With_Deep_Convolutional_Block_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Jo_SRFlow-DA_Super-Resolution_Using_Normalizing_Flow_With_Deep_Convolutional_Block_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multiple high-resolution (HR) images can be generated from a single low-resolution (LR) image, as super-resolution (SR) is an underdetermined problem. Recently, the conditional normalizing flow-based model, SRFlow, shows remarkable performance by learning an exact mapping from HR image manifold to a latent space. The flow-based SR model allows sampling multiple output images from a learned SR space with a given LR image. In this work, we propose SRFlow-DA which has a more suitable architecture for the SR task based on the original SRFlow model. Specifically, our approach enlarges the receptive field by stacking more convolutional layers in the affine couplings, and so our model can get more expressive power. At the same time, we reduce the total number of model parameters for efficiency. Compared to SRFlow, our SRFlow-DA achieves better or comparable PSNR and LPIPS for x4 and x8 SR tasks, while having a reduced number of parameters. In addition, our method generates visually clear results without excessive sharpness artifacts. ",
    "code_link": "https://github.com/yhjo09/SRFlow-DA"
  },
  "cvpr2021_ntire_perceptualimagequalityassessmentwithtransformers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Perceptual Image Quality Assessment With Transformers",
    "authors": [
      "Manri Cheon",
      "Sung-Jun Yoon",
      "Byungyeon Kang",
      "Junwoo Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Cheon_Perceptual_Image_Quality_Assessment_With_Transformers_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Cheon_Perceptual_Image_Quality_Assessment_With_Transformers_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose an image quality transformer (IQT) that successfully applies a transformer architecture to a perceptual full-reference image quality assessment (IQA) task. Perceptual representation becomes more important in image quality assessment. In this context, we extract the perceptual feature representations from each of input images using a convolutional neural network (CNN) backbone. The extracted feature maps are fed into the transformer encoder and decoder in order to compare a reference and distorted images. Following an approach of the transformer-based vision models, we use extra learnable quality embedding and position embedding. The output of the transformer is passed to a prediction head in order to predict a final quality score. The experimental results show that our proposed model has an outstanding performance for the standard IQA datasets. For a large-scale IQA dataset containing output images of generative model, our model also shows the promising results. The proposed IQT was ranked first among 13 participants in the NTIRE 2021 perceptual image quality assessment challenge. Our work will be an opportunity to further expand the approach for the perceptual IQA task. ",
    "code_link": ""
  },
  "cvpr2021_ntire_widereceptivefieldandchannelattentionnetworkforjpegcompressedimagedeblurring": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "NTIRE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Wide Receptive Field and Channel Attention Network for JPEG Compressed Image Deblurring",
    "authors": [
      "Donghyeon Lee",
      "Chulhee Lee",
      "Taesung Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Lee_Wide_Receptive_Field_and_Channel_Attention_Network_for_JPEG_Compressed_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Lee_Wide_Receptive_Field_and_Channel_Attention_Network_for_JPEG_Compressed_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A motion blurred image stored in the joint photographic experts group (JPEG) image compression format contains both motion blur and JPEG artifacts. Therefore, it is very difficult to restore the original image from a blurred and JPEG-compressed image. To address this problem, this paper proposes two methods: a wide receptive field and channel attention network (WRCAN), and JPEG auto-encoder loss. First, the WRCAN utilizes a large receptive field and considers the interdependencies among channels of a feature map. Second, the proposed JPEG auto-encoder loss enables the WRCAN to learn prior knowledge of JPEG compression artifacts such that the proposed WRCAN can effectively restore the original image from JPEG-compressed images. The proposed methods are evaluated on the JPEG-compressed REDS dataset by participating in the NTIRE 2021 workshop challenges on Image Deblurring Track 2 JPEG artifacts. The WRCAN trained with the proposed loss ranked third with an output of 29.60dB on the REDS test set, indicating that the proposed methods provide state-of-the-art results. The source codes, model, and data are available at https://github.com/dhyeonlee/WRCAN-PyTorch. ",
    "code_link": "https://github.com/dhyeonlee/WRCAN-PyTorch"
  },
  "cvpr2021_ug2_dissectingthehigh-frequencybiasinconvolutionalneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Dissecting the High-Frequency Bias in Convolutional Neural Networks",
    "authors": [
      "Antonio A. Abello",
      "Roberto Hirata",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Abello_Dissecting_the_High-Frequency_Bias_in_Convolutional_Neural_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Abello_Dissecting_the_High-Frequency_Bias_in_Convolutional_Neural_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " For convolutional neural networks (CNNs), a common hypothesis that explains both their capability of generalization and their characteristic brittleness is that these models are implicitly regularized to rely on imperceptible high-frequency patterns, more than humans would do. This hypothesis has seen some empirical validation, but most works do not rigorously divide the image frequency spectrum. We present a model to divide the spectrum in disjointed discs based on the distribution of energy and apply simple feature importance procedures to test whether high-frequencies are more important than lower ones. We find evidence that mid or high-level frequencies are disproportionately important for CNNs. The evidence is robust across different datasets and networks. Moreover, we find the diverse effects of the network's attributes, such as architecture and depth, on frequency bias and robustness in general. Code for reproducing our experiments is available at: https://github.com/Abello966/FrequencyBiasExperiments ",
    "code_link": ""
  },
  "cvpr2021_ug2_vrhivisibilityrestorationforhazyimagesusingahazedensitymodel": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "VRHI: Visibility Restoration for Hazy Images Using a Haze Density Model",
    "authors": [
      "Mingye Ju",
      "Chuheng Chen",
      "Juping Liu",
      "Kai Chen",
      "Dengyin Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Ju_VRHI_Visibility_Restoration_for_Hazy_Images_Using_a_Haze_Density_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Ju_VRHI_Visibility_Restoration_for_Hazy_Images_Using_a_Haze_Density_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, a image processing method called VRHI is developed to enhance single hazy images. More specifically, inspired by visual characteristics of haze, a haze density estimation model is designed to predict the haze distribution. According to this recognized haze distribution, a quadtree based recursive strategy is subsequently proposed to locate the atmospheric light. Finally, by combining a global-wise adjusting mechanism and atmospheric scattering model, the haze cover in an image can be easily excluded using the estimated parameters. It is worth mentioning that VRHI is based on whole image to search the unknown parameters, thereby avoiding some unfavorable phenomena, e.g., over-enhancement and color distortion. Extensive experiments on real-world images and well-known dehazing datasets show that VRHI outperforms state-of-the-art techniques in robustness and effectiveness. ",
    "code_link": ""
  },
  "cvpr2021_ug2_deltasamplingr-bertforlimiteddataandlow-lightactionrecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Delta Sampling R-BERT for Limited Data and Low-Light Action Recognition",
    "authors": [
      "Sanchit Hira",
      "Ritwik Das",
      "Abhinav Modi",
      "Daniil Pakhomov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Hira_Delta_Sampling_R-BERT_for_Limited_Data_and_Low-Light_Action_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Hira_Delta_Sampling_R-BERT_for_Limited_Data_and_Low-Light_Action_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present an approach to perform supervised action recognition in the dark. In this work, we present our results on the ARID dataset. Most previous works only evaluate performance on large, well illuminated datasets like Kinetics and HMDB51. We demonstrate that our work is able to achieve a very low error rate while being trained on a much smaller dataset of dark videos. We also explore a variety of training and inference strategies including domain transfer methodologies and also propose a simple but useful frame selection strategy. Our empirical results demonstrate that we beat previously published baseline models by 11%. ",
    "code_link": ""
  },
  "cvpr2021_ug2_two-stagenetworkforsingleimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Two-Stage Network for Single Image Super-Resolution",
    "authors": [
      "Yuzhuo Han",
      "Xiaobiao Du",
      "Zhi Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Han_Two-Stage_Network_for_Single_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Han_Two-Stage_Network_for_Single_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The task of single-image super-resolution (SISR) is a highly inverse problem because it is very challenging to reconstruct rich details from blurred images. Most previous super-resolution (SR) methods based on the convolutional neural networks (CNN) tend to design more complex network structures to directly learn the mapping between low-resolution images and high-resolution images. However, this is not the best choice to blindly increase the network depth, because the performance improvement may not increase, but it will increase the computational cost. To solve this problem, we propose an effective method that learns high-frequency information in high-resolution images to enhance the image reconstruction. In this work, we propose a two-stage network (TSN) to recover clear SR images. The proposed TSN firstly learns the high-frequency information in high-resolution images, then learns how to transform to high-resolution images. A large number of experiments show that our TSN achieves satisfactory performance. ",
    "code_link": ""
  },
  "cvpr2021_ug2_expectation-maximizationattentioncrossresidualnetworkforsingleimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Expectation-Maximization Attention Cross Residual Network for Single Image Super-Resolution",
    "authors": [
      "Xiaobiao Du",
      "Jie Niu",
      "Chongjin Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Du_Expectation-Maximization_Attention_Cross_Residual_Network_for_Single_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Du_Expectation-Maximization_Attention_Cross_Residual_Network_for_Single_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The depth of deep convolution neural network and self-attention mechanism is widely used for the single image super-resolution (SISR) task. Nevertheless, we observed that the deeper network was more hard to train and the self-attention mechanism is computationally consuming. Residual learning has been widely recognized as a common approach to improve network performance for deep learning, but most existing methods did not make the best of the learning ability of deep CNN, thus hindering the ability of representative CNN. In order to tackle these problems, we introduce a deep learning network namely expectation-maximization attention cross residual network (EACRN) to tackle the super-resolution task. Particularly, we propose a cross residual in cross residual (CRICR) structure that makes up very deep networks consisting of multiple cross residual groups (CRG) with global residual skip connections. Every cross residual group (CRG) consists of some cross residual blocks with cross short skip connections. At the same time, CRICR allows network focused on capturing high-frequency patterns by connecting rich low-frequency patterns to be bypassed and several short skip connections. In addition, we introduce various convolution kernel size so that adaptive capture the image pattern in different scales, which make these features get the more efficacious image information through interacting with each other. The introduced Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Extensive experiments demonstrate our EACRN obtains superior performance and visual effect relative to the most advanced algorithm. ",
    "code_link": ""
  },
  "cvpr2021_ug2_darklightnetworksforactionrecognitioninthedark": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "DarkLight Networks for Action Recognition in the Dark",
    "authors": [
      "Rui Chen",
      "Jiajun Chen",
      "Zixi Liang",
      "Huaien Gao",
      "Shan Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Chen_DarkLight_Networks_for_Action_Recognition_in_the_Dark_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Chen_DarkLight_Networks_for_Action_Recognition_in_the_Dark_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Human action recognition in the dark is a significanttask with various applications, e.g., night surveillance andself-driving at night. However, the lack of video datasetsfor human actions in the dark hinders its development. Recently, a public dataset ARID has been introduced tostimulate progress for the task of human action recogni-tion in dark videos. Currently, there are multiple mod-els that perform well for action recognition in videos shotunder normal illumination. However, research shows thatthese methods may not be effective in recognizing actionsin dark videos. In this paper, we construct a novel neuralnetwork architecture: DarkLight Networks, which involves (i) a dual-pathway structure where both dark videos andits brightened counterpart are utilized for effective videorepresentation; and (ii) a self-attention mechanism, whichfuses and extracts corresponding and complementary fea-tures from the two pathways. Our approach achieves state-of-the-art results on ARID. Code is available at:https://github.com/Ticuby/Darklight-Pytorch ",
    "code_link": "https://github.com/Ticuby/Darklight-Pytorch"
  },
  "cvpr2021_ug2_multi-scalehourglasshierarchicalfusionnetworkforsingleimagederaining": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Multi-Scale Hourglass Hierarchical Fusion Network for Single Image Deraining",
    "authors": [
      "Xiang Chen",
      "Yufeng Huang",
      "Lei Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Chen_Multi-Scale_Hourglass_Hierarchical_Fusion_Network_for_Single_Image_Deraining_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Chen_Multi-Scale_Hourglass_Hierarchical_Fusion_Network_for_Single_Image_Deraining_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Rain streaks bring serious blurring and visual quality degradation, which often vary in size, direction and density. Current CNN-based methods achieve encouraging performance, while are limited to depict rain characteristics and recover image details in the poor visibility environment. To address these issues, we present a Multi-scale Hourglass Hierarchical Fusion Network (MH2F-Net) in end-to-end manner, to exactly captures rain streak features with multi-scale extraction, hierarchical distillation and information aggregation. For better extracting the features, a novel Multi-scale Hourglass Extraction Block (MHEB) is proposed to get local and global features across different scales through down- and up-sample process. Besides, a Hierarchical Attentive Distillation Block (HADB) then employs the dual attention feature responses to adaptively recalibrate the hierarchical features and eliminate the redundant ones. Further, we introduce a Residual Projected Feature Fusion (RPFF) strategy to progressively discriminate feature learning and aggregate different features instead of directly concatenating or adding. Extensive experiments on both synthetic and real rainy datasets demonstrate the effectiveness of the designed MH2F-Net by comparing with recent state-of-the-art deraining algorithms. Our source code will be available on the GitHub: https://github.com/cxtalk/MH2F-Net. ",
    "code_link": "https://github.com/cxtalk/MH2F-Net"
  },
  "cvpr2021_ug2_ce-peoplesegreal-timepeoplesegmentationwith10%cpuusageforvideoconference": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "CE-PeopleSeg: Real-Time People Segmentation With 10% CPU Usage for Video Conference",
    "authors": [
      "Ziyu Jiang",
      "Zhenhua He",
      "Xueqin Huang",
      "Zibin Yang",
      "Pearl Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Jiang_CE-PeopleSeg_Real-Time_People_Segmentation_With_10_CPU_Usage_for_Video_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Jiang_CE-PeopleSeg_Real-Time_People_Segmentation_With_10_CPU_Usage_for_Video_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Nowadays, video conference solutions are widely adopted for companies, education, and government. People segmentation is crucial for supporting virtual background, an essential video conference function to protect users' privacy. This paper demonstrated a people segmentation framework called CE-PeopleSeg, which employed an efficient segmentation method, structural pruning, and dynamic frame skipping techniques, leading to a fast inference speed on CPU. Our extensive experiments show that the proposed CE-PeopleSeg can achieve a high prediction mIoU of 87.9% on Supervised People Dataset while reaching a real-time inference speed of 32.40 fps on CPU with very low usage of 10%. ",
    "code_link": "https://github.com/geekJZY/EfficientPeopleSeg.git"
  },
  "cvpr2021_ug2_e2vtsenergy-efficientvideotextspottingfromunmannedaerialvehicles": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "UG2",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "E2VTS: Energy-Efficient Video Text Spotting From Unmanned Aerial Vehicles",
    "authors": [
      "Zhenyu Hu",
      "Pengcheng Pi",
      "Zhenyu Wu",
      "Yunhe Xue",
      "Jiayi Shen",
      "Jianchao Tan",
      "Xiangru Lian",
      "Zhangyang Wang",
      "Ji Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/html/Hu_E2VTS_Energy-Efficient_Video_Text_Spotting_From_Unmanned_Aerial_Vehicles_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Hu_E2VTS_Energy-Efficient_Video_Text_Spotting_From_Unmanned_Aerial_Vehicles_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Unmanned Aerial Vehicles (UAVs) based video text spotting has been extensively used in civil and military domains. UAV's limited battery capacity motivates us to develop an energy-efficient video text spotting solution. In this paper, we first revisit RCNN's crop & resize training strategy and empirically find that it outperforms aligned RoI sampling on a real-world video text dataset captured by UAV. To reduce energy consumption, we further propose a multi-stage image processor that takes videos' redundancy, continuity, and mixed degradation into account. The model is pruned and quantized before deployed on Raspberry Pi. Our proposed energy-efficient video text spotting solution, dubbed as E^2VTS, outperforms all previous methods by achieving a competitive tradeoff between energy efficiency and performance. All our codes and pre-trained models are available at https://github.com/wuzhenyusjtu/LPCVC20-VideoTextSpotting. ",
    "code_link": "https://github.com/kolinger/rd-usb.git"
  },
  "cvpr2021_wmf_spocspoofingcamerafingerprints": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "SpoC: Spoofing Camera Fingerprints",
    "authors": [
      "Davide Cozzolino",
      "Justus Thies",
      "Andreas Rossler",
      "Matthias Niessner",
      "Luisa Verdoliva"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Cozzolino_SpoC_Spoofing_Camera_Fingerprints_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Cozzolino_SpoC_Spoofing_Camera_Fingerprints_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap \"rich\" fake news with enhanced credibility, spawning a new wave of high-impact, high-risk misinformation campaigns. Therefore, there is a fast-growing interest in reliable detectors of manipulated media. The most powerful detectors, to date, rely on the subtle traces left by any device on all images acquired by it. In particular, due to proprietary in-camera processes, like demosaicing or compression, each camera model leaves trademark traces that can be exploited for forensic analyses. The absence or distortion of such traces in the target image is a strong hint of manipulation. In this paper, we challenge such detectors to gain better insight into their vulnerabilities. This is an important study in order to build better forgery detectors able to face malicious attacks. Our proposal consists of a GAN-based approach that injects camera traces into synthetic images. Given a GAN-generated image, we insert the traces of a specific camera model into it and deceive state-of-the-art detectors into believing the image was acquired by that model. Likewise, we deceive independent detectors of synthetic GAN images into believing the image is real. Experiments prove the effectiveness of the proposed method in a wide array of conditions. Moreover, no prior information on the attacked detectors is needed, but only sample images from the target camera. ",
    "code_link": "https://github.com/BorealisAI/advertorch"
  },
  "cvpr2021_wmf_ontherobustnessandgeneralizabilityoffacesynthesisdetectionmethods": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "On the Robustness and Generalizability of Face Synthesis Detection Methods",
    "authors": [
      "Johan Sabel",
      "Fredrik Johansson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Sabel_On_the_Robustness_and_Generalizability_of_Face_Synthesis_Detection_Methods_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Sabel_On_the_Robustness_and_Generalizability_of_Face_Synthesis_Detection_Methods_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In recent years, significant progress has been made within human face synthesis. It is now possible, and easy for anyone, to generate credible high-resolution images of non-existing people. This calls for effective detection methods. In this paper, three state-of-the-art deep learning-based methods are evaluated with respect to their robustness and generalizability, which are two factors that must be taken into consideration for methods intended to be deployed in the wild. The robustness experiments show that it is possible to achieve near-perfect performance when discriminating between real and synthetic facial images that have been post-processed heavily with various perturbation techniques; especially when similar perturbations are incorporated during training of the detection models. The generalization experiments show that already trained detection models can achieve high performance on images from sources not known during training, provided that the models are fine-tuned on such images. One model achieved an average accuracy of 96.8% after being fine-tuned on 3 training images from each unknown source considered (one real and one synthetic source). However, additional images were required when fine-tuning using a different approach aimed at preventing catastrophic forgetting. Furthermore, in general, no method generalized well without fine-tuning. Hence, the limited generalization capability remains a shortcoming that must be overcome before the detection methods can be utilized in the wild. ",
    "code_link": ""
  },
  "cvpr2021_wmf_fretalgeneralizingdeepfakedetectionusingknowledgedistillationandrepresentationlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "FReTAL: Generalizing Deepfake Detection Using Knowledge Distillation and Representation Learning",
    "authors": [
      "Minha Kim",
      "Shahroz Tariq",
      "Simon S. Woo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Kim_FReTAL_Generalizing_Deepfake_Detection_Using_Knowledge_Distillation_and_Representation_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Kim_FReTAL_Generalizing_Deepfake_Detection_Using_Knowledge_Distillation_and_Representation_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As GAN-based video and image manipulation technologies become more sophisticated and easily accessible, there is an urgent need for effective deepfake detection technologies. Moreover, various deepfake generation techniques have emerged over the past few years. While many deepfake detection methods have been proposed, their performance suffers from new types of deepfake methods on which they are not sufficiently trained. To detect new types of deepfakes, the model should learn from additional data without losing its prior knowledge about deepfakes (catastrophic forgetting), especially when new deepfakes are significantly different. In this work, we employ the Representation Learning (ReL) and Knowledge Distillation (KD) paradigms to introduce a transfer learning-based Feature Representation Transfer Adaptation Learning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on new deepfake datasets while minimizing the catastrophic forgetting. Our student model can quickly adapt to new types of deepfake by distilling knowledge from a pre-trained teacher model and applying transfer learning without using source domain data during domain adaptation. Through experiments on FaceForensics++ datasets, we demonstrate that FReTAL outperforms all baselines on the domain adaptation task with up to 86.97% accuracy on low-quality deepfakes. ",
    "code_link": ""
  },
  "cvpr2021_wmf_onthefeasibilityof3dmodel-basedforensicheightandweightestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "On the Feasibility of 3D Model-Based Forensic Height and Weight Estimation",
    "authors": [
      "Neerja Thakkar",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Thakkar_On_the_Feasibility_of_3D_Model-Based_Forensic_Height_and_Weight_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Thakkar_On_the_Feasibility_of_3D_Model-Based_Forensic_Height_and_Weight_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Forensic DNA analysis has been critical in prosecuting crimes and overturning wrongful convictions. At the same time, other physical and digital forensic identification techniques---used to link a suspect to a crime scene---are plagued with problems of accuracy, reliability, and reproducibility. Flawed forensic science can have devastating consequences -- the National Registry of Exonerations identified that flawed forensic techniques contribute to almost a quarter of wrongful convictions in the United States. Even some of the most basic, general-purpose forensic techniques for measuring a person's height and weight are unreliable. We propose using recent advances in 3D body-pose estimation to estimate height and weight from a single, unconstrained image. The reliability of this method is assessed using large-scale simulations and an in-the-wild dataset, bounding the expected accuracy with which height and weight can be estimated, and providing a road map for further improvements. ",
    "code_link": ""
  },
  "cvpr2021_wmf_manipulationdetectioninsatelliteimagesusingvisiontransformer": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Manipulation Detection in Satellite Images Using Vision Transformer",
    "authors": [
      "Janos Horvath",
      "Sriram Baireddy",
      "Hanxiang Hao",
      "Daniel Mas Montserrat",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Horvath_Manipulation_Detection_in_Satellite_Images_Using_Vision_Transformer_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Horvath_Manipulation_Detection_in_Satellite_Images_Using_Vision_Transformer_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A growing number of commercial satellite companies provide easily accessible satellite imagery. Overhead imagery is used by numerous industries including agriculture, forestry, natural disaster analysis, and meteorology. Satellite images, just as any other images, can be tampered with image manipulation tools. Manipulation detection methods created for images captured by \"consumer cameras\" tend to fail when used on satellite images due to the differences in image sensors, image acquisition, and processing. In this paper we propose an unsupervised technique that uses a Vision Transformer to detect spliced areas within satellite images. We introduce a new dataset which includes manipulated satellite images that contain spliced objects. We show that our proposed approach performs better than existing unsupervised splicing detection techniques. ",
    "code_link": ""
  },
  "cvpr2021_wmf_dodeepfakesfeelemotions?asemanticapproachtodetectingdeepfakesviaemotionalinconsistencies": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Do Deepfakes Feel Emotions? A Semantic Approach to Detecting Deepfakes via Emotional Inconsistencies",
    "authors": [
      "Brian Hosler",
      "Davide Salvi",
      "Anthony Murray",
      "Fabio Antonacci",
      "Paolo Bestagini",
      "Stefano Tubaro",
      "Matthew C. Stamm"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Hosler_Do_Deepfakes_Feel_Emotions_A_Semantic_Approach_to_Detecting_Deepfakes_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Hosler_Do_Deepfakes_Feel_Emotions_A_Semantic_Approach_to_Detecting_Deepfakes_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recent advances in deep learning and computer vision have spawned a new class of media forgeries known as deepfakes, which typically consist of artificially generated human faces or voices. The creation and distribution of deepfakes raise many legal and ethical concerns. As a result, the ability to distinguish between deepfakes and authentic media is vital. While deepfakes can create plausible video and audio, it may be challenging for them to to generate content that is consistent in terms of high-level semantic features, such as emotions. Unnatural displays of emotion, measured by features such as valence and arousal, can provide significant evidence that a video has been synthesized. In this paper, we propose a novel method for detecting deepfakes of a human speaker using the emotion predicted from the speaker's face and voice. The proposed technique leverages LSTM networks that predict emotion from audio and video LLDs. Predicted emotion in time is used to classify videos as authentic or deepfakes through an additional supervised classifier. ",
    "code_link": ""
  },
  "cvpr2021_wmf_expressiontransferusingflow-basedgenerativemodels": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Expression Transfer Using Flow-Based Generative Models",
    "authors": [
      "Andrea Valenzuela",
      "Carlos Segura",
      "Ferran Diego",
      "Vicenc Gomez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Valenzuela_Expression_Transfer_Using_Flow-Based_Generative_Models_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Valenzuela_Expression_Transfer_Using_Flow-Based_Generative_Models_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Among the different deepfake generation techniques, flow-based methods appear as natural candidates. Due to the property of invertibility, flow-based methods eliminate the necessity of person-specific training and are able to reconstruct any input image almost perfectly to human perception. We present a method for deepfake generation based on facial expression transfer using flow-based generative models. Our approach relies on simple latent vector operations akin to the ones used for attribute manipulation, but for transferring expressions between identity source-target pairs. We show the feasibility of this approach using a pre-trained Glow model and small sets of source and target images, not necessarily considered during prior training. We also provide an evaluation pipeline of the generated images in terms of similarities between identities and Action Units encoding the expression to be transferred. Our results show that an efficient expression transfer is feasible by using the proposed approach setting up a first precedent in deepfake content creation, and its evaluation, independently of the training identities. ",
    "code_link": ""
  },
  "cvpr2021_wmf_deepimagecomparatorlearningtovisualizeeditorialchange": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Deep Image Comparator: Learning To Visualize Editorial Change",
    "authors": [
      "Alexander Black",
      "Tu Bui",
      "Hailin Jin",
      "Vishy Swaminathan",
      "John Collomosse"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Black_Deep_Image_Comparator_Learning_To_Visualize_Editorial_Change_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Black_Deep_Image_Comparator_Learning_To_Visualize_Editorial_Change_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a novel architecture for comparing a pair of images to identify image regions that have been subjected to editorial manipulation. We first describe a robust near-duplicate search, for matching a potentially manipulated image circulating online to an image within a trusted database of originals. We then describe a novel architecture for comparing that image pair, to localize regions that have been manipulated to differ from the retrieved original. The localization ignores discrepancies due to benign image transformations that commonly occur during online redistribution. These include artifacts due to noise and recompression degradation, as well as out-of-place transformations due to image padding, warping, and changes in size and shape. Robustness towards out-of-place transformations is achieved via the end-to-end training of a differentiable warping module within the comparator architecture. We demonstrate effective retrieval and comparison of benign transformed and manipulated images, over a dataset of millions of photographs. ",
    "code_link": ""
  },
  "cvpr2021_wmf_adversarialthreatstodeepfakedetectionapracticalperspective": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Adversarial Threats to DeepFake Detection: A Practical Perspective",
    "authors": [
      "Paarth Neekhara",
      "Brian Dolhansky",
      "Joanna Bitton",
      "Cristian Canton Ferrer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Neekhara_Adversarial_Threats_to_DeepFake_Detection_A_Practical_Perspective_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Neekhara_Adversarial_Threats_to_DeepFake_Detection_A_Practical_Perspective_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Facially manipulated images and videos or DeepFakes can be used maliciously to fuel misinformation or defame individuals. Therefore, detecting DeepFakes is crucial to increase the credibility of social media platforms and other media sharing web sites. State-of-the-art DeepFake detection techniques rely on neural network based classification models which are known to be vulnerable to adversarial examples. In this work, we study the vulnerabilities of state-of-the-art DeepFake detection methods from a practical standpoint. We perform adversarial attacks on DeepFake detectors in a black box setting where the adversary does not have complete knowledge of the classification models. We study the extent to which adversarial perturbations transfer across different models and propose techniques to improve the transferability of adversarial examples. We also create more accessible attacks using Universal Adversarial Perturbations which pose a very feasible attack scenario since they can be easily shared amongst attackers. We perform our evaluations on the winning entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they can be easily bypassed in a practical attack scenario by designing transferable and accessible adversarial attacks. ",
    "code_link": ""
  },
  "cvpr2021_wmf_forensicanalysisofvideofilesusingmetadata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Forensic Analysis of Video Files Using Metadata",
    "authors": [
      "Ziyue Xiang",
      "Janos Horvath",
      "Sriram Baireddy",
      "Paolo Bestagini",
      "Stefano Tubaro",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Xiang_Forensic_Analysis_of_Video_Files_Using_Metadata_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Xiang_Forensic_Analysis_of_Video_Files_Using_Metadata_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The unprecedented ease and ability to manipulate video content has led to a rapid spread of manipulated media. The availability of video editing tools greatly increased in recent years, allowing one to easily generate photo-realistic alterations. Such manipulations can leave traces in the metadata embedded in video files. This metadata information can be used to determine video manipulations, brand of video recording device, the type of video editing tool, and other important evidence. In this paper, we focus on the metadata contained in the popular MP4 video wrapper/container. We describe our method for metadata extractor that uses the MP4's tree structure. Our approach for analyzing the video metadata produces a more compact representation. We will describe how we construct features from the metadata and then use dimensionality reduction and nearest neighbor classification for forensic analysis of a video file. Our approach allows one to visually inspect the distribution of metadata features and make decisions. The experimental results confirm that the performance of our approach surpasses other methods. ",
    "code_link": ""
  },
  "cvpr2021_wmf_findingfacialforgeryartifactswithparts-baseddetectors": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Finding Facial Forgery Artifacts With Parts-Based Detectors",
    "authors": [
      "Steven Schwarcz",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Schwarcz_Finding_Facial_Forgery_Artifacts_With_Parts-Based_Detectors_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Schwarcz_Finding_Facial_Forgery_Artifacts_With_Parts-Based_Detectors_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Manipulated videos, especially those where the identity of an individual has been modified using deep neural networks, are becoming an increasingly relevant threat in the modern day. In this paper, we seek to develop a generalizable, explainable solution to detecting these manipulated videos. To achieve this, we design a series of forgery detection systems that each focus on one individual part of the face. These parts-based detection systems, which can be combined and used together in a single architecture, meet all of our desired criteria - they generalize effectively between datasets and give us valuable insights into what the network is looking at when making its decision. We thus use these detectors to perform detailed empirical analysis on the FaceForensics++, Celeb-DF, and Facebook Deepfake Detection Challenge datasets, examining not just what the detectors find but also collecting and analyzing useful related statistics on the datasets themselves. ",
    "code_link": ""
  },
  "cvpr2021_wmf_detectingdeep-fakevideosfromauralandoraldynamics": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Detecting Deep-Fake Videos From Aural and Oral Dynamics",
    "authors": [
      "Shruti Agarwal",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Agarwal_Detecting_Deep-Fake_Videos_From_Aural_and_Oral_Dynamics_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Agarwal_Detecting_Deep-Fake_Videos_From_Aural_and_Oral_Dynamics_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A face-swap deep fake replaces a person's face -- from eyebrows to chin -- with another face. A lip-sync deep fake replaces a person's mouth region to be consistent with an impersonated or synthesized audio track. An overlooked aspect in the creation of these deep-fake videos is the human ear. Statically, the shape of the human ear has been shown to provide a biometric signal. Dynamically, movement of the mandible (lower jaw) causes changes in the shape of the ear and ear canal. While the facial identity in a face-swap deep fake may accurately depict the co-opted identity, the ears belong to the original identity. While the mouth in a lip-sync deep fake may be well synchronized with the audio, the dynamics of the ear motion will be de-coupled from the mouth and jaw motion. We describe a forensic technique that exploits these static and dynamic aural properties. ",
    "code_link": ""
  },
  "cvpr2021_wmf_fashion-guidedadversarialattackonpersonsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WMF",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Media Forensics",
    "title": "Fashion-Guided Adversarial Attack on Person Segmentation",
    "authors": [
      "Marc Treu",
      "Trung-Nghia Le",
      "Huy H. Nguyen",
      "Junichi Yamagishi",
      "Isao Echizen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Treu_Fashion-Guided_Adversarial_Attack_on_Person_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WMF/papers/Treu_Fashion-Guided_Adversarial_Attack_on_Person_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents the first adversarial example based method for attacking human instance segmentation networks, namely person segmentation networks in short, which are harder to fool than classification networks. We propose a novel Fashion-Guided Adversarial Attack (FashionAdv) framework to automatically identify attackable regions in the target image to minimize the effect on image quality. It generates adversarial textures learned from fashion style images and then overlays them on the clothing regions in the original image to make all persons in the image invisible to person segmentation networks. The synthesized adversarial textures are inconspicuous and appear natural to the human eye. The effectiveness of the proposed method is enhanced by robustness training and by jointly attacking multiple components of the target network. Extensive experiments demonstrated the effectiveness of FashionAdv in terms of robustness to image manipulations and storage in cyberspace as well as appearing natural to the human eye. The code and data are publicly released on our project page: https://github.com/nii-yamagishilab/fashion_adv. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_self-supervisedmulti-imagesuper-resolutionforpush-framesatelliteimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Self-Supervised Multi-Image Super-Resolution for Push-Frame Satellite Images",
    "authors": [
      "Ngoc Long Nguyen",
      "Jeremy Anger",
      "Axel Davy",
      "Pablo Arias",
      "Gabriele Facciolo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Nguyen_Self-Supervised_Multi-Image_Super-Resolution_for_Push-Frame_Satellite_Images_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Nguyen_Self-Supervised_Multi-Image_Super-Resolution_for_Push-Frame_Satellite_Images_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recent constellations of optical satellites are adopting multi-image super-resolution (MISR) from bursts of push-frame images as a way to increase the resolution and reduce the noise of their products while maintaining a lower cost of operation. Most MISR techniques are currently based on the aggregation of samples from registered low resolution images. A promising research trend aimed at incorporating natural image priors in MISR consists in using data-driven neural networks. However, due to the unavailability of ground truth high resolution data, these networks cannot be trained on real satellite images. In this paper, we present a framework for training MISR algorithms from bursts of satellite images without requiring high resolution ground truth. This is achieved by adapting the recently proposed frame-to-frame framework to process bursts of satellite images. In addition we propose an architecture based on feature aggregation that allows to fuse a variable number of frames and is capable of handling degenerate samplings while also reducing noise. On synthetic datasets, the proposed self-supervision strategy attains results on par with those obtained with a supervised training. We applied our framework to real SkySat satellite image bursts leading to results that are more resolved and less noisy than the L1B product from Planet. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_towardsindirecttop-downroadtransportemissionsestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Towards Indirect Top-Down Road Transport Emissions Estimation",
    "authors": [
      "Ryan Mukherjee",
      "Derek Rollend",
      "Gordon Christie",
      "Armin Hadzic",
      "Sally Matson",
      "Anshu Saksena",
      "Marisa Hughes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Mukherjee_Towards_Indirect_Top-Down_Road_Transport_Emissions_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Mukherjee_Towards_Indirect_Top-Down_Road_Transport_Emissions_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Road transportation is one of the largest sectors of greenhouse gas (GHG) emissions affecting climate change. Tackling climate change as a global community will require new capabilities to measure and inventory road transport emissions. However, the large scale and distributed nature of vehicle emissions make this sector especially challenging for existing inventory methods. In this work, we develop machine learning models that use satellite imagery to perform indirect top-down estimation of road transport emissions. Our initial experiments focus on the United States, where a bottom-up inventory was available for training our models. We achieved a mean absolute error (MAE) of 39.5 kg CO2 of annual road transport emissions, calculated on a pixel-by-pixel (100 m^2) basis in Sentinel-2 imagery. We also discuss key model assumptions and challenges that need to be addressed to develop models capable of generalizing to global geography. We believe this work is the first published approach for automated indirect top-down estimation of road transport sector emissions using visual imagery and represents a critical step towards scalable, global, near-real-time road transportation emissions inventories that are measured both independently and objectively. ",
    "code_link": "https://github.com/qubvel/segmentation_models"
  },
  "cvpr2021_earthvision_trainingdomain-invariantobjectdetectorfasterwithfeaturereplayandslowlearner": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Training Domain-Invariant Object Detector Faster With Feature Replay and Slow Learner",
    "authors": [
      "Chaehyeon Lee",
      "Junghoon Seo",
      "Heechul Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Lee_Training_Domain-Invariant_Object_Detector_Faster_With_Feature_Replay_and_Slow_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Lee_Training_Domain-Invariant_Object_Detector_Faster_With_Feature_Replay_and_Slow_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In deep learning-based object detection on remote sensing domain, nuisance factors, which affect observed variables while not affecting predictor variables, often matters because they cause domain changes. Previously, nuisance disentangled feature transformation (NDFT) was proposed to build domain-invariant feature extractor with with knowledge of nuisance factors. However, NDFT requires enormous time in a training phase, so it has been impractical. In this paper, we introduce our proposed method, A-NDFT, which is an improvement to NDFT. A-NDFT utilizes two acceleration techniques, feature replay and slow learner. Consequently, on a large-scale UAVDT benchmark, it is shown that our framework can reduce the training time of NDFT from 31 hours to 3 hours while still maintaining the performance. The code will be made publicly available online. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_landcover.aidatasetforautomaticmappingofbuildings,woodlands,waterandroadsfromaerialimagery": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads From Aerial Imagery",
    "authors": [
      "Adrian Boguszewski",
      "Dominik Batorski",
      "Natalia Ziemba-Jankowska",
      "Tomasz Dziedzic",
      "Anna Zambrzycka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Boguszewski_LandCover.ai_Dataset_for_Automatic_Mapping_of_Buildings_Woodlands_Water_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Boguszewski_LandCover.ai_Dataset_for_Automatic_Mapping_of_Buildings_Woodlands_Water_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Monitoring of land cover and land use is crucial in natural resources management. Automatic visual mapping can carry enormous economic value for agriculture, forestry, or public administration. Satellite or aerial images combined with computer vision and deep learning enable precise assessment and can significantly speed up change detection. Aerial imagery usually provides images with much higher pixel resolution than satellite data allowing more detailed mapping. However, there is still a lack of aerial datasets made for the segmentation, covering rural areas with a resolution of tens centimeters per pixel, manual fine labels, and highly publicly important environmental instances like buildings, woods, water, or roads. Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for semantic segmentation. We collected images of 216.27 sq. km rural areas across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated four following classes of objects: buildings, woodlands, water, and roads. Additionally, we report simple benchmark results, achieving 85.56% of mean intersection over union on the test set. It proves that the automatic mapping of land cover is possible with a relatively small, cost-efficient, RGB-only dataset. The dataset is publicly available at https://landcover.ai ",
    "code_link": "https://github.com/aleju/imgaug"
  },
  "cvpr2021_earthvision_opengfanultra-large-scalegroundfilteringdatasetbuiltuponopenalspointcloudsaroundtheworld": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "OpenGF: An Ultra-Large-Scale Ground Filtering Dataset Built Upon Open ALS Point Clouds Around the World",
    "authors": [
      "Nannan Qin",
      "Weikai Tan",
      "Lingfei Ma",
      "Dedong Zhang",
      "Jonathan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Qin_OpenGF_An_Ultra-Large-Scale_Ground_Filtering_Dataset_Built_Upon_Open_ALS_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Qin_OpenGF_An_Ultra-Large-Scale_Ground_Filtering_Dataset_Built_Upon_Open_ALS_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Ground filtering has remained a widely studied but incompletely resolved bottleneck for decades in the automatic generation of high-precision digital elevation model, due to the dramatic changes of topography and the complex structures of objects. The recent breakthrough of supervised deep learning algorithms in 3D scene understanding brings new solutions for better solving such problems. However, there are few large-scale and scene-rich public datasets dedicated to ground extraction, which considerably limits the development of effective deep-learning-based ground filtering methods. To this end, we present OpenGF, first Ultra-Large-Scale Ground Filtering dataset covering over 47 km^2 of 9 different typical terrain scenes built upon open ALS point clouds of 4 different countries around the world. OpenGF contains more than half a billion finely labeled ground and non-ground points, thousands of times the number of labeled points than the de facto standard ISPRS filtertest dataset. We extensively evaluate the performance of state-of-the-art rule-based algorithms and 3D semantic segmentation networks on our dataset and provide a comprehensive analysis. The results have confirmed the capability of OpenGF to train deep learning models effectively. This dataset is released at https://github.com/Nathan-UW/OpenGF to promote more advancing research for ground filtering and large-scale 3D geographic environment understanding. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_addressingvisualsearchinopenandclosedsetsettings": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Addressing Visual Search in Open and Closed Set Settings",
    "authors": [
      "Nathan Drenkow",
      "Philippe Burlina",
      "Neil Fendley",
      "Onyekachi Odoemene",
      "Jared Markowitz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Drenkow_Addressing_Visual_Search_in_Open_and_Closed_Set_Settings_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Drenkow_Addressing_Visual_Search_in_Open_and_Closed_Set_Settings_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Searching for small objects in large images is a task that is both challenging for current deep learning systems and important in numerous real-world applications, such as remote sensing and medical imaging. Thorough scanning of very large images is computationally expensive, particularly at resolutions sufficient to capture small objects. The smaller an object of interest, the more likely it is to be obscured by clutter or otherwise deemed insignificant. We examine these issues in the context of two complementary problems: closed-set object detection and open-set target search. First, we present a method for predicting pixel-level objectness from a low resolution gist image, which we then use to select regions for performing object detection locally at high resolution. This approach has the benefit of not being fixed to a predetermined grid, thereby requiring fewer costly high-resolution glimpses than existing methods. Second, we propose a novel strategy for open-set visual search that seeks to find all instances of a target class which may be previously unseen and is defined by a single image. We interpret both detection problems through a probabilistic, Bayesian lens, whereby the objectness maps produced by our method serve as priors in a maximum-a-posteriori approach to the detection step. We evaluate the end-to-end performance of both the combination of our patch selection strategy with this target search approach and the combination of our patch selection strategy with standard object detection methods. Both elements of our approach are seen to significantly outperform baseline strategies. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_qfabricmulti-taskchangedetectiondataset": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "QFabric: Multi-Task Change Detection Dataset",
    "authors": [
      "Sagar Verma",
      "Akash Panigrahi",
      "Siddharth Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Verma_QFabric_Multi-Task_Change_Detection_Dataset_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Verma_QFabric_Multi-Task_Change_Detection_Dataset_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Detecting change through multi-image, multi-date remote sensing is essential to developing an understanding of global conditions. Despite recent advancements in remote sensing realized through deep learning, novel methods for accurate multi-image change detection remain unrealized. Recently, several promising methods have been proposed to address this topic, but a paucity of publicly available data limits the methods that can be assessed. In particular, there exists limited work on categorizing the nature and status of change across an observation period. This paper introduces the first labeled dataset available for such a task. We present an open-source change detection dataset, termed QFabric, with 450,000 change polygons annotated across 504 locations in 100 different cities covering a wide range of geographies and urban fabrics. QFabric is a temporal multi-task dataset with 6 change types and 9 change status classes. The geography and environment metadata around each polygon provides context that can be leveraged to build robust deep neural networks. We apply multiple benchmarks on our dataset for change detection, change type and status classification tasks. Project page: https://sagarverma.github.io/qfabric ",
    "code_link": ""
  },
  "cvpr2021_earthvision_shadowneuralradiancefieldsformulti-viewsatellitephotogrammetry": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Shadow Neural Radiance Fields for Multi-View Satellite Photogrammetry",
    "authors": [
      "Dawa Derksen",
      "Dario Izzo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Derksen_Shadow_Neural_Radiance_Fields_for_Multi-View_Satellite_Photogrammetry_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Derksen_Shadow_Neural_Radiance_Fields_for_Multi-View_Satellite_Photogrammetry_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_point2color3dpointcloudcolorizationusingaconditionalgenerativenetworkanddifferentiablerenderingforairbornelidar": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Point2color: 3D Point Cloud Colorization Using a Conditional Generative Network and Differentiable Rendering for Airborne LiDAR",
    "authors": [
      "Takayuki Shinohara",
      "Haoyi Xiu",
      "Masashi Matsuoka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Shinohara_Point2color_3D_Point_Cloud_Colorization_Using_a_Conditional_Generative_Network_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Shinohara_Point2color_3D_Point_Cloud_Colorization_Using_a_Conditional_Generative_Network_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Airborne LiDAR observations are very effective for providing accurate 3D point clouds, and archived data are becoming available to the public. In many cases, only geometric information is available in the published 3D point cloud observed by airborne LiDAR (airborne 3D point cloud), and geometric information alone is not readable. Thus, it is important to colorize airborne 3D point clouds to improve visual readability. A scheme for 3D point cloud colorization using a conditional generative adversarial network (cGAN) was proposed, but it is difficult to apply to airborne LiDAR because the method is for artificial CAD models. Since airborne 3D point clouds are spread over a wider area than simple CAD models, it is important to evaluate them spatially in two-dimensional (2D) images. Currently, the differentiable renderer is the most reliable method to bridge 3D and 2D images. In this paper, we propose an airborne 3D point cloud colorization scheme called point2color using cGAN with points and rendered images. To achieve airborne 3D point cloud colorization, we estimate the color of each point with PointNet++ and render the estimated colored airborne 3D point cloud into a 2D image with a differentiable renderer. The network is then trained by minimizing the distance between real color and colorized fake color. The experimental results demonstrate the effectiveness of point2color using the IEEE GRSS 2018 Data Fusion Contest dataset with lower error than previous studies. Furthermore, an ablation study demonstrates the effectiveness of using a cGAN pipeline and 2D images via a differentiable renderer. Our code will be available at https://github.com/shnhrtkyk/point2color. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_combiningremotelysensedimagerywithsurvivalmodelsforoutageriskestimationofthepowergrid": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Combining Remotely Sensed Imagery With Survival Models for Outage Risk Estimation of the Power Grid",
    "authors": [
      "Arpit Jain",
      "Tapan Shah",
      "Mohammed Yousefhussien",
      "Achalesh Pandey"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Jain_Combining_Remotely_Sensed_Imagery_With_Survival_Models_for_Outage_Risk_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Jain_Combining_Remotely_Sensed_Imagery_With_Survival_Models_for_Outage_Risk_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vegetation management of power grids is essential for reliable distribution of services, prevention of forest fires and disruption of electricity due to tree fall. In this paper, we introduce a vegetation analysis system that utilizes information from GIS data, aerial and satellite imagery to estimate vegetation profile within a buffer zone. This vegetation profile is further combined with operational parameters of the grid to develop a survival model which predicts the outage risk of a power-line in an electrical grid. Using historical data, we show that the risk scores thus obtained are significantly better at developing trimming schedules for grid power-lines, compared to existing available methods. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_earthnet2021alarge-scaledatasetandchallengeforearthsurfaceforecastingasaguidedvideopredictiontask.": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "EarthNet2021: A Large-Scale Dataset and Challenge for Earth Surface Forecasting as a Guided Video Prediction Task.",
    "authors": [
      "Christian Requena-Mesa",
      "Vitus Benson",
      "Markus Reichstein",
      "Jakob Runge",
      "Joachim Denzler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Requena-Mesa_EarthNet2021_A_Large-Scale_Dataset_and_Challenge_for_Earth_Surface_Forecasting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Requena-Mesa_EarthNet2021_A_Large-Scale_Dataset_and_Challenge_for_Earth_Surface_Forecasting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Satellite images are snapshots of the Earth surface. We propose to forecast them. We frame Earth surface forecasting as the task of predicting satellite imagery conditioned on future weather. EarthNet2021 is a large dataset suitable for training deep neural networks on the task. It contains Sentinel 2 satellite imagery at 20m resolution, matching topography and mesoscale (1.28km) meteorological variables packaged into 32000 samples. Additionally we frame EarthNet2021 as a challenge allowing for model intercomparison. Resulting forecasts will greatly improve (>x50) over the spatial resolution found in numerical models. This allows localized impacts from extreme weather to be predicted, thus supporting downstream applications such as crop yield prediction, forest health assessments or biodiversity monitoring. Find data, code, and how to participate at www.earthnet.tech ",
    "code_link": ""
  },
  "cvpr2021_earthvision_dotdistancefortinyobjectdetectioninaerialimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Dot Distance for Tiny Object Detection in Aerial Images",
    "authors": [
      "Chang Xu",
      "Jinwang Wang",
      "Wen Yang",
      "Lei Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Xu_Dot_Distance_for_Tiny_Object_Detection_in_Aerial_Images_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Xu_Dot_Distance_for_Tiny_Object_Detection_in_Aerial_Images_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Object detection has achieved great progress with the development of anchor-based and anchor-free detectors. However, the detection of tiny objects is still challenging due to the lack of appearance information. In this paper, we observe that Intersection over Union (IoU), the most widely used metric in object detection, is sensitive to slight offsets between predicted bounding boxes and ground truths when detecting tiny objects. Although some new metrics such as GIoU, DIoU and CIoU are proposed, their performance on tiny object detection is still below the expected level by a large margin. In this paper, we propose a simple but effective new metric called Dot Distance (DotD) for tiny object detection where DotD is defined as normalized Euclidean distance between the center points of two bounding boxes. Extensive experiments on tiny object detection dataset show that anchor-based detectors' performance is highly improved over their baselines with the application of DotD. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_machine-learned3dbuildingvectorizationfromsatelliteimagery": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Machine-Learned 3D Building Vectorization From Satellite Imagery",
    "authors": [
      "Yi Wang",
      "Stefano Zorzi",
      "Ksenia Bittner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Wang_Machine-Learned_3D_Building_Vectorization_From_Satellite_Imagery_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Wang_Machine-Learned_3D_Building_Vectorization_From_Satellite_Imagery_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a machine learning based approach for automatic 3D building reconstruction and vectorization. Taking a single-channel photogrammetric digital surface model (DSM) and panchromatic (PAN) image as input, we first filter out non-building objects and refine the building shapes of input DSM with a conditional generative adversarial network (cGAN). The refined DSM and the input PAN image are then used through a semantic segmentation network to detect edges and corners of building roofs. Later, a set of vectorization algorithms are proposed to build roof polygons. Finally, the height information from the refined DSM is added to the polygons to obtain a fully vectorized level of detail (LoD)-2 building model. We verify the effectiveness of our method on large-scale satellite images, where we obtain state-of-the-art performance. ",
    "code_link": ""
  },
  "cvpr2021_earthvision_singleviewgeocentricposeinthewild": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Single View Geocentric Pose in the Wild",
    "authors": [
      "Gordon Christie",
      "Kevin Foster",
      "Shea Hagstrom",
      "Gregory D. Hager",
      "Myron Z. Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Christie_Single_View_Geocentric_Pose_in_the_Wild_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Christie_Single_View_Geocentric_Pose_in_the_Wild_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Current methods for Earth observation tasks such as semantic mapping, map alignment, and change detection rely on near-nadir images; however, often the first available images in response to dynamic world events such as natural disasters are oblique. These tasks are much more difficult for oblique images due to observed object parallax. There has been recent success in learning to regress an object's geocentric pose, defined as height above ground and orientation with respect to gravity, by training with airborne lidar registered to satellite images. We present a model for this novel task that exploits affine invariance properties to outperform state of the art performance by a wide margin. We also address practical issues required to deploy this method in the wild for real-world applications. Our data and code are publicly available. ",
    "code_link": "https://github.com/pubgeo/monocular-geocentric-pose"
  },
  "cvpr2021_earthvision_learningtopredictcroptypefromheterogeneoussparselabelsusingmeta-learning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Learning To Predict Crop Type From Heterogeneous Sparse Labels Using Meta-Learning",
    "authors": [
      "Gabriel Tseng",
      "Hannah Kerner",
      "Catherine Nakalembe",
      "Inbal Becker-Reshef"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Tseng_Learning_To_Predict_Crop_Type_From_Heterogeneous_Sparse_Labels_Using_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Tseng_Learning_To_Predict_Crop_Type_From_Heterogeneous_Sparse_Labels_Using_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " There are many labelled datasets relating to land cover and crop type mapping that cover diverse geographies, agroecologies and land uses. However, these labels are often extremely sparse, particularly in low- and middle-income regions, with as few as tens of examples for certain crop types. This makes it challenging to train supervised machine learning models to detect specific crops in satellite observations of these regions. We investigate the utility of model-agnostic meta-learning (MAML) to learn from diverse global datasets and improve performance in data-sparse regions. We find that in a variety of countries (Togo, Kenya and Brazil) and across a variety of tasks (crop type mapping, crop vs. non-crop mapping), MAML improves performance compared to pretrained and random initial weights. We also investigate the utility of MAML for different target data-size regimes. We find MAML outperforms other methods for a wide range of training set sizes and positive to negative label ratios, indicating its general suitability for land use and crop type mapping. ",
    "code_link": "https://github.com/nasaharvest/crop-maml"
  },
  "cvpr2021_earthvision_self-supervisedlearningofremotesensingscenerepresentationsusingcontrastivemultiviewcoding": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EarthVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding",
    "authors": [
      "Vladan Stojnic",
      "Vladimir Risojevic"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Stojnic_Self-Supervised_Learning_of_Remote_Sensing_Scene_Representations_Using_Contrastive_Multiview_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Stojnic_Self-Supervised_Learning_of_Remote_Sensing_Scene_Representations_Using_Contrastive_Multiview_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In recent years self-supervised learning has emerged as a promising candidate for unsupervised representation learning. In the visual domain its applications are mostly studied in the context of images of natural scenes. However, its applicability is especially interesting in specific areas, like remote sensing and medicine, where it is hard to obtain huge amounts of labeled data. In this work, we conduct an extensive analysis of the applicability of self-supervised learning in remote sensing image classification. We analyze the influence of the number and domain of images used for self-supervised pre-training on the performance on downstream tasks. We show that, for the downstream task of remote sensing image classification, using self-supervised pre-training on remote sensing images can give better results than using supervised pre-training on images of natural scenes. Besides, we also show that self-supervised pre-training can be easily extended to multispectral images producing even better results on our downstream tasks. ",
    "code_link": "https://github.com/vladan-stojnic/CMC-RSSR"
  },
  "cvpr2021_lxcv_ionlyhaveeyesforyoutheimpactofmasksonconvolutional-basedfacialexpressionrecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "I Only Have Eyes for You: The Impact of Masks on Convolutional-Based Facial Expression Recognition",
    "authors": [
      "Pablo Barros",
      "Alessandra Sciutti"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Barros_I_Only_Have_Eyes_for_You_The_Impact_of_Masks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Barros_I_Only_Have_Eyes_for_You_The_Impact_of_Masks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The current COVID-19 pandemic has shown us that we are still facing unpredictable challenges in our society. The necessary constrain on social interactions affected heavily how we envision and prepare the future of social robots and artificial agents in general. Adapting current affective perception models towards constrained perception based on the hard separation between facial perception and affective understanding would help us to provide robust systems. In this paper, we perform an in-depth analysis of how recognizing affect from persons with masks differs from general facial expression perception. We evaluate how the recently proposed FaceChannel adapts towards recognizing facial expressions from persons with masks. In Our analysis, we evaluate different training and fine-tuning schemes to understand better the impact of masked facial expressions. We also perform specific feature-level visualization to demonstrate how the inherent capabilities of the FaceChannel to learn and combine facial features change when in a constrained social interaction scenario. ",
    "code_link": ""
  },
  "cvpr2021_lxcv_generalizablemulti-camera3dpedestriandetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "Generalizable Multi-Camera 3D Pedestrian Detection",
    "authors": [
      "Joao Paulo Lima",
      "Rafael Roberto",
      "Lucas Figueiredo",
      "Francisco Simoes",
      "Veronica Teichrieb"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Lima_Generalizable_Multi-Camera_3D_Pedestrian_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Lima_Generalizable_Multi-Camera_3D_Pedestrian_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a multi-camera 3D pedestrian detection method that does not need to train using data from the target scene. We estimate pedestrian location on the ground plane using a novel heuristic based on human body poses and person's bounding boxes from an off-the-shelf monocular detector. We then project these locations onto the world ground plane and fuse them with a new formulation of a clique cover problem. We also propose an optional step for exploiting pedestrian appearance during fusion by using a domain-generalizable person re-identification model. We evaluated the proposed approach on the challenging WILDTRACK dataset. It obtained a MODA of 0.569 and an F-score of 0.78, superior to state-of-the-art generalizable detection techniques. ",
    "code_link": "https://github.com/MVIG-SJTU/AlphaPose"
  },
  "cvpr2021_lxcv_baodbudget-awareobjectdetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "BAOD: Budget-Aware Object Detection",
    "authors": [
      "Alejandro Pardo",
      "Mengmeng Xu",
      "Ali Thabet",
      "Pablo Arbelaez",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Pardo_BAOD_Budget-Aware_Object_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Pardo_BAOD_Budget-Aware_Object_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We study the problem of object detection from a novel perspective in which annotation budget constraints are taken into consideration, appropriately coined Budget Aware Object Detection (BAOD). When provided with a fixed budget, we propose a strategy for building a diverse and informative dataset that can be used to optimally train a robust detector. We investigate both optimization and learning-based methods to sample which images to annotate and what type of annotation (strongly or weakly supervised) to annotate them with. We adopt a hybrid supervised learning framework to train the object detector from both these types of annotation. We conduct a comprehensive empirical study showing that a handcrafted optimization method outperforms other selection techniques including random sampling, uncertainty sampling and active learning. By combining an optimal image/annotation selection scheme with hybrid supervised learning to solve the BAOD problem, we show that one can achieve the performance of a strongly supervised detector on PASCAL-VOC 2007 while saving 12.8% of its original annotation budget. Furthermore, when 100% of the budget is used, it surpasses this performance by 2.0 mAP percentage points. ",
    "code_link": "https://github.com/jwyang/faster-rcnn.pytorch"
  },
  "cvpr2021_lxcv_talkingwithsignsasimplemethodtodetectnounsandnumbersinanon-annotatedsignslanguagecorpus": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "Talking With Signs: A Simple Method To Detect Nouns and Numbers in a Non-Annotated Signs Language Corpus",
    "authors": [
      "Eric Raphael Huiza Pereyra",
      "Cesar Augusto Olivares Poggi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Pereyra_Talking_With_Signs_A_Simple_Method_To_Detect_Nouns_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Pereyra_Talking_With_Signs_A_Simple_Method_To_Detect_Nouns_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " People with deafness or hearing disabilities who aim to use computer based systems rely on state-of-art video classification and human action recognition techniques that combine traditional movement pattern recognition and deep learning techniques. In this work we present a pipeline for semi-automatic video annotation applied to a non-annotated Peruvian Signs Language (PSL) corpus along with a novel method for a progressive detection of PSL elements (nSDm). We produced a set of video annotations indicating signs appearances for a small set of nouns and numbers along with a labeled PSL dataset (PSL dataset). A model obtained after ensemble a 2D CNN trained with movement patterns extracted from the PSL dataset using Lucas Kanade Opticalflow, and a RNN with LSTM cells trained with raw RGB frames extracted from the PSL dataset reporting state-of-art results over the PSL dataset on signs classification tasks in terms of AUC, Precision and Recall. ",
    "code_link": "https://github.com/erichuizapucp/signs-recognition"
  },
  "cvpr2021_lxcv_assistivesignalsfordeepneuralnetworkclassifiers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "Assistive Signals for Deep Neural Network Classifiers",
    "authors": [
      "Camilo Pestana",
      "Wei Liu",
      "David Glance",
      "Robyn Owens",
      "Ajmal Mian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Pestana_Assistive_Signals_for_Deep_Neural_Network_Classifiers_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Pestana_Assistive_Signals_for_Deep_Neural_Network_Classifiers_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep Neural Networks are brittle in that small changes in the input can drastically affect their prediction outcome and confidence. Consequently, research in this area mainly focus on adversarial attacks and defenses. In this paper, we take an alternative stance and introduce the concept of Assistive Signals, which are perturbations optimized to improve a model's confidence score regardless if it's under attack or not. We analyze some interesting properties of these assistive perturbations and extend the idea to optimize them in the 3D space simulating different lighting conditions and viewing angles. Experimental evaluations show that the assistive signals generated by our optimization method increase the accuracy and confidence of deep models more than those generated by conventional methods that work in the 2D space. `Assistive Signals' also illustrate bias of ML models towards certain patterns in real-life objects. ",
    "code_link": "https://github.com/elcronos/assistive-signals"
  },
  "cvpr2021_lxcv_ifindyourlackofuncertaintyincomputervisiondisturbing": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "I Find Your Lack of Uncertainty in Computer Vision Disturbing",
    "authors": [
      "Matias Valdenegro-Toro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Valdenegro-Toro_I_Find_Your_Lack_of_Uncertainty_in_Computer_Vision_Disturbing_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Valdenegro-Toro_I_Find_Your_Lack_of_Uncertainty_in_Computer_Vision_Disturbing_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural networks are used for many real world applications, but often they have problems estimating their own confidence. This is particularly problematic for computer vision applications aimed at making high stakes decisions with humans and their lives. In this paper we make a meta-analysis of the literature, showing that most if not all computer vision applications do not use proper epistemic uncertainty quantification, which means that these models ignore their own limitations. We describe the consequences of using models without proper uncertainty quantification, and motivate the community to adopt versions of the models they use that have proper calibrated epistemic uncertainty, in order to enable out of distribution detection. We close the paper with a summary of challenges on estimating uncertainty for computer vision applications and recommendations. ",
    "code_link": ""
  },
  "cvpr2021_lxcv_ondisentanglementandmutualinformationinsemi-supervisedvariationalauto-encoders": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "On Disentanglement and Mutual Information in Semi-Supervised Variational Auto-Encoders",
    "authors": [
      "Elliott Gordon Rodriguez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Rodriguez_On_Disentanglement_and_Mutual_Information_in_Semi-Supervised_Variational_Auto-Encoders_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Rodriguez_On_Disentanglement_and_Mutual_Information_in_Semi-Supervised_Variational_Auto-Encoders_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In the context of variational auto-encoders, learning disentangled latent variable representations remains a challenging problem. In this abstract, we consider the semi-supervised setting, in which the factors of variation are labelled for a small fraction of our samples. We examine how the quality of learned representations is affected by the dimension of the unsupervised component of the latent space. We also consider a variational lower bound for the mutual information between the data and the semi-supervised component of the latent space, and analyze its role in the context of disentangled representation learning. ",
    "code_link": ""
  },
  "cvpr2021_lxcv_abopandbeyondasecondorderoptimizerforbinarizedneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "A Bop and Beyond: A Second Order Optimizer for Binarized Neural Networks",
    "authors": [
      "Cuauhtemoc Daniel Suarez-Ramirez",
      "Miguel Gonzalez-Mendoza",
      "Leonardo Chang",
      "Gilberto Ochoa-Ruiz",
      "Mario Alberto Duran-Vega"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Suarez-Ramirez_A_Bop_and_Beyond_A_Second_Order_Optimizer_for_Binarized_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Suarez-Ramirez_A_Bop_and_Beyond_A_Second_Order_Optimizer_for_Binarized_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The optimization of Binary Neural Networks (BNNs) relies on approximating the real-valued weights with their binarized representations. Current techniques for weight-updating use the same approaches as traditional Neural Networks (NNs) with the extra requirement of using an approximation to the derivative of the sign function - as it is the Dirac-Delta function - for back-propagation; thus, efforts are focused adapting full-precision techniques to work on BNNs. In the literature, only one previous effort has tackled the problem of directly training the BNNs with bit-flips by using the first raw moment estimate of the gradients and comparing it against a threshold for deciding when to flip a weight (Bop). In this paper, we take an approach parallel to Adam which also uses the second raw moment estimate to normalize the first raw moment before doing the comparison with the threshold, we call this method Bop2ndOrder. We present two versions of the proposed optimizer: a biased one and a bias-corrected one, each with its own applications. Also, we present a complete ablation study of the hyperparameters space, as well as the effect of using schedulers on each of them. For these studies, we tested the optimizer in CIFAR10 using the BinaryNet architecture. Also, we tested it in ImageNet 2012 with the XnorNet and BiRealNet architectures for accuracy. In both datasets our approach proved to converge faster, was robust to changes of the hyperparameters, and achieved better accuracy values. ",
    "code_link": ""
  },
  "cvpr2021_lxcv_markerposerobustreal-timeplanartargettrackingforaccuratestereoposeestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "MarkerPose: Robust Real-Time Planar Target Tracking for Accurate Stereo Pose Estimation",
    "authors": [
      "Jhacson Meza",
      "Lenny A. Romero",
      "Andres G. Marrugo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Meza_MarkerPose_Robust_Real-Time_Planar_Target_Tracking_for_Accurate_Stereo_Pose_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Meza_MarkerPose_Robust_Real-Time_Planar_Target_Tracking_for_Accurate_Stereo_Pose_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Despite the attention marker-less pose estimation has attracted in recent years, marker-based approaches still provide unbeatable accuracy under controlled environmental conditions. Thus, they are used in many fields such as robotics or biomedical applications but are primarily implemented through classical approaches, which require lots of heuristics and parameter tuning for reliable performance under different environments. In this work, we propose MarkerPose, a robust, real-time pose estimation system based on a planar target of three circles and a stereo vision system. MarkerPose is meant for high-accuracy pose estimation applications. Our method consists of two deep neural networks for marker point detection. A SuperPoint-like network for pixel-level accuracy keypoint localization and classification, and we introduce EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level accuracy keypoint detection. The marker's pose is estimated through stereo triangulation. The target point detection is robust to low lighting and motion blur conditions. We compared MarkerPose with a detection method based on classical computer vision techniques using a robotic arm for validation. The results show our method provides better accuracy than the classical technique. Finally, we demonstrate the suitability of MarkerPose in a 3D freehand ultrasound system, which is an application where highly accurate pose estimation is required. Code is available in Python and C++ at https://github.com/jhacsonmeza/MarkerPose. ",
    "code_link": "https://github.com/jhacsonmeza/MarkerPose"
  },
  "cvpr2021_lxcv_filterdistributiontemplatesinconvolutionalnetworksforimageclassificationtasks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LXCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - LatinX in CV Research",
    "title": "Filter Distribution Templates in Convolutional Networks for Image Classification Tasks",
    "authors": [
      "Ramon Izquierdo-Cordova",
      "Walterio Mayol-Cuevas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/html/Izquierdo-Cordova_Filter_Distribution_Templates_in_Convolutional_Networks_for_Image_Classification_Tasks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LXCV/papers/Izquierdo-Cordova_Filter_Distribution_Templates_in_Convolutional_Networks_for_Image_Classification_Tasks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural network designers have reached progressive accuracy by increasing models depth, introducing new layer types and discovering new combinations of layers. A common element in many architectures is the distribution of the number of filters in each layer. Neural network models keep a pattern design of increasing filters in deeper layers such as those in LeNet, VGG, ResNet, MobileNet and even in automatic discovered architectures such as NASNet. It remains unknown if this pyramidal distribution of filters is the best for different tasks and constrains. In this work we present a series of modifications in the distribution of filters in three popular neural network models and their effects in accuracy and resource consumption. Results show that by applying this approach, some models improve up to 8.9% in accuracy showing reductions in parameters up to 54%. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_comparingrepresentationsintrackingforeventcamera-basedslam": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Comparing Representations in Tracking for Event Camera-Based SLAM",
    "authors": [
      "Jianhao Jiao",
      "Huaiyang Huang",
      "Liang Li",
      "Zhijian He",
      "Yilong Zhu",
      "Ming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Jiao_Comparing_Representations_in_Tracking_for_Event_Camera-Based_SLAM_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Jiao_Comparing_Representations_in_Tracking_for_Event_Camera-Based_SLAM_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper investigates two typical image-type representations for event camera-based tracking: time surface (TS) and event map (EM). Based on the original TS-based tracker, we make use of these two representations' complementary strengths to develop an enhanced version. The proposed tracker consists of a general strategy to evaluate the optimization problem's degeneracy online and then switch proper representations. Both TS and EM are motion- and scene-dependent, and thus it is important to figure out their limitations in tracking. We develop six tracker variations and conduct a thorough comparison of them on sequences covering various scenarios and motion complexities. We release our implementations and detailed results to benefit the research community on event cameras: https: //github.com/gogojjh/ESVO_extension. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_v2efromvideoframestorealisticdvsevents": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "v2e: From Video Frames to Realistic DVS Events",
    "authors": [
      "Yuhuang Hu",
      "Shih-Chii Liu",
      "Tobi Delbruck"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Hu_v2e_From_Video_Frames_to_Realistic_DVS_Events_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Hu_v2e_From_Video_Frames_to_Realistic_DVS_Events_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " To help meet the increasing need for dynamic vision sensor (DVS) event camera data, this paper proposes the v2e toolbox that generates realistic synthetic DVS events from intensity frames. It also clarifies incorrect claims about DVS motion blur and latency characteristics in recent literature. Unlike other toolboxes, v2e includes pixel-level Gaussian event threshold mismatch, finite intensity-dependent bandwidth, and intensity-dependent noise. Realistic DVS events are useful in training networks for uncontrolled lighting conditions. The use of v2e synthetic events is demonstrated in two experiments. The first experiment is object recognition with N-Caltech 101 dataset. Results show that pretraining on various v2e lighting conditions improves generalization when transferred on real DVS data for a ResNet model. The second experiment shows that for night driving, a car detector trained with v2e events shows an average accuracy improvement of 40% compared to the YOLOv3 trained on intensity frames. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_livedemonstrationincrementalmotionestimationforevent-basedcamerasbydispersionminimisation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Live Demonstration: Incremental Motion Estimation for Event-Based Cameras by Dispersion Minimisation",
    "authors": [
      "Urbano Miguel Nunes",
      "Yiannis Demiris"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Nunes_Live_Demonstration_Incremental_Motion_Estimation_for_Event-Based_Cameras_by_Dispersion_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Nunes_Live_Demonstration_Incremental_Motion_Estimation_for_Event-Based_Cameras_by_Dispersion_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Live Demonstration: Incremental Motion Estimation for Event-based Cameras by Dispersion Minimisation ",
    "code_link": "https://github.com/facontidavide/PlotJuggler"
  },
  "cvpr2021_eventvision_imagereconstructionfromneuromorphiceventcamerasusinglaplacian-predictionandpoissonintegrationwithspikingandartificialneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Image Reconstruction From Neuromorphic Event Cameras Using Laplacian-Prediction and Poisson Integration With Spiking and Artificial Neural Networks",
    "authors": [
      "Hadar Cohen Duwek",
      "Albert Shalumov",
      "Elishai Ezra Tsur"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Duwek_Image_Reconstruction_From_Neuromorphic_Event_Cameras_Using_Laplacian-Prediction_and_Poisson_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Duwek_Image_Reconstruction_From_Neuromorphic_Event_Cameras_Using_Laplacian-Prediction_and_Poisson_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Event cameras are robust neuromorphic visual sensors, which communicate transients in luminance as events. Current paradigms for image reconstruction from events mostly rely on direct optimization of artificial Convolutional Neural Networks (CNNs). Here we propose a two-phase neural network, which comprises a CNN, optimized for Laplacian prediction, and a Spiking Neural Network (SNN) optimized for Poisson integration. By introducing Laplacian prediction into the pipeline, we provide image reconstruction with a network comprising only 200 parameters. We converted the CNN to SNN, providing a full neuromorphic implementation. We further optimized the network with Mish activation and a novel convoluted CNN design, proposing a hybrid of spiking and artificial neural network with < 100 parameters. Models were evaluated on both N-MNIST and N-Caltech101 datasets. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_feedbackcontrolofeventcameras": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Feedback Control of Event Cameras",
    "authors": [
      "Tobi Delbruck",
      "Rui Graca",
      "Marcin Paluch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Delbruck_Feedback_Control_of_Event_Cameras_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Delbruck_Feedback_Control_of_Event_Cameras_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Dynamic vision sensor event cameras produce a variable data rate stream of brightness change events. Event production at the pixel level is controlled by threshold, bandwidth, and refractory period bias current parameter settings. Biases must be adjusted to match application requirements and the optimal settings depend on many factors. As a first step towards automatic control of biases, this paper proposes fixed-step feedback controllers that use measurements of event rate and noise. The controllers regulate the event rate within an acceptable range using threshold and refractory period control, and regulate noise using bandwidth control. Experiments demonstrate model validity and feedback control. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_differentiableeventstreamsimulatorfornon-rigid3dtracking": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Differentiable Event Stream Simulator for Non-Rigid 3D Tracking",
    "authors": [
      "Jalees Nehvi",
      "Vladislav Golyanik",
      "Franziska Mueller",
      "Hans-Peter Seidel",
      "Mohamed Elgharib",
      "Christian Theobalt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Nehvi_Differentiable_Event_Stream_Simulator_for_Non-Rigid_3D_Tracking_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Nehvi_Differentiable_Event_Stream_Simulator_for_Non-Rigid_3D_Tracking_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces the first differentiable simulator of event streams, i.e., streams of asynchronous brightness change signals recorded by event cameras. Our differentiable simulator enables non-rigid 3D tracking of deformable objects (such as human hands, isometric surfaces and general watertight meshes) from event streams by leveraging an analysis-by-synthesis principle. So far, event-based tracking and reconstruction of non-rigid objects in 3D, like hands and body, has been either tackled using explicit event trajectories or large-scale datasets. In contrast, our method does not require any such processing or data, and can be readily applied to incoming event streams. We show the effectiveness of our approach for various types of non-rigid objects and compare to existing methods for non-rigid 3D tracking. In our experiments, the proposed energy-based formulations outperform competing RGB-based methods in terms of 3D errors. The source code and the new data are publicly available. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_dvs-outlabaneuromorphicevent-basedlongtimemonitoringdatasetforreal-worldoutdoorscenarios": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "DVS-OUTLAB: A Neuromorphic Event-Based Long Time Monitoring Dataset for Real-World Outdoor Scenarios",
    "authors": [
      "Tobias Bolten",
      "Regina Pohle-Frohlich",
      "Klaus D. Tonnies"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Bolten_DVS-OUTLAB_A_Neuromorphic_Event-Based_Long_Time_Monitoring_Dataset_for_Real-World_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Bolten_DVS-OUTLAB_A_Neuromorphic_Event-Based_Long_Time_Monitoring_Dataset_for_Real-World_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neuromorphic vision sensors are biologically inspired devices which differ fundamentally from well known frame-based sensors. Even though developments in this research area are increasing, applications that rely entirely on event cameras are still relatively rare. This becomes particularly clear when considering real outdoor scenarios apart from laboratory conditions. One obstacle to the development of event-based vision applications in this context may be the lack of labeled datasets for algorithm development and evaluation. Therefore we describe a recording setting of a DVS-based long time monitoring of an urban public area and provide labeled DVS data that also contain effects of environmental outdoor influences recorded in this process. We also describe the processing chain used for label generation, as well as results from a performed denoising benchmark utilizing various spatio-temporal event stream filters. The dataset contains almost 7 hours of real world outdoor event-data with approx. 47k labeled regions of interest and can be downloaded at http://dnt.kr.hsnr.de/DVS-OUTLAB/ ",
    "code_link": "https://github.com/matterport/Mask_RCNN"
  },
  "cvpr2021_eventvision_acortically-inspiredarchitectureforevent-basedvisualmotionprocessingfromdesignprinciplestoreal-worldapplications": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "A Cortically-Inspired Architecture for Event-Based Visual Motion Processing: From Design Principles to Real-World Applications",
    "authors": [
      "Francesca Peveri",
      "Simone Testa",
      "Silvio P. Sabatini"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Peveri_A_Cortically-Inspired_Architecture_for_Event-Based_Visual_Motion_Processing_From_Design_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Peveri_A_Cortically-Inspired_Architecture_for_Event-Based_Visual_Motion_Processing_From_Design_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We developed and tested the architecture of a bio-inspired Spiking Neural Network for motion estimation. The computation performed by the retina is emulated by the neuromorphic event-based image sensor DAVIS346 which constitutes the input of our network. We obtained neurons highly tuned to spatial frequency and orientation of the stimulus through a combination of feed-forward excitatory connections modeled as an elongated Gaussian kernel and recurrent inhibitory connections from two clusters of neurons within the same cortical layers. Sums over adjacent nodes weighted by time-variable synapses are used to attain Gabor-like spatio-temporal V1 receptive fields with selectivity to the stimulus' motion. In order to gain the invariance to the stimulus phase, the two polarities of the events provided by the neuromorphic sensor were exploited,which allowed us to build two pairs of quadrature filters from which we obtain Motion Energy detectors as described in [2]. Finally, a decoding stage allows us to compute optic flow from the Motion Detector layers. We tested the approach proposed with both synthetic and natural stimuli. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_detectingstablekeypointsfromeventsthroughimagegradientprediction": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Detecting Stable Keypoints From Events Through Image Gradient Prediction",
    "authors": [
      "Philippe Chiberre",
      "Etienne Perot",
      "Amos Sironi",
      "Vincent Lepetit"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Chiberre_Detecting_Stable_Keypoints_From_Events_Through_Image_Gradient_Prediction_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Chiberre_Detecting_Stable_Keypoints_From_Events_Through_Image_Gradient_Prediction_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a method that detects stable keypoints from an event stream at high speed with a low memory footprint. Our key observation connects two points: It should be easier to reconstruct the image gradients rather than the image itself from the events, and the Harris corner detector, one of the most reliable keypoint detectors for short baseline regular images, depends on the image gradients, not the image. We therefore introduce a recurrent convolutional neural network to predict image gradients from events. As image gradients and events are correlated, this prediction task is relatively easy and we can keep this network very small. We train our network solely on synthetic data. Extracting Harris corners from these gradients is then very efficient. Moreover, in contrast to learned methods, we can change the hyperparameters of the detector without retraining. Our experiments confirm that predicting image gradients rather than images is much more efficient, and that our approach predicts stable corner points which are easier to track for a longer time compared to state-of-the-art event-based methods. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_efi-netvideoframeinterpolationfromfusionofeventsandframes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "EFI-Net: Video Frame Interpolation From Fusion of Events and Frames",
    "authors": [
      "Genady Paikin",
      "Yotam Ater",
      "Roy Shaul",
      "Evgeny Soloveichik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Paikin_EFI-Net_Video_Frame_Interpolation_From_Fusion_of_Events_and_Frames_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Paikin_EFI-Net_Video_Frame_Interpolation_From_Fusion_of_Events_and_Frames_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Event cameras are sensors with pixels that respond independently and asynchronously to changes in scene illumination. Event cameras have a number of advantages when compared to conventional cameras: low-latency, high temporal resolution, high dynamic range, low power and sparse data output. However, existing event cameras also suffer from comparatively low spatial resolution and are sensitive to noise. Recently, it has been shown that it is possible to reconstruct an intensity frame stream from an event stream. These reconstructions preserve the high temporal rate of the event stream, but tend to suffer from significant artifacts and low image quality due to the shortcomings of event cameras. In this work we demonstrate that it is possible to combine the best of both worlds, by fusing a color frame stream at low temporal resolution and high spatial resolution with an event stream at high temporal resolution and low spatial resolution to generate a video stream with both high temporal and spatial resolutions while preserving the original color information. We utilize a novel event frame interpolation network (EFI-Net), a multi-phase convolutional neural network which fuses the frame and event streams. EFI-Net is trained using only simulated data and generalizes exceptionally well to real-world experimental data. We show that our method is able to interpolate frames where traditional video interpolation approaches fail, while also outperforming event-only reconstructions. We further contribute a new dataset, containing event camera data synchronized with high speed video. This work opens the door to a new application for event cameras, enabling high fidelity fusion with frame based image streams for generation of high-quality high-speed video. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_howtocalibrateyoureventcamera": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "How To Calibrate Your Event Camera",
    "authors": [
      "Manasi Muglikar",
      "Mathias Gehrig",
      "Daniel Gehrig",
      "Davide Scaramuzza"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Muglikar_How_To_Calibrate_Your_Event_Camera_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Muglikar_How_To_Calibrate_Your_Event_Camera_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a generic event camera calibration framework using image reconstruction. Instead of relying on blinking patterns or external screens, we show that neural network-based image reconstruction is well suited for the task of intrinsic and extrinsic calibration of event cameras. The advantage of our proposed approach is that we can use standard calibration patterns that do not rely on active illumination. Furthermore, our approach enables the possibility to perform extrinsic calibration between frame-based and event-based sensors without additional complexity. Both simulation and real-world experiments indicate that calibration through image reconstruction is accurate under common distortion models and a wide variety of distortion parameters. ",
    "code_link": "https://github.com/gorchard/DVScalibration"
  },
  "cvpr2021_eventvision_liftingmonoculareventsto3dhumanposes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Lifting Monocular Events to 3D Human Poses",
    "authors": [
      "Gianluca Scarpellini",
      "Pietro Morerio",
      "Alessio Del Bue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Scarpellini_Lifting_Monocular_Events_to_3D_Human_Poses_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Scarpellini_Lifting_Monocular_Events_to_3D_Human_Poses_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents a novel 3D human pose estimation approach using a single stream of asynchronous events as input. Most of the state-of-the-art approaches solve this task with RGB cameras, however struggling when subjects are moving fast. On the other hand, event-based 3D pose estimation benefits from the advantages of event-cameras, especially their efficiency and robustness to appearance changes. Yet, finding human poses in asynchronous events is in general more challenging than standard RGB pose estimation, since little or no events are triggered in static scenes. Here we propose the first learning-based method for 3D human pose from a single stream of events. Our method consists of two steps. First, we process the event-camera stream to predict three orthogonal heatmaps per joint; each heatmap is the projection of of the joint onto one orthogonal plane. Next, we fuse the sets of heatmaps to estimate 3D localisation of the body joints. As a further contribution, we make available a new, challenging dataset for event-based human pose estimation by simulating events from the RGB Human3.6m dataset. Experiments demonstrate that our method achieves solid accuracy, narrowing the performance gap between standard RGB and event-based vision. The code is freely available at https://iit-pavis.github.io/lifting_events_to_3d_hpe. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_spiketiming-basedunsupervisedlearningoforientation,disparity,andmotionrepresentationsinaspikingneuralnetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "Spike Timing-Based Unsupervised Learning of Orientation, Disparity, and Motion Representations in a Spiking Neural Network",
    "authors": [
      "Thomas Barbier",
      "Celine Teuliere",
      "Jochen Triesch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Barbier_Spike_Timing-Based_Unsupervised_Learning_of_Orientation_Disparity_and_Motion_Representations_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Barbier_Spike_Timing-Based_Unsupervised_Learning_of_Orientation_Disparity_and_Motion_Representations_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neuromorphic vision sensors present unique advantages over their frame based counterparts. However, unsupervised learning of efficient visual representations from their asynchronous output is still a challenge, requiring a rethinking of traditional image and video processing methods. Here we present a network of leaky integrate and fire neurons that learns representations similar to those of simple and complex cells in the primary visual cortex of mammals from the input of two event-based vision sensors. Through the combination of spike timing-dependent plasticity and homeostatic mechanisms, the network learns visual feature detectors for orientation, disparity, and motion in a fully unsupervised fashion. We validate our approach on a mobile robotic platform. ",
    "code_link": ""
  },
  "cvpr2021_eventvision_n-rodaneuromorphicdatasetforsynthetic-to-realdomainadaptation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EventVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Event-Based Vision",
    "title": "N-ROD: A Neuromorphic Dataset for Synthetic-to-Real Domain Adaptation",
    "authors": [
      "Marco Cannici",
      "Chiara Plizzari",
      "Mirco Planamente",
      "Marco Ciccone",
      "Andrea Bottino",
      "Barbara Caputo",
      "Matteo Matteucci"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/html/Cannici_N-ROD_A_Neuromorphic_Dataset_for_Synthetic-to-Real_Domain_Adaptation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EventVision/papers/Cannici_N-ROD_A_Neuromorphic_Dataset_for_Synthetic-to-Real_Domain_Adaptation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Event cameras are novel neuromorphic sensors, which asynchronously capture pixel-level intensity changes in the form of \"events\". Event simulation from existing RGB datasets is commonly used to overcome the need of large amount of annotated data, which lacks due to the novelty of the event sensors. In this context, the possibility of using event simulation in synthetic scenarios, where data generation is not limited to pre-existing datasets, is to date still unexplored. In this work, we analyze the synth-to-real domain shift in event data, i.e., the gap arising between simulated events obtained from synthetic renderings and those captured with a real camera on real images. To this purpose, we extend to the event modality the popular RGB-D Object Dataset (ROD), which already comes with its synthetic version (SynROD). The resulting Neuromorphic ROD dataset (N-ROD) is the first to enable a synth-to-real analysis on event data, showing the effectiveness of Domain Adaptation techniques in reducing the synth-to-real shift. Moreover, through extensive experiments on multi-modal RGB-E data, we show that events can be effectively combined with conventional visual information, encouraging further research in this area. The N-ROD dataset is available at https://N-ROD-dataset.github.io/home/. ",
    "code_link": ""
  },
  "cvpr2021_biometrics_indirectsyntheticattackonthermalfacebiometricsystemsviavisible-to-thermalspectrumconversion": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Biometrics",
    "title": "Indirect Synthetic Attack on Thermal Face Biometric Systems via Visible-to-Thermal Spectrum Conversion",
    "authors": [
      "Khawla Mallat",
      "Jean-Luc Dugelay"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/html/Mallat_Indirect_Synthetic_Attack_on_Thermal_Face_Biometric_Systems_via_Visible-to-Thermal_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/papers/Mallat_Indirect_Synthetic_Attack_on_Thermal_Face_Biometric_Systems_via_Visible-to-Thermal_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The growing interest in exploring thermal face biometrics is mostly due to the robustness of thermal imagery to face spoofing attacks. However, this robustness lies in the acquisition of thermal properties by the thermal sensor and is limited to presentation attacks. In this paper, we propose a new type of attack on thermal face recognition systems, performed at the post-sensor level. In the visible spectrum, this attack would be carried out by simply injecting a face image of the claimed identity into the communication channel right after the sensor. However, unlike visible face images that are abundantly available on the web, thermal face images are not easy to obtain. Therefore, we propose to generate synthetic thermal attacks by converting visible face images into the thermal spectrum. To perform visible-to-thermal spectrum conversion, we use a cascaded refinement network trained using contextual loss. In a scenario where the attacker has prior knowledge about the spoofing countermeasure of the system, we introduce a new loss computed at the local binary pattern (LBP) maps level to fool an LBP-based spoofing attack detection algorithm. The vulnerability of thermal face biometric systems to the proposed attack is then assessed using two existing baselines of spoofing attack detection. When compared to the challenging presentation attack using silicone masks, the equal error rate has increased from 0.20% to 11.60% and from 2.28% to 58.54% when exposed to the proposed synthetic attack, using the two spoofing attack detection baselines. ",
    "code_link": ""
  },
  "cvpr2021_biometrics_toeprintsanapplicationstudyforbiometricverificationinadults": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Biometrics",
    "title": "Toe Prints: An Application Study for Biometric Verification in Adults",
    "authors": [
      "Syed Sadaf Ali",
      "Vivek Singh Baghel",
      "Iyyakutti Iyappan Ganapathi",
      "Surya Prakash",
      "Ngoc-Son Vu",
      "Naoufel Werghi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/html/Ali_Toe_Prints_An_Application_Study_for_Biometric_Verification_in_Adults_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/papers/Ali_Toe_Prints_An_Application_Study_for_Biometric_Verification_in_Adults_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Biometric recognition systems provide an easy way to verify an individual's identity through physiological and behavioral biometric traits, due to the persistence of these traits. The physiological traits are extensively utilized to secure the applications of numerous fields. Among these, toe print is one of the physiological traits that has been discussed and evaluated for the children; however, it has not been addressed in the past as this can also be utilized to verify the identity of an adult, especially for a person with different abilities. In this paper, we have come up with a feasibility study of toe prints while comparing them with the impression of a person's different fingers in terms of the overall performance of a biometric verification system. To accomplish this task, the toe print database is collected from different persons of age varying from 17 years to 64 years, which is the first database of its kind and is made available in public domain. Fingerprint impressions are also collected along with the toe print to compare the reliability of the toe print with respect to performance using a standard fingerprint verification tool. Verifinger trial version has been utilized by considering the two standard fingerprint verification protocols viz. 1-vs-1 and FVC to assess the performance of the toe print verification system in terms of the equal error rate (EER). The toe print verification system attains 0.04% and 0.01% EER values for 1-vs-1 and FVC protocol, respectively, which clearly depicts the feasibility of toe print as a potential biometric trait. ",
    "code_link": ""
  },
  "cvpr2021_biometrics_adeepadversarialframeworkforvisuallyexplainableperiocularrecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Biometrics",
    "title": "A Deep Adversarial Framework for Visually Explainable Periocular Recognition",
    "authors": [
      "Joao Brito",
      "Hugo Proenca"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/html/Brito_A_Deep_Adversarial_Framework_for_Visually_Explainable_Periocular_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/papers/Brito_A_Deep_Adversarial_Framework_for_Visually_Explainable_Periocular_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The ability to portray the reasoning behind a decision has been at the core of major research efforts. It serves not only to increase trust amongst the stakeholders of the automated agent, but also to potentially improve the entire system as a whole. In this work, we present our efforts towards a visually explainable periocular recognition framework, with a simple, yet effective solution that automatically provides a visual representation of the features in each region that sustained an impostor pairwise comparison. Based in our quantitative and qualitative experiments, the results validate the proposed goals and reiterate the notion that exploitability should be strongly considered when designing ML algorithms. ",
    "code_link": ""
  },
  "cvpr2021_biometrics_differentialmorphfacedetectionusingdiscriminativewaveletsub-bands": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Biometrics",
    "title": "Differential Morph Face Detection Using Discriminative Wavelet Sub-Bands",
    "authors": [
      "Baaria Chaudhary",
      "Poorya Aghdaie",
      "Sobhan Soleymani",
      "Jeremy Dawson",
      "Nasser M. Nasrabadi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/html/Chaudhary_Differential_Morph_Face_Detection_Using_Discriminative_Wavelet_Sub-Bands_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/papers/Chaudhary_Differential_Morph_Face_Detection_Using_Discriminative_Wavelet_Sub-Bands_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Face recognition systems are extremely vulnerable to morphing attacks, in which a morphed facial reference image can be successfully verified as two or more distinct identities. In this paper, we propose a morph attack detection algorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for identifying morphed face images. The core of our framework is that artifacts resulting from the morphing process that are not discernible in the image domain can be more easily identified in the spatial frequency domain. A discriminative wavelet sub-band can accentuate the disparity between a real and a morphed image. To this end, multi-level DWT is applied to all images, yielding 48 mid and high-frequency sub-bands each. The entropy distributions for each sub-band are calculated separately for both bona fide and morph images. For some of the sub-bands, there is a marked difference between the entropy of the sub-band in a bona fide image and the identical sub-band's entropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence (KLD) to exploit these differences and isolate the sub-bands that are the most discriminative. We measure how discriminative a sub-band is by its KLD value and the 22 sub-bands with the highest KLD values are chosen for network training. Then, we train a deep Siamese neural network using these 22 selected sub-bands for differential morph attack detection. We examine the efficacy of discriminative wavelet sub-bands for morph attack detection and show that a deep neural network trained on these sub-bands can accurately identify morph imagery. ",
    "code_link": ""
  },
  "cvpr2021_biometrics_multistagefusionoffacematchers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Biometrics",
    "title": "Multistage Fusion of Face Matchers",
    "authors": [
      "Sergey Tulyakov",
      "Nishant Sankaran",
      "Deen Mohan",
      "Srirangaraj Setlur",
      "Venu Govindaraju"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/html/Tulyakov_Multistage_Fusion_of_Face_Matchers_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/papers/Tulyakov_Multistage_Fusion_of_Face_Matchers_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multistage, or serial, fusion refers to the algorithms sequentially fusing an increased number of matching results at each step and making decisions about accepting or rejecting the match hypothesis, or going to the next step. Such fusion methods are beneficial in the situations where running additional matching algorithms needed for later stages is time consuming or expensive. The construction of multistage fusion methods is challenging, since it requires both learning fusion functions and finding optimal decision thresholds for each stage. In this paper, we propose the use of single neural network for learning the multistage fusion. In addition we discuss the choices for the performance measurements of the trained algorithms and for the selection of network training optimization criteria. We perform the experiments using three face matching algorithms and IJB-A and IJB-C databases. ",
    "code_link": ""
  },
  "cvpr2021_biometrics_groupleakageoverestimatesperformanceacasestudyinkeystrokedynamics": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Biometrics",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Biometrics",
    "title": "Group Leakage Overestimates Performance: A Case Study in Keystroke Dynamics",
    "authors": [
      "Blaine Ayotte",
      "Mahesh K. Banavar",
      "Daqing Hou",
      "Stephanie Schuckers"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/html/Ayotte_Group_Leakage_Overestimates_Performance_A_Case_Study_in_Keystroke_Dynamics_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Biometrics/papers/Ayotte_Group_Leakage_Overestimates_Performance_A_Case_Study_in_Keystroke_Dynamics_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Keystroke dynamics is a powerful behavioral biometric capable of user authentication based on typing patterns. As larger keystroke datasets become available, machine learning and deep learning algorithms are becoming popular. Knowledge of every possible impostor is not known during training which means that keystroke dynamics is an open set recognition problem. Treating open set recognition problems as closed set (assuming samples from all impostors are present) can cause models to incur data leakage, which can provide unrealistic overestimates of performance. It is a common problem in machine learning and can cause models to report higher accuracies than would be expected in the real world. In this paper, we outline open set recognition and discuss how, if not handled properly, it can lead to data leakage. The performance of common machine learning methods, such as SVM and MLP are investigated with and without leakage to clearly demonstrate the differences in performance. A synthetic dataset and a publicly available keystroke dynamics fixed-text dataset are used for research transparency and reproducibility. ",
    "code_link": ""
  },
  "cvpr2021_amfg_anefficient3dsyntheticmodelgenerationpipelineforhumanposedataaugmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "An Efficient 3D Synthetic Model Generation Pipeline for Human Pose Data Augmentation",
    "authors": [
      "Kathan Vyas",
      "Le Jiang",
      "Shuangjun Liu",
      "Sarah Ostadabbas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Vyas_An_Efficient_3D_Synthetic_Model_Generation_Pipeline_for_Human_Pose_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Vyas_An_Efficient_3D_Synthetic_Model_Generation_Pipeline_for_Human_Pose_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " 3D modeling of articulated bodies of humans or animals and using these models for synthetic 2D and 3D pose data generation can mitigate the small data challenges faced by many critical applications such as healthcare. In this paper, we present our efficient 3D synthetic model generation (3D-SMG) pipeline used for body pose data augmentation. 3D-SMG pipeline starts with scanning point clouds from various angles around the subject using an off-the-shelf RGBD camera. We then implement a dual objective iterative closest point (ICP) algorithm that uses both color (if available) as well as geometric information from point cloud and apply a pose graph node optimization to form one single rigid body mesh. 3D-SMG also includes a series of post processing steps to obtain a smooth mesh at the end of the pipeline. The approach allows it to be applied to any articulated object such as a human body or an animal. Our experiments also show high level of accuracy in dimensions of obtained 3D meshes, when compared to the original subject. As the final step towards developing augmented pose dataset, we perform model rigging to articulate the 3D model of the subject and generate dynamic avatars within variety of context-feasible poses. ",
    "code_link": ""
  },
  "cvpr2021_amfg_faceparsingfromrgbanddepthusingcross-domainmutuallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "Face Parsing From RGB and Depth Using Cross-Domain Mutual Learning",
    "authors": [
      "Jihyun Lee",
      "Binod Bhattarai",
      "Tae-Kyun Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Lee_Face_Parsing_From_RGB_and_Depth_Using_Cross-Domain_Mutual_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Lee_Face_Parsing_From_RGB_and_Depth_Using_Cross-Domain_Mutual_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Existing methods of face parsing have proven effective at classifying each pixel of an RGB image into different facial components. However, there is a lack of face parsing research that utilizes depth domain. To the best of our knowledge, we present the first study to exploit 2.5D data for face parsing. We introduce a novel framework to jointly learn (1) RGB face parsing, (2) depth face parsing and (3) RGB-to-depth domain translation, which can be effective even when only a small amount of annotated depth data is available for training. To this end, we also create the first RGB-D face parsing benchmarks based on CelebAMask-HQ, LaPa and Helen by utilizing an off-the-shelf 3D head reconstruction model. Overall, our approach makes two main contributions. First, our method leverages mutual learning between RGB and depth face parsing, which enables bidirectional knowledge distillation between the two data domains. Second, our method utilizes end-to-end learning of RGB-to-depth domain translation and depth face parsing, which can help overcome the scarcity of annotated depth data. We perform extensive experiments to validate the effectiveness of our method, in which we achieve state-of-the-art results in RGB face parsing. As far as we know, we also report the first results on face parsing from depth data. All experiments are conducted on our new RGB-D face parsing datasets, which are publicly available at https://github.com/jyunlee/CelebAMask-HQ-D_LaPa-D_Helen-D. ",
    "code_link": "https://github.com/jyunlee/CelebAMaskHQ-D_LaPa-D_Helen-D"
  },
  "cvpr2021_amfg_asmnetalightweightdeepneuralnetworkforfacealignmentandposeestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "ASMNet: A Lightweight Deep Neural Network for Face Alignment and Pose Estimation",
    "authors": [
      "Ali Pourramezan Fard",
      "Hojjat Abdollahi",
      "Mohammad Mahoor"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Fard_ASMNet_A_Lightweight_Deep_Neural_Network_for_Face_Alignment_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Fard_ASMNet_A_Lightweight_Deep_Neural_Network_for_Face_Alignment_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Active Shape Model (ASM) is a statistical model of object shapes that represents a target structure. ASM can guide machine learning algorithms to fit a set of points representing an object (e.g., face) onto an image. This paper presents a lightweight Convolutional Neural Network (CNN) architecture with a loss function being assisted by ASM for face alignment and estimating head pose in the wild. We use ASM to first guide the network towards learning a smoother distribution of the facial landmark points. Inspired by transfer learning, during the training process, we gradually harden the regression problem and guide the network towards learning the original landmark points distribution. We define multi-tasks in our loss function that are responsible for detecting facial landmark points as well as estimating the face pose. Learning multiple correlated tasks simultaneously builds synergy and improves the performance of individual tasks. We compare the performance of our proposed model called ASMNet with MobileNetV2 (which is about 2 times bigger than ASMNet) in both the face alignment and pose estimation tasks. Experimental results on challenging datasets show that by using the proposed ASM assisted loss function, the ASMNet performance is comparable with MobileNetV2 in the face alignment task. In addition, for face pose estimation, ASMNet performs much better than MobileNetV2. ASMNet achieves an acceptable performance for facial landmark points detection and pose estimation while having a significantly smaller number of parameters and floating-point operations compared to many CNN-based models. ",
    "code_link": ""
  },
  "cvpr2021_amfg_micro-expressionclassificationbasedonlandmarkrelationswithgraphattentionconvolutionalnetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "Micro-Expression Classification Based on Landmark Relations With Graph Attention Convolutional Network",
    "authors": [
      "Ankith Jain Rakesh Kumar",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Kumar_Micro-Expression_Classification_Based_on_Landmark_Relations_With_Graph_Attention_Convolutional_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Kumar_Micro-Expression_Classification_Based_on_Landmark_Relations_With_Graph_Attention_Convolutional_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Facial micro-expressions are brief, rapid, spontaneous gestures of the facial muscles that express an individual's genuine emotions. Because of their short duration and subtlety, detecting and classifying these micro-expressions by humans and machines is difficult. In this paper, a novel approach is proposed that exploits relationships between landmark points and the optical flow patch for the given landmark points. It consists of a two-stream graph attention convolutional network that extracts the relationships between the landmark points and local texture using an optical flow patch. A graph structure is built to draw out temporal information using the triplet of frames. One stream is for node feature location, and the other one is for a patch of optical-flow information. These two streams (node location stream and optical flow stream) are fused for classification. The results are shown on, CASME II and SAMM, publicly available datasets, for three classes and five classes of micro-expressions. The proposed approach outperforms the state-of-the-art methods for 3 and 5 categories of expressions. ",
    "code_link": ""
  },
  "cvpr2021_amfg_eva-gcnheadposeestimationbasedongraphconvolutionalnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "EVA-GCN: Head Pose Estimation Based on Graph Convolutional Networks",
    "authors": [
      "Miao Xin",
      "Shentong Mo",
      "Yuanze Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Head pose estimation is an important task in many real-world applications. Since the facial landmarks usually serve as the common input that is shared by multiple downstream tasks, utilizing landmarks to acquire high-precision head pose estimation is of practical value for many real-world applications. However, existing landmark-based methods have a major drawback in model expressive power, making them hard to achieve comparable performance to the landmark-free methods. In this paper, we propose a strong baseline method which views the head pose estimation as a graph regression problem. We construct a landmark-connection graph, and propose to leverage the Graph Convolutional Networks (GCN) to model the complex nonlinear mappings between the graph typologies and the head pose angles. Specifically, we design a novel GCN architecture which utilizes joint Edge-Vertex Attention (EVA) mechanism to overcome the unstable landmark detection. Moreover, we introduce the Adaptive Channel Attention (ACA) and the Densely-Connected Architecture (DCA) to boost the performance further. We evaluate the proposed method on three challenging benchmark datasets. Experiment results demonstrate that our method achieves better performance in comparison with the state-of-the-art landmark-based and landmark-free methods. ",
    "code_link": ""
  },
  "cvpr2021_amfg_eqfaceasimpleexplicitqualitynetworkforfacerecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "EQFace: A Simple Explicit Quality Network for Face Recognition",
    "authors": [
      "Rushuai Liu",
      "Weijun Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Liu_EQFace_A_Simple_Explicit_Quality_Network_for_Face_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Liu_EQFace_A_Simple_Explicit_Quality_Network_for_Face_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As the deep learning makes big progresses in still-image face recognition, unconstrained video face recognition is still a challenging task due to low quality face images caused by pose, blur, occlusion, illumination etc. In this paper we propose a network for face recognition which gives an explicit and quantitative quality score at the same time when a feature vector is extracted. To our knowledge this is the first network that implements these two functions in one network online. This network is very simple by adding a quality network branch to the baseline network of face recognition. It does not require training datasets with annotated face quality labels. We evaluate this network on both still-image face datasets and video face datasets and achieve the state-of-the-art performance in many cases. We demonstrate three applications of the explicit face quality, one of which is a progressive feature aggregation scheme in online video face recognition. We design an experiment to prove the benefits of using the face quality in this application. ",
    "code_link": ""
  },
  "cvpr2021_amfg_part-awaremeasurementforrobustmulti-viewmulti-human3dposeestimationandtracking": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking",
    "authors": [
      "Hau Chu",
      "Jia-Hong Lee",
      "Yao-Chih Lee",
      "Ching-Hsien Hsu",
      "Jia-Da Li",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Chu_Part-Aware_Measurement_for_Robust_Multi-View_Multi-Human_3D_Pose_Estimation_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Chu_Part-Aware_Measurement_for_Robust_Multi-View_Multi-Human_3D_Pose_Estimation_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces an approach for multi-human 3D pose estimation and tracking based on calibrated multi-view. The main challenge lies in finding the cross-view and temporal correspondences correctly even when several human pose estimations are noisy. Compare to previous solutions that construct 3D poses from multiple views, our approach takes advantage of temporal consistency to match the 2D poses estimated with previously constructed 3D skeletons in every view. Therefore cross-view and temporal associations are accomplished simultaneously. Since the performance suffers from mistaken association and noisy predictions, we design two strategies for aiming better correspondences and 3D reconstruction. Specifically, we propose a part-aware measurement for 2D-3D association and a filter that can cope with 2D outliers during reconstruction. Our approach is efficient and effective comparing to state-of-the-art methods; it achieves competitive results on two benchmarks: 96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus evaluation frames to be more challenging and our proposal also reach well-performed result. ",
    "code_link": ""
  },
  "cvpr2021_amfg_legandisentangledmanipulationofdirectionallightingandfacialexpressionswhilstleveraginghumanperceptualjudgements": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "LEGAN: Disentangled Manipulation of Directional Lighting and Facial Expressions Whilst Leveraging Human Perceptual Judgements",
    "authors": [
      "Sandipan Banerjee",
      "Ajjen Joshi",
      "Prashant Mahajan",
      "Sneha Bhattacharya",
      "Survi Kyal",
      "Taniya Mishra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Banerjee_LEGAN_Disentangled_Manipulation_of_Directional_Lighting_and_Facial_Expressions_Whilst_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Banerjee_LEGAN_Disentangled_Manipulation_of_Directional_Lighting_and_Facial_Expressions_Whilst_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Building facial analysis systems that generalize to extreme variations in lighting and facial expressions is a challenging problem that can potentially be alleviated using natural-looking synthetic data. Towards that, we propose LEGAN, a novel synthesis framework that leverages perceptual quality judgments for jointly manipulating lighting and expressions in face images, without requiring paired training data. LEGAN disentangles the lighting and expression subspaces and performs transformations in the feature space before upscaling to the desired output image. The fidelity of the synthetic image is further refined by integrating a perceptual quality estimation model, trained with face images rendered using multiple synthesis methods and their crowd-sourced naturalness ratings, into the LEGAN framework as an auxiliary discriminator. Using objective metrics like FID and LPIPS, LEGAN is shown to generate higher quality face images when compared with popular GAN models like StarGAN and StarGAN-v2 for lighting and expression synthesis. We also conduct a perceptual study using images synthesized by LEGAN and other GAN models and show the correlation between our quality estimation and visual fidelity. Finally, we demonstrate the effectiveness of LEGAN as training data augmenter for expression recognition and face verification tasks. ",
    "code_link": "https://github.com/Affectiva/LEGAN"
  },
  "cvpr2021_amfg_video-basedpersonre-identificationwithoutbellsandwhistles": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AMFG",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Analysis & Modeling of Faces & Gestures",
    "title": "Video-Based Person Re-Identification Without Bells and Whistles",
    "authors": [
      "Chih-Ting Liu",
      "Jun-Cheng Chen",
      "Chu-Song Chen",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Liu_Video-Based_Person_Re-Identification_Without_Bells_and_Whistles_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Liu_Video-Based_Person_Re-Identification_Without_Bells_and_Whistles_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Video-based person re-identification (Re-ID) aims at matching the video tracklets with cropped video frames for identifying the pedestrians under different cameras. However, there exists severe spatial and temporal misalignment for those cropped tracklets due to the imperfect detection and tracking results generated with obsolete methods. To address this issue, we present a simple re-Detect and Link (DL) module which can effectively reduce those unexpected noise through applying the deep learning-based detection and tracking on the cropped tracklets. Furthermore, we introduce an improved model called Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical Non-local Network, we replace the non-local module with three 1-D position-sensitive axial attentions, in addition to our proposed coarse-to-fine structure. With the developed CF-AAN, compared to the original non-local operation, we can not only significantly reduce the computation cost but also obtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on the large-scale MARS dataset. Meanwhile, by simply adopting our DL module for data alignment, to our surprise, several baseline models can achieve better or comparable results with the current state-of-the-arts. Besides, we discover the errors not only for the identity labels of tracklets but also for the evaluation protocol for the test data of MARS. We hope that our work can help the community for the further development of invariant representation without the hassle of the spatial and temporal alignment and dataset noise. The code, corrected labels, evaluation protocol, and the aligned data will be available at https://github.com/jackie840129/CF-AAN. ",
    "code_link": "https://github.com/jackie840129/CF-AAN"
  },
  "cvpr2021_auvi_lessismorepursuingthevisualturingtestwiththekuleshoveffect": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AUVi",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Affective Understanding in Video",
    "title": "Less Is More: Pursuing the Visual Turing Test With the Kuleshov Effect",
    "authors": [
      "Gustavo Olague",
      "Matthieu Olague",
      "Angel R. Jacobo-Lopez",
      "Gerardo Ibarra-Vazquez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AUVi/html/Olague_Less_Is_More_Pursuing_the_Visual_Turing_Test_With_the_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AUVi/papers/Olague_Less_Is_More_Pursuing_the_Visual_Turing_Test_With_the_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The Turing test centers on the idea that if a computer could trick a human into believing that it was human, then the machine was deemed to be \"intelligent\" or indistinguishable from people. Designing a visual Turing test involves recognizing objects and their relationships on images and creating a method to derive new concepts from the visual information. Until now, the proposed visual tests heavily use natural language processing to conduct the questionnaire or storytelling. We deviate from the mainstream, and we propose to reframe the visual Turing test through the Kuleshov effect to avoid written or spoken language. The idea resides on elucidating a method that creates the concept of montage synthetically. Like the first days of cinema, we would like to convey messages with the interpretation of image shots that a machine could decipher while comparing it with those scored by humans. The first implementation of this new test uses images from a psychology study where the circumplex model is applied to rate each image. We consider five deep learning methodologies and eight optimizers, and through semiotics, we derive an emotional state in the computer. The results are promising since we confirm that this version of the visual Turing test is challenging as a new research avenue. ",
    "code_link": ""
  },
  "cvpr2021_auvi_improvingparkinsondetectionusingdynamicfeaturesfromevokedexpressionsinvideo": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AUVi",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Affective Understanding in Video",
    "title": "Improving Parkinson Detection Using Dynamic Features From Evoked Expressions in Video",
    "authors": [
      "Luis F. Gomez",
      "Aythami Morales",
      "Juan R. Orozco-Arroyave",
      "Roberto Daza",
      "Julian Fierrez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AUVi/html/Gomez_Improving_Parkinson_Detection_Using_Dynamic_Features_From_Evoked_Expressions_in_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AUVi/papers/Gomez_Improving_Parkinson_Detection_Using_Dynamic_Features_From_Evoked_Expressions_in_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Hypomimia, also known as \"facial masking\", is a common symptom of Parkinson's Disease (PD). PD is a neurological disorder characterized by non-motor and motor impairments. Hypomimia is the reduction of facial expressiveness, including the emotion expressions. In this work, we explore the use of static and dynamic features for the analysis of evoked facial gestures in PD patients. The main contributions of this work are: (1) We propose a multimodal PD detection system based on both static and dynamic features obtained from evoked face gestures; (2) we propose a novel set of 17 dynamic features to characterize the facial expressiveness and demonstrate that facial dynamics features can be used to improve PD detection; and (3) we analyze different evoked facial expressions and its performance for PD detection. Different expressions activate different Action Units (AUs) and we analyze to what extent each of these AUs contribute to PD detection. The results show that the use of static features generated by pre-trained deep architectures yield up to 77.36% of accuracy for PD detection and the combination with dynamic features improves PD detection by up to 13.46% (from 75.00% to 88.46%). Our experiments also suggest differences in the performance of evoked face gestures in this PD detection task. ",
    "code_link": ""
  },
  "cvpr2021_auvi_micro-expressionrecognitionbasedonfacialgraphrepresentationlearningandfacialactionunitfusion": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AUVi",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Affective Understanding in Video",
    "title": "Micro-Expression Recognition Based on Facial Graph Representation Learning and Facial Action Unit Fusion",
    "authors": [
      "Ling Lei",
      "Tong Chen",
      "Shigang Li",
      "Jianfeng Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AUVi/html/Lei_Micro-Expression_Recognition_Based_on_Facial_Graph_Representation_Learning_and_Facial_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AUVi/papers/Lei_Micro-Expression_Recognition_Based_on_Facial_Graph_Representation_Learning_and_Facial_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Micro-expressions recognition is a challenge because it involves subtle variations in facial organs. In this paper, first, we propose a novel pipeline to learn a facial graph (nodes and edges) representation to capture these local subtle variations. We express the micro-expressions with multi-patches based on facial landmarks and then stack these patches into channels while using a depthwise convolution (DConv) to learn the features inside the patches, namely, node learning. Then, the encoder of the transformer (ETran) is utilized to learn the relationships between the nodes, namely, edge learning. Based on node and edge learning, a learned facial graph representation is obtained. Second, because the occurrence of an expression is closely bound to action units, we design an AU-GCN to learn the action unit's matrix by embedding and GCN. Finally, we propose a fusion model to introduce the action unit's matrix into the learned facial graph representation. The experiments are comparing with SOTA on various evaluation criteria, including common classifications on CASME II and SAMM datasets, and also conducted following Micro-expression Grand Challenge 2019 protocol. ",
    "code_link": ""
  },
  "cvpr2021_mula_radarcamerafusionviarepresentationlearninginautonomousdriving": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Radar Camera Fusion via Representation Learning in Autonomous Driving",
    "authors": [
      "Xu Dong",
      "Binnan Zhuang",
      "Yunxiang Mao",
      "Langechuan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Dong_Radar_Camera_Fusion_via_Representation_Learning_in_Autonomous_Driving_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dong_Radar_Camera_Fusion_via_Representation_Learning_in_Autonomous_Driving_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Radars and cameras are mature, cost-effective, and robust sensors and have been widely used in the perception stack of mass-produced autonomous driving systems. Due to their complementary properties, outputs from radar detection (radar pins) and camera perception (2D bounding boxes) are usually fused to generate the best perception results. The key to successful radar-camera fusion is the accurate data association. The challenges in the radar-camera association can be attributed to the complexity of driving scenes, the noisy and sparse nature of radar measurements, and the depth ambiguity from 2D bounding boxes. Traditional rule-based association methods are susceptible to performance degradation in challenging scenarios and failure in corner cases. In this study, we propose to address radar-camera association via deep representation learning, to explore feature-level interaction and global reasoning. Additionally, we design a loss sampling mechanism and an innovative ordinal loss to overcome the difficulty of imperfect labeling and to enforce critical human-like reasoning. Despite being trained with noisy labels generated by a rule-based algorithm, our proposed method achieves a performance of 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover, this data-driven method also lends itself to continuous improvement via corner case mining. ",
    "code_link": ""
  },
  "cvpr2021_mula_animprovedattentionforvisualquestionanswering": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "An Improved Attention for Visual Question Answering",
    "authors": [
      "Tanzila Rahman",
      "Shih-Han Chou",
      "Leonid Sigal",
      "Giuseppe Carenini"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Rahman_An_Improved_Attention_for_Visual_Question_Answering_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Rahman_An_Improved_Attention_for_Visual_Question_Answering_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an information vector and an attention gate using attention results and current context; and then adds another attention to generate final attended information by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves better performance than the baseline work. ",
    "code_link": "https://github.com/MILVLG/mcan-vqa"
  },
  "cvpr2021_mula_private-shareddisentangledmultimodalvaeforlearningoflatentrepresentations": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Private-Shared Disentangled Multimodal VAE for Learning of Latent Representations",
    "authors": [
      "Mihee Lee",
      "Vladimir Pavlovic"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Lee_Private-Shared_Disentangled_Multimodal_VAE_for_Learning_of_Latent_Representations_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Lee_Private-Shared_Disentangled_Multimodal_VAE_for_Learning_of_Latent_Representations_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-modal generative models represent an important family of deep models, whose goal is to facilitate representation learning on data with multiple views or modalities. However, current deep multi-modal models focus on the inference of shared representations, while neglecting the important private aspects of data within individual modalities. In this paper, we introduce a disentangled multi-modal variational autoencoder (DMVAE) that utilizes disentangled VAE strategy to separate the private and shared latent spaces of multiple modalities. We demonstrate the utility of DMVAE two image modalities of MNIST and Google Street View House Number (SVHN) datasets as well as image and text modalities from the Oxford-102 Flowers dataset. Our experiments indicate the essence of retaining the private representation as well as the private-shared disentanglement to effectively direct the information across multiple analysis-synthesis conduits. ",
    "code_link": ""
  },
  "cvpr2021_mula_dealingwithmissingmodalitiesinthevisualquestionanswer-differencepredictiontaskthroughknowledgedistillation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Dealing With Missing Modalities in the Visual Question Answer-Difference Prediction Task Through Knowledge Distillation",
    "authors": [
      "Jae Won Cho",
      "Dong-Jin Kim",
      "Jinsoo Choi",
      "Yunjae Jung",
      "In So Kweon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Cho_Dealing_With_Missing_Modalities_in_the_Visual_Question_Answer-Difference_Prediction_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Cho_Dealing_With_Missing_Modalities_in_the_Visual_Question_Answer-Difference_Prediction_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work, we address the issues of missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality-the ground truth answers-that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the \"Big\" Teacher, that takes the image/question/answer triplet as its input and outperforms the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performances of our method and a diverse possibility for future research. ",
    "code_link": ""
  },
  "cvpr2021_mula_self-supervisedfeaturelearningbycross-modalityandcross-viewcorrespondences": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Self-Supervised Feature Learning by Cross-Modality and Cross-View Correspondences",
    "authors": [
      "Longlong Jing",
      "Ling Zhang",
      "Yingli Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Jing_Self-Supervised_Feature_Learning_by_Cross-Modality_and_Cross-View_Correspondences_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Jing_Self-Supervised_Feature_Learning_by_Cross-Modality_and_Cross-View_Correspondences_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The success of supervised learning requires large-scale ground truth labels which are very expensive, time-consuming, or may need special skills to annotate. To address this issue, many self- or un-supervised methods are developed. Unlike most existing self-supervised methods to learn only 2D image features or only 3D point cloud features, this paper presents a novel and effective self-supervised learning approach to jointly learn both 2D image features and 3D point cloud features by exploiting cross-modality and cross-view correspondences without using any human annotated labels. Specifically, 2D image features of rendered images from different views are extracted by a 2D convolutional neural network, and 3D point cloud features are extracted by a graph convolution neural network. Two types of features are fed into a two-layer fully connected neural network to estimate the cross-modality correspondence. The three networks are jointly trained (i.e. cross-modality) by verifying whether two sampled data of different modalities belong to the same object, meanwhile, the 2D convolutional neural network is additionally optimized through minimizing intra-object distance while maximizing inter-object distance of rendered images in different views (i.e. cross-view). The effectiveness of the learned 2D and 3D features is evaluated by transferring them on five different tasks including multi-view 2D shape recognition, 3D shape recognition, multi-view 2D shape retrieval, 3D shape retrieval, and 3D part-segmentation. Extensive evaluations on all the five different tasks across different datasets demonstrate strong generalization and effectiveness of the learned 2D and 3D features by the proposed self-supervised method. ",
    "code_link": ""
  },
  "cvpr2021_mula_target-tailoredsource-transformationforscenegraphgeneration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Target-Tailored Source-Transformation for Scene Graph Generation",
    "authors": [
      "Wentong Liao",
      "Cuiling Lan",
      "Michael Ying Yang",
      "Wenjun Zeng",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Liao_Target-Tailored_Source-Transformation_for_Scene_Graph_Generation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Liao_Target-Tailored_Source-Transformation_for_Scene_Graph_Generation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Scene graph generation aims to provide a semantic and structural description of an image, denoting the objects (with nodes) and their relationships (with edges). The best performing works to date are based on exploiting the context surrounding objects or relations, e.g.no, by passing information among objects. In these approaches, to transform the representation of source objects is a critical process for extracting information for the use by target objects. In this paper, we argue that a source object should give what target object needs and give different objects different information rather than contributing common information to all targets. To achieve this goal, we propose a Target-Tailored Source-Transformation (TTST) method to propagate information among object proposals and relations. Particularly, for a source object proposal which will contribute information to other target objects, we transform the source object feature to the target object feature domain by simultaneously taking both the source and target into account. We further explore more powerful representation by integrating language prior with visual context in the transformation for scene graph generation. By doing so the target object is able to extract target-specific information from source object and source relation accordingly to refine its representation. Our framework is validated on the Visual Genome benchmark and demonstrated its state-of-the-art performance for the scene graph generation. The experimental results show that the performance of object detection and visual relationship detection are promoted mutually by our method. The code will be released upon acceptance. ",
    "code_link": ""
  },
  "cvpr2021_mula_beyondvqageneratingmulti-wordanswersandrationalestovisualquestions": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Beyond VQA: Generating Multi-Word Answers and Rationales to Visual Questions",
    "authors": [
      "Radhika Dua",
      "Sai Srinivas Kancheti",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer. We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test. ",
    "code_link": ""
  },
  "cvpr2021_mula_adaptiveintermediaterepresentationsforvideounderstanding": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Adaptive Intermediate Representations for Video Understanding",
    "authors": [
      "Juhana Kangaspunta",
      "AJ Piergiovanni",
      "Rico Jonschkowski",
      "Michael Ryoo",
      "Anelia Angelova"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Kangaspunta_Adaptive_Intermediate_Representations_for_Video_Understanding_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Kangaspunta_Adaptive_Intermediate_Representations_for_Video_Understanding_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A common strategy to video understanding is to incorporate spatial and motion information by fusing features derived from RGB frames and optical flow. In this work, we introduce a new way to leverage semantic segmentation as an intermediate representation for video understanding and use it in a way that requires no additional labeling. Second, we propose a general framework which learns the intermediate representations (optical flow and semantic segmentation) jointly with the final video understanding task and allows the adaptation of the representations to the end goal. Despite the use of intermediate representations within the network, during inference, no additional data beyond RGB sequences is needed. Finally, we present a way to find the optimal learning configuration by searching the best loss weighting via evolution. We obtain more powerful visual representations for videos which lead to performance gains over the state-of-the-art. ",
    "code_link": ""
  },
  "cvpr2021_mula_exploringthelimitsofzero-shotlearning-howlowcanyougo?": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Exploring the Limits of Zero-Shot Learning - How Low Can You Go?",
    "authors": [
      "Hemanth Dandu",
      "Karan Sharma",
      "Suchendra M. Bhandarkar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Dandu_Exploring_the_Limits_of_Zero-Shot_Learning_-_How_Low_Can_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dandu_Exploring_the_Limits_of_Zero-Shot_Learning_-_How_Low_Can_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Standard zero-shot learning (ZSL) methods use a large number of seen categories to predict very few unseen categories while maintaining unified data splits and evaluation metrics. This has enabled the research community to advance notably towards formulating a standard benchmark ZSL algorithm. However, the most substantial impact of ZSL lies in enabling the prediction of a large number of unseen categories from very few seen categories within a specific domain. This permits the collection and annotation of training data for only a few previously seen categories, thereby significantly mitigating the training data collection and annotation process. We address the difficult problem of predicting a large number of unseen object categories from very few previously seen categories and propose a framework that enables us to examine the limits of inferring several unseen object categories from very few previously seen object categories, i.e., the limits of ZSL. We examine the functional dependence of the classification accuracy of unseen object classes on the number and types of previously seen classes and determine the minimum number and types of previously seen classes required to achieve a prespecified classification accuracy for the unseen classes on three standard ZSL data sets. An experimental comparison of the proposed framework to a prominent ZSL technique on these data sets shows that the proposed framework achieves higher classification accuracy on average while providing valuable insights into the unseen class inference process. ",
    "code_link": ""
  },
  "cvpr2021_mula_progressiveknowledge-embeddedunifiedperceptualparsingforsceneunderstanding": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Progressive Knowledge-Embedded Unified Perceptual Parsing for Scene Understanding",
    "authors": [
      "Wenbo Zheng",
      "Lan Yan",
      "Fei-Yue Wang",
      "Chao Gou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Zheng_Progressive_Knowledge-Embedded_Unified_Perceptual_Parsing_for_Scene_Understanding_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Zheng_Progressive_Knowledge-Embedded_Unified_Perceptual_Parsing_for_Scene_Understanding_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Human can naturally understand scenes in depth with the help of various knowledge accumulated and by a comprehensive visual concept organization including category labels and different-level attributes. This inspires us to unify professional knowledge at different levels with deep neural network architectures progressively for scene understanding. Different from the general embedding approaches, we construct different knowledge graphs for different levels of vision tasks by organizing the rich visual concepts accordingly. We employ a gated graph neural network and relational graph convolutional networks to propagate node messages for different levels of tasks and generate progressively different levels of knowledge representation through the graph. Compared with existing methods, our framework has a main appealing property leading to a novel progressive knowledge-embedded representation learning framework that incorporates different level knowledge graphs into the learning of networks at corresponding level. Extensive experiments on the widely used Broden+ dataset demonstrate the superiority of the proposed framework over other existing state-of-the-art methods. ",
    "code_link": ""
  },
  "cvpr2021_mula_apesaudiovisualpersonsearchinuntrimmedvideo": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "APES: Audiovisual Person Search in Untrimmed Video",
    "authors": [
      "Juan Leon Alcazar",
      "Fabian Caba",
      "Long Mai",
      "Federico Perazzi",
      "Joon-Young Lee",
      "Pablo Arbelaez",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Alcazar_APES_Audiovisual_Person_Search_in_Untrimmed_Video_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Alcazar_APES_Audiovisual_Person_Search_in_Untrimmed_Video_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Humans are arguably one of the most important subjects in video streams, many real-world applications such as video summarization or video editing workflows often require the automatic search and retrieval of a person of interest. Despite tremendous efforts in the person re-identification and retrieval domains, few works have developed audiovisual search strategies. In this paper, we present the Audiovisual Person Search dataset (APES), a new dataset composed of untrimmed videos whose audio (voices) and visual (faces) streams are densely annotated. APES contains over 1.9K identities labeled along 36 hours of video, making it the largest dataset available for untrimmed audiovisual person search. A key property of APES is that it includes dense temporal annotations that link faces to speech segments of the same identity. To showcase the potential of our new dataset, we propose an audiovisual baseline and benchmark for person retrieval. Our study shows that modeling audiovisual cues benefits the recognition of people's identities. ",
    "code_link": "https://github.com/cvdfoundation/ava-dataset"
  },
  "cvpr2021_mula_practicalcross-modalmanifoldalignmentforroboticgroundedlanguagelearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Practical Cross-Modal Manifold Alignment for Robotic Grounded Language Learning",
    "authors": [
      "Andre T. Nguyen",
      "Luke E. Richards",
      "Gaoussou Youssouf Kebe",
      "Edward Raff",
      "Kasra Darvish",
      "Frank Ferraro",
      "Cynthia Matuszek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Nguyen_Practical_Cross-Modal_Manifold_Alignment_for_Robotic_Grounded_Language_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Nguyen_Practical_Cross-Modal_Manifold_Alignment_for_Robotic_Grounded_Language_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a cross-modality manifold alignment procedure that leverages triplet loss to jointly learn consistent, multi-modal embeddings of language-based concepts of real-world items. Our approach learns these embeddings by sampling triples of anchor, positive, and negative data points from RGB-depth images and their natural language descriptions. We show that our approach can benefit from, but does not require, post-processing steps such as Procrustes analysis, in contrast to some of our baselines which require it for reasonable performance. We demonstrate the effectiveness of our approach on two datasets commonly used to develop robotic-based grounded language learning systems, where our approach outperforms four baselines, including a state-of-the-art approach, across five evaluation metrics. ",
    "code_link": ""
  },
  "cvpr2021_mula_cross-modalspeakerverificationandrecognitionamultilingualperspective": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Cross-Modal Speaker Verification and Recognition: A Multilingual Perspective",
    "authors": [
      "Shah Nawaz",
      "Muhammad Saad Saeed",
      "Pietro Morerio",
      "Arif Mahmood",
      "Ignazio Gallo",
      "Muhammad Haroon Yousaf",
      "Alessio Del Bue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Nawaz_Cross-Modal_Speaker_Verification_and_Recognition_A_Multilingual_Perspective_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Nawaz_Cross-Modal_Speaker_Verification_and_Recognition_A_Multilingual_Perspective_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recent years have seen a surge in finding association between faces and voices within a cross-modal biometric application along with speaker recognition. Inspired from this, we introduce a challenging task in establishing association between faces and voices across multiple languages spoken by the same set of persons. The aim of this paper is to answer two closely related questions: \"Is face-voice association language independent?\" and \"Can a speaker be recognized irrespective of the spoken language?\". These two questions are important to understand effectiveness and to boost development of multilingual biometric systems. To answer these, we collected a Multilingual Audio-Visual dataset, containing human speech clips of 154 identities with 3 language annotations extracted from various videos uploaded online. Extensive experiments on the two splits of the proposed dataset have been performed to investigate and answer these novel research questions that clearly point out the relevance of the multilingual problem. ",
    "code_link": ""
  },
  "cvpr2021_mula_usingtexttoteachimageretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Using Text To Teach Image Retrieval",
    "authors": [
      "Haoyu Dong",
      "Ze Wang",
      "Qiang Qiu",
      "Guillermo Sapiro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Dong_Using_Text_To_Teach_Image_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dong_Using_Text_To_Teach_Image_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image retrieval relies heavily on the quality of the data modeling and the distance measurement in the feature space. Building on the concept of image manifold, we first propose to represent the feature space of images, learned via neural networks, as a graph. Neighborhoods in the feature space are now defined by the geodesic distance between images, represented as graph vertices or manifold samples. When limited images are available, this manifold is sparsely sampled, making the geodesic computation and the corresponding retrieval harder. To address this, we augment the manifold samples with geometrically aligned text, thereby using a plethora of sentences to teach us about images. In addition to extensive results on standard datasets illustrating the power of text to help in image retrieval, a new public dataset based on CLEVR is introduced to quantify the semantic similarity between visual data and text data The experimental results show that the joint embedding manifold is a robust representation, allowing it to be a better basis to perform image retrieval given only an image and a textual instruction on the desired modifications over the image. ",
    "code_link": ""
  },
  "cvpr2021_mula_editinglikehumansacontextual,multimodalframeworkforautomatedvideoediting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "Editing Like Humans: A Contextual, Multimodal Framework for Automated Video Editing",
    "authors": [
      "Sharath Koorathota",
      "Patrick Adelman",
      "Kelly Cotton",
      "Paul Sajda"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Koorathota_Editing_Like_Humans_A_Contextual_Multimodal_Framework_for_Automated_Video_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Koorathota_Editing_Like_Humans_A_Contextual_Multimodal_Framework_for_Automated_Video_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose an automated video editing model, which we term contextual and multimodal video editing (CMVE). The model leverages visual and textual metadata describing video clips, integrating essential information from both modalities, and uses a learned editing style from a single example video to coherently combine clips. The editing model is useful for tasks such as generating news clip montages and highlight reels given a text query that describes the video storyline. The model exploits the perceptual similarity between video frames, objects in videos and text descriptions to emulate coherent video editing. Amazon Mechanical Turk participants made judgements comparing CMVE to expert human editing. Experimental results showed no significant difference in the CMVE vs human edited video in terms of matching the text query and the level of interest each generates, suggesting CMVE is able to effectively integrate semantic information across visual and textual modalities and create perceptually coherent quality videos typical of human video editors. We publicly release an online demonstration of our method. ",
    "code_link": ""
  },
  "cvpr2021_mula_3dhandposeestimationviaalignedlatentspaceinjectionandkinematiclosses": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MULA",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Multimodal Learning and Applications",
    "title": "3D Hand Pose Estimation via Aligned Latent Space Injection and Kinematic Losses",
    "authors": [
      "Andreas Stergioulas",
      "Theocharis Chatzis",
      "Dimitrios Konstantinidis",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/html/Stergioulas_3D_Hand_Pose_Estimation_via_Aligned_Latent_Space_Injection_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Stergioulas_3D_Hand_Pose_Estimation_via_Aligned_Latent_Space_Injection_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a novel multi-stage deep learning methodology to accurately tackle the problem of hand pose estimation. More specifically, we initially propose a disentanglement stage to differentiate the significant pose-specific information from the irrelevant background noise and illumination variations of RGB images. Then, we introduce a variational alignment stage to accurately align the latent spaces of the pose-specific and the true hand pose information, effectively improving the discrimination ability of the proposed methodology. Finally, we propose the use of two loss terms to impose physiological and geometrical kinematic constraints to the predicted hand poses, empowering the proposed methodology to avoid non-plausible poses. During all stages, a novel injection decoder is introduced, improving significantly the decoding accuracy of the latent space. Extensive experimentation on two well-known datasets (i.e., RHD and STB) validate the ability of the proposed methodology to achieve accurate hand pose estimation results, overcoming current state-of-the-art methods. ",
    "code_link": ""
  },
  "cvpr2021_civ_grounded,controllableanddebiasedimagecompletionwithlexicalsemantics": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CiV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Causality in Vision",
    "title": "Grounded, Controllable and Debiased Image Completion With Lexical Semantics",
    "authors": [
      "Shengyu Zhang",
      "Tan Jiang",
      "Qinghao Huang",
      "Ziqi Tan",
      "Kun Kuang",
      "Zhou Zhao",
      "Siliang Tang",
      "Jin Yu",
      "Hongxia Yang",
      "Yi Yang",
      "Fei Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/html/Zhang_Grounded_Controllable_and_Debiased_Image_Completion_With_Lexical_Semantics_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/papers/Zhang_Grounded_Controllable_and_Debiased_Image_Completion_With_Lexical_Semantics_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we present an approach, namely Lexical Semantic Image Completion (LSIC), that may have potential applications in art, design, and heritage conservation, among several others. Existing image completion procedure is highly subjective by considering only visual context, which may trigger unpredictable results which are plausible but not faithful to a grounded knowledge. To permit both grounded and controllable completion process, we advocate generating results faithful to both visual and lexical semantic context, i.e., the description of leaving holes or blank regions in the image (e.g., hole description). One major challenge for LSIC comes from modeling and aligning the structure of visual-semantic context and translating across different modalities. We devise multi-grained reasoning blocks to address this challenge. Another challenge relates to the unimodal biases, which occurs when the model generates plausible results without using the textual description. We devise an unsupervised unpaired-creation learning path that explicitly performs counterfactual thinking, i.e., what the complete image would be if given an unpaired text description to the incomplete image. A cycle consistency loss is devised to guarantee counterfactual faithfulness. We conduct extensive quantitative and qualitative experiments that reveal the strengths of LSIC in being grounded, controllable, and debiased. ",
    "code_link": ""
  },
  "cvpr2021_civ_instance-wisecausalfeatureselectionformodelinterpretation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CiV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Causality in Vision",
    "title": "Instance-Wise Causal Feature Selection for Model Interpretation",
    "authors": [
      "Pranoy Panda",
      "Sai Srinivas Kancheti",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/html/Panda_Instance-Wise_Causal_Feature_Selection_for_Model_Interpretation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/papers/Panda_Instance-Wise_Causal_Feature_Selection_for_Model_Interpretation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We formulate a causal extension to the recently introduced paradigm of instance-wise feature selection to explain black-box visual classifiers. Our method selects a subset of input features that has the greatest causal effect on the model's output. We quantify the causal influence of a subset of features by the Relative Entropy Distance measure. Under certain assumptions this is equivalent to the conditional mutual information between the selected subset and the output variable. The resulting causal selections are sparser and cover salient objects in the scene. We show the efficacy of our approach on multiple vision datasets by measuring the post-hoc accuracy and Average Causal Effect of selected features on the model's output. ",
    "code_link": "https://github.com/pranoy-panda/Causal-Feature-Subset-Selection"
  },
  "cvpr2021_civ_shadow-mappingforunsupervisedneuralcausaldiscovery": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CiV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Causality in Vision",
    "title": "Shadow-Mapping for Unsupervised Neural Causal Discovery",
    "authors": [
      "Matthew J. Vowels",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/html/Vowels_Shadow-Mapping_for_Unsupervised_Neural_Causal_Discovery_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/papers/Vowels_Shadow-Mapping_for_Unsupervised_Neural_Causal_Discovery_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " An important goal across most scientific fields is the discovery of causal structures underling a set of observations. Unfortunately, causal discovery methods which are based on correlation or mutual information can often fail to identify causal links in systems which exhibit dynamic relationships. Such dynamic systems (including the famous coupled logistic map) exhibit `mirage' correlations which appear and disappear depending on the observation window. This means not only that correlation is not causation but, perhaps counter-intuitively, that causation may occur without correlation. In this paper we describe Neural Shadow-Mapping, a neural network based method which embeds high-dimensional video data into a low-dimensional shadow representation, for subsequent estimation of causal links. We demonstrate its performance at discovering causal links from video-representations of dynamic systems. ",
    "code_link": ""
  },
  "cvpr2021_civ_devlbertout-of-distributionvisio-linguisticpretrainingwithcausality": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CiV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Causality in Vision",
    "title": "DeVLBert: Out-of-Distribution Visio-Linguistic Pretraining With Causality",
    "authors": [
      "Shengyu Zhang",
      "Tan Jiang",
      "Tan Wang",
      "Kun Kuang",
      "Zhou Zhao",
      "Jianke Zhu",
      "Jin Yu",
      "Hongxia Yang",
      "Fei Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/html/Zhang_DeVLBert_Out-of-Distribution_Visio-Linguistic_Pretraining_With_Causality_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/papers/Zhang_DeVLBert_Out-of-Distribution_Visio-Linguistic_Pretraining_With_Causality_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose to investigate out-of-domain visio-linguistic pretraining, where the pretraining data distribution differs from that of downstream data on which the pretrained model will be fine-tuned. Existing methods for this problem are purely likelihood-based, leading to the spurious correlations and hurt the generalization ability when transferred to out-of-domain downstream tasks. By spurious correlation, we mean that the conditional probability of one token (object or word) given another one can be high (due to the dataset biases) without robust (causal) relationships between them. To mitigate such dataset biases, we propose a Deconfounded Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform intervention-based learning. We borrow the idea of the backdoor adjustment from the research field of causality and propose several neural-network based architectures for Bert-style out-of-domain pretraining. The quantitative results on three downstream tasks, Image Retrieval (IR), Zero-shot IR, and Visual Question Answering, show the effectiveness of DeVLBert by boosting generalization ability. ",
    "code_link": "https://github.com/shengyuzhang/DeVLBert"
  },
  "cvpr2021_civ_learningcontextualcausalitybetweendailyeventsfromtime-consecutiveimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CiV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Causality in Vision",
    "title": "Learning Contextual Causality Between Daily Events From Time-Consecutive Images",
    "authors": [
      "Hongming Zhang",
      "Yintong Huo",
      "Xinran Zhao",
      "Yangqiu Song",
      "Dan Roth"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/html/Zhang_Learning_Contextual_Causality_Between_Daily_Events_From_Time-Consecutive_Images_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CiV/papers/Zhang_Learning_Contextual_Causality_Between_Daily_Events_From_Time-Consecutive_Images_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Conventional textual-based causal knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causal knowledge records (e.g., ConceptNet) typically do not consider the context. In this paper, we move out of the textual domain to explore a more scalable way of acquiring causal knowledge and investigate the possibility of learning contextual causality from the visual signal. Specifically, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representations, it is possible to discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists and considering the contextual property can help better predict the causal relation between events. The Vis-Causal dataset and experiment code are available at https://github.com/HKUST-KnowComp/Vis_Causal. ",
    "code_link": "https://github.com/HKUST-KnowComp/Vis_Causal"
  },
  "cvpr2021_dynavis_dynamicappearancemodellingfromminimalcameras": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DynaVis",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - DynaVis: Dynamic Scene Reconstruction",
    "title": "Dynamic Appearance Modelling From Minimal Cameras",
    "authors": [
      "Lewis Bridgeman",
      "Jean-Yves Guillemaut",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/html/Bridgeman_Dynamic_Appearance_Modelling_From_Minimal_Cameras_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/papers/Bridgeman_Dynamic_Appearance_Modelling_From_Minimal_Cameras_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a novel method for modelling dynamic texture appearance from a minimal set of cameras. Previous methods to capture the dynamic appearance of a human from multi-view video have relied on large, expensive camera setups, and typically store texture on a frame-by-frame basis. We fit a parameterised human body model to multi-view video from minimal cameras (as few as 3), and combine the partial texture observations from multiple viewpoints and frames in a learned framework to generate full-body textures with dynamic details given an input pose. Key to our method are our multi-band loss functions, which apply separate blending functions to the high and low spatial frequencies to reduce texture artefacts. We evaluate our method on a range of multi-view datasets, and show that our model is able to accurately produce full-body dynamic textures, even with only partial camera coverage. We demonstrate that our method outperforms other texture generation methods on minimal camera setups. ",
    "code_link": ""
  },
  "cvpr2021_dynavis_super-resolutionappearancetransferfor4dhumanperformances": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DynaVis",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - DynaVis: Dynamic Scene Reconstruction",
    "title": "Super-Resolution Appearance Transfer for 4D Human Performances",
    "authors": [
      "Marco Pesavento",
      "Marco Volino",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/html/Pesavento_Super-Resolution_Appearance_Transfer_for_4D_Human_Performances_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/papers/Pesavento_Super-Resolution_Appearance_Transfer_for_4D_Human_Performances_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A common problem in the 4D reconstruction of people from multi-view video is the quality of the captured dynamic texture appearance which depends on both the camera resolution and capture volume. Typically the requirement to frame cameras to capture the volume of a dynamic performance (> 50m^3) results in the person occupying only a small proportion < 10% of the field of view. Even with ultra high-definition 4k video acquisition this results in sampling the person at less-than standard definition 0.5k video resolution resulting in low-quality rendering. In this paper we propose a solution to this problem through super-resolution appearance transfer from a static high-resolution appearance capture rig using digital stills cameras (> 8k) to capture the person in a small volume (< 8m^3). A pipeline is proposed for super-resolution appearance transfer from high-resolution static capture to dynamic video performance capture to produce super-resolution dynamic textures. This addresses two key problems: colour mapping between different camera systems; and dynamic texture map super-resolution using a learnt model. Comparative evaluation demonstrates a significant qualitative and quantitative improvement in rendering the 4D performance capture with super-resolution dynamic texture appearance. The proposed approach reproduces the high-resolution detail of the static capture whilst maintaining the appearance dynamics of the captured video. ",
    "code_link": ""
  },
  "cvpr2021_dynavis_consistent3dhumanshapefromrepeatableaction": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DynaVis",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - DynaVis: Dynamic Scene Reconstruction",
    "title": "Consistent 3D Human Shape From Repeatable Action",
    "authors": [
      "Keisuke Shibata",
      "Sangeun Lee",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/html/Shibata_Consistent_3D_Human_Shape_From_Repeatable_Action_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/papers/Shibata_Consistent_3D_Human_Shape_From_Repeatable_Action_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We introduce a novel method for reconstructing the 3D human body from a video of a person in action. Our method recovers a single clothed body model that can explain all frames in the input. Our method builds on two key ideas: exploit the repeatability of human action and use the human body for camera calibration and anchoring. The input is a set of image sequences captured with a single camera at different viewpoints but of different instances of a repeatable action (e.g., batting). Detected 2D joints are used to calibrate the videos in space and time. The sparse viewpoints of the input videos are significantly increased by bone-anchored transformations into rest-pose. These virtually expanded calibrated camera views let us reconstruct surface points and free-form deform a mesh model to extract the frame-consistent personalized clothed body surface. In other words, we show how a casually taken video sequence can be converted into a calibrated dense multiview image set from which the 3D clothed body surface can be geometrically measured. We introduce two new datasets to validate the effectiveness of our method quantitatively and qualitatively and demonstrate free-viewpoint video playback. ",
    "code_link": ""
  },
  "cvpr2021_dynavis_temporalconsistencylossforhighresolutiontexturedandclothed3dhumanreconstructionfrommonocularvideo": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DynaVis",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - DynaVis: Dynamic Scene Reconstruction",
    "title": "Temporal Consistency Loss for High Resolution Textured and Clothed 3D Human Reconstruction From Monocular Video",
    "authors": [
      "Akin Caliskan",
      "Armin Mustafa",
      "Adrian Hilton"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/html/Caliskan_Temporal_Consistency_Loss_for_High_Resolution_Textured_and_Clothed_3D_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DynaVis/papers/Caliskan_Temporal_Consistency_Loss_for_High_Resolution_Textured_and_Clothed_3D_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a novel method to learn temporally consistent 3D reconstruction of clothed people from a monocular video. Recent methods for 3D human reconstruction from monocular video using volumetric, implicit or parametric human shape models, produce per frame reconstructions giving temporally inconsistent output and limited performance when applied to video. In this paper we introduce an approach to learn temporally consistent features for textured reconstruction of clothed 3D human sequences from monocular video by proposing two advances: a novel temporal consistency loss function; and hybrid representation learning for implicit 3D reconstruction from 2D images and coarse 3D geometry. The proposed advances improve the temporal consistency and accuracy of both the 3D reconstruction and texture prediction from a monocular video. Comprehensive comparative performance evaluation on images of people demonstrates that the proposed method significantly outperforms the state-of-the-art learning-based single image 3D human shape estimation approaches achieving significant improvement of reconstruction accuracy, completeness, quality and temporal consistency. ",
    "code_link": ""
  },
  "cvpr2021_isic_d-lemadeeplearningensemblesfrommultipleannotations-applicationtoskinlesionsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Skin Imaging Collaboration on Skin Image Analysis",
    "title": "D-LEMA: Deep Learning Ensembles From Multiple Annotations - Application to Skin Lesion Segmentation",
    "authors": [
      "Zahra Mirikharaji",
      "Kumar Abhishek",
      "Saeed Izadi",
      "Ghassan Hamarneh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Mirikharaji_D-LEMA_Deep_Learning_Ensembles_From_Multiple_Annotations_-_Application_to_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Mirikharaji_D-LEMA_Deep_Learning_Ensembles_From_Multiple_Annotations_-_Application_to_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Medical image segmentation annotations suffer from inter- and intra-observer variations even among experts due to intrinsic differences in human annotators and ambiguous boundaries. Leveraging a collection of annotators' opinions for an image is an interesting way of estimating a gold standard. Although training deep models in a supervised setting with a single annotation per image has been extensively studied, generalizing their training to work with datasets containing multiple annotations per image remains a fairly unexplored problem. In this paper, we propose an approach to handle annotators' disagreements when training a deep model. To this end, we propose an ensemble of Bayesian fully convolutional networks (FCNs) for the segmentation task by considering two major factors in the aggregation of multiple ground truth annotations: (1) handling contradictory annotations in the training data originating from inter-annotator disagreements and (2) improving confidence calibration through the fusion of base models' predictions. We demonstrate the superior performance of our approach on the ISIC Archive and explore the generalization performance of our proposed method by cross-dataset evaluation on the PH^2 and DermoFit datasets. ",
    "code_link": ""
  },
  "cvpr2021_isic_canself-trainingidentifysuspiciousuglyducklinglesions?": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Skin Imaging Collaboration on Skin Image Analysis",
    "title": "Can Self-Training Identify Suspicious Ugly Duckling Lesions?",
    "authors": [
      "Mohammadreza Mohseni",
      "Jordan Yap",
      "William Yolland",
      "Arash Koochek",
      "Stella Atkins"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Mohseni_Can_Self-Training_Identify_Suspicious_Ugly_Duckling_Lesions_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Mohseni_Can_Self-Training_Identify_Suspicious_Ugly_Duckling_Lesions_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " One commonly used clinical approach towards detecting melanomas recognises the existence of Ugly Duckling nevi, or skin lesions which look different from the other lesions on the same patient. An automatic method of detecting and analysing these lesions would help to standardize studies, compared with manual screening methods. However, it is difficult to obtain expertly-labelled images for ugly duckling lesions. We therefore propose to use self-supervised machine learning to automatically detect outlier lesions. We first automatically detect and extract all the lesions from a wide-field skin image, and calculate an embedding for each detected lesion in a patient image, based on automatically identified features. These embeddings are then used to calculate the L2 distances as a way to measure dissimilarity. Using this deep learning method, Ugly Ducklings are identified as outliers which should deserve more attention from the examining physician. We evaluate through comparison with dermatologists, and achieve a sensitivity rate of 72.1% and diagnostic accuracy of 94.2% on the held-out test set. ",
    "code_link": ""
  },
  "cvpr2021_isic_gan-baseddataaugmentationandanonymizationforskin-lesionanalysisacriticalreview": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Skin Imaging Collaboration on Skin Image Analysis",
    "title": "GAN-Based Data Augmentation and Anonymization for Skin-Lesion Analysis: A Critical Review",
    "authors": [
      "Alceu Bissoto",
      "Eduardo Valle",
      "Sandra Avila"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Bissoto_GAN-Based_Data_Augmentation_and_Anonymization_for_Skin-Lesion_Analysis_A_Critical_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Bissoto_GAN-Based_Data_Augmentation_and_Anonymization_for_Skin-Lesion_Analysis_A_Critical_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Despite the growing availability of high-quality public datasets, the lack of training samples is still one of the main challenges of deep-learning for skin lesion analysis. Generative Adversarial Networks (GANs) appear as an enticing alternative to alleviate the issue, by synthesizing samples indistinguishable from real images, with a plethora of works employing them for medical applications. Nevertheless, carefully designed experiments for skin-lesion diagnosis with GAN-based data augmentation show favorable results only on out-of-distribution test sets. For GAN-based data anonymization -- where the synthetic images replace the real ones -- favorable results also only appear for out-of-distribution test sets. Because of the costs and risks associated with GAN usage, those results suggest caution in their adoption for medical applications. ",
    "code_link": ""
  },
  "cvpr2021_isic_conditionaldependencetestsrevealtheusageofabcdrulefeaturesandbiasvariablesinautomaticskinlesionclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Skin Imaging Collaboration on Skin Image Analysis",
    "title": "Conditional Dependence Tests Reveal the Usage of ABCD Rule Features and Bias Variables in Automatic Skin Lesion Classification",
    "authors": [
      "Christian Reimers",
      "Niklas Penzel",
      "Paul Bodesheim",
      "Jakob Runge",
      "Joachim Denzler"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Reimers_Conditional_Dependence_Tests_Reveal_the_Usage_of_ABCD_Rule_Features_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Reimers_Conditional_Dependence_Tests_Reveal_the_Usage_of_ABCD_Rule_Features_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Skin cancer is the most common form of cancer, and melanoma is the leading cause of cancer related deaths. To improve the chances of survival, early detection of melanoma is crucial. Automated systems for classifying skin lesions can assist with initial analysis. However, if we expect people to entrust their well-being to an automatic classification algorithm, it is important to ensure that the algorithm makes medically sound decisions. We investigate this question by testing whether two state-of-the-art models use the features defined in the dermoscopic ABCD rule or whether they rely on biases. We use a method that frames supervised learning as a structural causal model, thus reducing the question whether a feature is used to a conditional dependence test. We show that this conditional dependence method yields meaningful results on data from the ISIC archive. Furthermore, we find that the selected models incorporate asymmetry, border and dermoscopic structures in their decisions but not color. Finally, we show that the same classifiers also use bias features such as the patient's age, skin color or the existence of colorful patches. ",
    "code_link": ""
  },
  "cvpr2021_isic_evaluatingdeepneuralnetworkstrainedonclinicalimagesindermatologywiththefitzpatrick17kdataset": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Skin Imaging Collaboration on Skin Image Analysis",
    "title": "Evaluating Deep Neural Networks Trained on Clinical Images in Dermatology With the Fitzpatrick 17k Dataset",
    "authors": [
      "Matthew Groh",
      "Caleb Harris",
      "Luis Soenksen",
      "Felix Lau",
      "Rachel Han",
      "Aerin Kim",
      "Arash Koochek",
      "Omar Badri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Groh_Evaluating_Deep_Neural_Networks_Trained_on_Clinical_Images_in_Dermatology_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Groh_Evaluating_Deep_Neural_Networks_Trained_on_Clinical_Images_in_Dermatology_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " How does the accuracy of deep neural network models trained to classify clinical images of skin conditions vary across skin color? While recent studies demonstrate computer vision models can serve as a useful decision support tool in healthcare and provide dermatologist-level classification on a number of specific tasks, darker skin is underrepresented in the data. Most publicly available data sets do not include Fitzpatrick skin type labels. We annotate 16,577 clinical images sourced from two dermatology atlases with Fitzpatrick skin type labels and open-source these annotations. Based on these labels, we find that there are significantly more images of light skin types than dark skin types in this dataset. We train a deep neural network model to classify 114 skin conditions and find that the model is most accurate on skin types similar to those it was trained on. In addition, we evaluate how an algorithmic approach to identifying skin tones, individual typology angle, compares with Fitzpatrick skin type labels annotated by a team of human labelers. ",
    "code_link": "https://github.com/mattgroh/fitzpatrick17k"
  },
  "cvpr2021_isic_towardsdomain-specificexplainableaimodelinterpretationofaskinimageclassifierusingahumanapproach": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ISIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Skin Imaging Collaboration on Skin Image Analysis",
    "title": "Towards Domain-Specific Explainable AI: Model Interpretation of a Skin Image Classifier Using a Human Approach",
    "authors": [
      "Fabian Stieler",
      "Fabian Rabe",
      "Bernhard Bauer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Stieler_Towards_Domain-Specific_Explainable_AI_Model_Interpretation_of_a_Skin_Image_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Stieler_Towards_Domain-Specific_Explainable_AI_Model_Interpretation_of_a_Skin_Image_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Machine Learning models have started to outperform medical experts in some classification tasks. Meanwhile, the question of how these classifiers produce certain results is attracting increasing research attention. Current interpretation methods provide a good starting point in investigating such questions, but they still massively lack the relation to the problem domain. In this work, we present how explanations of an AI system for skin image analysis can be made more domain-specific. We apply the synthesis of Local Interpretable Model-agnostic Explanations (LIME) with the ABCD-rule, a diagnostic approach of dermatologists, and present the results using a Deep Neural Network (DNN) based skin image classifier. ",
    "code_link": ""
  },
  "cvpr2021_clic_selftexturetransfernetworksforlowbitrateimagecompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Self Texture Transfer Networks for Low Bitrate Image Compression",
    "authors": [
      "Shoma Iwai",
      "Tomo Miyazaki",
      "Yoshihiro Sugaya",
      "Shinichiro Omachi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Iwai_Self_Texture_Transfer_Networks_for_Low_Bitrate_Image_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Iwai_Self_Texture_Transfer_Networks_for_Low_Bitrate_Image_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Lossy image compression causes a loss of texture, especially at low bitrate. To mitigate this problem, we propose a novel image compression method that utilizes a reference-based image super-resolution model. We use two image compression models and a self texture transfer model. The image compression models encode and decode a whole input image and selected reference patches. The reference patches are small but compressed with high quality. The self texture transfer model transfers the texture of reference patches into similar regions in the compressed image. The experimental results show that our method can reconstruct accurate texture by transferring the texture of reference patches. ",
    "code_link": ""
  },
  "cvpr2021_clic_rate-distortionoptimizedlearning-basedimagecompressionusinganadaptivehierachicalautoencoderwithconditionalhyperprior": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Rate-Distortion Optimized Learning-Based Image Compression Using an Adaptive Hierachical Autoencoder With Conditional Hyperprior",
    "authors": [
      "Fabian Brand",
      "Kristian Fischer",
      "Andre Kaup"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Brand_Rate-Distortion_Optimized_Learning-Based_Image_Compression_Using_an_Adaptive_Hierachical_Autoencoder_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Brand_Rate-Distortion_Optimized_Learning-Based_Image_Compression_Using_an_Adaptive_Hierachical_Autoencoder_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep-learning-based compressive autoencoders consist of a single non-linear function mapping the image to a latent space which is quantized and transmitted. Afterwards, a second non-linear function transforms the received latent space back to a reconstructed image. This method achieves superior quality than many traditional image coders, which is due to a non-linear generalization of linear transforms used in traditional coders. However, modern image and video coder achieve large coding gains by applying rate-distortion optimization on dynamic block-partitioning. In this paper, we present RDONet, a novel approach to achieve similar effects in compression with full image autoencoders by using different hierarchical levels, which are transmitted adaptively after performing an external rate-distortion optimization. Using our model, we are able to save up to 20% rate over comparable non-hierarchical models while maintaining the same quality. ",
    "code_link": ""
  },
  "cvpr2021_clic_perceptualimagecompressionusingrelativisticaverageleastsquaresgans": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Perceptual Image Compression Using Relativistic Average Least Squares GANs",
    "authors": [
      "Zhengxue Cheng",
      "Ting Fu",
      "Jiapeng Hu",
      "Li Guo",
      "Shihao Wang",
      "Xiongxin Zhao",
      "Dajiang Zhou",
      "Yang Song"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Cheng_Perceptual_Image_Compression_Using_Relativistic_Average_Least_Squares_GANs_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Cheng_Perceptual_Image_Compression_Using_Relativistic_Average_Least_Squares_GANs_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work, we provide a detailed description on our submitted methods ANTxNN and ANTxNN_SSIM to Workshop and Challenge on Learned Image Compression (CLIC) 2021. We propose to incorporate Relativistic average Least Squares GANs (RaLSGANs) into Rate-Distortion Optimization for end-to-end training, to achieve perceptual image compression. We also compare two types of discriminator networks and visualize their reconstructed images. Experimental results have validated our method optimized by RaLSGANs can achieve higher subjective quality compared to PSNR, MS-SSIM or LPIPS-optimized models. ",
    "code_link": ""
  },
  "cvpr2021_clic_deeplearningbasedspatial-temporalin-loopfilteringforversatilevideocoding": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Deep Learning Based Spatial-Temporal In-Loop Filtering for Versatile Video Coding",
    "authors": [
      "Chi D. K. Pham",
      "Chen Fu",
      "Jinjia Zhou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Pham_Deep_Learning_Based_Spatial-Temporal_In-Loop_Filtering_for_Versatile_Video_Coding_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Pham_Deep_Learning_Based_Spatial-Temporal_In-Loop_Filtering_for_Versatile_Video_Coding_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The existing deep learning-based Versatile Video Coding (VVC) in-loop filtering (ILF) enhancement works mainly focus on learning the one-to-one mapping between the reconstructed and the original video frame, ignoring the potential resources at encoder and decoder. This work proposes a deep learning-based Spatial-Temporal In-Loop filtering (STILF) that takes advantage of the coding information to improve VVC in-loop filtering. Each CTU is filtered by VVC default in-loop filtering, self-enhancement Convolutional neural network (CNN) with CU map (SEC), and the reference-based enhancement CNN with the optical flow (REO). Bits indicating ILF mode are encoded under CABAC regular mode. Experimental results show that 3.78%, 6.34%, 6%, and 4.64% BD-rate reductions are obtained under All Intra, Low Delay P, Low Delay B, and Random Access configurations, respectively. ",
    "code_link": ""
  },
  "cvpr2021_clic_perceptualfriendlyvariablerateimagecompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Perceptual Friendly Variable Rate Image Compression",
    "authors": [
      "Yixin Gao",
      "Yaojun Wu",
      "Zongyu Guo",
      "Zhizheng Zhang",
      "Zhibo Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Gao_Perceptual_Friendly_Variable_Rate_Image_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Gao_Perceptual_Friendly_Variable_Rate_Image_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we study high fidelity variable rate compression framework. Both conventional and learned codecs in prior works are optimized for objective quality commonly measured by PSNR or SSIM, leaving perceptual quality optimization underexplored. Besides, to circumvent the need of training separate models under different rate conditions, we design a novel coding framework to support variable rate compression. Aside from the variable rate functionality, we propose an adaptive bit allocation unit to strengthen rate-distortion optimization across different rates. Extensive experimental results demonstrate that our proposed approach achieves better subjective quality than methods optimized by the objective metrics such as MSE, and MS-SSIM on CLIC 2021 validation dataset. ",
    "code_link": ""
  },
  "cvpr2021_clic_learnedimagecompressionwithsuper-resolutionresidualmodulesanddistsoptimization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Learned Image Compression With Super-Resolution Residual Modules and DISTS Optimization",
    "authors": [
      "Akifumi Suzuki",
      "Hiroaki Akutsu",
      "Takahiro Naruko",
      "Koki Tsubota",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Suzuki_Learned_Image_Compression_With_Super-Resolution_Residual_Modules_and_DISTS_Optimization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Suzuki_Learned_Image_Compression_With_Super-Resolution_Residual_Modules_and_DISTS_Optimization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural network-based image compressors have the ability to optimize various perceptual image quality metrics. We propose improved methods that is based on selective-detail decoding, which uses two decoders (a main decoder and selective-detail decoder) optimized for different image-quality metrics and applies the output result of a suitable decoder for each part of an image. The following three improvements are obtained with the proposed method. (1) Inspired by the super-resolution task, we add a super-resolution residual module to the main decoder, which is trained to up-sample an image to a resolution beyond the source image, aiming to output a visually clearer image. (2) To improve the perceptual image quality of the main decoder, we use an image quality metric based on Deep Image Structure and Texture Similarity (DISTS), the similarity of which is close to that of human senses with respect to texture. (3) To improve the mask accuracy for decoder selection, cross entropy loss is used for comparing predicted masks and ground truth masks. We also use the weighted mean squared error to improve the visual quality of the text part of an image. ",
    "code_link": ""
  },
  "cvpr2021_clic_end-to-endlearnedimagecompressionwithaugmentednormalizingflows": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "End-to-End Learned Image Compression With Augmented Normalizing Flows",
    "authors": [
      "Yung-Han Ho",
      "Chih-Chun Chan",
      "Wen-Hsiao Peng",
      "Hsueh-Ming Hang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Ho_End-to-End_Learned_Image_Compression_With_Augmented_Normalizing_Flows_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Ho_End-to-End_Learned_Image_Compression_With_Augmented_Normalizing_Flows_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents a new attempt at using augmented normalizing flows (ANF) for lossy image compression. ANF is a specific type of normalizing flow models that augment the input with an independent noise, allowing a smoother transformation from the augmented input space to the latent space. Inspired by the fact that ANF can offer greater expressivity by stacking multiple variational autoencoders (VAE), we generalize the popular VAE-based compression framework by the autoencoding transforms of ANF. When evaluated on Kodak dataset, our ANF-based model provides 3.4% higher BD-rate saving as compared with a VAE-based baseline that implements hyper-prior with mean prediction. Interestingly, it benefits even more from the incorporation of a post-processing network, showing 11.8% rate saving as compared to 6.0% with the baseline plus post-processing. ",
    "code_link": ""
  },
  "cvpr2021_clic_danicedomainadaptationwithoutforgettinginneuralimagecompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "DANICE: Domain Adaptation Without Forgetting in Neural Image Compression",
    "authors": [
      "Sudeep Katakol",
      "Luis Herranz",
      "Fei Yang",
      "Marta Mrak"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Katakol_DANICE_Domain_Adaptation_Without_Forgetting_in_Neural_Image_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Katakol_DANICE_Domain_Adaptation_Without_Forgetting_in_Neural_Image_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural image compression (NIC) is a new coding paradigm where coding capabilities are captured by deep models learned from data. This data-driven nature enables new potential functionalities. In this paper, we study the adaptability of codecs to custom domains of interest. We show that NIC codecs are transferable and that they can be adapted with relatively few target domain images. However, naive adaptation interferes with the solution optimized for the original source domain, resulting in forgetting the original coding capabilities in that domain, and may even break the compatibility with previously encoded bitstreams. Addressing these problems, we propose Codec Adaptation without Forgetting (CAwF), a framework that can avoid these problems by adding a small amount of custom parameters, where the source codec remains embedded and unchanged during adaptation process. Experiments demonstrate its effectiveness and provide useful insights on the characteristics of catastrophic interference in NIC. ",
    "code_link": "https://github.com/tensorflow/compression"
  },
  "cvpr2021_clic_variablerateroiimagecompressionoptimizedforvisualquality": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Variable Rate ROI Image Compression Optimized for Visual Quality",
    "authors": [
      "Yi Ma",
      "Yongqi Zhai",
      "Chunhui Yang",
      "Jiayu Yang",
      "Ruofan Wang",
      "Jing Zhou",
      "Kai Li",
      "Ying Chen",
      "Ronggang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Ma_Variable_Rate_ROI_Image_Compression_Optimized_for_Visual_Quality_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Ma_Variable_Rate_ROI_Image_Compression_Optimized_for_Visual_Quality_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " With the development of compression technology, objective metrics (e.g. PSNR, MS_SSIM) cannot satisfy our need, especially in extreme low bit-rate compression, thus more attention is being paid on perceptual quality. Since people have different standards for objective evaluation. For this reason, we simplify the topic with the consideration that people will strict more on interested region, so a ROI(region of interest) based image compression model is proposed with team name 'Sub201'. For the ROI, we expect its reconstructed part to be more accurate, while the background, server distortion is tolerable, and fake texture can be generated. Firstly, a weighted mask from saliency map is used. Secondly, to balance the difference of ROI and background area, different losses are applied separately. What's more, GAN and LPIPS are utilized to generate more texture in background. At last, variable rate method is adopted to realize rate control, and it performs well with perceptual metric. Experiment shows that our method can achieve better performance both in visual and objective quality. ",
    "code_link": ""
  },
  "cvpr2021_clic_imagecompressionwithrecurrentneuralnetworkandgeneralizeddivisivenormalization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Image Compression With Recurrent Neural Network and Generalized Divisive Normalization",
    "authors": [
      "Khawar Islam",
      "L. Minh Dang",
      "Sujin Lee",
      "Hyeonjoon Moon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Islam_Image_Compression_With_Recurrent_Neural_Network_and_Generalized_Divisive_Normalization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Islam_Image_Compression_With_Recurrent_Neural_Network_and_Generalized_Divisive_Normalization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image compression is a method to remove spatial redundancy between adjacent pixels and reconstruct a high-quality image. In the past few years, deep learning has gained huge attention from the research community and produced promising image reconstruction results. Therefore, recent methods focused on developing deeper and more complex networks, which significantly increased network complexity. In this paper, two effective novel blocks are developed: analysis and synthesis block that employs the convolution layer and Generalized Divisive Normalization (GDN) in the variable-rate encoder and decoder side. Our network utilizes a pixel RNN approach for quantization. Furthermore, to improve the whole network, we encode a residual image using LSTM cells to reduce unnecessary information. Experimental results demonstrated that the proposed variable-rate framework with novel blocks outperforms existing methods and standard image codecs, such as George's and JPEG in terms of image similarity. ",
    "code_link": "https://github.com/khawar512/cvpr"
  },
  "cvpr2021_clic_subjectivequalityoptimizedefficientimagecompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Subjective Quality Optimized Efficient Image Compression",
    "authors": [
      "Xining Wang",
      "Tong Chen",
      "Zhan Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Wang_Subjective_Quality_Optimized_Efficient_Image_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Wang_Subjective_Quality_Optimized_Efficient_Image_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose an efficient image compression framework that is optimized for subjective quality. Our framework is mainly based on the NLAIC (NonLocal Attention Optimized Image Coding) model which applied Variational Autoencoder (VAE) and non-local attention module to end-to-end image compression. This work makes two major contributions to the NLAIC framework. First, our models are optimized for subjective-friendly loss functions rather than conventional MSE (Mean Squared Error) or MS-SSIM (Multiscale Structural Similarity) which was widely used in previous works. Second, we introduce block-based inference mechanism to reduce the running memory consumption of the image compression network, and suggest a partial post-processing step to alleviate block artifacts caused by block-based inference in a lightweight computational fashion. Experiments have proved that the image reconstructed by our method can preserve more texture details than models trained for optimal MSE or MS-SSIM and also present capability for high-throughput decoding. ",
    "code_link": "https://github.com/mseitzer/pytorch-fid"
  },
  "cvpr2021_clic_deepimagecompressionwithlatentoptimizationandpiece-wisequantizationapproximation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Deep Image Compression With Latent Optimization and Piece-Wise Quantization Approximation",
    "authors": [
      "Yuyang Wu",
      "Zhiyang Qi",
      "Huiming Zheng",
      "Lvfang Tao",
      "Wei Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Wu_Deep_Image_Compression_With_Latent_Optimization_and_Piece-Wise_Quantization_Approximation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Wu_Deep_Image_Compression_With_Latent_Optimization_and_Piece-Wise_Quantization_Approximation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Benefit from its capability of learning high-dimensional compact representation from raw data, the auto-encoders are widely used in various tasks of data compression. In particular, for deep image compression, auto-encoders generally take the responsibility of mapping original images to the latent representation to be coded. In this paper, we propose a new framework for deep image compression by devising a loss function for latent optimization, and adopting the differentiable approximation of quantization. In our experiments, both subjective and objective results can confirm the effectiveness of our contributions. ",
    "code_link": ""
  },
  "cvpr2021_clic_multi-metricfusionnetworkforimagequalityassessment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Multi-Metric Fusion Network for Image Quality Assessment",
    "authors": [
      "Yanding Peng",
      "Jiahua Xu",
      "Ziyuan Luo",
      "Wei Zhou",
      "Zhibo Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Peng_Multi-Metric_Fusion_Network_for_Image_Quality_Assessment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Peng_Multi-Metric_Fusion_Network_for_Image_Quality_Assessment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " With the fast proliferation of multimedia applications, the reliable prediction of image/video quality is urgently needed. Many quality assessment metrics have been proposed in the past decades with various complexity and consistency with human beings. The metrics are designed from different aspects, e.g., pixel level fidelity, structural similarity, information theory and data-driven. In this paper,we design a Multi-Metric Fusion Network (MMFN) for aggregating the quality scores predicted by diverse metrics to generate more accurate results. To be specific, we utilize the image features extracted from the pretrained network to adaptively rescale the predicted quality from different metrics, and leverage the fully-connected layers to regress a single scalar as the final score. Pairwise images can be further integrated into the training procedure by adding a Score2Prob layer. Experimental results on the validation set demonstrate that our proposed MMFN achieves better prediction accuracy compared with other metrics. ",
    "code_link": ""
  },
  "cvpr2021_clic_learnedvideocompressionwithintra-guidedenhancementandimplicitmotioninformation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Learned Video Compression With Intra-Guided Enhancement and Implicit Motion Information",
    "authors": [
      "Nannan Zou",
      "Honglei Zhang",
      "Francesco Cricri",
      "Hamed R. Tavakoli",
      "Jani Lainema",
      "Emre Aksu",
      "Miska Hannuksela",
      "Esa Rahtu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Zou_Learned_Video_Compression_With_Intra-Guided_Enhancement_and_Implicit_Motion_Information_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Zou_Learned_Video_Compression_With_Intra-Guided_Enhancement_and_Implicit_Motion_Information_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Although learned approaches to video compression have been proposed with promising results, hand-engineered video codecs are still unbeaten. On the other hand, learned image compression has already surpassed traditional image codecs. In this paper, we propose a learned video compression system that mimics part of the pipeline of traditional codecs while leveraging learned image compression. It comprises two main modules: a learned intra-frame compression module, and a learned inter-frame compression module that is conditioned on intra-coded frames. These modules use separate learned probability models for entropy coding. The intra-frame codec uses a variant of non-local attention layers. Regarding the inter-frame codec, we propose an implicit motion information mechanism, and an enhancement of the inter-frame predictions by leveraging the high quality information of intra-coded frames. On the learned probability model side, we propose to use the reference frames as additional conditioning information. We used this system as our submitted entry for the 2021 Challenge on Learned Image Compression (CLIC). In our experiments, we show the effectiveness of our system and its components via a set of ablation studies. ",
    "code_link": ""
  },
  "cvpr2021_clic_end-to-endoptimizedimagecompressionwithcompetitionofpriordistributions": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "End-to-End Optimized Image Compression With Competition of Prior Distributions",
    "authors": [
      "Benoit Brummer",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Brummer_End-to-End_Optimized_Image_Compression_With_Competition_of_Prior_Distributions_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Brummer_End-to-End_Optimized_Image_Compression_With_Competition_of_Prior_Distributions_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Convolutional autoencoders are now at the forefront of image compression research. To improve their entropy coding, encoder output is typically analyzed with a second autoencoder to generate per-variable parametrized prior probability distributions. We instead propose a compression scheme that uses a single convolutional autoencoder and multiple learned prior distributions working as a competition of experts. Trained prior distributions are stored in a static table of cumulative distribution functions. During inference, this table is used by an entropy coder as a look-up-table to determine the best prior for each spatial location. Our method offers rate-distortion performance comparable to that obtained with a predicted parametrized prior with only a fraction of its entropy coding and decoding complexity. ",
    "code_link": ""
  },
  "cvpr2021_clic_beyondvvctowardsperceptualqualityoptimizedvideocompressionusingmulti-scalehybridapproaches": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "Beyond VVC: Towards Perceptual Quality Optimized Video Compression Using Multi-Scale Hybrid Approaches",
    "authors": [
      "Zhimeng Huang",
      "Kai Lin",
      "Chuanmin Jia",
      "Shanshe Wang",
      "Siwei Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Huang_Beyond_VVC_Towards_Perceptual_Quality_Optimized_Video_Compression_Using_Multi-Scale_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Huang_Beyond_VVC_Towards_Perceptual_Quality_Optimized_Video_Compression_Using_Multi-Scale_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a perceptual quality optimization oriented video compression framework using hybrid approaches. The proposed framework, which is built on the top of recently-published Versatile Video Coding (VVC), contains multi-scale optimized coding techniques. Specifically, three major aspects of efforts from coding unit level to video sequence level have been dedicated to obtain substantial compression efficiency improvement. We first propose a block-level rate-distortion optimization (RDO) method with the consideration of block artifacts removal. Subsequently, we propose frame-level perceptual quality optimized convolutional neural networks for the post-processing of each compressed image, within which the channel attention mechanism has been employed to capture and restore the crucial detail in subjective evaluation. We additionally model the bit allocation as sequence-level dynamic programming problem such that optimal perception and bitrate tradeoff could be obtained. Experimental results show that the proposed method achieves 0.98658 MS-SSIM on the validation set in video track of CLIC-2021. ",
    "code_link": ""
  },
  "cvpr2021_clic_auniversalencoderratedistortionoptimizationframeworkforlearnedcompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLIC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learned Image Compression",
    "title": "A Universal Encoder Rate Distortion Optimization Framework for Learned Compression",
    "authors": [
      "Jing Zhao",
      "Bin Li",
      "Jiahao Li",
      "Ruiqin Xiong",
      "Yan Lu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/html/Zhao_A_Universal_Encoder_Rate_Distortion_Optimization_Framework_for_Learned_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Zhao_A_Universal_Encoder_Rate_Distortion_Optimization_Framework_for_Learned_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Learning-based image compression has drawn increasing attention in recent years. Despite impressive progress has been made, it still lacks a universal encoder optimization method to seek efficient representation for different images. In this paper, we develop a universal rate distortion optimization framework for learning-based compression, which adaptively optimizes latents and side information together for each image. The proposed framework is independent of network architecture and can be flexibly applied to existing and potential future compression networks. Experimental results demonstrate that we can achieve 6.6% bit rate saving against the latest traditional codec, i.e., VVC, yielding the state-of-the -art compression ratio. Moreover, with the proposed optimization framework, we win the first place in CLIC validation phase for all the three different bit rates in terms of PSNR. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_spotthegeosatellitesfromdatasettokelvinsspotgeochallenge": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Spot the GEO Satellites: From Dataset to Kelvins SpotGEO Challenge",
    "authors": [
      "Bo Chen",
      "Daqi Liu",
      "Tat-Jun Chin",
      "Mark Rutten",
      "Dawa Derksen",
      "Marcus Martens",
      "Moritz von Looz",
      "Gurvan Lecuyer",
      "Dario Izzo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Chen_Spot_the_GEO_Satellites_From_Dataset_to_Kelvins_SpotGEO_Challenge_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Chen_Spot_the_GEO_Satellites_From_Dataset_to_Kelvins_SpotGEO_Challenge_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The Geosynchronous Equatorial Orbit (GEO) is home to many important space assets such as telecommunication and navigational satellites. Monitoring Resident Space Objects (RSOs) in GEO is a crucial aspect in achieving Space Situational Awareness (SSA) and in protecting critical space assets. However, ground-based GEO object detection is challenging due to the extreme distance of the targets, as well as nuisance factors including cloud coverage, atmospheric/weather effects, light pollution, sensor noise/defects, and star occlusions. The Kelvins SpotGEO Challenge is designed to establish to what extent images coming from a low-cost ground-based telescope can be used to detect GEO and near-GEO RSOs solely from photometric signals that are without any additional meta-data. At the same time, the SpotGEO dataset also addresses the lack of publicly available datasets from a computer vision perspective on the satellite detection problem; by assembling and releasing such a dataset, we hope to spur more efforts on the optical detection of RSOs and enable objective benchmarking for existing and future methods. In this work, we present details of the SpotGEO dataset development, challenge design, evaluation metric, and result analysis. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_aifordatingstarsabenchmarkingstudyforgyrochronology": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "AI for Dating Stars: A Benchmarking Study for Gyrochronology",
    "authors": [
      "Andres Moya",
      "Jarmi Recio-Martinez",
      "Roberto J. Lopez-Sastre"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Moya_AI_for_Dating_Stars_A_Benchmarking_Study_for_Gyrochronology_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Moya_AI_for_Dating_Stars_A_Benchmarking_Study_for_Gyrochronology_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In astronomy, age is one of the most difficult stellar properties to measure, and gyrochronology is one of the most promising techniques for the task. It consists in dating stars using their rotational period and empirical linear relations with other observed stellar properties, such as stellar effective temperature, parallax, and/or photometric colors in different passbands, for instance. However, these approaches do not allow to reproduce all the observed data, resulting in potential significant deviations in age estimation. In this context, we propose to explore the stellar dating problem using gyrochronology from the AI perspective. Technically, we replace other linear combinations and traditional techniques with a machine learning regression approach. For doing so, we introduce a thorough benchmarking study of state-of-the-art AI regression models trained and tested for stellar dating using gyrochronology. Our experiments reveal promising results, where some models report a mean average error <0.5 Gyr, which can be considered as an outstanding breakthrough in the field. We also release a dataset and propose a set of simple assessment protocols to aid research on AI for dating stars as part of this study. Code and data to reproduce all our results are available at https://github.com/gramuah/ai4datingstars ",
    "code_link": "https://github.com/gramuah/ai4datingstars"
  },
  "cvpr2021_ai4space_spacecrafttime-seriesanomalydetectionusingtransferlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Spacecraft Time-Series Anomaly Detection Using Transfer Learning",
    "authors": [
      "Sriram Baireddy",
      "Sundip R. Desai",
      "James L. Mathieson",
      "Richard H. Foster",
      "Moses W. Chan",
      "Mary L. Comer",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Baireddy_Spacecraft_Time-Series_Anomaly_Detection_Using_Transfer_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Baireddy_Spacecraft_Time-Series_Anomaly_Detection_Using_Transfer_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Anomaly detection in telemetry channels is a high priority for spacecraft, especially when considering the harsh environment of space and the magnitude of launch and operation costs. Traditional spacecraft anomaly detection methods are limited in scope and rely on domain experts to correctly determine abnormal behavior. However, with thousands of distinct telemetry channels being transmitted, the amount of data is difficult to monitor manually. Deep learning models can be used to learn the normal behavior of the telemetry channels and flag or label any deviations. The problem is that we have to train a unique model for each channel to ensure best performance. With the large number of channels to monitor, this may not always be possible. We propose using principles of transfer learning to quickly adapt a general pre-trained model to any specific telemetry channel, greatly reducing the number of unique models needed and reducing the training time for each model. We present the results of our approach on the NASA SMAP/MSL dataset to show that we can achieve performance comparable to state-of-the-art anomaly detection methods. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_spacesegautomateddetectionofbedjunctionmorphologiesindicatingsignsoflifeinediacaranperiod": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "SPACESeg: Automated Detection of Bed Junction Morphologies Indicating Signs of Life in Ediacaran Period",
    "authors": [
      "Padmaja Jonnalagedda",
      "Rachel Surprenant",
      "Mary Droser",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Jonnalagedda_SPACESeg_Automated_Detection_of_Bed_Junction_Morphologies_Indicating_Signs_of_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Jonnalagedda_SPACESeg_Automated_Detection_of_Bed_Junction_Morphologies_Indicating_Signs_of_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " With Perseverance out looking for life on Mars, we identify the need to equip ourselves with automated techniques for remote assessment of geological information. The first step in this translational research is studying early signs of life on Earth. More specifically, we examine the Ediacaran sedimentological record of the Flinders region in Australia, whose unique bed ripple junction morphologies have been determined as the definite indicators of early life on Earth. We propose an automated technique, SPACESeg, that robustly detects the artifact-clouded, miniature ripple structures from cross-sectional views of the Ediacaran rocks. We demonstrate the efficacy of SPACESeg in precisely extracting the desired structures with high accuracy, outperforming many techniques. We also establish the robustness of this technique as it extracts desired biosignatures from drastically varying image conditions, even when the ripples comprise of <1% of the image around significant artifacts. We provide quantitative and qualitative analysis and compare our method against many unsupervised rule-based and supervised deep learning methods, outperforming them all. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_vision-basedneuralscenerepresentationsforspacecraft": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Vision-Based Neural Scene Representations for Spacecraft",
    "authors": [
      "Anne Mergy",
      "Gurvan Lecuyer",
      "Dawa Derksen",
      "Dario Izzo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Mergy_Vision-Based_Neural_Scene_Representations_for_Spacecraft_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Mergy_Vision-Based_Neural_Scene_Representations_for_Spacecraft_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_autonomousplanetarylandingviadeepreinforcementlearningandtransferlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Autonomous Planetary Landing via Deep Reinforcement Learning and Transfer Learning",
    "authors": [
      "Giulia Ciabatti",
      "Shreyansh Daftry",
      "Roberto Capobianco"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Ciabatti_Autonomous_Planetary_Landing_via_Deep_Reinforcement_Learning_and_Transfer_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Ciabatti_Autonomous_Planetary_Landing_via_Deep_Reinforcement_Learning_and_Transfer_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The aim of this work is to develop an application for autonomous landing. We exploit the properties of Deep Reinforcement Learning and Transfer Learning, in order to tackle the problem of planetary landing on unknown or barely-known extra-terrestrial environments by learning good-performing policies, which are transferable from the training environment to other, new environments, without losing optimality. To this end, we model a real-physics simulator, by means of the Bullet/PyBullet library, composed by a lander, defined through the standard ROS/URDF framework and realistic 3D terrain models, for which we adapt official NASA 3D meshes, reconstructed from the data retrieved during missions. Where such model were not available, we reconstruct the terrain from mission imagery - generally SAR imagery. In this setup, we train a Deep Reinforcement Learning model - using DDPG - to autonomous land on the lunar environment. Moreover, we perform transfer learning on the Mars and Titan environment. While still preliminary, our results show that DDPG can learn a good landing policy, which can be transferred to other environments. ",
    "code_link": "https://github.com/nasa/nasa-3d-resources"
  },
  "cvpr2021_ai4space_lspneta2dlocalization-orientedspacecraftposeestimationneuralnetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "LSPnet: A 2D Localization-Oriented Spacecraft Pose Estimation Neural Network",
    "authors": [
      "Albert Garcia",
      "Mohamed Adel Musallam",
      "Vincent Gaudilliere",
      "Enjie Ghorbel",
      "Kassem Al Ismaeil",
      "Marcos Perez",
      "Djamila Aouada"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Garcia_LSPnet_A_2D_Localization-Oriented_Spacecraft_Pose_Estimation_Neural_Network_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Garcia_LSPnet_A_2D_Localization-Oriented_Spacecraft_Pose_Estimation_Neural_Network_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Being capable of estimating the pose of uncooperative objects in space has been proposed as a key asset for enabling safe close-proximity operations such as space rendezvous, in-orbit servicing and active debris removal. Usual approaches for pose estimation involve classical computer vision-based solutions or the application of Deep Learning (DL) techniques. This work explores a novel DL-based methodology, using Convolutional Neural Networks (CNNs), for estimating the pose of uncooperative spacecrafts. Contrary to other approaches, the proposed CNN directly regresses poses without needing any prior 3D information. Moreover, bounding boxes of the spacecraft in the image are predicted in a simple, yet efficient manner. The performed experiments show how this work competes with the state-of-the-art in uncooperative spacecraft pose estimation, including works which require 3D information as well as works which predict bounding boxes through sophisticated CNNs. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_aspacecraftdatasetfordetection,segmentationandpartsrecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "A Spacecraft Dataset for Detection, Segmentation and Parts Recognition",
    "authors": [
      "Hoang Anh Dung",
      "Bo Chen",
      "Tat-Jun Chin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Dung_A_Spacecraft_Dataset_for_Detection_Segmentation_and_Parts_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Dung_A_Spacecraft_Dataset_for_Detection_Segmentation_and_Parts_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Virtually all aspects of modern life depend on space technology. Thanks to the great advancement of computer vision in general and deep learning-based techniques in particular, over the decades, the world witnessed the growing use of deep learning in solving problems for space applications, such as self-driving robot, tracers, insect-like robot on cosmos and health monitoring of spacecraft. These are just some prominent examples that has advanced space industry with the help of deep learning. However, the success of deep learning models requires a lot of training data in order to have decent performance, while on the other hand, there are very limited amount of publicly available space datasets for the training of deep learning models. Currently, there is no public datasets for space-based object detection or instance segmentation, partly because manually annotating object segmentation masks is very time consuming as they require pixel-level labelling, not to mention the challenge of obtaining images from space. In this paper, we aim to fill this gap by releasing a dataset for spacecraft detection, instance segmentation and part recognition. The main contribution of this work is the development of the dataset using images of space stations and satellites, with rich annotations including bounding boxes of spacecrafts and masks to the level of object parts, which are obtained with a mixture of automatic processes and manual efforts. We also provide evaluations with state-of-the-art methods in object detection and instance segmentation as a benchmark for the dataset. The link for downloading the proposed dataset can be found on https://github.com/Yurushia1998/SatelliteDataset. ",
    "code_link": "https://github.com/Yurushia1998/SatelliteDataset"
  },
  "cvpr2021_ai4space_investigatingspikingneuralnetworksforenergy-efficienton-boardaiapplications.acasestudyinlandcoverandlanduseclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Investigating Spiking Neural Networks for Energy-Efficient On-Board AI Applications. A Case Study in Land Cover and Land Use Classification",
    "authors": [
      "Andrzej S. Kucik",
      "Gabriele Meoni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Kucik_Investigating_Spiking_Neural_Networks_for_Energy-Efficient_On-Board_AI_Applications._A_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Kucik_Investigating_Spiking_Neural_Networks_for_Energy-Efficient_On-Board_AI_Applications._A_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Spiking neural networks have been attracting the interest of researchers due to their potential energy efficiency. This feature makes them appealing for applications on board CubeSats or small Earth observation satellites, given their strict energy consumption requirements. However, the performance of spiking neural networks in terms of the accuracy on the space-scene classification datasets, and their benefits with respect to the energy efficiency still remain to be demonstrated. This work is a preliminary investigation on deploying spiking neural networks to land cover and land use classification problems. To train a spiking model, a VGG-16-based artificial neural network has been trained on EuroSAT RGB benchmark dataset. The parameters of this network were then used to initialise a spiking model, which was optimised by finetuning the connection weights and the synaptic filters parameters. By using the mean neuron activations, and the number of time-steps and their width as proxies for the energy consumption of the models, this study shows different tradeoffs between accuracy, latency, and energy efficiency, when comparing a spiking model to the deep learning approach. Moreover, some additional input data preprocessing strategies are investigated as a method of a further enhancement of the energy benefits of the spiking models. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_on-orbitinspectionofanunknown,tumblingtargetusingnasasastrobeeroboticfree-flyers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "On-Orbit Inspection of an Unknown, Tumbling Target Using NASA's Astrobee Robotic Free-Flyers",
    "authors": [
      "Charles Oestreich",
      "Antonio Teran Espinoza",
      "Jessica Todd",
      "Keenan Albee",
      "Richard Linares"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Oestreich_On-Orbit_Inspection_of_an_Unknown_Tumbling_Target_Using_NASAs_Astrobee_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Oestreich_On-Orbit_Inspection_of_an_Unknown_Tumbling_Target_Using_NASAs_Astrobee_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Autonomous spacecraft critically depend on on-orbit inspection (i.e., relative navigation and inertial properties estimation) to intercept tumbling debris objects or defunct satellites. This work presents a practical method for on-orbit inspection and demonstrates its performance in simulation using NASA's Astrobee robotic free-flyers. The problem is formulated as a simultaneous localization and mapping task, utilizing IMU data from an observing \"chaser\" spacecraft and point clouds of the observed \"target\" spacecraft obtained via a 3D time-of-flight camera. The relative navigation between the chaser and target is solved via a factor graph-based approach. The target's principal axes of inertia are then estimated via a conic fit optimization procedure using a polhode analysis. Simulation results indicate the accuracy of the proposed method in preparation for hardware experiments on the International Space Station. ",
    "code_link": "https://github.com/borglab/gtsam"
  },
  "cvpr2021_ai4space_ai4marsadatasetforterrain-awareautonomousdrivingonmars": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "AI4MARS: A Dataset for Terrain-Aware Autonomous Driving on Mars",
    "authors": [
      "R. Michael Swan",
      "Deegan Atha",
      "Henry A. Leopold",
      "Matthew Gildner",
      "Stephanie Oij",
      "Cindy Chiu",
      "Masahiro Ono"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Swan_AI4MARS_A_Dataset_for_Terrain-Aware_Autonomous_Driving_on_Mars_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Swan_AI4MARS_A_Dataset_for_Terrain-Aware_Autonomous_Driving_on_Mars_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep learning has quickly become a necessity for self-driving vehicles on Earth. In contrast, the self-driving vehicles on Mars, including NASA's latest rover, Perseverance, which is planned to land on Mars in February 2021, are still driven by classical machine vision systems. Deep learning capabilities, such as semantic segmentation and object recognition, would substantially benefit the safety and productivity of ongoing and future missions to the red planet. To this end, we created the first large-scale dataset, AI4Mars, for training and validating terrain classification models for Mars, consisting of326K semantic segmentation full image labels on 35K images from Curiosity, Opportunity, and Spirit rovers, collected through crowdsourcing. Each image was labeled by10 people to ensure greater quality and agreement of the crowdsourced labels. It also includes1.5K validation labels annotated by the rover planners and scientists from NASA's MSL (Mars Science Laboratory) mission, which operates the Curiosity rover, and MER (Mars Exploration Rovers) mission, which operated the Spirit and Opportunity rovers. We trained a DeepLabv3 model on the AI4Mars training dataset and achieved over 96% overall classification accuracy on the test set. The dataset is made publicly available. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_improvingastronomyimagequalitythroughreal-timewavefrontestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Improving Astronomy Image Quality Through Real-Time Wavefront Estimation",
    "authors": [
      "David Thomas",
      "Joshua Meyers",
      "Steven M. Kahn"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Thomas_Improving_Astronomy_Image_Quality_Through_Real-Time_Wavefront_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Thomas_Improving_Astronomy_Image_Quality_Through_Real-Time_Wavefront_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a new framework for detecting telescope optics aberrations in real-time. The framework divides the problem into two subproblems that are highly amenable to machine learning and optimization. The first involves making local wavefront estimates with a convolutional neural network. The second involves interpolating the optics wavefront from all the local estimates by minimizing a convex loss function. We test our framework with simulations of the Vera Rubin Observatory. In a realistic mini-survey, the algorithm reduces the total magnitude of the optics wavefront by 66%, the optics PSF FWHM by 27%, and increases the Strehl ratio by a factor of 6. The resulting sharper images have the potential to boost the scientific payload for astrophysics and cosmology. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_amonocularposeestimationcasestudythehayabusa2minerva-ii2deployment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "A Monocular Pose Estimation Case Study: The Hayabusa2 Minerva-II2 Deployment",
    "authors": [
      "Andrew Price",
      "Kazuya Yoshida"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Price_A_Monocular_Pose_Estimation_Case_Study_The_Hayabusa2_Minerva-II2_Deployment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Price_A_Monocular_Pose_Estimation_Case_Study_The_Hayabusa2_Minerva-II2_Deployment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In an environment of increasing orbital debris and remote operation, visual data acquisition methods are becoming a core competency of the next generation of spacecraft. However, deep space missions often generate limited data and noisy images, necessitating complex data analysis methods. Here, a state-of-the-art convolutional neural network (CNN) pose estimation pipeline is applied to the Hayabusa2 Minerva-II2 rover deployment; a challenging case with noisy images and a symmetric target. To enable training of this CNN, a custom dataset is created. The deployment velocity is estimated as 0.1908 m/s using a projective geometry approach and 0.1934 m/s using a CNN landmark detector approach, as compared to the official JAXA estimation of 0.1924 m/s (relative to the spacecraft). Additionally, the attitude estimation results from the real deployment images are shared and the associated tumble estimation is discussed. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_visualslamforasteroidrelativenavigation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Visual SLAM for Asteroid Relative Navigation",
    "authors": [
      "Mehregan Dor",
      "Katherine A. Skinner",
      "Travis Driver",
      "Panagiotis Tsiotras"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Dor_Visual_SLAM_for_Asteroid_Relative_Navigation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Dor_Visual_SLAM_for_Asteroid_Relative_Navigation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper focuses on the application of visual SLAM for the purpose of precise autonomous navigation around an asteroid. We develop a factor graph-based approach allowing for incremental growth and fusion of sensor orientation measurements, Earth-relative inertial position measurements, as well as in-situ monocular camera imagery-based measurements, with an emphasis on the initialization step. Crucially, and in contrast to typical simulated scenarios found in the literature, we validate our approach using real imagery from NASA's DAWN mission to asteroid Vesta, along with navigation comparison data from the NASA NAIF SPICE kernels. Quantitative comparisons show impressive accuracy for a typical target characterization phase segment, both in terms of the estimated trajectory as well as in terms of the tracked estimated landmarks. Based on these results, this paper further supports the viability of autonomous SLAM-based navigation for deep-space asteroid missions. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_event-basedspacecraftlandingusingtime-to-contact": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "Event-Based Spacecraft Landing Using Time-To-Contact",
    "authors": [
      "Olaf Sikorski",
      "Dario Izzo",
      "Gabriele Meoni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Sikorski_Event-Based_Spacecraft_Landing_Using_Time-To-Contact_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Sikorski_Event-Based_Spacecraft_Landing_Using_Time-To-Contact_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We study event-based sensors in the context of spacecraft guidance and control during a descent on Moon-like terrains. For this purpose, we develop a simulator reproducing the event-based camera outputs when exposed to synthetic images of a space environment. We find that it is possible to reconstruct, in this context, the divergence of optical flow vectors (and therefore the time to contact) and use it in a simple control feedback scheme during simulated descents. The results obtained are very encouraging, albeit insufficient to meet the stringent safety constraints and modelling accuracy imposed upon space missions. We thus conclude by discussing future work aimed at addressing these limitations. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_mrscattaspatio-channelattention-guidednetworkformarsroverimageclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "MRSCAtt: A Spatio-Channel Attention-Guided Network for Mars Rover Image Classification",
    "authors": [
      "Anirudh S. Chakravarthy",
      "Roshan Roy",
      "Praveen Ravirathinam"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Chakravarthy_MRSCAtt_A_Spatio-Channel_Attention-Guided_Network_for_Mars_Rover_Image_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Chakravarthy_MRSCAtt_A_Spatio-Channel_Attention-Guided_Network_for_Mars_Rover_Image_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As the exploration of human beings pushes deeper into the galaxy, the classification of images from space and other planets is becoming an increasingly critical task. Image classification on these planetary images can be very challenging due to differences in hue, quality, illumination, and clarity when compared to images captured on Earth. In this work, we try to bridge this gap by developing a deep learning network, MRSCAtt (Mars Rover Spatial and Channel Attention), which jointly uses spatial and channel attention to accurately classify images. We use images taken by NASA's Curiosity rover on Mars as a dataset to show the superiority of our approach by achieving state-of-the-art results with 81.53% test set accuracy on the MSL Surface Dataset, outperforming other methods. To necessitate the use of spatial and channel attention, we perform an ablation study to show the effectiveness of each of the components. We further show robustness of our approach by validating with images taken aboard NASA's recently-landed Perseverance rover. ",
    "code_link": ""
  },
  "cvpr2021_ai4space_fromrockstowallsamodel-freereinforcementlearningapproachtodrystackingwithirregularrocks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AI4Space",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI for Space",
    "title": "From Rocks to Walls: A Model-Free Reinforcement Learning Approach To Dry Stacking With Irregular Rocks",
    "authors": [
      "Andre Menezes",
      "Pedro Vicente",
      "Alexandre Bernardino",
      "Rodrigo Ventura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/html/Menezes_From_Rocks_to_Walls_A_Model-Free_Reinforcement_Learning_Approach_To_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Menezes_From_Rocks_to_Walls_A_Model-Free_Reinforcement_Learning_Approach_To_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In-situ resource utilization is a key aspect for an efficient human exploration of extraterrestrial environments. A cost-effective method for the construction of preliminary structures is dry stacking with locally found unprocessed rocks. This work focus on learning this task from scratch. Former approaches rely on previously acquired models of rocks, which may be hard to obtain in the context of a mission. In alternative, we propose a model-free, data driven approach. We formulate the problem as the task of selecting the position to place each rock on top of the currently built structure. The rocks are presented to the robot in sequence. The goal is to assemble a wall that approximates a target volume, given the 3D perception of the currently built structure, the next object and the target volume. An agent is developed to learn this task using reinforcement learning. The DQN algorithm is used, where the Q-network outputs a value map corresponding to the expected return of placing the object in each position of a top-view depth image. The learned policy outperforms engineered heuristics, both in terms of stability of the structure and similarity with the target volume. Despite the simplification of the task, the policy learned with this approach could be applied to a realistic setting as the high level planner in an autonomous construction pipeline. ",
    "code_link": ""
  },
  "cvpr2021_sketchdl_im2vecsynthesizingvectorgraphicswithoutvectorsupervision": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Sketch-Oriented Deep Learning",
    "title": "Im2Vec: Synthesizing Vector Graphics Without Vector Supervision",
    "authors": [
      "Pradyumna Reddy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/html/Reddy_Im2Vec_Synthesizing_Vector_Graphics_Without_Vector_Supervision_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/papers/Reddy_Im2Vec_Synthesizing_Vector_Graphics_Without_Vector_Supervision_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vector graphics are widely used to represent fonts, logos, digital artworks, and graphic designs. But, while a vast body of work has focused on generative algorithms for raster images, only a handful of options exists for vector graphics. One can always rasterize the input graphic and resort to image-based generative approaches, but this negates the advantages of the vector representation. The current alternative is to use specialized models that require explicit supervision on the vector graphics representation at training time. This is not ideal because large-scale high-quality vector-graphics datasets are difficult to obtain. Furthermore, the vector representation for a given design is not unique, so models that supervise on the vector representation are unnecessarily constrained. Instead, we propose a new neural network that can generate complex vector graphics with varying topologies, and only requires in-direct supervision from readily-available raster training images (i.e., with no vector counterparts). To enable this, we use a differentiable rasterization pipeline that renders the generated vector shapes and composites them together onto a raster canvas. We demonstrate our method on a range of datasets, and provide comparison with state-of-the-art SVG-VAE and DeepSVG, both of which require explicit vector graphics supervision. Finally, we also demonstrate our approach on the MNIST dataset, for which no groundtruth vector representation is available. ",
    "code_link": ""
  },
  "cvpr2021_sketchdl_ontrainingsketchrecognizersfornewdomains": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Sketch-Oriented Deep Learning",
    "title": "On Training Sketch Recognizers for New Domains",
    "authors": [
      "Kemal Tugrul Yesilbek",
      "Metin Sezgin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/html/Yesilbek_On_Training_Sketch_Recognizers_for_New_Domains_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/papers/Yesilbek_On_Training_Sketch_Recognizers_for_New_Domains_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Sketch recognition algorithms are engineered and evaluated using publicly available datasets contributed by the sketch recognition community over the years. While existing datasets contain sketches of a limited set of generic objects, each new domain inevitably requires collecting new data for training domain specific recognizers. This gives rise to two fundamental concerns: First, will the data collection protocol yield ecologically valid data? Second, will the amount of collected data suffice to train sufficiently accurate classifiers? In this paper, we draw attention to these two concerns. We show that the ecological validity of the data collection protocol and the ability to accommodate small datasets are significant factors impacting recognizer accuracy in realistic scenarios. More specifically, using sketch-based gaming as a use case, we show that deep learning methods, as well as more traditional methods, suffer significantly from dataset shift. Furthermore, we demonstrate that in realistic scenarios where data is scarce and expensive, standard measures taken for adapting deep learners to small datasets fall short of comparing favorably with alternatives. Although transfer learning, and extensive data augmentation help deep learners, they still perform significantly worse compared to standard setups (e.g., SVMs and GBMs with standard feature representations). We pose learning from small datasets as a key problem for the deep sketch recognition field, one which has been ignored in the bulk of the existing literature. ",
    "code_link": ""
  },
  "cvpr2021_sketchdl_compactandeffectiverepresentationsforsketch-basedimageretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Sketch-Oriented Deep Learning",
    "title": "Compact and Effective Representations for Sketch-Based Image Retrieval",
    "authors": [
      "Pablo Torres",
      "Jose M. Saavedra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/html/Torres_Compact_and_Effective_Representations_for_Sketch-Based_Image_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/papers/Torres_Compact_and_Effective_Representations_for_Sketch-Based_Image_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Sketch-based image retrieval (SBIR) has undergone an increasing interest in the community of computer vision bringing high impact in real applications. For instance, SBIR brings an increased benefit to eCommerce search engines because it allows users to formulate a query just by drawing what they need to buy. However, current methods showing high precision in retrieval work in a high dimensional space, which negatively affects aspects like memory consumption and time processing. Although some authors have also proposed compact representations, these drastically degrade the performance in a low dimension. Therefore in this work, we present different results of evaluating methods for producing compact embeddings in the context of sketch-based image retrieval. Our main interest is in strategies aiming to keep the local structure of the original space. The recent unsupervised local-topology preserving dimension reduction method UMAP fits our requirements and shows outstanding performance, improving even the precision achieved by SOTA methods. We evaluate six methods in two different datasets. We use Flickr15K and eCommerce datasets; the latter is another contribution of this work. We show that UMAP allows us to have feature vectors of 16 bytes improving precision by more than 35%. ",
    "code_link": ""
  },
  "cvpr2021_sketchdl_sketch-qnetaquadrupletconvnetforcolorsketch-basedimageretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Sketch-Oriented Deep Learning",
    "title": "Sketch-QNet: A Quadruplet ConvNet for Color Sketch-Based Image Retrieval",
    "authors": [
      "Anibal Fuentes",
      "Jose M. Saavedra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/html/Fuentes_Sketch-QNet_A_Quadruplet_ConvNet_for_Color_Sketch-Based_Image_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/papers/Fuentes_Sketch-QNet_A_Quadruplet_ConvNet_for_Color_Sketch-Based_Image_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Architectures based on siamese networks with triplet loss have shown outstanding performance on the image-based similarity search problem. This approach attempts to discriminate between positive (relevant) and negative (irrelevant) items. However, it undergoes a critical weakness. Given a query, it cannot discriminate weakly relevant items, for instance, items of the same type but different color or texture as the given query, which could be a serious limitation for many real-world search applications. Therefore, in this work, we present a quadruplet-based architecture that overcomes the aforementioned weakness. Moreover, we present an instance of this quadruplet network, which we call Sketch-QNet, to deal with the color sketch-based image retrieval (CSBIR) problem, achieving new state-of-the-art results. ",
    "code_link": ""
  },
  "cvpr2021_sketchdl_engineeringsketchgenerationforcomputer-aideddesign": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "SketchDL",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Sketch-Oriented Deep Learning",
    "title": "Engineering Sketch Generation for Computer-Aided Design",
    "authors": [
      "Karl D.D. Willis",
      "Pradeep Kumar Jayaraman",
      "Joseph G. Lambourne",
      "Hang Chu",
      "Yewen Pu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/html/Willis_Engineering_Sketch_Generation_for_Computer-Aided_Design_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/SketchDL/papers/Willis_Engineering_Sketch_Generation_for_Computer-Aided_Design_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Engineering sketches form the 2D basis of parametric Computer-Aided Design (CAD), the foremost modeling paradigm for manufactured objects. In this paper we tackle the problem of learning based engineering sketch generation as a first step towards synthesis and composition of parametric CAD models. We propose two generative models, CurveGen and TurtleGen, for engineering sketch generation. Both models generate curve primitives without the need for a sketch constraint solver and explicitly consider topology for downstream use with constraints and 3D CAD modeling operations. We find in our perceptual evaluation using human subjects that both CurveGen and TurtleGen produce more realistic engineering sketches when compared with the current state-of-the-art for engineering sketch generation. ",
    "code_link": ""
  },
  "cvpr2021_wicv_bgt-netbidirectionalgrutransformernetworkforscenegraphgeneration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation",
    "authors": [
      "Naina Dhingra",
      "Florian Ritter",
      "Andreas Kunz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Dhingra_BGT-Net_Bidirectional_GRU_Transformer_Network_for_Scene_Graph_Generation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Dhingra_BGT-Net_Bidirectional_GRU_Transformer_Network_for_Scene_Graph_Generation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Scene graphs are nodes and edges consisting of objects and object-object relationships, respectively. Scene graph generation (SGG) aims to identify the objects and their relationships. We propose a bidirectional GRU (BiGRU) transformer network (BGT-Net) for the scene graph generation for images. This model implements novel object-object communication to enhance the object information using a BiGRU layer. Thus, the information of all objects in the image is available for the other objects, which can be leveraged later in the object prediction step. This object information is used in a transformer encoder to predict the object class as well as to create object-specific edge information via the use of another transformer encoder. To handle the dataset bias induced by the long-tailed relationship distribution, softening with a log-softmax function and adding a bias adaptation term to regulate the bias for every relation prediction individually showed to be an effective approach. We conducted an elaborate study on experiments and ablations using open-source datasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection datasets, demonstrating the effectiveness of the proposed model over state of the art. ",
    "code_link": ""
  },
  "cvpr2021_wicv_collaborativeimageandobjectlevelfeaturesforimagecolourisation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "Collaborative Image and Object Level Features for Image Colourisation",
    "authors": [
      "Rita Pucci",
      "Christian Micheloni",
      "Niki Martinel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Pucci_Collaborative_Image_and_Object_Level_Features_for_Image_Colourisation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Pucci_Collaborative_Image_and_Object_Level_Features_for_Image_Colourisation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image colourisation is an ill-posed problem, with multiple correct solutions which depend on the context and object instances present in the input datum. Previous approaches attacked the problem either by requiring intense user-interactions or by exploiting the ability of convolutional neural networks (CNNs) in learning image-level(context) features. However, obtaining human hints is not always feasible and CNNs alone are not able to learn entity-level semantics unless multiple models pre-trained with supervision are considered. In this work, we propose a single network, named UCapsNet, that takes into consideration the image-level features obtained through convolutions and entity-level features captured by means of capsules. Then, by skip connections over different layers, we enforce collaboration between such the convolutional and entity factors to produce a high-quality and plausible image colourisation. We pose the problem as a classification task that can be addressed by a fully unsupervised approach, thus requires no human effort. Experimental results on three benchmark datasets show that our approach outperforms existing methods on standard quality metrics and achieves state-of-the-art performances on image colourisation. A large scale user study shows that our method is preferred over existing solutions. ",
    "code_link": ""
  },
  "cvpr2021_wicv_deepdnetdeepdensenetworkfordepthcompletiontask": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "DeepDNet: Deep Dense Network for Depth Completion Task",
    "authors": [
      "Girish Hegde",
      "Tushar Pharale",
      "Soumya Jahagirdar",
      "Vaishakh Nargund",
      "Ramesh Ashok Tabib",
      "Uma Mudenagudi",
      "Basavaraja Vandrotti",
      "Ankit Dhiman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Hegde_DeepDNet_Deep_Dense_Network_for_Depth_Completion_Task_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Hegde_DeepDNet_Deep_Dense_Network_for_Depth_Completion_Task_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a Deep Dense Network for Depth Completion Task (DeepDNet) towards generating dense depth map using sparse depth and captured view. Wide variety of scene understanding applications such as 3D reconstruction, mixed reality, robotics demand accurate and dense depth maps. Existing depth sensors capture accurate and reliable sparse depth and find challenges in acquiring dense depth maps. Towards this we plan to utilise the accurate sparse depth as input with RGB image to generate dense depth. We model the transformation of random sparse input to grid-based sparse input using Quad-tree decomposition. We propose Dense-Residual-Skip (DRS) Autoencoder along with an attention towards edge preservation using Gradient Aware Mean Squared Error (GAMSE) Loss. We demonstrate our results on the NYUv2 dataset and compare it with other state of the art methods. We also show our results on sparse depth captured by ARCore depth API with its dense depth map. Extensive experiments suggest consistent improvements over existing methods. ",
    "code_link": ""
  },
  "cvpr2021_wicv_ontherobustnessofmontecarlodropouttrainedwithnoisylabels": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "On the Robustness of Monte Carlo Dropout Trained With Noisy Labels",
    "authors": [
      "Purvi Goel",
      "Li Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Goel_On_the_Robustness_of_Monte_Carlo_Dropout_Trained_With_Noisy_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Goel_On_the_Robustness_of_Monte_Carlo_Dropout_Trained_With_Noisy_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The memorization effect of deep learning hinders its performance to effectively generalize on test set when learning with noisy labels. Prior study has discovered that epistemic uncertainty techniques are robust when trained with noisy labels compared with neural networks without uncertainty estimation. They obtain prolonged memorization effect and better generalization performance under the adversarial setting of noisy labels. Due to its superior performance amongst other selected epistemic uncertainty methods under noisy labels, we focus on Monte Carlo Dropout (MCDropout) and investigate why it is robust when trained with noisy labels. Through empirical studies on datasets MNIST, CIFAR-10, Animal-10n, we deep dive into three aspects of MCDropout under noisy label setting: 1. efficacy: understanding the learning behavior and test accuracy of MCDropout when training set contains artificially generated or naturally embedded label noise; 2. representation volatility: studying the responsiveness of neurons by examining the mean and standard deviation on each neuron's activation; 3. network sparsity: investigating the network support of MCDropout in comparison with deterministic neural networks. Our findings suggest that MCDropout further sparsifies and regularizes the deterministic neural networks and thus provides higher robustness against noisy labels. ",
    "code_link": ""
  },
  "cvpr2021_wicv_ruigrealisticunderwaterimagegenerationtowardsrestoration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "RUIG: Realistic Underwater Image Generation Towards Restoration",
    "authors": [
      "Chaitra Desai",
      "Ramesh Ashok Tabib",
      "Sai Sudheer Reddy",
      "Ujwala Patil",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Desai_RUIG_Realistic_Underwater_Image_Generation_Towards_Restoration_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Desai_RUIG_Realistic_Underwater_Image_Generation_Towards_Restoration_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we present a novel method for generating synthetic underwater images considering revised image formation model. We propose to use the generated synthetic underwater images to train a conditional generative adversarial network (CGAN) towards restoration of degraded underwater images. Restoration of degraded underwater images using traditional dehazing models is challenging as they are insensitive to wavelength, depth, water type and treat backscattering and direct signal attenuation coefficients to be equal. However, learning based models for restoration perform well but sensitive to availability of ground truth information. Generating ground truth labels in underwater scenario demands in-situ measurements using expensive equipments and is infeasible due to varying underwater currents. Towards this, we propose to generate synthetic underwater images using revised image formation model. Revised image formation model is sensitive to different attenuation coefficients: 1) back scattering, 2) direct scattering and 3) veiling light. We propose to estimate these attenuation coefficients considering proven facts from the literature. We demonstrate restoration of real underwater images through restoration framework trained using rendered synthetic underwater images, and compare results of restoration with state-of-the-art techniques. ",
    "code_link": ""
  },
  "cvpr2021_wicv_pointdccnet3dobjectcategorizationnetworkusingpointclouddecomposition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "PointDCCNet: 3D Object Categorization Network Using Point Cloud Decomposition",
    "authors": [
      "Siddharth Katageri",
      "Sameer Kulmi",
      "Ramesh Ashok Tabib",
      "Uma Mudenagudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Katageri_PointDCCNet_3D_Object_Categorization_Network_Using_Point_Cloud_Decomposition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Katageri_PointDCCNet_3D_Object_Categorization_Network_Using_Point_Cloud_Decomposition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose Point Decomposition Network (PointDCCNet) for 3D object categorization using point cloud decomposition. In the recent technologies for 3D data capture, point clouds have a surge in demand due to their simpler representation and computations. The point cloud analysis requires robust methods for feature extraction to tackle the permutation invariance and unorderdness in point sets and finds application in categorization, refinement, and super-resolution of 3D data. We propose a novel PointDCCNet towards the decomposition of point clouds into primitive geometric shapes, namely plane, sphere, cone and cylinder; and use it as a clue towards modelling a classifier for 3D object categorization. The decomposition of point clouds provides a geometrical signature of the 3D object towards categorization. We show the decomposition of 3D data into primitive shapes which assists the model in the categorization of 3D objects. We demonstrate the results using benchmark datasets and compare them with state-of-the-art techniques. ",
    "code_link": ""
  },
  "cvpr2021_wicv_contrastivedomainadaptation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "Contrastive Domain Adaptation",
    "authors": [
      "Mamatha Thota",
      "Georgios Leontidis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Thota_Contrastive_Domain_Adaptation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Thota_Contrastive_Domain_Adaptation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recently, contrastive self-supervised learning has become a key component for learning visual representations across many computer vision tasks and benchmarks. However, contrastive learning in the context of domain adaptation remains largely underexplored. In this paper, we propose to extend contrastive learning to a new domain adaptation setting, a particular situation occurring where the similarity is learned and deployed on samples following different probability distributions without access to labels. Contrastive learning learns by comparing and contrasting positive and negative pairs of samples in an unsupervised setting without access to source and target labels. We have developed a variation of a recently proposed contrastive learning framework that helps tackle the domain adaptation problem, further identifying and removing possible negatives similar to the anchor to mitigate the effects of false negatives. Extensive experiments demonstrate that the proposed method adapts well, and improves the performance on the downstream domain adaptation task. ",
    "code_link": ""
  },
  "cvpr2021_wicv_cnn-basedmorphologicaldecompositionofx-rayimagesfordetailsanddefectscontrastenhancement": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WiCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Women in Computer Vision",
    "title": "CNN-Based Morphological Decomposition of X-Ray Images for Details and Defects Contrast Enhancement",
    "authors": [
      "Tahani Madmad",
      "Nicolas Delinte",
      "Christophe De Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Madmad_CNN-Based_Morphological_Decomposition_of_X-Ray_Images_for_Details_and_Defects_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Madmad_CNN-Based_Morphological_Decomposition_of_X-Ray_Images_for_Details_and_Defects_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces a new learning based framework for X-ray images that relies on a morphological decomposition of the signal into two main components, separating images into local textures and piecewise smooth (cartoon) parts. The piecewise smooth component corresponds to the spatial variation of the average density of the objects, whereas the local texture component presents the inspected objects singularities. Our method builds on two convolutional neural network (CNN) branches to decompose an input image into its two morphological components. This CNN is trained with synthetic data, generated by randomly picking piecewise smooth and singular patterns in a parametric dictionary and enforcing the sum of the CNN branches to approximate the identity mapping. We demonstrate the relevance of the decomposition by enhancing the local textures component compared to the piecewise smooth one. Those enhanced images compare favorably to the ones obtained with existing works destined to visualize High Dynamic Range (HDR) images such as tone-mapping algorithms. ",
    "code_link": "https://github.com/tahanimadmad/CNN-Based-X-rayMorphological-Decomposition"
  },
  "cvpr2021_precognition_panopticsegmentationforecasting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Precognition: Seeing Through the Future",
    "title": "Panoptic Segmentation Forecasting",
    "authors": [
      "Colin Graber",
      "Grace Tsai",
      "Michael Firman",
      "Gabriel Brostow",
      "Alexander Schwing"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Graber_Panoptic_Segmentation_Forecasting_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/papers/Graber_Panoptic_Segmentation_Forecasting_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Our goal is to forecast the near future given a set of recent observations. We think this ability to forecast, i.e., to anticipate, is integral for the success of autonomous agents which need not only passively analyze an observation but also must react to it in real-time. Importantly, accurate forecasting hinges upon the chosen scene decomposition. We think that superior forecasting can be achieved by decomposing a dynamic scene into individual 'things' and background 'stuff'. Background 'stuff' largely moves because of camera motion, while foreground 'things' move because of both camera and individual object motion. Following this decomposition, we introduce panoptic segmentation forecasting. Panoptic segmentation forecasting opens up a middle-ground between existing extremes, which either forecast instance trajectories or predict the appearance of future image frames. To address this task we develop a two-component model: one component learns the dynamics of the background stuff by anticipating odometry, the other one anticipates the dynamics of detected things. We establish a leaderboard for this novel task, and validate a state-of-the-art model that outperforms available baselines ",
    "code_link": ""
  },
  "cvpr2021_precognition_end-to-endinteractivepredictionandplanningwithopticalflowdistillationforautonomousdriving": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Precognition: Seeing Through the Future",
    "title": "End-to-End Interactive Prediction and Planning With Optical Flow Distillation for Autonomous Driving",
    "authors": [
      "Hengli Wang",
      "Peide Cai",
      "Rui Fan",
      "Yuxiang Sun",
      "Ming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Wang_End-to-End_Interactive_Prediction_and_Planning_With_Optical_Flow_Distillation_for_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/papers/Wang_End-to-End_Interactive_Prediction_and_Planning_With_Optical_Flow_Distillation_for_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " With the recent advancement of deep learning technology, data-driven approaches for autonomous car prediction and planning have achieved extraordinary performance. Nevertheless, most of these approaches follow a non-interactive prediction and planning paradigm, hypothesizing that a vehicle's behaviors do not affect others. The approaches based on such a non-interactive philosophy typically perform acceptably in sparse traffic scenarios but can easily fail in dense traffic scenarios. Therefore, we propose an end-to-end interactive neural motion planner (INMP) for autonomous driving in this paper. Given a set of past surrounding-view images and a high definition map, our INMP first generates a feature map in bird's-eye-view space, which is then processed to detect other agents and perform interactive prediction and planning jointly. Also, we adopt an optical flow distillation paradigm, which can effectively improve the network performance while still maintaining its real-time inference speed. Extensive experiments on the nuScenes dataset and in the closed-loop Carla simulation environment demonstrate the effectiveness and efficiency of our INMP for the detection, prediction, and planning tasks. Our project page is at sites.google.com/view/inmp-ofd. ",
    "code_link": ""
  },
  "cvpr2021_precognition_glaucomaprecognitionbasedonconfocalscanninglaserophthalmoscopyimagesoftheopticdiscusingconvolutionalneuralnetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Precognition: Seeing Through the Future",
    "title": "Glaucoma Precognition Based on Confocal Scanning Laser Ophthalmoscopy Images of the Optic Disc Using Convolutional Neural Network",
    "authors": [
      "Krati Gupta",
      "Michael Goldbaum",
      "Siamak Yousefi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Gupta_Glaucoma_Precognition_Based_on_Confocal_Scanning_Laser_Ophthalmoscopy_Images_of_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/papers/Gupta_Glaucoma_Precognition_Based_on_Confocal_Scanning_Laser_Ophthalmoscopy_Images_of_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We develop an Artificial Intelligence (AI) framework for glaucoma precognition from baseline confocal scanning laser ophthalmoscopy imaging data, using a convolutional neural network (CNN) model. The proposed framework extracts 'deep features' from convolutional layers of the CNN model, which are used as input to the ensemble learning classifier in order to identify patients that will likely convert to glaucoma after few years. The prediction model achieved area under the receiver operating characteristic curve (AUC) of 0.83 using the data from baseline visit. The model predicted the onset of glaucoma more accurately than known glaucoma risk factors, Glaucoma Probability Score (GPS) and Moorfields Regression Analysis (MRA) parameters of the Heidelberg Retinal Tomograph (HRT) software. The proposed AI construct provides a highly specific and sensitive model that can predict the onset of glaucoma from baseline HRT parameters and has the potential to provide clinicians valuable information regarding the onset of glaucoma. ",
    "code_link": ""
  },
  "cvpr2021_precognition_multi-modaltemporalconvolutionalnetworkforanticipatingactionsinegocentricvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Precognition: Seeing Through the Future",
    "title": "Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos",
    "authors": [
      "Olga Zatsarynna",
      "Yazan Abu Farha",
      "Juergen Gall"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Zatsarynna_Multi-Modal_Temporal_Convolutional_Network_for_Anticipating_Actions_in_Egocentric_Videos_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/papers/Zatsarynna_Multi-Modal_Temporal_Convolutional_Network_for_Anticipating_Actions_in_Egocentric_Videos_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Anticipating human actions is an important task that needs to be addressed for the development of reliable intelligent agents, such as self-driving cars or robot assistants. While the ability to make future predictions with high accuracy is crucial for designing the anticipation approaches, the speed at which the inference is performed is not less important. Methods that are accurate but not sufficiently fast would introduce a high latency into the decision process. Thus, this will increase the reaction time of the underlying system. This poses a problem for domains such as autonomous driving, where the reaction time is crucial. In this work, we propose a simple and effective multi-modal architecture based on temporal convolutions. Our approach stacks a hierarchy of temporal convolutional layers and does not rely on recurrent layers to ensure a fast prediction. We further introduce a multi-modal fusion mechanism that captures the pairwise interactions between RGB, flow, and object modalities. Results on two large-scale datasets of egocentric videos, EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves comparable performance to the state-of-the-art approaches while being significantly faster. ",
    "code_link": ""
  },
  "cvpr2021_precognition_long-termheadposeforecastingconditionedonthegaze-guidingprior": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Precognition: Seeing Through the Future",
    "title": "Long-Term Head Pose Forecasting Conditioned on the Gaze-Guiding Prior",
    "authors": [
      "Shentong Mo",
      "Miao Xin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Mo_Long-Term_Head_Pose_Forecasting_Conditioned_on_the_Gaze-Guiding_Prior_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/papers/Mo_Long-Term_Head_Pose_Forecasting_Conditioned_on_the_Gaze-Guiding_Prior_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Forecasting head pose future states is a novel task in computer vision. Since future may have many possibilities, and the logical results are much more important than the impractical ones, the forecasting results for most of the scenarios should be not only diverse but also logically realistic. These requirements pose a real challenge to the current methods, which motivates us to seek for better head pose representation and methods to restrict the forecasting reasonably. In this paper, we adopt a spatial-temporal graph to model the interdependencies between the distribution of landmarks and head pose angles. Furthermore, we propose the conditional spatial-temporal variational graph autoencoder (CST-VGAE), a deep conditional generative model for learning restricted one-to-many mappings conditioned on the spatial-temporal graph input. Specifically, we improve the proposed CST-VGAE for the long-term head pose forecasting task in terms of several aspects. First, we introduce a gaze-guiding prior based on the physiology. Then we apply a temporal self-attention and self-supervised learning mechanism to learn the long-range dependencies on the gaze prior. To better model head poses structurally, we introduce a Gaussian Mixture Model (GMM), instead of a fixed Gaussian in the encoded latent space. Experiments demonstrate the effectiveness of the proposed method for the long-term head pose forecasting task. We achieve superior forecasting performance on the benchmark datasets compared to the existing methods. ",
    "code_link": ""
  },
  "cvpr2021_precognition_spatio-temporalpredictivenetworkforvideoswithphysicalproperties": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "Precognition",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Precognition: Seeing Through the Future",
    "title": "Spatio-Temporal Predictive Network for Videos With Physical Properties",
    "authors": [
      "Yuka Aoyagi",
      "Noboru Murata",
      "Hidetomo Sakaino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/html/Aoyagi_Spatio-Temporal_Predictive_Network_for_Videos_With_Physical_Properties_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/Precognition/papers/Aoyagi_Spatio-Temporal_Predictive_Network_for_Videos_With_Physical_Properties_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a spatio-temporal predictive network with attention weighting of multiple physical Deep Learning (DL) models for videos with various physical properties. Previous approaches have been models with multiple branches for difference properties in videos, but the outputs of branches have been simply summed even with properties that change in time and space. In addition, it is difficult to train previous models for sufficient representations of physical properties in videos. Therefore, we propose the design of the spatio-temporal prediction network and the training method for videos with multiple physical properties, motivated by the Mixtures of Experts framework. Multiple spatio-temporal DL branches/experts for multiple physical properties and pixel-wise and expert-wise attention mechanism for adaptively integrating outputs of experts, i.e., Spatial-Temporal Gating Networks (STGNs) are proposed. Experts are trained with a vast amount of synthetic image sequences by physical equations and noise models. Instead, the whole network including STGNs is allowed to be trained only with a limited number of real datasets. Experiments on various videos, i.e., traffic, pedestrian, Dynamic Texture videos, and radar images, show the superiority of our proposed approach compared with previous approaches. ",
    "code_link": ""
  },
  "cvpr2021_rcv_mtunetfew-shotimageclassificationwithvisualexplanations": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "MTUNet: Few-Shot Image Classification With Visual Explanations",
    "authors": [
      "Bowen Wang",
      "Liangzhi Li",
      "Manisha Verma",
      "Yuta Nakashima",
      "Ryo Kawasaki",
      "Hajime Nagahara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Wang_MTUNet_Few-Shot_Image_Classification_With_Visual_Explanations_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Wang_MTUNet_Few-Shot_Image_Classification_With_Visual_Explanations_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Few-shot learning (FSL) approaches, mostly neural network-based, are assuming that the pre-trained knowledge can be obtained from base (seen) categories and transferred to novel (unseen) categories. However, the black-box nature of neural networks makes it difficult to understand what is actually transferred, which may hamper its application in some risk-sensitive areas. In this paper, we reveal a new way to perform explainable FSL for image classification, using discriminative patterns and pairwise matching. Experimental results prove that the proposed method can achieve satisfactory explainability on two mainstream datasets. Code is available at https://github.com/wbw520/MTUNet. ",
    "code_link": "https://github.com/wbw520/MTUNet"
  },
  "cvpr2021_rcv_inaccuracyofstate-actionvaluefunctionfornon-optimalactionsinadversariallytraineddeepneuralpolicies": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "Inaccuracy of State-Action Value Function for Non-Optimal Actions in Adversarially Trained Deep Neural Policies",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Korkmaz_Inaccuracy_of_State-Action_Value_Function_for_Non-Optimal_Actions_in_Adversarially_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Korkmaz_Inaccuracy_of_State-Action_Value_Function_for_Non-Optimal_Actions_in_Adversarially_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The introduction of deep neural networks as function approximator for the state-action value function has led to the creation of a new research area for self-learning systems that explore policies from high dimensional input. While the success of deep neural policies has resulted in the deployment of these policies in diversified application domains, there are significant concerns regarding their robustness towards specifically crafted malicious perturbations introduced to their inputs. Several studies have focused on making deep neural policies resistant to such perturbations via training with the existence of these perturbations (i.e. adversarial training). In this paper we focus on conducting an investigation on the state-action value function learned by state-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies. We perform several experiments in the OpenAI Baselines and we show that the state-action value functions learned by vanilla trained deep neural policies have better estimates for the non-optimal actions than the state-of-the-art adversarially trained deep neural policies. We believe our study lays out intriguing properties of adversarial training and could be critical step towards obtaining robust and reliable policies. ",
    "code_link": ""
  },
  "cvpr2021_rcv_tiltedcross-entropy(tce)promotingfairnessinsemanticsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "Tilted Cross-Entropy (TCE): Promoting Fairness in Semantic Segmentation",
    "authors": [
      "Attila Szabo",
      "Hadi Jamali-Rad",
      "Siva-Datta Mannava"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Szabo_Tilted_Cross-Entropy_TCE_Promoting_Fairness_in_Semantic_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Szabo_Tilted_Cross-Entropy_TCE_Promoting_Fairness_in_Semantic_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Traditional empirical risk minimization (ERM) for semantic segmentation can disproportionately advantage or disadvantage certain target classes in favor of an (unfair but) improved overall performance. Inspired by the recently introduced tilted ERM (TERM), we propose tilted cross-entropy (TCE) loss and adapt it to the semantic segmentation setting to minimize performance disparity among target classes and promote fairness. Through quantitative and qualitative performance analyses, we demonstrate that the proposed Stochastic TCE for semantic segmentation can offer improved overall fairness by efficiently minimizing the performance disparity among the target classes of Cityscapes. ",
    "code_link": ""
  },
  "cvpr2021_rcv_revisitingtheevaluationofclassactivationmappingforexplainabilityanovelmetricandexperimentalanalysis": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "Revisiting the Evaluation of Class Activation Mapping for Explainability: A Novel Metric and Experimental Analysis",
    "authors": [
      "Samuele Poppi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Poppi_Revisiting_the_Evaluation_of_Class_Activation_Mapping_for_Explainability_A_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Poppi_Revisiting_the_Evaluation_of_Class_Activation_Mapping_for_Explainability_A_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As the request for deep learning solutions increases, the need for explainability is even more fundamental. In this setting, particular attention has been given to visualization techniques, that try to attribute the right relevance to each input pixel with respect to the output of the network. In this paper, we focus on Class Activation Mapping (CAM) approaches, which provide an effective visualization by taking weighted averages of the activation maps. To enhance the evaluation and the reproducibility of such approaches, we propose a novel set of metrics to quantify explanation maps, which show better effectiveness and simplify comparisons between approaches. To evaluate the appropriateness of the proposal, we compare different CAM-based visualization methods on the entire ImageNet validation set, fostering proper comparisons and reproducibility. ",
    "code_link": ""
  },
  "cvpr2021_rcv_casualconversationsadatasetformeasuringfairnessinai": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "Casual Conversations: A Dataset for Measuring Fairness in AI",
    "authors": [
      "Caner Hazirbas",
      "Joanna Bitton",
      "Brian Dolhansky",
      "Jacqueline Pan",
      "Albert Gordo",
      "Cristian Canton Ferrer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Hazirbas_Casual_Conversations_A_Dataset_for_Measuring_Fairness_in_AI_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Hazirbas_Casual_Conversations_A_Dataset_for_Measuring_Fairness_in_AI_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces a novel fairness dataset to measure the robustness of AI models to a diverse set of age, genders, apparent skin tones and ambient lighting conditions. Our dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple U.S. states with a diverse set of adults in various age, gender and apparent skin tone groups. A key feature is that each subject agreed to participate for their likenesses to be used. Additionally, our age and gender annotations are provided by the subjects themselves. A group of trained annotators labeled the subjects' apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations for videos recorded in low ambient lighting are also provided. As an application to measure robustness of predictions across certain attributes, we evaluate the state-of-the-art apparent age and gender classification methods. Our experiments provides a through analysis on these models in terms of fair treatment of people from various backgrounds. ",
    "code_link": ""
  },
  "cvpr2021_rcv_arealluserstreatedfairlyinfederatedlearningsystems?": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "Are All Users Treated Fairly in Federated Learning Systems?",
    "authors": [
      "Umberto Michieli",
      "Mete Ozay"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Michieli_Are_All_Users_Treated_Fairly_in_Federated_Learning_Systems_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Michieli_Are_All_Users_Treated_Fairly_in_Federated_Learning_Systems_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Federated Learning (FL) systems target distributed model training on decentralized and private local training data belonging to users. Most of the existing methods aggregate models prioritizing among them proportionally to the frequency of local samples. However, this leads to unfair aggregation with respect to users. Indeed, users with few local samples are considered less during aggregation and struggle to offer a real contribution to federated optimization of the models. In real-world settings, statistical heterogeneity (e.g., highly imbalanced and non-i.i.d. data) is diffused and can seriously harm model training. To this end, we empirically analyze the relationship between fairness of aggregation of user models, accuracy of aggregated models and convergence rate of FL methods. We compare a standard federated model aggregation and optimization method, FedAvg, against a fair (uniform) aggregation scheme, i.e., FairAvg on benchmark datasets. Experimental analyses show that fair model aggregation can be beneficial in terms of accuracy and convergence rate, whilst reducing at the same time fluctuations of accuracy of the aggregate model when clients observe non-i.i.d. data. ",
    "code_link": ""
  },
  "cvpr2021_rcv_reconsideringco2emissionsfromcomputervision": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "RCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Responsible Computer Vision",
    "title": "Reconsidering CO2 Emissions From Computer Vision",
    "authors": [
      "Andre Fu",
      "Mahdi S. Hosseini",
      "Konstantinos N. Plataniotis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Fu_Reconsidering_CO2_Emissions_From_Computer_Vision_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Fu_Reconsidering_CO2_Emissions_From_Computer_Vision_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Climate change is a pressing issue that is currently affecting and will affect every part of our lives. It's becoming incredibly vital we, as a society, address the climate crisis as a universal effort, including those in the Computer Vision (CV) community. In this work, we analyze the total cost of CO2 emissions by breaking it into (1) the architecture creation cost and (2) the life-time evaluation cost. We show that over time, these costs are non-negligible and are having a direct impact on our future. Importantly, we conduct an ethical analysis of how the CV-community is unintentionally overlooking its own ethical AI principles by emitting this level of CO2. To address these concerns, we propose adding \"enforcement\" as a pillar of ethical AI and provide some recommendations for how architecture designers and broader CV community can curb the climate crisis. ",
    "code_link": ""
  },
  "cvpr2021_mai_fastandaccuratecamerascenedetectiononsmartphones": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Fast and Accurate Camera Scene Detection on Smartphones",
    "authors": [
      "Angeline Pouget",
      "Sidharth Ramesh",
      "Maximilian Giang",
      "Ramithan Chandrapalan",
      "Toni Tanner",
      "Moritz Prussing",
      "Radu Timofte",
      "Andrey Ignatov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Pouget_Fast_and_Accurate_Camera_Scene_Detection_on_Smartphones_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Pouget_Fast_and_Accurate_Camera_Scene_Detection_on_Smartphones_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone, though the problem of accurate scene prediction has not yet been addressed by the research community. This paper for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD) containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper are available on the project website. ",
    "code_link": ""
  },
  "cvpr2021_mai_fastandaccuratesingle-imagedepthestimationonmobiledevices,mobileai2021challengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Fast and Accurate Single-Image Depth Estimation on Mobile Devices, Mobile AI 2021 Challenge: Report",
    "authors": [
      "Andrey Ignatov",
      "Grigory Malivenko",
      "David Plowman",
      "Samarth Shukla",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Fast_and_Accurate_Single-Image_Depth_Estimation_on_Mobile_Devices_Mobile_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_and_Accurate_Single-Image_Depth_Estimation_on_Mobile_Devices_Mobile_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Depth estimation is an important computer vision problem with many practical applications to mobile devices. While many solutions have been proposed for this task, they are usually very computationally expensive and thus are not applicable for on-device inference. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based depth estimation solutions that can demonstrate a nearly real-time performance on smartphones and IoT platforms. For this, the participants were provided with a new large-scale dataset containing RGB-depth image pairs obtained with a dedicated stereo ZED camera producing high-resolution depth maps for objects located at up to 50 meters. The runtime of all models was evaluated on the popular Raspberry Pi 4 platform with a mobile ARM-based Broadcom chipset. The proposed solutions can generate VGA resolution depth maps at up to 10 FPS on the Raspberry Pi 4 while achieving high fidelity results, and are compatible with any Android or Linux-based mobile devices. A detailed description of all models developed in the challenge is provided in this paper. ",
    "code_link": ""
  },
  "cvpr2021_mai_rscareal-timesegmentation-basedcontext-awarescenetextdetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "RSCA: Real-Time Segmentation-Based Context-Aware Scene Text Detection",
    "authors": [
      "Jiachen Li",
      "Yuan Lin",
      "Rongrong Liu",
      "Chiu Man Ho",
      "Humphrey Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Li_RSCA_Real-Time_Segmentation-Based_Context-Aware_Scene_Text_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Li_RSCA_Real-Time_Segmentation-Based_Context-Aware_Scene_Text_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Segmentation-based scene text detection methods have been widely adopted for arbitrary-shaped text detection recently, because they make accurate pixel-level predictions on curved text instances and facilitate real-time inference without time-consuming processing on anchors. However, current segmentation-based models are unable to learn the shapes of curved texts and often require complex label assignments or repeated feature aggregations for more accurate detection. In this paper, we propose RSCA: a Real-time Segmentation-based Context-Aware model for arbitrary-shaped scene text detection, which sets a strong baseline for arbitrary-shaped scene text detection with two simple yet effective strategies: Local Context-Aware Upsampling and Dynamic Text-Spine Labeling, which model local spatial transformation and simplify label assignments separately. Based on these strategies, RSCA achieves state-of-the-art performances without complex label assignments or repeated feature aggregations in a real-time inference speed. We conduct extensive experiments on multiple benchmarks to validate the effectiveness of our method. RSCA-640 reaches 83.9% F-measure at 48.3 FPS on CTW1500 dataset. ",
    "code_link": ""
  },
  "cvpr2021_mai_anchor-basedplainnetformobileimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Anchor-Based Plain Net for Mobile Image Super-Resolution",
    "authors": [
      "Zongcai Du",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Du_Anchor-Based_Plain_Net_for_Mobile_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Du_Anchor-Based_Plain_Net_for_Mobile_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand high computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training to further boost the performance. Our INT8 quantization model can even achieve nearly the same performance as the floating-point network, with only 0.07dB drop. ",
    "code_link": "https://github.com/NJUJet/SR_Mobile_Quantization"
  },
  "cvpr2021_mai_csanethighspeedchannelspatialattentionnetworkformobileisp": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "CSAnet: High Speed Channel Spatial Attention Network for Mobile ISP",
    "authors": [
      "Ming-Chun Hsyu",
      "Chih-Wei Liu",
      "Chao-Hung Chen",
      "Chao-Wei Chen",
      "Wen-Chia Tsai"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Hsyu_CSAnet_High_Speed_Channel_Spatial_Attention_Network_for_Mobile_ISP_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Hsyu_CSAnet_High_Speed_Channel_Spatial_Attention_Network_for_Mobile_ISP_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The Image Signal Processor (ISP) is a customized device to restore RGB images from the pixel signals of CMOS image sensor. In order to realize this function, a series of processing units are leveraged to tackle different artifacts, such as color shifts, signal noise, moire effects, and so on, that are introduced from the photo-capturing devices. However, tuning each processing unit is highly complicated and requires a lot of experience and effort from image experts. In this paper, a novel network architecture, CSANet, with emphases on inference speed and high PSNR is proposed for end-to-end learned ISP task. The proposed CSANet applies a double attention module employing both channel and spatial attentions. Particularly, its spatial attention is simplified to a light-weighted dilated depth-wise convolution and still performs as well as others. As proof of performance, CSANet won 2nd place in the Mobile AI 2021 Learned Smartphone ISP Challenge with 1st place PSNR score. ",
    "code_link": ""
  },
  "cvpr2021_mai_learnedsmartphoneisponmobilenpuswithdeeplearning,mobileai2021challengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Learned Smartphone ISP on Mobile NPUs With Deep Learning, Mobile AI 2021 Challenge: Report",
    "authors": [
      "Andrey Ignatov",
      "Cheng-Ming Chiang",
      "Hsien-Kai Kuo",
      "Anastasia Sycheva",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Learned_Smartphone_ISP_on_Mobile_NPUs_With_Deep_Learning_Mobile_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Learned_Smartphone_ISP_on_Mobile_NPUs_With_Deep_Learning_Mobile_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As the quality of mobile cameras starts to play a crucial role in modern smartphones, more and more attention is now being paid to ISP algorithms used to improve various perceptual aspects of mobile photos. In this Mobile AI challenge, the target was to develop an end-to-end deep learning-based image signal processing (ISP) pipeline that can replace classical hand-crafted ISPs and achieve nearly real-time performance on smartphone NPUs. For this, the participants were provided with a novel learned ISP dataset consisting of RAW-RGB image pairs captured with the Sony IMX586 Quad Bayer mobile sensor and a professional 102-megapixel medium format camera. The runtime of all models was evaluated on the MediaTek Dimensity 1000+ platform with a dedicated AI processing unit capable of accelerating both floating-point and quantized neural networks. The proposed solutions are fully compatible with the above NPU and are capable of processing Full HD photos under 60-100 milliseconds while achieving high fidelity results. A detailed description of all models developed in this challenge is provided in this paper. ",
    "code_link": "https://github.com/MediaTek-NeuroPilot/tfliteneuron-delegate"
  },
  "cvpr2021_mai_real-timeanaloguegaugetranscriptiononmobilephone": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Real-Time Analogue Gauge Transcription on Mobile Phone",
    "authors": [
      "Ben Howells",
      "James Charles",
      "Roberto Cipolla"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Howells_Real-Time_Analogue_Gauge_Transcription_on_Mobile_Phone_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Howells_Real-Time_Analogue_Gauge_Transcription_on_Mobile_Phone_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The objective of this paper is to automatically read any circular single pointer analogue gauge in real-time on mobile phone. We make the following contributions: (i) we show how to efficiently and accurately read gauges on mobile phones using a convolutional neural network (CNN) system which accepts both a high and low resolution gauge image; (ii) we introduce a large synthetic image dataset (far superior in size to prior works) with ground truth gauge readings, pointer layout and scale face homographies that is suitable for training a CNN for real world application; (iii) we also release a new real world analogue gauge dataset (larger meter variation than any previous) with annotation suitable for testing three different types of tasks and finally (iv) we beat state of the art performance for gauge reading on this dataset and an existing public dataset in multiple metrics by large margins, notably with pointer angle error less than 1 degree. Our method is fast and lightweight and runs up to 25fps on mobile devices. ",
    "code_link": ""
  },
  "cvpr2021_mai_asimplebaselineforfastandaccuratedepthestimationonmobiledevices": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "A Simple Baseline for Fast and Accurate Depth Estimation on Mobile Devices",
    "authors": [
      "Ziyu Zhang",
      "Yicheng Wang",
      "Zilong Huang",
      "Guozhong Luo",
      "Gang Yu",
      "Bin Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Zhang_A_Simple_Baseline_for_Fast_and_Accurate_Depth_Estimation_on_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Zhang_A_Simple_Baseline_for_Fast_and_Accurate_Depth_Estimation_on_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a simple but effective encoder-decoder based network for fast and accurate depth estimation on mobile devices. Unlike other depth estimation methods using heavy context modeling modules, the encoder with a fast downsampling strategy is employed to obtain sufficient receptive field and contexts at a faster rate. To obtain dense prediction, a light decoder is adopted to recover back to the original resolution. Additionally, to improve the representative ability of the light network, we introduce a teacher-student strategy. It relies on a distillation process ensuring that the student (the proposed light network) learns from the teacher. The proposed method achieves a good trade-off between latency and accuracy. We evaluated the proposed algorithm on the MAI 2021 Monocular Depth Estimation Challenge and achieved a score of 129.41, ranked the first place, which wins the second by a large margin (129.41 v.s. 14.51). More specifically, the proposed method achieves a si-RMSE score of 0.28 with 97 ms on the Raspberry Pi 4. ",
    "code_link": ""
  },
  "cvpr2021_mai_mobilehumanposetowardreal-time3dhumanposeestimationinmobiledevices": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "MobileHumanPose: Toward Real-Time 3D Human Pose Estimation in Mobile Devices",
    "authors": [
      "Sangbum Choi",
      "Seokeon Choi",
      "Changick Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Choi_MobileHumanPose_Toward_Real-Time_3D_Human_Pose_Estimation_in_Mobile_Devices_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Choi_MobileHumanPose_Toward_Real-Time_3D_Human_Pose_Estimation_in_Mobile_Devices_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Currently, 3D pose estimation methods are not compatible with a variety of low computational power devices because of efficiency and accuracy. In this paper, we revisit a pose estimation architecture from a viewpoint of both efficiency and accuracy. We propose a mobile-friendly model, MobileHumanPose, for real-time 3D human pose estimation from a single RGB image. This model consists of the modified MobileNetV2 backbone, a parametric activation function, and the skip concatenation inspired by U-Net. Especially, the skip concatenation structure improves accuracy by propagating richer features with negligible computational power. Our model achieves not only comparable performance to the state-of-the-art models but also has a seven times smaller model size compared to the ResNet-50 based model. In addition, our extra small model reduces inference time by 12.2ms on Galaxy S20 CPU, which is suitable for real-time 3D human pose estimation in mobile applications. The source code is available at: https://github.com/SangbumChoi/MobileHumanPose. ",
    "code_link": "https://github.com/SangbumChoi/MobileHumanPose"
  },
  "cvpr2021_mai_deepshifttowardsmultiplication-lessneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "DeepShift: Towards Multiplication-Less Neural Networks",
    "authors": [
      "Mostafa Elhoushi",
      "Zihao Chen",
      "Farhan Shafiq",
      "Ye Henry Tian",
      "Joey Yiwei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Elhoushi_DeepShift_Towards_Multiplication-Less_Neural_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Elhoushi_DeepShift_Towards_Multiplication-Less_Neural_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The high computation, memory, and power budgets of inferring convolutional neural networks (CNNs) are major bottlenecks of model deployment to edge computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is time and energy-intensive even on high-grade servers. Convolution layers and fully connected layers, because of their intense use of multiplications, are the dominant contributor to this computation budget. We propose to alleviate this problem by introducing two new operations: convolutional shifts and fully-connected shifts which replace multiplications with bitwise shift and sign flipping during both training and inference. During inference, both approaches require only 5 bits (or less) to represent the weights. This family of neural network architectures (that use convolutional shifts and fully connected shifts) is referred to as DeepShift models. We propose two methods to train DeepShift models: DeepShift-Q which trains regular weights constrained to powers of 2, and DeepShift-PS that trains the values of the shifts and sign flips directly. Very close accuracy, and in some cases higher accuracy, to baselines are achieved. Converting pre-trained 32-bit floating-point baseline models of ResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15 to 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the original model. Training the DeepShift versions of ResNet18 architecture from scratch, we obtained accuracies of 94.26% on the CIFAR10 dataset and Top-1/Top-5 accuracies of 65.32%/86.30% on the Imagenet dataset. Training the DeepShift version of VGG16 on ImageNet from scratch resulted in a drop of less than 0.3% in Top-5 accuracy. The code can be found at https://github.com/mostafaelhoushi/DeepShift. ",
    "code_link": "https://github.com/mostafaelhoushi/DeepShift"
  },
  "cvpr2021_mai_fastcameraimagedenoisingonmobilegpuswithdeeplearning,mobileai2021challengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Fast Camera Image Denoising on Mobile GPUs With Deep Learning, Mobile AI 2021 Challenge: Report",
    "authors": [
      "Andrey Ignatov",
      "Kim Byeoung-su",
      "Radu Timofte",
      "Angeline Pouget"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Fast_Camera_Image_Denoising_on_Mobile_GPUs_With_Deep_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_Camera_Image_Denoising_on_Mobile_GPUs_With_Deep_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image denoising is one of the most critical problems in mobile photo processing. While many solutions have been proposed for this task, they are usually working with synthetic data and are too computationally expensive to run on mobile devices. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image denoising solution that can demonstrate high efficiency on smartphone GPUs. For this, the participants were provided with a novel large-scale dataset consisting of noisy-clean image pairs captured in the wild. The runtime of all models was evaluated on the Samsung Exynos 2100 chipset with a powerful Mali GPU capable of accelerating floating-point and quantized neural networks. The proposed solutions are fully compatible with any mobile GPU and are capable of processing 480p resolution images under 40-80 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. ",
    "code_link": ""
  },
  "cvpr2021_mai_extremelylightweightquantizationrobustreal-timesingle-imagesuperresolutionformobiledevices": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices",
    "authors": [
      "Mustafa Ayazoglu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ayazoglu_Extremely_Lightweight_Quantization_Robust_Real-Time_Single-Image_Super_Resolution_for_Mobile_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ayazoglu_Extremely_Lightweight_Quantization_Robust_Real-Time_Single-Image_Super_Resolution_for_Mobile_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Single-Image Super Resolution (SISR) is a classical computer vision problem and it has been studied for over decades. With the recent success of deep learning methods, recent work on SISR focuses solutions with deep learning methodologies and achieves state-of-the-art results. However most of the state-of-the-art SISR methods contain millions of parameters and layers, which limits their practical applications. In this paper we propose a hardware (Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization robust real-time super resolution network (XLSR). The proposed model's building block is inspired from root modules introduced in DeepRoots for Image Classification. We succesfully applied root modules to SISR problem, further more to make the model uint8 quantizaiton robust we used Clipped ReLU at the last layer of the network and achieved great balance between reconstruction quality and runtime. Further more although the proposed network contains 30x fewer parameters than VDSR it's performance surpasses it on Div2K validation set. The network proved itself by winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge. ",
    "code_link": ""
  },
  "cvpr2021_mai_evsrnetefficientvideosuper-resolutionwithneuralarchitecturesearch": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "EVSRNet: Efficient Video Super-Resolution With Neural Architecture Search",
    "authors": [
      "Shaoli Liu",
      "Chengjian Zheng",
      "Kaidi Lu",
      "Si Gao",
      "Ning Wang",
      "Bofei Wang",
      "Diankai Zhang",
      "Xiaofeng Zhang",
      "Tianyu Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Liu_EVSRNet_Efficient_Video_Super-Resolution_With_Neural_Architecture_Search_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Liu_EVSRNet_Efficient_Video_Super-Resolution_With_Neural_Architecture_Search_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " With the development of convolutional neural networks (CNN), the super-resolution results of CNN-based method have far surpassed traditional method. In particular, the CNN-based single image super-resolution method has achieved excellent results. Video sequences contain more abundant information compare with image, but there are few video super-resolution methods that can be applied to mobile devices due to the requirement of heavy computation, which limits the application of video super-resolution. In this work, we propose the Efficient Video Super-Resolution Network (EVSRNet) with neural architecture search for real-time video super-resolution. Extensive experiments show that our method achieves a good balance between quality and efficiency. Finally, we achieve a competitive result of 7.36 where the PSNR is 27.85 dB and the inference time is 11.3 ms/f on the target snapdragon 865 SoC, resulting in a 2nd place in the Mobile AI(MAI)2021 real-time video super-resolution challenge. It is noteworthy that, our method is the fastest and significantly outperforms other competitors by large margins. ",
    "code_link": ""
  },
  "cvpr2021_mai_layerimportanceestimationwithimprintingforneuralnetworkquantization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Layer Importance Estimation With Imprinting for Neural Network Quantization",
    "authors": [
      "Hongyang Liu",
      "Sara Elkerdawy",
      "Nilanjan Ray",
      "Mostafa Elhoushi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Liu_Layer_Importance_Estimation_With_Imprinting_for_Neural_Network_Quantization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Liu_Layer_Importance_Estimation_With_Imprinting_for_Neural_Network_Quantization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural network quantization has achieved a high compression rate using a fixed low bit-width representation of weights and activations while maintaining the accuracy of the high-precision original network. However, mixed-precision (per-layer bit-width precision) quantization requires careful tuning to maintain accuracy while achieving further compression and higher granularity than fixed-precision quantization. We propose an accuracy-aware criterion to quantify the layer's importance rank. Our method applies imprinting per layer which acts as a proxy module for accuracy estimation in an efficient way. We rank the layers based on the accuracy gain from previous modules and iteratively quantize first those with less accuracy gain. Previous mixed-precision methods either rely on expensive search techniques such as reinforcement learning (RL) or end-to-end optimization with a lack of interpretation to the quantization configuration scheme. Our method is a one-shot, efficient, accuracy-aware information estimation and thus draws better interpretability to the selected bit-width configuration. ",
    "code_link": ""
  },
  "cvpr2021_mai_fastandaccuratequantizedcamerascenedetectiononsmartphones,mobileai2021challengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Fast and Accurate Quantized Camera Scene Detection on Smartphones, Mobile AI 2021 Challenge: Report",
    "authors": [
      "Andrey Ignatov",
      "Grigory Malivenko",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Fast_and_Accurate_Quantized_Camera_Scene_Detection_on_Smartphones_Mobile_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_and_Accurate_Quantized_Camera_Scene_Detection_on_Smartphones_Mobile_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Camera scene detection is among the most popular computer vision problem on smartphones. While many custom solutions were developed for this task by phone vendors, none of the designed models were available publicly up until now. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop quantized deep learning-based camera scene classification solutions that can demonstrate a real-time performance on smartphones and IoT platforms. For this, the participants were provided with a large-scale CamSDD dataset consisting of more than 11K images belonging to the 30 most important scene categories. The runtime of all models was evaluated on the popular Apple Bionic A11 platform that can be found in many iOS devices. The proposed solutions are fully compatible with all major mobile AI accelerators and can demonstrate more than 100-200 FPS on the majority of recent smartphone platforms while achieving a top-3 accuracy of more than 98%. A detailed description of all models developed in the challenge is provided in this paper. ",
    "code_link": ""
  },
  "cvpr2021_mai_real-timequantizedimagesuper-resolutiononmobilenpus,mobileai2021challengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI 2021 Challenge: Report",
    "authors": [
      "Andrey Ignatov",
      "Radu Timofte",
      "Maurizio Denna",
      "Abdel Younes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Real-Time_Quantized_Image_Super-Resolution_on_Mobile_NPUs_Mobile_AI_2021_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Real-Time_Quantized_Image_Super-Resolution_on_Mobile_NPUs_Mobile_AI_2021_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image super-resolution is one of the most popular computer vision problems with many important applications to mobile devices. While many solutions have been proposed for this task, they are usually not optimized even for common smartphone AI hardware, not to mention more constrained smart TV platforms that are often supporting INT8 inference only. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image super-resolution solutions that can demonstrate a real-time performance on mobile or edge NPUs. For this, the participants were provided with the DIV2K dataset and trained quantized models to do an efficient 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated NPU capable of accelerating quantized neural networks. The proposed solutions are fully compatible with all major mobile AI accelerators and are capable of reconstructing Full HD images under 40-60 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. ",
    "code_link": ""
  },
  "cvpr2021_mai_stackeddeepmulti-scalehierarchicalnetworkforfastbokeheffectrenderingfromasingleimage": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect Rendering From a Single Image",
    "authors": [
      "Saikat Dutta",
      "Sourya Dipta Das",
      "Nisarg A. Shah",
      "Anil Kumar Tiwari"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Dutta_Stacked_Deep_Multi-Scale_Hierarchical_Network_for_Fast_Bokeh_Effect_Rendering_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Dutta_Stacked_Deep_Multi-Scale_Hierarchical_Network_for_Fast_Bokeh_Effect_Rendering_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The Bokeh Effect is one of the most desirable effects in photography for rendering artistic and aesthetic photos. Usually, it requires a DSLR camera with different aperture and shutter settings and certain photography skills to generate this effect. In smartphones, computational methods and additional sensors are used to overcome the physical lens and sensor limitations to achieve such effect. Most of the existing methods utilized additional sensor's data or pretrained network for fine depth estimation of the scene and sometimes use portrait segmentation pretrained network module to segment salient objects in the image. Because of these reasons, networks have many parameters, become runtime intensive and unable to run in mid-range devices. In this paper, we used an end-to-end Deep Multi-Scale Hierarchical Network (DMSHN) model for direct Bokeh effect rendering of images captured from the monocular camera. To further improve the perceptual quality of such effect, a stacked model consisting of two DMSHN modules is also proposed. Our model does not rely on any pretrained network module for Monocular Depth Estimation or Saliency Detection, thus significantly reducing the size of model and run time. Stacked DMSHN achieves state-of-the-art results on a large scale EBB! dataset with around 6x less runtime compared to the current state-of-the-art model in processing HD quality images. ",
    "code_link": ""
  },
  "cvpr2021_mai_asymmnettowardsultralightconvolutionneuralnetworksusingasymmetricalbottlenecks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "AsymmNet: Towards Ultralight Convolution Neural Networks Using Asymmetrical Bottlenecks",
    "authors": [
      "Haojin Yang",
      "Zhen Shen",
      "Yucheng Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Yang_AsymmNet_Towards_Ultralight_Convolution_Neural_Networks_Using_Asymmetrical_Bottlenecks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Yang_AsymmNet_Towards_Ultralight_Convolution_Neural_Networks_Using_Asymmetrical_Bottlenecks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep convolutional neural networks (CNN) have achieved astonishing results in a large variety of applications. However, using these models on mobile or embedded devices is difficult due to the limited memory and computation resources. Recently, the inverted residual block becomes the dominating solution for the architecture design of compact CNNs. In this work, we comprehensively investigated the existing design concepts, rethink the functional characteristics of two pointwise convolutions in the inverted residuals. We propose a novel design, called asymmetrical bottlenecks. Precisely, we adjust the first pointwise convolution dimension, enrich the information flow by feature reuse, and migrate saved computations to the second pointwise convolution. Doing so we can further improve the accuracy without increasing the computation overhead. The asymmetrical bottlenecks can be adopted as a drop-in replacement for the existing CNN blocks. We can thus create AsymmNet by easily stack those blocks according to proper depth and width conditions. Extensive experiments demonstrate that our proposed block design is more beneficial than the original inverted residual bottlenecks for mobile networks, especially useful for those ultralight CNNs within the regime of <220M MAdds. Code is available at: https://github.com/Spark001/AsymmNet ",
    "code_link": ""
  },
  "cvpr2021_mai_real-timevideosuper-resolutiononsmartphoneswithdeeplearning,mobileai2021challengereport": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Real-Time Video Super-Resolution on Smartphones With Deep Learning, Mobile AI 2021 Challenge: Report",
    "authors": [
      "Andrey Ignatov",
      "Andres Romero",
      "Heewon Kim",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Real-Time_Video_Super-Resolution_on_Smartphones_With_Deep_Learning_Mobile_AI_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Real-Time_Video_Super-Resolution_on_Smartphones_With_Deep_Learning_Mobile_AI_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper. ",
    "code_link": ""
  },
  "cvpr2021_mai_computervision-basedassistancesystemforthevisuallyimpairedusingmobileedgeartificialintelligence": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Computer Vision-Based Assistance System for the Visually Impaired Using Mobile Edge Artificial Intelligence",
    "authors": [
      "Jagadish K. Mahendran",
      "Daniel T. Barry",
      "Anita K. Nivedha",
      "Suchendra M. Bhandarkar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Mahendran_Computer_Vision-Based_Assistance_System_for_the_Visually_Impaired_Using_Mobile_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Mahendran_Computer_Vision-Based_Assistance_System_for_the_Visually_Impaired_Using_Mobile_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Despite significant recent developments, visual assistance systems are still severely constrained by sensor capabilities, form factor, battery power consumption, computational resources and the use of traditional computer vision algorithms. Current visual assistance systems cannot adequately perform complex computer vision tasks that entail deep learning. We present the design and implementation of a novel visual assistance system that employs deep learning and point cloud processing to perform advanced perception tasks on a cost-effective, low-power mobile computing platform. The proposed system design circumvents the need for expensive, power-intensive Graphical Processing Unit (GPU)-based hardware required by most deep learning algorithms for real-time inference by employing instead edge Artificial Intelligence (AI) accelerators such as the Neural Compute Stick-2 (NCS2), model optimization techniques such as OpenVINO, and TensorflowLite, and smart depth sensors such as OpenCV AI Kit-Depth (OAK-D). Critical system design challenges such as training data collection, real-time capability, computational efficiency, power consumption, portability and reliability are addressed. The proposed system includes more advanced functionality than existing systems such as assessment of traffic conditions and detection and localization of hanging obstacles, crosswalks, moving obstacles and sudden elevation changes. The proposed system design incorporates an AI-based voice interface that allows for user-friendly interaction and control and is shown to realize a simple, cost-effective, power-efficient, portable and unobtrusive visual assistance device. ",
    "code_link": "https://github.com/jaggiK/cv_visual_assistance"
  },
  "cvpr2021_mai_doallmobilenetsquantizepoorly?gaininginsightsintotheeffectofquantizationondepthwiseseparableconvolutionalnetworksthroughtheeyesofmulti-scaledistributionaldynamics": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Do All MobileNets Quantize Poorly? Gaining Insights Into the Effect of Quantization on Depthwise Separable Convolutional Networks Through the Eyes of Multi-Scale Distributional Dynamics",
    "authors": [
      "Stone Yun",
      "Alexander Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Yun_Do_All_MobileNets_Quantize_Poorly_Gaining_Insights_Into_the_Effect_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Yun_Do_All_MobileNets_Quantize_Poorly_Gaining_Insights_Into_the_Effect_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " As the \"Mobile AI\" revolution continues to grow, so does the need to understand the behaviour of edge-deployed deep neural networks. In particular, MobileNets are the go-to family of deep convolutional neural networks (CNN) for mobile. However, they often have significant accuracy degradation under post-training quantization. While studies have introduced quantization-aware training and other methods to tackle this challenge, there is limited understanding into why MobileNets (and potentially depthwise-separable CNNs (DWSCNN) in general) quantize so poorly compared to other CNN architectures. Motivated to gain deeper insights into this phenomenon, we take a different strategy and study the multi-scale distributional dynamics of MobileNet-V1, a set of smaller DWSCNNs, and regular CNNs. Specifically, we investigate the impact of quantization on the weight and activation distributional dynamics as information propagates from layer to layer, as well as overall changes in distributional dynamics at the network level. This fine-grained analysis revealed significant dynamic range fluctuations and a \"distributional mismatch\" between channelwise and layerwise distributions in DWSCNNs that lead to increasing quantized degradation and distributional shift during information propagation. Furthermore, analysis of the activation quantization errors show that there is greater quantization error accumulation in DWSCNN compared to regular CNNs. The hope is that such insights can lead to innovative strategies for reducing such distributional dynamics changes and improve post-training quantization for mobile. ",
    "code_link": ""
  },
  "cvpr2021_mai_real-timemonoculardepthestimationwithsparsesupervisiononmobile": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Real-Time Monocular Depth Estimation With Sparse Supervision on Mobile",
    "authors": [
      "Mehmet Kerim Yucel",
      "Valia Dimaridou",
      "Anastasios Drosou",
      "Albert Saa-Garriga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Yucel_Real-Time_Monocular_Depth_Estimation_With_Sparse_Supervision_on_Mobile_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Yucel_Real-Time_Monocular_Depth_Estimation_With_Sparse_Supervision_on_Mobile_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Monocular (relative or metric) depth estimation is a critical task for various applications, such as autonomous vehicles, augmented reality and image editing. In recent years, with the increasing availability of mobile devices, accurate and mobile-friendly depth models have gained importance. Increasingly accurate models typically require more computational resources, which inhibits the use of such models on mobile devices. The mobile use case is arguably the most unrestricted one, which requires highly accurate yet mobile-friendly architectures. Therefore, we try to answer the following question: How can we improve a model without adding further complexity (i.e. parameters)? Towards this end, we systematically explore the design space of a relative depth estimation model from various dimensions and we show, with key design choices and ablation studies, even an existing architecture can reach highly competitive performance to the state of the art, with a fraction of the complexity. Our study spans an in-depth backbone model selection process, knowledge distillation, intermediate predictions, model pruning and loss rebalancing. We show that our model, using only DIW as the supervisory dataset, achieves 0.1156 WHDR on DIW with 2.6M parameters and reaches 37 FPS on a mobile GPU, without pruning or hardware-specific optimization. A pruned version of our model achieves 0.1208 WHDR on DIW with 1M parameters and reaches 44 FPS on a mobile GPU ",
    "code_link": "https://github.com/Turmac/DIW_TF_Implementation"
  },
  "cvpr2021_mai_pseudo-iouimprovinglabelassignmentinanchor-freeobjectdetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection",
    "authors": [
      "Jiachen Li",
      "Bowen Cheng",
      "Rogerio Feris",
      "Jinjun Xiong",
      "Thomas S. Huang",
      "Wen-Mei Hwu",
      "Humphrey Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Li_Pseudo-IoU_Improving_Label_Assignment_in_Anchor-Free_Object_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Li_Pseudo-IoU_Improving_Label_Assignment_in_Anchor-Free_Object_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Current anchor-free object detectors are quite simple and effective yet lack accurate sample assignment methods, which limits their potential in competing with classic anchor-based models that are supported by well-designed assignment methods based on the Intersection-over-Union (IoU) metric. In this paper, we present Pseudo-Intersection-over-Union (Pseudo-IoU): a simple metric that brings more standardized and accurate assignment rule into anchor-free object detection frameworks without any additional computational cost or extra parameters for training and testing, making it possible to further improve anchor-free object detection by utilizing training samples of good quality under effective assignment rules that have been previously applied in anchor-based methods. By incorporating Pseudo-IoU metric into an end-to-end single-stage anchor-free object detection framework, we observe consistent improvements in their performance on general object detection benchmarks such as PASCAL VOC and MSCOCO. Our method (single-model and single-scale) also achieves comparable performance to other recent state-of-the-art anchor-free methods without bells and whistles. Our code is based on mmdetection toolbox and will be made publicly available at https://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection. ",
    "code_link": "https://github.com/SHILabs/Pseudo-IoU-for-Anchor-Free-Object-Detection"
  },
  "cvpr2021_mai_knowledgedistillationforfastandaccuratemonoculardepthestimationonmobiledevices": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Knowledge Distillation for Fast and Accurate Monocular Depth Estimation on Mobile Devices",
    "authors": [
      "Yiran Wang",
      "Xingyi Li",
      "Min Shi",
      "Ke Xian",
      "Zhiguo Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Wang_Knowledge_Distillation_for_Fast_and_Accurate_Monocular_Depth_Estimation_on_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Wang_Knowledge_Distillation_for_Fast_and_Accurate_Monocular_Depth_Estimation_on_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Fast and accurate monocular depth estimation on mobile devices is a challenging task as one should always trade off the accuracy against the inference time. Most monocular depth methods adopt models with large computation overhead, which are not applicable on mobile devices. However, directly training a light-weight neural network to estimate depth can yield poor performance. To remedy this, we utilize knowledge distillation, transferring the knowledge and representation ability of a stronger teacher network to a light-weight student network. Experiments on Mobile AI 2021 (MAI2021) dataset demonstrate that our solution helps increase the fidelity of the output depth map and maintain fast inference speed. Specifically, with 94.7% less parameters than teacher network, the si-RMSE of student network only decrease by 0.04. Moreover, our method ranks second in the MAI2021 Monocular Depth Estimation Challenge, with a si-RMSE of 0.2602, a RMSE of 3.25, and the inference time is 1197 ms tested on the Raspberry Pi 4. ",
    "code_link": ""
  },
  "cvpr2021_mai_lowbandwidthvideo-chatcompressionusingdeepgenerativemodels": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Low Bandwidth Video-Chat Compression Using Deep Generative Models",
    "authors": [
      "Maxime Oquab",
      "Pierre Stock",
      "Daniel Haziza",
      "Tao Xu",
      "Peizhao Zhang",
      "Onur Celebi",
      "Yana Hasson",
      "Patrick Labatut",
      "Bobo Bose-Kolanu",
      "Thibault Peyronel",
      "Camille Couprie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Oquab_Low_Bandwidth_Video-Chat_Compression_Using_Deep_Generative_Models_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Oquab_Low_Bandwidth_Video-Chat_Compression_Using_Deep_Generative_Models_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " To unlock video chat for hundreds of millions of people hindered by poor connectivity or unaffordable data costs, we propose to authentically reconstruct faces on the receiver's device using facial landmarks extracted at the sender's side and transmitted over the network. In this context, we discuss and evaluate the benefits and disadvantages of several deep adversarial approaches. In particular, we explore quality and bandwidth trade-offs for approaches based on static landmarks, dynamic landmarks or segmentation maps. We design a mobile-compatible architecture based on the first order animation model of Siarohin et al. In addition, we leverage SPADE blocks to refine results in important areas such as the eyes and lips. We compress the networks down to about 3MB, allowing models to run in real time on iPhone8. This approach enables video calling at a few kbits per second, an order of magnitude lower than currently available alternatives. ",
    "code_link": ""
  },
  "cvpr2021_mai_filteringemptycameratrapimagesinembeddedsystems": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "MAI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Mobile AI",
    "title": "Filtering Empty Camera Trap Images in Embedded Systems",
    "authors": [
      "Fagner Cunha",
      "Eulanda M. dos Santos",
      "Raimundo Barreto",
      "Juan G. Colonna"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Cunha_Filtering_Empty_Camera_Trap_Images_in_Embedded_Systems_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Cunha_Filtering_Empty_Camera_Trap_Images_in_Embedded_Systems_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency. ",
    "code_link": ""
  },
  "cvpr2021_llid_weakmulti-viewsupervisionforsurfacemappingestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Weak Multi-View Supervision for Surface Mapping Estimation",
    "authors": [
      "Nishant Rai",
      "Aidas Liaudanskas",
      "Srinivas Rao",
      "Rodrigo Ortiz Cayon",
      "Matteo Munaro",
      "Stefan Holzer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Rai_Weak_Multi-View_Supervision_for_Surface_Mapping_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Rai_Weak_Multi-View_Supervision_for_Surface_Mapping_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a weakly-supervised multi-view learning approach to learn category-specific surface mapping without dense annotations. We learn the underlying surface geometry of common categories, such as human faces, cars, and airplanes, given instances from those categories. While traditional approaches solve this problem using extensive supervision in the form of pixel-level annotations, we take advantage of the fact that pixel-level UV and mesh predictions can be combined with 3D reprojections to form consistency cycles. As a result of exploiting these cycles, we can establish a dense correspondence mapping between image pixels and the mesh acting as a self-supervisory signal, which in turn helps improve our overall estimates. Our approach leverages information from multiple views of the object to establish additional consistency cycles, thus improving surface mapping understanding without the need for explicit annotations. We also propose the use of deformation fields for predictions of an instance specific mesh. Given the lack of datasets providing multiple images of similar object instances from different viewpoints, we generate and release a multi-view ShapeNet Cars and Airplanes dataset created by rendering ShapeNet meshes using a 360 degree camera trajectory around the mesh. For the human faces category, we process and adapt an existing dataset to a multi-view setup. Through experimental evaluations, we show that, at test time, our method can generate accurate variations away from the mean shape, is multi-view consistent, and performs comparably to fully supervised approaches. ",
    "code_link": ""
  },
  "cvpr2021_llid_learningfromincompletefeaturesbysimultaneoustrainingofneuralnetworksandsparsecoding": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Learning From Incomplete Features by Simultaneous Training of Neural Networks and Sparse Coding",
    "authors": [
      "Cesar F. Caiafa",
      "Ziyao Wang",
      "Jordi Sole-Casals",
      "Qibin Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Caiafa_Learning_From_Incomplete_Features_by_Simultaneous_Training_of_Neural_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Caiafa_Learning_From_Incomplete_Features_by_Simultaneous_Training_of_Neural_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, the problem of training a classifier on a dataset with incomplete features is addressed. We assume that different subsets of features (random or structured) are available at each data instance. This situation typically occurs in the applications when not all the features are collected for every data sample. A new supervised learning method is developed to train a general classifier, such as a logistic regression or a deep neural network, using only a subset of features per sample, while assuming sparse representations of data vectors on an unknown dictionary. Sufficient conditions are identified, such that, if it is possible to train a classifier on incomplete observations so that their reconstructions are well separated by a hyperplane, then the same classifier also correctly separates the original (unobserved) data samples. Extensive simulation results on synthetic and well known datasets are presented that validate our theoretical findings and demonstrate the effectiveness of the proposed method compared to traditional data imputation approaches and one state-of-the-art algorithm. ",
    "code_link": ""
  },
  "cvpr2021_llid_cluster-drivengraphfederatedlearningovermultipledomains": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Cluster-Driven Graph Federated Learning Over Multiple Domains",
    "authors": [
      "Debora Caldarola",
      "Massimiliano Mancini",
      "Fabio Galasso",
      "Marco Ciccone",
      "Emanuele Rodola",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Caldarola_Cluster-Driven_Graph_Federated_Learning_Over_Multiple_Domains_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Caldarola_Cluster-Driven_Graph_Federated_Learning_Over_Multiple_Domains_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Federated Learning (FL) deals with learning a central model (i.e. the server) in privacy-constrained scenarios, where data are stored on multiple devices (i.e. the clients). The central model has no direct access to the data, but only to the updates of the parameters computed locally by each client. This raises a problem, known as statistical heterogeneity, because the clients may have different data distributions (i.e. domains). This is only partly alleviated by clustering the clients. Clustering may reduce heterogeneity by identifying the domains, but it deprives each cluster model of the data and supervision of others. Here we propose a novel Cluster-driven Graph Federated Learning (FedCG). In FedCG, clustering serves to address statistical heterogeneity, while Graph Convolutional Networks (GCNs) enable sharing knowledge across them. FedCG: i) identifies the domains via an FL-compliant clustering and instantiates domain-specific modules (residual branches) for each domain; ii) connects the domain-specific modules through a GCN at training to learn the interactions among domains and share knowledge; and iii) learns to cluster unsupervised via teacher-student classifier-training iterations and to address novel unseen test domains via their domain soft-assignment scores. Thanks to the unique interplay of GCN over clusters, FedCG achieves the state-of-the-art on multiple FL benchmarks. ",
    "code_link": "https://github.com/TalwalkarLab/leaf"
  },
  "cvpr2021_llid_remprectifiedmetricpropagationforfew-shotlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "ReMP: Rectified Metric Propagation for Few-Shot Learning",
    "authors": [
      "Yang Zhao",
      "Chunyuan Li",
      "Ping Yu",
      "Changyou Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Zhao_ReMP_Rectified_Metric_Propagation_for_Few-Shot_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Zhao_ReMP_Rectified_Metric_Propagation_for_Few-Shot_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Few-shot learning features the capability of generalizing from a few examples. In this paper, we first identify that a discriminative feature space, namely a rectified metric space, that is learned to maintain the metric consistency from training to testing, is an essential component to the success of metric-based few-shot learning. Numerous analyses indicate that a simple modification of the objective can yield substantial performance gains. The resulting approach, called rectified metric propagation (ReMP), further optimizes an attentive prototype propagation network, and applies a repulsive force to make confident predictions. Extensive experiments demonstrate that the proposed ReMP is effective and efficient, and outperforms the state of the arts on various standard few-shot learning datasets. ",
    "code_link": ""
  },
  "cvpr2021_llid_acloserlookatself-trainingforzero-labelsemanticsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "A Closer Look at Self-Training for Zero-Label Semantic Segmentation",
    "authors": [
      "Giuseppe Pastore",
      "Fabio Cermelli",
      "Yongqin Xian",
      "Massimiliano Mancini",
      "Zeynep Akata",
      "Barbara Caputo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Pastore_A_Closer_Look_at_Self-Training_for_Zero-Label_Semantic_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Pastore_A_Closer_Look_at_Self-Training_for_Zero-Label_Semantic_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Being able to segment unseen classes not observed during training is an important technical challenge in deep learning, because of its potential to reduce the expensive annotation required for semantic segmentation. Prior zero-label semantic segmentation works approach this task by learning visual-semantic embeddings or generative models. However, they are prone to overfitting on the seen classes because there is no training signal for them. In this paper, we study the challenging generalized zero-label semantic segmentation task where the model has to segment both seen and unseen classes at test time. We assume that pixels of unseen classes could be present in the training images but without being annotated. Our idea is to capture the latent information on unseen classes by supervising the model with self-produced pseudo-labels for unlabeled pixels. We propose a consistency regularizer to filter out noisy pseudo-labels by taking the intersections of the pseudo-labels generated from different augmentations of the same image. Our framework generates pseudo-labels and then retrain the model with human-annotated and pseudo-labelled data. This procedure is repeated for several iterations. As a result, our approach achieves the new state-of-the-art on PascalVOC12 and COCO-stuff datasets in the challenging generalized zero-label semantic segmentation setting, surpassing other existing methods addressing this task with more complex strategies. Code can be found at https: //github.com/giuseppepastore10/STRICT. ",
    "code_link": "https://github.com/giuseppepastore10/STRICT"
  },
  "cvpr2021_llid_efficacyofbayesianneuralnetworksinactivelearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Efficacy of Bayesian Neural Networks in Active Learning",
    "authors": [
      "Vineeth Rakesh",
      "Swayambhoo Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Rakesh_Efficacy_of_Bayesian_Neural_Networks_in_Active_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Rakesh_Efficacy_of_Bayesian_Neural_Networks_in_Active_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Obtaining labeled data for machine learning tasks can be prohibitively expensive. Active learning mitigates this issue by exploring the unlabeled data space and prioritizing the selection of data that can best improve the model performance. A common approach to active learning is to pick a small sample of data for which the model is most uncertain. In this paper, we explore the efficacy of Bayesian neural networks for active learning, which naturally models uncertainty by learning distribution over the weights of neural networks. By performing a comprehensive set of experiments, we show that Bayesian neural networks are more efficient than ensemble based techniques in capturing uncertainty. Our findings also reveal some key drawbacks of the ensemble techniques, which was recently shown to be more effective than Monte Carlo dropouts. ",
    "code_link": "https://github.com/VRM1/ActiveLearning"
  },
  "cvpr2021_llid_theroleofdataforone-shotsemanticsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "The Role of Data for One-Shot Semantic Segmentation",
    "authors": [
      "Timo Luddecke",
      "Alexander Ecker"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Luddecke_The_Role_of_Data_for_One-Shot_Semantic_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Luddecke_The_Role_of_Data_for_One-Shot_Semantic_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work we investigate the potential of larger datasets for one-shot semantic segmentation. While computer vision models are often trained on millions of diverse samples, current one-shot semantic segmentation datasets encompass only a small number of samples (Pascal-5i), a small number of classes (Pascal-5i and COCO-20i) or have little variability (FSS-1000). To improve this situation, we introduce LVIS-OneShot, a one-shot variant of the LVIS dataset. With 718 classes and 114,347 images, it exceeds previous datasets substantially in terms of size. By controlled experiments we show that not only the number of images but also the number of different classes is crucial. We analyze transfer learning across common datasets and find that by training on LVIS-OneShot we outperform current state-of-the-art models on Pascal-5i. In particular, we observe that a simple baseline model (MaRF) learns to perform one-shot segmentation when trained on a large dataset although it has a generic architecture without strong inductive biases. Code and dataset are available here: eckerlab.org/code/one-shot-segmentation ",
    "code_link": ""
  },
  "cvpr2021_llid_learningunbiasedrepresentationsviamutualinformationbackpropagation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Learning Unbiased Representations via Mutual Information Backpropagation",
    "authors": [
      "Ruggero Ragonesi",
      "Riccardo Volpi",
      "Jacopo Cavazza",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Ragonesi_Learning_Unbiased_Representations_via_Mutual_Information_Backpropagation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Ragonesi_Learning_Unbiased_Representations_via_Mutual_Information_Backpropagation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We are interested in learning data-driven representations that can generalize well, even when trained on inherently biased data. In particular, we face the case where some attributes (bias) of the data, if learned by the model, can severely compromise its generalization properties. We tackle this problem through the lens of information theory, leveraging recent findings for a differentiable estimation of mutual information. We propose a novel end-to-end optimization strategy, which simultaneously estimates and minimizes the mutual information between the learned representation and the data attributes. When applied on standard benchmarks, our model shows comparable or superior classification performance with respect to the state-of-the-art. Moreover, our method is general enough to be applicable to the problem of \"algorithmic fairness\", with competitive results. ",
    "code_link": ""
  },
  "cvpr2021_llid_efficientpre-trainedfeaturesandrecurrentpseudo-labelinginunsuperviseddomainadaptation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Efficient Pre-Trained Features and Recurrent Pseudo-Labeling in Unsupervised Domain Adaptation",
    "authors": [
      "Youshan Zhang",
      "Brian D. Davison"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Zhang_Efficient_Pre-Trained_Features_and_Recurrent_Pseudo-Labeling_in_Unsupervised_Domain_Adaptation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Zhang_Efficient_Pre-Trained_Features_and_Recurrent_Pseudo-Labeling_in_Unsupervised_Domain_Adaptation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Domain adaptation (DA) mitigates the domain shift problem when transferring knowledge from one annotated domain to another similar but different unlabeled domain. However, existing models often utilize one of the ImageNet models as the backbone without exploring others, and fine-tuning or retraining the backbone ImageNet model is also time-consuming. Moreover, pseudo-labeling has been used to improve the performance in the target domain, while how to generate confident pseudo labels and explicitly align domain distributions has not been well addressed. In this paper, we show how to efficiently opt for the best pre-trained features from seventeen well-known ImageNet models in unsupervised DA problems. In addition, we propose a recurrent pseudo-labeling model using the best pre-trained features (termed PRPL) to improve classification performance. To show the effectiveness of PRPL, we evaluate it on three benchmark datasets, Office+Caltech-10, Office-31, and Office-Home. Extensive experiments show that our model reduces computation time and boosts the mean accuracy to 98.1%, 92.4%, and 81.2%, respectively, substantially outperforming the state of the art. ",
    "code_link": ""
  },
  "cvpr2021_llid_taentemporalawareembeddingnetworkforfew-shotactionrecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition",
    "authors": [
      "Rami Ben-Ari",
      "Mor Shpigel Nacson",
      "Ophir Azulai",
      "Udi Barzelay",
      "Daniel Rotman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Ben-Ari_TAEN_Temporal_Aware_Embedding_Network_for_Few-Shot_Action_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Ben-Ari_TAEN_Temporal_Aware_Embedding_Network_for_Few-Shot_Action_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Classification of new class entities requires collecting and annotating hundreds or thousands of samples that is often prohibitively costly. Few-shot learning suggests learning to classify new classes using just a few examples. Only a small number of studies address the challenge of few-shot learning on spatio-temporal patterns such as videos. In this paper, we present the Temporal Aware Embedding Network (TAEN) for few-shot action recognition, that learns to represent actions, in a metric space as a trajectory, conveying both short term semantics and longer term connectivity between action parts. We demonstrate the effectiveness of TAEN on two few shot tasks, video classification and temporal action detection and evaluate our method on the Kinetics-400 and on ActivityNet 1.2 few-shot benchmarks. With training of just a few fully connected layers we reach comparable results to prior art on both few shot video classification and temporal detection tasks, while reaching state-of-the-art in certain scenarios. ",
    "code_link": ""
  },
  "cvpr2021_llid_damsldomainagnosticmetascore-basedlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "DAMSL: Domain Agnostic Meta Score-Based Learning",
    "authors": [
      "John Cai",
      "Bill Cai",
      "Shen Sheng Mei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Cai_DAMSL_Domain_Agnostic_Meta_Score-Based_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Cai_DAMSL_Domain_Agnostic_Meta_Score-Based_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose Domain Agnostic Meta Score-based Learning (DAMSL), a novel, versatile and highly effective solution that delivers significant out-performance over state-of-the-art methods for cross-domain few-shot learning. We identify key problems in previous meta-learning methods over-fitting to the source domain, and previous transfer-learning methods under-utilizing the structure of the support set. The core idea behind our method is that instead of directly using the scores from a fine-tuned feature encoder, we use these scores to create input coordinates for a domain agnostic metric space. A graph neural network is applied to learn an embedding and relation function over these coordinates to process all information contained in the score distribution of the support set. We test our model on both established CD-FSL benchmarks and new domains and show that our method overcomes the limitations of previous meta-learning and transfer-learning methods to deliver substantial improvements in accuracy across both smaller and larger domain shifts. ",
    "code_link": ""
  },
  "cvpr2021_llid_boostingunconstrainedfacerecognitionwithauxiliaryunlabeleddata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Boosting Unconstrained Face Recognition With Auxiliary Unlabeled Data",
    "authors": [
      "Yichun Shi",
      "Anil K. Jain"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Shi_Boosting_Unconstrained_Face_Recognition_With_Auxiliary_Unlabeled_Data_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Shi_Boosting_Unconstrained_Face_Recognition_With_Auxiliary_Unlabeled_Data_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In recent years, significant progress has been made in face recognition, which can be partially attributed to the availability of large-scale labeled face datasets. However, since the faces in these datasets usually contain limited degree and types of variation, the resulting trained models generalize poorly to more realistic unconstrained face datasets. While collecting labeled faces with larger variations could be helpful, it is practically infeasible due to privacy and labor cost. In comparison, it is easier to acquire a large number of unlabeled faces from different domains, which could be used to regularize the learning of face representations. We present an approach to use such unlabeled faces to learn generalizable face representations, where we assume neither the access to identity labels nor domain labels for unlabeled images. Experimental results on unconstrained datasets show that a small amount of unlabeled data with sufficient diversity can (i) lead to an appreciable gain in recognition performance and (ii) outperform the supervised baseline when combined with less than half of the labeled data. Compared with the state-of-the-art face recognition methods, our method further improves their performance on challenging benchmarks, such as IJB-B, IJB-C and IJB-S. ",
    "code_link": ""
  },
  "cvpr2021_llid_trainingdeepgenerativemodelsinhighlyincompletedatascenarioswithpriorregularization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Training Deep Generative Models in Highly Incomplete Data Scenarios With Prior Regularization",
    "authors": [
      "Edgar A. Bernal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Bernal_Training_Deep_Generative_Models_in_Highly_Incomplete_Data_Scenarios_With_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Bernal_Training_Deep_Generative_Models_in_Highly_Incomplete_Data_Scenarios_With_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep generative frameworks including GANs and normalizing flow models have proven successful at filling in missing values in partially observed data samples by effectively learning -either explicitly or implicitly- complex, high-dimensional statistical distributions. In tasks where the data available for learning is only partially observed, however, their performance decays monotonically as a function of the data missingness rate. In high missing data rate regimes (e.g., 60% and above), it has been observed that state-of-the-art models tend to break down and produce unrealistic and/or semantically inaccurate data. We propose a novel framework to facilitate the learning of data distributions in high paucity scenarios that is inspired by traditional formulations of solutions to ill-posed problems. The proposed framework naturally stems from posing the process of learning from incomplete data as a joint optimization task of the parameters of the model being learned and the missing data values. The method involves enforcing a prior regularization term that seamlessly integrates with objectives used to train explicit and tractable deep generative frameworks such as deep normalizing flow models. We demonstrate via extensive experimental validation that the proposed framework outperforms competing techniques, particularly as the rate of data paucity approaches unity. ",
    "code_link": ""
  },
  "cvpr2021_llid_contrastivelearningimprovesmodelrobustnessunderlabelnoise": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Contrastive Learning Improves Model Robustness Under Label Noise",
    "authors": [
      "Aritra Ghosh",
      "Andrew Lan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Ghosh_Contrastive_Learning_Improves_Model_Robustness_Under_Label_Noise_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Ghosh_Contrastive_Learning_Improves_Model_Robustness_Under_Label_Noise_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep neural network-based classifiers trained with the categorical cross-entropy (CCE) loss are sensitive to label noise in the training data. One common type of method that can mitigate the impact of label noise can be viewed as supervised robust methods; one can simply replace the CCE loss with a loss that is robust to label noise, or re-weight training samples and down-weight those with higher loss values. Recently, another type of method using semi-supervised learning (SSL) has been proposed, which augments these supervised robust methods to exploit (possibly) noisy samples more effectively. Although supervised robust methods perform well across different data types, they have been shown to be inferior to the SSL methods on image classification tasks under label noise. Therefore, it remains to be seen that whether these supervised robust methods can also perform well if they can utilize the unlabeled samples more effectively. In this paper, we show that by initializing supervised robust methods using representations learned through contrastive learning leads to significantly improved performance under label noise. Surprisingly, even the simplest method (training a classifier with the CCE loss) can outperform the state-of-the-art SSL method by more than 50% under high label noise when initialized with contrastive learning. Our implementation will be publicly available athttps://github.com/arghosh/noisy_label_pretrain . ",
    "code_link": ""
  },
  "cvpr2021_llid_balagancross-modalimagetranslationbetweenimbalanceddomains": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "BalaGAN: Cross-Modal Image Translation Between Imbalanced Domains",
    "authors": [
      "Or Patashnik",
      "Dov Danon",
      "Hao Zhang",
      "Daniel Cohen-Or"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Patashnik_BalaGAN_Cross-Modal_Image_Translation_Between_Imbalanced_Domains_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Patashnik_BalaGAN_Cross-Modal_Image_Translation_Between_Imbalanced_Domains_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " State-of-the-art image translation methods tend to struggle in an imbalanced domain setting, where one image domain lacks richness and diversity. We introduce a new unsupervised translation network, BalaGAN, specifically designed to tackle the domain imbalance problem. We leverage the latent modalities of the richer domain to turn the image-to-image translation problem, between two imbalanced domains, into a multi-class translation problem, more resembling the style transfer setting. Specifically, we analyze the source domain and learn a decomposition of it into a set of latent modes or classes, without any supervision. This leaves us with a multitude of balanced cross-domain translation tasks, between all pairs of classes, including the target domain. During inference, the trained network takes as input a source image, as well as a reference style image from one of the modes as a condition, and produces an image which resembles the source on the pixel-wise level, but shares the same mode as the reference. We show that employing modalities within the dataset improves the quality of the translated images, and that BalaGAN outperforms strong baselines of both unconditioned and style-transfer-based image-to-image translation methods, in terms of image quality and diversity. ",
    "code_link": ""
  },
  "cvpr2021_llid_improvingsemi-superviseddomainadaptationusingeffectivetargetselectionandsemantics": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Improving Semi-Supervised Domain Adaptation Using Effective Target Selection and Semantics",
    "authors": [
      "Anurag Singh",
      "Naren Doraiswamy",
      "Sawa Takamuku",
      "Megh Bhalerao",
      "Titir Dutta",
      "Soma Biswas",
      "Aditya Chepuri",
      "Balasubramanian Vengatesan",
      "Naotake Natori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Singh_Improving_Semi-Supervised_Domain_Adaptation_Using_Effective_Target_Selection_and_Semantics_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Singh_Improving_Semi-Supervised_Domain_Adaptation_Using_Effective_Target_Selection_and_Semantics_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recently, semi-supervised domain adaptation (SSDA) approaches have shown impressive performance for the domain adaptation task. They effectively utilize few labeled target samples along with the unlabeled data to account for the distribution shift across the source and target domains. In this work, we make three-fold contributions, concentrating on the role of target samples and semantics for the SSDA task. First, we observe that choosing a few, but an equal number of labeled samples from each class in the target domain requires a significant amount of manual effort. To address this, we propose an active learning-based framework by modeling both the sample diversity and the classifier uncertainty. By utilizing k-means initialized cluster centers for picking a small pool of diverse unlabeled target samples, we compute a novel classifier adaptation uncertainty term to select the most effective samples from this pool, which are queried to obtain their true labels from an oracle. Second, we propose to weigh the hard target samples more, without explicitly using their predicted, possibly incorrect labels, which guides the adaptation process. Third, we note that irrespective of the domain shift, the semantics of the classes remain unchanged, so they can be effectively utilized for this task. We show that initializing the class-representations or prototypes with the class-semantics helps in bridging the domain gap significantly. These along with adversarially learnt entropy objective results in a novel framework, termed STar (Select TARgets), which sets a new state-of-the-art for the SSDA task. ",
    "code_link": ""
  },
  "cvpr2021_llid_distillonthegoonlineknowledgedistillationinself-supervisedlearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Distill on the Go: Online Knowledge Distillation in Self-Supervised Learning",
    "authors": [
      "Prashant Bhat",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Bhat_Distill_on_the_Go_Online_Knowledge_Distillation_in_Self-Supervised_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Bhat_Distill_on_the_Go_Online_Knowledge_Distillation_in_Self-Supervised_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Self-supervised representation learning solves pretext prediction tasks that do not require labeled data to learn feature representations. For vision tasks, pretext tasks such as predicting rotation, solve jigsaw are solely created from the input data. Yet, predicting this known information helps in learning representations useful for downstream tasks. However, recent works have shown that wider and deeper models benefit more from self-supervised learning than smaller models. To address the issue of self-supervised pre-training of smaller models, we propose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using single-stage online knowledge distillation to improve the representation quality of the smaller models. We employ deep mutual learning strategy where models of different capacities collaboratively learn from each other to improve one another. Specifically, each model is trained using self-supervised learning along with a distillation loss that aligns each model's softmax probabilities of similarity scores with that of the peer model. We conduct extensive experiments on multiple benchmark datasets, learning objectives, and architectures to demonstrate the potential of our proposed method. ",
    "code_link": ""
  },
  "cvpr2021_llid_one-shotactionrecognitioninchallengingtherapyscenarios": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "One-Shot Action Recognition in Challenging Therapy Scenarios",
    "authors": [
      "Alberto Sabater",
      "Laura Santos",
      "Jose Santos-Victor",
      "Alexandre Bernardino",
      "Luis Montesano",
      "Ana C. Murillo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Sabater_One-Shot_Action_Recognition_in_Challenging_Therapy_Scenarios_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Sabater_One-Shot_Action_Recognition_in_Challenging_Therapy_Scenarios_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " One-shot action recognition aims to recognize new action categories from a single reference example, typically referred to as the anchor example. This work presents a novel approach for one-shot action recognition in the wild that computes motion representations robust to variable kinematic conditions. One-shot action recognition is then performed by evaluating anchor and target motion representations. We also develop a set of complementary steps that boost the action recognition performance in the most challenging scenarios. Our approach is evaluated on the public NTU-120 one-shot action recognition benchmark, outperforming previous action recognition models. Besides, we evaluate our framework on a real use-case of therapy with autistic people. These recordings are particularly challenging due to high-level artifacts from the patient motion. Our results provide not only quantitative but also online qualitative measures, essential for the patient evaluation and monitoring during the actual therapy. ",
    "code_link": ""
  },
  "cvpr2021_llid_boostingco-teachingwithcompressionregularizationforlabelnoise": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Boosting Co-Teaching With Compression Regularization for Label Noise",
    "authors": [
      "Yingyi Chen",
      "Xi Shen",
      "Shell Xu Hu",
      "Johan A. K. Suykens"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Chen_Boosting_Co-Teaching_With_Compression_Regularization_for_Label_Noise_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Chen_Boosting_Co-Teaching_With_Compression_Regularization_for_Label_Noise_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we study the problem of learning image classification models in the presence of label noise. We revisit a simple compression regularization named Nested Dropout. We find that Nested Dropout, though originally proposed to perform fast information retrieval and adaptive data compression, can properly regularize a neural network to combat label noise. Moreover, owing to its simplicity, it can be easily combined with Co-teaching to further boost the performance. Our final model remains simple yet effective: it achieves comparable or even better performance than the state-of-the-art approaches on two real-world datasets with label noise which are Clothing1M and ANIMAL-10N. On Clothing1M, our approach obtains 74.9% accuracy which is slightly better than that of DivideMix. On ANIMAL-10N, we achieve 84.1% accuracy while the best public result by PLC is 83.4%. We hope that our simple approach can be served as a strong baseline for learning with label noise. Our implementation is available at https://github.com/yingyichen-cyy/Nested-Co-teaching. ",
    "code_link": "https://github.com/yingyichen-cyy/Nested-Co-teaching"
  },
  "cvpr2021_llid_plmpartiallabelmaskingforimbalancedmulti-labelclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "PLM: Partial Label Masking for Imbalanced Multi-Label Classification",
    "authors": [
      "Kevin Duarte",
      "Yogesh Rawat",
      "Mubarak Shah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Duarte_PLM_Partial_Label_Masking_for_Imbalanced_Multi-Label_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Duarte_PLM_Partial_Label_Masking_for_Imbalanced_Multi-Label_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural networks trained on real-world datasets with long-tailed label distributions are biased towards frequent classes and perform poorly on infrequent classes. The imbalance in the ratio of positive and negative samples for each class skews network output probabilities further from ground-truth distributions. We propose a method, Partial Label Masking (PLM), which utilizes this ratio during training. By stochastically masking labels during loss computation, the method balances this ratio for each class, leading to improved recall on minority classes and improved precision on frequent classes. The ratio is estimated adaptively based on the network's performance by minimizing the KL divergence between predicted and ground-truth distributions. Whereas most existing approaches addressing data imbalance are mainly focused on single-label classification and do not generalize well to the multi-label case, this work proposes a general approach to solve the long-tail data imbalance issue for multi-label classification. PLM is versatile: it can be applied to most objective functions and it can be used alongside other strategies for class imbalance. Our method achieves strong performance when compared to existing methods on both multi-label (MultiMNIST and MSCOCO) and single-label (imbalanced CIFAR-10 and CIFAR-100) image classification datasets. ",
    "code_link": ""
  },
  "cvpr2021_llid_rethinkingensemble-distillationforsemanticsegmentationbasedunsuperviseddomainadaption": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Rethinking Ensemble-Distillation for Semantic Segmentation Based Unsupervised Domain Adaption",
    "authors": [
      "Chen-Hao Chao",
      "Bo-Wun Cheng",
      "Chun-Yi Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Chao_Rethinking_Ensemble-Distillation_for_Semantic_Segmentation_Based_Unsupervised_Domain_Adaption_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Chao_Rethinking_Ensemble-Distillation_for_Semantic_Segmentation_Based_Unsupervised_Domain_Adaption_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recent researches on unsupervised domain adaptation (UDA) have demonstrated that end-to-end ensemble learning frameworks serve as a compelling option for UDA tasks. Nevertheless, these end-to-end ensemble learning methods often lack flexibility as any modification to the ensemble requires retraining of their frameworks. To address this problem, we propose a flexible ensemble-distillation framework for performing semantic segmentation based UDA, allowing any arbitrary composition of the members in the ensemble while still maintaining its superior performance. To achieve such flexibility, our framework is designed to be robust against the output inconsistency and the performance variation of the members within the ensemble. To examine the effectiveness and the robustness of our method, we perform an extensive set of experiments on both GTA5 to Cityscapes and SYNTHIA to Cityscapes benchmarks to quantitatively inspect the improvements achievable by our method. We further provide detailed analyses to validate that our design choices are practical and beneficial. The experimental evidence validates that the proposed method indeed offer superior performance, robustness and flexibility in semantic segmentation based UDA tasks against contemporary baseline methods. ",
    "code_link": ""
  },
  "cvpr2021_llid_trainingrareobjectdetectioninsatelliteimagerywithsyntheticganimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Training Rare Object Detection in Satellite Imagery With Synthetic GAN Images",
    "authors": [
      "Eric Martinson",
      "Bridget Furlong",
      "Andy Gillies"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Martinson_Training_Rare_Object_Detection_in_Satellite_Imagery_With_Synthetic_GAN_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Martinson_Training_Rare_Object_Detection_in_Satellite_Imagery_With_Synthetic_GAN_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " When creating a new labeled dataset, human analysts or data reductionists must review and annotate large numbers of images. This process is time consuming and a barrier to the deployment of new computer vision solutions, particularly for rarely occurring objects. To reduce the number of images requiring human attention, we evaluate the utility of images created from 3D models refined with a generative adversarial network to select confidence thresholds that significantly reduce false alarms rates. The resulting approach has been demonstrated to cut the number of images needing to be reviewed by 50% while preserving a 95% recall rate, with only 6 labeled examples of the target. ",
    "code_link": "https://github.com/yhenon/pytorch-retinanet"
  },
  "cvpr2021_llid_shotinthedarkfew-shotlearningwithnobase-classlabels": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Shot in the Dark: Few-Shot Learning With No Base-Class Labels",
    "authors": [
      "Zitian Chen",
      "Subhransu Maji",
      "Erik Learned-Miller"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Chen_Shot_in_the_Dark_Few-Shot_Learning_With_No_Base-Class_Labels_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Chen_Shot_in_the_Dark_Few-Shot_Learning_With_No_Base-Class_Labels_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Few-shot learning aims to build classifiers for new classes from a small number of labeled examples and is commonly facilitated by access to examples from a distinct set of 'base classes'. The difference in data distribution between the test set(novel classes) and the base classes used to learn an inductive bias often results in poor generalization on the novel classes. To alleviate problems caused by the distribution shift, previous research has explored the use of unlabeled examples from the novel classes, in addition to labeled examples of the base classes, which is known as the transductive setting. In this work, we show that, surprisingly, off-the-shelf self-supervised learn-ing outperforms transductive few-shot methods by 3.9% for 5-shot accuracy onminiImageNetwithout using any base class labels. This motivates us to examine more carefully the role of features learned through self-supervision in few-shot learning. Comprehensive experiments are conducted to compare the transferability, robustness, efficiency, and the complementarity of supervised and self-supervised features. ",
    "code_link": ""
  },
  "cvpr2021_llid_one-shotganlearningtogeneratesamplesfromsingleimagesandvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "One-Shot GAN: Learning To Generate Samples From Single Images and Videos",
    "authors": [
      "Vadim Sushko",
      "Jurgen Gall",
      "Anna Khoreva"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Sushko_One-Shot_GAN_Learning_To_Generate_Samples_From_Single_Images_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Sushko_One-Shot_GAN_Learning_To_Generate_Samples_From_Single_Images_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Training GANs in low-data regimes remains a challenge, as overfitting often leads to memorization or training divergence. In this work, we introduce One-Shot GAN that can learn to generate samples from a training set as little as one image or one video. We propose a two-branch discriminator, with content and layout branches designed to judge the internal content separately from the scene layout realism. This allows synthesis of visually plausible, novel compositions of a scene, with varying content and layout, while preserving the context of the original sample. Compared to previous single-image GAN models, One-Shot GAN achieves higher diversity and quality of synthesis. It is also not restricted to the single image setting, successfully learning in the introduced setting of a single video. ",
    "code_link": ""
  },
  "cvpr2021_llid_unlockingthefullpotentialofsmalldatawithdiversesupervision": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "LLID",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Learning From Limited or Imperfect Data",
    "title": "Unlocking the Full Potential of Small Data With Diverse Supervision",
    "authors": [
      "Ziqi Pang",
      "Zhiyuan Hu",
      "Pavel Tokmakov",
      "Yu-Xiong Wang",
      "Martial Hebert"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/html/Pang_Unlocking_the_Full_Potential_of_Small_Data_With_Diverse_Supervision_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Pang_Unlocking_the_Full_Potential_of_Small_Data_With_Diverse_Supervision_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Virtually all of deep learning literature relies on the assumption of large amounts of available training data. Indeed, even the majority of few-shot learning methods rely on a large set of \"base classes\" for pre-training. This assumption, however, does not always hold. For some tasks, annotating a large number of classes can be infeasible, and even collecting the images themselves can be a challenge in some scenarios. In this paper, we study this problem and call it \"Small Data\"' setting, in contrast to \"Big Data.\" To unlock the full potential of small data, we propose to augment the models with annotations for other related tasks, thus increasing their generalization abilities. In particular, we use the richly annotated scene parsing dataset ADE20K to construct our realistic Long-tail Recognition with Diverse Supervision (LRDS) benchmark, by splitting the object categories into head and tail based on their distribution. Following the standard few-shot learning protocol, we use the head classes for representation learning and the tail classes for evaluation. Moreover, we further subsample the head categories and images to generate two novel settings which we call Scarce-Class\" and \"Scarce-Image,\" respectively corresponding to the shortage of training classes and images. Finally, we analyze the effect of applying various additional supervision sources under the proposed settings. Our experiments demonstrate that densely labeling a small set of images can indeed largely remedy the small data constraints. Our code and benchmark are available at https://github.com/BinahHu/ADE-FewShot. ",
    "code_link": "https://github.com/BinahHu/ADE-FewShot"
  },
  "cvpr2021_wad_rethinkingofradarsroleacamera-radardatasetandsystematicannotatorviacoordinatealignment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Rethinking of Radar's Role: A Camera-Radar Dataset and Systematic Annotator via Coordinate Alignment",
    "authors": [
      "Yizhou Wang",
      "Gaoang Wang",
      "Hung-Min Hsu",
      "Hui Liu",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Wang_Rethinking_of_Radars_Role_A_Camera-Radar_Dataset_and_Systematic_Annotator_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Wang_Rethinking_of_Radars_Role_A_Camera-Radar_Dataset_and_Systematic_Annotator_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Radar has long been a common sensor on autonomous vehicles for obstacle ranging and speed estimation. However, as a robust sensor to all-weather conditions, radar's capability has not been well-exploited, compared with camera or LiDAR. Instead of just serving as a supplementary sensor, radar's rich information hidden in the radio frequencies can potentially provide useful clues to achieve more complicated tasks, like object classification and detection. In this paper, we propose a new dataset, named CRUW, with a systematic annotator and performance evaluation system to address the radar object detection (ROD) task, which aims to classify and localize the objects in 3D purely from radar's radio frequency (RF) images. To the best of our knowledge, CRUW is the first public large-scale dataset with a systematic annotation and evaluation system, which involves camera RGB images and radar RF images, collected in various driving scenarios. ",
    "code_link": ""
  },
  "cvpr2021_wad_videoclassagnosticsegmentationbenchmarkforautonomousdriving": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Video Class Agnostic Segmentation Benchmark for Autonomous Driving",
    "authors": [
      "Mennatullah Siam",
      "Alex Kendall",
      "Martin Jagersand"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Siam_Video_Class_Agnostic_Segmentation_Benchmark_for_Autonomous_Driving_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Siam_Video_Class_Agnostic_Segmentation_Benchmark_for_Autonomous_Driving_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Semantic segmentation approaches are typically trained on large-scale data with a closed finite set of known classes without considering unknown objects. In certain safety-critical robotics applications, especially autonomous driving, it is important to segment all objects, including those unknown at training time. We formalize the task of video class agnostic segmentation from monocular video sequences in autonomous driving to account for unknown objects. Video class agnostic segmentation can be formulated as an open-set or a motion segmentation problem. We discuss both formulations and provide datasets and benchmark different baseline approaches for both tracks. In the motion-segmentation track we benchmark real-time joint panoptic and motion instance segmentation, and evaluate the effect of ego-flow suppression. In the open-set segmentation track we evaluate baseline methods that combine appearance, and geometry to learn prototypes per semantic class. We then compare it to a model that uses an auxiliary contrastive loss to improve the discrimination between known and unknown objects. Datasets and models are publicly released at https://msiam.github.io/vca/. ",
    "code_link": ""
  },
  "cvpr2021_wad_lccnetlidarandcameraself-calibrationusingcostvolumenetwork": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "LCCNet: LiDAR and Camera Self-Calibration Using Cost Volume Network",
    "authors": [
      "Xudong Lv",
      "Boya Wang",
      "Ziwen Dou",
      "Dong Ye",
      "Shuo Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Lv_LCCNet_LiDAR_and_Camera_Self-Calibration_Using_Cost_Volume_Network_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Lv_LCCNet_LiDAR_and_Camera_Self-Calibration_Using_Cost_Volume_Network_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-sensor fusion is for enhancing environment perception and 3D reconstruction in self-driving and robot navigation. Calibration between sensors is the precondition of effective multi-sensor fusion. Laborious manual works and complex environment settings exist in old-fashioned calibration techniques for Light Detection and Ranging (LiDAR) and camera. We propose an online LiDAR-Camera Self-calibration Network (LCCNet), different from the previous CNN-based methods. LCCNet can be trained end-to-end and predict the extrinsic parameters in real-time. In the LCCNet, we exploit the cost volume layer to express the correlation between the RGB image features and the depth image projected from point clouds. Besides using the smooth L1-Loss of the predicted extrinsic calibration parameters as a supervised signal, an additional self-supervised signal, point cloud distance loss, is applied during training. Instead of directly regressing the extrinsic parameters, we predict the decalibrated deviation from initial calibration to the ground truth. The calibration error decreases further with iterative refinement and the temporal filtering approach in the inference stage. The execution time of the calibration process is 24ms for each iteration on a single GPU. LCCNet achieves a mean absolute calibration error of 0.297cm in translation and 0.017deg in rotation with miscalibration magnitudes of up to +-1.5m and +-20 on the KITTI-odometry dataset, which is better than the state-of-the-art CNN-based calibration methods. The code will be publicly available at https://github.com/LvXudong-HIT/LCCNet ",
    "code_link": "https://github.com/LvXudong-HIT/LCCNet"
  },
  "cvpr2021_wad_softcrossentropylossandbottlenecktri-costvolumeforefficientstereodepthprediction": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Soft Cross Entropy Loss and Bottleneck Tri-Cost Volume for Efficient Stereo Depth Prediction",
    "authors": [
      "Tyler Nuanes",
      "Matt Elsey",
      "Aswin Sankaranarayanan",
      "John Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Nuanes_Soft_Cross_Entropy_Loss_and_Bottleneck_Tri-Cost_Volume_for_Efficient_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Nuanes_Soft_Cross_Entropy_Loss_and_Bottleneck_Tri-Cost_Volume_for_Efficient_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Real-time, robust, and accurate stereo depth-prediction algorithms deliver cutting-edge performance in applications ranging from autonomous driving to augmented reality. Many state-of-the-art approaches produce subpixel error and subsecond runtimes on commodity hardware, but improving even these remains an area of active research. We focus on improving accuracy and efficiency in stereo-based depth prediction by contributing two generic techniques to improve performance and runtime. First, we propose encoding the ground truth disparity as a discrete distribution that can be trained via cross-entropy loss. Specifically, we use the minimum variance and unbiased 'Soft' encoding, where two adjacent bins are weighted so the expected value is ground truth. We demonstrate that training with cross entropy loss using this encoding decreases error rate by 10% on synthetic and LIDAR datasets over the more popular regression losses such as Huber and MAE. Second, we propose a bottleneck tri-cost volume composed of the sum of absolute difference of the features as well as two reference channels. Replacing the standard 64-channel concatenation popular in state-of-the-art networks with this 3-channel cost-volume maintains metric performance and can reduce runtime by over 22% on PSM-Net architectures. ",
    "code_link": ""
  },
  "cvpr2021_wad_latentspaceregularizationforunsuperviseddomainadaptationinsemanticsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Latent Space Regularization for Unsupervised Domain Adaptation in Semantic Segmentation",
    "authors": [
      "Francesco Barbato",
      "Marco Toldo",
      "Umberto Michieli",
      "Pietro Zanuttigh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Barbato_Latent_Space_Regularization_for_Unsupervised_Domain_Adaptation_in_Semantic_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Barbato_Latent_Space_Regularization_for_Unsupervised_Domain_Adaptation_in_Semantic_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep convolutional neural networks for semantic segmentation achieve outstanding accuracy, however they also have a couple of major drawbacks: first, they do not generalize well to distributions slightly different from the one of the training data; second, they require a huge amount of labeled data for their optimization. In this paper, we introduce feature-level space-shaping regularization strategies to reduce the domain discrepancy in semantic segmentation. In particular, for this purpose we jointly enforce a clustering objective, a perpendicularity constraint and a norm alignment goal on the feature vectors corresponding to source and target samples. Additionally, we propose a novel measure able to capture the relative efficacy of an adaptation strategy compared to supervised training. We verify the effectiveness of such methods in the autonomous driving setting achieving state-of-the-art results in multiple synthetic-to-real road scenes benchmarks. ",
    "code_link": ""
  },
  "cvpr2021_wad_accurate3dobjectdetectionusingenergy-basedmodels": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Accurate 3D Object Detection Using Energy-Based Models",
    "authors": [
      "Fredrik K. Gustafsson",
      "Martin Danelljan",
      "Thomas B. Schon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Gustafsson_Accurate_3D_Object_Detection_Using_Energy-Based_Models_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Gustafsson_Accurate_3D_Object_Detection_Using_Energy-Based_Models_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Accurate 3D object detection (3DOD) is crucial for safe navigation of complex environments by autonomous robots. Regressing accurate 3D bounding boxes in cluttered environments based on sparse LiDAR data is however a highly challenging problem. We address this task by exploring recent advances in conditional energy-based models (EBMs) for probabilistic regression. While methods employing EBMs for regression have demonstrated impressive performance on 2D object detection in images, these techniques are not directly applicable to 3D bounding boxes. In this work, we therefore design a differentiable pooling operator for 3D bounding boxes, serving as the core module of our EBM network. We further integrate this general approach into the state-of-the-art 3D object detector SA-SSD. On the KITTI dataset, our proposed approach consistently outperforms the SA-SSD baseline across all 3DOD metrics, demonstrating the potential of EBM-based regression for highly accurate 3DOD. ",
    "code_link": ""
  },
  "cvpr2021_wad_radrealtimeandaccurate3dobjectdetectiononembeddedsystems": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "RAD: Realtime and Accurate 3D Object Detection on Embedded Systems",
    "authors": [
      "Hamed H. Aghdam",
      "Elnaz J. Heravi",
      "Selameab S. Demilew",
      "Robert Laganiere"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Aghdam_RAD_Realtime_and_Accurate_3D_Object_Detection_on_Embedded_Systems_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Aghdam_RAD_Realtime_and_Accurate_3D_Object_Detection_on_Embedded_Systems_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " To our knowledge, the fastest 3D object detector on LiDAR data works at 42.03 point clouds-per-second on high-end machines and 6.15 point clouds-per-second on embedded boards. We propose a deep 3D object detector with higher detection accuracy running at 84.46 point clouds-per-second on high-end machines and 10.91 point clouds-per-second on computing boards that is 2 and 1.77 times faster compared to fastest published network. We achieve considerably higher processing rate without reducing the complexity of the network but by designing a more efficient decoder. Our extensive and practical experiments reveal that the detection accuracy of our proposed network is comparable to the best-performed method using practical metrics but it is 3.36 times faster. Besides, we carefully analyze the model and indicate that negligible error in two regression outputs contributes to the reduction in the average precision. Overall, considering the accuracy and speed, our proposed network is highly practical to be executed on embedded boards for ADAS applications. ",
    "code_link": ""
  },
  "cvpr2021_wad_multi-tasklearningwithattentionforend-to-endautonomousdriving": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Multi-Task Learning With Attention for End-to-End Autonomous Driving",
    "authors": [
      "Keishi Ishihara",
      "Anssi Kanervisto",
      "Jun Miura",
      "Ville Hautamaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Ishihara_Multi-Task_Learning_With_Attention_for_End-to-End_Autonomous_Driving_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Ishihara_Multi-Task_Learning_With_Attention_for_End-to-End_Autonomous_Driving_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Autonomous driving systems need to handle complex scenarios such as lane following, avoiding collisions, taking turns, and responding to traffic signals. In recent years, approaches based on end-to-end behavioral cloning have demonstrated remarkable performance in point-to-point navigational scenarios, using a realistic simulator and standard benchmarks. Offline imitation learning is readily available, as it does not require expensive hand annotation or interaction with the target environment, but it is difficult to obtain a reliable system. In addition, existing methods have not specifically addressed the learning of reaction for traffic lights, which are a rare occurrence in the training datasets. Inspired by the previous work on multi-task learning and attention modeling, we propose a novel multi-task attention-aware network in the conditional imitation learning (CIL) framework. This does not only improve the success rate of standard benchmarks, but also the ability to react to traffic lights, which we show with standard benchmarks. ",
    "code_link": ""
  },
  "cvpr2021_wad_occlusionguidedsceneflowestimationon3dpointclouds": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Occlusion Guided Scene Flow Estimation on 3D Point Clouds",
    "authors": [
      "Bojun Ouyang",
      "Dan Raviv"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Ouyang_Occlusion_Guided_Scene_Flow_Estimation_on_3D_Point_Clouds_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Ouyang_Occlusion_Guided_Scene_Flow_Estimation_on_3D_Point_Clouds_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " 3D scene flow estimation is a vital tool in perceiving our environment given depth or range sensors. Unlike optical flow, the data is usually sparse and in most cases partially occluded in between two temporal samplings. Here we propose a new scene flow architecture called OGSF-Net which tightly couples the learning for both flow and occlusions between frames. Their coupled symbiosis results in a more accurate prediction of flow in space. Unlike a traditional multi-action network, our unified approach is fused throughout the network, boosting performances for both occlusion detection and flow estimation. Our architecture is the first to gauge the occlusion in 3D scene flow estimation on point clouds. In key datasets such as Flyingthings3D and KITTI, we achieve the state-of-the-art results. ",
    "code_link": "https://github.com/BillOuyang/OGSFNet.git"
  },
  "cvpr2021_wad_semi-synthesisafastwaytoproduceeffectivedatasetsforstereomatching": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "Semi-Synthesis: A Fast Way To Produce Effective Datasets for Stereo Matching",
    "authors": [
      "Ju He",
      "Enyu Zhou",
      "Liusheng Sun",
      "Fei Lei",
      "Chenyang Liu",
      "Wenxiu Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/He_Semi-Synthesis_A_Fast_Way_To_Produce_Effective_Datasets_for_Stereo_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/He_Semi-Synthesis_A_Fast_Way_To_Produce_Effective_Datasets_for_Stereo_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Stereo matching is an important problem in computer vision which has drawn tremendous research attention for decades. Recent years, data-driven methods with convolutional neural networks (CNNs) are continuously pushing stereo matching to new heights. However, data-driven methods require large amount of training data, which is not an easy task for real stereo data due to the annotation difficulties of per-pixel ground-truth disparity. Though synthetic dataset is proposed to fill the gaps of large data demand, the fine-tuning on real dataset is still needed due to the domain variances between synthetic data and real data. In this paper, we found that in synthetic datasets, close-to-real-scene texture rendering is a key factor to boost up stereo matching performance, while close-to-real-scene 3D modeling is less important. We then propose semi-synthetic, an effective and fast way to synthesize large amount of data with close-to-real-scene texture to minimize the gap between synthetic data and real data. Extensive experiments demonstrate that models trained with our proposed semi-synthetic datasets achieve significantly better performance than with general synthetic datasets, especially on real data benchmarks with limited training data. With further fine-tuning on the real dataset, we also achieve SOTA performance on Middlebury and competitive results on KITTI and ETH3D datasets. ",
    "code_link": ""
  },
  "cvpr2021_wad_mvfusenetimprovingend-to-endobjectdetectionandmotionforecastingthroughmulti-viewfusionoflidardata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "WAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Autonomous Driving",
    "title": "MVFuseNet: Improving End-to-End Object Detection and Motion Forecasting Through Multi-View Fusion of LiDAR Data",
    "authors": [
      "Ankit Laddha",
      "Shivam Gautam",
      "Stefan Palombo",
      "Shreyash Pandey",
      "Carlos Vallespi-Gonzalez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Laddha_MVFuseNet_Improving_End-to-End_Object_Detection_and_Motion_Forecasting_Through_Multi-View_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Laddha_MVFuseNet_Improving_End-to-End_Object_Detection_and_Motion_Forecasting_Through_Multi-View_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work, we propose MVFuseNet, a novel end-to-end method for joint object detection and motion forecasting from a temporal sequence of LiDAR data. Most existing methods operate in a singular view by projecting data in either range view (RV) or bird's eye view (BEV). In contrast, we propose a method to successfully leverage the complementary strengths of both views. We accomplish this by proposing a novel method to effectively utilize both RV and BEV for spatio-temporal feature learning as part of a temporal fusion network, as well as for multi-scale feature learning in the backbone network. Further, we propose a novel sequential fusion approach that effectively utilizes multiple views in the temporal fusion network. We show the benefits of our novel multi-view approach for the tasks of detection and motion forecasting on two large-scale self-driving data sets, achieving state-of-the-art results. Furthermore, we show the scalability of MVFuseNet with respect to increased operating range, by demonstrating real-time performance. ",
    "code_link": ""
  },
  "cvpr2021_agrivision_phenologyalignmentnetworkanovelframeworkforcross-regionaltimeseriescropclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Phenology Alignment Network: A Novel Framework for Cross-Regional Time Series Crop Classification",
    "authors": [
      "Ziqiao Wang",
      "Hongyan Zhang",
      "Wei He",
      "Liangpei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/html/Wang_Phenology_Alignment_Network_A_Novel_Framework_for_Cross-Regional_Time_Series_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/papers/Wang_Phenology_Alignment_Network_A_Novel_Framework_for_Cross-Regional_Time_Series_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Timely and accurate crop type classification plays an essential role in the study of agricultural application. However, large area or cross-regional crop classification confronts huge challenges owing to dramatic phenology discrepancy among training and test regions. In this work, we propose a novel framework to address these challenges based on deep recurrent network and unsupervised domain adaptation (DA). Specifically, we firstly propose a Temporal Spatial Network (TSNet) for pixelwise crop classification, which contains stacked RNN and self-attention module to adaptively extract multi-level features from crop samples under various planting conditions. To deal with the cross-regional challenge, an unsupervised DA-based framework named Phenology Alignment Network (PAN) is proposed. PAN consists of two branches of two identical TSNet pre-trained on source domain; one branch takes source samples while the other takes target samples as input. Through aligning the hierarchical deep features extracted from two branches, the discrepancy between two regions is decreased and the pre-trained model is adapted to the target domain without using target label information. As another contribution, a time series dataset based on Sentinel-2 was annotated containing winter crop samples collected on three study sites of China. Cross-regional experiments demonstrate that TSNet shows comparable accuracy to state-of-the-art methods, and PAN further improves the overall accuracy by 5.62%, and macro average F1 score by 0.094 unsupervisedly. ",
    "code_link": ""
  },
  "cvpr2021_agrivision_multi-resolutionoutlierpoolingforsorghumclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Multi-Resolution Outlier Pooling for Sorghum Classification",
    "authors": [
      "Chao Ren",
      "Justin Dulay",
      "Gregory Rolwes",
      "Duke Pauli",
      "Nadia Shakoor",
      "Abby Stylianou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/html/Ren_Multi-Resolution_Outlier_Pooling_for_Sorghum_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/papers/Ren_Multi-Resolution_Outlier_Pooling_for_Sorghum_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Automated high throughput plant phenotyping involves leveraging sensors, such as RGB, thermal and hyperspectral cameras (among others), to make large scale and rapid measurements of the physical properties of plants for the purpose of better understanding the difference between crops and facilitating rapid plant breeding programs. One of the most basic phenotyping tasks is to determine the cultivar, or species, in a particular sensor product. This simple phenotype can be used to detect errors in planting and to learn the most differentiating features between cultivars. It is also a challenging visual recognition task, as a large number of highly related crops are grown simultaneously, leading to a classification problem with low inter-class variance. In this paper, we introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum captured by a state-of-the-art gantry system, a multi-resolution network architecture that learns both global and fine-grained features on the crops, and a new global pooling strategy called Dynamic Outlier Pooling which outperforms standard global pooling strategies on this task. ",
    "code_link": ""
  },
  "cvpr2021_agrivision_superpixelsandgraphconvolutionalneuralnetworksforefficientdetectionofnutrientdeficiencystressfromaerialimagery": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Superpixels and Graph Convolutional Neural Networks for Efficient Detection of Nutrient Deficiency Stress From Aerial Imagery",
    "authors": [
      "Saba Dadsetan",
      "David Pichler",
      "David Wilson",
      "Naira Hovakimyan",
      "Jennifer Hobbs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/html/Dadsetan_Superpixels_and_Graph_Convolutional_Neural_Networks_for_Efficient_Detection_of_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/papers/Dadsetan_Superpixels_and_Graph_Convolutional_Neural_Networks_for_Efficient_Detection_of_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Advances in remote sensing technology have led to the capture of massive amounts of data. Increased image resolution, more frequent revisit times, and additional spectral channels have created an explosion in the amount of data that is available to provide analyses and intelligence across domains, including agriculture. However, the processing of this data comes with a cost in terms of computation time and money, both of which must be considered when the goal of an algorithm is to provide real-time intelligence to improve efficiencies. Specifically, we seek to identify nutrient deficient areas from remotely sensed data to alert farmers to regions that require attention; detection of nutrient deficient areas is a key task in precision agriculture as farmers must quickly respond to struggling areas to protect their harvests. Past methods have focused on pixel-level classification (i.e. semantic segmentation) of the field to achieve these tasks, often using deep learning models with tens-of-millions of parameters. In contrast, we propose a much lighter graph-based method to perform node-based classification. We first use Simple Linear Iterative Cluster (SLIC) to produce superpixels across the field. Then, to perform segmentation across the non-Euclidean domain of superpixels, we leverage a Graph Convolutional Neural Network (GCN). This model has 4-orders-of-magnitude fewer parameters than a CNN model and trains in a matter of minutes. ",
    "code_link": ""
  },
  "cvpr2021_agrivision_towardscomputervisionanddeeplearningfacilitatedpollinationmonitoringforagriculture": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Towards Computer Vision and Deep Learning Facilitated Pollination Monitoring for Agriculture",
    "authors": [
      "Malika Nisal Ratnayake",
      "Adrian G. Dyer",
      "Alan Dorin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/html/Ratnayake_Towards_Computer_Vision_and_Deep_Learning_Facilitated_Pollination_Monitoring_for_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/papers/Ratnayake_Towards_Computer_Vision_and_Deep_Learning_Facilitated_Pollination_Monitoring_for_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Globally, pollinators affect 35% of agricultural land and play a key role in food production. Consequently, monitoring is useful to understand the contribution insects make towards crop pollination. Traditional sampling techniques used in insect monitoring have several drawbacks, including that they are labour intensive and potentially unreliable. Some of these drawbacks may be overcome using computer vision and deep learning-based approaches to automate pollination monitoring. In this paper, we present a pipeline for computer vision-based pollination monitoring and propose a novel algorithm, Polytrack, that tracks multiple insects simultaneously in complex agricultural environments. Our algorithm uses deep learning and foreground/background segmentation to detect and track insects. We achieved precision and recall rates of 0.975 and 0.972 respectively when monitoring honeybees foraging in our test sites within the polytunnels of an industrial strawberry farm. Polytrack includes a flower identification module to automate collection of insect-flower interaction data, and a low-resolution processing mode that reduces computational demands placed on the processor to bring the software towards the requirements of low-powered monitoring hardware. ",
    "code_link": ""
  },
  "cvpr2021_agrivision_determiningdendrometryusingdronescouting,convolutionalneuralnetworksandpointclouds": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Determining Dendrometry Using Drone Scouting, Convolutional Neural Networks and Point Clouds",
    "authors": [
      "Kim Jensen",
      "Oskar Kondrup Krogh",
      "Marius Willemoes Jorgensen",
      "Daniel Lehotsky",
      "Anton Bock Andersen",
      "Ernest Porqueras",
      "Jens Aksel S. Sondergaard",
      "Rikke Gade"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/html/Jensen_Determining_Dendrometry_Using_Drone_Scouting_Convolutional_Neural_Networks_and_Point_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/papers/Jensen_Determining_Dendrometry_Using_Drone_Scouting_Convolutional_Neural_Networks_and_Point_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents a solution for mapping the location of trees in an orchard and estimating the dendrometric data of the trees. The combined solution consists of a mapping and navigation algorithm, which allows for autonomous data collection at an orchard with a regular rectangular layout, and data processing for tree detection and dendrometric data estimation. The data collection is done using an Intel RealSense D435i camera, which can obtain both RGB and depth data. The paper presents a comparison between the performance of point cloud processing (PCP) and convolutional neural networks (CNNs) on RGB data for tree detection and dendrometric data estimation. The YOLOv3 CNN achieved a mAP50 of 63.53% with 65.5 FPS and a mean error of 20.6 cm in height estimation. Point cloud processing achieved a precision of 76.72% with 2.1 FPS and a mean error of 20.4 cm in height estimation. In conclusion, this work shows that point cloud processing shows comparable results to convolutional neural networks for height estimation, but trades off processing time for better precision in detection. ",
    "code_link": "https://github.com/AlexeyAB/darknet"
  },
  "cvpr2021_agrivision_fuse-pnanovelarchitectureforanomalypatternsegmentationinaerialagriculturalimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AgriVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Fuse-PN: A Novel Architecture for Anomaly Pattern Segmentation in Aerial Agricultural Images",
    "authors": [
      "Shubham Innani",
      "Prasad Dutande",
      "Bhakti Baheti",
      "Sanjay Talbar",
      "Ujjwal Baid"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/html/Innani_Fuse-PN_A_Novel_Architecture_for_Anomaly_Pattern_Segmentation_in_Aerial_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AgriVision/papers/Innani_Fuse-PN_A_Novel_Architecture_for_Anomaly_Pattern_Segmentation_in_Aerial_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep learning and pattern recognition in smart farming has seen rapid growth as a building bridge between crop science and computer vision. One of the important application is anomaly segmentation in agriculture like weed, standing water, cloud shadow, etc. Our research work focuses on aerial farmland image dataset known as Agriculture Vision.We propose to have data fusion of R, G, B, and NIR modalities that enhances the feature extraction and also propose Efficient Fused Pyramid Network (Fuse-PN) for anomaly pattern segmentation. The proposed encoder module is a bottom-up pathway having a compound scaled network and decoder module is a topdown pyramid network enhancing features at different scales having rich semantic features with lateral connections of low level features. This proposed approach achieved a mean dice similarity score of 0.8271 for six agricultural anomaly patterns of Agriculture Vision dataset and outperforms various approaches in literature. ",
    "code_link": ""
  },
  "cvpr2021_jrdb_learningtodetectphone-relatedpedestriandistractedbehaviorswithsyntheticdata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "JRDB",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Visual Perception for Navigation in Human Environments: The JackRabbot Social Grouping and Activity Dataset and Benchmark",
    "title": "Learning To Detect Phone-Related Pedestrian Distracted Behaviors With Synthetic Data",
    "authors": [
      "Emre Hatay",
      "Jin Ma",
      "Huiming Sun",
      "Jianwu Fang",
      "Zhiqiang Gao",
      "Hongkai Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/JRDB/html/Hatay_Learning_To_Detect_Phone-Related_Pedestrian_Distracted_Behaviors_With_Synthetic_Data_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/JRDB/papers/Hatay_Learning_To_Detect_Phone-Related_Pedestrian_Distracted_Behaviors_With_Synthetic_Data_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Due to the popularity and mobility of smart phones, phone-related pedestrian distracted behaviors, e.g., Texting, Game Playing, and Phone calls, have caused many traffic fatalities and accidents. As an advanced driver-assistance or autonomous-driving system, computer vision could be used to automatically detect distractions from cameras installed on the vehicle for useful safety intervention. The state-of-the-art method models this problem as a standard supervised learning method with a two-branch Convolutional Neural Network (CNN) followed by a voting on all image frames. In contrast, this paper proposes a new synthetic dataset named SYN-PPDB (448 synchronized video pairs of 53,760 computer game images) for this research problem and models it as a transfer learning problem from synthetic data to real data. A new deep learning model embedded with spatial-temporal feature learning and pose-aware transfer learning is proposed. Experimental results show that we could improve the state-of-the-art overall recognition accuracy from 84.27% to 96.67%. ",
    "code_link": ""
  },
  "cvpr2021_jrdb_knowyoursurroundingspanoramicmulti-objecttrackingbymultimodalitycollaboration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "JRDB",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Visual Perception for Navigation in Human Environments: The JackRabbot Social Grouping and Activity Dataset and Benchmark",
    "title": "Know Your Surroundings: Panoramic Multi-Object Tracking by Multimodality Collaboration",
    "authors": [
      "Yuhang He",
      "Wentao Yu",
      "Jie Han",
      "Xing Wei",
      "Xiaopeng Hong",
      "Yihong Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/JRDB/html/He_Know_Your_Surroundings_Panoramic_Multi-Object_Tracking_by_Multimodality_Collaboration_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/JRDB/papers/He_Know_Your_Surroundings_Panoramic_Multi-Object_Tracking_by_Multimodality_Collaboration_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we focus on the multi-object tracking (MOT) problem of automatic driving and robot navigation. Most existing MOT methods track multiple objects using a singular RGB camera, which are prone to camera field-of-view and suffer tracking failures in complex scenarios due to background clutters and poor light conditions. To meet these challenges, we propose a MultiModality PAnoramic multi-object Tracking framework (MMPAT), which takes both 2D panorama images and 3D point clouds as input and then infers target trajectories using the multimodality data. The proposed method contains four major modules, a panorama image detection module, a multimodality data fusion module, a data association module and a trajectory inference model. We evaluate the proposed method on the JRDB dataset, where the MMPAT achieves the top performance in both the detection and tracking tasks and significantly outperforms state-of-the-art methods by a large margin (15.7 and 8.5 improvement in terms of AP and MOTA, respectively). ",
    "code_link": ""
  },
  "cvpr2021_ecv_networkspacesearchforpareto-efficientspaces": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Network Space Search for Pareto-Efficient Spaces",
    "authors": [
      "Min-Fong Hong",
      "Hao-Yun Chen",
      "Min-Hung Chen",
      "Yu-Syuan Xu",
      "Hsien-Kai Kuo",
      "Yi-Min Tsai",
      "Hung-Jen Chen",
      "Kevin Jou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Hong_Network_Space_Search_for_Pareto-Efficient_Spaces_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Hong_Network_Space_Search_for_Pareto-Efficient_Spaces_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Network spaces have been known as a critical factor in both handcrafted network designs or defining search spaces for Neural Architecture Search (NAS). However, an effective space involves tremendous prior knowledge and/or manual effort, and additional constraints are required to discover efficiency-aware architectures. In this paper, we define a new problem, Network Space Search (NSS), as searching for favorable network spaces instead of a single architecture. We propose an NSS method to directly search for efficient-aware network spaces automatically, reducing the manual effort and immense cost in discovering satisfactory ones. The resultant network spaces, named Elite Spaces, are discovered from Expanded Search Space with minimal human expertise imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front under various complexity constraints and can be further served as NAS search spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an averagely 2.3% lower error rate and 3.7% closer to target constraint than the baseline with around 90% fewer samples required to find satisfactory networks). Moreover, our NSS approach is capable of searching for superior spaces in future unexplored spaces, revealing great potential in searching for network spaces automatically. ",
    "code_link": ""
  },
  "cvpr2021_ecv_alpsadaptivequantizationofdeepneuralnetworkswithgeneralizedposits": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "ALPS: Adaptive Quantization of Deep Neural Networks With GeneraLized PositS",
    "authors": [
      "Hamed F. Langroudi",
      "Vedant Karia",
      "Zachariah Carmichael",
      "Abdullah Zyarah",
      "Tej Pandit",
      "John L. Gustafson",
      "Dhireesha Kudithipudi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Langroudi_ALPS_Adaptive_Quantization_of_Deep_Neural_Networks_With_GeneraLized_PositS_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Langroudi_ALPS_Adaptive_Quantization_of_Deep_Neural_Networks_With_GeneraLized_PositS_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, a new adaptive quantization algorithm for generalized posit format is presented, to optimally represent the dynamic range and distribution of deep neural network parameters. Adaptation is achieved by minimizing the intra-layer posit quantization error with a compander. The efficacy of the proposed quantization algorithm is studied within a new low-precision framework, ALPS, on ResNet-50 and EfficientNet models for classification tasks. Results assert that the accuracy and energy dissipation of low-precision DNNs using generalized posits outperform other well-known numerical formats, including standard posits. ",
    "code_link": ""
  },
  "cvpr2021_ecv_isin-domaindatareallyneeded?apilotstudyoncross-domaincalibrationfornetworkquantization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization",
    "authors": [
      "Haichao Yu",
      "Linjie Yang",
      "Humphrey Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Yu_Is_In-Domain_Data_Really_Needed_A_Pilot_Study_on_Cross-Domain_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Yu_Is_In-Domain_Data_Really_Needed_A_Pilot_Study_on_Cross-Domain_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Post-training quantization methods use a set of calibration data to compute quantization ranges for network parameters and activations. The calibration data usually comes from the training dataset which could be inaccessible due to sensitivity of the data. In this work, we want to study such a problem: can we use out-of-domain data to calibrate the trained networks without knowledge of the original dataset? Specifically, we go beyond the domain of natural images to include drastically different domains such as X-ray images, satellite images and ultrasound images. We find cross-domain calibration leads to surprisingly stable performance of quantized models on 10 tasks in different image domains with 13 different calibration datasets. We also find that the performance of quantized models is correlated with the similarity of the Gram matrices between the source and calibration domains, which can be used as a criterion to choose calibration set for better performance. We believe our research opens the door to borrow cross-domain knowledge for network quantization and compression. ",
    "code_link": ""
  },
  "cvpr2021_ecv_generativezero-shotnetworkquantization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Generative Zero-Shot Network Quantization",
    "authors": [
      "Xiangyu He",
      "Jiahao Lu",
      "Weixiang Xu",
      "Qinghao Hu",
      "Peisong Wang",
      "Jian Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/He_Generative_Zero-Shot_Network_Quantization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/He_Generative_Zero-Shot_Network_Quantization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Convolutional neural networks are able to learn realistic image priors from numerous training samples in low-level image generation and restoration. We show that, for high-level image recognition tasks, we can further reconstruct \"realistic\" images of each category by leveraging intrinsic Batch Normalization (BN) statistics without any training data. Inspired by the popular VAE/GAN methods, we regard the zero-shot optimization process of synthetic images as generative modeling to match the distribution of BN statistics. The generated images serve as a calibration set for the following zero-shot network quantizations. Our method meets the needs for quantizing models based on sensitive information, e.g., due to privacy concerns, no data is available. Extensive experiments on benchmark datasets show that, with the help of generated data, our approach consistently outperforms existing data-free quantization methods. ",
    "code_link": ""
  },
  "cvpr2021_ecv_efficienttwo-streamactionrecognitiononfpga": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Efficient Two-Stream Action Recognition on FPGA",
    "authors": [
      "Jia-Ming Lin",
      "Kuan-Ting Lai",
      "Bin-Ray Wu",
      "Ming-Syan Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Lin_Efficient_Two-Stream_Action_Recognition_on_FPGA_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Lin_Efficient_Two-Stream_Action_Recognition_on_FPGA_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Action recognition is an important research field that has many applications in surveillance, video search, autonomous vehicles, etc. However, current state-of-the-art action classifiers are still not widely adopted in embedded applications yet. The major reason is that action recognition needs to process both spatial and temporal streaming data to precisely identify actions, which is compute-intensive and power hungry. To solve this issue, researchers start using FPGA to run action recognition models with minimum power. In this paper, we propose a new hardware architecture of action recognition on FPGA. Our model is based on the popular two-stream neural network. By optimizing the optical flow and convolution operations in the temporal domain, our method can achieve similar accuracy with one order of magnitude less operations than other C3D baseline models. We have implemented our model on Xilinx ZCU102 and released the source code. ",
    "code_link": "https://github.com/NetDBFPGA/ecv2021_demo"
  },
  "cvpr2021_ecv_discoveringmulti-hardwaremobilemodelsviaarchitecturesearch": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Discovering Multi-Hardware Mobile Models via Architecture Search",
    "authors": [
      "Grace Chu",
      "Okan Arikan",
      "Gabriel Bender",
      "Weijun Wang",
      "Achille Brighton",
      "Pieter-Jan Kindermans",
      "Hanxiao Liu",
      "Berkin Akin",
      "Suyog Gupta",
      "Andrew Howard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Chu_Discovering_Multi-Hardware_Mobile_Models_via_Architecture_Search_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Chu_Discovering_Multi-Hardware_Mobile_Models_via_Architecture_Search_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Hardware-aware neural architecture designs have been predominantly focusing on optimizing model performance on single hardware and model development complexity, where another important factor, model deployment complexity, has been largely ignored. In this paper, we argue that, for applications that may be deployed on multiple hardware, having different single-hardware models across the deployed hardware makes it hard to guarantee consistent outputs across hardware and duplicates engineering work for debugging and fixing. To minimize such deployment cost, we propose an alternative solution, multi-hardware models, where a single architecture is developed for multiple hardware. With thoughtful search space design and incorporating the proposed multi-hardware metrics in neural architecture search, we discover multi-hardware models that give state-of-the-art (SoTA) performance across multiple hardware in both average and worse case scenarios. For performance on individual hardware, the single multi-hardware model yields similar or better results than SoTA performance on accelerators like GPU, DSP and EdgeTPU which was achieved by different models, while having similar performance with MobilenetV3 Large Minimalistic model on mobile CPU. ",
    "code_link": ""
  },
  "cvpr2021_ecv_compconvacompactconvolutionmoduleforefficientfeaturelearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "CompConv: A Compact Convolution Module for Efficient Feature Learning",
    "authors": [
      "Chen Zhang",
      "Yinghao Xu",
      "Yujun Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Zhang_CompConv_A_Compact_Convolution_Module_for_Efficient_Feature_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Zhang_CompConv_A_Compact_Convolution_Module_for_Efficient_Feature_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Convolutional Neural Networks (CNNs) have achieved remarkable success in various computer vision tasks but rely on tremendous computational cost. To solve this problem, existing approaches either compress well-trained large-scale models or learn lightweight models with carefully designed network structures. In this work, we make a close study of the convolution operator, which is the basic unit used in CNNs, to reduce its computing load. In particular, we propose a compact convolution module, called CompConv, to facilitate efficient feature learning. With the divide-and-conquer strategy, CompConv is able to save a great many computations as well as parameters to produce a certain dimensional feature map. Furthermore, CompConv discreetly integrates the input features into the outputs to efficiently inherit the input information. More importantly, the novel CompConv is a plug-and-play module that can be directly applied to modern CNN structures to replace the vanilla convolution layers without further effort. Extensive experimental results suggest that CompConv can adequately compress the benchmark CNN structures yet barely sacrifice the performance, surpassing other competitors. ",
    "code_link": ""
  },
  "cvpr2021_ecv_in-hindsightquantizationrangeestimationforquantizedtraining": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "In-Hindsight Quantization Range Estimation for Quantized Training",
    "authors": [
      "Marios Fournarakis",
      "Markus Nagel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Fournarakis_In-Hindsight_Quantization_Range_Estimation_for_Quantized_Training_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Fournarakis_In-Hindsight_Quantization_Range_Estimation_for_Quantized_Training_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Quantization techniques applied to the inference of deep neural networks have enabled fast and efficient execution on resource-constraint devices. The success of quantization during inference has motivated the academic community to explore fully quantized training, i.e. quantizing back-propagation as well. However, effective gradient quantization is still an open problem. Gradients are unbounded and their distribution changes significantly during training, which leads to the need for dynamic quantization. As we show, dynamic quantization can lead to significant memory overhead and additional data traffic slowing down training. We propose a simple alternative to dynamic quantization, in-hindsight range estimation, that uses the quantization ranges estimated on previous iterations to quantize the present. Our approach enables fast static quantization of gradients and activations while requiring only minimal hardware support from the neural network accelerator to keep track of output statistics in an online fashion. It is intended as a drop-in replacement for estimating quantization ranges and can be used in conjunction with other advances in quantized training. We compare our method to existing methods for range estimation from the quantized training literature and demonstrate its effectiveness with a range of architectures, including MobileNetV2, on image classification benchmarks (Tiny ImageNet & ImageNet) ",
    "code_link": ""
  },
  "cvpr2021_ecv_dynamic-ofaruntimednnarchitectureswitchingforperformancescalingonheterogeneousembeddedplatforms": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Dynamic-OFA: Runtime DNN Architecture Switching for Performance Scaling on Heterogeneous Embedded Platforms",
    "authors": [
      "Wei Lou",
      "Lei Xun",
      "Amin Sabet",
      "Jia Bi",
      "Jonathon Hare",
      "Geoff V. Merrett"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Lou_Dynamic-OFA_Runtime_DNN_Architecture_Switching_for_Performance_Scaling_on_Heterogeneous_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Lou_Dynamic-OFA_Runtime_DNN_Architecture_Switching_for_Performance_Scaling_on_Heterogeneous_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Mobile and embedded platforms are increasingly required to efficiently execute computationally demanding DNNs across heterogeneous processing elements. At runtime, the available hardware resources to DNNs can vary considerably due to other concurrently running applications. The performance requirements of the applications could also change under different scenarios. To achieve the desired performance, dynamic DNNs have been proposed in which the number of channels/layers can be scaled in real time to meet different requirements under varying resource constraints. However, the training process of such dynamic DNNs can be costly, since platform-aware models of different deployment scenarios must be retrained to become dynamic. This paper proposes Dynamic-OFA, a novel dynamic DNN approach for state-of-the-art platform-aware NAS models (i.e. Once-for-all network (OFA)). Dynamic-OFA pre-samples a family of sub-networks from a static OFA backbone model, and contains a runtime manager to choose different sub-networks under different runtime environments. As such, Dynamic-OFA does not need the traditional dynamic DNN training pipeline. Compared to the state-of-the-art, our experimental results using ImageNet on a Jetson Xavier NX show that the approach is up to 3.5x (CPU), 2.4x (GPU) faster for similar Top-1 accuracy, or 3.8% (CPU), 5.1% (GPU) higher accuracy at similar latency. ",
    "code_link": ""
  },
  "cvpr2021_ecv_data-efficientlanguage-supervisedzero-shotlearningwithself-distillation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Data-Efficient Language-Supervised Zero-Shot Learning With Self-Distillation",
    "authors": [
      "Ruizhe Cheng",
      "Bichen Wu",
      "Peizhao Zhang",
      "Peter Vajda",
      "Joseph E. Gonzalez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Cheng_Data-Efficient_Language-Supervised_Zero-Shot_Learning_With_Self-Distillation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Cheng_Data-Efficient_Language-Supervised_Zero-Shot_Learning_With_Self-Distillation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised \"gold\" labels. Previous works, such as CLIP, use a simple pretraining task of predicting the pairings between images and text captions. CLIP, however, is data hungry and requires more than 400M image text pairs for training. We propose a data-efficient contrastive distillation method that uses soft labels to learn from noisy image-text pairs. Our model transfers knowledge from pretrained image and sentence encoders and achieves strong performance with only 3M image text pairs, 133x smaller than CLIP. Our method exceeds the previous SoTA of general zero-shot learning on ImageNet 21k+1k by 73% relatively with a ResNet50 image encoder and DeCLUTR text encoder. We also beat CLIP by 10.5% relatively on zero-shot evaluation on Google Open Images (19,958 classes). ",
    "code_link": ""
  },
  "cvpr2021_ecv_extracurricularlearningknowledgetransferbeyondempiricaldistribution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution",
    "authors": [
      "Hadi Pouransari",
      "Mojan Javaheripi",
      "Vinay Sharma",
      "Oncel Tuzel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Knowledge distillation has been used to transfer knowledge learned by a sophisticated model (teacher) to a simpler model (student). This technique is widely used to compress model complexity. However, in most applications the compressed student model suffers from an accuracy gap with its teacher. We propose extracurricular learning, a novel knowledge distillation method, that bridges this gap by (1) modeling student and teacher output distributions; (2) sampling examples from an approximation to the underlying data distribution; and (3) matching student and teacher output distributions over this extended set including uncertain samples. We conduct rigorous evaluations on regression and classification tasks and show that compared to the standard knowledge distillation, extracurricular learning reduces the gap by 46% to 68%. This leads to major accuracy improvements compared to the empirical risk minimization-based training for various recent neural network architectures: 16% regression error reduction on the MPIIGaze dataset, +3.4% to +9.1% improvement in top-1 classification accuracy on the CIFAR100 dataset, and +2.9% top-1 improvement on the ImageNet dataset. ",
    "code_link": ""
  },
  "cvpr2021_ecv_widthtransferonthe(in)varianceofwidthoptimization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Width Transfer: On the (In)variance of Width Optimization",
    "authors": [
      "Ting-Wu Chin",
      "Diana Marculescu",
      "Ari S. Morcos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Chin_Width_Transfer_On_the_Invariance_of_Width_Optimization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Chin_Width_Transfer_On_the_Invariance_of_Width_Optimization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Optimizing the channel counts for different layers of a CNN has shown great promise in improving the efficiency of CNNs at test-time. However, these methods often introduce large computational overhead (e.g., an additional 2x FLOPs of standard training). Minimizing this overhead could therefore significantly speed up training. In this work, we propose width transfer, a technique that harnesses the assumptions that the optimized widths (or channel counts) are regular across sizes and depths. We show that width transfer works well across various width optimization algorithms and networks. Specifically, we can achieve up to 320x reduction in width optimization overhead without compromising the top-1 accuracy on ImageNet, making the additional cost of width optimization negligible relative to initial training. Our findings not only suggest an efficient way to conduct width optimization, but also highlight that the widths that lead to better accuracy are invariant to various aspects of network architectures and training data. ",
    "code_link": ""
  },
  "cvpr2021_ecv_basisnettwo-stagemodelsynthesisforefficientinference": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "BasisNet: Two-Stage Model Synthesis for Efficient Inference",
    "authors": [
      "Mingda Zhang",
      "Chun-Te Chu",
      "Andrey Zhmoginov",
      "Andrew Howard",
      "Brendan Jou",
      "Yukun Zhu",
      "Li Zhang",
      "Rebecca Hwa",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Zhang_BasisNet_Two-Stage_Model_Synthesis_for_Efficient_Inference_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Zhang_BasisNet_Two-Stage_Model_Synthesis_for_Efficient_Inference_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work, we present BasisNet which combines recent advancements in efficient neural network architectures, conditional computation, and early termination in a simple new form. Our approach incorporates a lightweight model to preview the input and generate input-dependent combination coefficients, which later controls the synthesis of a more accurate specialist model to make final prediction. The two-stage model synthesis strategy can be applied to any network architectures and both stages are jointly trained. We also show that proper training recipes are critical for increasing generalizability for such high capacity neural networks. On ImageNet classification benchmark, our BasisNet with MobileNets as backbone demonstrated clear advantage on accuracy-efficiency trade-off over several strong baselines. Specifically, BasisNet-MobileNetV3 obtained 80.3% top-1 accuracy with only 290M Multiply-Add operations, halving the computational cost of previous state-of-the-art without sacrificing accuracy. With early termination, the average cost can be further reduced to 198M MAdds while maintaining accuracy of 80.0% on ImageNet. ",
    "code_link": ""
  },
  "cvpr2021_ecv_pareto-optimalquantizedresnetismostly4-bit": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-Bit",
    "authors": [
      "AmirAli Abdolrashidi",
      "Lisa Wang",
      "Shivani Agrawal",
      "Jonathan Malmaud",
      "Oleg Rybakov",
      "Chas Leichner",
      "Lukasz Lew"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Abdolrashidi_Pareto-Optimal_Quantized_ResNet_Is_Mostly_4-Bit_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Abdolrashidi_Pareto-Optimal_Quantized_ResNet_Is_Mostly_4-Bit_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher accuracy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt. ",
    "code_link": ""
  },
  "cvpr2021_ecv_rethinkingtheself-attentioninvisiontransformers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ECV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Rethinking the Self-Attention in Vision Transformers",
    "authors": [
      "Kyungmin Kim",
      "Bichen Wu",
      "Xiaoliang Dai",
      "Peizhao Zhang",
      "Zhicheng Yan",
      "Peter Vajda",
      "Seon Joo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Kim_Rethinking_the_Self-Attention_in_Vision_Transformers_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Kim_Rethinking_the_Self-Attention_in_Vision_Transformers_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Self-attention is a corner stone for transformer models. However, our analysis shows that self-attention in vision transformer inference is extremely sparse. When applying a sparsity constraint, our experiments on image (ImageNet-1K) and video (Kinetics-400) understanding show we can achieve 95% sparsity on the self-attention maps while maintaining the performance drop to be less than 2 points. This motivates us to rethink the role of self-attention in vision transformer models. ",
    "code_link": ""
  },
  "cvpr2021_gaze_appearance-basedgazeestimationusingattentionanddifferencemechanism": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Appearance-Based Gaze Estimation Using Attention and Difference Mechanism",
    "authors": [
      "Murthy L R D",
      "Pradipta Biswas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/html/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Appearance-based gaze estimation problem received wide attention over the past few years. Even though model-based approaches existed earlier, availability of large datasets and novel deep learning techniques made appearance-based methods achieve superior accuracy than model-based approaches. In this paper, we proposed two novel techniques to improve gaze estimation accuracy. Our first approach, I2D-Net uses a difference layer to eliminate any common features from left and right eyes of a subject that are not pertinent to gaze estimation task. Our second approach, AGE-Net adapted the idea of attentionmechanism and assigns weights to the features extracted from eye images. I2D-Net performed on par with the existing state-of-the-art approaches while AGE-Net reported state-of-the-art accuracy of 4.09 and 7.44 degree error on MPIIGaze and RT-Gene datasets respectively. We performed ablation studies to understand the effectiveness of the proposed approaches followed by analysis of gaze error distribution with respect to various factors of MPIIGaze dataset. ",
    "code_link": ""
  },
  "cvpr2021_gaze_pupiltanafew-shotadversarialpupillocalizer": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "PupilTAN: A Few-Shot Adversarial Pupil Localizer",
    "authors": [
      "Nikolaos Poulopoulos",
      "Emmanouil Z. Psarakis",
      "Dimitrios Kosmopoulos"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/html/Poulopoulos_PupilTAN_A_Few-Shot_Adversarial_Pupil_Localizer_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Poulopoulos_PupilTAN_A_Few-Shot_Adversarial_Pupil_Localizer_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The eye center localization is a challenging problem faced by many computer vision applications. The challenges typically stem from the scene variability, such as, the wide range of shapes, the lighting conditions, the view angles and the occlusions. Nowadays, the increasing interest on deep neural networks requires a large volume of training data. However, a significant issue is the dependency on labeled data, which are expensive to obtain and susceptible to errors. To address these issues, we propose a deep network, dubbed PupilTAN, that performs image-to-heatmap Translation and an Adversarial training framework that solves the eye localization problem in a few-shot unsupervised way. The key idea is to estimate, by using only a few ground-truth shots, the heatmaps centers' pdf and use it as a generator to create random heatmaps that follow the same probability distribution of the real ones. We showcase that training the deep network with these artificial heatmaps in an adversarial framework not only makes us less dependent on labeled data, but also leads to a significant accuracy improvement. The proposed network achieves realtime performance in a general-purpose computer environment and improves the state-of-the-art accuracy for both MUCT and BioID datasets, even compared with supervised techniques. Furthermore, our model is robust even in the case of reducing its size of up to 1/16 of the original network (0.2M parameters), demonstrating comparable accuracy to the state-of-the-art with high practical value to real-time applications. ",
    "code_link": ""
  },
  "cvpr2021_gaze_visualfocusofattentionestimationin3dscenewithanarbitrarynumberoftargets": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "Visual Focus of Attention Estimation in 3D Scene With an Arbitrary Number of Targets",
    "authors": [
      "Remy Siegfried",
      "Jean-Marc Odobez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/html/Siegfried_Visual_Focus_of_Attention_Estimation_in_3D_Scene_With_an_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Siegfried_Visual_Focus_of_Attention_Estimation_in_3D_Scene_With_an_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Visual Focus of Attention (VFOA) estimation in conversation is challenging as it relies on difficult to estimate information (gaze) combined with scene features like target positions and other contextual information (speaking status) allowing to disambiguate situations. Previous VFOA models fusing all these features are usually trained for a specific setup and using a fixed number of interacting people, and should be retrained to be applied to another one, which limits their usability. To address these limitations, we propose a novel deep learning method that encodes all input features as a fixed number of 2D maps, which makes the input more naturally processed by a convolutional neural network, provides scene normalization, and allows to consider an arbitrary number of targets. Experiments performed on two publicly available datasets demonstrate that the proposed method can be trained in a cross-dataset fashion without loss in VFOA accuracy compared to intra-dataset training. ",
    "code_link": ""
  },
  "cvpr2021_gaze_gooadatasetforgazeobjectpredictioninretailenvironments": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "GAZE",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Gaze Estimation and Prediction in the Wild",
    "title": "GOO: A Dataset for Gaze Object Prediction in Retail Environments",
    "authors": [
      "Henri Tomas",
      "Marcus Reyes",
      "Raimarc Dionido",
      "Mark Ty",
      "Jonric Mirando",
      "Joel Casimiro",
      "Rowel Atienza",
      "Richard Guinto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/html/Tomas_GOO_A_Dataset_for_Gaze_Object_Prediction_in_Retail_Environments_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/Tomas_GOO_A_Dataset_for_Gaze_Object_Prediction_in_Retail_Environments_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " One of the most fundamental and information-laden actions humans do is to look at objects. However, a survey of current works reveals that existing gaze-related datasets annotate only the pixel being looked at, and not the boundaries of a specific object of interest. This lack of object annotation presents an opportunity for further advancing gaze estimation research. To this end, we present a challenging new task called gaze object prediction, where the goal is to predict a bounding box for a person's gazed-at object. To train and evaluate gaze networks on this task, we present the Gaze On Objects (GOO) dataset. GOO is composed of a large set of synthetic images (GOO-Synth) supplemented by a smaller subset of real images (GOO-Real) of people looking at objects in a retail environment. Our work establishes extensive baselines on GOO by re-implementing and evaluating selected state-of-the-art models on the task of gaze following and domain adaptation. Code is available on github. ",
    "code_link": "https://github.com/upeee/GOO-GAZE2021"
  },
  "cvpr2021_vocvalc_hsipu2-anewhumanphysicalfitnessactiondatasetforrecognitionand3dreconstructionevaluation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues - With a Focus on Mobile Platform Applications",
    "title": "HSiPu2 - A New Human Physical Fitness Action Dataset for Recognition and 3D Reconstruction Evaluation",
    "authors": [
      "Chuanlei Zhang",
      "Lixin Liu",
      "Minda Yao",
      "Wei Chen",
      "Dufeng Chen",
      "Yuliang Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/VOCVALC/html/Zhang_HSiPu2_-_A_New_Human_Physical_Fitness_Action_Dataset_for_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/VOCVALC/papers/Zhang_HSiPu2_-_A_New_Human_Physical_Fitness_Action_Dataset_for_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper a human physical fitness action feature dataset named HSiPu2 is introduced, which contains 8,044 action data sequences and 80,440 images. The dataset is built for three physical fitness actions, which are situp, push-up and pull-up, and each action data has two categories corresponding to standard and non-standard actions. Two cameras work as sensors to capture features from different views. HSiPu2 data set facilitates the evaluation of the performance of machine learning algorithms used in recognition and evaluation problems related to human behaviours recognition. The dataset is freely and publicly available online. A new recognition method based on deep learning techniques, including the twobranch multi-stage convolutional neural networks (CNNs) and long short-term memory (LSTM) networks with attention, are employed to learn the long-term dependencies from videos for human physical fitness action recognition on the HSiPu2. For comparison purpose, traditional machine learning models, including Decision Tree, Random Forest, SVM, Bagging, GBDT, AdaBoost, XGBoost and Voting, are utilized for the action recolonization. Furhtermore, HSiPu2 also can serve as a dataset to evaluating a human action 3D model. ",
    "code_link": "https://github.com/mindayao/HSiPu2"
  },
  "cvpr2021_vocvalc_transformer-basedtextdetectioninthewild": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "VOCVALC",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Visual Odometry and Computer Vision Applications Based on Location Clues - With a Focus on Mobile Platform Applications",
    "title": "Transformer-Based Text Detection in the Wild",
    "authors": [
      "Zobeir Raisi",
      "Mohamed A. Naiel",
      "Georges Younes",
      "Steven Wardell",
      "John S. Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/VOCVALC/html/Raisi_Transformer-Based_Text_Detection_in_the_Wild_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/VOCVALC/papers/Raisi_Transformer-Based_Text_Detection_in_the_Wild_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A major limitation to most state-of-the-art visual localization methods is their ineptitude to make use of ubiquitous signs and directions that are typically intuitive to humans. Localization methods can greatly benefit from a system capable of reasoning about a variety of cues beyond low-level features, such as street signs, store names, building directories, room numbers, etc. In this work, we tackle the problem of text detection in the wild, an essential step towards achieving text-based localization and mapping. While current state-of-the-art text detection methods employ ad-hoc solutions with complex multi-stage components to solve the problem, we propose a Transformer-based architecture inherently capable of dealing with multi-oriented texts in images. A central contribution to our work is the introduction of a loss function tailored to the rotated text detection problem that leverages a rotated version of a generalized intersection over union score to properly capture the rotated text regions. We evaluate our proposed model qualitatively and quantitatively on several challenging datasets namely, ICDAR15, ICDAR17, and MSRA-TD500, and show that it outperforms current state-of-the-art methods in text detection in the wild. ",
    "code_link": ""
  },
  "cvpr2021_evw_phaseselectiveconvolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Embedded Vision",
    "title": "Phase Selective Convolution",
    "authors": [
      "Jamie Menjay Lin",
      "Parham Noorzad",
      "Yang Yang",
      "Nojun Kwak",
      "Fatih Porikli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/html/Lin_Phase_Selective_Convolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/papers/Lin_Phase_Selective_Convolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces Phase Selective Convolution (PSC), an enhanced convolution for more deliberate utilization of activations in convolutional networks. Unlike conventional use of convolutions with activation functions, PSC preserves the full space of activations while supporting desirable model nonlinearity. Similar to several other network operations, e.g., the ReLU operation, at the time of their introduction, PSC may not execute as efficiently on platforms without hardware specialization support. As a first step in addressing the need for optimization, we propose a hardware acceleration scheme to enable the intended efficiency for PSC execution. Moreover, we propose a PSC deployment strategy, with which PSC is applied only to selected layers of the networks, to avoid excessive increase in the total model size. To evaluate the results, we apply PSC as a drop-in replacement for selected convolution layers in several networks without affecting their macro network architectures. In particular, PSC-enhanced ResNets achieve higher accuracies by 1.0-2.0% and 0.7-1.0% on CIFAR-100 and ImageNet, respectively, in Pareto efficiency. PSC-enhanced MobileNets (V2 and V3 Large) and MobileNetV3 (Small) achieve 0.9-1.0% and 1.8% accuracy gains, respectively, on ImageNet at little (0.2-0.7%) total model size increase. ",
    "code_link": ""
  },
  "cvpr2021_evw_automotiveradarinterferencemitigationwithunfoldedrobustpcabasedonresidualovercompleteauto-encoderblocks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Embedded Vision",
    "title": "Automotive Radar Interference Mitigation With Unfolded Robust PCA Based on Residual Overcomplete Auto-Encoder Blocks",
    "authors": [
      "Nicolae-Catalin Ristea",
      "Andrei Anghel",
      "Radu Tudor Ionescu",
      "Yonina C. Eldar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/html/Ristea_Automotive_Radar_Interference_Mitigation_With_Unfolded_Robust_PCA_Based_on_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/papers/Ristea_Automotive_Radar_Interference_Mitigation_With_Unfolded_Robust_PCA_Based_on_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In autonomous driving, radar systems play an important role in detecting targets such as other vehicles on the road. Radars mounted on different cars can interfere with each other, degrading the detection performance. Deep learning methods for automotive radar interference mitigation can successfully estimate the amplitude of targets, but fail to recover the phase of the respective targets. In this paper, we propose an efficient and effective technique based on unfolded robust Principal Component Analysis (RPCA) that is able to estimate both amplitude and phase in the presence of interference. Our contribution consists in introducing residual overcomplete auto-encoder (ROC-AE) blocks into the recurrent architecture of unfolded RPCA, which results in a deeper model that significantly outperforms unfolded RPCA as well as other deep learning models. We also show that our approach achieves a faster processing time compared to state-of-the-art deep learning methods, thus being a suitable candidate to be deployed on devices embedded on vehicles. ",
    "code_link": ""
  },
  "cvpr2021_evw_combiningweightpruningandknowledgedistillationforcnncompression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Embedded Vision",
    "title": "Combining Weight Pruning and Knowledge Distillation for CNN Compression",
    "authors": [
      "Nima Aghli",
      "Eraldo Ribeiro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/html/Aghli_Combining_Weight_Pruning_and_Knowledge_Distillation_for_CNN_Compression_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/papers/Aghli_Combining_Weight_Pruning_and_Knowledge_Distillation_for_CNN_Compression_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Complex deep convolutional neural networks such as ResNet require expensive hardware such as powerful GPUs to achieve real-time performance. This problem is critical for applications that run on low-end embedded GPU or CPU systems with limited resources. As a result, model compression for deep neural networks becomes an important research topic. Popular compression methods such as weight pruning remove redundant neurons from the CNN without affecting the network's output accuracy. While these pruning methods work well on simple networks such as VGG or AlexNet, they are not suitable for compressing current state-of-the-art networks such as ResNets because of these networks' complex architectures with dimensionality dependencies. This dependency results in filter pruning breaking the structure of ResNets leading to an untrainable network. In this paper, we first use the weight pruning method only on a selective number of layers in the ResNet architecture to avoid breaking the network structure. Second, we introduce a knowledge distillation architecture and a loss function to compress the untouched layers during the pruning. We test our method on both image-based regression and classification networks for head-pose estimation and image classification. Our compression method reduces the models' size significantly while maintaining the accuracy very close to the baseline model. ",
    "code_link": ""
  },
  "cvpr2021_evw_depthdistillationunsupervisedmetricdepthestimationforuavsbyfindingconsensusbetweenkinematics,opticalflowanddeeplearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Embedded Vision",
    "title": "Depth Distillation: Unsupervised Metric Depth Estimation for UAVs by Finding Consensus Between Kinematics, Optical Flow and Deep Learning",
    "authors": [
      "Mihai Pirvu",
      "Victor Robu",
      "Vlad Licaret",
      "Dragos Costea",
      "Alina Marcu",
      "Emil Slusanschi",
      "Rahul Sukthankar",
      "Marius Leordeanu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/html/Pirvu_Depth_Distillation_Unsupervised_Metric_Depth_Estimation_for_UAVs_by_Finding_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/papers/Pirvu_Depth_Distillation_Unsupervised_Metric_Depth_Estimation_for_UAVs_by_Finding_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Estimating precise metric depth is an essential task for UAV navigation. Nevertheless, it is very difficult to do unsupervised learning without access to odometry. At the same time, depth recovery from kinematics and optical flow is mathematically precise, but less numerically stable and robust, especially in the focus of expansion areas. We propose a model that combines the analytical approach with deep learning, into a single formulation for metric depth estimation, that is both fast and accurate. The two pathways form a robust ensemble, which provides supervision to a single deep net that distills in this manner the consensus between scene geometry, pose, kinematics, camera intrinsics and the input RGB. The distilled net has low runtime and memory costs, being suitable for embedded devices. We validate our results against an off-the-shelf SfM-based solution. We also introduce a new real-world dataset of almost 20 minutes of continuous UAV flight, on which we demonstrate superior accuracy and capabilities to previous deep learning and classical approaches. ",
    "code_link": ""
  },
  "cvpr2021_evw_cassod-netcascadedandseparablestructuresofdilatedconvolutionforembeddedvisionsystemsandapplications": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "EVW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Embedded Vision",
    "title": "CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for Embedded Vision Systems and Applications",
    "authors": [
      "Tse-Wei Chen",
      "Deyu Wang",
      "Wei Tao",
      "Dongchao Wen",
      "Lingxiao Yin",
      "Tadayuki Ito",
      "Kinya Osa",
      "Masami Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/html/Chen_CASSOD-Net_Cascaded_and_Separable_Structures_of_Dilated_Convolution_for_Embedded_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/EVW/papers/Chen_CASSOD-Net_Cascaded_and_Separable_Structures_of_Dilated_Convolution_for_Embedded_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The field of view (FOV) of convolutional neural networks is highly related to the accuracy of inference. Dilated convolutions are known as an effective solution to the problems which require large FOVs. However, for general-purpose hardware or dedicated hardware, it usually takes extra time to handle dilated convolutions compared with standard convolutions. In this paper, we propose a network module, Cascaded and Separable Structure of Dilated (CASSOD) Convolution, and a special hardware system to handle the CASSOD networks efficiently. A CASSOD-Net includes multiple cascaded 2 x 2 dilated filters, which can be used to replace the traditional 3 x 3 dilated filters without decreasing the accuracy of inference. Two example applications, face detection and image segmentation, are tested with dilated convolutions and the proposed CASSOD modules. The new network for face detection achieves higher accuracy than the previous work with only 47% of filter weights in the dilated convolution layers of the context module. Moreover, the proposed hardware system can accelerate the computations of dilated convolutions, and it is 2.78 times faster than traditional hardware systems when the filter size is 3 x 3. ",
    "code_link": ""
  },
  "cvpr2021_tcv_sample-freewhite-boxout-of-distributiondetectionfordeeplearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Sample-Free White-Box Out-of-Distribution Detection for Deep Learning",
    "authors": [
      "Jean-Michel Begon",
      "Pierre Geurts"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Begon_Sample-Free_White-Box_Out-of-Distribution_Detection_for_Deep_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Begon_Sample-Free_White-Box_Out-of-Distribution_Detection_for_Deep_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Being able to detect irrelevant test examples with respect to deployed deep learning models is paramount to properly and safely using them. In this paper, we address the problem of rejecting such out-of-distribution (OOD) samples in a fully sample-free way, i.e., without requiring any access to in-distribution or OOD samples. We propose several indicators which can be computed alongside the prediction with little additional cost, assuming white-box access to the network. These indicators prove useful, stable and complementary for OOD detection on frequently-used architectures. We also introduce a surprisingly simple, yet effective summary OOD indicator. This indicator is shown to perform well across several networks and datasets and can furthermore be easily tuned as soon as samples become available. Lastly, we discuss how to exploit this summary in real-world settings. ",
    "code_link": ""
  },
  "cvpr2021_tcv_atheoretical-empiricalapproachtoestimatingsamplecomplexityofdnns": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "A Theoretical-Empirical Approach to Estimating Sample Complexity of DNNs",
    "authors": [
      "Devansh Bisla",
      "Apoorva Nandini Saridena",
      "Anna Choromanska"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Bisla_A_Theoretical-Empirical_Approach_to_Estimating_Sample_Complexity_of_DNNs_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Bisla_A_Theoretical-Empirical_Approach_to_Estimating_Sample_Complexity_of_DNNs_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper focuses on understanding how the generalization error scales with the amount of the training data for deep neural networks (DNNs). Existing techniques in statistical learning theory require a computation of capacity measures, such as VC dimension, to provably bound this error. It is however unclear how to extend these measures to DNNs and therefore the existing analyses are applicable to simple neural networks, which are not used in practice, e.g., linear or shallow (at most two-layer) ones or otherwise multi-layer perceptrons. Moreover many theoretical error bounds are not empirically verifiable. In this paper we derive estimates of the generalization error that hold for deep networks and do not rely on unattainable capacity measures. The enabling technique in our approach hinges on two major assumptions: i) the network achieves zero training error, ii) the probability of making an error on a test point is proportional to the distance between this point and its nearest training point in the feature space and at certain maximal distance (that we call radius) it saturates. Based on these assumptions we estimate the generalization error of DNNs. The obtained estimate scales as O(1 / (\\deltaN^(1/d)), where N is the size of the training data, and is parametrized by two quantities, the effective dimensionality of the data as perceived by the network (d) and the aforementioned radius (\\delta), both of which we find empirically. We show that our estimates match with the experimentally-obtained behavior of the error on multiple learning tasks using benchmark data-sets and realistic models. Estimating training data requirements is essential for deployment of safety critical applications such as autonomous driving, medical diagnostics etc. Furthermore, collecting and annotating training data requires a huge amount of financial, computational and human resources. Our empirical estimates will help to efficiently allocate resources. ",
    "code_link": ""
  },
  "cvpr2021_tcv_mlcapsuleguardedofflinedeploymentofmachinelearningasaservice": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service",
    "authors": [
      "Lucjan Hanzlik",
      "Yang Zhang",
      "Kathrin Grosse",
      "Ahmed Salem",
      "Maximilian Augustin",
      "Michael Backes",
      "Mario Fritz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Hanzlik_MLCapsule_Guarded_Offline_Deployment_of_Machine_Learning_as_a_Service_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Hanzlik_MLCapsule_Guarded_Offline_Deployment_of_Machine_Learning_as_a_Service_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user's input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user's client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference. ",
    "code_link": ""
  },
  "cvpr2021_tcv_x-manexplainingmultiplesourcesofanomaliesinvideo": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "X-MAN: Explaining Multiple Sources of Anomalies in Video",
    "authors": [
      "Stanislaw Szymanowicz",
      "James Charles",
      "Roberto Cipolla"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Szymanowicz_X-MAN_Explaining_Multiple_Sources_of_Anomalies_in_Video_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Szymanowicz_X-MAN_Explaining_Multiple_Sources_of_Anomalies_in_Video_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Our objective is to detect anomalies in video while also automatically explaining the reason behind the detector's response. In a practical sense, explainability is crucial for this task as the required response to an anomaly depends on its nature and severity. However, most leading methods (based on deep neural networks) are not interpretable and hide the decision making process in uninterpretable feature representations. In an effort to tackle this problem we make the following contributions: (1) we show how to build interpretable feature representations suitable for detecting anomalies with state of the art performance, (2) we propose an interpretable probabilistic anomaly detector which can describe the reason behind it's response using high level concepts, (3) we are the first to directly consider object interactions for anomaly detection and (4) we propose a new task of explaining anomalies and release a large dataset for evaluating methods on this task. Our method competes well with the state of the art on public datasets while also providing anomaly explanation based on objects and their interactions. ",
    "code_link": ""
  },
  "cvpr2021_tcv_amathematicalanalysisoflearninglossforactivelearninginregression": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "A Mathematical Analysis of Learning Loss for Active Learning in Regression",
    "authors": [
      "Megh Shukla",
      "Shuaib Ahmed"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Shukla_A_Mathematical_Analysis_of_Learning_Loss_for_Active_Learning_in_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Shukla_A_Mathematical_Analysis_of_Learning_Loss_for_Active_Learning_in_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Active learning continues to remain significant in the industry since it is data efficient. Not only is it cost effective on a constrained budget, continuous refinement of the model allows for early detection and resolution of failure scenarios during the model development stage. Identifying and fixing failures with the model is crucial as industrial applications demand that the underlying model performs accurately in all foreseeable use cases. One popular state-of-the-art technique that specializes in continuously refining the model via failure identification is Learning Loss. Although simple and elegant, this approach is empirically motivated. Our paper develops a foundation for Learning Loss which enables us to propose a novel modification we call LearningLoss++. We show that gradients are crucial in interpreting how Learning Loss works, with rigorous analysis and comparison of the gradients between Learning Loss and LearningLoss++. We also propose a convolutional architecture that combines features at different scales to predict the loss. We validate LearningLoss++ for regression on the task of human pose estimation (using MPII and LSP datasets), as done in Learning Loss. We show that LearningLoss++ outperforms in identifying scenarios where the model is likely to perform poorly, which on model refinement translates into reliable performance in the open world. ",
    "code_link": ""
  },
  "cvpr2021_tcv_infoscrubtowardsattributeprivacybytargetedobfuscation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "InfoScrub: Towards Attribute Privacy by Targeted Obfuscation",
    "authors": [
      "Hui-Po Wang",
      "Tribhuvanesh Orekondy",
      "Mario Fritz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Wang_InfoScrub_Towards_Attribute_Privacy_by_Targeted_Obfuscation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Wang_InfoScrub_Towards_Attribute_Privacy_by_Targeted_Obfuscation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Personal photos of individuals when shared online, apart from exhibiting a myriad of memorable details, also reveals a wide range of private information and potentially entails privacy risks (e.g., online harassment, tracking). To mitigate such risks, it is crucial to study techniques that allow individuals to limit the private information leaked in visual data. We tackle this problem in a novel image obfuscation framework: to maximize entropy on inferences over targeted privacy attributes, while retaining image fidelity. We approach the problem based on an encoder-decoder style architecture, with two key novelties: (a) introducing a discriminator to perform bi-directional translation simultaneously from multiple unpaired domains; (b) predicting an image interpolation that maximizes uncertainty over a target set of attributes. We find our approach generates obfuscated images faithful to the original input images and additionally increases uncertainty by 6.2x (or up to 0.85 bits) over the non-obfuscated counterparts. ",
    "code_link": ""
  },
  "cvpr2021_tcv_estimating(andfixing)theeffectoffaceobfuscationinvideorecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Estimating (and Fixing) the Effect of Face Obfuscation in Video Recognition",
    "authors": [
      "Matteo Tomei",
      "Lorenzo Baraldi",
      "Simone Bronzin",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Tomei_Estimating_and_Fixing_the_Effect_of_Face_Obfuscation_in_Video_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Tomei_Estimating_and_Fixing_the_Effect_of_Face_Obfuscation_in_Video_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Recent research has shown that faces can be obfuscated in large-scale datasets with a minimal performance impact on image classification and downstream tasks like object recognition. In this paper, we investigate the role of face obfuscation in video classification datasets and quantify a more significant reduction in performance caused by face blurring. To reduce such performance effects, we propose a generalized distillation approach in which a privacy-preserving action recognition network is trained with privileged information given by face identities. We show, through experiments performed on Kinetics-400, that the proposed approach can fully close the performance gap caused by face anonymization. ",
    "code_link": ""
  },
  "cvpr2021_tcv_explainabledeepclassificationmodelsfordomaingeneralization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Explainable Deep Classification Models for Domain Generalization",
    "authors": [
      "Andrea Zunino",
      "Sarah Adel Bargal",
      "Riccardo Volpi",
      "Mehrnoosh Sameki",
      "Jianming Zhang",
      "Stan Sclaroff",
      "Vittorio Murino",
      "Kate Saenko"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Zunino_Explainable_Deep_Classification_Models_for_Domain_Generalization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Zunino_Explainable_Deep_Classification_Models_for_Domain_Generalization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Conventionally, AI models are thought to trade off explainability for lower accuracy. We develop a training strategy that not only leads to a more explainable AI system for object classification, but as a consequence, suffers no perceptible accuracy degradation. Explanations are defined as regions of visual evidence upon which a deep classification network makes a decision. This is represented in the form of a saliency map conveying how much each pixel contributed to the network's decision. Our training strategy enforces a periodic saliency-based feedback to encourage the model to focus on the image regions that directly correspond to the ground-truth object. We quantify explainability using an automated metric, and using human judgement. We propose explainability as a means for bridging the visual-semantic gap between different domains where model explanations are used as a means of disentagling domain specific information from otherwise relevant features. We demonstrate that this leads to improved generalization to new domains without hindering performance on the original domain. ",
    "code_link": ""
  },
  "cvpr2021_tcv_awatermarking-basedframeworkforprotectingdeepimageclassifiersagainstadversarialattacks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "A Watermarking-Based Framework for Protecting Deep Image Classifiers Against Adversarial Attacks",
    "authors": [
      "Chen Sun",
      "En-Hui Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Sun_A_Watermarking-Based_Framework_for_Protecting_Deep_Image_Classifiers_Against_Adversarial_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Sun_A_Watermarking-Based_Framework_for_Protecting_Deep_Image_Classifiers_Against_Adversarial_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Although deep learning-based models have achieved tremendous success in image-related tasks, they are known to be vulnerable to adversarial examples---inputs with imperceptible, but subtly crafted perturbation which fool the models to produce incorrect outputs. To distinguish adversarial examples from benign images, in this paper, we propose a novel watermarking-based framework for protecting deep image classifiers against adversarial attacks. The proposed framework consists of a watermark encoder, a possible adversary, and a detector followed by a deep image classifier to be protected. Specific methods of watermarking and detection are also presented. It is shown by experiment on a subset of ImageNet validation dataset that the proposed framework along with the presented methods of watermarking and detection is effective against a wide range of advanced attacks (static and adaptive), achieving a near zero (effective) false negative rate for FGSM and PGD attacks (static and adaptive) with the guaranteed zero false positive rate. In addition, for all tested deep image classifiers (ResNet50V2, MobileNetV2, InceptionV3), the impact of watermarking on classification accuracy is insignificant with, on average, 0.63% and 0.49% degradation in top 1 and top 5 accuracy, respectively. ",
    "code_link": ""
  },
  "cvpr2021_tcv_towardsfairfederatedlearningwithzero-shotdataaugmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Towards Fair Federated Learning With Zero-Shot Data Augmentation",
    "authors": [
      "Weituo Hao",
      "Mostafa El-Khamy",
      "Jungwon Lee",
      "Jianyi Zhang",
      "Kevin J Liang",
      "Changyou Chen",
      "Lawrence Carin Duke"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Hao_Towards_Fair_Federated_Learning_With_Zero-Shot_Data_Augmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Hao_Towards_Fair_Federated_Learning_With_Zero-Shot_Data_Augmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Federated learning has emerged as an important distributed learning paradigm, where a server aggregates a global model from many client-trained models, while having no access to the client data. Although it is recognized that statistical heterogeneity of the client local data yields slower global model convergence, it is less commonly recognized that it also yields a biased federated global model with a high variance of accuracy across clients. In this work, we aim to provide federated learning schemes with improved fairness. To tackle this challenge, we propose a novel federated learning system that employs zero-shot data augmentation on under-represented data to mitigate statistical heterogeneity, and encourage more uniform accuracy performance across clients in federated networks. We study two variants of this scheme, Fed-ZDAC (federated learning with zero-shot data augmentation at the clients) and Fed-ZDAS (federated learning with zero-shot data augmentation at the server). Empirical results on a suite of datasets demonstrate the effectiveness of our methods on simultaneously improving the test accuracy and fairness. ",
    "code_link": ""
  },
  "cvpr2021_tcv_anadversarialapproachforexplainingthepredictionsofdeepneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "An Adversarial Approach for Explaining the Predictions of Deep Neural Networks",
    "authors": [
      "Arash Rahnama",
      "Andrew Tseng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Rahnama_An_Adversarial_Approach_for_Explaining_the_Predictions_of_Deep_Neural_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Rahnama_An_Adversarial_Approach_for_Explaining_the_Predictions_of_Deep_Neural_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Machine learning models have been successfully applied to a wide range of applications including computer vision, natural language processing, and speech recognition. A successful implementation of these models however, usually relies on deep neural networks (DNNs) which are treated as opaque black-box systems due to their incomprehensible complexity and intricate internal mechanism. In this work, we present a novel algorithm for explaining the predictions of a DNN using adversarial machine learning. Our approach identifies the relative importance of input features in relation to the predictions based on the behavior of an adversarial attack on the DNN. Our algorithm has the advantage of being fast, consistent, and easy to implement and interpret. We present our detailed analysis that demonstrates how the behavior of an adversarial attack, given a DNN and a task, stays consistent for any input test data point proving the generality of our approach. Our analysis enables us to produce consistent and efficient explanations. We illustrate the effectiveness of our approach by conducting experiments using a variety of DNNs, tasks, and datasets. Finally, we compare our work with other well-known techniques in the current literature. ",
    "code_link": ""
  },
  "cvpr2021_tcv_renofeationasimpletransferlearningmethodforimprovedadversarialrobustness": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Fair, Data-Efficient, and Trusted Computer Vision",
    "title": "Renofeation: A Simple Transfer Learning Method for Improved Adversarial Robustness",
    "authors": [
      "Ting-Wu Chin",
      "Cha Zhang",
      "Diana Marculescu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Chin_Renofeation_A_Simple_Transfer_Learning_Method_for_Improved_Adversarial_Robustness_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/TCV/papers/Chin_Renofeation_A_Simple_Transfer_Learning_Method_for_Improved_Adversarial_Robustness_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Fine-tuning through knowledge transfer from a pre-trained model on a large-scale dataset is a widely spread approach to effectively build models on small-scale datasets. In this work, we show that a recent adversarial attack designed for transfer learning via re-training the last linear layer can successfully deceive models trained with transfer learning via end-to-end fine-tuning. This raises security concerns for many industrial applications. In contrast, models trained with random initialization without transfer are much more robust to such attacks, although these models often exhibit much lower accuracy. To this end, we propose noisy feature distillation, a new transfer learning method that trains a network from random initialization while achieving clean-data performance competitive with fine-tuning. ",
    "code_link": ""
  },
  "cvpr2021_hvu_mdmmtmultidomainmultimodaltransformerforvideoretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Large Scale Holistic Video Understanding",
    "title": "MDMMT: Multidomain Multimodal Transformer for Video Retrieval",
    "authors": [
      "Maksim Dzabraev",
      "Maksim Kalashnikov",
      "Stepan Komkov",
      "Aleksandr Petiushko"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/html/Dzabraev_MDMMT_Multidomain_Multimodal_Transformer_for_Video_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Dzabraev_MDMMT_Multidomain_Multimodal_Transformer_for_Video_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present a new state-of-the-art on the text-to-video retrieval task on MSRVTT and LSMDC benchmarks where our model outperforms all previous solutions by a large margin. Moreover, state-of-the-art results are achieved using a single model and without finetuning. This multidomain generalisation is achieved by a proper combination of different video caption datasets. We show that our practical approach for training on different datasets can improve test results of each other. Additionally, we check intersection between many popular datasets and show that MSRVTT as well as ActivityNet contains a significant overlap between the test and the training parts. More details are available at https://github.com/papermsucode/mdmmt. ",
    "code_link": "https://github.com/papermsucode/mdmmt"
  },
  "cvpr2021_hvu_sail-vos3dasyntheticdatasetandbaselinesforobjectdetectionand3dmeshreconstructionfromvideodata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Large Scale Holistic Video Understanding",
    "title": "SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction From Video Data",
    "authors": [
      "Yuan-Ting Hu",
      "Jiahong Wang",
      "Raymond A. Yeh",
      "Alexander G. Schwing"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/html/Hu_SAIL-VOS_3D_A_Synthetic_Dataset_and_Baselines_for_Object_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Hu_SAIL-VOS_3D_A_Synthetic_Dataset_and_Baselines_for_Object_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Extracting detailed 3D information of objects from video data is an important goal for holistic scene understanding. While recent methods have shown impressive results when reconstructing meshes of objects from a single image, results often remain ambiguous as part of the object is unobserved. Moreover, existing image-based datasets for mesh reconstruction don't permit to study models which integrate temporal information. To alleviate both concerns we present SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh annotations which extends SAIL-VOS. We also develop first baselines for reconstruction of 3D meshes from video data via temporal models. We demonstrate efficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that temporal information improves reconstruction quality. ",
    "code_link": ""
  },
  "cvpr2021_hvu_coconcooperative-contrastivelearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Large Scale Holistic Video Understanding",
    "title": "CoCon: Cooperative-Contrastive Learning",
    "authors": [
      "Nishant Rai",
      "Ehsan Adeli",
      "Kuan-Hui Lee",
      "Adrien Gaidon",
      "Juan Carlos Niebles"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/html/Rai_CoCon_Cooperative-Contrastive_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Rai_CoCon_Cooperative-Contrastive_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Labeling videos at scale is impractical. Consequently, self-supervised visual representation learning is key for efficient video analysis. Recent success in learning image representations suggest contrastive learning is a promising framework to tackle this challenge. However, when applied to real-world videos, contrastive learning may unknowingly lead to separation of instances that contain semantically similar events. In our work, we introduce a cooperative variant of contrastive learning to address this issue. We use data-driven sampling to leverage implicit relationships between multiple input video views, whether observed (e.g. RGB) or inferred (e.g. flow, segmentation masks, poses). We experimentally evaluate our representations on the downstream task of action recognition. Our method sets a new state of the art on standard benchmarks (UCF101, HMDB51, Kinetics400). Furthermore, qualitative experiments illustrate that our models can capture higher-order class relationships. The code is available at http://github.com/nishantrai18/CoCon. ",
    "code_link": "https://github.com/nishantrai18/CoCon"
  },
  "cvpr2021_hvu_rethinkingtrainingdataformitigatingrepresentationbiasesinactionrecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Large Scale Holistic Video Understanding",
    "title": "Rethinking Training Data for Mitigating Representation Biases in Action Recognition",
    "authors": [
      "Kensho Hara",
      "Yuchi Ishikawa",
      "Hirokatsu Kataoka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/html/Hara_Rethinking_Training_Data_for_Mitigating_Representation_Biases_in_Action_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Hara_Rethinking_Training_Data_for_Mitigating_Representation_Biases_in_Action_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The purpose of this study is to train spatiotemporal 3D convolutional neural networks (3D CNNs) that properly leverage temporal information to recognize actions. Though 3D CNNs are an effective framework in action recognition, some studies showed the biases of video datasets for generic action recognition lead 3D CNNs to recognize not dynamic motions but static cues, such as objects, scenes, and people. On the other hand, video datasets for fine-grained action recognition, which classifies various actions in a specific domain, are expected to have small biases compared with the datasets for generic action recognition. In this study, we examine the biases of various video datasets, which include both generic and fine-grained action recognition tasks, for training 3D CNNs. Based on the results of experiments, the following conclusions could be obtained: (i) The representation biases learned from fine-grained action recognition datasets are smaller than those of generic action recognition datasets. (ii) The models pretrained on fine-grained action recognition datasets, of which the biases are small, leverage temporal information to recognize actions rather than static information. (iii) The models that leverage temporal information achieve better performance on fine-grained action recognition whereas the performance of the models pretrained on biased datasets is better on generic action recognition. We should evaluate models on both generic and fine-grained recognition datasets to properly evaluate their performance. ",
    "code_link": ""
  },
  "cvpr2021_hvu_objectgraphsusingobjectsandagraphconvolutionalnetworkforthebottom-uprecognitionandexplanationofeventsinvideo": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Large Scale Holistic Video Understanding",
    "title": "ObjectGraphs: Using Objects and a Graph Convolutional Network for the Bottom-Up Recognition and Explanation of Events in Video",
    "authors": [
      "Nikolaos Gkalelis",
      "Andreas Goulas",
      "Damianos Galanopoulos",
      "Vasileios Mezaris"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/html/Gkalelis_ObjectGraphs_Using_Objects_and_a_Graph_Convolutional_Network_for_the_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Gkalelis_ObjectGraphs_Using_Objects_and_a_Graph_Convolutional_Network_for_the_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper a novel bottom-up video event recognition approach is proposed, ObjectGraphs, which utilizes a rich frame representation and the relations between objects within each frame. Following the application of an object detector (OD) on the frames, graphs are used to model the object relations and a graph convolutional network (GCN) is utilized to perform reasoning on the graphs. The resulting object-based frame-level features are then forwarded to a long short-term memory (LSTM) network for video event recognition. Moreover, the weighted in degrees (WiDs) derived from the graph's adjacency matrix at frame level are used for identifying the objects that were considered most (or least) salient for event recognition and contributed the most (or least) to the final event recognition decision, thus providing an explanation for the latter. The experimental results show that the proposed method achieves state-of-the-art performance on the publicly available FCVID and YLI-MED datasets. ",
    "code_link": ""
  },
  "cvpr2021_hvu_integralactionpose-drivenfeatureintegrationforrobusthumanactionrecognitioninvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "HVU",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Large Scale Holistic Video Understanding",
    "title": "IntegralAction: Pose-Driven Feature Integration for Robust Human Action Recognition in Videos",
    "authors": [
      "Gyeongsik Moon",
      "Heeseung Kwon",
      "Kyoung Mu Lee",
      "Minsu Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/html/Moon_IntegralAction_Pose-Driven_Feature_Integration_for_Robust_Human_Action_Recognition_in_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Moon_IntegralAction_Pose-Driven_Feature_Integration_for_Robust_Human_Action_Recognition_in_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Most current action recognition methods heavily rely on appearance information by taking an RGB sequence of entire image regions as input. While being effective in exploiting contextual information around humans, e.g., human appearance and scene category, they are easily fooled by out-of-context action videos where the contexts do not exactly match with target actions. In contrast, pose-based methods, which take a sequence of human skeletons only as input, suffer from inaccurate pose estimation or ambiguity of human pose per se. Integrating these two approaches has turned out to be non-trivial; training a model with both appearance and pose ends up with a strong bias towards appearance and does not generalize well to unseen videos. To address this problem, we propose to learn pose-driven feature integration that dynamically combines appearance and pose streams by observing pose features on the fly. The main idea is to let the pose stream decide how much and which appearance information is used in integration based on whether the given pose information is reliable or not. We show that the proposed IntegralAction achieves highly robust performance across in-context and out-of-context action video datasets. The codes are available in https://github.com/mks0601/IntegralAction_RELEASE. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_isolatedsignlanguagerecognitionwithmulti-scalespatial-temporalgraphconvolutionalnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Isolated Sign Language Recognition With Multi-Scale Spatial-Temporal Graph Convolutional Networks",
    "authors": [
      "Manuel Vazquez-Enriquez",
      "Jose L. Alba-Castro",
      "Laura Docio-Fernandez",
      "Eduardo Rodriguez-Banga"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Vazquez-Enriquez_Isolated_Sign_Language_Recognition_With_Multi-Scale_Spatial-Temporal_Graph_Convolutional_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Vazquez-Enriquez_Isolated_Sign_Language_Recognition_With_Multi-Scale_Spatial-Temporal_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Isolated Sign Language Recognition (ISLR) fits nicely in the domain of problems that can be handled by graph-structured spatial-temporal algorithms. A recent multiscale spatial-temporal graph convolution operator, MS-G3D, takes advantage of the semantic connectivity among non-neighbor nodes of the graph in a flexible temporal scale, which results in improved performance in classical Human Action Recognition datasets. In this work, we present a solution for ISLR using a skeleton graph that includes body and finger joints and makes use of this specific property of MS-G3D, which seems crucial to capture the internal relationship among semantically connected distant nodes in sign language dynamics. To complete the analysis, we compare the results with a 3D-CNN architecture, S3D, already used for SLR, and fuse it with MS-G3D. The performance achieved on the AUTSL dataset shows that MS-G3D alone stands out as a viable technique for ISLR. In fact, the improvement after fusing with a 3D-CNN approach, at least on this medium-scale dataset, appears marginal. The transfer learning capability of the trained models is also explored using pre-training with the larger WLASL dataset and post-training with the smaller LSE UVIGO dataset. The classification performance based on the MS-G3D model over AUTSL does not benefit from pre-training with WLASL, but the performance on the more similarly acquired LSE UVIGO dataset improves significantly from fine-tuning the MS-G3D AUTSL model. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_mutualsupportofdatamodalitiesinthetaskofsignlanguagerecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Mutual Support of Data Modalities in the Task of Sign Language Recognition",
    "authors": [
      "Ivan Gruber",
      "Zdenek Krnoul",
      "Marek Hruz",
      "Jakub Kanis",
      "Matyas Bohacek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Gruber_Mutual_Support_of_Data_Modalities_in_the_Task_of_Sign_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Gruber_Mutual_Support_of_Data_Modalities_in_the_Task_of_Sign_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents a method for automatic sign language recognition that was utilized in the CVPR 2021 ChaLearn Challenge (RGB track). Our method is composed of several approaches combined in an ensemble scheme to perform isolated sign-gesture recognition. We combine modalities of video sample frames processed by a 3D ConvNet (I3D), with body-pose information in the form of joint locations processed by a Transformer, hand region images transformed into a semantic space, and linguistically defined locations of hands. Although the individual models perform sub-par (60% to 93% accuracy on validation data), the weighted ensemble results in 95.46% accuracy. ",
    "code_link": "https://github.com/IBM/action-recognition-pytorch"
  },
  "cvpr2021_chalearn_isolatedsignrecognitionfromrgbvideousingposeflowandself-attention": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Isolated Sign Recognition From RGB Video Using Pose Flow and Self-Attention",
    "authors": [
      "Mathieu De Coster",
      "Mieke Van Herreweghe",
      "Joni Dambre"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/De_Coster_Isolated_Sign_Recognition_From_RGB_Video_Using_Pose_Flow_and_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/De_Coster_Isolated_Sign_Recognition_From_RGB_Video_Using_Pose_Flow_and_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Automatic sign language recognition lies at the intersection of natural language processing (NLP) and computer vision. The highly successful transformer architectures, based on multi-head attention, originate from the field of NLP. The Video Transformer Network (VTN) is an adaptation of this concept for tasks that require video understanding, e.g., action recognition. However, due to the limited amount of labeled data that is commonly available for training automatic sign (language) recognition, the VTN cannot reach its full potential in this domain. In this work, we reduce the impact of this data limitation by automatically pre-extracting useful information from the sign language videos. In our approach, different types of information are offered to a VTN in a multi-modal setup. It includes per-frame human pose keypoints (extracted by OpenPose) to capture the body movement and hand crops to capture the (evolution of) hand shapes. We evaluate our method on the recently released AUTSL dataset for isolated sign recognition and obtain 92.92% accuracy on the test set using only RGB data. For comparison: the VTN architecture without hand crops and pose flow achieved 82% accuracy. A qualitative inspection of our model hints at further potential of multi-modal multi-head attention in a sign language recognition context. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_skeletorskeletaltransformersforrobustbody-poseestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Skeletor: Skeletal Transformers for Robust Body-Pose Estimation",
    "authors": [
      "Tao Jiang",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Jiang_Skeletor_Skeletal_Transformers_for_Robust_Body-Pose_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Jiang_Skeletor_Skeletal_Transformers_for_Robust_Body-Pose_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Predicting 3D human pose from a single monoscopic video can be highly challenging due to factors such as low resolution, motion blur and occlusion, in addition to the fundamental ambiguity in estimating 3D from 2D. Approaches that directly regress the 3D pose from independent images can be particularly susceptible to these factors and result in jitter, noise and/or inconsistencies in skeletal estimation. Much of which can be overcome if the temporal evolution of the scene and skeleton are taken into account. However, rather than tracking body parts and trying to temporally smooth them, we propose a novel transformer based network that can learn a distribution over both pose and motion in an unsupervised fashion. We call our approach Skeletor. Skeletor overcomes inaccuracies in detection and corrects partial or entire skeleton corruption. Skeletor uses strong priors learn from on 25 million frames to correct skeleton sequences smoothly and consistently. Skeletor can achieve this as it implicitly learns the spatio-temporal context of human motion via a transformer based neural network. Extensive experiments show that Skeletor achieves improved performance on 3D human pose estimation and further provides benefits for downstream tasks such as sign language translation. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_signsegmentationwithchangepoint-modulatedpseudo-labelling": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Sign Segmentation With Changepoint-Modulated Pseudo-Labelling",
    "authors": [
      "Katrin Renz",
      "Nicolaj C. Stache",
      "Neil Fox",
      "Gul Varol",
      "Samuel Albanie"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Renz_Sign_Segmentation_With_Changepoint-Modulated_Pseudo-Labelling_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Renz_Sign_Segmentation_With_Changepoint-Modulated_Pseudo-Labelling_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The objective of this work is to find temporal boundaries between signs in continuous sign language. Motivated by the paucity of annotation available for this task, we propose a simple yet effective algorithm to improve segmentation performance on unlabelled signing footage from a domain of interest. We make the following contributions: (1) We motivate and introduce the task of source-free domain adaptation for sign language segmentation, in which labelled source data is available for an initial training phase, but is not available during adaptation. (2) We propose the Changepoint-Modulated Pseudo-Labelling (CMPL) algorithm to leverage cues from abrupt changes in motion-sensitive feature space to improve pseudo-labelling quality for adaptation. (3) We showcase the effectiveness of our approach for category-agnostic sign segmentation, transferring from the BSLCORPUS to the BSL-1K and RWTH-PHOENIX-Weather 2014 datasets, where we outperform the prior state of the art. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_chalearnlaplargescalesignerindependentisolatedsignlanguagerecognitionchallengedesign,resultsandfutureresearch": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "ChaLearn LAP Large Scale Signer Independent Isolated Sign Language Recognition Challenge: Design, Results and Future Research",
    "authors": [
      "Ozge Mercanoglu Sincan",
      "Julio C. S. Jacques Junior",
      "Sergio Escalera",
      "Hacer Yalim Keles"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Sincan_ChaLearn_LAP_Large_Scale_Signer_Independent_Isolated_Sign_Language_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Sincan_ChaLearn_LAP_Large_Scale_Signer_Independent_Isolated_Sign_Language_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The performances of Sign Language Recognition (SLR) systems have improved considerably in recent years. However, several open challenges still need to be solved to allow SLR to be useful in practice. The research in the field is in its infancy in regards to the robustness of the models to a large diversity of signs and signers, and to fairness of the models to performers from different demographics. This work summarises the ChaLearn LAP Large Scale Signer Independent Isolated SLR Challenge, organised at CVPR 2021 with the goal of overcoming some of the aforementioned challenges. We analyse and discuss the challenge design, top winning solutions and suggestions for future research. The challenge attracted 132 participants in the RGB track and 59 in the RGB+Depth track, receiving more than 1.5K submissions in total. Participants were evaluated using a new large-scale multi-modal Turkish Sign Language (AUTSL) dataset, consisting of 226 sign labels and 36,302 isolated sign video samples performed by 43 different signers. Winning teams achieved more than 96% recognition rate, and their approaches benefited from pose/hand/face estimation, transfer learning, external data, fusion/ensemble of modalities and different strategies to model spatio-temporal information. However, methods still fail to distinguish among very similar signs, in particular those sharing similar hand trajectories. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_skeletonawaremulti-modalsignlanguagerecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Skeleton Aware Multi-Modal Sign Language Recognition",
    "authors": [
      "Songyao Jiang",
      "Bin Sun",
      "Lichen Wang",
      "Yue Bai",
      "Kunpeng Li",
      "Yun Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Jiang_Skeleton_Aware_Multi-Modal_Sign_Language_Recognition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Jiang_Skeleton_Aware_Multi-Modal_Sign_Language_Recognition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Sign language is commonly used by deaf or speech impaired people to communicate but requires significant effort to master. Sign Language Recognition (SLR) aims to bridge the gap between sign language users and others by recognizing signs from given videos. It is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. Recently, skeleton-based action recognition attracts increasing attention due to the independence between the subject and background variation. However, skeleton-based SLR is still under exploration due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors with pose estimators to extract hand key points and learn to recognize sign language via Neural Networks, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton Aware Multi-modal SLR framework (SAMSLR) to take advantage of multi-modal information towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. RGB and depth modalities are also incorporated and assembled into our framework to provide global information that is complementary to the skeleton-based methods SL-GCN and SSTCN. As a result, SAM-SLR achieves the highest performance in both RGB (98.42%) and RGBD (98.53%) tracks in 2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/CVPR21Chal-SLR ",
    "code_link": "https://github.com/openmmlab/mmpose"
  },
  "cvpr2021_chalearn_signlanguageproductionareview": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Sign Language Production: A Review",
    "authors": [
      "Razieh Rastgoo",
      "Kourosh Kiani",
      "Sergio Escalera",
      "Mohammad Sabokrou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Rastgoo_Sign_Language_Production_A_Review_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Rastgoo_Sign_Language_Production_A_Review_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearing-impaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to briefly summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research. ",
    "code_link": ""
  },
  "cvpr2021_chalearn_evaluatingtheimmediateapplicabilityofposeestimationforsignlanguagerecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "ChaLearn",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - ChaLearn Looking at People: Sign Language Recognition in the Wild & Large Scale Signer Independent Isolated SLR Challenge",
    "title": "Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition",
    "authors": [
      "Amit Moryossef",
      "Ioannis Tsochantaridis",
      "Joe Dinn",
      "Necati Cihan Camgoz",
      "Richard Bowden",
      "Tao Jiang",
      "Annette Rios",
      "Mathias Muller",
      "Sarah Ebling"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Moryossef_Evaluating_the_Immediate_Applicability_of_Pose_Estimation_for_Sign_Language_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Moryossef_Evaluating_the_Immediate_Applicability_of_Pose_Estimation_for_Sign_Language_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Signed languages are visual languages produced by the movement of the hands, face, and body. In this paper, we evaluate representations based on skeleton poses, as these are explainable, person-independent, privacy-preserving, low-dimensional representations. Basically, skeletal representations generalize over an individual's appearance and background, allowing us to focus on the recognition of motion. But how much information is lost by the skeletal representation? We perform two independent studies using two state-of-the-art pose estimation systems. We analyze the applicability of the pose estimation systems to sign language recognition by evaluating the failure cases of the recognition models. Importantly, this allows us to characterize the current limitations of skeletal pose estimation approaches in sign language recognition. ",
    "code_link": "https://github.com/AmitMY/pose-format"
  },
  "cvpr2021_clvision_insightsfromthefutureforcontinuallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Insights From the Future for Continual Learning",
    "authors": [
      "Arthur Douillard",
      "Eduardo Valle",
      "Charles Ollion",
      "Thomas Robert",
      "Matthieu Cord"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Douillard_Insights_From_the_Future_for_Continual_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Douillard_Insights_From_the_Future_for_Continual_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Continual learning aims to learn tasks sequentially, with (often severe) constraints on the storage of old learning samples, without suffering from catastrophic forgetting. In this work, we propose prescient continual learning, a novel experimental setting, to incorporate existing information about the classes, prior to any training data. Usually, each task in a traditional continual learning setting evaluates the model on present and past classes, the latter with a limited number of training samples. Our setting adds future classes, with no training samples at all. We introduce Ghost Model, a representation-learning-based model for continual learning using ideas from zero-shot learning. A generative model of the representation space in concert with a careful adjustment of the losses allows us to exploit insights from future classes to constraint the spatial arrangement of the past and current classes. Quantitative results on the AwA2 and aP&Y datasets and detailed visualizations showcase the interest of this new setting and the method we propose to address it. ",
    "code_link": "https://github.com/arthurdouillard/incremental"
  },
  "cvpr2021_clvision_ternaryfeaturemaskszero-forgettingfortask-incrementallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Ternary Feature Masks: Zero-Forgetting for Task-Incremental Learning",
    "authors": [
      "Marc Masana",
      "Tinne Tuytelaars",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Masana_Ternary_Feature_Masks_Zero-Forgetting_for_Task-Incremental_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Masana_Ternary_Feature_Masks_Zero-Forgetting_for_Task-Incremental_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose an approach without any forgetting to continual learning for the task-aware regime, where at inference the task-label is known. By using ternary masks we can upgrade a model to new tasks, reusing knowledge from previous tasks while not forgetting anything about them. Using masks prevents both catastrophic forgetting and backward transfer. We argue -- and show experimentally -- that avoiding the former largely compensates for the lack of the latter, which is rarely observed in practice. In contrast to earlier works, our masks are applied to the features (activations) of each layer instead of the weights. This considerably reduces the number of mask parameters for each new task; with more than three orders of magnitude for most networks. The encoding of the ternary masks into two bits per feature creates very little overhead to the network, avoiding scalability issues. To allow already learned features to adapt to the current task without changing the behavior of these features for previous tasks, we introduce task-specific feature normalization. Extensive experiments on several finegrained datasets and ImageNet show that our method outperforms current state-of-the-art while reducing memory overhead in comparison to weight-based approaches. ",
    "code_link": "https://github.com/mmasana/TernaryFeatureMasks"
  },
  "cvpr2021_clvision_supervisedcontrastivereplayrevisitingthenearestclassmeanclassifierinonlineclass-incrementalcontinuallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning",
    "authors": [
      "Zheda Mai",
      "Ruiwen Li",
      "Hyunwoo Kim",
      "Scott Sanner"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Mai_Supervised_Contrastive_Replay_Revisiting_the_Nearest_Class_Mean_Classifier_in_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mai_Supervised_Contrastive_Replay_Revisiting_the_Nearest_Class_Mean_Classifier_in_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Online class-incremental continual learning (CL) studies the problem of learning new classes continually from an online non-stationary data stream, intending to adapt to new data while mitigating catastrophic forgetting. While memory replay has shown promising results, the recency bias in online learning caused by the commonly used Softmax classifier remains an unsolved challenge. Although the Nearest-Class-Mean (NCM) classifier is significantly undervalued in the CL community, we demonstrate that it is a simple yet effective substitute for the Softmax classifier. It addresses the recency bias and avoids structural changes in the fully-connected layer for new classes. Moreover, we observe considerable and consistent performance gains when replacing the Softmax classifier with the NCM classifier for several state-of-the-art replay methods. To leverage the NCM classifier more effectively, data embeddings belonging to the same class should be clustered and well-separated from those with a different class label. To this end, we contribute Supervised Contrastive Replay (SCR), which explicitly encourages samples from the same class to cluster tightly in embedding space while pushing those of different classes further apart during replay-based training. Overall, we observe that our proposed SCR substantially reduces catastrophic forgetting and outperforms state-of-the-art CL methods by a significant margin on a variety of datasets. ",
    "code_link": ""
  },
  "cvpr2021_clvision_cross-domainmulti-tasklearningforobjectdetectionandsaliencyestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Cross-Domain Multi-Task Learning for Object Detection and Saliency Estimation",
    "authors": [
      "Apoorv Khattar",
      "Srinidhi Hegde",
      "Ramya Hebbalaguppe"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Khattar_Cross-Domain_Multi-Task_Learning_for_Object_Detection_and_Saliency_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Khattar_Cross-Domain_Multi-Task_Learning_for_Object_Detection_and_Saliency_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-task learning (MTL) is a learning paradigm that aims at joint optimization of multiple tasks using a single neural network for better performance and generalization. In practice, MTL rests on the inherent assumption of availability of common datasets with ground truth labels for each of the downstream tasks. However, collecting such a common annotated dataset is laborious for complex computer vision tasks such as the saliency estimation which would require the eye fixation points as the ground truth data. To this end, we propose a novel MTL framework in the absence of common annotated dataset for joint estimation of important downstream tasks in computer vision - object detection and saliency estimation. Unlike many state-of-the-art methods, that rely on common annotated datasets for training, we consider the annotations from different datasets for jointly training different tasks, calling this setting as cross-domain MTL. We adapt MUTAN framework to fuse features from different datasets to learn domain invariant features capturing the relatedness of different tasks. We demonstrate the improvement in the performance and generalizability of our MTL architecture. We also show that the proposed MTL network offers a 13% reduction in memory footprint due to parameter sharing between the related tasks. ",
    "code_link": ""
  },
  "cvpr2021_clvision_cl-gymfull-featuredpytorchlibraryforcontinuallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "CL-Gym: Full-Featured PyTorch Library for Continual Learning",
    "authors": [
      "Seyed Iman Mirzadeh",
      "Hassan Ghasemzadeh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Mirzadeh_CL-Gym_Full-Featured_PyTorch_Library_for_Continual_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mirzadeh_CL-Gym_Full-Featured_PyTorch_Library_for_Continual_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Continual learning (CL) has become one of the most active research venues within the artificial intelligence community in recent years. Given the significant amount of attention paid to continual learning, the need for a library that facilitates both research and development in this field is more visible than ever. However, CL algorithms' codes are currently scattered over isolated repositories written with different frameworks, making it difficult for researchers and practitioners to work with various CL algorithms and benchmarks using the same interface. In this paper, we introduce CL-Gym, a full-featured continual learning library that overcomes this challenge and accelerates the research and development. In addition to the necessary infrastructure for running end-to-end continual learning experiments, CL-Gym includes benchmarks for various CL scenarios and several state-of-the-art CL algorithms. In this paper, we present the architecture, design philosophies, and technical details behind CL-Gym. ",
    "code_link": ""
  },
  "cvpr2021_clvision_avalancheanend-to-endlibraryforcontinuallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Avalanche: An End-to-End Library for Continual Learning",
    "authors": [
      "Vincenzo Lomonaco",
      "Lorenzo Pellegrini",
      "Andrea Cossu",
      "Antonio Carta",
      "Gabriele Graffieti",
      "Tyler L. Hayes",
      "Matthias De Lange",
      "Marc Masana",
      "Jary Pomponi",
      "Gido M. van de Ven",
      "Martin Mundt",
      "Qi She",
      "Keiland Cooper",
      "Jeremy Forest",
      "Eden Belouadah",
      "Simone Calderara",
      "German I. Parisi",
      "Fabio Cuzzolin",
      "Andreas S. Tolias",
      "Simone Scardapane",
      "Luca Antiga",
      "Subutai Ahmad",
      "Adrian Popescu",
      "Christopher Kanan",
      "Joost van de Weijer",
      "Tinne Tuytelaars",
      "Davide Bacciu",
      "Davide Maltoni"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Lomonaco_Avalanche_An_End-to-End_Library_for_Continual_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Lomonaco_Avalanche_An_End-to-End_Library_for_Continual_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms. ",
    "code_link": ""
  },
  "cvpr2021_clvision_graph-basedpersonsignatureforpersonre-identifications": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Graph-Based Person Signature for Person Re-Identifications",
    "authors": [
      "Binh X. Nguyen",
      "Binh D. Nguyen",
      "Tuong Do",
      "Erman Tjiputra",
      "Quang D. Tran",
      "Anh Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Nguyen_Graph-Based_Person_Signature_for_Person_Re-Identifications_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Nguyen_Graph-Based_Person_Signature_for_Person_Re-Identifications_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The task of person re-identification (ReID) is to match images of the same person over multiple non-overlapping camera views. Due to the variations in visual factors, previous works have investigated how the person identity, body parts, and attributes benefit the person ReID problem. However, the correlations between attributes, body parts, and within each attribute are not fully utilized. In this paper, we propose a new method to effectively aggregate detailed person descriptions (attributes labels) and visual features (body parts and global features) into a graph, namely Graph-based Person Signature, and utilize Graph Convolutional Networks to learn the topological structure of the visual signature of a person. The graph is integrated into a multi-branch multi-task framework for person re-identification. The extensive experiments are conducted to demonstrate the effectiveness of our proposed approach on two large-scale datasets, including Market-1501 and DukeMTMC-ReID. Our approach achieves competitive results among the state of the art and outperforms other attribute-based or mask-guided methods. ",
    "code_link": "https://github.com/aioz-ai/CVPRW21_GPS"
  },
  "cvpr2021_clvision_ataleoftwocilstheconnectionsbetweenclassincrementallearningandclassimbalancedlearning,andbeyond": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "A Tale of Two CILs: The Connections Between Class Incremental Learning and Class Imbalanced Learning, and Beyond",
    "authors": [
      "Chen He",
      "Ruiping Wang",
      "Xilin Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/He_A_Tale_of_Two_CILs_The_Connections_Between_Class_Incremental_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/He_A_Tale_of_Two_CILs_The_Connections_Between_Class_Incremental_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Catastrophic forgetting, the main challenge of Class Incremental Learning, is closely related to the classifier's bias due to imbalanced data, and most researchers resort to empirical techniques to remove the bias. Such anti-bias tricks share many ideas with the field of Class Imbalanced Learning, which encourages us to reflect on why these tricks work, and how we can design more principled solutions from a different perspective. In this paper, we comprehensively analyze the connections and seek possible collaborations between these two fields, i.e. Class Incremental Learning and Class Imbalanced Learning. Specifically, we first provide a panoramic view of recent bias correction tricks from the perspective of handling class imbalance. Then, we show that an adapted post-scaling technique which originates from Class Imbalanced Learning is on par with or even outperforms SOTA Class Incremental Learning method. Visualization via violin plots and polar charts further sheds light on how SOTA methods address the class imbalance problem from a more intuitive geometric perspective. These findings may encourage further infiltration between the two closely connected fields, but also raise concerns about whether it is correct that Class Incremental Learning degenerates into a class imbalance problem. ",
    "code_link": "https://github.com/TonyPod/Two-CILs"
  },
  "cvpr2021_clvision_class-incrementalexperiencereplayforcontinuallearningunderconceptdrift": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Class-Incremental Experience Replay for Continual Learning Under Concept Drift",
    "authors": [
      "Lukasz Korycki",
      "Bartosz Krawczyk"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Korycki_Class-Incremental_Experience_Replay_for_Continual_Learning_Under_Concept_Drift_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Korycki_Class-Incremental_Experience_Replay_for_Continual_Learning_Under_Concept_Drift_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Modern machine learning systems need to be able to cope with constantly arriving and changing data. Two main areas of research dealing with such scenarios are continual learning and data stream mining. Continual learning focuses on accumulating knowledge and avoiding forgetting, assuming information once learned should be stored. Data stream mining focuses on adaptation to concept drift and discarding outdated information, assuming that only the most recent data is relevant. While these two areas are mainly being developed in separation, they offer complementary views on the problem of learning from dynamic data. There is a need for unifying them, by offering architectures capable of both learning and storing new information, as well as revisiting and adapting to changes in previously seen concepts. We propose a novel continual learning approach that can handle both tasks. Our experience replay method is fueled by a centroid-driven memory storing diverse instances of incrementally arriving classes. This is enhanced with a reactive subspace buffer that tracks concept drift occurrences in previously seen classes and adapts clusters accordingly. The proposed architecture is thus capable of both remembering valid and forgetting outdated information, offering a holistic framework for continual learning under concept drift. ",
    "code_link": "https://github.com/lkorycki/rsb"
  },
  "cvpr2021_clvision_ilcocanincrementallearningframeworkbasedoncontrastiveone-classclassifiers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "ILCOC: An Incremental Learning Framework Based on Contrastive One-Class Classifiers",
    "authors": [
      "Wenju Sun",
      "Jing Zhang",
      "Danyu Wang",
      "Yangli-ao Geng",
      "Qingyong Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Sun_ILCOC_An_Incremental_Learning_Framework_Based_on_Contrastive_One-Class_Classifiers_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Sun_ILCOC_An_Incremental_Learning_Framework_Based_on_Contrastive_One-Class_Classifiers_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In the class incremental learning, the number of classes to be handled dynamically raises with the number of considered tasks. The main challenge of this learning schema is catastrophic forgetting, that is the performance degradation on old tasks after learning new tasks. Existing incremental learning algorithms generally choose to train a multi-class classifier (e.g. softmax classifier), which learns a decision boundary to divide the feature space into several parts. Therefore, when new data arrive, the learned boundary will be updated and thus may cause forgetting. Compared with multi-class classifiers, a one-class classifier focuses on characterizing the distribution of a single class. As a result, the decision boundary learned for each category is tighter and does not change during learning new tasks. Inspired by this characteristic of one-class classifier, we propose a novel incremental learning framework based on contrastive one-class classifiers (ILCOC) to avoid catastrophic forgetting. Specifically, we train a specific one-class classifier for each category and parallelly use them to achieve incremental multi-class recognition. Besides, we design a scale-boundary loss, a classifier-contrastive loss and a negative-suppression loss to strengthen the comparability of classifiers outputs and the discrimination ability of each one-class classifier. We evaluate ILCOC on MNIST, CIFAR-10 and Tiny-ImageNet datasets, and the experimental results show that ILCOC achieves state-of-the-art performance. ",
    "code_link": "https://github.com/SunWenJu123/ILCOC"
  },
  "cvpr2021_clvision_selectivereplayenhanceslearninginonlinecontinualanalogicalreasoning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Selective Replay Enhances Learning in Online Continual Analogical Reasoning",
    "authors": [
      "Tyler L. Hayes",
      "Christopher Kanan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Hayes_Selective_Replay_Enhances_Learning_in_Online_Continual_Analogical_Reasoning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Hayes_Selective_Replay_Enhances_Learning_in_Online_Continual_Analogical_Reasoning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In continual learning, a system learns from non-stationary data streams or batches without catastrophic forgetting. While this problem has been heavily studied in supervised image classification and reinforcement learning, continual learning in neural networks designed for abstract reasoning has not yet been studied. Here, we study continual learning of analogical reasoning. Analogical reasoning tests such as Raven's Progressive Matrices (RPMs) are commonly used to measure non-verbal abstract reasoning in humans, and recently offline neural networks for the RPM problem have been proposed. In this paper, we establish experimental baselines, protocols, and forward and backward transfer metrics to evaluate continual learners on RPMs. We employ experience replay to mitigate catastrophic forgetting. Prior work using replay for image classification tasks has found that selectively choosing the samples to replay offers little, if any, benefit over random selection. In contrast, we find that selective replay can significantly outperform random selection for the RPM task. ",
    "code_link": ""
  },
  "cvpr2021_clvision_continuallearningincross-modalretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Continual Learning in Cross-Modal Retrieval",
    "authors": [
      "Kai Wang",
      "Luis Herranz",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Wang_Continual_Learning_in_Cross-Modal_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Wang_Continual_Learning_in_Cross-Modal_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multimodal representations and continual learning are two areas closely related to human intelligence. The former considers the learning of shared representation spaces where information from different modalities can be compared and integrated (we focus on cross-modal retrieval between language and visual representations). The latter studies how to prevent forgetting a previously learned task when learning a new one. While humans excel in these two aspects, deep neural networks are still quite limited. In this paper, we propose a combination of both problems into a continual cross-modal retrieval setting, where we study how the catastrophic interference caused by new tasks impacts the embedding spaces and their cross-modal alignment required for effective retrieval. We propose a general framework that decouples the training, indexing and querying stages. We also identify and study different factors that may lead to forgetting, and propose tools to alleviate it. We found that the indexing stage pays an important role and that simply avoiding reindexing the database with updated embedding networks can lead to significant gains. We evaluated our methods in two image-text retrieval datasets, obtaining significant gains with respect to the fine tuning baseline. ",
    "code_link": ""
  },
  "cvpr2021_clvision_plasticandstablegatedclassifiersforcontinuallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Plastic and Stable Gated Classifiers for Continual Learning",
    "authors": [
      "Nicholas I-Hsien Kuo",
      "Mehrtash Harandi",
      "Nicolas Fourrier",
      "Christian Walder",
      "Gabriela Ferraro",
      "Hanna Suominen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Kuo_Plastic_and_Stable_Gated_Classifiers_for_Continual_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Kuo_Plastic_and_Stable_Gated_Classifiers_for_Continual_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Conventional neural networks are mostly high in plasticity but low in stability. Hence, catastrophic forgetting tends to occur over the sequential training of multiple tasks and a backbone learner loses its ability in solving a previously learnt task. Several studies have shown that catastrophic forgetting can be partially mitigated through freezing the feature extractor weights while only sequentially training the classifier network. Though these are effective methods in retaining knowledge, forgetting could still become severe if the classifier network is over-parameterised over many tasks. As a remedy, this paper presents a novel classifier design with high stability. Highway-Connection Classifier Networks (HCNs) leverage gated units to alleviate forgetting. When employed alone, they exhibit strong robustness against forgetting. In addition, they synergise well with many existing and popular continual learning archetypes. ",
    "code_link": ""
  },
  "cvpr2021_clvision_class-incrementallearningwithgenerativeclassifiers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Class-Incremental Learning With Generative Classifiers",
    "authors": [
      "Gido M. van de Ven",
      "Zhe Li",
      "Andreas S. Tolias"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/van_de_Ven_Class-Incremental_Learning_With_Generative_Classifiers_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/van_de_Ven_Class-Incremental_Learning_With_Generative_Classifiers_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Incrementally training deep neural networks to recognize new classes is a challenging problem. Most existing class-incremental learning methods store data or use generative replay, both of which have drawbacks, while 'rehearsal-free' alternatives such as parameter regularization or bias-correction methods do not consistently achieve high performance. Here, we put forward a new strategy for class-incremental learning: generative classification. Rather than directly learning the conditional distribution p(y|x), our proposal is to learn the joint distribution p(x,y), factorized as p(x|y)p(y), and to perform classification using Bayes' rule. As a proof-of-principle, here we implement this strategy by training a variational autoencoder for each class to be learned and by using importance sampling to estimate the likelihoods p(x|y). This simple approach performs very well on a diverse set of continual learning benchmarks, outperforming generative replay and other existing baselines that do not store data. ",
    "code_link": ""
  },
  "cvpr2021_clvision_essentialsforclassincrementallearning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Essentials for Class Incremental Learning",
    "authors": [
      "Sudhanshu Mittal",
      "Silvio Galesso",
      "Thomas Brox"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Mittal_Essentials_for_Class_Incremental_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mittal_Essentials_for_Class_Incremental_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Contemporary neural networks are limited in their ability to learn from evolving streams of training data. When trained sequentially on new or evolving tasks, their accuracy drops sharply, making them unsuitable for many real-world applications. In this work, we shed light on the causes of this well-known yet unsolved phenomenon - often referred to as catastrophic forgetting - in a class-incremental setup. We show that a combination of simple components and a loss that balances intra-task and inter-task learning can already resolve forgetting to the same extent as more complex measures proposed in the literature. Moreover, we identify the poor quality of the learned representation as another reason for catastrophic forgetting in class-IL. We show that performance is correlated with secondary class information (dark knowledge) learned by the model and it can be improved by an appropriate regularizer. With these lessons learned, class-incremental learning results on CIFAR-100 and ImageNet improve over the state-of-the-art by a large margin, while keeping the approach simple. ",
    "code_link": ""
  },
  "cvpr2021_clvision_ib-drr-incrementallearningwithinformation-backdiscreterepresentationreplay": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "IB-DRR - Incremental Learning With Information-Back Discrete Representation Replay",
    "authors": [
      "Jian Jiang",
      "Edoardo Cetin",
      "Oya Celiktutan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Jiang_IB-DRR_-_Incremental_Learning_With_Information-Back_Discrete_Representation_Replay_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Jiang_IB-DRR_-_Incremental_Learning_With_Information-Back_Discrete_Representation_Replay_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Incremental learning aims to enable machine learning models to continuously acquire new knowledge given new classes, while maintaining the knowledge already learned for old classes. Saving a subset of training samples of previously seen classes in the memory and replaying them during new training phases is proven to be an efficient and effective way to fulfil this aim. It is evident that the larger number of exemplars the model inherits the better performance it can achieve. However, finding a trade-off between the model performance and the number of samples to save for each class is still an open problem for replay-based incremental learning and is increasingly desirable for real-life applications. In this paper, we approach this open problem by tapping into a two-step compression approach. The first step is a lossy compression, we propose to encode input images and save their discrete latent representations in the form of codes that are learned using a hierarchical Vector Quantised Variational Autoencoder (VQ-VAE). In the second step, we further compress codes losslessly by learning a hierarchical latent variable model with bits-back asymmetric numeral systems (BB-ANS). To compensate for the information lost in the first step compression, we introduce an Information Back (IB) mechanism that utilizes real exemplars for a contrastive learning loss to regularize the training of a classifier. By maintaining all seen exemplars' representations in the format of `codes', Discrete Representation Replay (DRR) outperforms the state-of-art method on CIFAR-100 by a margin of 4% accuracy with a much less memory cost required for saving samples. Incorporated with IB and saving a small set of old raw exemplars as well, the accuracy of DRR can be further improved by 2% accuracy. ",
    "code_link": ""
  },
  "cvpr2021_clvision_dual-teacherclass-incrementallearningwithdata-freegenerativereplay": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay",
    "authors": [
      "Yoojin Choi",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Choi_Dual-Teacher_Class-Incremental_Learning_With_Data-Free_Generative_Replay_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Choi_Dual-Teacher_Class-Incremental_Learning_With_Data-Free_Generative_Replay_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper proposes two novel knowledge transfer techniques for class-incremental learning (CIL). First, we propose data-free generative replay (DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples from a generative model. In the conventional generative replay, the generative model is pre-trained for old data and shared in extra memory for later incremental learning. In our proposed DF-GR, we train a generative model from scratch without using any training data, based on the pre-trained classification model from the past, so we curtail the cost of sharing pre-trained generative models. Second, we introduce dual-teacher information distillation (DT-ID) for knowledge distillation from two teachers to one student. In CIL, we use DT-ID to learn new classes incrementally based on the pre-trained model for old classes and another model (pre-)trained on the new data for new classes. We implemented the proposed schemes on top of one of the state-of-the-art CIL methods and showed the performance improvement on CIFAR-100 and ImageNet datasets. ",
    "code_link": ""
  },
  "cvpr2021_clvision_neuralarchitecturesearchofdeeppriorstowardscontinuallearningwithoutcatastrophicinterference": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CLVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Continual Learning in Computer Vision",
    "title": "Neural Architecture Search of Deep Priors: Towards Continual Learning Without Catastrophic Interference",
    "authors": [
      "Martin Mundt",
      "Iuliia Pliushch",
      "Visvanathan Ramesh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Mundt_Neural_Architecture_Search_of_Deep_Priors_Towards_Continual_Learning_Without_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mundt_Neural_Architecture_Search_of_Deep_Priors_Towards_Continual_Learning_Without_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper we analyze the classification performance of neural network structures without parametric inference. Making use of neural architecture search, we empirically demonstrate that it is possible to find random weight architectures, a deep prior, that enables a linear classification to perform on par with fully trained deep counterparts. Through ablation experiments, we exclude the possibility of winning a weight initialization lottery and confirm that suitable deep priors do not require additional inference. In an extension to continual learning, we investigate the possibility of catastrophic interference free incremental learning. Under the assumption of classes originating from the same data distribution, a deep prior found on only a subset of classes is shown to allow discrimination of further classes through training of a simple linear classifier. ",
    "code_link": "https://github.com/ccc-frankfurt/DP-NAS"
  },
  "cvpr2021_omnicv_3dobjectdetectionfromasinglefisheyeimagewithoutasinglefisheyetrainingimage": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "3D Object Detection From a Single Fisheye Image Without a Single Fisheye Training Image",
    "authors": [
      "Elad Plaut",
      "Erez Ben Yaacov",
      "Bat El Shlomo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Plaut_3D_Object_Detection_From_a_Single_Fisheye_Image_Without_a_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Plaut_3D_Object_Detection_From_a_Single_Fisheye_Image_Without_a_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Existing monocular 3D object detection methods have been demonstrated on rectilinear perspective images and fail in images with alternative projections such as those acquired by fisheye cameras. Previous works on object detection in fisheye images have focused on 2D object detection, partly due to the lack of 3D datasets of such images. In this work, we show how to use existing monocular 3D object detection models, trained only on rectilinear images, to detect 3D objects in images from fisheye cameras, without using any fisheye training data. We outperform the only existing method for monocular 3D object detection in panoramas on a benchmark of synthetic data, despite the fact that the existing method trains on the target non-rectilinear projection whereas we train only on rectilinear images. We also experiment with an internal dataset of real fisheye images. ",
    "code_link": ""
  },
  "cvpr2021_omnicv_omnilayoutroomlayoutreconstructionfromindoorsphericalpanoramas": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "OmniLayout: Room Layout Reconstruction From Indoor Spherical Panoramas",
    "authors": [
      "Shivansh Rao",
      "Vikas Kumar",
      "Daniel Kifer",
      "C. Lee Giles",
      "Ankur Mali"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Rao_OmniLayout_Room_Layout_Reconstruction_From_Indoor_Spherical_Panoramas_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Rao_OmniLayout_Room_Layout_Reconstruction_From_Indoor_Spherical_Panoramas_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Given a single RGB panorama, the goal of 3D layout reconstruction is to estimate the room layout by predicting the corners, floor boundary, and ceiling boundary. A common approach has been to use standard convolutional networks to predict the corners and boundaries, followed by post-processing to generate the 3D layout. However, the space-varying distortions in panoramic images are not compatible with the translational equivariance property of standard convolutions, thus degrading performance. Instead, we propose to use spherical convolutions. The resulting network, which we call OmniLayout performs convolutions directly on the sphere surface, sampling according to inverse equirectangular projection and hence invariant to equirectangular distortions. Using a new evaluation metric, we show that our network reduces the error in the heavily distorted regions (near the poles) by 25% when compared to standard convolutional networks. Experimental results show that OmniLayout outperforms the state-of-the-art by 4% on two different benchmark datasets (PanoContext and Stanford 2D-3D). Code is available at https://github.com/rshivansh/OmniLayout. ",
    "code_link": "https://github.com/rshivansh/OmniLayout"
  },
  "cvpr2021_omnicv_omniflowhumanomnidirectionalopticalflow": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "OmniFlow: Human Omnidirectional Optical Flow",
    "authors": [
      "Roman Seidel",
      "Andre Apitzsch",
      "Gangolf Hirtz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Seidel_OmniFlow_Human_Omnidirectional_Optical_Flow_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Seidel_OmniFlow_Human_Omnidirectional_Optical_Flow_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Optical flow is the motion of a pixel between at least two consecutive video frames and can be estimated through an end-to-end trainable convolutional neural network. To this end, large training datasets are required to improve the accuracy of optical flow estimation. Our paper presents OmniFlow: a new synthetic omnidirectional human optical flow dataset. Based on a rendering engine we create a naturalistic 3D indoor environment with textured rooms, characters, actions, objects, illumination and motion blur where all components of the environment are shuffled during the data capturing process. The simulation has as output rendered images of household activities and the corresponding forward and backward optical flow. To verify the data for training volumetric correspondence networks for optical flow estimation we train different subsets of the data and test on OmniFlow with and without Test-Time-Augmentation. As a result we have generated 23,653 image pairs and corresponding forward and backward optical flow. Our dataset can be downloaded from: https://mytuc.org/byfs ",
    "code_link": ""
  },
  "cvpr2021_omnicv_detectinglow-rankregionsinomnidirectionalimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Detecting Low-Rank Regions in Omnidirectional Images",
    "authors": [
      "Zoltan Kato",
      "Gabor Nagy",
      "Martin Humenberger",
      "Gabriela Csurka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Kato_Detecting_Low-Rank_Regions_in_Omnidirectional_Images_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Kato_Detecting_Low-Rank_Regions_in_Omnidirectional_Images_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Planar low-rank regions commonly found in man-made environments, can be used to estimate a rectifying homography that provides valuable information about the camera and the 3D plane they observe. Methods to recover such a homography exist, but detection of low-rank regions is largely unsolved, especially for omnidirectional cameras where significant distortions make the problem even more challenging. In this paper we address this problem as follows. First we propose a method to generate a low-rank probability map on an omnidirectional image and use it to build a training set in a self-supervised manner to train deep models to predict low-rank likelihood maps for omnidirectional images. Second, we propose to adapt regular CNN operators to equirectangular images and to combine them seamlessly into a network where each layer preserves the properties of the equirectangular representation. Finally, on the new KITTI360 dataset, we show that the rectifying homography of detected low-rank regions in such predicted maps allows to factorize out the camera-plane pose up to certain ambiguities that can be easily overcome. ",
    "code_link": ""
  },
  "cvpr2021_omnicv_evaluatingtheimpactofwide-anglelensdistortiononlearning-baseddepthestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Evaluating the Impact of Wide-Angle Lens Distortion on Learning-Based Depth Estimation",
    "authors": [
      "Julie Buquet",
      "Jinsong Zhang",
      "Patrice Roulet",
      "Simon Thibault",
      "Jean-Francois Lalonde"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Buquet_Evaluating_the_Impact_of_Wide-Angle_Lens_Distortion_on_Learning-Based_Depth_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Buquet_Evaluating_the_Impact_of_Wide-Angle_Lens_Distortion_on_Learning-Based_Depth_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Most computer vision research focuses on narrow angle lenses and is not adapted to super-wide-angle (aka spherical) lenses. This is mainly because current neural networks are not designed or trained to interpret the significant barrel distortion that is introduced in the captured image by such wide angle lenses.As these lenses capture a half-sphere or a section of sphere on the object space, barrel distortion appears when the image is projected on a 2D flat image sensor. By controlling this distortion at the lens design stage, camera designers can create some areas with augmented resolution. In this work, we present an analysis of the impact of such augmented resolution on computer vision algorithm accuracy, using the problem of single image depth estimation as a case study. To this end, 360deg panorama datasets are warped to simulate different wide-angle lens datasets, which are then used to train identical neural networks. Each lens presents specific areas of the image with augmented resolution using spatially-varying non-linear distortion. We show that this property leads to better local accuracy in depth estimation. We also demonstrate that considering lens manufacturing improves performance when tested on realistic lenses, especially in the area of augmented resolution. We further show that this property helps to locally come closer to performances obtained on perspective images without cropping the field of view. ",
    "code_link": ""
  },
  "cvpr2021_omnicv_scaled360layoutsrevisitingnon-centralpanoramas": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Scaled 360 Layouts: Revisiting Non-Central Panoramas",
    "authors": [
      "Bruno Berenguel-Baeta",
      "Jesus Bermudez-Cameo",
      "Jose J. Guerrero"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Berenguel-Baeta_Scaled_360_Layouts_Revisiting_Non-Central_Panoramas_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Berenguel-Baeta_Scaled_360_Layouts_Revisiting_Non-Central_Panoramas_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " From a non-central panorama, 3D lines can be recovered by geometric reasoning. However, their sensitivity to noise and the complex geometric modeling required has led these panoramas being very little investigated. In this work we present a novel approach for 3D layout recovery of indoor environments using single non-central panoramas. We obtain the boundaries of the structural lines of the room from a non-central panorama using deep learning and exploit the properties of non-central projection systems in a new geometrical processing to recover the scaled layout. We solve the problem for Manhattan environments, handling occlusions, and also for Atlanta environments in an unified method. The experiments performed improve the state-of-the-art methods for 3D layout recovery from a single panorama. Our approach is the first work using deep learning with non-central panoramas and recovering the scale of single panorama layouts. ",
    "code_link": ""
  },
  "cvpr2021_omnicv_panodrsphericalpanoramadiminishedrealityforindoorscenes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "PanoDR: Spherical Panorama Diminished Reality for Indoor Scenes",
    "authors": [
      "Vasileios Gkitsas",
      "Vladimiros Sterzentsenko",
      "Nikolaos Zioulis",
      "Georgios Albanis",
      "Dimitrios Zarpalas"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Gkitsas_PanoDR_Spherical_Panorama_Diminished_Reality_for_Indoor_Scenes_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Gkitsas_PanoDR_Spherical_Panorama_Diminished_Reality_for_Indoor_Scenes_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The rising availability of commercial 360o cameras that democratize indoor scanning, has increased the interest for novel applications, such as interior space re-design. Diminished Reality (DR) fulfills the requirement of such applications, to remove existing objects in the scene, essentially translating this to a counterfactual inpainting task. While recent advances in data-driven inpainting have shown significant progress in generating realistic samples, they are not constrained to produce results with reality mapped structures. To preserve the 'reality' in indoor (re-)planning applications, the scene's structure preservation is crucial. To ensure structure-aware counterfactual inpainting, we propose a model that initially predicts the structure of a indoor scene and then uses it to guide the reconstruction of an empty - background only - representation of the same scene. We train and compare against other state-of-the-art methods on a version of the Structured3D dataset [47] modified for DR, showing superior results in both quantitative metrics and qualitative results, but more interestingly, our approach exhibits a much faster convergence rate. Code and models are available at github.com/VCL3D/PanoDR/ ",
    "code_link": "https://github.com/VCL3D/PanoDR"
  },
  "cvpr2021_omnicv_pano3daholisticbenchmarkandasolidbaselinefor360degdepthestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Pano3D: A Holistic Benchmark and a Solid Baseline for 360deg Depth Estimation",
    "authors": [
      "Georgios Albanis",
      "Nikolaos Zioulis",
      "Petros Drakoulis",
      "Vasileios Gkitsas",
      "Vladimiros Sterzentsenko",
      "Federico Alvarez",
      "Dimitrios Zarpalas",
      "Petros Daras"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Albanis_Pano3D_A_Holistic_Benchmark_and_a_Solid_Baseline_for_360deg_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Albanis_Pano3D_A_Holistic_Benchmark_and_a_Solid_Baseline_for_360deg_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Pano3D is a new benchmark for depth estimation from spherical panoramas. It aims to assess performance across all depth estimation traits, the primary direct depth estimation performance targeting precision and accuracy, and also the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D moves beyond typical intro-dataset evaluation to inter-dataset performance assessment. By disentangling the capacity to generalize in unseen data into different test splits, Pano3D represents a holistic benchmark for 360 depth estimation. We use it as a basis for an extended analysis seeking to offer insights into classical choices for depth estimation. This results in a solid baseline for panoramic depth that follow-up works can build upon to steer future progress ",
    "code_link": "https://github.com/ai-in-motion/moai"
  },
  "cvpr2021_omnicv_fastsolversforminimalradialdistortionrelativeposeproblems": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "OmniCV",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Fast Solvers for Minimal Radial Distortion Relative Pose Problems",
    "authors": [
      "Magnus Oskarsson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/html/Oskarsson_Fast_Solvers_for_Minimal_Radial_Distortion_Relative_Pose_Problems_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/OmniCV/papers/Oskarsson_Fast_Solvers_for_Minimal_Radial_Distortion_Relative_Pose_Problems_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper we present a unified formulation for a large class of relative pose problems with radial distortion and varying calibration. For minimal cases, we show that one can eliminate the number of parameters down to one to three. The relative pose can then be expressed using varying calibration constraints on the fundamental matrix, with entries that are polynomial in the parameters. We can then apply standard techniques based on the action matrix and Sturm sequences to construct our solvers. This enables efficient solvers for a large class of relative pose problems with radial distortion, using a common framework. We evaluate a number of these solvers for robust two-view inlier and epipolar geometry estimation, used as minimal solvers in RANSAC. ",
    "code_link": "https://github.com/hamburgerlady/fast-radial-solvers"
  },
  "cvpr2021_cvmi_x-netwithdifferentlossfunctionsforcellimagesegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "X-Net With Different Loss Functions for Cell Image Segmentation",
    "authors": [
      "Haruki Fujii",
      "Hayato Tanaka",
      "Momoko Ikeuchi",
      "Kazuhiro Hotta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Fujii_X-Net_With_Different_Loss_Functions_for_Cell_Image_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Fujii_X-Net_With_Different_Loss_Functions_for_Cell_Image_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Convolutional neural network is valid for object segmentation. In recent years, it has been applied to the fields of medicine and cell biology. Each class has a different number of pixels in an image. Therefore, the accuracy of semantic segmentation varies drastically between objects with a large number of pixels and objects with a small number of pixels. In this paper, we propose X-Net that integrates two encoders and decoders to solve this problem. This has the advantage of extracting rich features from two encoders and using two decoders to complement the location information and small objects. By using different loss functions for each decoder, we can use the ensemble of two decoders with different viewpoints. We evaluated our method on the Arabidopsis thaliana cell images and Drosophila cell images. Experimental results show that our method achieved better accuracy than the conventional methods. ",
    "code_link": ""
  },
  "cvpr2021_cvmi_quantifyingvariabilityinmicroscopyimageanalysesforcovid-19drugdiscovery": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Quantifying Variability in Microscopy Image Analyses for COVID-19 Drug Discovery",
    "authors": [
      "Mylene Simon",
      "Sunny Yu",
      "Jayapriya Nagarajan",
      "Peter Bajcsy",
      "Nicholas J. Schaub",
      "Mohamed Ouladi",
      "Sudharsan Prativadi",
      "Nathan Hotaling"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Simon_Quantifying_Variability_in_Microscopy_Image_Analyses_for_COVID-19_Drug_Discovery_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Simon_Quantifying_Variability_in_Microscopy_Image_Analyses_for_COVID-19_Drug_Discovery_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Microscopy image-based measurement variability in high-throughput imaging experiments for biological drug discoveries, such as COVID-19 therapies was addressed in this study. Variability of measurements came from (1) computational approaches (methods), (2) implementations of methods, (3) parameter settings, (4) chaining methods into workflows, and (5) stabilities of floating-point arithmetic on diverse hardware. Measurement variability was addressed by (a) introducing interoperability between algorithms, (b) enforcing automated capture of computational provenance and parameter settings, and (c) quantifying multiple sources of variabilities for 10 nucleus measurements, from 8 workflow streams, executed in 2 workflow graph configurations, on 2 computational hardware platforms at 2 locations. Using modified Mean Absolute Error (mMAE [%]) to compare measurements, We concluded that for the task of image-based nucleus measurements the variability sources were (1) implementations (0.10 % - 5.72 % per measurement), (2) methods (3.08 % - 3.11 % between Otsu thresholding and CellPose segmentation), (3) parameters (1.16 %-1.17 % between 4- and 8-neighbor connectivity), (4) workflow graph construction and computer hardware (negligible). ",
    "code_link": ""
  },
  "cvpr2021_cvmi_hierarchicalspatialpyramidnetworkforcervicalprecanceroussegmentationbyreconstructingdeepsegmentationnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Hierarchical Spatial Pyramid Network for Cervical Precancerous Segmentation by Reconstructing Deep Segmentation Networks",
    "authors": [
      "Zhu Meng",
      "Zhicheng Zhao",
      "Fei Su",
      "Limei Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Meng_Hierarchical_Spatial_Pyramid_Network_for_Cervical_Precancerous_Segmentation_by_Reconstructing_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Meng_Hierarchical_Spatial_Pyramid_Network_for_Cervical_Precancerous_Segmentation_by_Reconstructing_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Cervical cancer is one of the leading causes of cancer death in women aged 20 to 39 years, which emphasizes the importance of cervical precancerous diagnosis and treatment. Although there are many attempts on medical image processing, the research on the automatic diagnosis of cervical precancerous pathology is still scarce. In this paper, a challenging end-to-end automatic segmentation task for cervical precancerous diagnosis is focused. Specifically, considering that the diagnosis of cervical lesions relies heavily on spatial information, a hierarchical spatial pyramid network (HSP-Net) is proposed to enhance the representation ability of cervical structural features. First, a vertical hierarchical spatial pyramid (V-HSP) network is devised to aggregate the multiscale information during the feature extraction of the encoder. Second, a horizontal hierarchical spatial pyramid (H-HSP) network is designed to fuse information of multiscale receptive fields before and after cascading features from different branches. Experiments on the public dataset MTCHI demonstrate that HSP-Net achieves the state-of-the-art performance, reflecting the potential to assist doctors and patients clinically. ",
    "code_link": ""
  },
  "cvpr2021_cvmi_3dfibersegmentationwithdeepcenterregressionandgeometricclustering": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "3D Fiber Segmentation With Deep Center Regression and Geometric Clustering",
    "authors": [
      "Camilo Aguilar",
      "Mary Comer",
      "Imad Hanhan",
      "Ronald Agyei",
      "Michael Sangid"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Aguilar_3D_Fiber_Segmentation_With_Deep_Center_Regression_and_Geometric_Clustering_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Aguilar_3D_Fiber_Segmentation_With_Deep_Center_Regression_and_Geometric_Clustering_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Material and biological sciences frequently generate large amounts of microscope data that require 3D object-level segmentation. Often, the objects of interest have a common geometry, for example spherical, ellipsoidal, or cylindrical shapes. Neural networks have became a popular approach for object detection but they are often limited by their training dataset and have difficulties adapting to new data. In this paper, we propose a volumetric object detection approach for microscopy volumes comprised of fibrous structures by using deep centroid regression and geometric regularization. To this end, we train encoder-decoder networks for segmentation and centroid regression. We use the regression information combined with prior system knowledge to propose cylindrical objects and enforce geometric regularization in the segmentation. We train our networks on synthetic data and then test the trained networks in several experimental datasets. Our approach shows competitive results against other 3D segmentation methods when tested on the synthetic data and outperforms those other methods across different datasets. ",
    "code_link": "https://github.com/camilo-aguilar/3DFiber-Segmentation"
  },
  "cvpr2021_cvmi_learningmelanocyticproliferationsegmentationinhistopathologyimagesfromimperfectannotations": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Learning Melanocytic Proliferation Segmentation in Histopathology Images From Imperfect Annotations",
    "authors": [
      "Kechun Liu",
      "Mojgan Mokhtari",
      "Beibin Li",
      "Shima Nofallah",
      "Caitlin May",
      "Oliver Chang",
      "Stevan Knezevich",
      "Joann Elmore",
      "Linda Shapiro"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Liu_Learning_Melanocytic_Proliferation_Segmentation_in_Histopathology_Images_From_Imperfect_Annotations_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Liu_Learning_Melanocytic_Proliferation_Segmentation_in_Histopathology_Images_From_Imperfect_Annotations_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Melanoma is the third most common type of skin cancer and is responsible for the most skin cancer deaths. A diagnosis of melanoma is made by the visual interpretation of tissue sections by a pathologist, a challenging task given the complexity and breadth of melanocytic lesions and the subjective nature of biopsy interpretation. We leverage advances in computer vision to aid melanoma diagnosis by segmenting potential regions of lesions on digital images of whole slide skin biopsies. In this study, we demonstrate a Mask-R-CNN-based segmentation framework for such a purpose. To alleviate the cost of data annotation, we leverage a sparse annotation pipeline. Our model can be trained on sparse and noisy labels and achieves state-of-the-art performance in identifying melanocytic proliferations, producing a segmentation with Dice score 0.719, mIOU 0.740 and overall pixel accuracy 0.927. ",
    "code_link": ""
  },
  "cvpr2021_cvmi_rcnn-slicenetasliceandclusterapproachfornucleicentroiddetectioninthree-dimensionalfluorescencemicroscopyimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images",
    "authors": [
      "Liming Wu",
      "Shuo Han",
      "Alain Chen",
      "Paul Salama",
      "Kenneth W. Dunn",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Wu_RCNN-SliceNet_A_Slice_and_Cluster_Approach_for_Nuclei_Centroid_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Wu_RCNN-SliceNet_A_Slice_and_Cluster_Approach_for_Nuclei_Centroid_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Robust and accurate nuclei centroid detection is important for the understanding of biological structures in fluorescence microscopy images. Existing automated nuclei localization methods face three main challenges: (1) Most of object detection methods work only on 2D images and are difficult to extend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes but it is computational expensive for large microscopy volumes and they have difficulty distinguishing different instances of objects; (3) Hand annotated ground truth is limited for 3D microscopy volumes. To address these issues, we present a scalable approach for nuclei centroid detection of 3D microscopy volumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each slice of the volume from different directions and 3D agglomerative hierarchical clustering (AHC) is used to estimate the 3D centroids of nuclei in a volume. The model was trained with the synthetic microscopy data generated using Spatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and tested on different types of real 3D microscopy data. Extensive experimental results demonstrate that our proposed method can accurately count and detect the nuclei centroids in a 3D microscopy volume. ",
    "code_link": ""
  },
  "cvpr2021_cvmi_ajointspatialandmagnificationbasedattentionframeworkforlargescalehistopathologyclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "A Joint Spatial and Magnification Based Attention Framework for Large Scale Histopathology Classification",
    "authors": [
      "Jingwei Zhang",
      "Ke Ma",
      "John Van Arnam",
      "Rajarsi Gupta",
      "Joel Saltz",
      "Maria Vakalopoulou",
      "Dimitris Samaras"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Zhang_A_Joint_Spatial_and_Magnification_Based_Attention_Framework_for_Large_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Zhang_A_Joint_Spatial_and_Magnification_Based_Attention_Framework_for_Large_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep learning has achieved great success in processing large size medical images such as histopathology slides. However, conventional deep learning methods cannot handle the enormous image sizes; instead, they split the image into patches which are exhaustively processed, usually through multi-instance learning approaches. Moreover and especially in histopathology, determining the most appropriate magnification to generate these patches is also exhaustive: a model needs to traverse all the possible magnifications to select the optimal one. These limitations make the application of deep learning on large medical images and in particular histopathological images markedly inefficient. To tackle these problems, we propose a novel spatial and magnification based attention sampling strategy. First, we use a down-sampled large size image to estimate an attention map that represents a spatial probability distribution of informative patches at different magnifications. Then a small number of patches are cropped from the large size medical image at certain magnifications based on the obtained attention. The final label of the large size image is predicted solely by these patches using an end-to-end training strategy. Our experiments on two different histopathology datasets, the publicly available BACH and a subset of the TCGA-PRAD dataset, demonstrate that the proposed method runs 2.5 times faster with automatic magnification selection in training and at least 1.6 times faster than using all patches in inference as the most of state-of-the-art methods do, without loosing in performance. ",
    "code_link": ""
  },
  "cvpr2021_cvmi_unsuperviseddetectionofcancerousregionsinhistologyimageryusingimage-to-imagetranslation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVMI",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Unsupervised Detection of Cancerous Regions in Histology Imagery Using Image-to-Image Translation",
    "authors": [
      "Dejan Stepec",
      "Danijel Skocaj"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/html/Stepec_Unsupervised_Detection_of_Cancerous_Regions_in_Histology_Imagery_Using_Image-to-Image_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVMI/papers/Stepec_Unsupervised_Detection_of_Cancerous_Regions_in_Histology_Imagery_Using_Image-to-Image_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Detection of visual anomalies refers to the problem of finding patterns in different imaging data that do not conform to the expected visual appearance and is a widely studied problem in different domains. Due to the nature of anomaly occurrences and underlying generating processes, it is hard to characterize them and obtain labeled data. Obtaining labeled data is especially difficult in biomedical applications, where only trained domain experts can provide labels, which often come in large diversity and complexity. Recently presented approaches for unsupervised detection of visual anomalies approaches omit the need for labeled data and demonstrate promising results in domains, where anomalous samples significantly deviate from the normal appearance. Despite promising results, the performance of such approaches still lags behind supervised approaches and does not provide a one-fits-all solution. In this work, we present an image-to-image translation-based framework that significantly surpasses the performance of existing unsupervised methods and approaches the performance of supervised methods in a challenging domain of cancerous region detection in histology imagery. ",
    "code_link": "https://github.com/CODAIT/deep-histopath"
  },
  "cvpr2021_cvpm_combiningmagnificationandmeasurementfornon-contactcardiacmonitoring": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Combining Magnification and Measurement for Non-Contact Cardiac Monitoring",
    "authors": [
      "Ewa M. Nowara",
      "Daniel McDuff",
      "Ashok Veeraraghavan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Nowara_Combining_Magnification_and_Measurement_for_Non-Contact_Cardiac_Monitoring_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Nowara_Combining_Magnification_and_Measurement_for_Non-Contact_Cardiac_Monitoring_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep learning approaches currently achieve the state-of-the-art results on camera-based vital signs measurement. One of the main challenges with using neural models for these applications is the lack of sufficiently large and diverse datasets. Limited data increases the chances of overfitting models to the available data which in turn can harm generalization. In this paper, we show that the generalizability of imaging photoplethysmography models can be improved by augmenting the training set with \"magnified\" videos. These augmentations are specifically geared towards revealing useful features for recovering the photoplethysmogram. We show that using augmentations of this form is more effective at improving model robustness than other commonly used data augmentation approaches. We show better within-dataset and especially cross-dataset performance with our proposed data augmentation approach on three publicly available datasets. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_aninfraredthermographymodelenablingremotebodytemperaturescreeningupto10meters": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "An Infrared Thermography Model Enabling Remote Body Temperature Screening Up to 10 Meters",
    "authors": [
      "Jing Wei Chin",
      "Kwan Long Wong",
      "Tsz Tai Chan",
      "Kristian Suhartono",
      "Richard H.Y. So"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Chin_An_Infrared_Thermography_Model_Enabling_Remote_Body_Temperature_Screening_Up_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Chin_An_Infrared_Thermography_Model_Enabling_Remote_Body_Temperature_Screening_Up_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " During the COVID-19 pandemic, temperature screening has emerged as a common practice in the infection control pipeline. In particular, thermal imaging systems have risen in popularity for preliminary screening of individuals with elevated temperatures, especially in high throughput areas. However, remote temperature measurement is intrinsically complex and susceptible to unavoidable influences from the measuring environment. We study the effects of sensor-subject distance on remote temperature readings and present an infrared-based system for rapid temperature screening over long distances (2 m to 10 m). The system applies a state-of-the-art pose estimation algorithm to extract the face box locations, sensor-subject distances, and facial temperatures within a scene. For the use of infrared thermography in humans, we propose a thermal compensation model to correct the temperature of subjects measured at different distances and perform analyses to evaluate the trade-off between missing rate (elevated temperature does not trigger an alarm) and false alarm rate (normal temperature triggers an alarm). The experimental results show our system's promise to identify subjects with elevated temperatures and the potential to improve temperature screening protocols in different environments. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_alstm-basedrealtimesignalqualityassessmentforphotoplethysmogramandremotephotoplethysmogram": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "A LSTM-Based Realtime Signal Quality Assessment for Photoplethysmogram and Remote Photoplethysmogram",
    "authors": [
      "Haoyuan Gao",
      "Xiaopei Wu",
      "Chenyun Shi",
      "Qing Gao",
      "Jidong Geng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Gao_A_LSTM-Based_Realtime_Signal_Quality_Assessment_for_Photoplethysmogram_and_Remote_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Gao_A_LSTM-Based_Realtime_Signal_Quality_Assessment_for_Photoplethysmogram_and_Remote_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Monitoring physiological parameters is very important to access individuals' health status. Recent years, remote photoplethysmogram (rPPG) captured from human face by consumer-level cameras is used to estimate heart rate (HR). However, remote sensing signals are more easily affected by motion artifacts and environmental noise, which make the evaluation results unreliable. In this paper, we propose a long-short term memory network (LSTM) to assess the quality of the PPG(rPPG) signals in real time. This algorithm can also seek out the high quality segments from the ultra-long signals quickly. First, we labeled the PPG data by the combination of three traditional methods. Then, a LSTM network was trained to distinguish between clean signals and noisy signals in the PPG database. Finally, the network from the PPG data was verified in the rPPG data. The results of the experiments show that our method can get the signal quality index in real time, and the high-quality fragments extracted by our method indirectly increase the accuracy of HR evaluation. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_towardsautomatedandmarker-lessparkinsondiseaseassessmentpredictingupdrsscoresusingsit-standvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Towards Automated and Marker-Less Parkinson Disease Assessment: Predicting UPDRS Scores Using Sit-Stand Videos",
    "authors": [
      "Deval Mehta",
      "Umar Asif",
      "Tian Hao",
      "Erhan Bilal",
      "Stefan von Cavallar",
      "Stefan Harrer",
      "Jeffrey Rogers"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Mehta_Towards_Automated_and_Marker-Less_Parkinson_Disease_Assessment_Predicting_UPDRS_Scores_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Mehta_Towards_Automated_and_Marker-Less_Parkinson_Disease_Assessment_Predicting_UPDRS_Scores_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents a novel deep learning enabled, video based analysis framework for assessing the Unified Parkinson's Disease Rating Scale (UPDRS) that can be used in the clinic or at home. We report results from comparing the performance of the framework to that of trained clinicians on a population of 32 Parkinson's disease (PD) patients. In-person clinical assessments by trained neurologists are used as the ground truth for training our framework and for comparing the performance. We find that the standard sit-to-stand activity can be used to evaluate the UPDRS sub-scores of bradykinesia (BRADY) and posture instability and gait disorders (PIGD). For BRADY we find F1-scores of 0.75 using our framework compared to 0.50 for the video based rater clinicians, while for PIGD we find 0.78 for the framework and 0.45 for the video based rater clinicians. We believe our proposed framework has potential to provide clinically acceptable end points of PD in greater granularity without imposing burdens on patients and clinicians, which empowers a variety of use cases such as passive tracking of PD progression in spaces such as nursing homes, in-home self-assessment, and enhanced tele-medicine. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_oxygensaturationestimationbasedonoptimalbandselectionfrommulti-bandvideo": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Oxygen Saturation Estimation Based on Optimal Band Selection From Multi-Band Video",
    "authors": [
      "Ryo Takahashi",
      "Koichi Ashida",
      "Yasuo Kobayashi",
      "Rumi Tokunaga",
      "Shuhei Kodama",
      "Norimichi Tsumura"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Takahashi_Oxygen_Saturation_Estimation_Based_on_Optimal_Band_Selection_From_Multi-Band_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Takahashi_Oxygen_Saturation_Estimation_Based_on_Optimal_Band_Selection_From_Multi-Band_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this study, we propose a method to estimate oxygen saturation by selecting the best bands from video images captured by a multiband camera. Oxygen saturation is one of the most important bioindicators for measuring human health. For example, when a person contracts COVID-19, which is currently prevalent, oxygen uptake does not work properly and oxygen saturation drops without the person being aware of it, which may lead to severe symptoms. Monitoring oxygen saturation is very important so that the person receives treatment before such a situation occurs. The commonly used contact sensor is uncomfortable because of its pressure and it is difficult to wear on a daily basis, so non-contact estimation of oxygen saturation is desirable. To estimate oxygen saturation using a contact sensor, the difference in the absorption coefficients of oxidized hemoglobin and deoxidized hemoglobin is used. Using the same principle, it is possible to estimate oxygen saturation without contact using the signals from two channels obtained by an RGB camera. Currently, many smartphones are equipped with infrared cameras for face recognition, and increasingly more models are equipped with multi-camera systems consisting of RGB and infrared cameras. In such cases, it is difficult to take advantage of the multiple bands because the optimal combination of bands for oxygen saturation estimation varies depending on the imaging environment and the subject. In this study, to select the optimal combination of bands from multi-band video images, we used a Monte Carlo simulation of light scattering on the skin to simulate pulse waves during oxygen saturation changes while measuring the signals with a multi-band camera. We further propose a method to select the most accurate combination for estimating the oxygen saturation based on the features obtained from the pulse wave. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_assessmentofdeeplearningbasedbloodpressurepredictionfromppgandrppgsignals": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Assessment of Deep Learning Based Blood Pressure Prediction From PPG and rPPG Signals",
    "authors": [
      "Fabian Schrumpf",
      "Patrick Frenzel",
      "Christoph Aust",
      "Georg Osterhoff",
      "Mirco Fuchs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Schrumpf_Assessment_of_Deep_Learning_Based_Blood_Pressure_Prediction_From_PPG_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Schrumpf_Assessment_of_Deep_Learning_Based_Blood_Pressure_Prediction_From_PPG_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Exploiting photoplethysmography signals (PPG) for non-invasive blood pressure BP) measurement is interesting for various reasons. First, PPG can easily be measured using fingerclip sensors. Second, camera-based approaches allow to derive remote PPG (rPPG) signals similar to PPG and therefore provide the opportunity for non-invasive measurements of BP. Various methods relying on machine learning techniques have recently been published. Performances are often reported as the mean average error (MAE) on the data which is problematic. This work aims to analyze the PPG- and rPPG-based BP prediction error with respect to the underlying data distribution. First, we train established neural network (NN) architectures and derive an appropriate parameterization of input segments drawn from continuous PPG signals. Second, we apply this parameterization to a larger PPG dataset and train NNs to predict BP. The resulting prediction errors increase towards less frequent BP values. Third, we use transfer learning to train the NNs for rPPG based BP prediction. The resulting performances are similar to the PPG-only case. Finally, we apply a personalization technique and retrain our NNs with subject-specific data. This slightly reduces the prediction errors. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_nosebreathingormouthbreathing?athermography-basednewmeasurementforsleepmonitoring": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Nose Breathing or Mouth Breathing? A Thermography-Based New Measurement for Sleep Monitoring",
    "authors": [
      "Zhengjie Huang",
      "Wenjin Wang",
      "Gerard de Haan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Huang_Nose_Breathing_or_Mouth_Breathing_A_Thermography-Based_New_Measurement_for_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Huang_Nose_Breathing_or_Mouth_Breathing_A_Thermography-Based_New_Measurement_for_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Nose breathing is preferred during sleep, although health issues may cause a subject to breathe through the mouth, and long-term mouth breathing may raise other health issues like sleep apnea. This paper proposes a first-ever classification of nose breathing and mouth breathing using the thermography of the subject. The measurement uses the relative temperature variations of different facial regions to classify mouth or nose breathing. This measurement is particularly health-/well-being relevant as it can be used as an early sign for sleep disorders or an indicator of sleep quality. An end-to-end processing flowchart has been provided for proof-of-concept validation on real-life recordings of thermal videos. Eight volunteers participated in our experiments and our proposed method achieved an overall classification accuracy of 91% in ideal lab conditions. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_improvingaccuracyofrespiratoryrateestimationbyrestoringhighresolutionfeatureswithtransformersandrecursiveconvolutionalmodels": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Improving Accuracy of Respiratory Rate Estimation by Restoring High Resolution Features With Transformers and Recursive Convolutional Models",
    "authors": [
      "Alicja Kwasniewska",
      "Maciej Szankin",
      "Jacek Ruminski",
      "Anthony Sarah",
      "David Gamba"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Kwasniewska_Improving_Accuracy_of_Respiratory_Rate_Estimation_by_Restoring_High_Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Kwasniewska_Improving_Accuracy_of_Respiratory_Rate_Estimation_by_Restoring_High_Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Non-contact evaluation of vital signs has been becoming increasingly important, especially in light of the COVID-19 pandemic, which is causing the whole world to examine people's interactions in public places at a scale never seen before. However, evaluating one's vital signs can be a relatively complex procedure, which requires both time and physical contact between examiner and examinee. These requirements limit the number of people who can be efficiently checked, either due to the medical station throughput, patients' remote locations or the need for social distancing. This study is a first step to increasing the accuracy of computer vision-based respiratory rate estimation by transferring texture information from images acquired in different domains. Experiments conducted with two deep neural network topologies, a recursive convolutional model and transformers, proved their robustness in the analyzed scenario by reducing estimation error by 50% compared to low resolution sequences. All resources used in this research, including links to the dataset and code, have been made publicly available. ",
    "code_link": ""
  },
  "cvpr2021_cvpm_markerlesscamera-basedverticaljumpheightmeasurementusingopenpose": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVPM",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Physiological Measurement",
    "title": "Markerless Camera-Based Vertical Jump Height Measurement Using OpenPose",
    "authors": [
      "Fritz Webering",
      "Holger Blume",
      "Issam Allaham"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/html/Webering_Markerless_Camera-Based_Vertical_Jump_Height_Measurement_Using_OpenPose_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVPM/papers/Webering_Markerless_Camera-Based_Vertical_Jump_Height_Measurement_Using_OpenPose_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vertical jump height is an important tool to measure athletes' lower body power in sports science and medicine. This work improves upon a previously published self-calibrating algorithm, which determines jump height using a single smartphone camera. The algorithm uses the parabolic fall trajectory obtained by tracking a single feature in a high-speed video. Instead of tracking an ArUco marker, which must be attached to the jumping subject, this work uses the OpenPose neural network for human pose estimation in order to calculate an approximation of the body center of mass. Jump heights obtained this way are compared to the reference heights from a motion capture system and to the results of the original work. The result is a trade-off between increased ease-of-use and slightly diminished accuracy of the jump height measurement. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_dressinginorderrecurrentpersonimagegenerationforposetransfer,virtualtry-onandoutfitediting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-On and Outfit Editing",
    "authors": [
      "Aiyu Cui",
      "Daniel McKee",
      "Svetlana Lazebnik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Cui_Dressing_in_Order_Recurrent_Person_Image_Generation_for_Pose_Transfer_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Cui_Dressing_in_Order_Recurrent_Person_Image_Generation_for_Pose_Transfer_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We propose a flexible person generation framework called Dressing in Order (DiOr), which supports 2D pose transfer, virtual try-on, and several fashion editing tasks. The key to DiOr is a novel recurrent generation pipeline to sequentially put garments on a person, so that trying on the same garments in different orders will result in different looks. Our system can produce dressing effects not achievable by existing work, including different interactions of garments (e.g., wearing a top tucked into the bottom or over it), as well as layering of multiple garments of the same type (e.g., jacket over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each garment, enabling these elements to be edited separately. Extensive evaluations show that DiOr outperforms other recent methods like ADGAN in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_modelingfashioncompatibilitywithexplanationbyusingbidirectionallstm": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Modeling Fashion Compatibility With Explanation by Using Bidirectional LSTM",
    "authors": [
      "Pang Kaicheng",
      "Zou Xingxing",
      "Wai Keung Wong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Kaicheng_Modeling_Fashion_Compatibility_With_Explanation_by_Using_Bidirectional_LSTM_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Kaicheng_Modeling_Fashion_Compatibility_With_Explanation_by_Using_Bidirectional_LSTM_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The goal of this paper is to model the fashion compatibility of an outfit and provide the explanations. We first extract features of all attributes of all items via convolutional neural networks, and then train the bidirectional Long Short-term Memory (Bi-LSTM) model to learn the compatibility of an outfit by treating these attribute features as a sequence. Gradient penalty regularization is exploited for training inter-factor compatibility net which is used to compute the loss for judgment and provide its explanation which is generated from the recognized reasons related to the judgment. To train and evaluate the proposed approach, we expanded the EVALUATION3 dataset in terms of the number of items and attributes. Experiment results show that our approach can successfully evaluate compatibility with reason. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_localizedtripletlossforfine-grainedfashionimageretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Localized Triplet Loss for Fine-Grained Fashion Image Retrieval",
    "authors": [
      "Antonio D'Innocente",
      "Nikhil Garg",
      "Yuan Zhang",
      "Loris Bazzani",
      "Michael Donoser"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/DInnocente_Localized_Triplet_Loss_for_Fine-Grained_Fashion_Image_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/DInnocente_Localized_Triplet_Loss_for_Fine-Grained_Fashion_Image_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Fashion retrieval methods aim at learning a clothing specific embedding space where images are ranked based on their global visual similarity with a given query. However, global embeddings struggle to capture localized fine grained similarities between images, because of aggregation operations. Our work deals with this problem by learning localized representations for fashion retrieval based on local interest points of prominent visual features specified by a user. We introduce a localized triplet loss function that compares samples based on corresponding patterns. We incorporate random local perturbation on the interest point as a key regularization technique to enforce local invariance of visual representations. Due to the absence of existing fashion datasets to train on localized representations, we introduce FashionLocalTriplets, a new highquality dataset annotated by fashion specialists that contains triplets of women's dresses and interest points. The proposed model outperforms state-of-the-art global representations on FashionLocalTriplets. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_differentiablerendering-basedpose-conditionedhumanimagegeneration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Differentiable Rendering-Based Pose-Conditioned Human Image Generation",
    "authors": [
      "Yusuke Horiuchi",
      "Edgar Simo-Serra",
      "Satoshi Iizuka",
      "Hiroshi Ishikawa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Horiuchi_Differentiable_Rendering-Based_Pose-Conditioned_Human_Image_Generation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Horiuchi_Differentiable_Rendering-Based_Pose-Conditioned_Human_Image_Generation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Conditional human image generation, or generation of human images with specified pose based on one or more reference images, is an inherently ill-defined problem, as there can be multiple plausible appearance for parts that are occluded in the reference. Using multiple images can mitigate this problem while boosting the performance. In this work, we introduce a differentiable vertex and edge renderer for incorporating the pose information to realize human image generation conditioned on multiple reference images. The differentiable renderer has parameters that can be jointly optimized with other parts of the system to obtain better results by learning more meaningful shape representation of human pose. We evaluate our method on the Market-1501 and DeepFashion datasets and comparison with existing approaches validates the effectiveness of our approach. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_effectivelyleveragingattributesforvisualsimilarity": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Effectively Leveraging Attributes for Visual Similarity",
    "authors": [
      "Samarth Mishra",
      "Zhongping Zhang",
      "Yuan Shen",
      "Ranjitha Kumar",
      "Venkatesh Saligrama",
      "Bryan Plummer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Mishra_Effectively_Leveraging_Attributes_for_Visual_Similarity_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Mishra_Effectively_Leveraging_Attributes_for_Visual_Similarity_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Measuring similarity between two images often requiresperforming complex reasoning along different axes (e.g.,color, texture, or shape). Insights into what might be important for measuring similarity can be provided by annotated attributes. Prior work tends to view these annotations as complete, resulting in them using a simplistic approach of predicting attributes on single images, which are, in turn, used to measure similarity. However, it is impractical for a dataset to fully annotate every attribute that may be important. Thus, only representing images based on these incomplete annotations may miss out on key information. To address this issue, we propose the Pairwise Attribute-informed similarity Network (PAN), which breaks similarity learning into capturing similarity conditions and relevance scores from a joint representation of two images. This enables our model to identify that two images contain the same attribute, but can have it deemed irrelevant (e.g., due to fine-grained differences between them) and ignored for measuring similarity between the two images. Notably, while prior methods of using attribute annotations are often unable to outperform prior art, PAN obtains a 4-9% improvement on compatibility prediction between clothing items on Polyvore Outfits and a 5% gain on few shot classification of images using Caltech-UCSD Birds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval ",
    "code_link": ""
  },
  "cvpr2021_cvfad_indofashionapparelclassificationforindianethnicclothes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "IndoFashion: Apparel Classification for Indian Ethnic Clothes",
    "authors": [
      "Pranjal Singh Rajput",
      "Shivangi Aneja"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Rajput_IndoFashion_Apparel_Classification_for_Indian_Ethnic_Clothes_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Rajput_IndoFashion_Apparel_Classification_for_Indian_Ethnic_Clothes_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Cloth categorization is an important research problem that is used by e-commerce websites for displaying correct products to the end-users. Indian clothes have a large number of clothing categories both for men and women. The traditional Indian clothes like \"Saree\" and \"Dhoti\" are worn very differently from western clothes like t-shirts and jeans. Moreover, the style and patterns of ethnic clothes have a very different distribution from western outfits. Thus the models trained on standard cloth datasets fail on ethnic outfits. We introduce the first large-scale ethnic dataset of over 106K images with 15 different categories for fine-grained classification of Indian ethnic clothes. We gathered a diverse dataset from a large number of Indian e-commerce websites. We then evaluate several baselines for the cloth classification task on our dataset. We obtain 88.43% classification accuracy. We hope that our dataset would foster research in the development of several algorithms such as cloth classification, landmark detection, especially for ethnic clothes. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_astudyontherelativeimportanceofconvolutionalneuralnetworksinvisually-awarerecommendersystems": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "A Study on the Relative Importance of Convolutional Neural Networks in Visually-Aware Recommender Systems",
    "authors": [
      "Yashar Deldjoo",
      "Tommaso Di Noia",
      "Daniele Malitesta",
      "Felice Antonio Merra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Deldjoo_A_Study_on_the_Relative_Importance_of_Convolutional_Neural_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Deldjoo_A_Study_on_the_Relative_Importance_of_Convolutional_Neural_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Visually-aware recommender systems (VRSs) enhance the semantics of user-item interactions with visual features extracted from item images when they are available. Traditionally, VRSs leverage the representational power of pretrained convolutional neural networks (CNNs) to perform the item recommendation task. The adoption of CNNs is mainly attributed to their outstanding performance in representing visual data for supervised learning tasks, such as image classification. Their main drawback is that the learned representation of these networks is not entirely in line with the RS tasks - learning users' preferences. This work aims to provide a better understanding of the representation power of pretrained CNNs commonly adopted by the community when integrated with state-of-the-art VRSs algorithms. In particular, we evaluate the recommendation performance of a suite of VRSs using several pretrained CNNs as the image feature extractors on two datasets from a real-world e-commerce platform. Additionally, we propose a novel qualitative and quantitative evaluation paradigm to assess the visual diversity of recommended items compared to the interacted user's items. ",
    "code_link": "https://github.com/sisinflab/CNNs-in-VRSs"
  },
  "cvpr2021_cvfad_surprisingimagecompositions": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Surprising Image Compositions",
    "authors": [
      "Othman Sbai",
      "Camille Couprie",
      "Mathieu Aubry"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Sbai_Surprising_Image_Compositions_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Sbai_Surprising_Image_Compositions_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Visual metaphors are a powerful and effective way of communication in advertising, news and art. By taking objects out of their natural context, artists create new and surprising composite images by leveraging visual, linguistic or phonetic analogies. We build on recent image retrieval, completion and composition methods to design a new collage generation tool with the aim of assisting artists in creating interesting composite images. Given a selected object in an image, our model searches for visually similar but semantically different objects and performs the image blending automatically, leading to surprising image combinations. Using automatic metrics and a human study, we test our approach against improved baselines and show the potential of this novel artistic application. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_wherearemyclothes?amulti-levelapproachforevaluatingdeepinstancesegmentationarchitecturesonfashionimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Where Are My Clothes? A Multi-Level Approach for Evaluating Deep Instance Segmentation Architectures on Fashion Images",
    "authors": [
      "Warren Jouanneau",
      "Aurelie Bugeau",
      "Marc Palyart",
      "Nicolas Papadakis",
      "Laurent Vezard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Jouanneau_Where_Are_My_Clothes_A_Multi-Level_Approach_for_Evaluating_Deep_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Jouanneau_Where_Are_My_Clothes_A_Multi-Level_Approach_for_Evaluating_Deep_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper we present an extensive evaluation of instance segmentation in the context of images containing clothes. We propose a multi level evaluation that completes the classical overlapping criteria given by IoU. In particular, we quantify both the contour and color content accuracy of the the predicted segmentation masks. We demonstrate that the proposed evaluation framework is relevant to obtain meaningful insights on models performance through experiments conducted on five state of the art instance segmentation methods. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_scalableandexplainableoutfitgeneration": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Scalable and Explainable Outfit Generation",
    "authors": [
      "Alexander Lorbert",
      "David Neiman",
      "Arik Poznanski",
      "Eduard Oks",
      "Larry Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Lorbert_Scalable_and_Explainable_Outfit_Generation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Lorbert_Scalable_and_Explainable_Outfit_Generation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present an end-to-end system for learning outfit recommendations. The core problem we address is how a customer can receive clothing/accessory recommendations based on a current outfit and what type of item the customer wishes to add to the outfit. Using a repository of coherent and stylish outfits, we leverage self-attention to learn a mapping from the current outfit and the customer-requested category to a visual descriptor output. This output is then fed into nearest-neighbor-based visual search, which, during training, is learned via triplet loss and mini-batch retrievals. At inference time, we use a beam search with a desired outfit composition to generate outfits at scale. Moreover, the attention networks provide a diagnostic look into the recommendation process, serving as a fashion-based sanity check. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_lineartcolorizationwithconcatenatedspatialattention": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Line Art Colorization With Concatenated Spatial Attention",
    "authors": [
      "Mingcheng Yuan",
      "Edgar Simo-Serra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Yuan_Line_Art_Colorization_With_Concatenated_Spatial_Attention_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Yuan_Line_Art_Colorization_With_Concatenated_Spatial_Attention_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Line art plays a fundamental role in illustration and design, and allows for iteratively polishing designs. However, as they lack color, they can have issues in conveying final designs. In this work, we propose an interactive colorization approach based on a conditional generative adversarial network that takes both the line art and color hints as inputs to produce a high-quality colorized image. Our approach is based on a U-net architecture with a multi-discriminator framework. We propose a Concatenation and Spatial Attention module that is able to generate more consistent and higher quality of line art colorization from user given hints. We evaluate on a large-scale illustration dataset and comparison with existing approaches corroborate the effectiveness of our approach. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_explainablenoisylabelflippingformulti-labelfashionimageclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Explainable Noisy Label Flipping for Multi-Label Fashion Image Classification",
    "authors": [
      "Beatriz Quintino Ferreira",
      "Joao P. Costeira",
      "Joao P. Gomes"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Ferreira_Explainable_Noisy_Label_Flipping_for_Multi-Label_Fashion_Image_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Ferreira_Explainable_Noisy_Label_Flipping_for_Multi-Label_Fashion_Image_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In online shopping applications, the daily insertion of new products requires an overwhelming annotation effort. Usually done by humans, it comes at a huge cost and yet generates high rates of noisy/missing labels that seriously hinder the effectiveness of CNNs in multi-label classification. We propose SELF-ML, a classification framework that exploits the relation between visual attributes and appearance together with the \"\"low-rank\"\" nature of the feature space. It learns a sparse reconstruction of image features as a convex combination of very few images - a basis - that are correctly annotated. Building on this representation, SELF-ML has a module that relabels noisy annotations from the derived combination of the clean data. Due to such structured reconstruction, SELF-ML gives an explanation of its label-flipping decisions. Experiments on a real-world shopping dataset show that SELF-ML significantly increases the number of correct labels even with few clean annotations. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_leveragingstyleandcontentfeaturesfortextconditionedimageretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Leveraging Style and Content Features for Text Conditioned Image Retrieval",
    "authors": [
      "Pranit Chawla",
      "Surgan Jandial",
      "Pinkesh Badjatiya",
      "Ayush Chopra",
      "Mausoom Sarkar",
      "Balaji Krishnamurthy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Chawla_Leveraging_Style_and_Content_Features_for_Text_Conditioned_Image_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Chawla_Leveraging_Style_and_Content_Features_for_Text_Conditioned_Image_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Image Search is a fundamental task playing a significant role in the success of wide variety of frameworks and applications. However, with the increasing sizes of product catalogues and the number of attributes per product, it has become difficult for users to express their needs effectively. Therefore, we focus on the problem of Image Retrieval with Text Feedback, which involves retrieving modified images according to the natural language feedback provided by users. In this work, we hypothesise that since an image can be delineated by its content and style features, modifications to the image can also take place in the two sub spaces respectively. Hence, we decompose an input image into its corresponding style and content features, apply modification of the text feedback individually in both the style and content spaces and finally fuse them for retrieval. Our experiments show that our approach outperforms a recent state of the art method in this task, TIRG, that seeks to use a single vector in contrast to leveraging the modification via text over style and content spaces separately. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_afrifashion1600acontemporaryafricanfashiondatasetforcomputervision": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "AFRIFASHION1600: A Contemporary African Fashion Dataset for Computer Vision",
    "authors": [
      "Wuraola Fisayo Oyewusi",
      "Olubayo Adekanmbi",
      "Sharon Ibejih",
      "Opeyemi Osakuade",
      "Ifeoma Okoh",
      "Mary Salami"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Oyewusi_AFRIFASHION1600_A_Contemporary_African_Fashion_Dataset_for_Computer_Vision_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Oyewusi_AFRIFASHION1600_A_Contemporary_African_Fashion_Dataset_for_Computer_Vision_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This work presents AFRIFASHION1600, an openly accessible contemporary African fashion image dataset containing 1600 samples labelled into 8 classes representing some African fashion styles. Each sample is coloured and has an image size of 128 x 128. This is a niche dataset that aims to improve visibility, inclusion, and familiarity of African fashion in computer vision tasks.AFRIFASHION1600 dataset is available here(https://git.io/JOlxE) ",
    "code_link": ""
  },
  "cvpr2021_cvfad_fine-grainedvisualattributeextractionfromfashionwear": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Fine-Grained Visual Attribute Extraction From Fashion Wear",
    "authors": [
      "Viral Parekh",
      "Karimulla Shaik",
      "Soma Biswas",
      "Muthusamy Chelliah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Parekh_Fine-Grained_Visual_Attribute_Extraction_From_Fashion_Wear_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Parekh_Fine-Grained_Visual_Attribute_Extraction_From_Fashion_Wear_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Automatically extracting visual attributes for e-commerce data has widespread applications in cataloging, catalogue qualification and enrichment, visual search, etc. Here, we address the task of visual attribute extraction for a highly challenging real-world fashion data from Flipkart catalogue (an Indian e-commerce platform), which is collected from seller uploaded product images. This data not only contains widely varying categories (e.g., shirt, sari, shoes), but also has both coarse-grained (e.g., occasion, top type, sari type) and fine-grained (e.g., neck type, print type) attributes. Training examples available for different attributes are highly imbalanced, making this task even more challenging. To this end, we propose an end-to-end framework which integrates multi-task learning with transformer as an attention module, in addition to handling the data imbalance. The proposed architecture supports multiple attributes across various product categories in a scalable manner. Extensive experiments on the in-house dataset shows effectiveness of the proposed framework in improving performance of the fine-grained attributes by 13% on the baseline across the attributes. ",
    "code_link": ""
  },
  "cvpr2021_cvfad_clip-artcontrastivepre-trainingforfine-grainedartclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "CLIP-Art: Contrastive Pre-Training for Fine-Grained Art Classification",
    "authors": [
      "Marcos V. Conde",
      "Kerem Turgutlu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Conde_CLIP-Art_Contrastive_Pre-Training_for_Fine-Grained_Art_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Conde_CLIP-Art_Contrastive_Pre-Training_for_Fine-Grained_Art_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Existing computer vision research in artwork struggles with artwork's fine-grained attributes recognition and lack of curated annotated datasets due to their costly creation. In this work, we use CLIP (Contrastive Language-Image Pre-Training) for training a neural network on a variety of art images and text pairs, being able to learn directly from raw descriptions about images, or if available, curated labels. Model's zero-shot capability allows predicting the most relevant natural language description for a given image, without directly optimizing for the task. Our approach aims to solve 2 challenges: instance retrieval and fine-grained artwork attribute recognition. We use the iMet Dataset, which we consider the largest annotated artwork dataset. Our code and models will be available at https://github.com/KeremTurgutlu/clip_art ",
    "code_link": "https://github.com/KeremTurgutlu/clip_art"
  },
  "cvpr2021_cvfad_deepgraphicsencoderforreal-timevideomakeupsynthesisfromexample": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Deep Graphics Encoder for Real-Time Video Makeup Synthesis From Example",
    "authors": [
      "Robin Kips",
      "Ruowei Jiang",
      "Sileye Ba",
      "Edmund Phung",
      "Parham Aarabi",
      "Pietro Gori",
      "Matthieu Perrot",
      "Isabelle Bloch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Kips_Deep_Graphics_Encoder_for_Real-Time_Video_Makeup_Synthesis_From_Example_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Kips_Deep_Graphics_Encoder_for_Real-Time_Video_Makeup_Synthesis_From_Example_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " While makeup virtual-try-on is now widespread, parametrizing a computer graphics rendering engine for synthesizing images of a given cosmetics product remains a challenging task. In this paper, we introduce an inverse computer graphics method for automatic makeup synthesis from a reference image, by learning a model that maps an example portrait image with makeup to the space of rendering parameters. This method can be used by artists to automatically create realistic virtual cosmetics image samples, or by consumers, to virtually try-on a makeup extracted from their favorite reference image. ",
    "code_link": "https://github.com/Tencent/ncnn"
  },
  "cvpr2021_cvfad_colormegoodbrandinginthecoloringstyleofmovieposters": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVFAD",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision for Fashion, Art, and Design",
    "title": "Color Me Good: Branding in the Coloring Style of Movie Posters",
    "authors": [
      "Rishabh Agrawal",
      "Sarath Sivaprasad",
      "Niranjan Pedanekar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Agrawal_Color_Me_Good_Branding_in_the_Coloring_Style_of_Movie_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Agrawal_Color_Me_Good_Branding_in_the_Coloring_Style_of_Movie_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Brand logos are often rendered in a different style based on a context such as an event promotion. For example, Warner Bros. uses a different variety of their brand logo for different movies for promotion and aesthetic appeal. In this paper, we propose an automated method to render brand logos in the coloring style of branding material such as movie posters. For this, we adopt a photo-realistic neural style transfer method using movie posters as the style source. We propose a color-based image segmentation and matching method to assign style segments to logo segments. Using these, we render the well-known Warner Bros. logo in the coloring style of 141 movie posters. We also present survey results where 287 participants rate the machine-stylized logos for their representativeness and visual appeal. ",
    "code_link": ""
  },
  "cvpr2021_aicity_box-leveltubetrackingandrefinementforvehiclesanomalydetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Box-Level Tube Tracking and Refinement for Vehicles Anomaly Detection",
    "authors": [
      "Jie Wu",
      "Xionghui Wang",
      "Xuefeng Xiao",
      "Yitong Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Wu_Box-Level_Tube_Tracking_and_Refinement_for_Vehicles_Anomaly_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Wu_Box-Level_Tube_Tracking_and_Refinement_for_Vehicles_Anomaly_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Traffic Anomaly detection is an essential computer vision task and plays a critical role in video structure analysis and urban traffic analysis. In this paper, we propose a box-level tracking and refinement algorithm to identify anomaly detection in road scenes. We first link the detection results to construct candidate spatio-temporal tubes via greedy search. Then the box-level refinement scheme is introduced to employ auxiliary detection cues to promote the abnormal predictions, which consists of spatial fusion, still-thing filter, temporal fusion, and feedforward optimization. Still-thing filter and feedforward optimization employ complementary detection concepts to promote the abnormal predictions, which helps determine an accurate abnormal period. The experimental results show that our approach is superior in the Traffic Anomaly Detection Track test set of the NVIDIA AI CITY 2021 CHALLENGE, which ranked second in this competition, with a 93.18% F1-score and 3.1623 root mean square error. It reveals that the proposed approach contributes to fine-grained anomaly detection in actual traffic accident scenarios and promoting the development of intelligent transportation. ",
    "code_link": ""
  },
  "cvpr2021_aicity_dual-modalityvehicleanomalydetectionviabilateraltrajectorytracing": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Dual-Modality Vehicle Anomaly Detection via Bilateral Trajectory Tracing",
    "authors": [
      "Jingyuan Chen",
      "Guanchen Ding",
      "Yuchen Yang",
      "Wenwei Han",
      "Kangmin Xu",
      "Tianyi Gao",
      "Zhe Zhang",
      "Wanping Ouyang",
      "Hao Cai",
      "Zhenzhong Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Chen_Dual-Modality_Vehicle_Anomaly_Detection_via_Bilateral_Trajectory_Tracing_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Chen_Dual-Modality_Vehicle_Anomaly_Detection_via_Bilateral_Trajectory_Tracing_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Traffic anomaly detection has played a crucial role in Intelligent Transportation System (ITS). The main challenges of this task lie in the highly diversified anomaly scenes and variational lighting conditions. Although much work has managed to identify the anomaly in homogenous weather and scene, few resolved to cope with complex ones. In this paper, we proposed a dual-modality modularized methodology for the robust detection of abnormal vehicles. We introduced an integrated anomaly detection framework comprising the following modules: background modeling, vehicle tracking with detection, mask construction, Region of Interest (ROI) backtracking, and dual-modality tracing. Concretely, we employed background modeling to filter the motion information and left the static information for later vehicle detection. For the vehicle detection and tracking module, we adopted YOLOv5 and multi-scale tracking to localize the anomalies. Besides, we utilized the frame difference and tracking results to identify the road and obtain the mask. In addition, we introduced multiple similarity estimation metrics to refine the anomaly period via backtracking. Finally, we proposed a dual-modality bilateral tracing module to refine the time further. The experiments conducted on the Track 4 testset of the NVIDIA 2021 AI City Challenge yielded a result of 0.9302 F1-Score and 3.4039 root mean square error (RMSE), indicating the effectiveness of our framework. ",
    "code_link": ""
  },
  "cvpr2021_aicity_tracklet-refinedmulti-cameratrackingbasedonbalancedcross-domainre-identificationforvehicles": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Tracklet-Refined Multi-Camera Tracking Based on Balanced Cross-Domain Re-Identification for Vehicles",
    "authors": [
      "Kai-Siang Yang",
      "Yu-Kai Chen",
      "Tsai-Shien Chen",
      "Chih-Ting Liu",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Yang_Tracklet-Refined_Multi-Camera_Tracking_Based_on_Balanced_Cross-Domain_Re-Identification_for_Vehicles_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Yang_Tracklet-Refined_Multi-Camera_Tracking_Based_on_Balanced_Cross-Domain_Re-Identification_for_Vehicles_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Mutli-camera vehicle tracking and re-identification (re-ID) have gradually gained attention due to their applications in the intelligent transportation system. However, these problems are fundamentally challenging. Specifically, for vehicle tracking, we observe that the results generated from single camera tracking algorithm usually recognize tracklets with same identity as different vehicles when the tracklets are occluded. Hence, we propose a Tracklet Reconnection technique to refine tracking results with predefined zone areas and GPS information. The proposed method can efficiently filter invalid tracklet pairs and reconnect the split tracklets into complete ones, which is important for the afterwards multi-target multi-camera tracking. As for re-ID, we also find that when a large-scale auxiliary dataset is used to assist the learning of main dataset for better model capability and generalization, there is a performance drop caused by data imbalance when the full auxiliary dataset is applied. To tackle this problem, we introduce Balanced Cross-Domain Learning to avoid the overemphasis on larger auxiliary dataset by a newly introduced training data sampler and loss function. The extensive experiments validate the empirical effectiveness of our proposed components. ",
    "code_link": ""
  },
  "cvpr2021_aicity_amulti-cameravehicletrackingsystembasedoncity-scalevehiclere-idandspatial-temporalinformation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "A Multi-Camera Vehicle Tracking System Based on City-Scale Vehicle Re-ID and Spatial-Temporal Information",
    "authors": [
      "Minghu Wu",
      "Yeqiang Qian",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Wu_A_Multi-Camera_Vehicle_Tracking_System_Based_on_City-Scale_Vehicle_Re-ID_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Wu_A_Multi-Camera_Vehicle_Tracking_System_Based_on_City-Scale_Vehicle_Re-ID_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " With the demands of the intelligent city and city-scale traffic management, city-scale multi-camera vehicle tracking (MCVT) has become a vital problem. The MCVT is challenging due to frequent occlusion, similar vehicle models, significant feature variation by different lighting conditions, and viewing perspective in different cameras. This paper proposes an MCVT system composed of single-camera tracking (SCT), vehicle re-identification (Re-ID), and multi-camera tracks matching (MCTM). In the SCT phase, we designed a tracker update strategy and used the Re-ID model in advance. We also adopted a template matching method to re-associate the discontinuous tracklets. As for vehicle Re-ID, we implemented a spatial attention mechanism based on the background model. Then we fully leveraged the labels of synthetic data to train attributes Re-ID models as the attributes features extractor. Finally, we proposed an MCTM method to leverage tracklets representation and spatial-temporal information efficiently. Our system is evaluated both on the City-Scale Multi-Camera Vehicle Re-Identification task (Track 2) and City-Scale Multi-Camera Vehicle Tracking task (Track 3) at the AI City Challenge. Our vehicle Re-ID method has achieved 3rd place of Track 2, with an mAP score of 66.50%, and achieved state-of-the-art results on the VeRi776 dataset. Our MCVT system has achieved 3rd place, yielding 76.51% IDF1 of Track 3. Experimental results demonstrate that our system has achieved competitive performance for city-scale traffic management. ",
    "code_link": ""
  },
  "cvpr2021_aicity_progressivedataminingandadaptiveweightedmulti-modelensembleforvehiclere-identification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Progressive Data Mining and Adaptive Weighted Multi-Model Ensemble for Vehicle Re-Identification",
    "authors": [
      "Yongli Sun",
      "Wenpeng Li",
      "Hua Wei",
      "Longtao Zhang",
      "Jiahao Tian",
      "Guangze Sun",
      "Gang Wang",
      "Junliang Cao",
      "Zhifeng Zhao",
      "Junfeng Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Sun_Progressive_Data_Mining_and_Adaptive_Weighted_Multi-Model_Ensemble_for_Vehicle_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Sun_Progressive_Data_Mining_and_Adaptive_Weighted_Multi-Model_Ensemble_for_Vehicle_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we introduce our solution to the vehicle re-identification (vehicle ReID) track2 in AI City Challenge 2021. As the key point of intelligent Traffic System, vehicle ReID has been a challenging task due to the higher intra-class and inter-class errors which are owing to variable vehicle orientation, camera and lighting. To reduce this error, at first, we innovatively propose a progressive data mining method to obtain more valid data from testing set. Then, we use the image to the mean of each tracklet method in the matching stage which can ensure the precision of image matching by reducing the error with the information of tracklets. Besides, we propose an adaptive weighted ensemble method which effectively improve the model capability. Finally, our method achieves 0.6533 in the mAP score which yields 4th place in the competition. ",
    "code_link": ""
  },
  "cvpr2021_aicity_allyoucanembednaturallanguagebasedvehicleretrievalwithspatio-temporaltransformers": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "All You Can Embed: Natural Language Based Vehicle Retrieval With Spatio-Temporal Transformers",
    "authors": [
      "Carmelo Scribano",
      "Davide Sapienza",
      "Giorgia Franchini",
      "Micaela Verucchi",
      "Marko Bertogna"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Scribano_All_You_Can_Embed_Natural_Language_Based_Vehicle_Retrieval_With_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Scribano_All_You_Can_Embed_Natural_Language_Based_Vehicle_Retrieval_With_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Combining Natural Language with Vision represents a unique and interesting challenge in the domain of Artificial Intelligence. The AI City Challenge Track 5 for Natural Language-Based Vehicle Retrieval focuses on the problem of combining visual and textual information, applied to a smart-city use case. In this paper, we present All You Can Embed (AYCE), a modular solution to correlate single-vehicle tracking sequences with natural language. The main building blocks of the proposed architecture are (i) BERT to provide an embedding of the textual descriptions, (ii) a convolutional backbone along with a Transformer model to embed the visual information. For the training of the retrieval model, a variation of the Triplet Margin Loss is proposed to learn a distance measure between the visual and language embeddings. The code is publicly available at https://github.com/cscribano/AYCE_2021. ",
    "code_link": "https://github.com/cscribano/AYCE_2021"
  },
  "cvpr2021_aicity_tiny-pirateatinymodelwithparallelizedintelligenceforreal-timeanalysisasatrafficcounter": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Tiny-PIRATE: A Tiny Model With Parallelized Intelligence for Real-Time Analysis as a Traffic countEr",
    "authors": [
      "Synh Viet-Uyen Ha",
      "Nhat Minh Chung",
      "Tien-Cuong Nguyen",
      "Hung Ngoc Phan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Ha_Tiny-PIRATE_A_Tiny_Model_With_Parallelized_Intelligence_for_Real-Time_Analysis_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Ha_Tiny-PIRATE_A_Tiny_Model_With_Parallelized_Intelligence_for_Real-Time_Analysis_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Due to the rapid growth in the number of vehicles over the last decade, there has been a dramatic increase in demand for highway capacity analysis. Vehicle counting, in particular, has become a key element of vision-based intelligent traffic systems deployed across metropolitan areas. Most methods solved the vehicle counting problem under the assumption of state-of-the-art computing systems. However, large-scale deployment of such systems for multi-camera processing is very inefficient. With the recent advancement of cost-efficient Internet-of-Things (IoT) devices alongside machine learning methods developed specifically for such devices, solving the vehicle counting problem for real-time traffic analysis on IoT edge devices, and thereby facilitating its large-scale deployment have become highly favorable. In this paper, we propose a framework of vehicle counting designed specifically for IoT edge computers which follows the detection-tracking-counting (DTC) model. The proposed solution aims at addressing the multimodality of contextual dynamics in traffic scenes with a small detector model, a robust tracker and a counting process that accurately estimate both a vehicle's motion of interest and its exit time from observation areas. Experimental results on AI City 2021 Track-1 Dataset showed that ours outperformed related methods with promising results regarding both accuracy and execution speed. ",
    "code_link": ""
  },
  "cvpr2021_aicity_astrongbaselineforvehiclere-identification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "A Strong Baseline for Vehicle Re-Identification",
    "authors": [
      "Su V. Huynh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Huynh_A_Strong_Baseline_for_Vehicle_Re-Identification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Huynh_A_Strong_Baseline_for_Vehicle_Re-Identification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vehicle Re-Identification (Re-ID) aims to identify the same vehicle across different cameras, hence plays an important role in modern traffic management systems. The technical challenges require the algorithms must be robust in different views, resolution, occlusion and illumination conditions. In this paper, we first analyze the main factors hindering the Vehicle Re-ID performance. We then present our solutions, specifically targeting the dataset Track 2 of the 5th AI City Challenge, including (1) reducing the domain gap between real and synthetic data, (2) network modification by stacking multi heads with attention mechanism, (3) adaptive loss weight adjustment. Our method achieves 61.34% mAP on the private CityFlow testset without using external dataset or pseudo labeling, and outperforms all previous works at 87.1% mAP on the Veri benchmark. The code is available at https://github.com/cybercore-co-ltd/track2_aicity_2021. ",
    "code_link": ""
  },
  "cvpr2021_aicity_arobustmtmctrackingsystemforai-citychallenge2021": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "A Robust MTMC Tracking System for AI-City Challenge 2021",
    "authors": [
      "Jin Ye",
      "Xipeng Yang",
      "Shuai Kang",
      "Yue He",
      "Weiming Zhang",
      "Leping Huang",
      "Minyue Jiang",
      "Wei Zhang",
      "Yifeng Shi",
      "Meng Xia",
      "Xiao Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Ye_A_Robust_MTMC_Tracking_System_for_AI-City_Challenge_2021_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Ye_A_Robust_MTMC_Tracking_System_for_AI-City_Challenge_2021_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-Target Multi-Camera tracking (MTMC) is an essential task in the intelligent city and traffic analysis. It is a great challenging task due to several problems such as heavy occlusions and appearance variance caused by various camera perspectives and congested vehicles. In this paper, we propose a practical framework for dealing with the MTMC problem. The proposed framework contains three stage. Firstly, in the vehicles detection and Re-ID stage, the proposed system leverages Cascade R-CNN to detect all vehicles and extract appearance features with a Re-ID module for all cameras. Secondly, in the Multi-Target Single-Camera tracking (MTSC) stage, on the basis of the detected boxes and appearance features, it tracks multiple vehicles to generate candidate trajectories within each single camera with Tracklet-Plane Matching (TPM) tracking algorithm. Finally, in the Inter-Camera Association (ICA) stage, it associates all candidate trajectories between two successive cameras using the established distance matrix, and combines all successively matching results for final submission. The established distance matrix is simply computed by the Re-ID features and refined by the constraints of traveling time, road structures, and traffic rules to accelerate matching time as well as reduce search space. Extensive experiments on the public track3 test set of NVIDIA AI CITY 2021 CHALLENGE demonstrate the effectiveness of our method, which achieves IDF1 of 77.87%. ",
    "code_link": ""
  },
  "cvpr2021_aicity_multi-cameratrackingbycandidateintersectionratiotrackletmatching": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Multi-Camera Tracking by Candidate Intersection Ratio Tracklet Matching",
    "authors": [
      "Yun-Lun Li",
      "Zhi-Yi Chin",
      "Ming-Ching Chang",
      "Chen-Kuo Chiang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Li_Multi-Camera_Tracking_by_Candidate_Intersection_Ratio_Tracklet_Matching_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Li_Multi-Camera_Tracking_by_Candidate_Intersection_Ratio_Tracklet_Matching_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-camera vehicle tracking at the city scale is an essential task in traffic management for smart cities. Large-scale video analytics is challenging due to the vehicle variabilities, view variations, frequent occlusions, degraded pixel quality, and appearance differences. In this work, we develop a multi-target multi-camera (MTMC) vehicle tracking system based on a newly proposed Candidates Intersection Ratio (CIR) metric that can effectively evaluate vehicle tracklets for matching across views. Our system consists of four modules: (1) Faster-RCNN vehicle detection, (2) detection association based on re-identification feature matching, (3) single-camera tracking (SCT) to produce initial tracklets, (4) multi-camera vehicle tracklet matching and re-identification that creates longer, consistent tracklets across the city scale. Based on popular DNN object detection and SCT modules, we focus on the development of tracklet creation, association, and linking in SCT and MTMC. Specifically, SCT filters are proposed to effectively eliminate unreliable tracklets. The CIR metric improves robust vehicle tracklet linking across visually distinct views. Our system obtains IDF1 score of 0.1343 on the AI City 2021 Challenge Track 3 public leaderboard. ",
    "code_link": ""
  },
  "cvpr2021_aicity_the5thaicitychallenge": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "The 5th AI City Challenge",
    "authors": [
      "Milind Naphade",
      "Shuo Wang",
      "David C. Anastasiu",
      "Zheng Tang",
      "Ming-Ching Chang",
      "Xiaodong Yang",
      "Yue Yao",
      "Liang Zheng",
      "Pranamesh Chakraborty",
      "Christian E. Lopez",
      "Anuj Sharma",
      "Qi Feng",
      "Vitaly Ablavsky",
      "Stan Sclaroff"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Naphade_The_5th_AI_City_Challenge_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Naphade_The_5th_AI_City_Challenge_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The AI City Challenge was created with two goals in mind: (1) pushing the boundaries of research and development in intelligent video analysis for smarter cities use cases, and (2) assessing tasks where the level of performance is enough to cause real-world adoption. Transportation is a segment ripe for such adoption. The fifth AI City Challenge attracted 305 participating teams across 38 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in five challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation being conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. Track 5 was a new track addressing vehicle retrieval using natural language descriptions. The evaluation system shows a general leader board of all submitted results, and a public leader board of results limited to the contest participation rules, where teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data is limited. Results show the promise of AI in Smarter Transportation. State-of-the-art performance for some tasks shows that these technologies are ready for adoption in real-world systems. ",
    "code_link": ""
  },
  "cvpr2021_aicity_dundual-pathtemporalmatchingnetworkfornaturallanguage-basedvehicleretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "DUN: Dual-Path Temporal Matching Network for Natural Language-Based Vehicle Retrieval",
    "authors": [
      "Ziruo Sun",
      "Xinfang Liu",
      "Xiaopeng Bi",
      "Xiushan Nie",
      "Yilong Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Sun_DUN_Dual-Path_Temporal_Matching_Network_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Sun_DUN_Dual-Path_Temporal_Matching_Network_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Retrieving vehicles matching natural language descriptions from collections of videos is a novel and uniquely challenging task, requiring consideration not only of vehicle types and colors, but also of temporal relations, e.g., \"A white crossover keeping straight behind a silver hatchback.\" To perform this task, we propose Dual-path Temporal Matching Network (DUN). DUN uses a pre-trained CNN and GloVe to extract visual and text features, respectively, and GRUs to mine temporal relationships in videos and sentences. Furthermore, the proposed network can attain superior performance by including techniques such as re-ranking. With its simple structure, DUN achieved second place on the AI City Challenge 2021 Track 5. ",
    "code_link": ""
  },
  "cvpr2021_aicity_real-timeandrobustsystemforcountingmovement-specificvehicleatcrowdedintersections": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Real-Time and Robust System for Counting Movement-Specific Vehicle at Crowded Intersections",
    "authors": [
      "Vu-Hoang Tran",
      "Le-Hoai-Hieu Dang",
      "Chinh-Nghiep Nguyen",
      "Ngoc-Hoang-Lam Le",
      "Khanh-Phong Bui",
      "Lam-Truong Dam",
      "Quang-Thang Le",
      "Dinh-Hiep Huynh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Tran_Real-Time_and_Robust_System_for_Counting_Movement-Specific_Vehicle_at_Crowded_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Tran_Real-Time_and_Robust_System_for_Counting_Movement-Specific_Vehicle_at_Crowded_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In order to reduce traffic congestion and improve the efficiency of traffic light signals, intelligent traffic systems are being developed by researchers, and vehicle counting is one of the key techniques in the system. The traditional methods mostly focus on increasing the vehicle counting effectiveness without regard to the program execution efficiency. The practical value of these systems will be reduced if they cannot be operated in real-time on compact IoT device. Therefore, in this paper, we mainly focus on designing a real-time and robust system for the problem of counting specific-movement vehicles. The system is able to detect and track objects in the area of interest, then count those tracked trajectories using the movements. To improve performance of tracking multiple objects, a high recall detection method and an efficient feature matching strategy were proposed. Moreover, to minimize the wrong direction of movement prediction and improve the results of vehicle counting, a cosine similarity-based vehicle counting scheme is applied. Experiments are conducted on AI City 2021 Track-1 dataset. Our method is evaluated on both sides of efficiency and effectiveness. ",
    "code_link": ""
  },
  "cvpr2021_aicity_robustvehiclere-identificationviarigidstructureprior": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Robust Vehicle Re-Identification via Rigid Structure Prior",
    "authors": [
      "Minyue Jiang",
      "Xuanmeng Zhang",
      "Yue Yu",
      "Zechen Bai",
      "Zhedong Zheng",
      "Zhigang Wang",
      "Jian Wang",
      "Xiao Tan",
      "Hao Sun",
      "Errui Ding",
      "Yi Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Jiang_Robust_Vehicle_Re-Identification_via_Rigid_Structure_Prior_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Jiang_Robust_Vehicle_Re-Identification_via_Rigid_Structure_Prior_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vehicle re-identification (re-id) is one of the most important components in the current intelligence transport system, benefiting both the smart traffic management and the optimal path planning. In this paper, we focus on developing a robust part-aware structure-based vehicle re-id system against the massive appearance changes due to the pose and illumination variants. Specifically, we apply the strong convolutional neural networks to extract the visual representation, which is based on the detected vehicle images. Taking one step further, we deploy a part detector to recognize different vehicle parts, such as front, back, left, and right, which explicitly introduce the prior knowledge on the structure of the rigid objective, i.e., vehicle. With the geometry information, we further harness different part feature extractors to filter wrong matches. By using this simple but effective strategy, we remove the hard negative candidates while maintaining high recall accuracy, combing general global-level coarse-grained re-id feature models with part-level fine-grained features. We achieved 71.51% mAP in the vehicle re-id track of the AI City Challenge 2021, which verified the effectiveness and scalability of the proposed structure-based method. ",
    "code_link": "https://github.com/XuanmengZhang/AICITY2021-Track2"
  },
  "cvpr2021_aicity_multi-targetmulti-cameravehicletrackingforcity-scaletrafficmanagement": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Multi-Target Multi-Camera Vehicle Tracking for City-Scale Traffic Management",
    "authors": [
      "Kyujin Shim",
      "Sungjoon Yoon",
      "Kangwook Ko",
      "Changick Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Shim_Multi-Target_Multi-Camera_Vehicle_Tracking_for_City-Scale_Traffic_Management_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Shim_Multi-Target_Multi-Camera_Vehicle_Tracking_for_City-Scale_Traffic_Management_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-target multi-camera (MTMC) tracking is one of the important fields in computer vision, where multiple objects are tracked across multiple cameras. MTMC tracking can be applied to various tasks such as video surveillance systems, city-scale traffic management, and transportation systems analysis for intelligent city planning. However, it is challenging due to the large variety of conditions of each camera, such as perspective and illumination. Furthermore, MTMC tracking for vehicles is more problematic because of the relatively large inter-class similarity and intra-class variability. In this paper, we tackle the MTMC tracking problem for vehicles by dividing it into three main steps: (i) vehicle detection and feature extraction, (ii) multi-target single-camera tracking using the appearance feature of each vehicle, and (iii) multi-camera association of local trajectories from each camera. Our method shows comparable results with other highly-ranked methods in AI City Challenge 2021 and outperforms a recent MTMC tracking method that ranked first place in AI City Challenge 2020. ",
    "code_link": ""
  },
  "cvpr2021_aicity_sbnetsegmentation-basednetworkfornaturallanguage-basedvehiclesearch": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "SBNet: Segmentation-Based Network for Natural Language-Based Vehicle Search",
    "authors": [
      "Sangrok Lee",
      "Taekang Woo",
      "Sang Hun Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Lee_SBNet_Segmentation-Based_Network_for_Natural_Language-Based_Vehicle_Search_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Lee_SBNet_Segmentation-Based_Network_for_Natural_Language-Based_Vehicle_Search_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Natural language-based vehicle retrieval is a task to find a target vehicle within a given image based on a natural language description as a query. This technology can be applied to various areas including police searching for a suspect vehicle. However, it is challenging due to the ambiguity of language descriptions and the difficulty of processing multi-modal data. To tackle this problem, we propose a deep neural network called SBNet that performs natural language-based segmentation for vehicle retrieval. We also propose two task-specific modules to improve performance: a substitution module that helps features from different domains to be embedded in the same space and a future prediction module that learns temporal information. SBNet has been trained using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three unique natural language descriptions each and tested 530 unique vehicle tracks and their corresponding query sets. SBNet achieved a significant improvement over the baseline in the natural language-based vehicle tracking track in the AI City Challenge 2021. ",
    "code_link": ""
  },
  "cvpr2021_aicity_keyword-basedvehicleretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Keyword-Based Vehicle Retrieval",
    "authors": [
      "Eun-Ju Park",
      "Hoyoung Kim",
      "Seonghwan Jeong",
      "Byungkon Kang",
      "YoungMin Kwon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Park_Keyword-Based_Vehicle_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Park_Keyword-Based_Vehicle_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Natural language-based vehicle retrieval system makes controlling a city-scale traffic system easy to maintain and adaptable to changing requirements. It provides a convenient means in managing traffic flows or detecting accidents related to a specific vehicle. Such a system is different from most query-based video retrieval systems because the language for traffic situations and visible objects in traffic video streams are limited. Existing techniques for language-based general video retrieval problems measure the similarity between language representations and video representations. Our system focuses on several features that can distinguish vehicles from others. Particularly, our proposed vehicle retrieval system defines a set of features that can differentiate a vehicle from others and calculates the similarity between queries and video frames based on the features. The proposed technique places our approach in the third place in the 2021 AI City Challenge. ",
    "code_link": ""
  },
  "cvpr2021_aicity_anempiricalstudyofvehiclere-identificationontheaicitychallenge": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "An Empirical Study of Vehicle Re-Identification on the AI City Challenge",
    "authors": [
      "Hao Luo",
      "Weihua Chen",
      "Xianzhe Xu",
      "Jianyang Gu",
      "Yuqi Zhang",
      "Chong Liu",
      "Yiqi Jiang",
      "Shuting He",
      "Fan Wang",
      "Hao Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Luo_An_Empirical_Study_of_Vehicle_Re-Identification_on_the_AI_City_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Luo_An_Empirical_Study_of_Vehicle_Re-Identification_on_the_AI_City_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper introduces our solution for the Track2 in AI City Challenge 2021 (AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the real-world data and synthetic data. We mainly focus on four points, i.e. training data, unsupervised domain-adaptive (UDA) training, post-processing, model ensembling in this challenge. (1) Both cropping training data and using synthetic data can help the model learn more discriminative features. (2) Since there is a new scenario in the test set that dose not appear in the training set, UDA methods perform well in the challenge. (3) Post-processing techniques including re-ranking, image-to-track retrieval, inter-camera fusion, etc, significantly improve final performance. (4) We ensemble CNN-based models and transformer-based models which provide different representation diversity. With aforementioned techniques, our method finally achieves 0.7445 mAP score, yielding the first place in the competition. Codes are available at https://github.com/michuanhaohao/AICITY2021_Track2_DMT. ",
    "code_link": ""
  },
  "cvpr2021_aicity_multi-classmulti-movementvehiclecountingbasedoncentertrack": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Multi-Class Multi-Movement Vehicle Counting Based on CenterTrack",
    "authors": [
      "Viktor Kocur",
      "Milan Ftacnik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Kocur_Multi-Class_Multi-Movement_Vehicle_Counting_Based_on_CenterTrack_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Kocur_Multi-Class_Multi-Movement_Vehicle_Counting_Based_on_CenterTrack_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper we present our approach to the Track 1 of the 2021 AI City Challenge. The goal of the challenge track is to to analyse footage captured with traffic cameras by counting the number of vehicles performing various pre-defined motions of interest. Our approach is based on the CenterTrack object detection and tracking neural network used in conjunction with a simple IoU-based tracking algorithm. In the public evaluation server our system achieved the S1 score of 0.8449 placing it at the 8th place on the public leaderboard. ",
    "code_link": "https://github.com/liwenwei123/AIC_2020"
  },
  "cvpr2021_aicity_towardsaccuratevisualandnaturallanguage-basedvehicleretrievalsystems": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Towards Accurate Visual and Natural Language-Based Vehicle Retrieval Systems",
    "authors": [
      "Pirazh Khorramshahi",
      "Sai Saketh Rambhatla",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Khorramshahi_Towards_Accurate_Visual_and_Natural_Language-Based_Vehicle_Retrieval_Systems_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Khorramshahi_Towards_Accurate_Visual_and_Natural_Language-Based_Vehicle_Retrieval_Systems_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this work, we consider two tracks of the 2021 NVIDIA AI City Challenge, the City-Scale Multi-Camera Vehicle Re-identification and Natural language-based Vehicle Retrieval. For the vehicle re-identification task, we employ the state-of-art Excited Vehicle Re-Identification deep representation learning model coupled with best training practices and domain adaptation techniques to obtain robust embeddings. We further refine the re-identification results through a series of post-processing steps to remove camera and vehicle orientation bias that is inherent in the task of re-identification. We also take advantage of multiple observations of a vehicle using track-level information and finally obtain fine-grained retrieval results. For the task of Natural language-based vehicle retrieval we leverage the recently proposed Contrastive Language-Image Pre-training model and propose a simple yet effective text-based vehicle retrieval system. We compare our performance against the top submissions to the challenge and our systems are ranked 8^\\text thin the public leaderboard for both tracks. ",
    "code_link": ""
  },
  "cvpr2021_aicity_avision-basedsystemfortrafficanomalydetectionusingdeeplearninganddecisiontrees": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "A Vision-Based System for Traffic Anomaly Detection Using Deep Learning and Decision Trees",
    "authors": [
      "Armstrong Aboah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Aboah_A_Vision-Based_System_for_Traffic_Anomaly_Detection_Using_Deep_Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Aboah_A_Vision-Based_System_for_Traffic_Anomaly_Detection_Using_Deep_Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Any intelligent traffic monitoring system must be able to detect anomalies such as traffic accidents in real-time. In this paper, we propose a Decision-Tree enabled approach powered by Deep Learning for extracting anomalies from traffic cameras while accurately estimating the start and end time of the anomalous event. Our approach included creating a detection model, followed by anomaly detection and analysis. YOLOv5 served as the foundation for our detection model. The anomaly detection and analysis step entail traffic scene background estimation, road mask extraction, and adaptive thresholding. Candidate anomalies were passed through a decision tree to detect and analyze final anomalies. The proposed approach yielded an F1 score of 0.8571, and an s4 score of 0.5686, per the experimental validation. ",
    "code_link": ""
  },
  "cvpr2021_aicity_aregion-and-trajectorymovementmatchingformultipleturn-countsatroadintersectiononedgedevice": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "A Region-and-Trajectory Movement Matching for Multiple Turn-Counts at Road Intersection on Edge Device",
    "authors": [
      "Duong Nguyen-Ngoc Tran",
      "Long Hoang Pham",
      "Huy-Hung Nguyen",
      "Tai Huu-Phuong Tran",
      "Hyung-Joon Jeon",
      "Jae Wook Jeon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Tran_A_Region-and-Trajectory_Movement_Matching_for_Multiple_Turn-Counts_at_Road_Intersection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Tran_A_Region-and-Trajectory_Movement_Matching_for_Multiple_Turn-Counts_at_Road_Intersection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In intelligent traffic systems, vehicle detection and counting have become an important task. The counting information is essential for reducing traffic congestion and improving traffic signal capability. Traditional methods have been focusing on counting vehicles in a single frame or consecutive frames. However, they have not yet considered the movement of interest (MOI) of the vehicles moving in different lanes and directions. This paper proposes a region-and-trajectory movement matching method that aims to detect and count vehicles for each movement on the road. First, the YOLOv5 detection model is used to detect candidate vehicles in the region of interest (ROI). Second, the SORT tracking method associates vehicles of the same instance in consecutive images to create tracked trajectories. Then, the counting method using the combination of MOI regions and predefined movement tracks. Each tracked trajectory is assigned to the corresponding movement id and is outputted to the result file. The efficiency and effectiveness of the proposed method have been evaluated and ranked 3rd on AI City Challenge 2021 Track 1 leaderboard. Further experiments showed that the method could achieve around 120 fps on an NVIDIA Quadro RTX 8000 and 20 fps on an NVIDIA Jetson Xavier AGX. ",
    "code_link": ""
  },
  "cvpr2021_aicity_multi-cameravehicletrackingsystembasedonspatial-temporalfiltering": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Multi-Camera Vehicle Tracking System Based on Spatial-Temporal Filtering",
    "authors": [
      "Pengfei Ren",
      "Kang Lu",
      "Yu Yang",
      "Yun Yang",
      "Guangze Sun",
      "Wei Wang",
      "Gang Wang",
      "Junliang Cao",
      "Zhifeng Zhao",
      "Wei Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Ren_Multi-Camera_Vehicle_Tracking_System_Based_on_Spatial-Temporal_Filtering_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Ren_Multi-Camera_Vehicle_Tracking_System_Based_on_Spatial-Temporal_Filtering_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-Camera multi-target tracking is essential in the research field of urban intelligence traffic. It shows that the task becomes extremely difficult due to the changes of illumination, angle, and occlusion under different cameras. In this paper, we propose an efficient multi-camera vehicle tracking system, which contains a model trained with multi-loss to extract appearance feature, and a filter with spatial-temporal information between cameras. The proposed system includes 3 parts. Firstly, we generate tracklets in single-camera with different views by vehicle detection and multi-target tracking. Secondly, we extract the appearance feature of each tracklet through the trained vehicle ReID model. Thirdly, we innovatively propose a matching strategy that calculates several factors, the similarity of appearance features, the time information, and the space information of target ID between adjacent cameras. The proposed system ranks the sixth place in the City-Scale Multi-Camera Vehicle Tracking of AI City 2021 Challenge (Track 3) with a score of 0.5763. ",
    "code_link": ""
  },
  "cvpr2021_aicity_goodpracticesandastrongbaselinefortrafficanomalydetection": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Good Practices and a Strong Baseline for Traffic Anomaly Detection",
    "authors": [
      "Yuxiang Zhao",
      "Wenhao Wu",
      "Yue He",
      "Yingying Li",
      "Xiao Tan",
      "Shifeng Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Zhao_Good_Practices_and_a_Strong_Baseline_for_Traffic_Anomaly_Detection_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Zhao_Good_Practices_and_a_Strong_Baseline_for_Traffic_Anomaly_Detection_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The detection of traffic anomalies is a critical component of the intelligent city transportation management system. Previous works have proposed a variety of notable insights and taken a step forward in this field, however, dealing with the complex traffic environment remains a challenge. Moreover, the lack of high-quality data and the complexity of the traffic scene, motivate us to study this problem from a hand-crafted perspective. In this paper, we propose a straightforward and efficient framework that includes pre-processing, a dynamic track module, and post-processing. With video stabilization, background modeling, and vehicle detection, the pro-processing phase aims to generate candidate anomalies. The dynamic tracking module seeks and locates the start time of anomalies by utilizing vehicle motion patterns and spatiotemporal status. Finally, we use the post-processing to fine-tune the temporal boundary of anomalies. Not surprisingly, our proposed framework was ranked 1st in the NVIDIA AI CITY 2021 leaderboard for traffic anomaly detection. Codes will be available. ",
    "code_link": "https://github.com/paddlepaddle/paddle"
  },
  "cvpr2021_aicity_vehiclere-identificationbasedonensemblingdeeplearningfeaturesincludingasynthetictrainingdataset,orientationandbackgroundfeatures,andcameraverification.": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Vehicle Re-Identification Based on Ensembling Deep Learning Features Including a Synthetic Training Dataset, Orientation and Background Features, and Camera Verification.",
    "authors": [
      "Marta Fernandez",
      "Paula Moral",
      "Alvaro Garcia-Martin",
      "Jose M. Martinez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Fernandez_Vehicle_Re-Identification_Based_on_Ensembling_Deep_Learning_Features_Including_a_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Fernandez_Vehicle_Re-Identification_Based_on_Ensembling_Deep_Learning_Features_Including_a_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vehicle re-identification has the objective of finding a specific vehicle among different vehicle crops captured by multiple cameras placed at multiple intersections. Among the different difficulties, high intra-class variability and high inter-class similarity can be highlighted. Moreover, the resolution of the images can be different, which also means a challenge in the re-identification task. Intending to face these problems, we use as baseline our previous work based on obtaining different deep learning features and ensembling them to get a single, stable and robust feature vector. It also includes post-processing techniques that explode all the information provided by the CityFlowV2-ReID dataset, including a re-ranking step. Then, in this paper, several newly included improvements are described. Background and orientation similarity matrices are added to the system to reduce bias towards these characteristics. Furthermore, we take into account the camera labels to penalize the gallery images that share camera with the query image. Additionally, to improve the training step, a synthetic dataset is added to the original one. ",
    "code_link": ""
  },
  "cvpr2021_aicity_tiedacycleconsistentencoder-decodermodelfortext-to-imageretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "TIED: A Cycle Consistent Encoder-Decoder Model for Text-to-Image Retrieval",
    "authors": [
      "Clint Sebastian",
      "Raffaele Imbriaco",
      "Panagiotis Meletis",
      "Gijs Dubbelman",
      "Egor Bondarev",
      "Peter H.N. de With"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Sebastian_TIED_A_Cycle_Consistent_Encoder-Decoder_Model_for_Text-to-Image_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Sebastian_TIED_A_Cycle_Consistent_Encoder-Decoder_Model_for_Text-to-Image_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Retrieving specific vehicle tracks by Natural Language (NL)-based descriptions is a convenient way to monitor vehicle movement patterns and traffic-related events. NL-based image retrieval has several applications in smart cities, traffic control, etc. In this work, we propose TIED, a text-to-image encoder-decoder model for the simultaneous extraction of visual and textual information for vehicle track retrieval. The model consists of an encoder network that enforces the two modalities into a common latent space and a decoder network that performs an inverse mapping to the text descriptions. The method exploits visual semantic attributes of a target vehicle along with a cycle-consistency loss. The proposed method employs both intra-class and inter-class relationships to improve retrieval performance. Our system yields competitive performance achieving the 7th position in the Natural Language-Based Vehicle Retrieval public track of the 2021 NVIDIA AI City Challenge. We demonstrate that the proposed TIED model obtains six times higher Mean Reciprocal Rank (MRR) than the baseline, achieving an MRR of 15.48. The code and models will be made publicly available. ",
    "code_link": ""
  },
  "cvpr2021_aicity_trafficvideoeventretrievalviatextqueryusingvehicleappearanceandmotionattributes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Traffic Video Event Retrieval via Text Query Using Vehicle Appearance and Motion Attributes",
    "authors": [
      "Tien-Phat Nguyen",
      "Ba-Thinh Tran-Le",
      "Xuan-Dang Thai",
      "Tam V. Nguyen",
      "Minh N. Do",
      "Minh-Triet Tran"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Nguyen_Traffic_Video_Event_Retrieval_via_Text_Query_Using_Vehicle_Appearance_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Nguyen_Traffic_Video_Event_Retrieval_via_Text_Query_Using_Vehicle_Appearance_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Traffic event retrieval is one of the important tasks for intelligent traffic system management. To find accurate candidate events in traffic videos corresponding to a specific text query, it is necessary to understand the text query's attributes, represent the visual and motion attributes of vehicles in videos, and measure the similarity between them. Thus we propose a promising method for vehicle event retrieval from a natural-language-based specification. We utilize both appearance and motion attributes of a vehicle and adapt the COOT model to evaluate the semantic relationship between a query and a video track. Experiments with the test dataset of Track 5 in AI City Challenge 2021 show that our method is among the top 6 with a score of 0.1560. ",
    "code_link": ""
  },
  "cvpr2021_aicity_connectinglanguageandvisionfornaturallanguage-basedvehicleretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Connecting Language and Vision for Natural Language-Based Vehicle Retrieval",
    "authors": [
      "Shuai Bai",
      "Zhedong Zheng",
      "Xiaohan Wang",
      "Junyang Lin",
      "Zhu Zhang",
      "Chang Zhou",
      "Hongxia Yang",
      "Yi Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Bai_Connecting_Language_and_Vision_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Bai_Connecting_Language_and_Vision_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Vehicle search is one basic task for the efficient traffic management in terms of the AI City. Most existing practices focus on the image-based vehicle matching, including vehicle re-identification and vehicle tracking. In this paper, we apply one new modality, i.e., the language description, to search the vehicle of interest and explore the potential of this task in the real-world scenario. The natural language-based vehicle search poses one new challenge of fine-grained understanding of both vision and language modalities. To connect language and vision, we propose to jointly train the state-of-the-art vision models with the transformer-based language model in an end-to-end manner. Except for the network structure design and the training strategy, several optimization objectives are also revisited in this work. The qualitative and quantitative experiments verify the effectiveness of the proposed method. Our proposed method has achieved the 1st place on the 5th AI City Challenge, yielding competitive performance 18.69% MRR accuracy on the private test set. We hope this work can pave the way for the future study on using language description effectively and efficiently for real-world vehicle retrieval systems. The code will be available at https://github.com/ShuaiBai623/AIC2021-T5-CLV. ",
    "code_link": ""
  },
  "cvpr2021_aicity_fastvehicleturning-movementcountingusinglocalization-basedtracking": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Fast Vehicle Turning-Movement Counting Using Localization-Based Tracking",
    "authors": [
      "Derek Gloudemans",
      "Daniel B. Work"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Gloudemans_Fast_Vehicle_Turning-Movement_Counting_Using_Localization-Based_Tracking_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Gloudemans_Fast_Vehicle_Turning-Movement_Counting_Using_Localization-Based_Tracking_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Despite the high utility of traffic volume and turning movement data, such data is still hard to come by for the vast majority of roadways and intersections in nearly every city. Edge computing devices offer a promising tool for recording turning movement data if lightweight algorithms can be designed to run in real-time with relatively modest computational complexity. To that end, this work presents Vehicle Turning-Movement Counting using Localization-based Tracking (LBT-Count). This method is fast because it never performs detection on a full frame. Instead, only a few portions of the image are cropped and used to detect objects within the frame. The method achieves competitive performance on the public evaluation server for Track 1 of the AI City Challenge (7th overall on the first 50% of data). Furthermore, we show that LBT-Count is 52% faster than an analogous counting algorithm utilizing a traditional tracking-by-detection framework on available challenge data. ",
    "code_link": "https://github.com/DerekGloudemans/LBT-count"
  },
  "cvpr2021_aicity_city-scalemulti-cameravehicletrackingguidedbycrossroadzones": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "City-Scale Multi-Camera Vehicle Tracking Guided by Crossroad Zones",
    "authors": [
      "Chong Liu",
      "Yuqi Zhang",
      "Hao Luo",
      "Jiasheng Tang",
      "Weihua Chen",
      "Xianzhe Xu",
      "Fan Wang",
      "Hao Li",
      "Yi-Dong Shen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Liu_City-Scale_Multi-Camera_Vehicle_Tracking_Guided_by_Crossroad_Zones_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Liu_City-Scale_Multi-Camera_Vehicle_Tracking_Guided_by_Crossroad_Zones_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-Target Multi-Camera Tracking has a wide range of applications and is the basis for many advanced inferences and predictions. This paper describes our solution to the Track 3 multi-camera vehicle tracking task in 2021 AI City Challenge (AICITY21). This paper proposes a multi-target multi-camera vehicle tracking framework guided by the crossroad zones. The framework includes: (1) Use mature detection and vehicle re-identification models to extract targets and appearance features. (2) Use modified JDETracker (without detection module) to track single-camera vehicles and generate single-camera tracklets. (3) According to the characteristics of the crossroad, the Tracklet Filter Strategy and the Direction Based Temporal Mask are proposed. (4) Propose Sub-clustering in Adjacent Cameras for multi-camera tracklets matching. Through the above techniques, our method obtained an IDF1 score of 0.8095, ranking first on the leaderboard. The code will be released later. ",
    "code_link": "https://github.com/ultralytics/YOLOv5"
  },
  "cvpr2021_aicity_robustandonlinevehiclecountingatcrowdedintersections": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Robust and Online Vehicle Counting at Crowded Intersections",
    "authors": [
      "Jincheng Lu",
      "Meng Xia",
      "Xu Gao",
      "Xipeng Yang",
      "Tianran Tao",
      "Hao Meng",
      "Wei Zhang",
      "Xiao Tan",
      "Yifeng Shi",
      "Guanbin Li",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Lu_Robust_and_Online_Vehicle_Counting_at_Crowded_Intersections_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Lu_Robust_and_Online_Vehicle_Counting_at_Crowded_Intersections_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose an online movement-specific vehicle counting system to realize robust traffic flow analysis at crowed intersections. Our proposed framework adopts PP-YOLO as the vehicle detector and adapts the Deep-Sort algorithm to perform multi-object tracking. In order to realize online and robust vehicle counting, we further adopt a shape-based movement assignment strategy to differentiate movements and carefully designed spatial constraints to effectively reduce false-positive counts. Our proposed framework achieves the overall S1-score of 0.9467, ranking the first in the AICITY2021-track1 challenge. ",
    "code_link": ""
  },
  "cvpr2021_aicity_anefficientapproachforanomalydetectionintrafficvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "An Efficient Approach for Anomaly Detection in Traffic Videos",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Doshi_An_Efficient_Approach_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Doshi_An_Efficient_Approach_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Due to its relevance in intelligent transportation systems, anomaly detection in traffic videos has recently received much interest. It remains a difficult problem due to a variety of factors influencing the video quality of a real-time traffic feed, such as temperature, perspective, lighting conditions, and so on. Even though state-of-the-art methods perform well on the available benchmark datasets, they need a large amount of external training data as well as substantial computational resources. In this paper, we propose an efficient approach for a video anomaly detection system which is capable of running at the edge devices, e.g., on a roadside camera. The proposed approach comprises a pre-processing module that detects changes in the scene and removes the corrupted frames, a two-stage background modelling module and a two-stage object detector. Finally, a backtracking anomaly detection algorithm computes a similarity statistic and decides on the onset time of the anomaly. We also propose a sequential change detection algorithm that can quickly adapt to a new scene and detect changes in the similarity statistic. Experimental results on the Track 4 test set of the 2021 AI City Challenge show the efficacy of the proposed framework as we achieve an F1-score of 0.9157 along with 8.4027 root mean square error (RMSE) and are ranked fourth in the competition. ",
    "code_link": ""
  },
  "cvpr2021_aicity_anocclusion-awaremulti-targetmulti-cameratrackingsystem": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "An Occlusion-Aware Multi-Target Multi-Camera Tracking System",
    "authors": [
      "Andreas Specker",
      "Daniel Stadler",
      "Lucas Florin",
      "Jurgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Specker_An_Occlusion-Aware_Multi-Target_Multi-Camera_Tracking_System_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Specker_An_Occlusion-Aware_Multi-Target_Multi-Camera_Tracking_System_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Multi-camera tracking of vehicles on a city-scale level is a crucial task for efficient traffic monitoring. Most of the errors made by such multi-target multi-camera tracking systems arise due to tracking failures or misleading visual information of detection boxes under occlusion. Therefore, we propose an occlusion-aware approach that leverages temporal information from tracks to improve the single-camera tracking performance by an occlusion handling strategy and additional modules to filter false detections. For the multi-camera tracking, we discard obstacle-occluded detection boxes by a background filtering technique and boxes overlapping with other targets using the available track information to improve the quality of extracted visual features. Furthermore, topological and temporal constraints are incorporated to simplify the re-identification task in the multi-camera clustering. We give detailed insights into our method with ablative experiments and show its competitiveness on the CityFlowV2 dataset, where we achieve promising results ranking 4th in Track 3 of the 2021 AI City Challenge. ",
    "code_link": ""
  },
  "cvpr2021_aicity_contrastivelearningfornaturallanguage-basedvehicleretrieval": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "AICity",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - AI City Challenge",
    "title": "Contrastive Learning for Natural Language-Based Vehicle Retrieval",
    "authors": [
      "Tam Minh Nguyen",
      "Quang Huu Pham",
      "Linh Bao Doan",
      "Hoang Viet Trinh",
      "Viet-Anh Nguyen",
      "Viet-Hoang Phan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Nguyen_Contrastive_Learning_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Nguyen_Contrastive_Learning_for_Natural_Language-Based_Vehicle_Retrieval_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " AI City Challenge 2021 Task 5: The Natural Language-Based Vehicle Tracking is a Natural Language-based Vehicle Retrieval task, which requires retrieving a single-camera track using a set of three natural language descriptions of the specific targets. In this paper, we present our methods to tackle the difficulties of the provided task. Experiments with our approaches on the competitive dataset from AICity Challenge 2021 show that our techniques achieve Mean Reciprocal Rank score of 0.1701 on the public test dataset and 0.1571 on the private test dataset. ",
    "code_link": ""
  },
  "cvpr2021_imw_perceptuallossforrobustunsupervisedhomographyestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Image Matching: Local Features and Beyond",
    "title": "Perceptual Loss for Robust Unsupervised Homography Estimation",
    "authors": [
      "Daniel Koguciuk",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/IMW/html/Koguciuk_Perceptual_Loss_for_Robust_Unsupervised_Homography_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/IMW/papers/Koguciuk_Perceptual_Loss_for_Robust_Unsupervised_Homography_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Homography estimation is often an indispensable step in many computer vision tasks. The existing approaches, however, are not robust to illumination and/or larger viewpoint changes. In this paper, we propose bidirectional implicit Homography Estimation (biHomE) loss for unsupervised homography estimation. biHomE minimizes the distance in the feature space between the warped image from the source viewpoint and the corresponding image from the target viewpoint. Since we use a fixed pre-trained feature extractor and the only learnable component of our framework is the homography network, we effectively decouple the homography estimation from representation learning. We use an additional photometric distortion step in the synthetic COCO dataset generation to better represent the illumination variation of the real-world scenarios. We show that biHomE achieves state-of-the-art performance on synthetic COCO dataset, which is also comparable or better compared to supervised approaches. Furthermore, the empirical results demonstrate the robustness of our approach to illumination variation compared to existing methods. ",
    "code_link": ""
  },
  "cvpr2021_imw_dfmaperformancebaselinefordeepfeaturematching": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "IMW",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Image Matching: Local Features and Beyond",
    "title": "DFM: A Performance Baseline for Deep Feature Matching",
    "authors": [
      "Ufuk Efe",
      "Kutalmis Gokalp Ince",
      "Aydin Alatan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/IMW/html/Efe_DFM_A_Performance_Baseline_for_Deep_Feature_Matching_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/IMW/papers/Efe_DFM_A_Performance_Baseline_for_Deep_Feature_Matching_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " A novel image matching method is proposed that utilizes learned features extracted by an off-the-shelf deep neural network to obtain a promising performance. The proposed method uses pre-trained VGG architecture as a feature extractor and does not require any additional training specific to improve matching. Inspired by well-established concepts in the psychology area, such as the Mental Rotation paradigm, an initial warping is performed as a result of a preliminary geometric transformation estimate. These estimates are simply based on dense matching of nearest neighbors at the terminal layer of VGG network outputs of the images to be matched. After this initial alignment, the same approach is repeated again between reference and aligned images in a hierarchical manner to reach a good localization and matching performance. Our algorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching Accuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches dataset [4], which indicates a better performance than the state-of-the-art. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_quad-dipforx-raycargoimagedecomposition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Quad-DIP for X-Ray Cargo Image Decomposition",
    "authors": [
      "Zheng Hu",
      "Qiang Li",
      "Gang Fu",
      "Yuxiang Xing",
      "Li Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Hu_Quad-DIP_for_X-Ray_Cargo_Image_Decomposition_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Hu_Quad-DIP_for_X-Ray_Cargo_Image_Decomposition_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " To identify different cargoes on vehicles accurately in scanned image is a tough issue. An unsupervised image decomposition method, based on a novel dual-stage double-DIP (DDIP) network, named as Quad-DIP, was proposed for the decomposition of X-ray scanned image of a cargo vehicle into vehicle and goods separately without ground truth data. The model could be effectively trained based on the fact that, firstly, the structure contents of same type vehicles were similar in the images, and secondly, the contents of goods on different vehicles were different and independent to each other. Our work focus on the content-wise correlation between them. The vehicle structure could be identified from two inputs containing the same type of vehicles, and the image could be decomposed into two components of vehicle structure and cargo information accurately after the training of Quad-DIP. We examine the accuracy of this method on the collected X-ray cargo vehicle dataset. The decomposition of Quad-DIP was more accurate than those of other published methods in literature. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_instancesegmentation-basedidentificationofpelagicspeciesinacousticbackscatterdata": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Instance Segmentation-Based Identification of Pelagic Species in Acoustic Backscatter Data",
    "authors": [
      "Tunai Porto Marques",
      "Melissa Cote",
      "Alireza Rezvanifar",
      "Alexandra Branzan Albu",
      "Kaan Ersahin",
      "Todd Mudge",
      "Stephane Gauthier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Marques_Instance_Segmentation-Based_Identification_of_Pelagic_Species_in_Acoustic_Backscatter_Data_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Marques_Instance_Segmentation-Based_Identification_of_Pelagic_Species_in_Acoustic_Backscatter_Data_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper addresses the automatic identification of pelagic species in acoustic backscatter data. Large quantities of data acquired during underwater acoustic surveys for environmental monitoring and resources management, visualized as echograms, are typically analyzed manually or semi-automatically by marine biologists, which is time-consuming and prone to errors and inter-expert disagreements. In this paper, we propose to detect pelagic species (schools of herring and of juvenile salmon) from echograms with a deep learning (DL) framework based on instance segmentation, allowing us to carefully study the acoustic properties of the targets and to address specific challenges such as close proximity between schools and varying size. Experimental results demonstrate our system's ability to correctly detect pelagic species from echograms and to outperform an existing object detection framework designed for schools of herring in terms of detection performance and computational resources utilization. Our pixel-level detection method has the advantage of generating a precise identification of the pixel groups forming each detection, opening up many possibilities for automatic biological analyses. ",
    "code_link": "https://github.com/facebookresearch/detectron2"
  },
  "cvpr2021_pbvs_leveragingmultiscalebackbonewithmultilevelsupervisionforthermalimagesuperresolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Leveraging Multi Scale Backbone With Multilevel Supervision for Thermal Image Super Resolution",
    "authors": [
      "Sabari Nathan",
      "Priya Kansal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Nathan_Leveraging_Multi_Scale_Backbone_With_Multilevel_Supervision_for_Thermal_Image_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Nathan_Leveraging_Multi_Scale_Backbone_With_Multilevel_Supervision_for_Thermal_Image_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper proposes an attention based multi-level model with multi-scale backbone for thermal image super-resolution. The model leverages the multi-scale backbone as well. The thermal image dataset is provided by PBVS 2020 in their thermal image super-resolution challenge. This dataset contains the images with three different resolution scales(low, medium, high) [??]. However, only the medium and high resolution images are used to train to train the proposed architecture to generate the super-resolution images in x2, x4 scales. The proposed architecture is based on the Res2net blocks as the backbone of the network. Along with this, the coordinate convolution layer and a dual attention are also used in the architecture. Further, the multi-level supervision is implemented to supervise the output image resolution similarity with the real image at each block during training. To test the robustness of the proposed model, we evaluated our model on the Thermal-6 dataset [??]. The results show that our model is efficient to achieve the state of art results on the PBVS dataset. Further the results on the Thermal-6 dataset show that the model has a decent generalization capacity. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_generalizedunsupervisedclusteringofhyperspectralimagesofgeologicaltargetsinthenearinfrared": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Generalized Unsupervised Clustering of Hyperspectral Images of Geological Targets in the Near Infrared",
    "authors": [
      "Angela F. Gao",
      "Brandon Rasmussen",
      "Peter Kulits",
      "Eva L. Scheller",
      "Rebecca Greenberger",
      "Bethany L. Ehlmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Gao_Generalized_Unsupervised_Clustering_of_Hyperspectral_Images_of_Geological_Targets_in_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Gao_Generalized_Unsupervised_Clustering_of_Hyperspectral_Images_of_Geological_Targets_in_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The application of infrared hyperspectral imagery to geological problems is becoming more popular as data become more accessible and cost-effective. Clustering and classifying spectrally similar materials is often a first step in applications ranging from economic mineral exploration on Earth to planetary exploration on Mars. Semi-manual classification guided by expertly developed spectral parameters can be time consuming and biased, while supervised methods require abundant labeled data and can be difficult to generalize. Here we develop a fully unsupervised workflow for feature extraction and clustering informed by both expert spectral geologist input and quantitative metrics. Our pipeline uses a lightweight autoencoder followed by Gaussian mixture modeling to map the spectral diversity within any image. We validate the performance of our pipeline at submillimeter-scale with expert-labelled data from the Oman ophiolite drill core and evaluate performance at meters-scale with partially classified orbital data of Jezero Crater on Mars (the landing site for the Perseverance rover). We additionally examine the effects of various preprocessing techniques used in traditional analysis of hyperspectral imagery. This pipeline provides a fast and accurate clustering map of similar geological materials and consistently identifies and separates major mineral classes in both laboratory imagery and remote sensing imagery. We refer to our pipeline as \"\"Generalized Pipeline for Spectroscopic Unsupervised clustering of Minerals (GyPSUM).\"\" ",
    "code_link": ""
  },
  "cvpr2021_pbvs_self-trainingguidedadversarialdomainadaptationforthermalimagery": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Self-Training Guided Adversarial Domain Adaptation for Thermal Imagery",
    "authors": [
      "Ibrahim Batuhan Akkaya",
      "Fazil Altinel",
      "Ugur Halici"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Akkaya_Self-Training_Guided_Adversarial_Domain_Adaptation_for_Thermal_Imagery_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Akkaya_Self-Training_Guided_Adversarial_Domain_Adaptation_for_Thermal_Imagery_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Deep models trained on large-scale RGB image datasets have shown tremendous success. It is important to apply such deep models to real-world problems. However, these models suffer from a performance bottleneck under illumination changes. Thermal IR cameras are more robust against such changes, and thus can be very useful for the real-world problems. In order to investigate efficacy of combining feature-rich visible spectrum and thermal image modalities, we propose an unsupervised domain adaptation method which does not require RGB-to-thermal image pairs. We employ large-scale RGB dataset MS-COCO as source domain and thermal dataset FLIR ADAS as target domain to demonstrate results of our method. Although adversarial domain adaptation methods aim to align the distributions of source and target domains, simply aligning the distributions cannot guarantee perfect generalization to the target domain. To this end, we propose a self-training guided adversarial domain adaptation method to promote generalization capabilities of adversarial domain adaptation methods. To perform self-training, pseudo labels are assigned to the samples on the target thermal domain to learn more generalized representations for the target domain. Extensive experimental analyses show that our proposed method achieves better results than the state-of-the-art adversarial domain adaptation methods. The code and models are publicly available. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_learningfromthewebweblysupervisedmeta-learningformaskedfacerecognition": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Learning From the Web: Webly Supervised Meta-Learning for Masked Face Recognition",
    "authors": [
      "Wenbo Zheng",
      "Lan Yan",
      "Fei-Yue Wang",
      "Chao Gou"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Zheng_Learning_From_the_Web_Webly_Supervised_Meta-Learning_for_Masked_Face_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Zheng_Learning_From_the_Web_Webly_Supervised_Meta-Learning_for_Masked_Face_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Mask wearing has been considered as an effective measure to prevent the spread of COVID-19 during the current pandemic. However, most advanced face recognition approaches are not adequate for masked face recognition, particularly in dealing with the issue of training through the datasets covering only a limited number of images with ground-truth labels. In this work, we propose to learn from the large scale of web images and corresponding tags without any manual annotations along with limited fully annotated datasets. In particular, inspired by the recent success of webly supervised learning in deep neural networks, we capitalize on readily-available web images with noisy annotations to learn a robust representation for masked faces. Besides, except for the conventional spatial representation learning, we propose to leverage the power of frequency domain to capture the local representative information of unoccluded facial parts. This approach learns robust feature embeddings derived from our feature fusion architecture to make joint and full use of information from both spatial and frequency domains. Experimental results on seven benchmarks show that the proposed approach significantly improves the performance compared with other state-of-theart methods. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_reconstructionofcassi-ramanimageswithmachine-learning": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Reconstruction of CASSI-Raman Images With Machine-Learning",
    "authors": [
      "Andreas Brorsson",
      "Markus Nordberg",
      "David Gustafsson"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Brorsson_Reconstruction_of_CASSI-Raman_Images_With_Machine-Learning_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Brorsson_Reconstruction_of_CASSI-Raman_Images_With_Machine-Learning_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Raman spectroscopy is a well-established method to detect small amounts of potentially dangerous substances. In a Coded Aperture Snapshot Spectral Imaging (CASSI) system spatial and spectral information are mixed resulting in an ensemble of compressed sensing measurements. A reconstruction method is applied to the Compressed Sensing (CS) measurement to reconstruct a hyperspectral cube containing the Raman spectra for the locations in the scene. Traditional reconstruction methods based on regularization such as Total Variation (TV) are time consuming which reduce the number of applications where the technology is applicable. A machine learning reconstruction approach using Convolutional Neural Network (CNN) is presented. The loss function for the CNN is a combination of reconstruction error and re-projection error of the reconstructed Raman spectra. Simulation of CS-measurements of samples containing different chemical substances and different concentration levels are reconstructed with high precision. The reconstruction time using the novel machine learning approach decreases several orders of magnitude. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_thermalimagesuper-resolutionchallenge-pbvs2021": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Thermal Image Super-Resolution Challenge - PBVS 2021",
    "authors": [
      "Rafael E. Rivadeneira",
      "Angel D. Sappa",
      "Boris X. Vintimilla",
      "Sabari Nathan",
      "Priya Kansal",
      "Armin Mehri",
      "Parichehr Behjati Ardakani",
      "Anurag Dalal",
      "Aparna Akula",
      "Darshika Sharma",
      "Shashwat Pandey",
      "Basant Kumar",
      "Jiaxin Yao",
      "Rongyuan Wu",
      "Kai Feng",
      "Ning Li",
      "Yongqiang Zhao",
      "Heena Patel",
      "Vishal Chudasama",
      "Kalpesh Prajapati",
      "Anjali Sarvaiya",
      "Kishor P. Upla",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Christoph Busch",
      "Feras Almasri",
      "Thomas Vandamme",
      "Olivier Debeir",
      "Nolan B. Gutierrez",
      "Quan H. Nguyen",
      "William J. Beksi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_-_PBVS_2021_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_-_PBVS_2021_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper presents results from the second Thermal Image Super-Resolution (TISR) challenge organized in the framework of the Perception Beyond the Visible Spectrum (PBVS) 2021 workshop. For this second edition, the same thermal image dataset considered during the first challenge has been used; only mid-resolution (MR) and high-resolution (HR) sets have been considered. The dataset consists of 951 training images and 50 testing images for each resolution. A set of 20 images for each resolution is kept aside for evaluation. The two evaluation methodologies proposed for the first challenge are also considered in this opportunity. The first evaluation task consists of measuring the PSNR and SSIM between the obtained SR image and the corresponding ground truth (i.e., the HR thermal image downsampled by four). The second evaluation also consists of measuring the PSNR and SSIM, but in this case, considers the x2 SR obtained from the given MR thermal image; this evaluation is performed between the SR image with respect to the semi-registered HR image, which has been acquired with another camera. The results outperformed those from the first challenge, thus showing an improvement in both evaluation metrics. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_deepfusionofappearanceandframedifferencingformotionsegmentation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Deep Fusion of Appearance and Frame Differencing for Motion Segmentation",
    "authors": [
      "Marc Ellenfeld",
      "Sebastian Moosbauer",
      "Ruben Cardenes",
      "Ulrich Klauck",
      "Michael Teutsch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Ellenfeld_Deep_Fusion_of_Appearance_and_Frame_Differencing_for_Motion_Segmentation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Ellenfeld_Deep_Fusion_of_Appearance_and_Frame_Differencing_for_Motion_Segmentation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Motion segmentation is a technique to detect and localize class-agnostic motion in videos. This motion is assumed to be relative to a stationary background and usually originates from objects such as vehicles or humans. When the camera moves, too, frame differencing approaches that do not have to model the stationary background over minutes, hours, or even days are more promising compared to background subtraction methods. In this paper, we propose a Deep Convolutional Neural Network (DCNN) for multi-modal motion segmentation: the current image contributes with appearance information to distinguish between relevant and irrelevant motion and frame differencing captures the temporal information, which is the scene's motion independent of the camera motion. We fuse this information to receive an effective and efficient approach for robust motion segmentation. The effectiveness is demonstrated using the multi-spectral CDNet-2014 dataset that we re-labeled for motion segmentation. We specifically show that we can detect tiny moving objects significantly better compared to methods based on optical flow. ",
    "code_link": "https://github.com/HensoldtOptronicsCV/MotionSegmentation"
  },
  "cvpr2021_pbvs_semanticlabelingoflidarpointcloudsforuavapplications": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Semantic Labeling of Lidar Point Clouds for UAV Applications",
    "authors": [
      "Maria Axelsson",
      "Max Holmberg",
      "Sabina Serra",
      "Hannes Ovren",
      "Michael Tulldahl"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Axelsson_Semantic_Labeling_of_Lidar_Point_Clouds_for_UAV_Applications_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Axelsson_Semantic_Labeling_of_Lidar_Point_Clouds_for_UAV_Applications_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Small Unmanned Aerial Vehicle (UAV) platforms equipped with compact laser scanners provides a low-cost option for many applications, including surveillance, mapping, and reconnaissance. For these applications, semantic segmentation or semantic labeling of each point in the lidar point cloud, is important for scene-understanding. In this work, we evaluate methods for semantic segmentation of three-dimensional (3D) point clouds of outdoor scenes measured with a laser scanner mounted on a small UAV. We compare the performance of four different semantic segmentation methods, which are all applied in a scan-by-scan fashion, on semi-sparse laser data from outdoor scenes. The best method achieves 95.3% on the three classes ground, vegetation, and vehicle in terms of mean intersection over union (mIoU) on a previously unseen scene from a different geographical area. The results demonstrate that it is possible to achieve good performance on the semantic segmentation task on data measured using a combination of a small UAV and a compact laser scanner. ",
    "code_link": ""
  },
  "cvpr2021_pbvs_channelsplitconvolutionalneuralnetwork(chasnet)forthermalimagesuper-resolution": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "PBVS",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Perception Beyond the Visible Spectrum",
    "title": "Channel Split Convolutional Neural Network (ChaSNet) for Thermal Image Super-Resolution",
    "authors": [
      "Kalpesh Prajapati",
      "Vishal Chudasama",
      "Heena Patel",
      "Anjali Sarvaiya",
      "Kishor P. Upla",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/html/Prajapati_Channel_Split_Convolutional_Neural_Network_ChaSNet_for_Thermal_Image_Super-Resolution_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/PBVS/papers/Prajapati_Channel_Split_Convolutional_Neural_Network_ChaSNet_for_Thermal_Image_Super-Resolution_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The ability of thermal sensors to penetrate smoke, mist, dust and aerosol makes them attractive for deployment in essential applications in military, medical, agricultural and animal ecology over the regular optical cameras. However, unlike optical imaging devices, the most sophisticated commercial thermal imaging sensor does not match the megapixel imaging ability. The Low-Resolution (LR) images from thermal sensors can be enhanced through a software-driven solution called Super-Resolution (SR). A number of works have been proposed to employ deep networks for SR task; however, they are overloaded with redundant features due to the deep architecture. This paper introduces a Channel Splitting-based Convolutional Neural Network (ChasNet) for thermal image SR eliminating the redundant features in the network. The use of channel splitting extracts the versatile features from Low-Resolution (LR) thermal image, helping to preserve high-frequency details in the SR images. We demonstrate the applicability proposed network for SR task in two different scenarios organized in the PBVS-2021 Thermal SR Challenge, consisting of noise elimination (Track-1) and domain shifting (Track-2). The efficacy is justified by comparing the SR results with other state-of-the-art thermal SR techniques in qualitative and quantitative metrics. A set of extensive experiments separately analyzes the importance of each block in the proposed architecture. The code of this work is also published online. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_srvfnetagenerativenetworkforunsupervisedmultiplediffeomorphicfunctionalalignment": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "SrvfNet: A Generative Network for Unsupervised Multiple Diffeomorphic Functional Alignment",
    "authors": [
      "Elvis Nunez",
      "Andrew Lizarraga",
      "Shantanu H. Joshi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Nunez_SrvfNet_A_Generative_Network_for_Unsupervised_Multiple_Diffeomorphic_Functional_Alignment_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Nunez_SrvfNet_A_Generative_Network_for_Unsupervised_Multiple_Diffeomorphic_Functional_Alignment_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present SrvfNet, a generative deep learning framework for the joint multiple alignment of large collections of functional data comprising square-root velocity functions (SRVF) to their templates. Our proposed framework is fully unsupervised and is capable of aligning to a predefined template as well as jointly predicting an optimal template from data while simultaneously achieving alignment. Our network is constructed as a generative encoder-decoder architecture comprising fully-connected layers capable of producing a distribution space of the warping functions. We demonstrate the strength of our framework by validating it on synthetic data as well as diffusion profiles from magnetic resonance imaging (MRI) data. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_unitingstereoanddepth-from-defocusathinlens-basedvariationalframeworkformultiviewreconstruction": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Uniting Stereo and Depth-From-Defocus: A Thin Lens-Based Variational Framework for Multiview Reconstruction",
    "authors": [
      "Robert D. Friedlander",
      "Huizong Yang",
      "Anthony J. Yezzi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Friedlander_Uniting_Stereo_and_Depth-From-Defocus_A_Thin_Lens-Based_Variational_Framework_for_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Friedlander_Uniting_Stereo_and_Depth-From-Defocus_A_Thin_Lens-Based_Variational_Framework_for_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " The problem of reconstructing three-dimensional (3D) scene geometry and radiometry from images is an important problem in computer vision and has applications in a variety of fields such as medicine and artifact preservation. However, state-of-the-art multiview algorithms assume a pinhole camera that incorrectly models defocus blur as a property of the scene instead of a property of the imaging process. We address the problem of dense 3D shape reconstruction from multiple viewpoints in situations where the image data exhibits noticeable defocus. We develop a mathematical framework for a fully generative variational algorithm that iteratively deforms an estimate of the foreground surface shapes and scene radiance such that irradiance estimates given by the thin lens forward model are photometrically consistent with the actual image data. This framework is founded on novel geometric computations of flux differentials across an evolving surface as well as gradients along occluding boundaries and their projections. While more work is needed to make them fit for practical use, the future potential of methods based on these computations is shown with experiments reconstructing simple object shapes from both synthetically generated and real defocused images. While our reconstruction algorithm has a higher computational cost than pinhole-based methods due to the more general optical model, it better reconstructs object proportions as well as sharp features that are blurred due to image defocus. As such, our geometry-based method provides a unified framework that extends the applicability of multiview reconstruction techniques to the poorly supported domain of defocused images where state-of-the-art pinhole-based methods fail. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_deepsphericalmanifoldgaussiankernelforunsuperviseddomainadaptation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Deep Spherical Manifold Gaussian Kernel for Unsupervised Domain Adaptation",
    "authors": [
      "Youshan Zhang",
      "Brian D. Davison"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Zhang_Deep_Spherical_Manifold_Gaussian_Kernel_for_Unsupervised_Domain_Adaptation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Zhang_Deep_Spherical_Manifold_Gaussian_Kernel_for_Unsupervised_Domain_Adaptation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Unsupervised Domain adaptation is an effective method in addressing the domain shift issue when transferring knowledge from an existing richly labeled domain to a new domain. Existing manifold-based methods either are based on traditional models or largely rely on Grassmannian manifold via minimizing differences of single covariance matrices of two domains. In addition, existing pseudo-labeling algorithms inadequately consider the quality of pseudo labels in aligning the conditional distribution between two domains. In this work, a deep spherical manifold Gaussian kernel (DSGK) framework is proposed to map the source and target subspaces into a spherical manifold and reduce the discrepancy between them by embedding both extracted features and a Gaussian kernel. To align the conditional distributions, we further develop an easy-to-hard pseudo label refinement process to improve the quality of the pseudo labels and then reduce categorical spherical manifold Gaussian kernel geodesic loss. Extensive experimental results show that DSGK outperforms state-of-the-art methods, especially on challenging cross-domain learning tasks. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_geometricempiricalbayesianmodelforclassificationoffunctionaldataunderdiversesamplingregimes": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Geometric Empirical Bayesian Model for Classification of Functional Data Under Diverse Sampling Regimes",
    "authors": [
      "James Matuk",
      "Karthik Bharath",
      "Oksana Chkrebtii",
      "Sebastian Kurtek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Matuk_Geometric_Empirical_Bayesian_Model_for_Classification_of_Functional_Data_Under_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Matuk_Geometric_Empirical_Bayesian_Model_for_Classification_of_Functional_Data_Under_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Functional data analysis (FDA) is focused on various statistical tasks, including inference, for observations that vary over a continuum, which are not effectively addressed by multivariate methods. A feature of these functional observations is the presence of two distinct forms of variability: amplitude that describes differences in magnitudes of features, e.g., extrema, and phase that describes differences in timings of amplitude features. One area of focus in FDA is the classification of new observations based on previously observed training data that has been split into predefined classes. Existing methods fail to directly account for both phase and amplitude variability, and work under the restrictive assumption that functional observations are measured on a common, fine grid over the input domain. In this work, we address these issues directly by formulating a Bayesian hierarchical model for irregular, fragmented or sparsely sampled functional observations, where training data from different classes are available. Our approach builds on a recently developed inferential framework for incomplete functional observations and the elastic FDA framework for characterizing amplitude and phase variability. The approach operates by inferring individual parameters that separately track amplitude and phase, which can be combined to infer complete functions underlying each observation, and a class parameter, which can be used to discern the class membership of an observation based on the training data. We validate the proposed framework using simulation studies and real data applications, and showcase the advantages of this perspective when both amplitude and phase variability are present in the data. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_gilda++grassmannincrementallineardiscriminantanalysis": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "GILDA++: Grassmann Incremental Linear Discriminant Analysis",
    "authors": [
      "Navya Nagananda",
      "Andreas Savakis"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Nagananda_GILDA_Grassmann_Incremental_Linear_Discriminant_Analysis_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Nagananda_GILDA_Grassmann_Incremental_Linear_Discriminant_Analysis_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Linear Discriminant Analysis (LDA) is an important supervised dimensionality reduction method. Traditional LDA makes use of the eigenvalue decomposition of the scatter matrices based on the entire dataset. However, in some settings, the whole dataset may not be available at once. Our approach considers an incremental LDA framework where the model receives training data in the form of chunks for subsequent analysis. We propose the Grassmann-Incremental Linear Discriminant Analysis (GILDA++) using the proxy matrix optimization method (PMO). The PMO method does not directly optimize a matrix on the manifold but uses an auxiliary or proxy matrix in ambient space which is retracted to the closest location on the manifold along the loss minimizing geodesic. PMO makes use of an LDA objective by incrementally updating the scatter matrices to handle chunks of data. It makes use of automatic differentiation and stochastic gradient descent to find the lower dimensional LDA projection matrix. GILDA++ is able to handle chunk data, where each chunk has new samples from existing classes or novel classes. Our experiments demonstrate that GILDA++ outperforms the prevailing incremental LDA methods in various datasets. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_multiscalediffeomorphicmetricmappingofspatialtranscriptomicsdatasets": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Multi Scale Diffeomorphic Metric Mapping of Spatial Transcriptomics Datasets",
    "authors": [
      "Michael I. Miller",
      "Jean Fan",
      "Daniel J. Tward"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Miller_Multi_Scale_Diffeomorphic_Metric_Mapping_of_Spatial_Transcriptomics_Datasets_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Miller_Multi_Scale_Diffeomorphic_Metric_Mapping_of_Spatial_Transcriptomics_Datasets_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Spatially resolved transcriptomic imaging is a family of promising new technologies that can produce a series of images that quantify gene expression at every pixel. These technologies, such as multiplex error-robust fluorescence in situ hybridization (MERFISH) which is the focus of this work, produce data that is inherently multi scale. They describe molecules at nanometer resolution, cell types at micron resolution, and tissue types at millimeter resolution. To harness the potential of these techniques, new mathematical and computational tools are required to quantify similarities and differences between images across experimental conditions. In this work we demonstrate the application of multi scale diffeomorphic metric mapping to MERFISH images. This recently developed framework uses varifold measures on reproducing kernel Hilbert spaces to describe shape and signal across spatial scales, and computes distances between samples in a Riemannian setting. Using experimental data from serial sections of the mouse preoptic hypothalamus, we use this technique to compute optimal nonrigid alignments between neighboring sections. This approach will ultimately be extended to 3D reconstruction and alignment to common coordinates of a brain atlas. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_learninglowbendingandlowdistortionmanifoldembeddings": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Learning Low Bending and Low Distortion Manifold Embeddings",
    "authors": [
      "Juliane Braunsmann",
      "Marko Rajkovic",
      "Martin Rumpf",
      "Benedikt Wirth"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Braunsmann_Learning_Low_Bending_and_Low_Distortion_Manifold_Embeddings_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Braunsmann_Learning_Low_Bending_and_Low_Distortion_Manifold_Embeddings_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Autoencoders are a widespread tool in machine learning to transform high-dimensional data into a lower-dimensional representation which still exhibits the essential characteristics of the input. The encoder provides an embedding from the input data manifold into a latent space which may then be used for further processing. For instance, learning interpolation on the manifold may be simplified via the new manifold representation in latent space. The efficiency of such further processing heavily depends on the regularity and structure of the embedding. In this article, the embedding into latent space is regularized via a loss function that promotes an as isometric and as flat embedding as possible. The required training data comprises pairs of nearby points on the input manifold together with their local distance and their local Frechet average. This regularity loss functional even allows to train the encoder on its own. The loss functional is computed via a Monte Carlo integration which is shown to be consistent with a geometric loss functional defined directly on the embedding map. Numerical tests are performed using image data that encodes different data manifolds. The results show that smooth manifold embeddings in latent space are obtained. These embeddings are regular enough such that interpolation between not too distant points on the manifold is well approximated by linear interpolation in latent space. ",
    "code_link": "https://github.com/deepmind/dsprites-dataset"
  },
  "cvpr2021_diffcvml_srvfregnetelasticfunctionregistrationusingdeepneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "SrvfRegNet: Elastic Function Registration Using Deep Neural Networks",
    "authors": [
      "Chao Chen",
      "Anuj Srivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Chen_SrvfRegNet_Elastic_Function_Registration_Using_Deep_Neural_Networks_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Chen_SrvfRegNet_Elastic_Function_Registration_Using_Deep_Neural_Networks_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Registering functions (curves) using time warpings (reparameterizations) is central to many computer vision and shape analysis solutions. While traditional registration methods minimize penalized-L2 norm, the elastic Riemannian metric and square-root velocity functions (SRVFs) have resulted in significant improvements in terms of theory and practical performance. This solution uses the dynamic programming algorithm to minimize the L2 norm between SRVFs of given functions. However, the computational cost of this elastic dynamic programming framework - O(nT2k) - where T is the number of time samples along a curve, n is the number of curves, and k < T is a parameter - limits its use in applications involving big data. This paper introduces a deep-learning approach, named SRVF Registration Net or SrvfRegNet to overcome these limitations. SrvfRegNet architecture trains by optimizing the elastic metric-based objective function on the training data and then applies this trained network to the test data to perform super-fast registration. In case the training and the test data are from different classes, it generalizes to the test data using transfer learning, i.e., retraining of only the last few layers. It achieves close to the state-of-the-art alignment performance but at much reduced computational cost. We demonstrate the efficiency and efficacy of this framework using several standard curve datasets ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_superviseddeeplearningofelasticsrvdistancesontheshapespaceofcurves": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Supervised Deep Learning of Elastic SRV Distances on the Shape Space of Curves",
    "authors": [
      "Emmanuel Hartman",
      "Yashil Sukurdeep",
      "Nicolas Charon",
      "Eric Klassen",
      "Martin Bauer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Hartman_Supervised_Deep_Learning_of_Elastic_SRV_Distances_on_the_Shape_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Hartman_Supervised_Deep_Learning_of_Elastic_SRV_Distances_on_the_Shape_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Motivated by applications from computer vision to bioinformatics, the field of shape analysis deals with problems where one wants to analyze geometric objects, such as curves, while ignoring actions that preserve their shape, such as translations, rotations, scalings, or reparametrizations. Mathematical tools have been developed to define notions of distances, averages, and optimal deformations for geometric objects. One such framework, which has proven to be successful in many applications, is based on the square root velocity (SRV) transform, which allows one to define a computable distance between spatial curves regardless of how they are parametrized. This paper introduces a supervised deep learning framework for the direct computation of SRV distances between curves, which usually requires an optimization over the group of reparametrizations that act on the curves. The benefits of our approach in terms of computational speed and accuracy are illustrated via several numerical experiments on both synthetic and real data. ",
    "code_link": ""
  },
  "cvpr2021_diffcvml_asheafandtopologyapproachtodetectinglocalmergingrelationsindigitalimages": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "DiffCVML",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Sheaf and Topology Approach to Detecting Local Merging Relations in Digital Images",
    "authors": [
      "Chuan-Shen Hu",
      "Yu-Min Chung"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/html/Hu_A_Sheaf_and_Topology_Approach_to_Detecting_Local_Merging_Relations_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/DiffCVML/papers/Hu_A_Sheaf_and_Topology_Approach_to_Detecting_Local_Merging_Relations_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper concerns a theoretical approach that combines topological data analysis (TDA) and sheaf theory. Topological data analysis, a rising field in mathematics and computer science, concerns the shape of the data and has been proven effective in many scientific disciplines. Sheaf theory, a mathematics subject in algebraic geometry, provides a framework for describing the local consistency in geometric objects. Persistent homology (PH) is one of the main driving forces in TDA, and the idea is to track changes in geometric objects at different scales. The persistence diagram (PD) summarizes the information of PH in the form of a multi-set. While PD provides useful information about the underlying objects, it lacks fine relations about the local consistency of specific pairs of generators in PD, such as the merging relation between two connected components in the PH. The sheaf structure provides a novel point of view for describing the merging relation of local objects in PH. It is the goal of this paper to establish a theoretic framework that utilizes the sheaf theory to uncover finer information from the PH. We also show that the proposed theory can be applied to identify the merging relations of local objects in digital images. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_lol-v2tlarge-scaleesportsvideodescriptiondataset": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "LoL-V2T: Large-Scale Esports Video Description Dataset",
    "authors": [
      "Tsunehiko Tanaka",
      "Edgar Simo-Serra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Tanaka_LoL-V2T_Large-Scale_Esports_Video_Description_Dataset_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Tanaka_LoL-V2T_Large-Scale_Esports_Video_Description_Dataset_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Esports is a fastest-growing new field with a largely online-presence, and is creating a demand for automatic domain-specific captioning tools. However, at the current time, there are few approaches that tackle the esports video description problem. In this work, we propose a large-scale dataset for esports video description, focusing on the popular game \"League of Legends\". The dataset, which we call LoL-V2T, is the largest video description dataset in the video game domain, and includes 9,723 clips with 62,677 captions. This new dataset presents multiple new video captioning challenges such as large amounts of domain-specific vocabulary, subtle motions with large importance, and a temporal gap between most captions and the events that occurred. In order to tackle the issue of vocabulary, we propose a masking the domain-specific words and provide additional annotations for this. In our results, we show that the dataset poses a challenge to existing video captioning approaches, and the masking can significantly improve performance. Our dataset and code is publicly available. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_contrastivelearningforsportsvideounsupervisedplayerclassification": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Contrastive Learning for Sports Video: Unsupervised Player Classification",
    "authors": [
      "Maria Koshkina",
      "Hemanth Pidaparthy",
      "James H. Elder"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Koshkina_Contrastive_Learning_for_Sports_Video_Unsupervised_Player_Classification_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Koshkina_Contrastive_Learning_for_Sports_Video_Unsupervised_Player_Classification_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We address the problem of unsupervised classification of players in a team sport according to their team affiliation, when jersey colours and design are not known a priori. We adopt a contrastive learning approach in which an embedding network learns to maximize the distance between representations of players on different teams relative to players on the same team, in a purely unsupervised fashion, without any labelled data. We evaluate the approach using a new hockey dataset and find that it outperforms prior unsupervised approaches by a substantial margin, particularly for real-time application when only a small number of frames are available for unsupervised learning before team assignments must be made. Remarkably, we show that our contrastive method achieves 94% accuracy after unsupervised training on only a single frame, with accuracy rising to 97% within 500 frames (17 seconds of game time). We further demonstrate how accurate team classification allows accurate team-conditional heat maps of player positioning to be computed. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_automatedtackleinjuryriskassessmentincontact-basedsports-arugbyunionexample": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Automated Tackle Injury Risk Assessment in Contact-Based Sports - A Rugby Union Example",
    "authors": [
      "Zubair Martin",
      "Sharief Hendricks",
      "Amir Patel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Martin_Automated_Tackle_Injury_Risk_Assessment_in_Contact-Based_Sports_-_A_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Martin_Automated_Tackle_Injury_Risk_Assessment_in_Contact-Based_Sports_-_A_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Video analysis in tackle-collision based sports is highly subjective and exposed to bias, which is inherent in human observation, especially under time constraints. This limitation of match analysis in tackle-collision based sports can be seen as an opportunity for computer vision applications. Objectively tracking, detecting and recognising an athlete's movements and actions during match play from a distance using video, along with our improved understanding of injury aetiology and skill execution will enhance our understanding how injury occurs, assist match day injury management, reduce referee subjectivity. In this paper, we present a system of objectively evaluating in-game tackle risk in rugby union matches. First, a ball detection model is trained using the You Only Look Once (YOLO) framework, these detections are then tracked by a Kalman Filter. Following this, a separate YOLO model is used to detect persons/players within a tackle segment and then the ball-carrier and tackler are identified. Subsequently, we utilize OpenPose to determine the pose of ball-carrier and tackle, the relative pose of these is then used to evaluate the risk of the tackle. We tested the system on a diverse collection of rugby tackles and achieved an evaluation accuracy of 62.50%. These results will enable referees in tackle-contact based sports to make more subjective decisions, ultimately making these sports safer. ",
    "code_link": "https://github.com/AlexeyAB/darknet"
  },
  "cvpr2021_cvsports_towardimprovingthevisualcharacterizationofsportactivitieswithabstractedscenegraphs": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Toward Improving the Visual Characterization of Sport Activities With Abstracted Scene Graphs",
    "authors": [
      "Amir M. Rahimi",
      "Kevin Lee",
      "Amit Agarwal",
      "Hyukseong Kwon",
      "Rajan Bhattacharyya"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Rahimi_Toward_Improving_the_Visual_Characterization_of_Sport_Activities_With_Abstracted_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Rahimi_Toward_Improving_the_Visual_Characterization_of_Sport_Activities_With_Abstracted_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We present techniques for abstracting relevant information from scene graph features to improve action recognition in sports videos. Feature representation with relevant information can dramatically increase machine learning's utility across many tasks. Despite the advantages of incorporating objects and relations as building blocks of semantic information, we still encounter too many irrelevant objects and relations in sports videos, adding uncertainty to the classifiers. This paper describes four fundamentally different scene abstraction techniques, each searching for the relevant information within aggregated features from pixel-level to object-level. In each method, we formulate relevancy through co-occurrence statistics, semantic similarity, feature decomposition, and correlation-based mapping and evaluate each technique's efficacy through performance gains in action recognition and decay rate of training loss. We demonstrate that by creating a relevant and more concise knowledge representation, we improve performance (mAP) of action recognition in sports by 26.6% and achieve faster converging models due to higher representation power. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_detectingandmatchingrelatedobjectswithoneproposalmultiplepredictions": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Detecting and Matching Related Objects With One Proposal Multiple Predictions",
    "authors": [
      "Yang Liu",
      "Luiz G. Hafemann",
      "Michael Jamieson",
      "Mehrsan Javan"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Liu_Detecting_and_Matching_Related_Objects_With_One_Proposal_Multiple_Predictions_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Liu_Detecting_and_Matching_Related_Objects_With_One_Proposal_Multiple_Predictions_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Tracking players in sports videos is commonly done in a tracking-by-detection framework, first detecting players in each frame, and then performing association over time. While for some sports tracking players is sufficient for game analysis, sports like hockey, tennis and polo may require additional detections, that include the object the player is holding (e.g. racket, stick). The baseline solution for this problem involves detecting these objects as separate classes, and matching them to player detections based on the intersection over union (IoU). This approach, however, leads to poor matching performance in crowded situations, as it does not model the relationship between players and objects. In this paper, we propose a simple yet efficient way to detect and match players and related objects at once without extra cost, by considering an implicit association for prediction of multiple objects through the same proposal box. We evaluate the method on a dataset of broadcast ice hockey videos, and also a new public dataset we introduce called COCO +Torso. On the ice hockey dataset, the proposed method boosts matching performance from 57.1% to 81.4%, while also improving the meanAP of player+stick detections from 68.4% to 88.3%. On the COCO +Torso dataset, we see matching improving from 47.9% to 65.2%. The COCO +Torso dataset, code and pre-trained models will be released at https://github.com/foreverYoungGitHub/detect-and-match-related-objects. ",
    "code_link": "https://github.com/foreverYoungGitHub/detectand-match-related-objects"
  },
  "cvpr2021_cvsports_soccernet-v2adatasetandbenchmarksforholisticunderstandingofbroadcastsoccervideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos",
    "authors": [
      "Adrien Deliege",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Meisam J. Seikavandi",
      "Jacob V. Dueholm",
      "Kamal Nasrollahi",
      "Bernard Ghanem",
      "Thomas B. Moeslund",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Deliege_SoccerNet-v2_A_Dataset_and_Benchmarks_for_Holistic_Understanding_of_Broadcast_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Deliege_SoccerNet-v2_A_Dataset_and_Benchmarks_for_Holistic_Understanding_of_Broadcast_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Understanding broadcast videos is a challenging task in computer vision, as it requires generic reasoning capabilities to appreciate the content offered by the video editing. In this work, we propose SoccerNet-v2, a novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production. Specifically, we release around 300k annotations within SoccerNet's 500 untrimmed broadcast soccer videos. We extend current tasks in the realm of soccer to include action spotting, camera shot segmentation with boundary detection, and we define a novel replay grounding task. For each task, we provide and discuss benchmark results, reproducible with our open-source adapted implementations of the most relevant works in the field. SoccerNet-v2 is presented to the broader research community to help push computer vision closer to automatic solutions for more general video understanding and production purposes. ",
    "code_link": "https://github.com/scikitvideo/scikit-video"
  },
  "cvpr2021_cvsports_tabletennisstrokerecognitionusingtwo-dimensionalhumanposeestimation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Table Tennis Stroke Recognition Using Two-Dimensional Human Pose Estimation",
    "authors": [
      "Kaustubh Milind Kulkarni",
      "Sucheth Shenoy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Kulkarni_Table_Tennis_Stroke_Recognition_Using_Two-Dimensional_Human_Pose_Estimation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Kulkarni_Table_Tennis_Stroke_Recognition_Using_Two-Dimensional_Human_Pose_Estimation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We introduce a novel method for collecting table tennis video data and perform stroke detection and classification. A diverse dataset containing video data of 11 basic strokes obtained from 14 professional table tennis players, summing up to a total of 22111 videos has been collected using the proposed setup. The temporal convolutional neural network model developed using 2D pose estimation performs multiclass classification of these 11 table tennis strokes with a validation accuracy of 99.37%. Moreover, the neural network generalizes well over the data of a player excluded from the training and validation dataset, classifying the fresh strokes with an overall best accuracy of 98.72%. Various model architectures using machine learning and deep learning based approaches have been trained for stroke recognition and their performances have been compared and benchmarked. Inferences such as performance monitoring and stroke comparison of the players using the model have been discussed. Therefore, we are contributing to the development of a computer vision based sports analytics system for the sport of table tennis that focuses on the previously unexploited aspect of the sport i.e., a player's strokes, which is extremely insightful for performance improvement. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_pucklocalizationandmulti-taskeventrecognitioninbroadcasthockeyvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Puck Localization and Multi-Task Event Recognition in Broadcast Hockey Videos",
    "authors": [
      "Kanav Vats",
      "Mehrnaz Fani",
      "David A. Clausi",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Vats_Puck_Localization_and_Multi-Task_Event_Recognition_in_Broadcast_Hockey_Videos_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Vats_Puck_Localization_and_Multi-Task_Event_Recognition_in_Broadcast_Hockey_Videos_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Puck localization is an important problem in ice hockey video analytics useful for analyzing the game, determining play location, and assessing puck possession. The problem is challenging due to the small size of the puck, excessive motion blur due to high puck velocity, and occlusions due to players and boards. In this paper, we introduce and implement a network for puck localization in broadcast hockey video. The network leverages expert NHL play-by-play annotations and uses temporal context to locate the puck. Player locations are incorporated into the network through an attention mechanism by encoding player positions with a Gaussian-based spatial heatmap drawn at player positions. Since event occurrence on the rink and puck location are related, we also perform event recognition by augmenting the puck localization network with an event recognition head and training the network through multi-task learning. Experimental results demonstrate that the network is able to localize the puck with an AUC of 73.1 % on the test set. The puck location can be inferred in 720p broadcast videos at 5 frames per second. It is also demonstrated that multi-task learning with puck location improves event recognition accuracy. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_temporally-awarefeaturepoolingforactionspottinginsoccerbroadcasts": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Temporally-Aware Feature Pooling for Action Spotting in Soccer Broadcasts",
    "authors": [
      "Silvio Giancola",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Giancola_Temporally-Aware_Feature_Pooling_for_Action_Spotting_in_Soccer_Broadcasts_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Giancola_Temporally-Aware_Feature_Pooling_for_Action_Spotting_in_Soccer_Broadcasts_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Toward the goal of automatic production for sports broadcasts, a paramount task consists in understanding the high-level semantic information of the game in play. For instance, recognizing and localizing the main actions of the game would allow producers to adapt and automatize the broadcast production, focusing on the important details of the game and maximizing the spectator engagement. In this paper, we focus our analysis on action spotting in soccer broadcast, which consists in temporally localizing the main actions in a soccer game. To that end, we propose a novel feature pooling method based on NetVLAD, dubbed NetVLAD++, that embeds temporally-aware knowledge. Different from previous pooling methods that consider the temporal context as a single set to pool from, we split the context before and after an action occurs. We argue that considering the contextual information around the action spot as a single entity leads to a sub-optimal learning for the pooling module. With NetVLAD++, we disentangle the context from the past and future frames and learn specific vocabularies of semantics for each subsets, avoiding to blend and blur such vocabulary in time. Injecting such prior knowledge creates more informative pooling modules and more discriminative pooled features, leading into a better understanding of the actions. We train and evaluate our methodology on the recent large-scale dataset SoccerNet-v2, reaching 53.4% Average-mAP for action spotting, a +12.7% improvement w.r.t the current state-of-the-art. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_automaticplaysegmentationofhockeyvideos": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Automatic Play Segmentation of Hockey Videos",
    "authors": [
      "Hemanth Pidaparthy",
      "Michael H. Dowling",
      "James H. Elder"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Pidaparthy_Automatic_Play_Segmentation_of_Hockey_Videos_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Pidaparthy_Automatic_Play_Segmentation_of_Hockey_Videos_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Most team sports such as hockey involve periods of active play interleaved with breaks in play. When watching a game remotely, many fans would prefer an abbreviated game showing only periods of active play. Here we address the problem of identifying these periods in order to produce a time-compressed viewing experience. Our approach is based on a hidden Markov model of play state driven by deep visual and optional auditory cues. We find that our deep visual cues generalize well across different cameras and that auditory cues can improve performance but only if unsupervised methods are used to adapt emission distributions to domain shift across games. Our system achieves temporal compression rates of 20-50% at a recall of 96%. ",
    "code_link": ""
  },
  "cvpr2021_cvsports_deepdartsmodelingkeypointsasobjectsforautomaticscorekeepingindartsusingasinglecamera": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts Using a Single Camera",
    "authors": [
      "William McNally",
      "Pascale Walters",
      "Kanav Vats",
      "Alexander Wong",
      "John McPhee"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/McNally_DeepDarts_Modeling_Keypoints_as_Objects_for_Automatic_Scorekeeping_in_Darts_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/McNally_DeepDarts_Modeling_Keypoints_as_Objects_for_Automatic_Scorekeeping_in_Darts_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Existing multi-camera solutions for automatic scorekeeping in steel-tip darts are very expensive and thus inaccessible to most players. Motivated to develop a more accessible low-cost solution, we present a new approach to keypoint detection and apply it to predict dart scores from a single image taken from any camera angle. This problem involves detecting multiple keypoints that may be of the same class and positioned in close proximity to one another. The widely adopted framework for regressing keypoints using heatmaps is not well-suited for this task. To address this issue, we instead propose to model keypoints as objects. We develop a deep convolutional neural network around this idea and use it to predict dart locations and dartboard calibration points within an overall pipeline for automatic dart scoring, which we call DeepDarts. Additionally, we propose several task-specific data augmentation strategies to improve the generalization of our method. As a proof of concept, two datasets comprising 16k images originating from two different dartboard setups were manually collected and annotated to evaluate the system. In the primary dataset containing 15k images captured from a face-on view of the dartboard using a smartphone, DeepDarts predicted the total score correctly in 94.7% of the test images. In a second more challenging dataset containing limited training data (830 images) and various camera angles, we utilize transfer learning and extensive data augmentation to achieve a test accuracy of 84.0%. Because DeepDarts relies only on single images, it has the potential to be deployed on edge devices, giving anyone with a smartphone access to an automatic dart scoring system for steel-tip darts. The code and datasets are available. ",
    "code_link": "https://github.com/wmcnally/deep-darts"
  },
  "cvpr2021_cvsports_cameracalibrationandplayerlocalizationinsoccernet-v2andinvestigationoftheirrepresentationsforactionspotting": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "CVSports",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Computer Vision in Sports",
    "title": "Camera Calibration and Player Localization in SoccerNet-v2 and Investigation of Their Representations for Action Spotting",
    "authors": [
      "Anthony Cioppa",
      "Adrien Deliege",
      "Floriane Magera",
      "Silvio Giancola",
      "Olivier Barnich",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/html/Cioppa_Camera_Calibration_and_Player_Localization_in_SoccerNet-v2_and_Investigation_of_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/CVSports/papers/Cioppa_Camera_Calibration_and_Player_Localization_in_SoccerNet-v2_and_Investigation_of_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Soccer broadcast video understanding has been drawing a lot of attention in recent years within data scientists and industrial companies. This is mainly due to the lucrative potential unlocked by effective deep learning techniques developed in the field of computer vision. In this work, we focus on the topic of camera calibration and on its current limitations for the scientific community. More precisely, we tackle the absence of a large-scale calibration dataset and of a public calibration network trained on such a dataset. Specifically, we distill a powerful commercial calibration tool in a recent neural network architecture on the large-scale SoccerNet dataset, composed of untrimmed broadcast videos of 500 soccer games. We further release our distilled network, and leverage it to provide 3 ways of representing the calibration results along with player localization. Finally, we exploit those representations within the current best architecture for the action spotting task of SoccerNet-v2, and achieve new state-of-the-art performances. ",
    "code_link": ""
  },
  "cvpr2021_bivision_adaptivebinary-ternaryquantization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "Adaptive Binary-Ternary Quantization",
    "authors": [
      "Ryan Razani",
      "Gregoire Morin",
      "Eyyub Sari",
      "Vahid Partovi Nia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Razani_Adaptive_Binary-Ternary_Quantization_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Razani_Adaptive_Binary-Ternary_Quantization_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Neural network models are resource hungry. It is difficult to deploy such deep networks on devices with limited resources, like smart wearables, cellphones, drones, and autonomous vehicles. Low bit quantization such as binary and ternary quantization is a common approach to alleviate this resource requirements. Ternary quantization provides a more flexible model and outperforms binary quantization in terms of accuracy, however doubles the memory footprint and increases the computational cost. Contrary to these approaches, mixed quantized models allow a trade-off between accuracy and memory footprint. In such models, quantization depth is often chosen manually, or is tuned using a separate optimization routine. The latter requires training a quantized network multiple times. Here, we propose an adaptive combination of binary and ternary quantization, namely Smart Quantization (SQ), in which the quantization depth is modified directly via a regularization function, so that the model is trained only once. Our experimental results show that the proposed method adapts quantization depth successfully while keeping the model accuracy high on MNIST and CIFAR10 benchmarks. ",
    "code_link": ""
  },
  "cvpr2021_bivision_ontheapplicationofbinaryneuralnetworksinobliviousinference": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "On the Application of Binary Neural Networks in Oblivious Inference",
    "authors": [
      "Mohammad Samragh",
      "Siam Hussain",
      "Xinqiao Zhang",
      "Ke Huang",
      "Farinaz Koushanfar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Samragh_On_the_Application_of_Binary_Neural_Networks_in_Oblivious_Inference_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Samragh_On_the_Application_of_Binary_Neural_Networks_in_Oblivious_Inference_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper explores the application of Binary Neural Networks (BNN) in oblivious inference, a service provided by a server to mistrusting clients. Using this service, a client can obtain the inference result on her data by a trained model held by the server without disclosing the data or leaning the model parameters. We make two contributions to this field. First, we devise light-weight cryptographic protocols designed specifically to exploit the unique characteristics of BNNs. Second, we present dynamic exploration of the runtime-accuracy tradeoff of BNNs in a single-shot training process. While previous works trained multiple BNNs with different computational complexities (which is cumbersome due to the slow convergence of BNNs), we train a single BNN that can perform inference under different computational budgets. Compared to CryptFlow2, the state-of-the-art in oblivious inference of non-binary DNNs, our approach reaches 2x faster inference at the same accuracy. Compared to XONN, the state-of-the-art in oblivious inference of binary networks, we achieve 2x to 11x faster inference while obtaining higher accuracy. ",
    "code_link": ""
  },
  "cvpr2021_bivision_trainingdynamicalbinaryneuralnetworkswithequilibriumpropagation": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "Training Dynamical Binary Neural Networks With Equilibrium Propagation",
    "authors": [
      "Jeremie Laydevant",
      "Maxence Ernoult",
      "Damien Querlioz",
      "Julie Grollier"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Laydevant_Training_Dynamical_Binary_Neural_Networks_With_Equilibrium_Propagation_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Laydevant_Training_Dynamical_Binary_Neural_Networks_With_Equilibrium_Propagation_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Equilibrium Propagation (EP) is an algorithm intrinsically adapted to the training of physical networks, thanks to the local updates of weights given by the internal dynamics of the system. However, the construction of such a hardware requires to make the algorithm compatible with the existing neuromorphic CMOS technology, which generally exploits digital communication between neurons and offers a limited amount of local memory. In this work, we demonstrate that EP can train dynamical networks with binary activations and weights. We first train systems with binary weights and full-precision activations, achieving an accuracy equivalent to that of full-precision models trained by standard EP on MNIST, and losing only 1.9% accuracy on CIFAR-10 with equal architecture. We then extend our method to the training of models with binary activations and weights on MNIST, achieving an accuracy within 1% of the full-precision reference for fully connected architectures and reaching the full-precision reference accuracy for the convolutional architecture. Our extension of EP to binary networks opens new solutions for on-chip learning and provides a compact framework for training BNNs end-to-end with the same circuitry as for inference. ",
    "code_link": ""
  },
  "cvpr2021_bivision_fastwalsh-hadamardtransformandsmooth-thresholdingbasedbinarylayersindeepneuralnetworks": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks",
    "authors": [
      "Hongyi Pan",
      "Diaa Badawi",
      "Ahmet Enis Cetin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Pan_Fast_Walsh-Hadamard_Transform_and_Smooth-Thresholding_Based_Binary_Layers_in_Deep_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Pan_Fast_Walsh-Hadamard_Transform_and_Smooth-Thresholding_Based_Binary_Layers_in_Deep_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " In this paper, we propose a novel layer based on fast Walsh-Hadamard transform (WHT) and smooth-thresholding to replace 1x1 convolution layers in deep neural networks. In the WHT domain, we denoise the transform domain coefficients using the new smooth-thresholding non-linearity, a smoothed version of the well-known soft-thresholding operator. We also introduce a family of multiplication-free operators from the basic 2x2 Hadamard transform to implement 3x3 depthwise separable convolution layers. Using these two types of layers, we replace the bottleneck layers in MobileNet-V2 to reduce the network's number of parameters with a slight loss in accuracy. For example, by replacing the final third bottleneck layers, we reduce the number of parameters from 2.270M to 947K. This reduces the accuracy from 95.21% to 92.88% on the CIFAR-10 dataset. Our approach significantly improves the speed of data processing. The fast Walsh-Hadamard transform has a computational complexity of O(m\\log_2 m). As a result, it is computationally more efficient than the 1x1 convolution layer. The fast Walsh-Hadamard layer processes a tensor in \\mathbb R ^ 10x32x32x1024about 2 times faster than 1x1 convolution layer on NVIDIA Jetson Nano computer board. ",
    "code_link": ""
  },
  "cvpr2021_bivision_initializationandtransferlearningofstochasticbinarynetworksfromreal-valuedones": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "Initialization and Transfer Learning of Stochastic Binary Networks From Real-Valued Ones",
    "authors": [
      "Anastasiia Livochka",
      "Alexander Shekhovtsov"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Livochka_Initialization_and_Transfer_Learning_of_Stochastic_Binary_Networks_From_Real-Valued_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Livochka_Initialization_and_Transfer_Learning_of_Stochastic_Binary_Networks_From_Real-Valued_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " We consider the training of binary neural networks (BNNs) using the stochastic relaxation approach, which leads to stochastic binary networks (SBNs). We identify that a severe obstacle to training deep SBNs without skip connections is already the initialization phase. While smaller models can be trained from a random (possibly data-driven) initialization, for deeper models and large datasets, it becomes increasingly difficult to obtain non-vanishing and low variance gradients when initializing randomly. In this work, we initialize SBNs from real-valued networks with ReLU activations. Real valued networks are well established, easier to train and benefit from many techniques to improve their generalization properties. We propose that closely approximating their internal features can provide a good initialization for SBN. We transfer features incrementally, layer-by-layer, accounting for noises in the SBN, exploiting equivalent reparametrizations of ReLU networks and using a novel transfer loss formulation. We demonstrate experimentally that with the proposed initialization, binary networks can be trained faster and achieve a higher accuracy than when initialized randomly. ",
    "code_link": ""
  },
  "cvpr2021_bivision_bnn-bn=?trainingbinaryneuralnetworkswithoutbatchnormalization": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "\"BNN - BN = ?\": Training Binary Neural Networks Without Batch Normalization",
    "authors": [
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Xu Ouyang",
      "Zechun Liu",
      "Zhiqiang Shen",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Chen_BNN_-_BN___Training_Binary_Neural_Networks_Without_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Chen_BNN_-_BN___Training_Binary_Neural_Networks_Without_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " Batch normalization (BN) is a key facilitator and considered essential for state-of-the-art binary neural networks (BNN). However, the BN layer is costly to calculate and is typically implemented with non-binary parameters, leaving a hurdle for the efficient implementation of BNN training. It also introduces undesirable dependence between samples within each batch. Inspired by the latest advance on Batch Normalization Free (BN-Free) training, we extend their framework to training BNNs, and for the first time demonstrate that BNs can be completely removed from BNN training and inference regimes. By plugging in and customizing techniques including adaptive gradient clipping, scale weight standardization, and specialized bottleneck block, a BN-free BNN is capable of maintaining competitive accuracy compared to its BN-based counterpart. Extensive experiments validate the effectiveness of our proposal across diverse BNN backbones and datasets. For example, after removing BNs from the state-of-the-art ReActNets, it can still be trained with our proposed methodology to achieve 92.08%, 68.34%, and 68.0% accuracy on CIFAR-10, CIFAR-100, and ImageNet respectively, with marginal performance drop (0.23% 0.44% on CIFAR and 1.40% on ImageNet). Codes and pre-trained models are available at: https://github.com/VITA-Group/BNN_NoBN. ",
    "code_link": "https://github.com/VITA-Group/BNN_NoBN"
  },
  "cvpr2021_bivision_bcnnabinarycnnwithallmatrixopsquantizedto1bitprecision": {
    "conf_id": "CVPR2021",
    "conf_sub_id": "BiVision",
    "is_workshop": true,
    "conf_name": "CVPR2021_workshops - Binary Networks for Computer Vision",
    "title": "BCNN: A Binary CNN With All Matrix Ops Quantized to 1 Bit Precision",
    "authors": [
      "Arthur J. Redfern",
      "Lijun Zhu",
      "Molly K. Newquist"
    ],
    "page_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/html/Redfern_BCNN_A_Binary_CNN_With_All_Matrix_Ops_Quantized_to_CVPRW_2021_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/CVPR2021W/BiVision/papers/Redfern_BCNN_A_Binary_CNN_With_All_Matrix_Ops_Quantized_to_CVPRW_2021_paper.pdf",
    "published": "2021-06",
    "summary": " This paper describes a CNN where all CNN style 2D convolution operations that lower to matrix matrix multiplication are fully binary. The network is derived from a common building block structure that is consistent with a constructive proof outline showing that binary neural networks are universal function approximators. 71.24% top 1 accuracy on the 2012 ImageNet validation set was achieved with a 2 step training procedure and implementation strategies optimized for binary operands are provided. ",
    "code_link": ""
  }
}