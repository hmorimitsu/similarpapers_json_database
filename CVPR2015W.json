{
  "cvpr2015_w1_fromphotographytomicrobiologyeigenbiomemodelsforskinappearance": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - BioImage Computing Workshop",
    "title": "From Photography to Microbiology: Eigenbiome Models for Skin Appearance",
    "authors": [
      "Parneet Kaur",
      "Kristin J. Dana",
      "Gabriela Oana Cula"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W01/html/Kaur_From_Photography_to_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W01/papers/Kaur_From_Photography_to_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Skin appearance modeling using high resolution photography has led to advances in recognition, rendering and analysis. Computational appearance provides an exciting new opportunity for integrating macroscopic imaging and microscopic biology. Recent studies indicate that skin appearance is dependent on the unseen distribution of microbes on the skin surface, i.e. the skin microbiome. While modern sequencing methods can be used to identify microbes, these methods are costly and time-consuming. We develop a computational skin texture model to characterize image-based patterns and link them to underlying microbiome clusters. The pattern analysis uses ultraviolet and blue fluorescence multimodal skin photography. The intersection of appearance and microbiome clusters reveals a pattern of microbiome that is predictable with high accuracy based on skin appearance. Furthermore, the use of non-negative matrix factorization allows a representation of the microbiome eigenvector as a physically plausible positive distribution of bacterial components. In this paper, we present the first results in this area of predicting microbiome clusters based on computational skin texture.",
    "code_link": ""
  },
  "cvpr2015_w1_fastregistrationofsegmentedimagesbynormalsampling": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - BioImage Computing Workshop",
    "title": "Fast Registration of Segmented Images by Normal Sampling",
    "authors": [
      "Jan Kybic",
      "Martin Dolejsi",
      "Jiri Borovec"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W01/html/Kybic_Fast_Registration_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W01/papers/Kybic_Fast_Registration_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " It is known that image registration is mostly driven by image edges. We have taken this idea to the extreme. In segmented images, we ignore the interior of the components and focus on their boundaries only. Furthermore, by assuming spatial compactness of the components, the similarity criterion can be approximated by sampling only a small number of points on the normals passing through a sparse set of keypoints. This leads to an order-of-magnitude speed advantage in comparison with classical registration algorithms. Surprisingly, despite the crude approximation, the accuracy is comparable. By virtue of the segmentation and by using a suitable similarity criterion such as mutual information on labels, the method can handle large appearance differences and large variability in the segmentations. The segmentation does not need not be perfectly coherent between images and over-segmentation is acceptable.We demonstrate the performance of the method on a range of different datasets, including histological slices and Drosophila imaginal discs, using rigid transformations. ",
    "code_link": ""
  },
  "cvpr2015_w1_deepneuralnetworksforanatomicalbrainsegmentation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W1",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - BioImage Computing Workshop",
    "title": "Deep Neural Networks for Anatomical Brain Segmentation",
    "authors": [
      "Alexander de Brebisson",
      "Giovanni Montana"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W01/html/Brebisson_Deep_Neural_Networks_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W01/papers/Brebisson_Deep_Neural_Networks_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We present a novel approach to automatically segment magnetic resonance (MR) images of the human brain into anatomical regions. Our methodology is based on a deep artificial neural network that assigns each voxel in an MR image of the brain to its corresponding anatomical region. The inputs of the network capture information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches capture a local spatial context while large compressed 2D orthogonal patches and distances to the regional centroids enforce global spatial consistency. Contrary to commonly used segmentation methods, our technique does not require any non-linear registration of the MR images. To benchmark our model, we used the dataset provided for the MICCAI 2012 challenge on multi-atlas labelling, which consists of 35 manually segmented MR images of the brain. We obtained competitive results (mean dice coefficient 0.725, error rate 0.163) showing the potential of our approach. To our knowledge, our technique is the first to tackle the anatomical segmentation of the whole brain using deep neural networks.",
    "code_link": "https://github.com/adbrebs/brain"
  },
  "cvpr2015_w2_taefacross-distance/environmentfacerecognitionmethod": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "TAEF: A Cross-Distance/Environment Face Recognition Method",
    "authors": [
      "Chun-Ting Huang",
      "Zhengning Wang",
      "C.-C. Jay Kuo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Huang_TAEF_A_Cross-DistanceEnvironment_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Huang_TAEF_A_Cross-DistanceEnvironment_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " A solution to long distance outdoor face recognition is presented in this work. The proposed method, called the Two-Stage Alignment/Enhancement Filtering (TAEF) system, consists of three main components: a cross-distance face alignment technique, a cross-environment face enhancement technique, and a two-stage filtering system. Given a probe image, the procedure of face alignment, enhancement and matching is executed against all gallery images to eliminate unlikely candidates at once at the first stage for efficiency. Then, the procedure is conducted for every individual probe/gallery image pair for higher accuracy at the second stage. The first rank recognition rates of the TAEF method are 100\\%, 100\\% and 97\\% for 60-, 100- and 150-meter visible-light images in the LDHF database, respectively. ",
    "code_link": ""
  },
  "cvpr2015_w2_perspectivedistortionmodeling,learningandcompensation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Perspective Distortion Modeling, Learning and Compensation",
    "authors": [
      "Joachim Valente",
      "Stefano Soatto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Valente_Perspective_Distortion_Modeling_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Valente_Perspective_Distortion_Modeling_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We describe a method to model perspective distortion as a one-parameter family of warping functions. This can be used to mitigate its effects on face recognition, or in synthesis to manipulate the perceived characteristics of a face. The warps are learned from a novel dataset and, by comparing one-parameter families of images, instead of images themselves, we improve performance of face recognition algorithms, most significantly when small focal lengths are used. Additional applications are presented to image editing, videoconference, and multi-view validation of recognition systems.",
    "code_link": ""
  },
  "cvpr2015_w2_locality-constraineddiscriminativelearningandcoding": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Locality-Constrained Discriminative Learning and Coding",
    "authors": [
      "Shuyang Wang",
      "Yun Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Wang_Locality-Constrained_Discriminative_Learning_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Wang_Locality-Constrained_Discriminative_Learning_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper explores the enhancement by locality constraint to both learning and coding schemes, more specifically, discriminative low-rank dictionary learning and auto-encoder. Previous Fisher discriminative based dictionary learning has led to interesting results by learning more discerning sub-dictionaries. Also, the low-rank regularization term has been introduced to take advantage of the global structure of the data. However, such methods fail to consider data's intrinsic manifold structure. To this end, first, we apply locality constraint on dictionary learning to explore whether the identification capability will be enhanced or not by using the geometric structure information. Moreover, inspired by the recent advances from auto-encoders for learning compact feature spaces, we propose a locality-constrained collaborative auto-encoder (LCAE) for feature extraction. The improvement from applying locality to dictionary learning and auto-encoder is evaluated on several datasets. Experimental results have demonstrated the effectiveness of locality information compared with state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2015_w2_apreliminaryinvestigationonthesensitivityofcotsfacerecognitionsystemstoforensicanalyst-stylefaceprocessingforocclusions": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "A Preliminary Investigation on the Sensitivity of COTS Face Recognition Systems to Forensic Analyst-Style Face Processing for Occlusions",
    "authors": [
      "Felix Juefei-Xu",
      "Dipan K. Pal",
      "Karanhaar Singh",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Juefei-Xu_A_Preliminary_Investigation_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Juefei-Xu_A_Preliminary_Investigation_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Modern day law enforcement banks heavily on the use of commercial off-the-shelf (COTS) face recognition systems (FRS) as a tool for biometric evaluation and identification. However, in many real-world scenarios, when the face of an individual is occluded or degraded in some way, commercial recognition systems fail to accept the face for evaluation or simply return unusable matched faces. In these kinds of cases, forensic experts rely on image processing techniques and tools, to make the face fit to be processed by the commercial recognition systems (\\eg use partial face images from another subject to fill in the occluded parts of the face of interest, or have a tight crop around the face). In this study, we evaluate the sensitivity of commercial recognition systems to such forensic techniques. More specifically, we study the change in the rank-1 identification result that is caused by forensic processing of faces-of-interest that are unusable by the commercial recognition systems. Further, forensic processing of such faces is more of an art and it is extremely difficult to process faces consistently such that there is a predictable effect on the rank-$n$ identification result. This study is meant to serve as an evaluation of the effect of a few forensic techniques intended to allow commercial recognition systems to process and match face images that were otherwise unusable. Our results indicate that COTS FRS can be sensitive to the subjectivity in facial part swapping and cropping, resulting in inconsistencies in the identification rankings and similarity scores.",
    "code_link": ""
  },
  "cvpr2015_w2_exploratoryanalysisofanoperationalirisrecognitiondatasetfromacbsaborder-crossingapplication": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Exploratory Analysis of an Operational Iris Recognition Dataset From a CBSA Border-Crossing Application",
    "authors": [
      "Estefan Ortiz",
      "Kevin W. Bowyer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Ortiz_Exploratory_Analysis_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Ortiz_Exploratory_Analysis_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper presents an exploratory analysis of an iris recognition dataset from the NEXUS border-crossing program run by the Canadian Border Services Agency.The distribution of the normalized Hamming distance for successful border-crossing transactions is examined in the context of various properties of the operational scenario.The effects of properties such as match score censoring and truncation, same-sensor and cross-sensor matching, sequence-dependent matching, and multiple-kiosk matching are illustrated.Implications of these properties of the operational dataset for the study of iris template aging are discussed.",
    "code_link": ""
  },
  "cvpr2015_w2_evaluationofcombinedvisible/nircameraforirisauthenticationonsmartphones": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Evaluation of Combined Visible/NIR Camera for Iris Authentication on Smartphones",
    "authors": [
      "Shejin Thavalengal",
      "Petronel Bigioi",
      "Peter Corcoran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Thavalengal_Evaluation_of_Combined_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Thavalengal_Evaluation_of_Combined_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Iris biometrics provide a mature and robust method of authentication, but are typically applied in a controlled environment and under constrained acquisition conditions. In this paper, the adaption of iris biometrics for unconstrained, hand-held use cases such as smartphones is investigated. A prototype optics-sensor combination is analysed in terms of its optical properties and iris imaging capabilities. The corresponding camera system with dual visible/NIR sensing capabilities and 4 Megapixel resolution is tested for suitability to implement iris recognition on smartphones. Recognition performance is analysed together with image quality comparisons. Preliminary results indicate that there are challenges to achieve reliable recognition performance in unconstrained use cases. Current optical systems are not diffraction limited, particularly at NIR wavelengths; pixel resolutions are close to the useful limits for iris recognition and acquisition conditions are challenging. Nevertheless, our findings indicate a similar camera module, with an improved optics and sensor, could combine biometric authentication with more conventional front-camera functions such as the capture of selfie images.",
    "code_link": ""
  },
  "cvpr2015_w2_apreliminarystudyonidentifyingsensorsfromirisimages": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "A Preliminary Study on Identifying Sensors From Iris Images",
    "authors": [
      "Nathan Kalka",
      "Nick Bartlow",
      "Bojan Cukic",
      "Arun Ross"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Kalka_A_Preliminary_Study_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Kalka_A_Preliminary_Study_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper we explore the possibility of examining an iris image and identifying the sensor that was used to acquire it. This is accomplished based on a classical pixel non-uniformity (PNU) noise analysis of the iris sensor. For each iris sensor, a noise reference pattern is generated and subsequently correlated with noise residuals extracted from iris images. We conduct experiments using data from seven iris databases, viz., West Virginia University (WVU) non-ideal, WVU off-angle, Iris Challenge Evaluation (ICE) 1.0, CASIAv2 Device1, CASIAv2-Device2, CASIAv3 interval, and CASIAv3 lamp. Results indicate that iris sensor identification using PNU noise is very encouraging, with rank-1 identification rates ranging from 86%-99% for unit level testing (distinguishing sensors from the same vendor) and 81%-96% for the combination of brand (distinguishing sensors from different vendors) and unit level testing. Our analysis also suggests that in many cases, sensor identification can be performed even with a limited number of training images. We also observe that JPEG compression degrades identification performance, specifically at the sensor unit level.",
    "code_link": ""
  },
  "cvpr2015_w2_theemperorsnewmasksondemographicdifferencesanddisguises": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "The Emperor's New Masks: On Demographic Differences and Disguises",
    "authors": [
      "Katherine L. Gibson",
      "Jonathan M. Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Gibson_The_Emperors_New_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Gibson_The_Emperors_New_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Remaining unrecognized in an era of ubiquitous camera surveillance remains desirable to some, but advances in face recognition technology make it increasingly difficult to do so. A large database of high-quality imagery was used to explore the effectiveness of disguise as an approach to avoiding recognition. A commercial system that was highly rated in NIST's Face Recognition Vendor Test was used to evaluate a variety of disguises worn by each member of a study population that was diverse in age, gender, and race. Analysis of the recognition results for subsets extracted from the population shows that disguise can be remarkably effective. However, the efficacy of the disguises against face recognition varies so significantly with demographics that, for some, the disguises are not worth wearing.",
    "code_link": ""
  },
  "cvpr2015_w2_latentmax-marginmetriclearningforcomparingvideofacetubes": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Latent Max-Margin Metric Learning for Comparing Video Face Tubes",
    "authors": [
      "Gaurav Sharma",
      "Patrick Perez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Sharma_Latent_Max-Margin_Metric_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Sharma_Latent_Max-Margin_Metric_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Comparing \"face tubes\" is a key component of modern systems for face biometrics based video analysis and annotation. We present a novel algorithm to learn a distance metric between such spatio-temporal face tubes in videos. The main novelty in the algorithm is based on incorporation of latent variables in a max-margin metric learning framework. The latent formulation allows us to model, and learn metrics to compare faces under different challenging variations in pose, expressions and lighting. We propose a novel dataset named TV Series Face Tubes (TSFT) for evaluating the task. The dataset is collected from 12 different episodes of 8 popular TV series and has 94 subjects with 569 manually annotated face tracks in total. We show quantitatively how incorporating latent variables in max-margin metric learning leads to improvement of current state-of-the-art metric learning methods for the two cases when the testing is done with subjects that were seen during training and when the test subjects were not seen at all during training. We also give results on a challenging benchmark dataset: YouTube faces, and place our algorithm in context w.r.t. existing methods. ",
    "code_link": ""
  },
  "cvpr2015_w2_unsupervisedlearningofovercompletefacedescriptors": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Unsupervised Learning of Overcomplete Face Descriptors",
    "authors": [
      "Juha Ylioinas",
      "Juho Kannala",
      "Abdenour Hadid",
      "Matti Pietikainen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Ylioinas_Unsupervised_Learning_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/paper/Ylioinas_Unsupervised_Learning_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The current state-of-the-art indicates that a very discriminative unsupervised face representation can be constructed by encoding overlapping multi-scale face image patches at facial landmarks. If fixed as such, there are even suggestions (albeit subtle) that the underlying features may no longer have as much meaning. In spite of the effectiveness of this strategy, we argue that one may still afford to improve especially at the feature level. In this paper, we investigate the role of overcompleteness in features for building unsupervised face representations. In our approach, we first learn an overcomplete basis from a set of sampled face image patches. Then, we use this basis to produce features that are further encoded using the Bag-of-Features (BoF) approach. Using our method, without an extensive use of facial landmarks, one is able to construct a single-scale representation reaching state-of-the-art performance in face recognition and age estimation following the protocols of LFW, FERET, and Adience benchmarks. Furthermore, we make several interesting findings related, for example, to the positive impact of applying soft feature encoding scheme preceding standard dimensionality reduction. To this end, making the encoding faster, we propose a novel method for approximative soft-assignment which we show to perform better than its hard-assigned counterpart.",
    "code_link": ""
  },
  "cvpr2015_w2_personidentificationfromactionstyles": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Person Identification From Action Styles",
    "authors": [
      "Igor Kviatkovsky",
      "Ilan Shimshoni",
      "Ehud Rivlin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Kviatkovsky_Person_Identification_From_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Kviatkovsky_Person_Identification_From_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We consider a problem of identifying people based on their styles in performing actions from an arbitrary predefined set of action types. We present a generative model describing the action instance creation process and derive a probabilistic identity inference scheme, which implicitly includes action type inference as one of its components. Our experiments validate the power of the approach. We report high recognition rates on four publicly available action recognition datasets and one dataset for person authentication, on which we obtain state-of-the-art results. We make use of existing action representations and show that combining them with an action-specific Mahalanobis metric, learned from examples, improves the results.",
    "code_link": ""
  },
  "cvpr2015_w2_afacialfeaturesdetectorintegratingholisticfacialinformationandpart-basedmodel": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "A Facial Features Detector Integrating Holistic Facial Information and Part-Based Model",
    "authors": [
      "Eslam Mostafa",
      "Asem A. Ali",
      "Ahmed Shalaby",
      "Aly Farag"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Mostafa_A_Facial_Features_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Mostafa_A_Facial_Features_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We propose a facial landmarks detector, in which a part based model is incorporated with holistic face information. In the part based model, the face is modeled by the appearance of different face parts and their geometric relation. The appearance is described by pixel normalized difference representation. This representation is the lowest computational complexity as compared with existing state of art while it has a similar accuracy. On the other hand, to model the geometric relation between the face parts, the complex Bingham distribution is adapted. This is because the complex Bingham distribution has a symmetric property so it is invariant to rotation, scale, and translation. After that the global information is incorporated with the local part model using a regression model. The regression model estimates the displacement to the final face shape model. The the proposed detector is evaluated on two datasets. Experimental results show that it outperforms the state-of-art approaches in detecting facial landmarks accurately.",
    "code_link": ""
  },
  "cvpr2015_w2_geneticalgorithmattackonminutiae-basedfingerprintauthenticationandprotectedtemplatefingerprintsystems": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Genetic Algorithm Attack on Minutiae-Based Fingerprint Authentication and Protected Template Fingerprint Systems",
    "authors": [
      "Andras Rozsa",
      "Albert E. Glock",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Rozsa_Genetic_Algorithm_Attack_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Rozsa_Genetic_Algorithm_Attack_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper describes a new generic attack against minutiae-based fingerprint authentication systems. The goal of the attack is to construct a fingerprint minutiae template that matches a fixed but unknown reference template. The effectiveness of our attacking system is experimentally demonstrated against multiple fingerprint authentication systems. The paper discusses this attack on two leading privacy-enhanced template schemes and shows it can easily recover high matching score templates.A more general and novel aspect of our work is showing that despite high scores of the attack, the resulting templates do not match the original fingerprint and therefore the underlying data is still privacy protected. We conjecture that the ambiguity caused by collisions from projections/hashing during the privacy-enhanced template production provides for a multitude of minima, which trap attacks in a high-score but non-authentic region.",
    "code_link": ""
  },
  "cvpr2015_w2_electromyographandkeystrokedynamicsforspoof-resistantbiometricauthentication": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Electromyograph and Keystroke Dynamics for Spoof-Resistant Biometric Authentication",
    "authors": [
      "Shreyas Venugopalan",
      "Felix Juefei-Xu",
      "Benjamin Cowley",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Venugopalan_Electromyograph_and_Keystroke_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Venugopalan_Electromyograph_and_Keystroke_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Biometrics has come a long way over the past decade in terms of technologies and devices that are used to verify user identities. Three of the more well studied modalities in this field are the face, iris and fingerprint, with the latter two reporting very high user identification/verification rates. In the biometric community there has been little work in studying biomedical signals for user recognition purposes. In this paper, we propose using electromyograph (EMG) signals as a person's biometric signature. The EMG records the motor unit action potentials (MUAP) during any physical motion. Our study is done within the context of a person using a keyboard to type a password or any other fixed phrase. Along with EMG signals, we log key press times for the user and study the feasibility of using this data too as a biometric feature. Keypress timings alone if used as a biometric, are very easy to spoof and hence we fuse this modality with EMG signals. In order to classify these features, we use subspace modeling as well as Bayesian classifiers. The experiments have been performed within the context of a user typing a fixed pass phrase at a workstation. The idea is to monitor both biometric modalities when this action is performed and study user verification across data capture sessions and within capture sessions. Our approach yields high values of verification rates, which shows the promise of using these modalities as user specific biometric signatures.",
    "code_link": ""
  },
  "cvpr2015_w2_amultipleserverschemeforfingerprintfuzzyvaults": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "A Multiple Server Scheme for Fingerprint Fuzzy Vaults",
    "authors": [
      "Jesse Hartloff",
      "Matthew Morse",
      "Bingsheng Zhang",
      "Thomas Effland",
      "Jennifer Cordaro",
      "Jim Schuler",
      "Sergey Tulyakov",
      "Atri Rudra",
      "Venu Govindaraju"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Hartloff_A_Multiple_Server_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Hartloff_A_Multiple_Server_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this work, we present a multiple server fingerprint verification scheme that provides enhanced template security by eliminating several known vulnerabilities of the fuzzy vault scheme. We secure templates from adversarial attacks in honest-but-curious server scenarios by utilizing commutative encryption in which the raw fingerprint template is never used in matching or storage. In this system, there is a matching server that performs the enrollment and matching functions on fingerprint data that has been encrypted by a separate encryption server. Since the encrypted template is stored at one server and the encryption key is on another server, an attacker would have to compromise both servers to decrypt the data. Even in this case, the templates are protected by the fuzzy vault scheme. Thus, this scheme limits an attacker's ability to attack active users even after compromising both servers providing multiple layers of template security. ",
    "code_link": ""
  },
  "cvpr2015_w2_pore-basedridgereconstructionforfingerprintrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W2",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Biometrics",
    "title": "Pore-Based Ridge Reconstruction for Fingerprint Recognition",
    "authors": [
      "Mauricio Pamplona Segundo",
      "Rubisley de Paula Lemes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/html/Segundo_Pore-Based_Ridge_Reconstruction_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W02/papers/Segundo_Pore-Based_Ridge_Reconstruction_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The use of sweat pores in fingerprint recognition is becoming increasingly popular, mostly because of the wide availability of pores, which provides complementary information for matching distorted or incomplete images. In this work we present a fully automatic pore-based fingerprint recognition framework that combines both pores and ridges to measure the similarity of two images. To obtain the ridge structure, we propose a novel pore-based ridge reconstruction approach by considering a connect-the-dots strategy. To this end, Kruskal's minimum spanning tree algorithm is employed to connect consecutive pores and form a graph representing the ridge skeleton. We evaluate our framework on the PolyU HRF database, and the obtained results are favorably compared to previous results in the literature.",
    "code_link": ""
  },
  "cvpr2015_w3_self-tuneddeepsuperresolution": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Self-Tuned Deep Super Resolution",
    "authors": [
      "Zhangyang Wang",
      "Yingzhen Yang",
      "Zhaowen Wang",
      "Shiyu Chang",
      "Wei Han",
      "Jianchao Yang",
      "Thomas Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Wang_Self-Tuned_Deep_Super_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Wang_Self-Tuned_Deep_Super_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Deep learning has been successfully applied to image super resolution (SR). In this paper, we propose a deep joint super resolution (DJSR) model to exploit both external and self similarities for SR. A Stacked Denoising Convolutional Auto Encoder (SDCAE) is first pre-trained on external examples with proper data augmentations. It is then fine-tuned with multi-scale self examples from each input, where the reliability of self examples is explicitly taken into account. We also enhance the model performance by sub-model training and selection. The DJSR model is extensively evaluated and compared with state-of-the-arts, and show noticeable performance improvements both quantitatively and perceptually on a wide range of images.",
    "code_link": "https://github.com/ifp-uiuc/anna"
  },
  "cvpr2015_w3_channel-max,channel-dropandstochasticmax-pooling": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Channel-Max, Channel-Drop and Stochastic Max-Pooling",
    "authors": [
      "Yuchi Huang",
      "Xiuyu Sun",
      "Ming Lu",
      "Ming Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Huang_Channel-Max_Channel-Drop_and_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Huang_Channel-Max_Channel-Drop_and_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We propose three regularization techniques to overcome drawbacks of local winner-take-all methods used in deep convolutional networks. Channel-Max inherits the max activation unit from Maxout networks, but otherwise adopts complementary subsets of input and filters with different kernel sizes as better companions to the max function. To balance the training on different pathways, Channel-Drop is employed to randomly discard half pathways before their inputs are convolved respectively. Stochastic Max-pooling is defined to reduce the overfitting caused by conventional max-pooling, in which half activations are randomly dropped in each pooling region during training and top largest activations are probabilistically averaged during testing. Using Channel-Max, Channel-Drop and Stochastic Max-pooling, we demonstrate state-of-the-art performance on four benchmark datasets: CIFAR-10, CIFAR-100, STL-10 and SVHN.",
    "code_link": ""
  },
  "cvpr2015_w3_convolutionalrecurrentneuralnetworkslearningspatialdependenciesforimagerepresentation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Convolutional Recurrent Neural Networks: Learning Spatial Dependencies for Image Representation",
    "authors": [
      "Zhen Zuo",
      "Bing Shuai",
      "Gang Wang",
      "Xiao Liu",
      "Xingxing Wang",
      "Bing Wang",
      "Yushi Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Zuo_Convolutional_Recurrent_Neural_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Zuo_Convolutional_Recurrent_Neural_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between different image regions have been taken into consideration. Such dependencies represent useful spatial structure information in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spatial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are firstly processed to generate middle level features. RNN layer is then learned to encode spatial dependencies. The C-RNN can learn better image representation, especially for images with obvious spatial contextual dependencies. Our method achievescompetitive performance on ILSVRC 2012, SUN 397, and MIT indoor.",
    "code_link": ""
  },
  "cvpr2015_w3_deeplearningofbinaryhashcodesforfastimageretrieval": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Deep Learning of Binary Hash Codes for Fast Image Retrieval",
    "authors": [
      "Kevin Lin",
      "Huei-Fang Yang",
      "Jen-Hao Hsiao",
      "Chu-Song Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Lin_Deep_Learning_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate the scalability and efficacy of the proposed approach on the large-scale dataset of 1 million clothing images.",
    "code_link": ""
  },
  "cvpr2015_w3_fromgenerictospecificdeeprepresentationsforvisualrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "From Generic to Specific Deep Representations for Visual Recognition",
    "authors": [
      "Hossein Azizpour",
      "Ali Sharif Razavian",
      "Josephine Sullivan",
      "Atsuto Maki",
      "Stefan Carlsson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Azizpour_From_Generic_to_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Azizpour_From_Generic_to_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Evidence is mounting that ConvNets are the best representation learning method for recognition. In the common scenario, a ConvNet is trained on a large labeled dataset and the feed-forward units activation, at a certain layer of the network, is used as a generic representation of an input image. Recent studies have shown this form of representation to be astoundingly effective for a wide range of recognition tasks. This paper thoroughly investigates the transferability of such representations w.r.t. several factors. It includes parameters for training the network such as its architecture and parameters of feature extraction. We further show that different visual recognition tasks can be categorically ordered based on their distance from the source task. We then show interesting results indicating a clear correlation between the performance of tasks and their distance from the source task conditioned on proposed factors. Furthermore, by optimizing these factors, we achieve state-of-the-art performances on 16 visual recognition tasks.",
    "code_link": ""
  },
  "cvpr2015_w3_subsetfeaturelearningforfine-grainedcategoryclassification": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Subset Feature Learning for Fine-Grained Category Classification",
    "authors": [
      "ZongYuan Ge",
      "Christopher McCool",
      "Conrad Sanderson",
      "Peter Corke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Ge_Subset_Feature_Learning_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Ge_Subset_Feature_Learning_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Fine-grained categorisation has been a challenging problem due to small inter-class variation, large intra-class variation and low number of training images. We propose a learning system which first clusters visually similar classes and then learns deep convolutional neural network features specific to each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset show that the proposed method outperforms recent fine-grained categorisation methods under the most difficult setting: no bounding boxes are presented at test time. It achieves a mean accuracy of 77.5%, compared to the previous best performance of 73.2%. We also show that progressive transfer learning allows us to first learn domain-generic features (for bird classification) which can then be adapted to specific set of bird classes, yielding improvements in accuracy.",
    "code_link": ""
  },
  "cvpr2015_w3_exploitinglocalfeaturesfromdeepnetworksforimageretrieval": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Exploiting Local Features From Deep Networks for Image Retrieval",
    "authors": [
      "Joe Yue-Hei Ng",
      "Fan Yang",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Ng_Exploiting_Local_Features_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Ng_Exploiting_Local_Features_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Deep convolutional neural networks have been successfully applied to image classification tasks. When these same networks have been applied to image retrieval, the assumption has been made that the last layers would give the best performance, as they do in classification. We show that for instance-level image retrieval, lower layers often perform better than the last layers in convolutional neural networks. We present a novel approach for extracting convolutional features from different layers of the networks, and adopt VLAD encoding to encode features into a single vector for each image. We investigate the effect of different layers and scales of input images on the performance of convolutional features using the recent deep networks OxfordNet and GoogLeNet. Experiments demonstrate that intermediate layers or higher layers with finer scales produce better results for image retrieval, compared to the last layer. When using compressed 128-D VLAD descriptors, our method obtains state-of-the-art results and outperforms other VLAD and CNN based approaches on two out of three test datasets. Our work provides guidance for transferring deep networks trained on image classification to image retrieval tasks.",
    "code_link": ""
  },
  "cvpr2015_w3_objectleveldeepfeaturepoolingforcompactimagerepresentation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Object Level Deep Feature Pooling for Compact Image Representation",
    "authors": [
      "Konda Reddy Mopuri",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Mopuri_Object_Level_Deep_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Mopuri_Object_Level_Deep_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Convolutional Neural Network (CNN) features have been successfully employed in recent works as an image descriptor for various vision tasks. But the inability of the deep CNN features to exhibit invariance to geometric transformations and object compositions poses a great challenge for image search. In this work, we demonstrate the effectiveness of the objectness prior over the deep CNN features of image regions for obtaining an invariant image representation. The proposed approach represents the image as a vector of pooled CNN features describing the underlying objects. This representation provides robustness to spatial layout of the objects in the scene and achieves invariance to general geometric transformations, such as translation, rotation and scaling. The proposed approach also leads to a compact representation of the scene, making each image occupy a smaller memory footprint. Experiments show that the proposed representation achieves state of the art retrieval results on a set of challenging benchmark image datasets, while maintaining a compact representation.",
    "code_link": ""
  },
  "cvpr2015_w3_multi-scalepyramidpoolingfordeepconvolutionalrepresentation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Multi-Scale Pyramid Pooling for Deep Convolutional Representation",
    "authors": [
      "Donggeun Yoo",
      "Sunggyun Park",
      "Joon-Young Lee",
      "In So Kweon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Yoo_Multi-Scale_Pyramid_Pooling_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Yoo_Multi-Scale_Pyramid_Pooling_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Our representation demonstrates new state-of-the-art performances on three public datasets: 80.78% (Acc.) on MIT Indoor 67, 83.20% (mAP) on PASCAL VOC 2007 and 91.28% (Acc.) on Oxford 102 Flowers. The results suggest that our proposal can be used as a primary image representation for better performances in wide visual recognition tasks.",
    "code_link": ""
  },
  "cvpr2015_w3_colorconstancyusingcnns": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Color Constancy Using CNNs",
    "authors": [
      "Simone Bianco",
      "Claudio Cusano",
      "Raimondo Schettini"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Bianco_Color_Constancy_Using_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Bianco_Color_Constancy_Using_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this work we describe a Convolutional Neural Network (CNN) to accurately predict the scene illumination. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max pooling, one fully connected layer and three output nodes. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating scene illumination. This approach achieves state-of-the-art performance on a standard dataset of RAW images. Preliminary experiments on images with spatially varying illumination demonstrate the stability of the local illuminant estimation ability of our CNN.",
    "code_link": ""
  },
  "cvpr2015_w3_learningtocountwithdeepobjectfeatures": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W3",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Deep Vision: Deep Learning in Computer Vision 2015",
    "title": "Learning to Count With Deep Object Features",
    "authors": [
      "Santi Segui",
      "Oriol Pujol",
      "Jordi Vitria"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/html/Segui_Learning_to_Count_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W03/papers/Segui_Learning_to_Count_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene.",
    "code_link": ""
  },
  "cvpr2015_w4_walkingandtalkingabilinearapproachtomulti-labelactionrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "Walking and Talking: A Bilinear Approach to Multi-Label Action Recognition",
    "authors": [
      "Sameh Khamis",
      "Larry S. Davis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Khamis_Walking_and_Talking_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Khamis_Walking_and_Talking_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Action recognition is a fundamental problem in computer vision. However, all the current approaches pose the problem in a multi-class setting, where each actor is modeled as performing a single action at a time. In this work we pose the action recognition as a multi-label problem, i.e, an actor can be performing any plausible subset of actions. Determining which subsets of labels can co-occur is typically treated as a separate problem, typically modeled sparsely or fixed apriori to label correlation coefficients. In contrast, we formulate multi-label training and label correlation estimation as a joint max-margin bilinear classification problem. Our joint approach effectively trains discriminative bilinear classifiers that leverage label correlations. To evaluate our approach we relabeled the UCLA Courtyard dataset for the multi-label setting. We demonstrate that our joint model outperforms baselines on the same task and report state-of-the-art per-label accuracies on the dataset.",
    "code_link": ""
  },
  "cvpr2015_w4_discoveringhumaninteractionsinvideoswithlimiteddatalabeling": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "Discovering Human Interactions in Videos With Limited Data Labeling",
    "authors": [
      "Mehran Khodabandeh",
      "Arash Vahdat",
      "Guang-Tong Zhou",
      "Hossein Hajimirsadeghi",
      "Mehrsan Javan Roshtkhari",
      "Greg Mori",
      "Stephen Se"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Khodabandeh_Discovering_Human_Interactions_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Khodabandeh_Discovering_Human_Interactions_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We present a novel approach for discovering human interactions in videos. Activity understanding techniques usually require a large number of labeled examples, which are not available in many practical cases. Here, we focus on recovering semantically meaningful clusters of human-human and human-object interaction in an unsupervised fashion. A new iterative solution is introduced based on Maximum Margin Clustering (MMC), which also accepts user feedback to refine clusters. This is achieved by formulating the whole process as a unified constrained latent max-margin clustering problem. Extensive experiments have been carried out over three challenging datasets, Collective Activity, VIRAT, and UT-interaction. Empirical results demonstrate that the proposed algorithm can efficiently discover perfect semantic clusters of human interactions with only a small amount of labeling effort.",
    "code_link": ""
  },
  "cvpr2015_w4_museumvisitorsadatasetforpedestrianandgroupdetection,gazeestimationandbehaviorunderstanding": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "MuseumVisitors: A Dataset for Pedestrian and Group Detection, Gaze Estimation and Behavior Understanding",
    "authors": [
      "Federico Bartoli",
      "Giuseppe Lisanti",
      "Lorenzo Seidenari",
      "Svebor Karaman",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Bartoli_MuseumVisitors_A_Dataset_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Bartoli_MuseumVisitors_A_Dataset_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper we describe a new dataset, under construction, acquired inside the National Museum of Bargello in Florence. It was recorded with three IP cameras at a resolution of 1280x800 pixels and an average framerate of five frames per second. Sequences were recorded following two scenarios. The first scenario consists of visitors watching different artworks (individuals), while the second one consists of groups of visitors watching the same artworks (groups). This dataset is specifically designed to support research on group detection, occlusion handling, tracking, re-identification and behavior analysis. In order to ease the annotation process we designed a user friendly web interface that allows to annotate: bounding boxes, occlusion area, body orientation and head gaze, group belonging, and artwork under observation. We provide a comparison with other existing datasets that have group and occlusion annotations. In order to assess the difficulties of this dataset we have also performed some tests exploiting seven representative state-of-the-art pedestrian detectors. ",
    "code_link": ""
  },
  "cvpr2015_w4_subjectcentricgroupfeatureforpersonre-identification": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "Subject Centric Group Feature for Person Re-Identification",
    "authors": [
      "Li Wei",
      "Shishir K. Shah"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Wei_Subject_Centric_Group_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Wei_Subject_Centric_Group_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper presents a subject centric group feature for person re-identification. Our approach is inspired by the observation that people often tend to walk alongside others or in a group. We argue that co-travelers' information, including geometry and visual cues, can reduce the re-identification ambiguity and lead to better accuracy, compared to approaches that rely only on visual cues. We introduce person-group feature to capture both geometry and visual information of co-travelers around a subject. We compute the dis-similarity between person-group features by solving an integer programming problem. The proposed approach is evaluated in its ability to improve the accuracy of re-identification of people traveling within groups. The results show that our approach outperforms state-of-the-art visual based as well as group information based methods. ",
    "code_link": ""
  },
  "cvpr2015_w4_thegrodemetricsexploringtheperformanceofgroupdetectionapproaches": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "The GRODE Metrics: Exploring the Performance of Group Detection Approaches",
    "authors": [
      "Francesco Setti",
      "Marco Cristani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Setti_The_GRODE_Metrics_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Setti_The_GRODE_Metrics_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The detection of groups of people is attracting the attention of many researchers in diverse fields, with a growing number of approaches published each year; despite this, the evaluation metrics are not consolidated, with some measures inherited from the people detection fields, other ones designed specifically for a particular approach, generating a set of not comparable figure of merits. Moreover, existent methods of analysis are scarcely expressive, for example ignoring the fact that groups have different cardinalities, and that obviously larger groups are harder to find. This paper fills this gap presenting GRODE, a suite of measures of accuracy which defines precision and recall on the groups, including the group cardinality as variable. This gives the possibility to investigate aspects never considered so far, such as the tendency of a method of over- or undersegmenting groups, or of better dealing with specific group cardinalities. The metrics have been applied to all the publicly available approaches of group detection, discovering interesting strength and pitfalls of the current literature, and promoting further research in the field.",
    "code_link": ""
  },
  "cvpr2015_w4_learningtoidentifyleadersincrowd": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "Learning to Identify Leaders in Crowd",
    "authors": [
      "Francesco Solera",
      "Simone Calderara",
      "Rita Cucchiara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Solera_Learning_to_Identify_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Solera_Learning_to_Identify_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Leader identification is a crucial task in social analysis, crowd management and emergency planning. In this paper, we investigate a computational model for the individuation of leaders in crowded scenes. We deal with the lack of a formal definition of leadership by learning, in a supervised fashion, a metric space based exclusively on people spatiotemporal information. Based on Tarde's work on crowd psychology, individuals are modeled as nodes of a directed graph and leaders inherits their relevance thanks to other members references. We note this is analogous to the way websites are ranked by the PageRank algorithm. During experiments, we observed different feature weights depending on the specific type of crowd, highlighting the impossibility to provide a unique interpretation of leadership. To our knowledge, this is the first attempt to study leader identification as a metric learning problem.",
    "code_link": ""
  },
  "cvpr2015_w4_acomparisonofcrowdcommotionmeasuresfromgenerativemodels": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "A Comparison of Crowd Commotion Measures From Generative Models",
    "authors": [
      "Sadegh Mohammadi",
      "Hamed Kiani",
      "Alessandro Perina",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Mohammadi_A_Comparison_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Mohammadi_A_Comparison_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Detecting abnormal events in video sequences is a challenging task that has been broadly investigated over the last decade. The main challenges come from the lack of a clear definition of abnormality and from the scarcity, often absence, of abnormal training samples. To address these two shortages, the computer vision community made use of generative models to learn normal behavioral patterns in videos. Then, for each test observation, a (crowd) commotion measure is computed quantifying the deviation from the normal model. In this paper, we evaluated two different families of generative models, namely topic models, representing the standard choice, and the most recent Counting Grids which have never been considered for this task. Moreover, we also extended the 2D Counting Grid, introduced for the analysis of images, to three dimensions making the model able to capture the spatial-temporal relationships of the videos. In the experimental section, we compared all the approaches on five challenging sequences showing the superiority of the 3-D counting grid.",
    "code_link": ""
  },
  "cvpr2015_w4_real-timeanomalydetectionandlocalizationincrowdedscenes": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "Real-Time Anomaly Detection and Localization in Crowded Scenes",
    "authors": [
      "Mohammad Sabokrou",
      "Mahmood Fathy",
      "Mojtaba Hoseini",
      "Reinhard Klette"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Sabokrou_Real-Time_Anomaly_Detection_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Sabokrou_Real-Time_Anomaly_Detection_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we propose a method for real-time anomaly detection and localization in crowded scenes. Each video is defined as a set of non-overlapping cubic patches, and is described using two local and global descriptors. These descriptors capture the video properties from different aspects. By incorporating simple and cost-effective Gaussian classifiers, we can distinguish normal activities and anomalies in videos. The local and global features are based on structure similarity between adjacent patches and the features learned in an unsupervised way, using a sparse autoencoder. Experimental results show that our algorithm is comparable to a state-of-the-art procedure on UCSD ped2 and UMN benchmarks, but even more time-efficient. The experiments confirm that our system can reliably detect and localize anomalies as soon as they happen in a video.",
    "code_link": ""
  },
  "cvpr2015_w4_dominantflowextractionandanalysisintrafficsurveillancevideos": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W4",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Group And Crowd Behavior Analysis And Understanding",
    "title": "Dominant Flow Extraction and Analysis in Traffic Surveillance Videos",
    "authors": [
      "Srinivas S S Kruthiventi",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/html/Kruthiventi_Dominant_Flow_Extraction_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W04/papers/Kruthiventi_Dominant_Flow_Extraction_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Flow analysis of crowd and traffic videos is an important video surveillance task. In this work, we propose an algorithm for long-term flow segmentation and dominant flow extraction in traffic videos. Each flow segment is a temporal sequence of image segments indicating the motion of a vehicle in the camera view. This flow segmentation is done in the framework of Conditional Random Fields using motion and color features. We also propose a distance measure between any two flow segments based on Dynamic Time Warping and use this distance for clustering the flow segments into dominant flows. We then model each dominant flow by generating a representative flow segment, which is the mean of all the time-warped flow segments belonging to its cluster. Using these dominant flow models, we perform path prediction for the vehicles entering the view and detect anomalous motions. Experimental evaluation on a diverse set of challenging traffic videos demonstrates the effectiveness of the proposed method.",
    "code_link": ""
  },
  "cvpr2015_w5_comparisonofinfraredandvisibleimageryforobjecttrackingtowardtrackerswithsuperiorirperformance": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Comparison of Infrared and Visible Imagery for Object Tracking: Toward Trackers With Superior IR Performance",
    "authors": [
      "Erhan Gundogdu",
      "Huseyin Ozkan",
      "H. Seckin Demir",
      "Hamza Ergezer",
      "Erdem Akagunduz",
      "S. Kubilay Pakin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Gundogdu_Comparison_of_Infrared_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Gundogdu_Comparison_of_Infrared_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The subject of this paper is the object tracking problem in infrared (IR) videos. Our contribution is twofold. First, the performance behaviour of the state-of-the-art trackers is investigated via a comparative study using IR-visible band video conjugates, i.e., video pairs captured observing the same scene simultaneously, to identify the IR specific challenges. Second, we propose a novel ensemble based tracking method that is tuned to IR data. The proposed algorithm sequentially constructs and maintains a dynamical ensemble of simple correlators and produces tracking decisions by switching among the ensemble correlators depending on the target appearance in a computationally highly efficient manner. We empirically show that our algorithm significantly outperforms the state-of-the-art trackers in our extensive set of experiments with IR imagery.",
    "code_link": ""
  },
  "cvpr2015_w5_vaisadatasetforrecognizingmaritimeimageryinthevisibleandinfraredspectrums": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "VAIS: A Dataset for Recognizing Maritime Imagery in the Visible and Infrared Spectrums",
    "authors": [
      "Mabel M. Zhang",
      "Jean Choi",
      "Kostas Daniilidis",
      "Michael T. Wolf",
      "Christopher Kanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Zhang_VAIS_A_Dataset_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Zhang_VAIS_A_Dataset_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The development of fully autonomous seafaring vessels has enormous implications to the world's global supply chain and militaries. To obey international marine traffic regulations, these vessels must be equipped with machine vision systems that can classify other ships nearby during the day and night. In this paper, we address this problem by introducing VAIS, the world's first publicly available dataset of paired visible and infrared ship imagery. This dataset contains more than 1,000 paired RGB and infrared images among six ship categories - merchant, sailing, passenger, medium, tug, and small - which are salient for control and following maritime traffic regulations. We provide baseline results on this dataset using two off-the-shelf algorithms: gnostic fields and deep convolutional neural networks. Using these classifiers, we are able to achieve 87.4% mean per-class recognition accuracy during the day and 61.0% at night.",
    "code_link": ""
  },
  "cvpr2015_w5_acomparisonofstereoandmultiview3-dreconstructionusingcross-sensorsatelliteimagery": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "A Comparison of Stereo and Multiview 3-D Reconstruction Using Cross-Sensor Satellite Imagery",
    "authors": [
      "Ozge C. Ozcanli",
      "Yi Dong",
      "Joseph L. Mundy",
      "Helen Webb",
      "Riad Hammoud",
      "Victor Tom"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Ozcanli_A_Comparison_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Ozcanli_A_Comparison_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " High-resolution and accurate Digital Elevation Model (DEM) generation from satellite imagery is a challenging problem. In this work, a stereo 3-D reconstruction framework is outlined that is applicable to nonstereoscopic satellite image pairs that may be captured by different satellites. The orthographic height maps given by stereo reconstruction are compared to height maps given by a multiview approach based on Probabilistic Volumetric Representation (PVR). Height map qualities are measured in comparison to manually prepared ground-truth height maps in three sites from different parts of the world with urban, semi-urban and rural features. The results along with strengths and weaknesses of the two techniques are summarized.",
    "code_link": ""
  },
  "cvpr2015_w5_onlinemultimodalvideoregistrationbasedonshapematching": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Online Multimodal Video Registration Based on Shape Matching",
    "authors": [
      "Pierre-Luc St-Charles",
      "Guillaume-Alexandre Bilodeau",
      "Robert Bergevin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/St-Charles_Online_Multimodal_Video_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/St-Charles_Online_Multimodal_Video_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The registration of video sequences captured using different types of sensors often relies on dense feature matching methods, which are very costly. In this paper, we study theproblem of \"almost planar\" scene registration (i.e. where the planar ground assumption is almost respected) in multimodal imagery using target shape information. We introduce a new strategy for robustly aligning scene elements based on the random sampling of shape contour correspondences and on the continuous update of our transformation model's parameters. We evaluate our solution on a public dataset and show its superiority by comparing it to a recently published method that targets the same problem. To make comparisons between such methods easier in the future, we provide our evaluation tools along with a full implementation of our solution online.",
    "code_link": ""
  },
  "cvpr2015_w5_automatedfeatureweightingandrandompixelsamplingink-meansclusteringforterahertzimagesegmentation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Automated Feature Weighting and Random Pixel Sampling in k-Means Clustering for Terahertz Image Segmentation",
    "authors": [
      "Mohamed Walid Ayech",
      "Djemel Ziou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Ayech_Automated_Feature_Weighting_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Ayech_Automated_Feature_Weighting_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Terahertz (THz) imaging is an innovative technology of imaging which can supply a large amount of data unavailable through other sensors. However, the higher dimension of THz images can be a hurdle to their display, their analysis and their interpretation. In this study, we propose a weighted feature space and a simple random sampling in k-means clustering for THz image segmentation. Our approach consists to estimate the expected centers, select the relevant features and their scores, and classify the observed pixels of THz images. It is more appropriate for achieving the best compactness inside clusters, the best discrimination of features, and the best tradeoff between the clustering accuracy and the low computational cost. Our approach of segmentation is evaluated by measuring performances and appraised by a comparison with some related works.",
    "code_link": ""
  },
  "cvpr2015_w5_amodel-basedapproachtofindingtracksinsarccdimages": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "A Model-Based Approach to Finding Tracks in SAR CCD Images",
    "authors": [
      "Tu-Thach Quach",
      "Rebecca Malinas",
      "Mark W. Koch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Quach_A_Model-Based_Approach_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Quach_A_Model-Based_Approach_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Combining multiple synthetic aperture radar (SAR) images taken at different times of the same scene produces coherent change detection (CCD) images that can detect small surface changes such as tire tracks. The resulting CCD images can be used in an automated approach to identify and label tracks. Existing techniques have limited success due to the noisy nature of these CCD images. In particular, existing techniques require some user cues and can only trace a single track. This paper presents an approach to automatically identify and label multiple tracks in CCD images. We use an explicit objective function that utilizes the Bayesian information criterion to find the simplest set of curves that explains the observed data. Experimental results show that it is capable of identifying tracks under various scenes and can correctly declare when no tracks are present.",
    "code_link": ""
  },
  "cvpr2015_w5_efficientpersonre-identificationbyhybridspatiogramandcovariancedescriptor": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Efficient Person Re-Identification by Hybrid Spatiogram and Covariance Descriptor",
    "authors": [
      "Mingyong Zeng",
      "Zemin Wu",
      "Chang Tian",
      "Lei Zhang",
      "Lei Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Zeng_Efficient_Person_Re-Identification_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Zeng_Efficient_Person_Re-Identification_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Feature and metric researchings are two vital aspects in person re-identification. Metric learning seems to have gained extra advantage over feature in recent evaluations. In this paper, we explore the neglected potential of feature designing for re-identification. We propose a novel and efficient person descriptor, which is motivated by traditional spatiogram and covariance descriptors. The spatiogram feature accumulates multiple spatial histograms of different image regions from several color channels and then extracts three descriptive sub-features. The covariance feature exploits several colorspaces and intensity gradients as pixel features and then extracts multiple statistical feature vectors from a pyramid of covariance matrices. Moreover, we also propose an effective and efficient multi-shot re-id metric without learning, which fuses the residual and coding coefficients after collaboratively coding samples on all person classes. The proposed descriptor and metric are evaluated with current methods on benchmark datasets. Our methods not only achieve state-of-the-art results but also are straightforward and computationally efficient, facilitating real-time surveillance applications such as pedestrian tracking and robotic perception in various dynamic scenes.",
    "code_link": "https://github.com/Myles-ZMY/HSCD"
  },
  "cvpr2015_w5_articulatedgaussiankernelcorrelationforhumanposeestimation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Articulated Gaussian Kernel Correlation for Human Pose Estimation",
    "authors": [
      "Meng Ding",
      "Guoliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Ding_Articulated_Gaussian_Kernel_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Ding_Articulated_Gaussian_Kernel_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we address the problem of human pose estimation through a novel articulated Gaussian kernel correlation function which is applied to human pose tracking from a single depth sensor. We first derive a unified Gaussian kernel correlation that can generalize the previous Sum-of-Gaussians (SoG)-based methods for the similarity measure between a template and the observation. Furthermore, we develop an articulated Gaussian kernel correlation by embedding a tree-structured skeleton model, which enables us to estimate the full-body pose parameters. Also, the new kernel correlation framework can easily penalize undesired body intersection which is more natural than the clamping function in previous methods. Our algorithm is general, simple yet effective and can achieve real-time performance. The experimental results on a public depth dataset are promising and competitive when compared with state-of-the-art algorithms.",
    "code_link": ""
  },
  "cvpr2015_w5_automationofdormantpruninginspecialtycropproductionanadaptiveframeworkforautomaticreconstructionandmodelingofappletrees": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Automation of Dormant Pruning in Specialty Crop Production: An Adaptive Framework for Automatic Reconstruction and Modeling of Apple Trees",
    "authors": [
      "Noha M. Elfiky",
      "Shayan A. Akbar",
      "Jianxin Sun",
      "Johnny Park",
      "Avinash Kak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Elfiky_Automation_of_Dormant_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Elfiky_Automation_of_Dormant_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Dormant pruning is one of the most costly and labor-intensive operations in specialty crop production. During winter, a large crew of trained seasonal workers has to carefully remove the branches from hundreds of trees using a set of pre-defined rules. The goal of automatic pruning is to reduce this dependence on a large workforce that is currently needed for the job. Automatically applying the pruning \"rules\" entails construction of 3D models of the trees in their dormant condition (that is, without foliage) and accurate estimation of the pruning points on the branches.This paper investigates the use of Skeleton-based Geometric (SbG) features in a 3D reconstruction scheme. The results obtained demonstrate the effectiveness of the SbG features for automatic reconstruction using only two views -- the front and the back. Our results show that our proposed scheme locates the pruning points on the tree branches with an accuracy of 96.0%. The algorithm that locates the pruning points is based on a new adaptive circle-based-layer-aware modeling scheme for the trunks and the primary branches \"PBs\" of the trees. Its three main steps are detection, segmentation, and modeling. Localization of the pruning points on the tree branches is a part of the modeling step. Both qualitative and quantitative evaluation are performed on a new challenging apple-trees dataset that is collected for the purpose of evaluating our approach.",
    "code_link": ""
  },
  "cvpr2015_w5_acloudinfrastructurefortargetdetectionandtrackingusingaudioandvideofusion": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "A Cloud Infrastructure for Target Detection and Tracking Using Audio and Video Fusion ",
    "authors": [
      "Kui Liu",
      "Bingwei Liu",
      "Erik Blasch",
      "Dan Shen",
      "Zhonghai Wang",
      "Haibin Ling",
      "Genshe Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Liu_A_Cloud_Infrastructure_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Liu_A_Cloud_Infrastructure_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper presents a Cloud-based architecture for detecting and tracking multiple moving targets from airborne videos combined with the audio assistance, which is called Cloud-based Audio-Video (CAV) fusion. The CAV system innovation is a method for user-based voice-to-text color feature descriptor track matching with an automated hue feature extraction from image pixels. The introduced CAV approach is general purpose for detecting and tracking different valuable targets' movement for suspicious behavior recognition through multi-intelligence data fusion. Using Cloud computing leads to real-time performance as compared a single machine workflow. The obtained multiple moving target tracking results from airborne videos demonstrate that the CAV approach provides improved frame rate, enhanced detection, and real-time tracking and classification performance under realistic conditions.",
    "code_link": ""
  },
  "cvpr2015_w5_makemyday-high-fidelitycolordenoisingwithnear-infrared": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Make My Day - High-Fidelity Color Denoising With Near-Infrared",
    "authors": [
      "Hiroto Honda",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Honda_Make_My_Day_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Honda_Make_My_Day_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We address the task of restoring RGB images taken under low illumination (e.g. night time), when an aligned near infrared (NIR or simply N) image taken under stronger NIR illumination is available. Such restoration holds the promise that algorithms designed to work under daylight conditions could be used around the clock. Increasingly, RGBN cameras are becoming available, as car cameras tend to include a Near-Infrared (N) band, next to R, G, and B bands, and NIR artificial lighting is applied. Under low lighting conditions, the NIR band is less noisy than the others and this is all the more the case if stronger illumination is only available in the NIR band. We address the task of restoring the R, G, and B bands on the basis of the NIR band in such cases. Even if the NIR band is less strongly correlated with the R, G, and B bands than these bands are mutually, there is sufficient such correlation to pick up important textural and gradient information in the NIR band and inject it into the others. The algorithm that we propose - coined `Make My Day' or MMD for short - is akin to the previously published BM3D denoising algorithm. MMD denoises the three (visible - NIR) differential images to then add back the original NIR image. It not only effectively reduces the noise but also includes the texture and edge information in the high spatial frequency range. MMD outperforms other state-of-art denoising methods in terms of PSNR, texture quality, and color fidelity. We publish our codes and images.",
    "code_link": ""
  },
  "cvpr2015_w5_usdotnumberlocalizationandrecognitionfromvehicleside-viewnirimages": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "USDOT Number Localization and Recognition From Vehicle Side-View NIR Images",
    "authors": [
      "Orhan Bulan",
      "Safwan Wshah",
      "Ramesh Palghat",
      "Vladimir Kozitsky",
      "Aaron Burry"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Bulan_USDOT_Number_Localization_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Bulan_USDOT_Number_Localization_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Commercial motor vehicles are mandated to display a valid U.S. Department of Transportation (USDOT) identification number on the side of the vehicle. Automatic recognition of USDOT numbers is of interest to government agencies for the efficient enforcement and management of the commercial trucks. Near infrared (NIR) cameras installed on the side of the road, to capture an image of an incoming truck, can capture USDOT images without distracting the drivers. In this paper, we propose a computer vision based method for recognizing USDOT numbers using an NIR camera system directed at the side of the commercial vehicles. The developed method consists of two stages; first, we localize the USDOT tag in the captured image using the deformable part model (DPM). Next, we train a convolutional neural network (CNN) using street-view house number (SVHN) dataset and sweep the trained classifier across the localized region. Based on the calculated scores, we infer the digits and their locations using a probabilistic inference method based on Hidden Markov Models (HMM). The most likely digit sequence is determined by applying the Viterbi algorithm. A data set of 1549 images was collected on a public roadway and is used to perform the experiments.",
    "code_link": ""
  },
  "cvpr2015_w5_robustobjectrecognitioninrgb-degocentricvideosbasedonsparseaffinehullkernel": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Robust Object Recognition in RGB-D Egocentric Videos Based on Sparse Affine Hull Kernel",
    "authors": [
      "Shaohua Wan",
      "J.K. Aggarwal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Wan_Robust_Object_Recognition_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Wan_Robust_Object_Recognition_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we propose a novel kernel function for recognizing objects in RGB-D egocentric videos. In order to effectively exploit the varied object appearance in a video, we take a set-based recognition approach and represent the target object using the set of frames contained in the video. Our kernel function measures the similarity of two sets by the minimum distance between the sparse affine hulls of the two sets. Our kernel function also allows convenient integration of heterogeneous data modalities beyond RGB and depth. We extensively evaluate the proposed method on three benchmark datasets, including two RGB-D object datasets and one thermal/visible face dataset. All the results clearly show that the proposed method outperforms state-of-the-art methods. ",
    "code_link": ""
  },
  "cvpr2015_w5_improvingsuperpixelboundariesusinginformationbeyondthevisualspectrum": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Improving Superpixel Boundaries Using Information Beyond the Visual Spectrum",
    "authors": [
      "Keith Sullivan",
      "Wallace Lawson",
      "Donald Sofge"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Sullivan_Improving_Superpixel_Boundaries_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Sullivan_Improving_Superpixel_Boundaries_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Superpixels enable a scene to be analyzed on a larger scale, by examining regions that have a high level of similarity. These regions can change depending on how similarity is measured. Color is a simple and effective measure, but it is adversely affected in environments where the boundary between objects and the surrounding environment are difficult to detect due to similar colors and/or shadows. We extend a common superpixel algorithm (SLIC) to include near-infrared intensity information and measured distance information to help oversegmentation in complex environments. We demonstrate the efficacy of our approach on two problems: object segmentation and scene segmentation.",
    "code_link": ""
  },
  "cvpr2015_w5_asimplyintegrateddual-sensorbasednon-intrusiveirisimageacquisitionsystem": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "A Simply Integrated Dual-Sensor Based Non-Intrusive Iris Image Acquisition System",
    "authors": [
      "Jang-Hee Yoo",
      "Byung Jun Kang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Yoo_A_Simply_Integrated_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Yoo_A_Simply_Integrated_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We describe an image acquisition system based on an integrated dual-sensor with a visible and infrared spectrum, which enables a multimodal biometric system, including non-intrusive iris recognition. To implement this as capable of simultaneously acquiring facial and iris images, a beam splitter for reflecting or transmitting visible and infrared light representing an image of a target object is used along an identical ray path divided into different bands. Namely, the beam splitter divides incident light from an object into the dual-sensor. Accordingly, an image of the iris area and an image of the facial region can be simultaneously acquired from a single image acquisition system, and an iris image can be acquired from a relatively long distance. The iris biometric system is implemented by using the facial detection and iris recognition SDK. In experiments, we have successfully evaluated the performance of the proposed image acquisition system with a non-intrusive iris recognition algorithm.",
    "code_link": ""
  },
  "cvpr2015_w5_heterogeneousstructurefusionfortargetrecognitionininfraredimagery": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Heterogeneous Structure Fusion for Target Recognition in Infrared Imagery",
    "authors": [
      "Guangfeng Lin",
      "Guoliang Fan",
      "Liangjiang Yu",
      "Xiaobing Kang",
      "Erhu Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Lin_Heterogeneous_Structure_Fusion_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Lin_Heterogeneous_Structure_Fusion_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We study Automatic Target Recognition (ATR) in infrared (IR) imagery from the perspective of feature fusion. The key to feature fusion is to take advantage of the discriminative and complementary information from different feature sets, which can be represented as internal (within each feature set) or external structures (across different feature sets). Traditional approaches tend to preserve either internal or external structures via certain feature projection. Some early attempts consider both structures implicitly or indirectly without revealing their relative importance and relevance. We propose a new unsupervised heterogeneous structure fusion (HSF) algorithm that is able to jointly optimize two kinds of structures explicitly and directly via a unified feature projection. The objective function of HSF integrates two feature structures in a closed form which can be optimized alternately via linear programming and eigenvector methods. The HSF solution provides not only the optimal feature projection but also the weight coefficients that encode the relative importance between two kinds of structures and among multiple feature sets. The experimental results on the COMANCHE IR dataset demonstrate that HSF outperforms state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2015_w5_non-rigidarticulatedpointsetregistrationwithlocalstructurepreservation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Non-Rigid Articulated Point Set Registration With Local Structure Preservation",
    "authors": [
      "Song Ge",
      "Guoliang Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Ge_Non-Rigid_Articulated_Point_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Ge_Non-Rigid_Articulated_Point_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We propose a new Gaussian mixture model (GMM)-based probabilistic point set registration method, called Local Structure Preservation (LSP), which is aimed at complex non-rigid and articulated deformations. LSP integrates two complementary shape descriptors to preserve the local structure. The first one is the Local Linear Embedding (LLE)-based topology constraint to retain the local neighborhood relationship, and the other is the Laplacian Coordinate (LC)-based energy to encode the local neighborhood scale. The registration is formulated as a density estimation problem where the LLE and LC terms are embedded in the GMM-based Coherent Point Drift (CPD) framework. A closed form solution is solved by an Expectation Maximization (EM) algorithm where the two local terms are jointly optimized along with the CPD coherence constraint. The experimental results on a challenging 3D human dataset show the accuracy and efficiency of our proposed approach to handle non-rigid highly articulated deformations.",
    "code_link": ""
  },
  "cvpr2015_w5_sonarautomatictargetrecognitionforunderwateruxoremediation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Sonar Automatic Target Recognition for Underwater UXO Remediation",
    "authors": [
      "Jason C. Isaacs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Isaacs_Sonar_Automatic_Target_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Isaacs_Sonar_Automatic_Target_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Automatic target recognition (ATR) for unexploded ordnance (UXO) detection and classification using sonar data of opportunity from open oceans survey sites is an open research area. The goal here is to develop ATR spanning real-aperture and synthetic aperture sonar imagery. The classical paradigm of anomaly detection in images breaks down in cluttered and noisy environments. In this work we present an upgraded and ultimately more robust approach to object detection and classification in image sensor data. In this approach, object detection is performed using an insitu weighted highlight-shadow detector; features are generated on geometric moments in the imaging domain; and finally, classification is performed using an Ada-boosted decision tree classifier. These techniques are demonstrated on simulated real aperture sonar data with varying noise levels.",
    "code_link": ""
  },
  "cvpr2015_w5_nir-visheterogeneousfacerecognitionviacross-spectraljointdictionarylearningandreconstruction": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "NIR-VIS Heterogeneous Face Recognition via Cross-Spectral Joint Dictionary Learning and Reconstruction",
    "authors": [
      "Felix Juefei-Xu",
      "Dipan K. Pal",
      "Marios Savvides"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Juefei-Xu_NIR-VIS_Heterogeneous_Face_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Juefei-Xu_NIR-VIS_Heterogeneous_Face_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " A lot of real-world data is spread across multiple domains. Handling such data has been a challenging task. Heterogeneous face biometrics has begun to receive attention in recent years. In real-world scenarios, many surveillance cameras capture data in the NIR (near infrared) spectrum. However, most datasets accessible to law enforcement have been collected in the VIS (visible light) domain. Thus, there exists a need to match NIR to VIS face images. In this paper, we approach the problem by developing a method to reconstruct VIS images in the NIR domain and vice-versa. This approach is more applicable to real-world scenarios since it does not involve having to project millions of VIS database images into learned common subspace for subsequent matching. We present a cross-spectral joint $\\ell_0$ minimization based dictionary learning approach to learn a mapping function between the two domains. One can then use the function to reconstruct facial images between the domains. Our method is open set and can reconstruct any face not present in the training data. We present results on the CASIA NIR-VIS v2.0 database and report state-of-the-art results.",
    "code_link": ""
  },
  "cvpr2015_w5_roadsegmentationusingmultipasssingle-polsyntheticapertureradarimagery": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W5",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 11th IEEE Workshop on Perception Beyond the Visible Spectrum",
    "title": "Road Segmentation Using Multipass Single-Pol Synthetic Aperture Radar Imagery",
    "authors": [
      "Mark W. Koch",
      "Mary M. Moya",
      "James G. Chow",
      "Jeremy Goold",
      "Rebecca Malinas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/html/Koch_Road_Segmentation_Using_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W05/papers/Koch_Road_Segmentation_Using_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Synthetic aperture radar (SAR) is a remote sensing technology that can truly operate 24/7. It's an all-weather system that can operate at any time except in the most extreme conditions. By making multiple passes over a wide area, a SAR can provide surveillance over a long time period. For high level processing it is convenient to segment and classify the SAR images into objects that identify various terrains and man-made structures that we call \"static features.\" In this paper we concentrate on automatic road segmentation. This not only serves as a surrogate for finding other static features, butroad detection in of itself is important for aligning SAR images with other data sources. In this paper we introduce a novel SAR image product that captures how different regions decorrelate at different rates. We also show how a modified Kolmogorov-Smirnov test can be used to model the static features even when the independent observation assumption is violated. ",
    "code_link": ""
  },
  "cvpr2015_w6_3dhumanmotioncapturefrommonocularimagesequences": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Vision Meets Cognition Workshop: Functionality, Physics, Intentionality and Causality",
    "title": "3D Human Motion Capture From Monocular Image Sequences",
    "authors": [
      "Bastian Wandt",
      "Hanno Ackermann",
      "Bodo Rosenhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/html/Wandt_3D_Human_Motion_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/papers/Wandt_3D_Human_Motion_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper tackles the problem of estimating non-rigid human 3D shape and motion from image sequences taken by uncalibrated cameras. Similar to other state-of-the-art solutions we factorize 2D observations in camera parameters, base poses and mixing coefficients. Existing methods require sufficient camera motion during the sequence to achieve a correct 3D reconstruction. To obtain convincing 3D reconstructions from arbitrary camera motion, our method is based on a-priorly trained base poses. We show that strong periodic assumptions on the coefficients can be used to define an efficient and accurate algorithm for estimatingperiodic motion such as walking patterns. For the extension to non-periodic motion we propose our novel regularization term based on temporal bone length constancy. In contrast to other works, the proposed method does not use a predefined skeleton or anthropometric constraints and can handle arbitrary camera motion.Multiple experiments based on a 3D error metric demonstrate the stability of the proposed method. Compared to other state-of-the-art methods our algorithm shows a significant improvement.",
    "code_link": ""
  },
  "cvpr2015_w6_multinomialprocessingmodelsinvisualcognitiveeffortdiagnostics": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Vision Meets Cognition Workshop: Functionality, Physics, Intentionality and Causality",
    "title": "Multinomial Processing Models in Visual Cognitive Effort Diagnostics",
    "authors": [
      "Joshua D. Elkins",
      "Gahangir Hossain"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/html/Elkins_Multinomial_Processing_Models_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/papers/Elkins_Multinomial_Processing_Models_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The pupillary response has been used to measure mental workload because of its sensitivity to stimuli and high resolution. The goal of this study was to diagnose the cognitive effort involved with a task that was presented visually. A multinomial processing tree (MPT) was used as an analytical tool in order to disentangle and predict separate cognitive processes, with the resulting output being a change in pupil diameter.This model was fitted to previous test data related to the pupillary response when presented a mental multiplication task.An MPT model describes observed response frequencies from a set of response categories.The parameter values of an MPT model are the probabilities of moving from latent state to the next.An EM algorithm was used to estimate the parameter values based on the response frequency of each category. This results in a parsimonious, causal model that facilitates in the understanding the pupillary response to cognitive load.This model eventually could be instrumental in bridging the gap between human vision and computer vision.",
    "code_link": ""
  },
  "cvpr2015_w6_actionclassificationinstillimagesusinghumaneyemovements": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Vision Meets Cognition Workshop: Functionality, Physics, Intentionality and Causality",
    "title": "Action Classification in Still Images Using Human Eye Movements",
    "authors": [
      "Gary Ge",
      "Kiwon Yun",
      "Dimitris Samaras",
      "Gregory J. Zelinsky"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/html/Ge_Action_Classification_in_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/papers/Ge_Action_Classification_in_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Despite recent advances in computer vision, image categorization aimed at recognizing the semantic category of an image such as scene, objects or actions remains one of the most challenging tasks in the field. However, human gaze behavior can be harnessed to recognize different classes of actions for automated image understanding. To quantify the spatio-temporal information in gaze we use segments in each image (person, upper-body, lower-body, context) and derive gaze features, which include: number of transitions between segment pairs, avg/max of fixation-density map per segment, dwell time per segment, and a measure of when fixations were made on the person versus the context. We evaluate our gaze features on a subset of images from the challenging PASCAL VOC 2012 Action Classes dataset, while visual features using a Convolutional Neural Network are obtained as a baseline. Two support vector machine classifiers are trained, one with the gaze features and the other with the visual features. Although the baseline classifier outperforms the gaze classifier for classification of 10 actions, analysis of classification results over reveals four behaviorally meaningful action groups where classes within each group are often confused by the gaze classifier. When classifiers are retrained to discriminate between these groups, the performance of the gaze classifier improves significantly relative to the baseline. Furthermore, combining gaze and the baseline outperforms both gaze alone and the baseline alone, suggesting both are contributing to the classification decision and illustrating how gaze can improve state of the art methods of automated action classification.",
    "code_link": ""
  },
  "cvpr2015_w6_semantically-enriched3dmodelsforcommon-senseknowledge": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W6",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Vision Meets Cognition Workshop: Functionality, Physics, Intentionality and Causality",
    "title": "Semantically-Enriched 3D Models for Common-Sense Knowledge",
    "authors": [
      "Manolis Savva",
      "Angel X. Chang",
      "Pat Hanrahan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/html/Savva_Semantically-Enriched_3D_Models_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W06/papers/Savva_Semantically-Enriched_3D_Models_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We identify and connect a set of physical properties to 3D models to create a richly-annotated 3D model dataset with data on physical sizes, static support, attachment surfaces, material compositions, and weights.To collect these physical property priors, we leverage observations of 3D models within 3D scenes and information from images and text. By augmenting 3D models with these properties we create a semantically rich, multi-layered dataset of common indoor objects.We demonstrate the usefulness of these annotations for improving 3D scene synthesis systems, enabling faceted semantic queries into 3D model datasets, and reasoning about how objects can be manipulated by people using weight and static friction estimates.",
    "code_link": ""
  },
  "cvpr2015_w7_3dobjectclassdetectioninthewild": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W7",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 3D from a Single Image",
    "title": "3D Object Class Detection in the Wild",
    "authors": [
      "Bojan Pepik",
      "Michael Stark",
      "Peter Gehler",
      "Tobias Ritschel",
      "Bernt Schiele"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W07/html/Pepik_3D_Object_Class_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W07/papers/Pepik_3D_Object_Class_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Object class detection has been a synonym for 2D bounding box localization for the longest time, fueled by the success of powerful statistical learning techniques, combined with robust image representations.Only recently, there has been a growing interest in revisiting the promise of computer vision from the early days: to precisely delineate the contents of a visual scene, object by object, in 3D. In this paper, we draw from recent advances in object detection and 2D-3D object lifting in order to design an object class detector that is particularly tailored towards 3D object class detection. Our 3D object class detection method consists of several stages gradually enriching the object detection output with object viewpoint, keypoints and 3D shape estimates. Following careful design, in each stage it constantly improves the performance and achieves state-of the-art performance in simultaneous 2D bounding box and viewpoint estimation on the challenging Pascal3D+ dataset.",
    "code_link": ""
  },
  "cvpr2015_w8_towardsrobustcascadedregressionforfacealignmentinthewild": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Towards Robust Cascaded Regression for Face Alignment in the Wild",
    "authors": [
      "Chengchao Qu",
      "Hua Gao",
      "Eduardo Monari",
      "Jurgen Beyerer",
      "Jean-Philippe Thiran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Qu_Towards_Robust_Cascaded_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Qu_Towards_Robust_Cascaded_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Most state-of-the-art solutions for localizing facial feature landmarks build on the recent success of the cascaded regression framework, which progressively predicts the shape update based on the previous shape estimate and its feature calculation.We revisit several core aspects of this framework and show that proper selection of regression method, local image feature and fine-tuning of further fitting strategies can achieve top performance for face alignment using the generic cascaded regression algorithm. In particular, our strongest model features Iteratively Reweighted Least Squares (IRLS) for training robust regressors in the presence of outliers in the training data, RootSIFT as the image patch descriptor that replaces the original Euclidean distance in SIFT with the Hellinger distance, as well as coarse-to-fine fitting and in-plane pose normalization during shape update.We show the benefit of each proposed improvement by extensive individual experiments compared to the baseline approach on the LFPW dataset. On the currently most challenging 300-W dataset and COFW dataset, we report state-of-the-art results that are superior to or on par with recently published algorithms.",
    "code_link": ""
  },
  "cvpr2015_w8_multi-observationfacerecognitioninvideosbasedonlabelpropagation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Multi-Observation Face Recognition in Videos Based on Label Propagation",
    "authors": [
      "Bogdan Raducanu",
      "Alireza Bosaghzadeh",
      "Fadi Dornaika"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Raducanu_Multi-Observation_Face_Recognition_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Raducanu_Multi-Observation_Face_Recognition_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In order to deal with the huge amount of content generated by social media, especially for indexing and retrieval purposes, the focus shifted from single object recognition to multi-observation object recognition. Of particular interest is the problem of face recognition (used as primary cue for persons' identity assessment), since it is highly required by popular social media search engines like Facebook and Youtube. Recently, several approaches for graph-based label propagation were proposed. However, the associated graphs were constructed in an ad-hoc manner (e.g., using the KNN graph) that cannot cope properly with the rapid and frequent changes in data appearance, a phenomenon intrinsically related with video sequences. In this paper, we propose a novel approach for efficient and adaptive graph construction, based on a two-phase scheme: (i) the first phase is used to adaptively find the neighbors of a sample and also to find the adequate weights for the minimization function of the second phase; (ii) in the second phase, the selected neighbors along with their corresponding weights are used to locally and collaboratively estimate the sparse affinity matrix weights. Experimental results performed on Honda Video Database (HVDB) and a subset of video sequences extracted from the popular TV-series 'Friends' show a distinct advantage of the proposed method over the existing standard graph construction methods.",
    "code_link": ""
  },
  "cvpr2015_w8_exemplarhiddenmarkovmodelsforclassificationoffacialexpressionsinvideos": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Exemplar Hidden Markov Models for Classification of Facial Expressions in Videos",
    "authors": [
      "Karan Sikka",
      "Abhinav Dhall",
      "Marian Bartlett"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Sikka_Exemplar_Hidden_Markov_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Sikka_Exemplar_Hidden_Markov_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Facial expressions are dynamic events comprised of meaningful temporal segments. A common approach to facial expression recognition in video is to first convert variable-length expression sequences into a vector representation by computing summary statistics of image-level features or of spatio-temporal features. These representations are then passed to a discriminative classifier such as a support vector machines (SVM). However, these approaches don't fully exploit the temporal dynamics of facial expressions. Hidden Markov Models (HMMs), provide a method for modeling variable-length expression time-series. Although HMMs have been explored in the past for expression classification, they are rarely used since classification performance is often lower than discriminative approaches, which may be attributed to the challenges of estimating generative models. This paper explores an approach for combining the modeling strength of HMMs with the discriminative power of SVMs via a model-based similarity framework. Each example is first instantiated into an Exemplar-HMM model. A probabilistic kernel is then used to compute a kernel matrix, to be used along with an SVM classifier. This paper proposes that dynamical models such as HMMs are advantageous for the facial expression problem space, when employed in a discriminative, exemplar-based classification framework. The approach yields state-of-the-art results on both posed (CK+ and OULU-CASIA) and spontaneous (FEEDTUM and AM-FED) expression datasets highlighting the performance advantages of the approach.",
    "code_link": ""
  },
  "cvpr2015_w8_usinghankelmatricesfordynamics-basedfacialemotionrecognitionandpaindetection": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Using Hankel Matrices for Dynamics-Based Facial Emotion Recognition and Pain Detection",
    "authors": [
      "Liliana Lo Presti",
      "Marco La Cascia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Presti_Using_Hankel_Matrices_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Presti_Using_Hankel_Matrices_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix.The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted.",
    "code_link": ""
  },
  "cvpr2015_w8_ageandgenderclassificationusingconvolutionalneuralnetworks": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Age and Gender Classification Using Convolutional Neural Networks",
    "authors": [
      "Gil Levi",
      "Tal Hassner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Levi_Age_and_Gender_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Levi_Age_and_Gender_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Automatic age and gender classification has become relevant to an increasing amount of applications, particularly since the rise of social platforms and social media. Nevertheless, performance of existing methods on real-world images is still significantly lacking, especially when compared to the tremendous leaps in performance recently reported for the related task of face recognition. In this paper we show that by learning representations through the use of deep-convolutional neural networks (CNN), a significant increase in performance can be obtained on these tasks. To this end, we propose a simple convolutional net architecture that can be used even when the amount of learning data is limited. We evaluate our method on the recent Adience benchmark for age and gender estimation and show it to dramatically outperform current state-of-the-art methods.",
    "code_link": ""
  },
  "cvpr2015_w8_famefaceassociationthroughmodelevolution": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "FAME: Face Association Through Model Evolution",
    "authors": [
      "Eren Golge",
      "Pinar Duygulu-Sahin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Golge_FAME_Face_Association_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Golge_FAME_Face_Association_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We attack the problem of building classifiers for public faces from web images collected through querying a name. The search results are very noisy even after face detection, with several irrelevant faces corresponding to other people. Moreover, the photographs are taken in the wild with large variety in poses and expressions. We propose a novel method,{\\bf Face Association through Model Evolution (FAME)}, that is able to prune the data in an iterative way, for the models associated to a name to evolve. The idea is based on capturing discriminative and representative properties of each instance and eliminating the outliers. The final models are used to classify faces on novel datasets with different characteristics.On benchmark datasets, our results are comparable to or better than the state-of-the-art studies for the task of face identification. ",
    "code_link": ""
  },
  "cvpr2015_w8_headposeestimationinthewildusingapproximateviewmanifolds": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Head Pose Estimation in the Wild Using Approximate View Manifolds",
    "authors": [
      "Kalaivani Sundararajan",
      "Damon L. Woodard"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Sundararajan_Head_Pose_Estimation_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Sundararajan_Head_Pose_Estimation_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we present a head pose estimation method for unconstrained images using feature-based manifold embedding. The main challenge of manifold embedding methods is to learn a similarity kernel that is reflective of variations only due to head pose and ignore other sources of variation. To address this challenge, we have used the feature correspondences of identity-invariant Geometric Blur features to learn a similarity kernel. To speed up the computation of the similarity kernel, we have used spatial pyramidal matching to approximate feature correspondences and random subsampling of training samples to approximate graph neighborhood. In addition to these approximations, we have used the Nystrom approximation to embed out-of-sample test images in an efficient manner. Using these approximations, an approximate view manifold was learned for 14000 images in the Annotated Facial Landmarks in the Wild (AFLW) dataset. With the learned manifold, head pose estimation was performed on four in-the-wild face datasets - AFLW (remaining 7000 images), AFW, McGill and YouTube Faces. The Approximate View Manifold training achieves a 7X speedup compared to the non-approximated Learning-manifold-in-the-wild approach. Further, pose estimation using the proposed approach shows significant improvement in accuracy and reduced Mean Angular Error(MAE) compared to other methods on the challenging AFLW (7041 images), McGill (6833 images) and YouTube Faces (22534 images) datasets.",
    "code_link": ""
  },
  "cvpr2015_w8_mixtureofpartsrevisitedexpressivepartinteractionsforposeestimation": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Mixture of Parts Revisited: Expressive Part Interactions for Pose Estimation",
    "authors": [
      "Anoop R. Katti",
      "Anurag Mittal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Katti_Mixture_of_Parts_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Katti_Mixture_of_Parts_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Part-based models with restrictive tree-structured interactions for the Human Pose Estimation problem, leave many part interactions unhandled. Two of the most common and strong manifestations of such unhandled interactions are self-occlusion among the parts and the confusion in the localization of the non-adjacent symmetric parts. By handling the self-occlusion in a data efficient manner, we improve the performance of the basic Mixture of Parts model by a large margin, especially on difficult poses. We address the confusion in the symmetric limb localization using a combination of two complementing trees, showing an improvement in the performance on all the parts with a very small trade-off in the running time. Finally, we show that the combination of the two solutions improves the results. We compare our HOG-based method with other methods using similar features and report results equivalent to the best method on two standard datasets with a large reduction in the running time.",
    "code_link": ""
  },
  "cvpr2015_w8_towardsprivacy-preservingactivityrecognitionusingextremelylowtemporalandspatialresolutioncameras": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Towards Privacy-Preserving Activity Recognition Using Extremely Low Temporal and Spatial Resolution Cameras",
    "authors": [
      "Ji Dai",
      "Jonathan Wu",
      "Behrouz Saghafi",
      "Janusz Konrad",
      "Prakash Ishwar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Dai_Towards_Privacy-Preserving_Activity_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Dai_Towards_Privacy-Preserving_Activity_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Although extensive research on action recognition has been carried out using standard video cameras, little work has explored recognition performance at extremely low temporal or spatial camera resolutions. Reliable action recognition in such a \"degraded\" environment would promote the development of privacy-preserving smart rooms that would facilitate intelligent interaction with its occupants while mitigating privacy concerns. This paper aims to explore the trade-off between action recognition performance, number of cameras, and temporal and spatial resolution in a smart-room environment. As it is impractical to build a physical platform to test every combination of camera positions and resolutions, we use a graphics engine (Unity3D) to simulate a room with various avatars animated using motions captured from real subjects with a Kinect v2 sensor. We study the performance impact of spatial resolutions from a single pixel up-to 10 x 10 pixels, the impact of temporal resolutions from 2 Hz up-to 30 Hz and the impact of using up-to 5 ceiling cameras. We found that reliable action recognition for smart-room centric gestures can still occur in environments with extremely low temporal and spatial resolutions. When using 5, single-pixel cameras at 30Hz we achieved a correct classification rate (CCR) of 75.70% across 9 actions, only 13.9% lower than the CCR for the same camera setup at 10 x 10 pixels. We also found that, in terms of the impact on action recognition performance, spatial resolution has the highest impact, followed by number of cameras, and temporal resolution (frame-rate).",
    "code_link": ""
  },
  "cvpr2015_w8_sparsecodingtreeswithapplicationtoemotionclassification": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W8",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Analysis and Modeling of Faces and Gestures",
    "title": "Sparse Coding Trees With Application to Emotion Classification",
    "authors": [
      "Kevin Chen",
      "Marcus Z. Comiter",
      "H. T. Kung",
      "Brad McDanel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/html/Chen_Sparse_Coding_Trees_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W08/papers/Chen_Sparse_Coding_Trees_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We present Sparse Coding trees (SC-trees), a sparse coding-based framework for resolving misclassifications arising when multiple classes map to a common set of features.SC-trees are novel supervised classification trees that use node-specific dictionaries and classifiers to direct input based on classification results in the feature space at each node.We have applied SC-trees to emotion classification of facial expressions.This paper uses this application to illustrate concepts of SC-trees and how they can achieve high performance in classification tasks.When used in conjunction with a nonnegativity constraint on the sparse codes and a method to exploit facial symmetry, SC-trees achieve results comparable with or exceeding the state-of-the-art classification performance on a number of realistic and standard datasets.",
    "code_link": ""
  },
  "cvpr2015_w9_chalearnlookingatpeople2015challengesactionspottingandculturaleventrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "ChaLearn Looking at People 2015 Challenges: Action Spotting and Cultural Event Recognition",
    "authors": [
      "Xavier Baro",
      "Jordi Gonzalez",
      "Junior Fabian",
      "Miguel A. Bautista",
      "Marc Oliu",
      "Hugo Jair Escalante",
      "Isabelle Guyon",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Baro_ChaLearn_Looking_at_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Baro_ChaLearn_Looking_at_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Following previous series on Looking at People (LAP) challenges [7, 6, 5], ChaLearn ran two competitions to be presented at CVPR 2015: action/interaction spotting and cultural event recognition in RGB data. We ran a sec- ond round on human activity recognition on RGB data se- quences. In terms of cultural event recognition, tens of categories have to be recognized. This involves scene un- derstanding and human analysis. This paper summarizes the three performed challenges and obtained results. De- tails of the ChaLearn LAP competitions can be found at http://gesture.chalearn.org/.",
    "code_link": ""
  },
  "cvpr2015_w9_exploringfishervectoranddeepnetworksforactionspotting": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Exploring Fisher Vector and Deep Networks for Action Spotting",
    "authors": [
      "Zhe Wang",
      "Limin Wang",
      "Wenbin Du",
      "Yu Qiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Wang_Exploring_Fisher_Vector_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Wang_Exploring_Fisher_Vector_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": "This paper describes our method and attempt on track 2 at ChaLearn Looking at People(LAP) challenge. Our approach utilizes Fisher vector and iDT features for action spotting, and improve its performance from two aspects: (i) We take account of interaction labels into the training process, (ii) By visualizing our results on validation set, we find that previous method is weak in detecting action class 2, and improve it by introducing multiple thresholds. Moreover, we exploit deep neural networks to extract both appearance and motion representation for this task. However, our current deep networks fails to yield better performance than our Fisher vector based approach and may need further exploration. For this reason, we submit the results obtained by our Fisher vector approach which achieves a Jaccard Index of 0.5385 and ranks the 1st in track 2",
    "code_link": ""
  },
  "cvpr2015_w9_applyingactionattributeclassvalidationtoimprovehumanactivityrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Applying Action Attribute Class Validation to Improve Human Activity Recognition",
    "authors": [
      "David Tahmoush"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Tahmoush_Applying_Action_Attribute_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Tahmoush_Applying_Action_Attribute_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " When learning a new classifier, poor quality training data can significantly degrade performance. Applying selection conditions to the training data can prevent mislabeled, noisy, or damaged data from skewing the classifier. We extend a set of action attributes and apply training case attribute selection conditions to a challenging action recognition dataset. Short-range 3D imagers produce three-dimensional point cloud movies which can be analyzed for structure and motion information like actions. We skeletonize the human point cloud to try to estimate the joint motion, and this produces a significant number of errors as well as damaged and misrepresented cases. By selectively pruning the training cases using the extended action attributes, we improve the classifier performance on some classes by over 5% and improve on the state-of-the-art from 85% accuracy to over 88%.In addition, discovering attribute inconsistencies in the subject actions has provided a reason behind the consistently disappointing performance of multiple algorithms upon the same data. ",
    "code_link": ""
  },
  "cvpr2015_w9_keepitaccurateanddiverseenhancingactionrecognitionperformancebyensemblelearning": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Keep it Accurate and Diverse: Enhancing Action Recognition Performance by Ensemble Learning",
    "authors": [
      "Mohammad Bagheri",
      "Qigang Gao",
      "Sergio Escalera",
      "Albert Clapes",
      "Kamal Nasrollahi",
      "Michael B. Holte",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Bagheri_Keep_it_Accurate_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Bagheri_Keep_it_Accurate_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The performance of different action recognition techniques has recently been studied by several computer vision researchers. However, the potential improvement in classification through classifier fusion by ensemble-based methods has remained unattended. In this work, we evaluate the performance of an ensemble of action learning techniques, each performing the recognition task from a different perspective. The underlying idea is that instead of aiming a very sophisticated and powerful representation/learning technique, we can learn action categories using a set of relatively simple and diverse classifiers, each trained with different feature set. In addition, combining the outputs of several learners can reduce the risk of an unfortunate selection of a learner on an unseen action recognition scenario. This leads to having a more robust and general-applicable framework. In order to improve the recognition performance, a powerful combination strategy is utilized based on the Dempster-Shafer theory, which can effectively make use of diversity of base learners trained on different sources of information. The recognition results of the individual classifiers are compared with those obtained from fusing the classifiers' output, showing enhanced performance of the proposed methodology.",
    "code_link": ""
  },
  "cvpr2015_w9_object-sceneconvolutionalneuralnetworksforeventrecognitioninimages": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Object-Scene Convolutional Neural Networks for Event Recognition in Images",
    "authors": [
      "Limin Wang",
      "Zhe Wang",
      "Wenbin Du",
      "Yu Qiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Wang_Object-Scene_Convolutional_Neural_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Wang_Object-Scene_Convolutional_Neural_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Event recognition from still images is of great importance for image understanding. However, compared with event recognition in videos, there are much fewer research works on event recognition in images. This paper addresses the issue of event recognition from images and proposes an effective method with deep neural networks. Specifically, we design a new architecture, called Object-Scene Convolutional Neural Network (OS-CNN). This architecture is decomposed into object net and scene net, which extract useful information for event understanding from the perspective of objects and scene context, respectively. Meanwhile, we investigate different network architectures for OS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks to the task of event recognition. Furthermore, we find that the deep and very-deep networks are complementary to each other. Finally, based on the pro- posed OS-CNN and comparative study of different network architectures, we come up with a solution of five-stream CNN for the track of cultural event recognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method obtains the performance of 85.5% and ranks the 1st place in this challenge.",
    "code_link": ""
  },
  "cvpr2015_w9_culturaleventrecognitionwithvisualconvnetsandtemporalmodels": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Cultural Event Recognition With Visual ConvNets and Temporal Models",
    "authors": [
      "Amaia Salvador",
      "Matthias Zeppelzauer",
      "Daniel Manchon-Vizuete",
      "Andrea Calafell",
      "Xavier Giro-i-Nieto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Salvador_Cultural_Event_Recognition_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Salvador_Cultural_Event_Recognition_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper presents our contribution to the ChaLearn Challenge 2015 on Cultural Event Classification. The challenge in this task is to automatically classify images from 50 different cultural events. Our solution is based on the combination of visual features extracted from convolutional neural networks with temporal information using a hierarchical classifier scheme. We extract visual features from the last three fully connected layers of both CaffeNet (pretrained with ImageNet) and our fine tuned version for the ChaLearn challenge. We propose a late fusion strategy that trains a separate low-level SVM on each of the extracted neural codes. The class predictions of the low-level SVMs form the input to a higher level SVM, which gives the final event scores. We achieve our best result by adding a temporal refinement step into our classification scheme, which is applied directly to the output of each low-level SVM. Our approach penalizes high classification scores based on visual features when their time stamp does not match well an event-specific temporal distribution learned from the training and validation data. Our system achieved the second best result in theChaLearn Challenge 2015 on Cultural Event Classification with a mean average precision of 0.767 on the test set.",
    "code_link": ""
  },
  "cvpr2015_w9_culturaleventrecognitionbysubregionclassificationwithconvolutionalneuralnetwork": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Cultural Event Recognition by Subregion Classification With Convolutional Neural Network",
    "authors": [
      "Sungheon Park",
      "Nojun Kwak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Park_Cultural_Event_Recognition_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Park_Cultural_Event_Recognition_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, a novel cultural event classification algorithm based on convolutional neural networks is proposed. The proposed method firstly extracts regions that contain meaningful information. Then, convolutional neural networks are trained to classify the extracted regions. The final classification of a scene is performed by combining the classification results of each extracted region of the scene probabilistically. Compared to the state-of-the-art methods for classifying Chalearn Looking at People cultural event recognition database, the proposed methods shows competitive results.",
    "code_link": ""
  },
  "cvpr2015_w9_recognizingculturaleventsinimagesastudyofimagecategorizationmodels": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Recognizing Cultural Events in Images: A Study of Image Categorization Models",
    "authors": [
      "Heeyoung Kwon",
      "Kiwon Yun",
      "Minh Hoai",
      "Dimitris Samaras"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Kwon_Recognizing_Cultural_Events_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Kwon_Recognizing_Cultural_Events_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The goal of this work is to study recognition of cultural events represented in still images. We pose cultural event recognition as an image categorization problem, and we study the performance of several state-of-the-art image categorization approaches, including Spatial Pyramid Matching and Regularized Max Pooling. We consider SIFT and color features as well as the recently proposed CNN features. Experiments on the ChaLearn dataset of 50 cultural events, we find that Regularized Max Pooling with CNN features achieves the best performance.",
    "code_link": ""
  },
  "cvpr2015_w9_articulatedposeestimationwithtinysyntheticvideos": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Articulated Pose Estimation With Tiny Synthetic Videos ",
    "authors": [
      "Dennis Park",
      "Deva Ramanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Park_Articulated_Pose_Estimation_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Park_Articulated_Pose_Estimation_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We address the task of articulated pose estimation from video sequences. We consider an interactive setting where the initial pose is annotated in the first frame. Our system synthesizes a large number of hypothetical scenes with different poses and camera positions by applying geometric deformations to the first frame. We use these synthetic images to generate a custom labeled training set for the video in question. This training data is then used to learn a regressor (for future frames) that predicts joint locations from image data. Notably, our training set is so accurate that nearest-neighbor (NN) matching on low-resolution pixel features works well. As such, we name our underlying representation \"tiny synthetic videos\". We present quantitative results the Friends benchmark dataset that suggests our simple approach matches or exceed state-of-the-art.",
    "code_link": ""
  },
  "cvpr2015_w9_asemanticocclusionmodelforhumanposeestimationfromasingledepthimage": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "A Semantic Occlusion Model for Human Pose Estimation From a Single Depth Image",
    "authors": [
      "Umer Rafi",
      "Juergen Gall",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Rafi_A_Semantic_Occlusion_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Rafi_A_Semantic_Occlusion_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Human pose estimation from depth data has made significant progress in recent years and commercial sensors estimate human poses in real-time. However, state-of-the-art methods fail in many situations when the humans are partially occluded by objects. In this work, we introduce a semantic occlusion model that is incorporated into a regression forest approach for human pose estimation from depth data. The approach exploits the context information of occluding objects like a table to predict the locations of occluded joints. In our experiments on real and synthetic data, we show that our occlusion model increases the joint estimation accuracy and outperforms the commercial Kinect 2 SDK for occluded joints.",
    "code_link": ""
  },
  "cvpr2015_w9_anewretexturingmethodforvirtualfittingroomusingkinect2camera": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "A New Retexturing Method for Virtual Fitting Room Using Kinect 2 Camera",
    "authors": [
      "Andres Traumann",
      "Gholamreza Anbarjafari",
      "Sergio Escalera"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Traumann_A_New_Retexturing_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Traumann_A_New_Retexturing_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This research work proposes a new method for garment retexturing using a single static image along with depth information obtained using the Microsoft Kinect 2 camera. First the garment is segmented out from the image and texture domain coordinates are computed for each pixel of the shirt using 3D information. After that shading is applied on the new colours from the texture image by applying linear stretching of the luminance of the segmented garment. The proposed method is colour and pattern invariant and results in to visually realistic retexturing. The proposed method has been tested on various images and it is shown that it generally performs better and produces more realistic results compared to the state-of-the-art methods. The proposed method can be an application for the virtual fitting room.",
    "code_link": ""
  },
  "cvpr2015_w9_painrecognitionusingspatiotemporalorientedenergyoffacialmuscles": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Pain Recognition Using Spatiotemporal Oriented Energy of Facial Muscles",
    "authors": [
      "Ramin Irani",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Irani_Pain_Recognition_Using_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Irani_Pain_Recognition_Using_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Pain is a critical sign in many medical situations and its automatic detection and recognition using computer vision techniques is of great importance. Utilizes this fact that pain is a spatiotemporal process, the proposed system in this paper employs steerable and separable filters to measures energies released by the facial muscles during the pain process. The proposed system not only detects the pain but recognizes its level. Experimental results on the publicly available pain database of UNBC show promising outcome for automatic pain detection and recognition",
    "code_link": ""
  },
  "cvpr2015_w9_spatiotemporalanalysisofrgb-d-tfacialimagesformultimodalpainlevelrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W9",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Challenge and Workshop on Pose Recovery, Action Recognition, and Cultural Event Recognition",
    "title": "Spatiotemporal Analysis of RGB-D-T Facial Images for Multimodal Pain Level Recognition",
    "authors": [
      "Ramin Irani",
      "Kamal Nasrollahi",
      "Marc O. Simon",
      "Ciprian A. Corneanu",
      "Sergio Escalera",
      "Chris Bahnsen",
      "Dennis H. Lundtoft",
      "Thomas B. Moeslund",
      "Tanja L. Pedersen",
      "Maria-Louise Klitgaard",
      "Laura Petrini"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/html/Irani_Spatiotemporal_Analysis_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W09/papers/Irani_Spatiotemporal_Analysis_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Pain is a vital sign of human health and its automatic detection can be of crucial importance in many different contexts, including medical scenarios. While most available computer vision techniques are based on RGB, in this paper, we investigate the effect of combining RGB, depth, and thermal facial images for pain detection and pain intensity level recognition. For this purpose, we extract energies released by facial pixels using a spatiotemporal filter. Experiments on a group of 12 elderly people applying the multimodal approach show that the proposed method successfully detects pain and recognizes between three intensity levels in 82% of the analyzed frames improving more than 6% over RGB only analysis in similar conditions.",
    "code_link": ""
  },
  "cvpr2015_w10_sparklevisionseeingtheworldthroughrandomspecularmicrofacets": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "SparkleVision: Seeing the World Through Random Specular Microfacets",
    "authors": [
      "Zhengdong Zhang",
      "Phillip Isola",
      "Edward H. Adelson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Zhang_SparkleVision_Seeing_the_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Zhang_SparkleVision_Seeing_the_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we study the problem of reproducing the world lighting from a single image of an object covered with random specular microfacets on the surface. We show that such reflectors can be interpreted as a randomized mapping from the lighting to the image. Such specular objects have very different optical properties from both diffuse surfaces and smooth specular objects like metals, so we design special imaging system to robustly and effectively photograph them. We present simple yet reliable algorithms to calibrate the proposed system and do the inference. We conduct experiments to verify the correctness of our model assumptions and prove the effectiveness of our pipeline.",
    "code_link": ""
  },
  "cvpr2015_w10_efficient3dkernelestimationfornon-uniformcamerashakeremovalusingperpendicularcamerasystem": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Efficient 3D Kernel Estimation for Non-Uniform Camera Shake Removal Using Perpendicular Camera System",
    "authors": [
      "Tao Yue",
      "Jinli Suo",
      "Qionghai Dai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Yue_Efficient_3D_Kernel_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Yue_Efficient_3D_Kernel_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Non-uniform camera shake removal is a knotty problem which plagues the researchers due to the huge computational cost of high-dimensional blur kernel estimation. To address this problem, we propose an acceleration method to compute the 3D projection of 2D local blur kernels fast, and then derive the 3D kernel by interpolating from a minimal set of local blur kernels. Under this scheme, a perpendicular acquisition system is proposed to increase the projection variance for reducing the ill-posedness of 3D kernel estimation. Finally, based on the minimal 3D kernel solver, a RANSAC-based framework is developed to raise the robustness to estimation error of 2D local blur kernels. In experiments, we validate the effectiveness and efficiency of our approach on both synthetic and real captured data, and promising results are obtained.",
    "code_link": ""
  },
  "cvpr2015_w10_reconstruction-freeinferenceoncompressivemeasurements": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Reconstruction-Free Inference on Compressive Measurements",
    "authors": [
      "Suhas Lohit",
      "Kuldeep Kulkarni",
      "Pavan Turaga",
      "Jian Wang",
      "Aswin C. Sankaranarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Lohit_Reconstruction-Free_Inference_on_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Lohit_Reconstruction-Free_Inference_on_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Spatial-multiplexing cameras have emerged as a promising alternative to classical imaging devices, often enabling acquisition of `more for less'. One popular architecture for spatial multiplexing is the so called single-pixel camera, which acquires coded measurements of the scene with pseudo-random spatial masks. Significant theoretical developments over the past few years provide a means for reconstruction of the original imagery with sub-Nyquist sampling. Yet, accurate reconstruction generally requires high measurement rates and high signal-to-noise ratios. In this paper, we enquire if one can perform high-level visual inference problems (e.g. face recognition or action recognition) from compressive cameras without the need for signal reconstruction. This is an interesting question since in many practical scenarios, our goals extend beyond signal reconstruction. However, most inference tasks often require non-linear features and it is not clear how to extract such features directly from compressed measurements. In this paper, we show that one can extract non-trivial correlational features directly without reconstruction of the imagery. As a specific example, we consider the problem of face recognition beyond the visible spectrum e.g in the short-wave infra-red region (SWIR) -- where pixels are expensive. We base our framework on {\\em smashed filters} which suggests that inner-products between high-dimensional signals can be computed in the compressive domain to a high degree of accuracy. We show that one can indeed perform reconstruction-free inference with a very small loss of accuracy at very high compression ratios of 100 and more.",
    "code_link": ""
  },
  "cvpr2015_w10_densesamplingof3dcolortransferfunctionsusinghdrphotography": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Dense Sampling of 3D Color Transfer Functions Using HDR Photography",
    "authors": [
      "Marcel Heinz",
      "Guido Brunnett"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Heinz_Dense_Sampling_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Heinz_Dense_Sampling_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " To apply brightness and color adjustments to projected images,the color transfer function (CTF) of the projector has to be known. We propose a novel approach to determine the CTF using a high sampling density, which is suitable for modern DLP projectors working with color wheels with additional primaries. Our approach is based on the principle of measuring patterns consisting thousands of color samples at once, using a DSLR camera and high dynamic range photography. To ensure high accuracy, additional correction patterns are introduced to compensate for the influence of the dynamic background light caused by displaying the patterns itself. Furthermore, several permutations of the samples in the patterns are captured to address spatial variances of both the projector and the camera. We show that our method achieves comparable accuracy to existing methods, but is one to two orders of magnitude faster. A 64^3 sampling of the CTF can be acquired in a few hours, compared to several weeks that sequential spot measurements would take. Additionally, we demonstrate that a different configuration of our method can be used to capture 17^3 samples extremely fast, indicating the applicability for cases where sparse sampling is sufficient.",
    "code_link": ""
  },
  "cvpr2015_w10_fresnellensimagingwithpost-captureimageprocessing": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Fresnel Lens Imaging With Post-Capture Image Processing",
    "authors": [
      "Artem Nikonorov",
      "Roman Skidanov",
      "Vladimir Fursov",
      "Maksim Petrov",
      "Sergey Bibikov",
      "Yuriy Yuzifovich"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Nikonorov_Fresnel_Lens_Imaging_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Nikonorov_Fresnel_Lens_Imaging_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper describes a unified approach to correct optical distortions in images formed by a Fresnel lens with computational post-processing that opens up new opportunities to use Fresnel lenses in lightweight and inexpensive computer vision devices. Traditional methods of aberration correction do not address artifacts introduced by a Fresnel lens in a systematic way and thus fail to deliver image quality acceptable for generalpurpose color imaging. In our approach, the image is restored using three steps: first, by deblurring the base color channel, then by sharpening other two channels, and finally by applying color correction. Deblurring and sharpening remove significant chromatic aberration and are similar to the restoration technique used for images formed by simple refraction lenses. Color correction stage removes strong color shift caused by energy redistribution between diffraction orders of Fresnel lens. This post-capture processing was tested on real images formed by a four-step approximation of the Fresnel lens manufactured in our optics laboratory.",
    "code_link": ""
  },
  "cvpr2015_w10_videostitchingwithspatial-temporalcontent-preservingwarping": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Video Stitching With Spatial-Temporal Content-Preserving Warping",
    "authors": [
      "Wei Jiang",
      "Jinwei Gu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Jiang_Video_Stitching_With_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Jiang_Video_Stitching_With_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We propose a novel algorithm for stitching multiple synchronized video streams into a single panoramic video with spatial-temporal content-preserving warping. Compared to image stitching, video stitching faces several new challenges including temporal coherence, dominate foreground objects moving across views, and camera jittering. To overcome these issues, the proposed algorithm draws upon ideas from recent local warping methods in image stitching and video stabilization. For video frame alignment, we propose spatial-temporal local warping, which locally aligns frames from different videos while maintaining the temporal consistency.For aligned video frame composition, we find stitching seams with 3D graphcut on overlapped spatial-temporal volumes, where the 3D graph is weighted with object and motion saliency to reduce stitching artifacts.Experimental results show the advantages of the proposed algorithm over several state-of-the-art alternatives, especially in challenging conditions. ",
    "code_link": ""
  },
  "cvpr2015_w10_videocompressivesensingwithon-chipprogrammablesubsampling": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Video Compressive Sensing With On-Chip Programmable Subsampling",
    "authors": [
      "Leonidas Spinoulas",
      "Kuan He",
      "Oliver Cossairt",
      "Aggelos Katsaggelos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Spinoulas_Video_Compressive_Sensing_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Spinoulas_Video_Compressive_Sensing_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The maximum achievable frame-rate for a video camera is limited by the sensor's pixel readout rate. The same sensor may achieve either a slow frame-rate at full resolution (e.g., 60 fps at 4 Mpixel resolution) or a fast frame-rate at low resolution (e.g., 240 fps at 1 Mpixel resolution). Higher frame-rates are achieved using pixel readout modes (e.g., subsampling or binning) that sacrifice spatial for temporal resolution within a fixed bandwidth. A number of compressive video cameras have been introduced to overcome this fixed bandwidth constraint and achieve high frame-rates without sacrificing spatial resolution. These methods use electro-optic components (e.g., LCOS, DLPs, piezo actuators) to introduce high speed spatio-temporal multiplexing in captured images. Full resolution, high speed video is then restored by solving an undetermined system of equations using a sparse regularization framework. In this work, we introduce the first all-digital temporal compressive video camera that uses custom subsampling modes to achieve spatio-temporal multiplexing. Unlike previous compressive video cameras, ours requires no additional optical components, enabling it to be implemented in a compact package such as a mobile camera module. We demonstrate results using a TrueSense development kit with a 12 Mpixel sensor and programmable FPGA read out circuitry.",
    "code_link": ""
  },
  "cvpr2015_w10_fastsingle-frequencytime-of-flightrangeimaging": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "Fast Single-Frequency Time-of-Flight Range Imaging",
    "authors": [
      "Ryan Crabb",
      "Roberto Manduchi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Crabb_Fast_Single-Frequency_Time-of-Flight_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Crabb_Fast_Single-Frequency_Time-of-Flight_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper proposes a solution to the 2-D phase unwrapping problem, inherent to time-of-flight range sensing technology due to the cyclic nature of phase. Our method uses a single frequency capture period to improve frame rate and decrease the presence of motion artifacts encountered in multiple frequency solutions. We present an illumination model that considers intensity image and estimates of the surface normal in addition to the phase image. Considering the number of phase wrap as the 'label', the likelihood of each label is estimated at each pixel, and support for the labeling is shared between pixels throughout the image by Non-Local Cost Aggregation. Comparative experimental results confirm the effectiveness of the proposed approach.",
    "code_link": ""
  },
  "cvpr2015_w10_highspeedsequentialilluminationwithelectronicrollingshuttercameras": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W10",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 4th IEEE International Workshop on Computational Cameras and Displays",
    "title": "High Speed Sequential Illumination With Electronic Rolling Shutter Cameras",
    "authors": [
      "Matis Hudon",
      "Paul Kerbiriou",
      "Arno Schubert",
      "Kadi Bouatouch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/html/Hudon_High_Speed_Sequential_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W10/papers/Hudon_High_Speed_Sequential_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Nowadays flashes are commonly used in photography: they bring light and sharpness to images. It is very tempting to use flash lights in cinema, to take profit of controlled light as photographers may do. But using flashes with video recording is not as easy as in photography. Actually, flashes cause many temporal artifacts in video recordings, especially with high speed CMOS cameras equipped with electronic rolling shutters. This paper proposes a video recording method that uses periodic strobbed illumination sources together with any electronic rolling shutter camera, even without any synchronization device between the camera and the controlled lights. The objective is to avoid recording artifacts by controlling the timings and periods of the flash lights, and then reconstructing images using rows that correspond to the same flash instant. We will show that our method can be easily applied to photometric stereo.",
    "code_link": ""
  },
  "cvpr2015_w11_seamlesschangedetectionandmosaicingforaerialimagery": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater Vehicles",
    "title": "Seamless Change Detection and Mosaicing for Aerial Imagery",
    "authors": [
      "Nimisha T.M",
      "A. N. Rajagopalan",
      "Rangarajan Aravind"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/html/T.M_Seamless_Change_Detection_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/papers/T.M_Seamless_Change_Detection_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The color appearance of an object can vary widely as a function of camera sensitivity and ambient illumination. In this paper, we discuss a methodology for seamless interfacing across imaging sensors and under varying illumination conditions for two very relevant problems in aerial imaging, namely, change detection and mosaicing. The proposed approach works by estimating surface reflectance which is an intrinsic property of the scene and is invariant to both camera and illumination. We advocate SIFT-based feature detection and matching in the reflectance domain followed by registration. We demonstrate that mosaicing and change detection when performed in the high-dimensional reflectance space yields better results as compared to operating in the 3-dimensional color space.",
    "code_link": ""
  },
  "cvpr2015_w11_absolutegeo-localizationthankstohiddenmarkovmodelandexemplar-basedmetriclearning": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater Vehicles",
    "title": "Absolute Geo-Localization Thanks to Hidden Markov Model and Exemplar-Based Metric Learning",
    "authors": [
      "Cedric Le Barz",
      "Nicolas Thome",
      "Matthieu Cord",
      "Stephane Herbin",
      "Martial Sanfourche"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/html/Barz_Absolute_Geo-Localization_Thanks_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/papers/Barz_Absolute_Geo-Localization_Thanks_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper addresses the problem of absolute visual ego-localization of an autonomous vehicle equipped with a monocular camera that has to navigate in an urban environment. The proposed method is based on a combination of: 1) a Hidden Markov Model (HMM) exploiting the spatio-temporal coherency of acquired images and 2) learnt metrics dedicated to robust visual localization in complex scenes, such as streets. The HMM merges odometric measurements and visual similarities computed from specific (local) metrics learnt for each image of the database. To achieve this goal, we define some constraints so that the distance between a database image and a query image representing the same scene is smaller than the distance between this query image and other neighbor images of the database. Successful experiments, conducted using a freely available geo-referenced image database, reveal that the proposed method significantly improves results: the mean localization error is reduced from 12.9m to 3.9m over a 11km path.",
    "code_link": ""
  },
  "cvpr2015_w11_sequencesearchingwithdeep-learntdepthforcondition-andviewpoint-invariantroute-basedplacerecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater Vehicles",
    "title": "Sequence Searching With Deep-Learnt Depth for Condition- and Viewpoint-Invariant Route-Based Place Recognition",
    "authors": [
      "Michael Milford",
      "Chunhua Shen",
      "Stephanie Lowry",
      "Niko Suenderhauf",
      "Sareh Shirazi",
      "Guosheng Lin",
      "Fayao Liu",
      "Edward Pepperell",
      "Cesar Lerma",
      "Ben Upcroft",
      "Ian Reid"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/html/Milford_Sequence_Searching_With_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/papers/Milford_Sequence_Searching_With_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Vision-based localization on robots and vehicles remains unsolved when extreme appearance change and viewpoint change are present simultaneously. The current state of the art approaches to this challenge either deal with only one of these two problems; for example FAB-MAP (viewpoint invariance) or SeqSLAM (appearance-invariance), or use extensive training within the test environment, an impractical requirement in many application scenarios. In this paper we significantly improve the viewpoint invariance of the SeqSLAM algorithm by using state-of-the-art deep learning techniques to generate synthetic viewpoints. Our approach is different to other deep learning approaches in that it does not rely on the ability of the CNN network to learn invariant features, but only to produce \"good enough\" depth images from day-time imagery only. We evaluate the system on a new multi-lane day-night car dataset specifically gathered to simultaneously test both appearance and viewpoint change. Results demonstrate that the use of synthetic viewpoints improves the maximum recall achieved at 100% precision by a factor of 2.2 and maximum recall by a factor of 2.7, enabling correct place recognition across multiple road lanes and significantly reducing the time between correct localizations",
    "code_link": ""
  },
  "cvpr2015_w11_robustandfastdetectionofmovingvehiclesinaerialvideosusingslidingwindows": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater Vehicles",
    "title": "Robust and Fast Detection of Moving Vehicles in Aerial Videos Using Sliding Windows",
    "authors": [
      "Michael Teutsch",
      "Wolfgang Kruger"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/html/Teutsch_Robust_and_Fast_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/papers/Teutsch_Robust_and_Fast_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The detection of vehicles driving on busy urban streets in videos acquired by airborne cameras is challenging due to the large distance between camera and vehicles, simultaneous vehicle and camera motion, shadows, or low contrast due to weak illumination. However, it is an important processing step for applications such as automatic traffic monitoring, detection of abnormal behavior, border protection, or surveillance of restricted areas. In contrast to commonly applied object segmentation methods based on background subtraction or frame differencing, we detect moving vehicles using the combination of a track-before-detect (TBD) approach and machine learning: an AdaBoost classifier learns the appearance of vehicles in low resolution and is applied within a sliding window algorithm to detect vehicles inside a region of interest determined by the TBD approach. Our main contribution lies in the identification, optimization, and evaluation of the most important parameters to achieve both high detection rates and real-time processing.",
    "code_link": ""
  },
  "cvpr2015_w11_drivercellphoneusagedetectiononstrategichighwayresearchprogram(shrp2)faceviewvideos": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W11",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater Vehicles",
    "title": "Driver Cell Phone Usage Detection on Strategic Highway Research Program (SHRP2) Face View Videos",
    "authors": [
      "Keshav Seshadri",
      "Felix Juefei-Xu",
      "Dipan K. Pal",
      "Marios Savvides",
      "Craig P. Thor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/html/Seshadri_Driver_Cell_Phone_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W11/papers/Seshadri_Driver_Cell_Phone_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The harmful effects of cell phone usage on driver behavior have been well investigated and the growing problem has motivated several several research efforts aimed at developing automated cell phone usage detection systems. Computer vision based approaches for dealing with this problem have only emerged in recent years. In this paper, we present a vision based method to automatically determine if a driver is holding a cell phone close to one of his/her ears (thus keeping only one hand on the steering wheel) and quantitatively demonstrate the method's efficacy on challenging Strategic Highway Research Program (SHRP2) face view videos from the head pose validation data that was acquired to monitor driver head pose variation under naturalistic driving conditions. To the best of our knowledge, this is the first such evaluation carried out using this relatively new data. Our approach utilizes the Supervised Descent Method (SDM) based facial landmark tracking algorithm to track the locations of facial landmarks in order to extract a crop of the region of interest. Following this, features are extracted from the crop and are classified using previously trained classifiers in order to determine if a driver is holding a cell phone. We adopt a through approach and benchmark the performance obtained using raw pixels and Histogram of Oriented Gradients (HOG) features in combination with various classifiers.",
    "code_link": ""
  },
  "cvpr2015_w12_on-boardreal-timetrackingofpedestriansonauav": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "On-Board Real-Time Tracking of Pedestrians on a UAV",
    "authors": [
      "Floris De Smedt",
      "Dries Hulens",
      "Toon Goedeme"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Smedt_On-Board_Real-Time_Tracking_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Smedt_On-Board_Real-Time_Tracking_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Recent technical advances in Unmanned Aerial Vehicles (UAV) made a realm of applications possible. In this paper we focus on the application of following a walking pedestrian in real-time, using optimised pedestrian detection and object tracking. For this we use an on-board embedded system, offering an optimal ratio of computational power and weight. We extend the commonly used ground plane estimation technique, used to reduce the search space, based on the sensor data off the UAV. The integration of the ground plane constraint obtains a significant speed-up over the already optimised Aggregate Channel Feature (ACF) detector. To compensate for the frames without detections, we use a particle tracker based on color information. We successfully validated our system on a flying UAV.",
    "code_link": ""
  },
  "cvpr2015_w12_guidanceavisualsensingplatformforroboticapplications": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "Guidance: A Visual Sensing Platform For Robotic Applications",
    "authors": [
      "Guyue Zhou",
      "Lu Fang",
      "Ketan Tang",
      "Honghui Zhang",
      "Kai Wang",
      "Kang Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Zhou_Guidance_A_Visual_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Zhou_Guidance_A_Visual_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Visual sensing, such as vision based localization, navigation, tracking, are crucial for intelligent robots, which have shown great advantage in many robotic applications. However, the market is still in lack of a powerful visual sensing platform to deal with most of the visual processing tasks. In this paper we propose a powerful and efficient platform, Guidance, which is composed of one processor and multiple (up to five) stereo sensing units. Basic visual tasks including visual odometry, obstacle avoidance, depth generation, are given as built-in functions. Additionally, with the aid of a well documented SDK, Guidance is extremely flexible for users to develop other applications, such as autonomous navigation, SLAM, tracking.",
    "code_link": ""
  },
  "cvpr2015_w12_off-the-shelfsensorintegrationformono-slamonsmartdevices": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "Off-the-Shelf Sensor Integration for Mono-SLAM on Smart Devices",
    "authors": [
      "Philipp Tiefenbacher",
      "Timo Schulze",
      "Gerhard Rigoll"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Tiefenbacher_Off-the-Shelf_Sensor_Integration_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Tiefenbacher_Off-the-Shelf_Sensor_Integration_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This work proposes a fusion of inertial measurement units (IMUs) and a visual tracking system on an embedded device. The sensor-to-sensor calibration and the pose estimation are both achieved through an unscented Kalman filter (UKF). Two approaches for a UKF-based pose estimation are presented: The first uses the estimated pose of the visual SLAM system as measurement input for the UKF; The second modifies the motion model of the visual tracking system. Our results show that IMUs increase tracking accuracy even if the visual SLAM system is untouched, while requiring little computational power. Furthermore, an accelerometer-based map scale estimation is presented and discussed.",
    "code_link": ""
  },
  "cvpr2015_w12_retrievinggray-levelinformationfromabinarysensoranditsapplicationtogesturedetection": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "Retrieving Gray-Level Information From a Binary Sensor and Its Application to Gesture Detection",
    "authors": [
      "Orazio Gallo",
      "Iuri Frosio",
      "Leonardo Gasparini",
      "Kari Pulli",
      "Massimo Gottardi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Gallo_Retrieving_Gray-Level_Information_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Gallo_Retrieving_Gray-Level_Information_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We report on the use of a CMOS Contrast-based Binary Vision Sensor (CBVS), with embedded contrast extraction,for gesture detection applications. The first advantage of using this sensor over commercial imagers is a dynamic range of 120dB, made possible by a pixel design that effectively performs auto-exposure control. Another benefit is that, by only delivering the pixels detecting a contrast, the sensor requires a very limited bandwidth.We leverage the sensor's fast 150us readout speed, to perform multiple reads during a single exposure; this allows us to estimate gray-level information from the otherwise binary pixels. As a use case for this novel readout strategy, we selected in-car gesture detection, for which we carried out preliminary tests showing encouraging results.",
    "code_link": ""
  },
  "cvpr2015_w12_recursiveedge-awarefiltersforstereomatching": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "Recursive Edge-Aware Filters for Stereo Matching",
    "authors": [
      "Cevahir Cigla"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Cigla_Recursive_Edge-Aware_Filters_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Cigla_Recursive_Edge-Aware_Filters_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this study, taxonomy of recursive edge-aware filters (REAF) is provided, with the introduction of new approaches to the state-of-the-art. The one tap recursive filters are classified according to recursion rate calculation, recursion type involving normalized-un-normalized recursion and the unification of reverse directions that is also valid for higher order recursions. In that manner, eight types of edge-aware recursive filters are defined, where only three of them are addressed in literature so far. Comprehensive analyses are provided based on computational complexity and filter characteristics which affect the use of such filters for various applications. In order to compare the capabilities of these filters, stereo matching, as the most common application area of edge-aware filters, is considered and extensive experiments are provided through well known datasets. The evaluation is conducted on large number of stereo pairs with independent parameter optimization of each filter providing fair comparison. According to the experimental results, advantages of un-normalized recursion for matching accuracy and sequential integration of reverse directions for execution speed are illustrated as important conclusions for future directions of REAFs.",
    "code_link": ""
  },
  "cvpr2015_w12_areal-timehighdynamicrangehdvideocamera": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "A Real-Time High Dynamic Range HD Video Camera",
    "authors": [
      "Rajesh Narasimha",
      "Umit Batur"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Narasimha_A_Real-Time_High_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Narasimha_A_Real-Time_High_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Standard cameras suffer from low video quality in high dynamic range scenes. In such scenes, parts of the video are either too dark or too bright. This is because lighting is very unevenly distributed in a high dynamic range scene. A standard camera, which typically has a 10 or 12 bit sensor, cannot capture the information both in dark and bright regions with one exposure. High dynamic range (HDR) imaging addresses this problem by fusing information from multiple exposures and rendering a dynamic range compressed result, which is also called dynamic range enhancement (DRE) or tone mapping. DRE is a challenging problem because human perception of brightness/contrast is quite complex and is highly dependent on image content. We present an embedded real-time HDR video camera system that provides 30fps, 720p/1080p high dynamic range video.",
    "code_link": ""
  },
  "cvpr2015_w12_fpgaaccelerationforfeaturebasedprocessingapplications": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "FPGA Acceleration for Feature Based Processing Applications",
    "authors": [
      "Gooitzen Van der Wal",
      "David Zhang",
      "Indu Kandaswamy",
      "Jim Marakowitz",
      "Kevin Kaighn",
      "Joe Zhang",
      "Sek Chai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Wal_FPGA_Acceleration_for_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Wal_FPGA_Acceleration_for_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Feature based vision applications rely on highly efficient extraction and analysis of features from images to reach satisfactory levels of performance and latency. In this paper, we describe the implementation of an algorithm that combines distributed feature detector (D-HCD) with a rotational invariant feature descriptor (R-HOG). Based on an algorithmic comparison with other feature detectors and descriptors, we show that our algorithms have the lowest error rate for 3D aerial scene matching. We present implementation on a low-cost Zynq FPGA that achieves 15x speedup, 5x reduction in latency over a quad core CPU. Our results show the considerable promise of our proposed implementation for fast and efficient robotic and aerial drone / UAV applications.",
    "code_link": ""
  },
  "cvpr2015_w12_locallynon-rigidregistrationformobilehdrphotography": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "Locally Non-Rigid Registration for Mobile HDR Photography",
    "authors": [
      "Orazio Gallo",
      "Alejandro Troccoli",
      "Jun Hu",
      "Kari Pulli",
      "Jan Kautz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Gallo_Locally_Non-Rigid_Registration_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Gallo_Locally_Non-Rigid_Registration_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Image registration for stack-based HDR photography is challenging. If not properly accounted for, camera motion and scene changes result in artifacts in the composite image. Unfortunately, existing methods to address this problem are either accurate, but too slow for mobile devices, or fast, but prone to failing. We propose a method that fills this void: our approach is extremely fast---under 700ms on a commercial tablet for a pair of 5MP images---and prevents the artifacts that arise from insufficient registration quality.",
    "code_link": ""
  },
  "cvpr2015_w12_real-timeembeddedageandgenderclassificationinunconstrainedvideo": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "Real-Time Embedded Age and Gender Classification in Unconstrained Video",
    "authors": [
      "Ramin Azarmehr",
      "Robert Laganiere",
      "Won-Sook Lee",
      "Christina Xu",
      "Daniel Laroche"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Azarmehr_Real-Time_Embedded_Age_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Azarmehr_Real-Time_Embedded_Age_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we present a complete framework for video-based age and gender classification which performs accurately on embedded systems in real-time and under unconstrained conditions. We propose a segmental dimensionality reduction technique using Enhanced Discriminant Analysis (EDA) to reduce the memory requirements up to 99.5%. A non-linear Support Vector Machine (SVM) along with a discriminative demographics classification strategy is exploited to improve both accuracy and performance. Also, we introduce novel improvements for face alignment and illumination normalization in unconstrained environments. Our cross-database evaluations demonstrate competitive recognition rates compared to the resource-demanding state-of-the-art approaches.",
    "code_link": ""
  },
  "cvpr2015_w12_fpga-basedpedestriandetectionunderstrongdistortions": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W12",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - The Eleventh IEEE Embedded Vision Workshop",
    "title": "FPGA-Based Pedestrian Detection Under Strong Distortions",
    "authors": [
      "Daniele Tasson",
      "Alessio Montagnini",
      "Roberto Marzotto",
      "Michela Farenzena",
      "Marco Cristani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/html/Tasson_FPGA-Based_Pedestrian_Detection_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W12/papers/Tasson_FPGA-Based_Pedestrian_Detection_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Pedestrian detection is one of the most popular computer vision challenges in the automotive, security and domotics industries, with several new approaches and benchmarks proposed every year. All of them typically consider the pedestrians in a standing pose, but this assumption is not always applicable. It is the case of embedded camera systems used for crowd monitoring or in driving assistance systems for big vehicles maneuvering. Such systems are commonly installed as higher as possible and make use of fish-eye lenses to provide a top and wide field of view. Actually, such configurations introduce both perspective and optical distortions in the image that, even when corrected, still provide stretched silhouettes that can hardly be detected by cutting-edge pedestrian detection algorithms. In this paper we focus on this scenario, showing a) that one of the most effective models for pedestrian detection, that is the Deformable Part Model (DPM), can be efficiently implemented in FPGA to dramatically speed up the computation, and b) how it can be modified for dealing with highly distorted pictures of humans. The resulting framework, dubbed Deformable Part Model for Local Spatial Deformations (DPM-LSD), gives convincing figure of merits in terms of accuracy and throughput, on a new top-view fish-eye based pedestrian dataset (dubbed Fish-Eyed Pedestrians), also comparing with widely-used competitors (standard DPM and Dalal-Triggs).",
    "code_link": ""
  },
  "cvpr2015_w13_semanticsegmentationofurbanscenesbylearninglocalclassinteractions": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Semantic Segmentation of Urban Scenes by Learning Local Class Interactions",
    "authors": [
      "Michele Volpi",
      "Vittorio Ferrari"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Volpi_Semantic_Segmentation_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Volpi_Semantic_Segmentation_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Traditionally, land-cover mapping from remote sensing images is performed by classifying each atomic region in the image in isolation and by enforcing simple smoothing priors via random fields models as two independent steps. In this paper, we propose to model the segmentation problem by a discriminatively trained Conditional Random Field (CRF). To this end, we employ Structured Support Vector Machines (SSVM) to learn the weights of an informative set of appearance descriptors jointly with local class interactions. We propose a principled strategy to learn pairwise potentials encoding local class preferences from sparsely annotated ground truth. We show that this approach outperform standard baselines and more expressive CRF models, improving by 4-6 points the average class accuracy on a challenging dataset involving urban high resolution satellite imagery.",
    "code_link": ""
  },
  "cvpr2015_w13_activelearningapproachtodetectingstandingdeadtreesfromalspointcloudscombinedwithaerialinfraredimagery": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Active Learning Approach to Detecting Standing Dead Trees From ALS Point Clouds Combined With Aerial Infrared Imagery",
    "authors": [
      "Przemyslaw Polewski",
      "Wei Yao",
      "Marco Heurich",
      "Peter Krzystek",
      "Uwe Stilla"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Polewski_Active_Learning_Approach_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Polewski_Active_Learning_Approach_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Due to their role in certain essential forest processes, dead trees are an interesting object of study within the environmental and forest sciences. This paper describes an active learning-based approach to detecting individual standing dead trees, known as snags, from ALS point clouds and aerial color infrared imagery. We first segment individual trees within the 3D point cloud and subsequently find an approximate bounding polygon for each tree within the image. We utilize these polygons to extract features based on the pixel intensity values in the visible and infrared bands, which forms the basis for classifying the associated trees as either dead or living. We define a two-step scheme of selecting a small subset of training examples from a large initially unlabeled set of objects. In the first step, a greedy approximation of the kernelized feature matrix is conducted, yielding a smaller pool of the most representative objects. We then perform active learning on this moderate-sized pool, using expected error reduction as the basic method. We explore how the use of semi-supervised classifiers with minimum entropy regularizers can benefit the learning process. Based on validation with reference data manually labeled on images from the Bavarian Forest National Park, our method attains an overall accuracy of up to 89% with less than 100 training examples, which corresponds to 10% of the pre-selected data pool.",
    "code_link": ""
  },
  "cvpr2015_w13_universalityofwavelet-basednon-homogeneoushiddenmarkovchainmodelfeaturesforhyperspectralsignatures": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Universality of Wavelet-Based Non-Homogeneous Hidden Markov Chain Model Features for Hyperspectral Signatures",
    "authors": [
      "Siwei Feng",
      "Marco F. Duarte",
      "Mario Parente"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Feng_Universality_of_Wavelet-Based_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Feng_Universality_of_Wavelet-Based_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Feature design is a crucial step in many hyperspectral signal processing applications like hyperspectral signature classification and unmixing, etc. In this paper, we describe a technique for automatically designing universal features of hyperspectral signatures. Universality is considered both in terms of the application to a multitude of classification problems and in terms of the use of specific vs. generic training datasets. The core component of our feature design is to use a non-homogeneous hidden Markov chain (NHMC) to characterize wavelet coefficients which capture the spectrum semantics (i.e., structural information) at multiple levels. Results of our simulation experiments show that the designed features meet our expectation in terms of universality.",
    "code_link": ""
  },
  "cvpr2015_w13_asemi-supervisedapproachforice-waterclassificationusingdual-polarizationsarsatelliteimagery": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "A Semi-Supervised Approach for Ice-Water Classification Using Dual-Polarization SAR Satellite Imagery",
    "authors": [
      "Fan Li",
      "David A. Clausi",
      "Lei Wang",
      "Linlin Xu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Li_A_Semi-Supervised_Approach_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Li_A_Semi-Supervised_Approach_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The daily interpretation of SAR sea ice imagery is very important for ship navigation and climate monitoring. Currently, the interpretation is still performed manually by ice analysts due to the complexity of data and the difficulty of creating fine-level ground truth. To overcome these problems, a semi-supervised approach for ice-water classification based on self-training is presented. The proposed algorithm integrates the spatial context model, region merging, and the self-training technique into a single framework. The backscatter intensity, texture, and edge strength features are incorporated in a CRF model using multi-modality Gaussian model as its unary classifier. Region merging is used to build a hierarchical data-adaptive structure to make the inference more efficient. Self-training is concatenated with region merging, so that the spatial location information of the original training samples can be used. Our algorithm has been tested on a large-scale RADARSAT-2 dual-polarization dataset over the Beaufort and Chukchi sea, and the classification results are significantly better than the supervised methods without self-training.",
    "code_link": ""
  },
  "cvpr2015_w13_effectivesemanticpixellabellingwithconvolutionalnetworksandconditionalrandomfields": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Effective Semantic Pixel Labelling With Convolutional Networks and Conditional Random Fields",
    "authors": [
      "Sakrapee Paisitkriangkrai",
      "Jamie Sherrah",
      "Pranam Janney",
      "Anton Van-Den Hengel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Paisitkriangkrai_Effective_Semantic_Pixel_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Paisitkriangkrai_Effective_Semantic_Pixel_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Large amounts of available training data and increasing computing power have led to the recent success of deepconvolutional neural networks (CNN) on a large number of applications. In this paper, we propose an effective semantic pixel labelling using CNN features, hand-crafted features and Conditional Random Fields (CRFs). Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. The CRF infers a labelling that smooths regions while respecting the edges present in the imagery. The method is applied to the ISPRS 2D semantic labelling challenge dataset with competitive classification accuracy.",
    "code_link": ""
  },
  "cvpr2015_w13_dodeepfeaturesgeneralizefromeverydayobjectstoremotesensingandaerialscenesdomains?": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Do Deep Features Generalize From Everyday Objects to Remote Sensing and Aerial Scenes Domains?",
    "authors": [
      "Otavio A. B. Penatti",
      "Keiller Nogueira",
      "Jefersson A. dos Santos"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Penatti_Do_Deep_Features_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Penatti_Do_Deep_Features_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we evaluate the generalization power of deep features (ConvNets) in two new scenarios: aerial and remote sensing image classification. We evaluate experimentally ConvNets trained for recognizing everyday objects for the classification of aerial and remote sensing images. ConvNets obtained the best results for aerial images, while for remote sensing, they performed well but were outperformed by low-level color descriptors, such as BIC. We also present a correlation analysis, showing the potential for combining/fusing different ConvNets with other descriptors or even for combining multiple ConvNets. A preliminary set of experiments fusing ConvNets obtains state-of-the-art results for the well-known UCMerced dataset.",
    "code_link": ""
  },
  "cvpr2015_w13_matchingpersistentscattererstoopticalobliqueimages": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Matching Persistent Scatterers to Optical Oblique Images",
    "authors": [
      "Lukas Schack",
      "Uwe Soergel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Schack_Matching_Persistent_Scatterers_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Schack_Matching_Persistent_Scatterers_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Persistent Scatterer Interferometry is a well established method for subsidence monitoring of buildings especially in urban areas. Even though the very high resolution of current SAR missions allow for ground resolutions of some decimeters, the assignment of Persistent Scatterers to single parts of buildings is not well investigated yet. We present a new approach how to incorporate optical oblique imagery to assign Persistent Scatterers to their presumed correspondences in the optical data in order to establish a link between Persistent Scatterers and single structures of buildings. This is a crucial step for advanced subsidence monitoring in urban ares. The centerpiece of the presented work is a measure which quantifies the quality of the bipartite matching between single Persistent Scatterers and their correspondences at a regular lattice in the optical data. The applicability of our approach is presented in two exemplary case studies.",
    "code_link": ""
  },
  "cvpr2015_w13_simultaneousregistrationandchangedetectioninmultitemporal,veryhighresolutionremotesensingdata": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Simultaneous Registration and Change Detection in Multitemporal, Very High Resolution Remote Sensing Data",
    "authors": [
      "Maria Vakalopoulou",
      "Konstantinos Karantzalos",
      "Nikos Komodakis",
      "Nikos Paragios"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Vakalopoulou_Simultaneous_Registration_and_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Vakalopoulou_Simultaneous_Registration_and_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In order to exploit the currently continuous streams of massive, multi-temporal, high-resolution remote sensing datasets there is an emerging need to address efficiently the image registration and change detection challenges. To this end, in this paper we propose a modular, scalable, metric free single shot change detection/registrationmethod. The approach exploits a decomposed interconnected graphical model formulation where registration similarity constraints are relaxed in the presence of change detection. The deformation space is discretized, while efficient linear programming and duality principles are used to optimize a joint solution space where local consistency is imposed on the deformation and the detection space. Promising results onlarge scale experiments demonstrate the extreme potentials of our method.",
    "code_link": ""
  },
  "cvpr2015_w13_onthelocationdependenceofconvolutionalneuralnetworkfeatures": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "On the Location Dependence of Convolutional Neural Network Features",
    "authors": [
      "Scott Workman",
      "Nathan Jacobs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Workman_On_the_Location_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Workman_On_the_Location_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " As the availability of geotagged imagery has increased, so has the interest in geolocation-related computer vision applications, ranging fromwide-area image geolocalization to the extraction of environmental data from social network imagery. Encouraged by the recent success of deep convolutional networks for learning high-level features, we investigate the usefulness of deep learned features for such problems. We compare features extracted from various layers of convolutional neural networks and analyze their discriminative ability with regards to location. Our analysis spans several problem settings, including region identification, visualizing land cover in aerial imagery, and ground-image localization in regions without ground-image reference data (where we achieve state-of-the-art performance on a benchmark dataset). We present results on multiple datasets, including a new dataset we introduce containing hundreds of thousands of ground-level and aerial images in a large region centered around San Francisco.",
    "code_link": ""
  },
  "cvpr2015_w13_oilspillcandidatedetectionfromsarimageryusingathresholding-guidedstochasticfully-connectedconditionalrandomfieldmodel": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Oil Spill Candidate Detection From SAR Imagery Using a Thresholding-Guided Stochastic Fully-Connected Conditional Random Field Model",
    "authors": [
      "Linlin Xu",
      "M. Javad Shafiee",
      "Alex Wong",
      "Fan Li",
      "Lei Wang",
      "David Clausi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Xu_Oil_Spill_Candidate_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Xu_Oil_Spill_Candidate_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " The detection of marine oil spill candidate from synthetic aperture radar (SAR) images is largely hampered by SAR speckle noise and the complex marine environment. In this paper, we develop a thresholding-guided stochastic fully-connected conditional random field (TGSFCRF) model for inferring the binary label from SAR imagery. First, an intensity thresholding approach is used to estimate the initial labels of oil spill candidates and the background. Second, a Gaussian mixture model (GMM) is trained using all the pixels based on the initial labels. Last, based on the GMM model, a graph-cut optimization approach is used for inferring the final labels. By using a threholding-guided approach, TGSFCRF can exploit the statistical characteristics of the two classes for better label inference. Moreover, by using a stochastic clique approach, TGSFCRF efficiently addresses the global-scale spatial correlation effect, and thereby can better resist the influence of SAR speckle noise and background heterogeneity. Experimental results on RADARSAT-1 ScanSAR imagery demonstrate that TGSFCRF can accurately delineate oil spill candidates without committing too much false alarms.",
    "code_link": ""
  },
  "cvpr2015_w13_detectionofincompleteenclosuresofrectangularshapeinremotelysensedimages": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W13",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - Looking from Above: When Earth Observation Meets Vision",
    "title": "Detection of Incomplete Enclosures of Rectangular Shape in Remotely Sensed Images",
    "authors": [
      "Igor Zingman",
      "Dietmar Saupe",
      "Karsten Lambers"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/html/Zingman_Detection_of_Incomplete_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W13/papers/Zingman_Detection_of_Incomplete_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We develop an approach for detection of ruins of livestock enclosures in alpine areas captured by high-resolution remotely sensed images. These structures are usually of approximately rectangular shape and appear in images as faint fragmented contours in complex background. We address this problem by introducing a new rectangularity feature that quantifies the degree of alignment of an optimal subset of extracted linear segments with a contour of rectangular shape. The rectangularity feature has high values not only for perfect enclosures, but also for broken ones with distorted angles, fragmented walls, or even a completely missing wall. However, it has zero value for spurious structures withless than three sides of a perceivable rectangle. Performance analysis using large imagery of an alpine environment is provided. We show how the detection performance can be improved by learning from only a few representative examples and a large number of negatives. ",
    "code_link": ""
  },
  "cvpr2015_w14_geometricinpaintingof3dstructures": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 2nd Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Geometric Inpainting of 3D Structures",
    "authors": [
      "Pratyush Sahay",
      "A. N. Rajagopalan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/html/Sahay_Geometric_Inpainting_of_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/papers/Sahay_Geometric_Inpainting_of_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper, we addressthe problem ofinpainting in 3D digital models with large holes. The missing region inference problem is solved with a dictionary learning-based method that harnesses a geometric prior derived from a single self-similar structure and online depth databases. The underlying surface is recovered by adaptively propagating local 3D surface smoothness fromaround the boundary of the hole by appropriately harvesting the cue provided by the geometric prior. We showcase the relevance of our method in the archaeological domain which warrants 'filling-in' missing information in damaged heritage sites. The performance of our method is demonstrated on holes with different complexities and sizes on synthetic as well as real examples.",
    "code_link": ""
  },
  "cvpr2015_w14_real-timenon-rigidmulti-framedepthvideosuper-resolution": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 2nd Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Real-Time Non-Rigid Multi-Frame Depth Video Super-Resolution",
    "authors": [
      "Kassem Al Ismaeil",
      "Djamila Aouada",
      "Thomas Solignac",
      "Bruno Mirbach",
      "Bjorn Ottersten"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/html/Ismaeil_Real-Time_Non-Rigid_Multi-Frame_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/papers/Ismaeil_Real-Time_Non-Rigid_Multi-Frame_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper proposes to enhance low resolution dynamic depth videos containing freely non-rigidly moving objects with a new dynamic multi-frame super-resolution algorithm. Existent methods are either limited to rigid objects, or restricted to global lateral motions discarding radial displacements. We address these shortcomings by accounting for non-rigid displacements in 3D. In addition to 2D optical flow, we estimate the depth displacement, and simultaneously correct the depth measurement by Kalman filtering. This concept is incorporated efficiently in a multi-frame super-resolution framework. This is formulated in a recursive manner that ensures an efficient deployment in real-time. Results show the overall improved performance of the proposed method as compared to alternative approaches, specifically in handling relatively large 3D motions. Test examples range from a full moving human body to a highly dynamic facial video with varying expressions.",
    "code_link": ""
  },
  "cvpr2015_w14_accuratelocalizationbyfusingimagesandgpssignals": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 2nd Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Accurate Localization by Fusing Images and GPS Signals",
    "authors": [
      "Kumar Vishal",
      "C. V. Jawahar",
      "Visesh Chari"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/html/Vishal_Accurate_Localization_by_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/papers/Vishal_Accurate_Localization_by_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Localization in 3D is an important problem with wide ranging applications from autonomous navigation in robotics to location specific services on mobile devices. GPS sensors are a commercially viable option for localization, and are ubiquitous in their use, especially in portable de- vices. With the proliferation of mobile cameras however, maturing localization algorithms based on computer vision are emerging as a viable alternative. Although both vision and GPS based localization algorithms have many limita- tions and inaccuracies, there are some interesting compli- mentarities in their success/failure scenarios that justify an investigation into their joint utilization. Such investigations are further justified considering that many of the modern wearable and mobile computing devices come with sensors for both GPS and vision. In this work, we investigate approaches to reinforce GPS localization with vision algorithms and vice versa. Specif- ically, we show how noisy GPS signals can be rectified by vision based localization of images captured in the vicin- ity. Alternatively, we also show how GPS readouts might be used to disambiguate images when they are visually similar looking but belong to different places. Finally, we empiri- cally validate our solutions to show that fusing both these approaches can result in a more accurate and reliable lo- calization of videos captured with a Contour action camera, over a 600 meter long path, over 10 different days. ",
    "code_link": ""
  },
  "cvpr2015_w14_exploitingglobalpriorsforrgb-dsaliencydetection": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 2nd Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Exploiting Global Priors for RGB-D Saliency Detection",
    "authors": [
      "Jianqiang Ren",
      "Xiaojin Gong",
      "Lu Yu",
      "Wenhui Zhou",
      "Michael Ying Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/html/Ren_Exploiting_Global_Priors_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/papers/Ren_Exploiting_Global_Priors_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Inspired by the effectiveness of global priors for 2D saliency analysis, this paper aims to explore those particular to RGB-D data. To this end, we propose two priors, which are the normalized depth prior and the global-context surface orientation prior, and formulate them in the forms simple for computation. A two-stage RGB-D salient object detection framework is presented. It first integrates the region contrast, together with the background, depth, and orientation priors to achieve a saliency map. Then, a saliency restoration scheme is proposed, which integrates the PageRank algorithm for sampling high confident regions and recovers saliency for those ambiguous. The saliency map is thus reconstructed and refined globally. We conduct comparative experiments on two publicly available RGB-D datasets. Experimental results show that our approach consistently outperforms other state-of-the-art algorithms on both datasets.",
    "code_link": ""
  },
  "cvpr2015_w14_sparsere-idblocksparsityforpersonre-identification": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W14",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - 2nd Joint Workshop on Multi-Sensor Fusion for Dynamic Scene Understanding",
    "title": "Sparse Re-Id: Block Sparsity for Person Re-Identification",
    "authors": [
      "Srikrishna Karanam",
      "Yang Li",
      "Richard J. Radke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/html/Karanam_Sparse_Re-Id_Block_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W14/papers/Karanam_Sparse_Re-Id_Block_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " This paper presents a novel approach to solve the problem of person re-identification in non-overlapping camera views. We hypothesize that the feature vector of a probe image approximately lies in the linear span of the corresponding gallery feature vectors in a learned embedding space. We then formulate the re-identification problem as a block sparse recovery problem and solve the associated optimization problem using the alternating directions framework. We evaluate our approach on the publicly available PRID 2011 and iLIDS-VID multi-shot re-identification datasets and demonstrate superior performance in comparison with the current state of the art.",
    "code_link": ""
  },
  "cvpr2015_w15_handgesturerecognitionwith3dconvolutionalneuralnetworks": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Observing and Understanding Hands in Action",
    "title": "Hand Gesture Recognition With 3D Convolutional Neural Networks",
    "authors": [
      "Pavlo Molchanov",
      "Shalini Gupta",
      "Kihwan Kim",
      "Jan Kautz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/html/Molchanov_Hand_Gesture_Recognition_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/papers/Molchanov_Hand_Gesture_Recognition_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Touchless hand gesture recognition systems are becoming important in automotive user interfaces as they improve safety and comfort. Various computer vision algorithms have employed color and depth cameras for hand gesture recognition, but robust classification of gestures from different subjects performed under widely varying lighting conditions is still challenging. We propose an algorithm for drivers' hand gesture recognition from challenging depth and intensity data using 3D convolutional neural networks. Our solution combines information from multiple spatial scales for the final prediction. It also employs spatio-temporal data augmentation for more effective training and to reduce potential overfitting. Our method achieves a correct classification rate of 77.5\\% on the VIVA challenge dataset.",
    "code_link": ""
  },
  "cvpr2015_w15_hierarchicalparticlefilteringfor3dhandtracking": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Observing and Understanding Hands in Action",
    "title": "Hierarchical Particle Filtering for 3D Hand Tracking",
    "authors": [
      "Alexandros Makris",
      "Nikolaos Kyriazis",
      "Antonis A. Argyros"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/html/Makris_Hierarchical_Particle_Filtering_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/papers/Makris_Hierarchical_Particle_Filtering_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We present a fast and accurate 3D hand tracking method which relies on RGB-D data. The method follows a model based approach using a hierarchical particle filter variant to track the model's state. The filter estimates the probability density function of the state's posterior. As such, it has increased robustness to observation noise and compares favourably to existing methods that can be trapped in local minima resulting in track loses. The data likelihood term is calculated by measuring the discrepancy between the rendered 3D model and the observations. Extensive experiments with real and simulated data show that hand tracking is achieved at a frame rate of90fps with less that 10mm average error using a GPU implementation, thus comparing favourably tothe state of the art in terms of both speed and tracking accuracy.",
    "code_link": ""
  },
  "cvpr2015_w15_on-the-flyhanddetectiontrainingwithapplicationinegocentricactionrecognition": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Observing and Understanding Hands in Action",
    "title": "On-the-Fly Hand Detection Training With Application in Egocentric Action Recognition",
    "authors": [
      "Jayant Kumar",
      "Qun Li",
      "Survi Kyal",
      "Edgar A. Bernal",
      "Raja Bala"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/html/Kumar_On-the-Fly_Hand_Detection_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/papers/Kumar_On-the-Fly_Hand_Detection_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " We propose a novel approach to segment hand regions in egocentric video that requires no manual labeling of training samples. The user wearing a head-mounted camera is prompted to perform a simple gesture during an initial calibration step. A combination of color and motion analysis that exploits knowledge of the expected gesture is applied on the calibration video frames to automatically label hand pixels in an unsupervised fashion. The hand pixels identified in this manner are used to train a statistical-model based hand detector. Superpixel region growing is used to perform segmentation refinement and improve robustness to noise. Experiments show that our hand detection technique based on the proposed on the-fly training approach significantly outperforms state-of the-art techniques with respect to accuracy and robustness on a variety of challenging videos. This is due primarily to the fact that training samples are personalized to a specific user and environmental conditions. We also demonstrate the utility of our hand detection technique to inform an adaptive video sampling strategy that improves both computational speed and accuracy of egocentric action recognition algorithms. Finally, we offer an egocentric video dataset of an insulin self-injection procedure with action labels and hand masks that can serve towards future research on both hand detection and egocentric action recognition.",
    "code_link": ""
  },
  "cvpr2015_w15_icpikinversekinematicsbasedarticulated-icp": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Observing and Understanding Hands in Action",
    "title": "ICPIK: Inverse Kinematics Based Articulated-ICP",
    "authors": [
      "Shachar Fleishman",
      "Mark Kliger",
      "Alon Lerner",
      "Gershom Kutliroff"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/html/Fleishman_ICPIK_Inverse_Kinematics_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/papers/Fleishman_ICPIK_Inverse_Kinematics_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " In this paper we address the problem of matching a kinematic model of an articulated body to a point cloud obtained from a consumer grade 3D sensor. We present the ICPIK algorithm - an Articulated Iterative Closest Point algorithm based on a solution to the Inverse Kinematic problem. The main virtue of the presented algorithm is its computational efficiency, achieved by relying on inverse-kinematics framework for analytical derivation of the Jacobian matrix, and the enforcement of kinematic constraints. We demonstrate the performance of the ICPIK algorithm by integrating it into a real-time hand tracking system.The presented algorithm achieves similar accuracy as state of the art methods, while significantly reducing computation time.",
    "code_link": ""
  },
  "cvpr2015_w15_miningdiscriminativestatesofhandsandobjectstorecognizeegocentricactionswithawearablergbdcamera": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Observing and Understanding Hands in Action",
    "title": "Mining Discriminative States of Hands and Objects to Recognize Egocentric Actions With a Wearable RGBD Camera",
    "authors": [
      "Shaohua Wan",
      "J.K. Aggarwal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/html/Wan_Mining_Discriminative_States_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/papers/Wan_Mining_Discriminative_States_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " Of increasing interest to the computer vision community is to recognize egocentric actions. Conceptually, egocentric actions are largely identifiable by the states of hands and objects. For example, \"drinking soda\" is essentially composed of two sequential states where one first \"opens the bottle\", then \"drinks from the bottle\". While existing algorithms commonly use manually defined states to train action classifiers, we present a novel model that automatically mines discriminative states for recognizing egocentric actions. To mine discriminative states, we propose a novel kernel function and formulate a Multiple Kernel Learning based framework to learn adaptive weights for different states. Our state model demonstrates significant performance improvement over the state-of-the-art methods on 3 benchmark datasets, i.e., RGBD-Ego, ADL, and GTEA.",
    "code_link": ""
  },
  "cvpr2015_w15_americansignlanguagealphabetrecognitionusingmicrosoftkinect": {
    "conf_id": "CVPR2015",
    "conf_sub_id": "W15",
    "is_workshop": true,
    "conf_name": "CVPR2015_workshops - IEEE Computer Society Workshop on Observing and Understanding Hands in Action",
    "title": "American Sign Language Alphabet Recognition Using Microsoft Kinect",
    "authors": [
      "Cao Dong",
      "Ming C. Leu",
      "Zhaozheng Yin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/html/Dong_American_Sign_Language_2015_CVPR_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2015_workshops/../content_cvpr_workshops_2015/W15/papers/Dong_American_Sign_Language_2015_CVPR_paper.pdf",
    "published": "2015-06",
    "summary": " American Sign Language (ASL) alphabet recognition using marker-less vision sensors is a challenging task due to the complexity of ASL alphabet signs, self-occlusion of the hand, and limited resolution of the sensors. This paper describes a new method for ASL alphabet recognition using a low-cost depth camera, which is Microsoft's Kinect. A segmented hand configuration is first obtained by using a depth contrast feature based per-pixel classification algorithm. Then, a hierarchical mode-seeking method is developed and implemented to localize hand joint positions under kinematic constraints. Finally, a Random Forest (RF) classifier is built to recognize ASL signs using the joint angles. To validate the performance of this method, we used a publicly available dataset from Surrey University. The results have shown that our method can achieve above 90% accuracy in recognizing 24 static ASL alphabet signs, which is significantly higher in comparison to the previous benchmarks.",
    "code_link": ""
  }
}