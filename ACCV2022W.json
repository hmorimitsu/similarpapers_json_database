{
  "accv2022_lldfa_fapnfacealignmentpropagationnetworkforfacevideosuper-resolution": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "LLDFA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Learning with Limited Data for Face Analysis",
    "title": "FAPN: Face Alignment Propagation Network for Face Video Super-Resolution",
    "authors": [
      "Sige Bian",
      "He Li",
      "Feng Yu",
      "Jiyuan Liu",
      "Song Changjun",
      "Yongming Tang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/LLDFA/html/Bian_FAPN_Face_Alignment_Propagation_Network_for_Face_Video_Super-Resolution_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/LLDFA/papers/Bian_FAPN_Face_Alignment_Propagation_Network_for_Face_Video_Super-Resolution_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Face video super-resolution (FVSR) aims to use continuous low resolution (LR) video frames to reconstruct face and recover facial details under the premise of ensuring authenticity. The existing video super-resolution (VSR) technology usually uses inter-frame information to achieve better super-resolution (SR) performance. However, due to the complex temporal dependence between frames, as the number of input frames increases, the information cannot be fully utilized, and even wrong information is introduced, resulting in poor performance. In this work, we propose an alignment propagation network for accumulating facial prior information (FAPN). We design a neighborhood information coupling (NIC) module based on optical flow estimation and alignment, where the current frame, the adjacent frames and the SR results of the previous frame are locally fused. The coupled frames are sent to a unidirectional propagation (UP) structure for propagation. Meanwhile, in the UP structure, the facial prior information is filtered and accumulated in the face super-resolution cell (FSRC), and the high-dimensional hidden state is introduced to propagate effective temporal information between frames along the unidirectional structure. Extensive evaluations and comparisons validate the strengths of our approach, FAPN can accumulate more facial details while ensuring the authenticity of the face. And the experimental results demonstrated that the proposed framework achieves better performance on PSNR (up to 0.31 dB), SSIM (up to 0.15 dB) and face recognition accuracy (up to 1.99%) compared with state-of-the-art methods."
  },
  "accv2022_lldfa_micro-expressionrecognitionusingashallowconvlstm-basednetwork": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "LLDFA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Learning with Limited Data for Face Analysis",
    "title": "Micro-expression recognition using a shallow ConvLSTM-based network",
    "authors": [
      "Saurav Shukla",
      "Prabodh Kant Rai",
      "Tanmay T. Verlekar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/LLDFA/html/Shukla_Micro-expression_recognition_using_a_shallow_ConvLSTM-based_network_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/LLDFA/papers/Shukla_Micro-expression_recognition_using_a_shallow_ConvLSTM-based_network_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Micro-expressions reflect people's genuine emotions, making their recognition of great interest to the research community. Most state-of-the-art methods focus on the use of spatial features to perform micro-expression recognition. Thus, they fail to capture the spatiotemporal information available in a video sequence. This paper proposes a shallow convolutional long short-term memory (ConvLSTM) based network to perform micro-expression recognition. The convolutional and recurrent structures within the ConvLSTM allow the network to effectively capture the spatiotemporal information available in a video sequence. To highlight its effectiveness, the proposed ConvLSTM-based network is evaluated on the SAMM dataset. It is trained to perform micro-expression recognition across three (positive, negative, and surprise) and five (happiness, other, anger, contempt, and surprise) classes. When compared with the state-of-the-art, the results report a significant improvement in accuracy and the F1 score. The proposal is also robust against the unbalanced class sizes of the SAMM dataset."
  },
  "accv2022_amlavs_enhancingfederatedlearningrobustnessthroughclusteringnon-iidfeatures": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "AMLAVS",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Adversarial Machine Learning towards Advanced Vision Systems (AMLAVS)",
    "title": "Enhancing Federated Learning Robustness Through clustering Non-IID Features",
    "authors": [
      "Yanli Li",
      "Abubakar Sadiq Sani",
      "Dong Yuan",
      "Wei Bao"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/AMLAVS/html/Li_Enhancing_Federated_Learning_Robustness_Through_clustering_Non-IID_Features_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/AMLAVS/papers/Li_Enhancing_Federated_Learning_Robustness_Through_clustering_Non-IID_Features_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Federated learning (FL) enables many clients to train a joint model without sharing the raw data. While many byzantine-robust FL methods have been proposed, FL remains vulnerable to security attacks (such as poisoning attacks and evasion attacks) because of its distributed nature. Additionally, real-world training data used in FL are usually Non-Independent and Identically Distributed (Non-IID), which further weakens the robustness of the existing FL methods (such as Krum, Median, Trimmed-Mean, etc.), thereby making it possible for a global model in FL to be broken in extreme Non-IID scenarios. In this work, we mitigate the vulnerability of existing FL methods in Non-IID scenarios by proposing a new FL framework called Mini-Federated Learning (Mini-FL). Mini-FL follows the general FL approach but considers the Non-IID sources of FL and aggregates the gradients by groups. Specifically, Mini-FL first performs unsupervised learning for the gradients received to define the grouping policy. Then, the server divides the gradients received into different groups according to the grouping policy defined and performs byzantine-robust aggregation. Finally, the server calculates the weighted mean of gradients from each group to update the global model. Owning the strong generality, Mini-FL can utilize the most existing byzantine-robust method. We demonstrate that Mini-FL effectively enhances FL robustness and achieves greater global accuracy than existing FL methods when against the security attacks and in Non-IID settings."
  },
  "accv2022_amlavs_towardsimprovingtheanti-attackcapabilityoftherangenet++": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "AMLAVS",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Adversarial Machine Learning towards Advanced Vision Systems (AMLAVS)",
    "title": "Towards Improving the Anti-attack Capability of the RangeNet++",
    "authors": [
      "Qingguo Zhou",
      "Ming Lei",
      "Peng Zhi",
      "Rui Zhao",
      "Jun Shen",
      "Binbin Yong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/AMLAVS/html/Zhou_Towards_Improving_the_Anti-attack_Capability_of_the_RangeNet__ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/AMLAVS/papers/Zhou_Towards_Improving_the_Anti-attack_Capability_of_the_RangeNet__ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "With the possibility of deceiving deep learning models by appropri-ately modifying images verified, lots of researches on adversarial attacks and adversarial defenses have been carried out in academia. However, there is few research on adversarial attacks and adversarial defenses of point cloud semantic segmentation models, especially in the field of autonomous driving. The stabil-ity and robustness of point cloud semantic segmentation models are our primary concerns in this paper. Aiming at the point cloud segmentation model Range-Net++ in the field of autonomous driving, we propose novel approaches to im-prove the security and anti-attack capability of the RangeNet++ model. One is to calculate the local geometry that can reflect the surface shape of the point cloud based on the range image. The other is to obtain a general adversarial sample related only to the image itself and closer to the real world based on the range image, then add it into the training set for training. The experimental re-sults show that the proposed approaches can effectively improve the Range-Net++'s defense ability against adversarial attacks, and meanwhile enhance the RangeNet++ model's robustness."
  },
  "accv2022_amlavs_advfilteradversarialexamplegeneratedbyperturbingopticalpath": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "AMLAVS",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Adversarial Machine Learning towards Advanced Vision Systems (AMLAVS)",
    "title": "ADVFilter: Adversarial Example Generated by Perturbing Optical Path",
    "authors": [
      "Lili Zhang",
      "Xiaodong Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/AMLAVS/html/Zhang_ADVFilter_Adversarial_Example_Generated_by_Perturbing_Optical_Path_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/AMLAVS/papers/Zhang_ADVFilter_Adversarial_Example_Generated_by_Perturbing_Optical_Path_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Deep Neural Networks (DNNs) have achieved great success in many applications, and they are taking over more and more systems in the real world. As a result, the security of DNN system has attracted great attention from the community. In typical scenes, the input images of DNN are collected through the camera. In this paper, we propose a new type of security threat, which attacks a DNN classifier by perturbing the optical path of the camera input through a specially designed filter. It involves many challenges to generate such a filter. First, the filter should be input-free. Second, the filter should be simple enough for manufacturing. We propose a framework to generate such filters, called ADVFilter. ADVFilter models the optical path perturbation by thin plate spline, and optimizes for the minimal distortion of the input images. ADVFilter can generate adversarial pattern for a specific class. This adversarial pattern is universal for the class, which means that it can mislead the DNN model on all input images of the class with high probability. We demonstrate our idea on MNIST dataset, and the results show that ADVFilter can achieve up to 90% success rate with only 16 corresponding points. To the best of our knowledge, this is the first work to propose such security threat for DNN models."
  },
  "accv2022_cvmc_photorealisticfacialwrinklesremoval": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "CVMC",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Computer Vision for Medical Computing",
    "title": "Photorealistic Facial Wrinkles Removal",
    "authors": [
      "Marcelo Sanchez",
      "Gil Triginer",
      "Coloma Ballester",
      "Lara Raad",
      "Eduard Ramon"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/html/Sanchez_Photorealistic_Facial_Wrinkles_Removal_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/papers/Sanchez_Photorealistic_Facial_Wrinkles_Removal_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Editing and retouching facial attributes is a complex task that usually requires human artists to obtain photo-realistic results. Its applications are numerous and can be found in several contexts such as cosmetics or digital media retouching, to name a few. Recently, advancements in conditional generative modeling have shown astonishing results at modifying facial attributes in a realistic manner. However, current methods are still prone to artifacts, and focus on modifying global attributes like age and gender, or local mid-sized attributes like glasses or moustaches. In this work, we revisit a two-stage approach for retouching facial wrinkles and obtain results with unprecedented realism. First, a state of the art wrinkle segmentation network is used to detect the wrinkles within the facial region. Then, an inpainting module is used to remove the detected wrinkles, filling them in with a texture that is statistically consistent with the surrounding skin. To achieve this, we introduce a novel loss term that reuses the wrinkle segmentation network to penalize those regions that still contain wrinkles after the inpainting. We evaluate our method qualitatively and quantitatively, showing state of the art results for the task of wrinkle removal. Moreover, we introduce the first high-resolution dataset, named FFHQ-Wrinkles, to evaluate wrinkle detection methods."
  },
  "accv2022_cvmc_handlingdomainshiftforlesiondetectionviasemi-superviseddomainadaptation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "CVMC",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Computer Vision for Medical Computing",
    "title": "Handling Domain Shift for Lesion Detection via Semi-Supervised Domain Adaptation",
    "authors": [
      "Manu Sheoran",
      "Monika Sharma",
      "Meghal Dani",
      "Lovekesh Vig"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/html/Sheoran_Handling_Domain_Shift_for_Lesion_Detection_via_Semi-Supervised_Domain_Adaptation_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/papers/Sheoran_Handling_Domain_Shift_for_Lesion_Detection_via_Semi-Supervised_Domain_Adaptation_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "As the community progresses towards automated Universal Lesion Detection (ULD), it is vital that the techniques developed are robust and easily adaptable across a variety of datasets coming from different scanners, hospitals, and acquisition protocols. In practice, this remains a challenge due to the complexities of the different types of domain shifts. In this paper, we address the domain-shift by proposing a novel domain adaptation framework for ULD. The proposed model allows for the transfer of lesion knowledge from a large labeled source domain to detect lesions on a new target domain with minimal labeled samples. The proposed method first aligns the feature distribution of the two domains by training a detector on the source domain using a supervised loss, and a discriminator on both source and unlabeled target domains using an adversarial loss. Subsequently, a few labeled samples from the target domain along with labeled source samples are used to adapt the detector using an over-fitting aware and periodic gradient update based joint few-shot fine-tuning technique. Further, we utilize a self-supervision scheme to obtain pseudo-labels having highconfidence on the unlabeled target domain which are used to further train the detector in a semi-supervised manner and improve the detection sensitivity. We evaluate our proposed approach on domain adaptation for lesion detection from CT-scans wherein a ULD network trained on the DeepLesion dataset is adapted to 3 target domain datasets such as LiTS, KiTS and 3Dircadb. By utilizing adversarial, few-shot and incremental semi-supervised training, our method achieves comparable detection sensitivity to the previous methods for few-shot and semisupervised methods as well as to the Oracle model trained on the labeled target domain."
  },
  "accv2022_cvmc_ensemblemodelofvisualtransformerandcnnhelpsbadiagnosisfordoctorsinunderdevelopedareas": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "CVMC",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Computer Vision for Medical Computing",
    "title": "Ensemble Model of Visual Transformer and CNN Helps BA Diagnosis for Doctors in Underdeveloped Areas",
    "authors": [
      "Zhenghao Wei"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/html/Wei_Ensemble_Model_of_Visual_Transformer_and_CNN_Helps_BA_Diagnosis_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/papers/Wei_Ensemble_Model_of_Visual_Transformer_and_CNN_Helps_BA_Diagnosis_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "The diagnosis of Biliary Atresia (BA) is still complicated and high resource consumed. Though sonographic gallbladder images can be used as an initial detection tool, lack of experienced experts limits BA infants to be treated timely, resulting in liver transplantation or even death. We developed a diagnosis tool by ViT-CNN ensemble model to help doctors in underdeveloped area to diagnose BA. It performs better than human expert (with 88.1% accuracy versus 87.4%, 0.921 AUC versus 0.837), and still has an acceptable performance on severely noised images photographed by smartphone, providing doctors in clinical facilities with outdated Ultrasound instruments a simple and feasible solution to diagnose BA with our online tool."
  },
  "accv2022_cvmc_understandingtumormicroenvironmentusinggraphtheory": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "CVMC",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Computer Vision for Medical Computing",
    "title": "Understanding Tumor Micro Environment using Graph theory",
    "authors": [
      "Kinza Rohail",
      "Saba Bashir",
      "Hazrat Ali",
      "Tanvir Alam",
      "Sheheryar Khan",
      "Jia Wu",
      "Pingjun Chen",
      "Rizwan Qureshi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/html/Rohail_Understanding_Tumor_Micro_Environment_using_Graph_theory_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/papers/Rohail_Understanding_Tumor_Micro_Environment_using_Graph_theory_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Based over the historical data statistics of about past 50 years from National Cancer Institute's Surveillance, the survival rate of patients affected with Chronic Lymphocytic Leukemia (CLL) is about 65%. Other neoplastic lymphomas accelerated Chronic Lymphocytic Leukemia (aCLL) and Richter Transformation - Diffuse Large B-cell Lymphoma (RT-DLBL) are the aggressive and rare variant of this cancer that are subjected to less survival rate in patients and becomes worse with age of the patients. In this study, we developed a framework based over Graph Theory, Gaussian Mixture Modeling and Fuzzy C-mean Clustering, for learning the cell characteristics in neoplastic lymphomas along with quantitative analysis of pathological facts observed with integration of Image and Nuclei level analysis. On H&E slides of 60 hematolymphoid neoplasms, we evaluated the proposed algorithm and compared it to four cell level graph-based algorithms, including the global cell graph, cluster cell graph, hierarchical graph modeling and FLocK. The proposed method achieves better performance than the existing algorithms with mean diagnosis accuracy of 0.70833."
  },
  "accv2022_cvmc_improvingsegmentationofbreastarterialcalcificationsfromdigitalmammographygoodannotationisallyouneed": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "CVMC",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Computer Vision for Medical Computing",
    "title": "Improving Segmentation of Breast Arterial Calcifications from Digital Mammography: Good Annotation Is All You Need",
    "authors": [
      "Kaier Wang",
      "Melissa Hill",
      "Seymour Knowles-Barley",
      "Aristarkh Tikhonov",
      "Lester Litchfield",
      "James Christopher Bare"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/html/Wang_Improving_Segmentation_of_Breast_Arterial_Calcifications_from_Digital_Mammography_Good_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/CVMC/papers/Wang_Improving_Segmentation_of_Breast_Arterial_Calcifications_from_Digital_Mammography_Good_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Breast arterial calcifications (BACs) are frequently observed on screening mammography as calcified tracks along the course of an artery. These build-ups of calcium within the arterial wall may be associated with cardiovascular diseases (CVD). Accurate segmentation of BACs is a critical step in its quantification for the risk assessment of CVD but is challenging due to severely imbalanced positive/negative pixels and annotation quality, which is highly dependent on annotator's experience. In this study, we collected 6,573 raw tomosynthesis images where 95% had BACs in the initial pixel-wise annotation (performed by a third-party annotation company). The data were split with stratified sampling to 80% train, 10% validation and 10% test. Then we evaluated the performance of the deep learning models deeplabV3+ and Unet in segmenting BACs with varying training strategies such as different loss functions, encoders, image size and pre-processing methods. During the evaluation, large numbers of false positive labels were found in the annotations that significantly hindered the segmentation performance. Manual re-annotation of all images would be impossible owing to the required resources. Thus, we developed an automatic label correction algorithm based on BACs' morphology and physical properties. The algorithm was applied to training and validation labels to remove false positives. In comparison, we also manually re-annotated the test labels. The deep learning model re-trained on the algorithm-corrected labels resulted in a 29% improvement in the dice similarity score against the re-annotated test labels, suggesting that our label auto-correction algorithm is effective and that good annotations are important. Finally, we examined the drawbacks of an area-based segmentation metric, and proposed a length-based metric to assess the structural similarity between"
  },
  "accv2022_mlcsa_caltechfndistortedandpartiallyoccludeddigits": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "CaltechFN: Distorted and Partially Occluded Digits",
    "authors": [
      "Patrick Rim",
      "Snigdha Saha",
      "Marcus Rim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Rim_CaltechFN_Distorted_and_Partially_Occluded_Digits_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Rim_CaltechFN_Distorted_and_Partially_Occluded_Digits_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Digit datasets are widely used as compact, generalizable benchmarks for novel computer vision models. However, modern deep learning architectures have surpassed the human performance benchmarks on existing digit datasets, given that these datasets contain digits that have limited variability. In this paper, we introduce Caltech Football Numbers (CaltechFN), an image dataset of highly variable American football digits that aims to serve as a more difficult state-of-the-art benchmark for classification and detection tasks. Currently, CaltechFN contains 61,728 images with 264,572 labeled digits. Given the many different ways that digits on American football jerseys can be distorted and partially occluded in a live-action capture, we find that in comparison to humans, current computer vision models struggle to classify and detect the digits in our dataset. By comparing the performance of the latest task-specific models on CaltechFN and on an existing digit dataset, we show that our dataset indeed presents a far more difficult set of digits and that models trained on it still demonstrate high cross-dataset generalization. We also provide human performance benchmarks for our dataset to demonstrate the current gap between the abilities of humans and computers in the tasks of classifying and detecting the digits in our dataset. Finally, we describe two real-world applications that can be advanced using our dataset. CaltechFN is publicly available at https://data.caltech.edu/records/33qmq-a2n15, and all benchmark code is available at https://github.com/patrickqrim/CaltechFN."
  },
  "accv2022_mlcsa_temporalextensiontopologylearningforvideo-basedpersonre-identification": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Temporal Extension Topology Learning for Video-based Person Re-Identification",
    "authors": [
      "Jiaqi Ning",
      "Fei Li",
      "Rujie Liu",
      "Shun Takeuchi",
      "Genta Suzuki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Ning_Temporal_Extension_Topology_Learning_for_Video-based_Person_Re-Identification_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Ning_Temporal_Extension_Topology_Learning_for_Video-based_Person_Re-Identification_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Video-based person re-identification aims to match the same identification from video clips captured by multiple non-overlapping cameras. By effectively exploiting both temporal and spatial clues of a video clip, a more comprehensive representation of the identity in the video clip can be obtained. In this manuscript, we propose a novel graph-based framework, referred as Temporal Extension Adaptive Graph Convolution (TE-AGC) which could effectively mine features in spatial and temporal dimensions in one graph convolution operation. Specifically, TE-AGC adopts a CNN backbone and a key-point detector to extract global and local features as graph nodes. Moreover, a delicate adaptive graph convolution module is designed, which encourages meaningful information transfer by dynamically learning the reliability of local features from multiple frames.Comprehensive experiments on two video person re-identification benchmark datasets have demonstrated the effectiveness and state-of-the-art performance of the proposed method."
  },
  "accv2022_mlcsa_objectcentricpointsetsfeaturelearningwithmatrixdecomposition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Object Centric Point Sets Feature Learning with Matrix Decomposition",
    "authors": [
      "Zijia Wang",
      "Wenbin Yang",
      "Zhisong Liu",
      "Qiang Chen",
      "Jiacheng Ni",
      "Zhen Jia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Wang_Object_Centric_Point_Sets_Feature_Learning_with_Matrix_Decomposition_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Wang_Object_Centric_Point_Sets_Feature_Learning_with_Matrix_Decomposition_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "A representation matching the invariance/equivariance characteristics must be learnt to rebuild a morphable 3D model from a single picture input. However, present approaches for dealing with 3D point clouds depend heavily on a huge quantity of labeled data, while unsupervised methods need a large number of parameters. This is not productive. In the field of 3D morphable model building, the encoding of input photos has received minimal consideration. In this paper, we design a unique framework that strictly adheres to the permutation invariance of input points. Matrix Decomposition-based Invariant (MDI) learning is a system that offers a unified architecture for unsupervised invariant point set feature learning. The key concept behind our technique is to derive invariance and equivariance qualities for a point set via a simple but effective matrix decomposition. MDI is incredibly efficient and effective while being basic. Empirically, its performance is comparable to or even surpasses the state of the art. In addition, we present a framework for manipulating avatars based on CLIP and TBGAN, and the results indicate that our learnt features may help the model achieve better manipulation outcomes."
  },
  "accv2022_mlcsa_lightweighthyperspectralimagereconstructionnetworkwithdeepfeaturehallucination": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Lightweight Hyperspectral Image Reconstruction Network with Deep Feature Hallucination",
    "authors": [
      "Kazuhiro Yamawaki",
      "Xian-Hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Yamawaki_Lightweight_Hyperspectral_Image_Reconstruction_Network_with_Deep_Feature_Hallucination_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Yamawaki_Lightweight_Hyperspectral_Image_Reconstruction_Network_with_Deep_Feature_Hallucination_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Hyperspectral image reconstruction from a compressive snapshot is an dispensable step in the advanced hyperspectral imaging systems to solve the low spatial and/or temporal resolution issue. Most existing methods extensively exploit various hand-crafted priors to regularize the ill-posed hyperspectral reconstruction problem, and are incapable of handling wide spectral variety, often resulting in poor reconstruction quality. In recent year, deep convolution neural network (CNN) has became the dominated paradigm for hyperspectral image reconstruction, and demonstrated superior performance with complicated and deep network architectures. However, the current impressive CNNs usually yield large model size and high computational cost, which limit the wide applicability in the real imaging systems. This study proposes a novel lightweight hyperspectral reconstruction network via effective deep feature hallucination, and aims to construct a practical model with small size and high efficiency for real imaging systems. Specifically, we exploit a deep feature hallucination module (DFHM) for duplicating more features with cheap operations as the main component, and stack multiple of them to compose the lightweight architecture. In detail, the DFHM consists of spectral hallucination block for synthesizing more channel of features and spatial context aggregation block for exploiting various scales of contexts, and then enhance the spectral and spatial modeling capability with more cheap operation than the vanilla convolution layer. Experimental results on two benchmark hyperspectral datasets demonstrate that our proposed method has great superiority over the state-of-the-art CNN models in reconstruction performance as well as model size."
  },
  "accv2022_mlcsa_deeprgb-drivenlearningnetworkforunsupervisedhyperspectralimagesuper-resolution": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Deep RGB-driven Learning Network for Unsupervised Hyperspectral Image Super-resolution",
    "authors": [
      "Zhe Liu",
      "Xian-Hua Han"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Liu_Deep_RGB-driven_Learning_Network_for_Unsupervised_Hyperspectral_Image_Super-resolution_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Liu_Deep_RGB-driven_Learning_Network_for_Unsupervised_Hyperspectral_Image_Super-resolution_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Hyperspectral (HS) images are used in many fields to improve the analysis and understanding performance of captured scenes, as they contain a wide range of spectral information. However, the spatial resolution of hyperspectral images is usually very low, which limits their wide applicability in real tasks. To address the problem of low spatial resolution, super-resolution (SR) methods for hyperspectral images (HSI) have attracted widespread interest, which aims to mathematically generate high spatial resolution hyperspectral (HR-HS) images by combining degraded observational data: low spatial resolution hyperspectral (LR-HS) images and high resolution multispectral or RGB (HR-MS/RGB) images. Recently, paradigms based on deep learning have been widely explored as an alternative to automatically learn the inherent priors for the latent HR-HS images. These learning-based approaches are usually implemented in a fully supervised manner and require large external datasets including degraded observational data: LR-HS/HR-RGB images and corresponding HR-HS data, which are difficult to collect, especially for HSI SR scenarios. Therefore, in this study, a new unsupervised HSI SR method is proposed that uses only the observed LR-HS and HR-RGB images without any other external samples. Specifically, we use an RGB-driven deep generative network to learn the desired HR-HS images using a encoding-decoding-based network architecture. Since the observed HR-RGB images have a more detailed spatial structure and may be more suitable for two-dimensional convolution operations, we employ the observed HR-RGB images as input to the network as a conditional guide and adopt the observed LR-HS/HR-RGB images to formulate the loss function that guides the network learning. Experimental results on two HS image datasets show that our proposed unsupervised approach provides superior results compared to the SoTA deep learning paradigms."
  },
  "accv2022_mlcsa_towardssceneunderstandingforautonomousoperationsonairportaprons": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Towards Scene Understanding for Autonomous Operations on Airport Aprons",
    "authors": [
      "Daniel Steininger",
      "Andreas Kriegler",
      "Wolfgang Pointner",
      "Verena Widhalm",
      "Julia Simon",
      "Oliver Zendel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Steininger_Towards_Scene_Understanding_for_Autonomous_Operations_on_Airport_Aprons_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Steininger_Towards_Scene_Understanding_for_Autonomous_Operations_on_Airport_Aprons_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Enhancing logistics vehicles on airport aprons with assistant and autonomous capabilities offers the potential to significantly increase safety and efficiency of operations. However, this research area is still underrepresented compared to other automotive domains, especially regarding available image data, which is essential for training and benchmarking AI-based approaches. To mitigate this gap, we introduce a novel dataset specialized on static and dynamic objects commonly encountered while navigating apron areas. We propose an efficient approach for image acquisition as well as annotation of object instances and environmental parameters. Furthermore, we derive multiple dataset variants on which we conduct baseline classification and detection experiments. The resulting models are evaluated with respect to their overall performance and robustness against specific environmental conditions. The results are quite promising for future applications and provide essential insights regarding the selection of aggregation strategies as well as current potentials and limitations of similar approaches in this research domain."
  },
  "accv2022_mlcsa_aerialimagesegmentationvianoisedispellingandcontentdistilling": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Aerial Image Segmentation via Noise Dispelling and Content Distilling",
    "authors": [
      "Yongqing Sun",
      "Xiaomeng Wu",
      "Yukihiro Bandoh",
      "Masaki Kitahara"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Sun_Aerial_Image_Segmentation_via_Noise_Dispelling_and_Content_Distilling_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Sun_Aerial_Image_Segmentation_via_Noise_Dispelling_and_Content_Distilling_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Aerial image segmentation is an essential problem for land management which can be used for change detection and policy planning. However, traditional semantic segmentation methods focus on single-perspective images in road scenes, while aerial images are top-down views and objects are of a small size. Existing aerial segmentation methods tend to modify the network architectures proposed for traditional semantic segmentation problems, yet to the best of our knowledge, none of them focus on the noisy information present in the aerial images. In this work, we conduct an investigation on the effectiveness of each channels of the aerial image on the segmentation performance. Then, we propose a disentangle learning method to investigate the differences and similarities between channels and images, so that potential noisy information can be removed for higher segmentation accuracy."
  },
  "accv2022_mlcsa_atransformer-basedmodelforpreoperativeearlyrecurrencepredictionofhepatocellularcarcinomawithmuti-phasemri": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "A Transformer-based Model for Preoperative Early Recurrence Prediction of Hepatocellular Carcinoma with Muti-phase MRI",
    "authors": [
      "Gan Zhan",
      "Fang Wang",
      "Weibin Wang",
      "Yinhao Li",
      "Qingqing Chen",
      "Hongjie Hu",
      "Yen-Wei Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Zhan_A_Transformer-based_Model_for_Preoperative_Early_Recurrence_Prediction_of_Hepatocellular_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Zhan_A_Transformer-based_Model_for_Preoperative_Early_Recurrence_Prediction_of_Hepatocellular_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Hepatocellular carcinoma (HCC) is the most common pri_x005f_x0002_mary liver cancer which accounts for a high mortality rate in clinical,and the most effective treatment for HCC is surgical resection. However, patients with HCC are still at a huge risk of recurrence after tumor resection. In this light, preoperative early recurrence prediction methods are necessary to guide physicians to develop an individualized preoperative treatment plan and postoperative follow-up, thus prolonging the survival time of patients. Nevertheless, existing methods based on clinical data neglect information on the image modality; existing methods based on radiomics are limited by the ability of its predefined features compared with deep learning methods; and existing methods based on CT scans are constrained by the inability to capture the details of images compared with MRI. With these observations, we propose a deep learning transformer-based model on multi-phase MRI to tackle the preoperative early recurrence prediction task of HCC. Enlightened by the vigorous capacity of context modeling of the transformer architecture, our proposed model exploits it to dig out the inter-phase correlations, and the performance significantly improves. Our experimental results reveal that our transformer-based model can achieve better performance than other state-of-the-art existing methods."
  },
  "accv2022_mlcsa_giftfromnaturepotentialenergyminimizationforexplainabledatasetdistillation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "MLCSA",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - The Fourth IEEE International Workshop on Machine Learning and Computing for Visual Semantic Analysis",
    "title": "Gift from nature: Potential Energy Minimization for explainable dataset distillation",
    "authors": [
      "Zijia Wang",
      "Wenbin Yang",
      "Zhisong Liu",
      "Qiang Chen",
      "Jiacheng Ni",
      "Zhen Jia"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/html/Wang_Gift_from_nature_Potential_Energy_Minimization_for_explainable_dataset_distillation_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/MLCSA/papers/Wang_Gift_from_nature_Potential_Energy_Minimization_for_explainable_dataset_distillation_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Dataset distillation aims to reduce the dataset size by capturing important information from original dataset. It can significantly improve the feature extraction effectiveness, storage efficiency and training robustness. Furthermore, we study the features from the data distillation and found unique discriminative properties that can be exploited. Therefore, based on Potential Energy Minimization, we propose a generalized and explainable dataset distillation algorithm, called Potential Energy Minimization Dataset Distillation(PEMDD). The motivation is that when the distribution for each class is regular (that is, almost a compact high-dimensional ball in the feature space) and has minimal potential energy in its location, the mixed-distributions of all classes should be stable. In this stable state, Unscented Transform (UT) can be implemented to distill the data and reconstruct the stable distribution using these distilled data. Moreover, a simple but efficient framework of using the distilled data to fuse different datasets is proposed, where only a lightweight finetune is required. To demonstrate the superior performance over other works, we first visualize the classification results in terms of storage cost and performance. We then report quantitative improvement by comparing our proposed method with other state-of-the-art methods on several datasets. Finally, we conduct experiments on few-shot learning, and show the efficiency of our proposed methods with significant improvement in terms of the storage size requirement."
  },
  "accv2022_tcv_transformerbasedmotionin-betweening": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Vision Transformers Theory and Applications (TCV)",
    "title": "Transformer Based Motion In-Betweening",
    "authors": [
      "Pavithra Sridhar",
      "Aananth V",
      "Madhav Aggarwal",
      "R Leela Velusamy"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/html/Sridhar_Transformer_Based_Motion_In-Betweening_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/papers/Sridhar_Transformer_Based_Motion_In-Betweening_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "In-Betweening is the process of drawing transition frames between temporally-sparse keyframes to create a smooth animation sequence. This work presents a novel transformer based in betweening technique that serves as a tool for 3D animators. We first show that this problem can be represented as a sequence to sequence problem and introduce TweenTransformers - a model that synthesizes high-quality animations using temporally-sparse keyframes as input constraints. We evaluate the model's performance via two complementary methods-quantitative evaluation and qualitative evaluation. The model is compared quantitatively with the state-of-the-art models using LaFAN1, a high-quality animation dataset. Mean-squared metrics like L2P, L2Q, and NPSS are used for evaluation. Qualitatively, we provide two straightforward methods to assess the model's output. First, we implement a custom ThreeJs-based motion visualizer to render the ground truth, input, and output sequences side by side for comparison. The visualizer renders custom sequences by specifying skeletal positions at temporally-sparse keyframes in JSON format. Second, we build a motion generator to generate custom motion sequences using the model. Code can be found in https://github.com/Pavi114/motion-completion-using-transformers"
  },
  "accv2022_tcv_cross-attentiontransformerforvideointerpolation": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Vision Transformers Theory and Applications (TCV)",
    "title": "Cross-Attention Transformer for Video Interpolation",
    "authors": [
      "Hannah Halin Kim",
      "Shuzhi Yu",
      "Shuai Yuan",
      "Carlo Tomasi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/html/Kim_Cross-Attention_Transformer_for_Video_Interpolation_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/papers/Kim_Cross-Attention_Transformer_for_Video_Interpolation_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We propose TAIN (Transformers and Attention for video INterpolation), a residual neural network for video interpolation, which aims to interpolate an intermediate frame given two consecutive image frames around it. We first present a novel vision transformer module, named Cross-Similarity (CS), to globally aggregate input image features with similar appearance as those of the predicted interpolated frame. These CS features are then used to refine the interpolated prediction. To account for occlusions in the CS features, we propose an Image Attention (IA) module to allow the network to focus on CS features from one frame over those of the other. TAIN outperforms existing methods that do not require flow estimation and performs comparably to flow-based methods while being computationally efficient in terms of inference time on Vimeo90k, UCF101, and SNU-FILM benchmarks."
  },
  "accv2022_tcv_convolutionalpointtransformer": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Vision Transformers Theory and Applications (TCV)",
    "title": "Convolutional point Transformer",
    "authors": [
      "Chaitanya Kaul",
      "Joshua Mitton",
      "Hang Dai",
      "Roderick Murray-Smith"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/html/Kaul_Convolutional_point_Transformer_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/papers/Kaul_Convolutional_point_Transformer_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "We present CpT: Convolutional point Transformer - a novel neural network layer for dealing with the unstructured nature of 3D point cloud data. CpT is an improvement over existing MLP and convolution layers for point cloud processing, as well as existing 3D point cloud processing transformer layers. It achieves this feat due to its effectiveness in creating a novel and robust attention-based point set embedding through a convolutional projection layer crafted for processing dynamically local point set neighbourhoods. The resultant point set embedding is robust to the permutations of the input points. Our novel layer builds over local neighbourhoods of points obtained via a dynamic graph computation at each layer of the network's structure. It is fully differentiable and can be stacked just like convolutional layers to learn intrinsic properties of the points. Further, we propose a novel Adaptive Global Feature layer that learns to aggregate features from different representations into a better global representation of the point cloud. We evaluate our models on standard benchmark ModelNet40 classification and ShapeNet part segmentation datasets to show that our layer can serve as an effective addition for various point cloud processing tasks while effortlessly integrating into existing point cloud processing architectures to provide significant performance boosts."
  },
  "accv2022_tcv_temporalcross-attentionforactionrecognition": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "TCV",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Vision Transformers Theory and Applications (TCV)",
    "title": "Temporal Cross-attention for Action Recognition",
    "authors": [
      "Ryota Hashiguchi",
      "Toru Tamaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/html/Hashiguchi_Temporal_Cross-attention_for_Action_Recognition_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/TCV/papers/Hashiguchi_Temporal_Cross-attention_for_Action_Recognition_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the framewise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%."
  },
  "accv2022_dlsod_exploringspatial-temporalinstancerelationshipsinanintermediatedomainforimage-to-videoobjectdetection": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "DLSOD",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Deep Learning-Based Small Object Detection from Images and Videos",
    "title": "Exploring Spatial-temporal Instance Relationships In an Intermediate Domain For Image-to-video Object Detection",
    "authors": [
      "Zihan Wen",
      "Jin Chen",
      "Xinxiao Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/DLSOD/html/Wen_Exploring_Spatial-temporal_Instance_Relationships_In_an_Intermediate_Domain_For_Image-to-video_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/DLSOD/papers/Wen_Exploring_Spatial-temporal_Instance_Relationships_In_an_Intermediate_Domain_For_Image-to-video_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Image-to-video object detection leverages annotated images to help detect objects in unannotated videos, so as to break the heavy dependency on the expensive annotation of large-scale video frames. This task is extremely challenging due to the serious domain discrepancy between images and video frames caused by appearance variance and motion blur. Previous methods perform both image-level and instance-level alignments to reduce the domain discrepancy, but the existing false instance alignments may limit their performance in real scenarios. We propose a novel spatial-temporal graph to model the contextual relationships between instances to alleviate the false alignments. Through message propagation over the graph, the visual information from the spatial and temporal neighboring object proposals are adaptively aggregated to enhance the current instance representation. Moreover, to adapt the source-biased decision boundary to the target data, we generate an intermediate domain between images and frames. It is worth mentioning that our method can be easily applied as a plug-and-play component to other image-to-video object detection models based on the instance alignment. Experiments on several datasets demonstrate the effectiveness of our method. Code will be available at: https://github.com/wenzihan/STMP."
  },
  "accv2022_dlsod_evaluatingandbench-markingobjectdetectionmodelsfortrafficsignandtrafficlightdatasets": {
    "conf_id": "ACCV2022",
    "conf_sub_id": "DLSOD",
    "is_workshop": true,
    "conf_name": "ACCV2022_workshops - Deep Learning-Based Small Object Detection from Images and Videos",
    "title": "Evaluating and Bench-marking Object Detection Models for Traffic Sign and Traffic Light Datasets",
    "authors": [
      "Ashutosh Mishra",
      "Aman Kumar",
      "Shubham Mandloi",
      "Khushboo Anand",
      "John Zakkam",
      "Seeram Sowmya",
      "Avinash Thakur"
    ],
    "page_url": "http://openaccess.thecvf.com/content/ACCV2022W/DLSOD/html/Mishra_Evaluating_and_Bench-marking_Object_Detection_Models_for_Traffic_Sign_and_ACCVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/ACCV2022W/DLSOD/papers/Mishra_Evaluating_and_Bench-marking_Object_Detection_Models_for_Traffic_Sign_and_ACCVW_2022_paper.pdf",
    "published": "2022-12",
    "summary": "Object detection is an important sub-problem for many computer vision applications. There has been substantial research in improving and evaluating object detection models for generic objects but it is still not known how latest deep learning models perform on small road scene objects such as traffic lights and traffic signs. In fact, locating small object of interest such as traffic light and traffic sign is a priority task for an autonomous vehicle to maneuver in complex scenarios. Although some researchers have tried to investigate the performance of deep learning based object detection models on various public datasets, however there exists no comprehensive benchmark. We present a more detailed evaluation by providing in-depth analysis of state-of-theart deep learning based anchor and anchor-less object detection models such as Faster-RCNN, Single Shot Detector (SSD), Yolov3, RetinaNet, CenterNet and Cascade-RCNN. We compare the performance of these models on popular and publicly available traffic light datasets and traffic sign datasets from varied geographies. For traffic light datasets, we consider LISA Traffic Light (TL), Bosch, WPI and recently introduced S2TLD dataset for traffic light detection. For traffic sign benchmarking, we use LISA Traffic Sign (TS), GTSD, TT100K and recently published Mapillary Traffic Sign Dataset (MTSD). We compare the quantitative and qualitative performance of all the models on the aforementioned datasets and find that CenterNet outperforms all other baselines on almost all the datasets. We also compare inference time on specific CPU and GPU versions, flops and parameters for comparison. Understanding such behavior of the models on these datasets can help in solving a variety of practical difficulties and assists in the development of realworld applications. The source code and the models are available at https://github.com/OppoResearchIndia/DLSOD-ACCVW."
  }
}