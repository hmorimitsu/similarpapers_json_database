{
  "icml2015_main_stochasticoptimizationwithimportancesamplingforregularizedlossminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Optimization with Importance Sampling for Regularized Loss Minimization",
    "authors": [
      "Peilin Zhao",
      "Tong Zhang"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhaoa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhaoa15.pdf",
    "published": "2015-06",
    "summary": " Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness. ",
    "code_link": ""
  },
  "icml2015_main_approvalvotingandincentivesincrowdsourcing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Approval Voting and Incentives in Crowdsourcing",
    "authors": [
      "Nihar Shah",
      "Dengyong Zhou",
      "Yuval Peres"
    ],
    "page_url": "http://proceedings.mlr.press/v37/shaha15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/shaha15.pdf",
    "published": "2015-06",
    "summary": " The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\"strictly proper\") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach. ",
    "code_link": ""
  },
  "icml2015_main_alowvarianceconsistenttestofrelativedependency": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A low variance consistent test of relative dependency",
    "authors": [
      "Wacha Bounliphone",
      "Arthur Gretton",
      "Arthur Tenenhaus",
      "Matthew Blaschko"
    ],
    "page_url": "http://proceedings.mlr.press/v37/bounliphone15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/bounliphone15.pdf",
    "published": "2015-06",
    "summary": " We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep/. ",
    "code_link": ""
  },
  "icml2015_main_analignedsubtreekernelforweightedgraphs": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An Aligned Subtree Kernel for Weighted Graphs",
    "authors": [
      "Lu Bai",
      "Luca Rossi",
      "Zhihong Zhang",
      "Edwin Hancock"
    ],
    "page_url": "http://proceedings.mlr.press/v37/bai15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/bai15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depth-based representations. We demonstrate that this kernel can be seen as an \\textbfaligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy. ",
    "code_link": ""
  },
  "icml2015_main_spectralclusteringviathepowermethod-provably": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Spectral Clustering via the Power Method - Provably",
    "authors": [
      "Christos Boutsidis",
      "Prabhanjan Kambadur",
      "Alex Gittens"
    ],
    "page_url": "http://proceedings.mlr.press/v37/boutsidis15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/boutsidis15.pdf",
    "published": "2015-06",
    "summary": " Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the \u201cpower method\u201d from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the first such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the k-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors. ",
    "code_link": ""
  },
  "icml2015_main_informationgeometryandminimumdescriptionlengthnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Information Geometry and Minimum Description Length Networks",
    "authors": [
      "Ke Sun",
      "Jun Wang",
      "Alexandros Kalousis",
      "Stephan Marchand-Maillet"
    ],
    "page_url": "http://proceedings.mlr.press/v37/suna15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/suna15.pdf",
    "published": "2015-06",
    "summary": " We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture models, and then to simple mixture models. We present a geometric picture, where all these representations are regarded as free points in the space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with theories, algorithms, and simulations. ",
    "code_link": ""
  },
  "icml2015_main_efficienttrainingofldaonagpubymean-for-modeestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Training of LDA on a GPU by Mean-for-Mode Estimation",
    "authors": [
      "Jean-Baptiste Tristan",
      "Joseph Tassarotti",
      "Guy Steele"
    ],
    "page_url": "http://proceedings.mlr.press/v37/tristan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/tristan15.pdf",
    "published": "2015-06",
    "summary": " We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler \u2014 and unlike an uncollapsed Gibbs sampler \u2014 it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler \u2014 and unlike a collapsed Gibbs sampler \u2014 it is embarrassingly parallel, and can use approximate counters. ",
    "code_link": ""
  },
  "icml2015_main_adaptivestochasticalternatingdirectionmethodofmultipliers": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Adaptive Stochastic Alternating Direction Method of Multipliers",
    "authors": [
      "Peilin Zhao",
      "Jinwei Yang",
      "Tong Zhang",
      "Ping Li"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhaob15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhaob15.pdf",
    "published": "2015-06",
    "summary": " The Alternating Direction Method of Multipliers (ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed to replace the expected loss function with a random loss function associated with one uniformly drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived from a simple 2nd-order proximal function, i.e., the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order proximal functions, which produce a new family of adaptive stochastic ADMM methods. We theoretically prove that the regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms. ",
    "code_link": ""
  },
  "icml2015_main_alowerboundfortheoptimizationoffinitesums": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Lower Bound for the Optimization of Finite Sums",
    "authors": [
      "Alekh Agarwal",
      "Leon Bottou"
    ],
    "page_url": "http://proceedings.mlr.press/v37/agarwal15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/agarwal15.pdf",
    "published": "2015-06",
    "summary": " This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is \u03bc-strongly convex. We show that no algorithm can reach an error \u03b5in minimizing all functions from this class in fewer than \u03a9(n + \\sqrtn(\u03ba-1)\\log(1/\u03b5)) iterations, where \u03ba=L/\u03bcis a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally. ",
    "code_link": ""
  },
  "icml2015_main_learningwordrepresentationswithhierarchicalsparsecoding": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Word Representations with Hierarchical Sparse Coding",
    "authors": [
      "Dani Yogatama",
      "Manaal Faruqui",
      "Chris Dyer",
      "Noah Smith"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yogatama15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yogatama15.pdf",
    "published": "2015-06",
    "summary": " We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks\u2014word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis\u2014demonstrate that the method outperforms or is competitive with state-of-the-art methods. ",
    "code_link": ""
  },
  "icml2015_main_learningtransferablefeatureswithdeepadaptationnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Transferable Features with Deep Adaptation Networks",
    "authors": [
      "Mingsheng Long",
      "Yue Cao",
      "Jianmin Wang",
      "Michael Jordan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/long15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/long15.pdf",
    "published": "2015-06",
    "summary": " Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks. ",
    "code_link": ""
  },
  "icml2015_main_robustpartiallyobservablemarkovdecisionprocess": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Robust partially observable Markov decision process",
    "authors": [
      "Takayuki Osogami"
    ],
    "page_url": "http://proceedings.mlr.press/v37/osogami15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/osogami15.pdf",
    "published": "2015-06",
    "summary": " We seek to find the robust policy that maximizes the expected cumulative reward for the worst case when a partially observable Markov decision process (POMDP) has uncertain parameters whose values are only known to be in a given region. We prove that the robust value function, which represents the expected cumulative reward that can be obtained with the robust policy, is convex with respect to the belief state. Based on the convexity, we design a value-iteration algorithm for finding the robust policy. We prove that our value iteration converges for an infinite horizon. We also design point-based value iteration for fining the robust policy more efficiency possibly with approximation. Numerical experiments show that our point-based value iteration can adequately find robust policies. ",
    "code_link": ""
  },
  "icml2015_main_ontherelationshipbetweensum-productnetworksandbayesiannetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Relationship between Sum-Product Networks and Bayesian Networks",
    "authors": [
      "Han Zhao",
      "Mazen Melibari",
      "Pascal Poupart"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhaoc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhaoc15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of \\em normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN. ",
    "code_link": ""
  },
  "icml2015_main_learningfromcorruptedbinarylabelsviaclass-probabilityestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning from Corrupted Binary Labels via Class-Probability Estimation",
    "authors": [
      "Aditya Menon",
      "Brendan Van Rooyen",
      "Cheng Soon Ong",
      "Bob Williamson"
    ],
    "page_url": "http://proceedings.mlr.press/v37/menon15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/menon15.pdf",
    "published": "2015-06",
    "summary": " Many supervised learning problems involve learning from samples whose labels are corrupted in some way. For example, each sample may have some constant probability of being incorrectly labelled (learning with label noise), or one may have a pool of unlabelled samples in lieu of negative samples (learning from positive and unlabelled data). This paper uses class-probability estimation to study these and other corruption processes belonging to the mutually contaminated distributions framework (Scott et al., 2013), with three conclusions. First, one can optimise balanced error and AUC without knowledge of the corruption process parameters. Second, given estimates of the corruption parameters, one can minimise a range of classification risks. Third, one can estimate the corruption parameters using only corrupted data. Experiments confirm the efficacy of class-probability estimation in learning from corrupted labels. ",
    "code_link": ""
  },
  "icml2015_main_anexplicitsamplingdependentspectralerrorboundforcolumnsubsetselection": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection",
    "authors": [
      "Tianbao Yang",
      "Lijun Zhang",
      "Rong Jin",
      "Shenghuo Zhu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yanga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yanga15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we consider the problem of column subset selection. We present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities. The sampling dependent error bound (i) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities, (ii) exhibits more insights than existing error bounds that exploit specific probability distributions, and (iii) implies better sampling distributions. In particular, we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is better than uniform sampling, and is better than leverage-based sampling when the statistical leverage scores are very nonuniform. And by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores. Numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms. ",
    "code_link": ""
  },
  "icml2015_main_astochasticpcaandsvdalgorithmwithanexponentialconvergencerate": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate",
    "authors": [
      "Ohad Shamir"
    ],
    "page_url": "http://proceedings.mlr.press/v37/shamir15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/shamir15.pdf",
    "published": "2015-06",
    "summary": " We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis. ",
    "code_link": ""
  },
  "icml2015_main_attributeefficientlinearregressionwithdistribution-dependentsampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Attribute Efficient Linear Regression with Distribution-Dependent Sampling",
    "authors": [
      "Doron Kukliansky",
      "Ohad Shamir"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kukliansky15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kukliansky15.pdf",
    "published": "2015-06",
    "summary": " We consider a budgeted learning setting, where the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for Ridge and Lasso linear regression, which utilize the geometry of the data by a novel distribution-dependent sampling scheme, and have excess risk bounds which are better a factor of up to O(d/k) over the state-of-the-art, where d is the dimension and k+1 is the number of observed attributes per example. Moreover, under reasonable assumptions, our algorithms are the first in our setting which can provably use *less* attributes than full-information algorithms, which is the main concern in budgeted learning. We complement our theoretical analysis with experiments which support our claims. ",
    "code_link": ""
  },
  "icml2015_main_learninglocalinvariantmahalanobisdistances": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Local Invariant Mahalanobis Distances",
    "authors": [
      "Ethan Fetaya",
      "Shimon Ullman"
    ],
    "page_url": "http://proceedings.mlr.press/v37/fetaya15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/fetaya15.pdf",
    "published": "2015-06",
    "summary": " For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance. ",
    "code_link": ""
  },
  "icml2015_main_findinglinearstructureinlargedatasetswithscalablecanonicalcorrelationanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis",
    "authors": [
      "Zhuang Ma",
      "Yichao Lu",
      "Dean Foster"
    ],
    "page_url": "http://proceedings.mlr.press/v37/maa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/maa15.pdf",
    "published": "2015-06",
    "summary": " Canonical Correlation Analysis (CCA) is a widely used spectral technique for finding correlation structures in multi-view datasets. In this paper, we tackle the problem of large scale CCA, where classical algorithms, usually requiring computing the product of two huge matrices and huge matrix decomposition, are computationally and storage expensive. We recast CCA from a novel perspective and propose a scalable and memory efficient \\textitAugmented Approximate Gradient (AppGrad) scheme for finding top k dimensional canonical subspace which only involves large matrix multiplying a thin matrix of width k and small matrix decomposition of dimension k\\times k. Further, \\textitAppGrad achieves optimal storage complexity O(k(p_1+p_2)), compared with classical algorithms which usually require O(p_1^2+p_2^2) space to store two dense whitening matrices. The proposed scheme naturally generalizes to stochastic optimization regime, especially efficient for huge datasets where batch algorithms are prohibitive. The online property of stochastic \\textitAppGrad is also well suited to the streaming scenario, where data comes sequentially. To the best of our knowledge, it is the first stochastic algorithm for CCA. Experiments on four real data sets are provided to show the effectiveness of the proposed methods. ",
    "code_link": ""
  },
  "icml2015_main_abstractionselectioninmodel-basedreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Abstraction Selection in Model-based Reinforcement Learning",
    "authors": [
      "Nan Jiang",
      "Alex Kulesza",
      "Satinder Singh"
    ],
    "page_url": "http://proceedings.mlr.press/v37/jiang15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/jiang15.pdf",
    "published": "2015-06",
    "summary": " State abstractions are often used to reduce the complexity of model-based reinforcement learning when only limited quantities of data are available. However, choosing the appropriate level of abstraction is an important problem in practice. Existing approaches have theoretical guarantees only under strong assumptions on the domain or asymptotically large amounts of data, but in this paper we propose a simple algorithm based on statistical hypothesis testing that comes with a finite-sample guarantee under assumptions on candidate abstractions. Our algorithm trades off the low approximation error of finer abstractions against the low estimation error of coarser abstractions, resulting in a loss bound that depends only on the quality of the best available abstraction and is polynomial in planning horizon. ",
    "code_link": ""
  },
  "icml2015_main_surrogatefunctionsformaximizingprecisionatthetop": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Surrogate Functions for Maximizing Precision at the Top",
    "authors": [
      "Purushottam Kar",
      "Harikrishna Narasimhan",
      "Prateek Jain"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kar15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kar15.pdf",
    "published": "2015-06",
    "summary": " The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k. ",
    "code_link": ""
  },
  "icml2015_main_optimizingnon-decomposableperformancemeasuresataleoftwoclasses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes",
    "authors": [
      "Harikrishna Narasimhan",
      "Purushottam Kar",
      "Prateek Jain"
    ],
    "page_url": "http://proceedings.mlr.press/v37/narasimhana15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/narasimhana15.pdf",
    "published": "2015-06",
    "summary": " Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data. ",
    "code_link": ""
  },
  "icml2015_main_coresetsfornonparametricestimation-thecaseofdp-means": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Coresets for Nonparametric Estimation - the Case of DP-Means",
    "authors": [
      "Olivier Bachem",
      "Mario Lucic",
      "Andreas Krause"
    ],
    "page_url": "http://proceedings.mlr.press/v37/bachem15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/bachem15.pdf",
    "published": "2015-06",
    "summary": " Scalable training of Bayesian nonparametric models is a notoriously difficult challenge. We explore the use of coresets - a data summarization technique originating from computational geometry - for this task. Coresets are weighted subsets of the data such that models trained on these coresets are provably competitive with models trained on the full dataset. Coresets sublinear in the dataset size allow for fast approximate inference with provable guarantees. Existing constructions, however, are limited to parametric problems. Using novel techniques in coreset construction we show the existence of coresets for DP-Means - a prototypical nonparametric clustering problem - and provide a practical construction algorithm. We empirically demonstrate that our algorithm allows us to efficiently trade off computation time and approximation error and thus scale DP-Means to large datasets. For instance, with coresets we can obtain a computational speedup of 45x at an approximation error of only 2.4% compared to solving on the full data set. In contrast, for the same subsample size, the \u201cnaive\u201d approach of uniformly subsampling the data incurs an approximation error of 22.5%. ",
    "code_link": ""
  },
  "icml2015_main_arelativeexponentialweighingalgorithmforadversarialutility-basedduelingbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits",
    "authors": [
      "Pratik Gajane",
      "Tanguy Urvoy",
      "Fabrice Cl\u00e9rot"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gajane15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gajane15.pdf",
    "published": "2015-06",
    "summary": " We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications. ",
    "code_link": ""
  },
  "icml2015_main_functionalsubspaceclusteringwithapplicationtotimeseries": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Functional Subspace Clustering with Application to Time Series",
    "authors": [
      "Mohammad Taha Bahadori",
      "David Kale",
      "Yingying Fan",
      "Yan Liu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/bahadori15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/bahadori15.pdf",
    "published": "2015-06",
    "summary": " Functional data, where samples are random functions, are increasingly common and important in a variety of applications, such as health care and traffic analysis. They are naturally high dimensional and lie along complex manifolds. These properties warrant use of the subspace assumption, but most state-of-the-art subspace learning algorithms are limited to linear or other simple settings. To address these challenges, we propose a new framework called Functional Subspace Clustering (FSC). FSC assumes that functional samples lie in deformed linear subspaces and formulates the subspace learning problem as a sparse regression over operators. The resulting problem can be efficiently solved via greedy variable selection, given access to a fast deformation oracle. We provide theoretical guarantees for FSC and show how it can be applied to time series with warped alignments. Experimental results on both synthetic data and real clinical time series show that FSC outperforms both standard time series clustering and state-of-the-art subspace clustering. ",
    "code_link": ""
  },
  "icml2015_main_acceleratedonlinelowranktensorlearningformultivariatespatiotemporalstreams": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams",
    "authors": [
      "Rose Yu",
      "Dehua Cheng",
      "Yan Liu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yua15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yua15.pdf",
    "published": "2015-06",
    "summary": " Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with large-scale tensor streams, which poses significant challenges to existing solution in terms of computational costs and limited response time. In this paper, we propose an online accelerated low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to the subspace of low-rank tensors in order to perform efficient tensor decomposition, then recover the decomposition of the new tensor. By randomly glancing at additional subspaces, we successfully avoid local optima at negligible extra computational cost. We evaluate our method on two tasks in streaming multivariate spatio-temporal analysis: online forecasting and multi-model ensemble, which shows that our method achieves comparable predictive accuracy with significant boost in run time. ",
    "code_link": ""
  },
  "icml2015_main_atomicspatialprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Atomic Spatial Processes",
    "authors": [
      "Sean Jewell",
      "Neil Spencer",
      "Alexandre Bouchard-C\u00f4t\u00e9"
    ],
    "page_url": "http://proceedings.mlr.press/v37/jewell15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/jewell15.pdf",
    "published": "2015-06",
    "summary": " The emergence of compact GPS systems and the establishment of open data initiatives has resulted in widespread availability of spatial data for many urban centres. These data can be leveraged to develop data-driven intelligent resource allocation systems for urban issues such as policing, sanitation, and transportation. We employ techniques from Bayesian non-parametric statistics to develop a process which captures a common characteristic of urban spatial datasets. Specifically, our new spatial process framework models events which occur repeatedly at discrete spatial points, the number and locations of which are unknown a priori. We develop a representation of our spatial process which facilitates posterior simulation, resulting in an interpretable and computationally tractable model. The framework\u2019s superiority over both empirical grid-based models and Dirichlet process mixture models is demonstrated by fitting, interpreting, and comparing models of graffiti prevalence for both downtown Vancouver and Manhattan. ",
    "code_link": ""
  },
  "icml2015_main_classificationwithlowrankandmissingdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Classification with Low Rank and Missing Data",
    "authors": [
      "Elad Hazan",
      "Roi Livni",
      "Yishay Mansour"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hazan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hazan15.pdf",
    "published": "2015-06",
    "summary": " We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data. ",
    "code_link": ""
  },
  "icml2015_main_dynamicsensingbetterclassificationunderacquisitionconstraints": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dynamic Sensing: Better Classification under Acquisition Constraints",
    "authors": [
      "Oran Richman",
      "Shie Mannor"
    ],
    "page_url": "http://proceedings.mlr.press/v37/richman15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/richman15.pdf",
    "published": "2015-06",
    "summary": " In many machine learning applications the quality of the data is limited by resource constraints (may it be power, bandwidth, memory, ...). In such cases, the constraints are on the average resources allocated, therefore there is some control over each sample\u2019s quality. In most cases this option remains unused and the data\u2019s quality is uniform over the samples. In this paper we propose to actively allocate resources to each sample such that resources are used optimally overall. We propose a method to compute the optimal resource allocation. We further derive generalization bounds for the case where the problem\u2019s model is unknown. We demonstrate the potential benefit of this approach on both simulated and real-life problems. ",
    "code_link": ""
  },
  "icml2015_main_amodifiedorthant-wiselimitedmemoryquasi-newtonmethodwithconvergenceanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Modified Orthant-Wise Limited Memory Quasi-Newton Method with Convergence Analysis",
    "authors": [
      "Pinghua Gong",
      "Jieping Ye"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gonga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gonga15.pdf",
    "published": "2015-06",
    "summary": " The Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method has been demonstrated to be very effective in solving the \\ell_1-regularized sparse learning problem. OWL-QN extends the L-BFGS from solving unconstrained smooth optimization problems to \\ell_1-regularized (non-smooth) sparse learning problems. At each iteration, OWL-QN does not involve any \\ell_1-regularized quadratic optimization subproblem and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although many empirical studies have demonstrated that OWL-QN works quite well in practice, several recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As the main technical contribution of this paper, we establish a rigorous convergence proof for the mOWL-QN algorithm. To the best of our knowledge, our work fills the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving \\ell_1-regularized sparse learning problems. We also provide empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN. ",
    "code_link": ""
  },
  "icml2015_main_tellingcausefromeffectindeterministiclineardynamicalsystems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Telling cause from effect in deterministic linear dynamical systems",
    "authors": [
      "Naji Shajarisales",
      "Dominik Janzing",
      "Bernhard Schoelkopf",
      "Michel Besserve"
    ],
    "page_url": "http://proceedings.mlr.press/v37/shajarisales15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/shajarisales15.pdf",
    "published": "2015-06",
    "summary": " Telling a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause through a linear system, we propose a new approach based on the hypothesis that nature chooses the \u201ccause\u201d and the \u201cmechanism generating the effect from the cause\u201d independently of each other. Specifically we postulate that the power spectrum of the \u201ccause\u201d time series is uncorrelated with the square of the frequency response of the linear filter (system) generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that exist even in deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable. In particular, we show a scenario where the method works but Granger causality fails. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on observed time series. ",
    "code_link": ""
  },
  "icml2015_main_highdimensionalbayesianoptimisationandbanditsviaadditivemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models",
    "authors": [
      "Kirthevasan Kandasamy",
      "Jeff Schneider",
      "Barnabas Poczos"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kandasamy15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kandasamy15.pdf",
    "published": "2015-06",
    "summary": " Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive. ",
    "code_link": "https://github.com/kirthevasank/add-gp-ucb"
  },
  "icml2015_main_theoryofdual-sparseregularizedrandomizedreduction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Theory of Dual-sparse Regularized Randomized Reduction",
    "authors": [
      "Tianbao Yang",
      "Lijun Zhang",
      "Rong Jin",
      "Shenghuo Zhu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yangb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yangb15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data. ",
    "code_link": ""
  },
  "icml2015_main_generalizationerrorboundsforlearningtorankdoesthelengthofdocumentlistsmatter?": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Generalization error bounds for learning to rank: Does the length of document lists matter?",
    "authors": [
      "Ambuj Tewari",
      "Sougata Chaudhuri"
    ],
    "page_url": "http://proceedings.mlr.press/v37/tewari15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/tewari15.pdf",
    "published": "2015-06",
    "summary": " We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under \\ell_1 regularization and faster convergence rates if the loss function is smooth. ",
    "code_link": ""
  },
  "icml2015_main_peaksegconstrainedoptimalsegmentationandsupervisedpenaltylearningforpeakdetectionincountdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PeakSeg: constrained optimal segmentation and supervised penalty learning for peak detection in count data",
    "authors": [
      "Toby Hocking",
      "Guillem Rigaill",
      "Guillaume Bourque"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hocking15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hocking15.pdf",
    "published": "2015-06",
    "summary": " Peak detection is a central problem in genomic data analysis, and current algorithms for this task are unsupervised and mostly effective for a single data type and pattern (e.g. H3K4me3 data with a sharp peak pattern). We propose PeakSeg, a new constrained maximum likelihood segmentation model for peak detection with an efficient inference algorithm: constrained dynamic programming. We investigate unsupervised and supervised learning of penalties for the critical model selection problem. We show that the supervised method has state-of-the-art peak detection across all data sets in a benchmark that includes both sharp H3K4me3 and broad H3K36me3 patterns. ",
    "code_link": ""
  },
  "icml2015_main_mindthedualitygapsaferrulesforthelasso": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Mind the duality gap: safer rules for the Lasso",
    "authors": [
      "Olivier Fercoq",
      "Alexandre Gramfort",
      "Joseph Salmon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/fercoq15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/fercoq15.pdf",
    "published": "2015-06",
    "summary": " Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called \\textitsafe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules. ",
    "code_link": ""
  },
  "icml2015_main_ageneralanalysisoftheconvergenceofadmm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A General Analysis of the Convergence of ADMM",
    "authors": [
      "Robert Nishihara",
      "Laurent Lessard",
      "Ben Recht",
      "Andrew Packard",
      "Michael Jordan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/nishihara15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/nishihara15.pdf",
    "published": "2015-06",
    "summary": " We provide a new proof of the linear convergence of the alternating direction method of multipliers (ADMM) when one of the objective terms is strongly convex. Our proof is based on a framework for analyzing optimization algorithms introduced in Lessard et al. (2014), reducing algorithm convergence to verifying the stability of a dynamical system. This approach generalizes a number of existing results and obviates any assumptions about specific choices of algorithm parameters. On a numerical example, we demonstrate that minimizing the derived bound on the convergence rate provides a practical approach to selecting algorithm parameters for particular ADMM instances. We complement our upper bound by constructing a nearly-matching lower bound on the worst-case rate of convergence. ",
    "code_link": ""
  },
  "icml2015_main_stochasticprimal-dualcoordinatemethodforregularizedempiricalriskminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
    "authors": [
      "Yuchen Zhang",
      "Xiao Lin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhanga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhanga15.pdf",
    "published": "2015-06",
    "summary": " We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and non-strongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods. ",
    "code_link": ""
  },
  "icml2015_main_discodistributedoptimizationforself-concordantempiricalloss": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "DiSCO: Distributed Optimization for Self-Concordant Empirical Loss",
    "authors": [
      "Yuchen Zhang",
      "Xiao Lin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhangb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhangb15.pdf",
    "published": "2015-06",
    "summary": " We propose a new distributed algorithm for empirical risk minimization in machine learning. The algorithm is based on an inexact damped Newton method, where the inexact Newton steps are computed by a distributed preconditioned conjugate gradient method. We analyze its iteration complexity and communication efficiency for minimizing self-concordant empirical loss functions, and discuss the results for distributed ridge regression, logistic regression and binary classification with a smoothed hinge loss. In a standard setting for supervised learning, where the n data points are i.i.d. sampled and when the regularization parameter scales as 1/\\sqrtn, we show that the proposed algorithm is communication efficient: the required round of communication does not increase with the sample size n, and only grows slowly with the number of machines. ",
    "code_link": ""
  },
  "icml2015_main_spectralmletop-krankaggregationfrompairwisecomparisons": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons",
    "authors": [
      "Yuxin Chen",
      "Changho Suh"
    ],
    "page_url": "http://proceedings.mlr.press/v37/chena15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/chena15.pdf",
    "published": "2015-06",
    "summary": " This paper explores the preference-based top-K rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and one wishes to recover a consistent ordering that emphasizes the top-K ranked items, based on partially revealed preferences. We focus on the Bradley-Terry-Luce (BTL) model that postulates a set of latent preference scores underlying all items, where the odds of paired comparisons depend only on the relative scores of the items involved. We characterize the minimax limits on identifiability of top-K ranked items, in the presence of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the K-th and (K+1)-th ranked items. The minimum sample complexity required for reliable top-K ranking scales inversely with the separation measure irrespective of other preference distribution metrics. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called Spectral MLE, that returns the indices of the top-K items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an initial score estimate with minimal squared loss (obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-K item identification under minimal sample complexity. The practical applicability of Spectral MLE is further corroborated by numerical experiments. ",
    "code_link": ""
  },
  "icml2015_main_paired-duallearningforfasttrainingoflatentvariablehinge-lossmrfs": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs",
    "authors": [
      "Stephen Bach",
      "Bert Huang",
      "Jordan Boyd-Graber",
      "Lise Getoor"
    ],
    "page_url": "http://proceedings.mlr.press/v37/bach15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/bach15.pdf",
    "published": "2015-06",
    "summary": " Latent variables allow probabilistic graphical models to capture nuance and structure in important domains such as network science, natural language processing, and computer vision. Naive approaches to learning such complex models can be prohibitively expensive\u2014because they require repeated inferences to update beliefs about latent variables\u2014so lifting this restriction for useful classes of models is an important problem. Hinge-loss Markov random fields (HL-MRFs) are graphical models that allow highly scalable inference and learning in structured domains, in part by representing structured problems with continuous variables. However, this representation leads to challenges when learning with latent variables. We introduce paired-dual learning, a framework that greatly speeds up training by using tractable entropy surrogates and avoiding repeated inferences. Paired-dual learning optimizes an objective with a pair of dual inference problems. This allows fast, joint optimization of parameters and dual variables. We evaluate on social-group detection, trust prediction in social networks, and image reconstruction, finding that paired-dual learning trains models as accurate as those trained by traditional methods in much less time, often before traditional methods make even a single parameter update. ",
    "code_link": ""
  },
  "icml2015_main_structuralmaxentmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Structural Maxent Models",
    "authors": [
      "Corinna Cortes",
      "Vitaly Kuznetsov",
      "Mehryar Mohri",
      "Umar Syed"
    ],
    "page_url": "http://proceedings.mlr.press/v37/cortes15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/cortes15.pdf",
    "published": "2015-06",
    "summary": " We present a new class of density estimation models, Structural Maxent models, with feature functions selected from possibly very complex families. The design of our models is motivated by data-dependent convergence bounds and benefits from new data-dependent learning bounds expressed in terms of the Rademacher complexities of the sub-families composing the family of features considered. We prove a duality theorem, which we use to derive our Structural Maxent algorithm. We give a full description of our algorithm, including the details of its derivation and report the results of several experiments demonstrating that its performance compares favorably to that of existing regularized Maxent. We further similarly define conditional Structural Maxent models for multi-class classification problems. These are conditional probability models making use of possibly complex feature families. We also prove a duality theorem for these models which shows the connection between these models and existing binary and multi-class deep boosting algorithms. ",
    "code_link": ""
  },
  "icml2015_main_aprovablegeneralizedtensorspectralmethodforuniformhypergraphpartitioning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning",
    "authors": [
      "Debarghya Ghoshdastidar",
      "Ambedkar Dukkipati"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ghoshdastidar15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ghoshdastidar15.pdf",
    "published": "2015-06",
    "summary": " Matrix spectral methods play an important role in statistics and machine learning, and most often the word \u2018matrix\u2019 is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of \u201chigher-order\u201d algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings. ",
    "code_link": ""
  },
  "icml2015_main_thebenefitsoflearningwithstronglyconvexapproximateinference": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Benefits of Learning with Strongly Convex Approximate Inference",
    "authors": [
      "Ben London",
      "Bert Huang",
      "Lise Getoor"
    ],
    "page_url": "http://proceedings.mlr.press/v37/london15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/london15.pdf",
    "published": "2015-06",
    "summary": " We explore the benefits of strongly convex free energies in variational inference, providing both theoretical motivation and a new meta-algorithm. Using the duality between strong convexity and stability, we prove a high-probability bound on the error of learned marginals that is inversely proportional to the modulus of convexity of the free energy, thereby motivating free energies whose moduli are constant with respect to the size of the graph. We identify sufficient conditions for \u03a9(1)-strong convexity in two popular variational techniques: tree-reweighted and counting number entropies. Our insights for the latter suggest a novel counting number optimization framework, which guarantees strong convexity for any given modulus. Our experiments demonstrate that learning with a strongly convex free energy, using our optimization framework to guarantee a given modulus, results in substantially more accurate marginal probabilities, thereby validating our theoretical claims and the effectiveness of our framework. ",
    "code_link": ""
  },
  "icml2015_main_pushingthelimitsofaffinerankminimizationbyadaptingprobabilisticpca": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Pushing the Limits of Affine Rank Minimization by Adapting Probabilistic PCA",
    "authors": [
      "Bo Xin",
      "David Wipf"
    ],
    "page_url": "http://proceedings.mlr.press/v37/xin15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/xin15.pdf",
    "published": "2015-06",
    "summary": " Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equals the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this property. The algorithm has also been successfully deployed on a computer vision application involving image rectification and a standard collaborative filtering benchmark. ",
    "code_link": ""
  },
  "icml2015_main_budgetallocationproblemwithmultipleadvertisersagametheoreticview": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Budget Allocation Problem with Multiple Advertisers: A Game Theoretic View",
    "authors": [
      "Takanori Maehara",
      "Akihiro Yabe",
      "Ken-ichi Kawarabayashi"
    ],
    "page_url": "http://proceedings.mlr.press/v37/maehara15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/maehara15.pdf",
    "published": "2015-06",
    "summary": " In marketing planning, advertisers seek to maximize the number of customers by allocating given budgets to each media channel effectively. The budget allocation problem with a bipartite influence model captures this scenario; however, the model is problematic because it assumes there is only one advertiser in the market. In reality, there are many advertisers which are in conflict of advertisement; thus we must extend the model for such a case. By extending the budget allocation problem with a bipartite influence model, we propose a game-theoretic model problem that considers many advertisers. By simulating our model, we can analyze the behavior of a media channel market, e.g., we can estimate which media channels are allocated by an advertiser, and which customers are influenced by an advertiser. Our model has many attractive features. First, our model is a potential game; therefore, it has a pure Nash equilibrium. Second, any Nash equilibrium of our game has 2-optimal social utility, i.e., the price of anarchy is 2. Finally, the proposed model can be simulated very efficiently; thus it can be used to analyze large markets. ",
    "code_link": ""
  },
  "icml2015_main_trackingapproximatesolutionsofparameterizedoptimizationproblemsovermulti-dimensional(hyper-)parameterdomains": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Tracking Approximate Solutions of Parameterized Optimization Problems over Multi-Dimensional (Hyper-)Parameter Domains",
    "authors": [
      "Katharina Blechschmidt",
      "Joachim Giesen",
      "Soeren Laue"
    ],
    "page_url": "http://proceedings.mlr.press/v37/blechschmidt15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/blechschmidt15.pdf",
    "published": "2015-06",
    "summary": " Many machine learning methods are given as parameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tuned carefully since the choice of their values can have a significant impact on the statistical performance of the learning methods. In most cases the parameter space does not carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one parameter received quite some attention over the years. First, algorithms for tracking an optimal solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since these algorithms can suffer from numerical problems more robust and efficient approximate path tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization-and kernel hyperparameter paths with optimal path complexities that depend only on the prescribed approximation error. Here we extend the work on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the complexity of approximately exploring a multi-dimensional parameter domain that is the product of the corresponding path complexities. We also show a matching upper bound that can be turned into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm the theoretical complexity analysis. ",
    "code_link": ""
  },
  "icml2015_main_batchnormalizationacceleratingdeepnetworktrainingbyreducinginternalcovariateshift": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "authors": [
      "Sergey Ioffe",
      "Christian Szegedy"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ioffe15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ioffe15.pdf",
    "published": "2015-06",
    "summary": " Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters. ",
    "code_link": ""
  },
  "icml2015_main_distributedestimationofgeneralizedmatrixrankefficientalgorithmsandlowerbounds": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds",
    "authors": [
      "Yuchen Zhang",
      "Martin Wainwright",
      "Michael Jordan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhangc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhangc15.pdf",
    "published": "2015-06",
    "summary": " We study the following generalized matrix rank estimation problem: given an n-by-n matrix and a constant c > 0, estimate the number of eigenvalues that are greater than c. In the distributed setting, the matrix of interest is the sum of m matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate \u03a9(n^2) bits, which is order-equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only O(n) bits. The upper bound is matched by an \u03a9(n) lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments. ",
    "code_link": ""
  },
  "icml2015_main_landmarkingmanifoldswithgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Landmarking Manifolds with Gaussian Processes",
    "authors": [
      "Dawen Liang",
      "John Paisley"
    ],
    "page_url": "http://proceedings.mlr.press/v37/liang15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/liang15.pdf",
    "published": "2015-06",
    "summary": " We present an algorithm for finding landmarks along a manifold. These landmarks provide a small set of locations spaced out along the manifold such that they capture the low-dimensional non-linear structure of the data embedded in the high-dimensional space. The approach does not select points directly from the dataset, but instead we optimize each landmark by moving along the continuous manifold space (as approximated by the data) according to the gradient of an objective function. We borrow ideas from active learning with Gaussian processes to define the objective, which has the property that a new landmark is \"repelled\" by those currently selected, allowing for exploration of the manifold. We derive a stochastic algorithm for learning with large datasets and show results on several datasets, including the Million Song Dataset and articles from the New York Times. ",
    "code_link": ""
  },
  "icml2015_main_markovmixedmembershipmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Markov Mixed Membership Models",
    "authors": [
      "Aonan Zhang",
      "John Paisley"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhangd15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhangd15.pdf",
    "published": "2015-06",
    "summary": " We present a Markov mixed membership model (Markov M3) for grouped data that learns a fully connected graph structure among mixing components. A key feature of Markov M3 is that it interprets the mixed membership assignment as a Markov random walk over this graph of nodes. This is in contrast to tree-structured models in which the assignment is done according to a tree structure on the mixing components. The Markov structure results in a simple parametric model that can learn a complex dependency structure between nodes, while still maintaining full conjugacy for closed-form stochastic variational inference. Empirical results demonstrate that Markov M3 performs well compared with tree structured topic models, and can learn meaningful dependency structure between topics. ",
    "code_link": ""
  },
  "icml2015_main_aunifiedframeworkforoutlier-robustpca-likealgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Unified Framework for Outlier-Robust PCA-like Algorithms",
    "authors": [
      "Wenzhuo Yang",
      "Huan Xu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yangc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yangc15.pdf",
    "published": "2015-06",
    "summary": " We propose a unified framework for making a wide range of PCA-like algorithms \u2013 including the standard PCA, sparse PCA and non-negative sparse PCA, etc. \u2013 robust when facing a constant fraction of arbitrarily corrupted outliers. Our theoretic analysis establishes solid performance guarantees of the proposed framework: its estimation error is upper bounded by a term depending on the intrinsic parameters of the data model, the selected PCA-like algorithm and the fraction of outliers. Comprehensive experiments on synthetic and real-world datasets demonstrate that the outlier-robust PCA-like algorithms derived from our framework have outstanding performance. ",
    "code_link": ""
  },
  "icml2015_main_streamingsparseprincipalcomponentanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Streaming Sparse Principal Component Analysis",
    "authors": [
      "Wenzhuo Yang",
      "Huan Xu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yangd15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yangd15.pdf",
    "published": "2015-06",
    "summary": " This paper considers estimating the leading k principal components with at most s non-zero attributes from p-dimensional samples collected sequentially in memory limited environments. We develop and analyze two memory and computational efficient algorithms called streaming sparse PCA and streaming sparse ECA for analyzing data generated according to the spike model and the elliptical model respectively. In particular, the proposed algorithms have memory complexity O(pk), computational complexity O(pk mink,slogp) and sample complexity \u0398(s \\log p). We provide their finite sample performance guarantees, which implies statistical consistency in the high dimensional regime. Numerical experiments on synthetic and real-world datasets demonstrate good empirical performance of the proposed algorithms. ",
    "code_link": ""
  },
  "icml2015_main_adivideandconquerframeworkfordistributedgraphclustering": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Divide and Conquer Framework for Distributed Graph Clustering",
    "authors": [
      "Wenzhuo Yang",
      "Huan Xu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yange15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yange15.pdf",
    "published": "2015-06",
    "summary": " Graph clustering is about identifying clusters of closely connected nodes, and is a fundamental technique of data analysis with many applications including community detection, VLSI network partitioning, collaborative filtering, and many others. In order to improve the scalability of existing graph clustering algorithms, we propose a novel divide and conquer framework for graph clustering, and establish theoretical guarantees of exact recovery of the clusters. One additional advantage of the proposed framework is that it can identify small clusters \u2013 the size of the smallest cluster can be of size o(\\sqrtn), in contrast to \u03a9(\\sqrtn) required by standard methods. Extensive experiments on synthetic and real-world datasets demonstrate the efficiency and effectiveness of our framework. ",
    "code_link": ""
  },
  "icml2015_main_howcandeeprectifiernetworksachievelinearseparabilityandpreservedistances?": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?",
    "authors": [
      "Senjian An",
      "Farid Boussaid",
      "Mohammed Bennamoun"
    ],
    "page_url": "http://proceedings.mlr.press/v37/an15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/an15.pdf",
    "published": "2015-06",
    "summary": " This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer. ",
    "code_link": ""
  },
  "icml2015_main_improvedregretboundsforundiscountedcontinuousreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning",
    "authors": [
      "K. Lakshmanan",
      "Ronald Ortner",
      "Daniil Ryabko"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lakshmanan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lakshmanan15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of undiscounted reinforcement learning in continuous state space. Regret bounds in this setting usually hold under various assumptions on the structure of the reward and transition function. Under the assumption that the rewards and transition probabilities are Lipschitz, for 1-dimensional state space a regret bound of O(T^3/4) after any T steps has been given by Ortner and Ryabko (2012). Here we improve upon this result by using non-parametric kernel density estimation for estimating the transition probability distributions, and obtain regret bounds that depend on the smoothness of the transition probability distributions. In particular, under the assumption that the transition probability functions are smoothly differentiable, the regret bound is shown to be O(T^2/3) asymptotically for reinforcement learning in 1-dimensional state space. Finally, we also derive improved regret bounds for higher dimensional state space. ",
    "code_link": ""
  },
  "icml2015_main_thefundamentalincompatibilityofscalablehamiltonianmontecarloandnaivedatasubsampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling",
    "authors": [
      "Michael Betancourt"
    ],
    "page_url": "http://proceedings.mlr.press/v37/betancourt15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/betancourt15.pdf",
    "published": "2015-06",
    "summary": " Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo. ",
    "code_link": ""
  },
  "icml2015_main_fasterratesforthefrank-wolfemethodoverstrongly-convexsets": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets",
    "authors": [
      "Dan Garber",
      "Elad Hazan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/garbera15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/garbera15.pdf",
    "published": "2015-06",
    "summary": " The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of \\frac1t^2. This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order \\frac1t, and known to be tight. We show that various balls induced by \\ell_p norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fast-rate results for the FW method follow easily from our analysis. ",
    "code_link": ""
  },
  "icml2015_main_orderedstick-breakingpriorforsequentialmcmcinferenceofbayesiannonparametricmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Ordered Stick-Breaking Prior for Sequential MCMC Inference of Bayesian Nonparametric Models",
    "authors": [
      "Mrinal Das",
      "Trapit Bansal",
      "Chiranjib Bhattacharyya"
    ],
    "page_url": "http://proceedings.mlr.press/v37/das15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/das15.pdf",
    "published": "2015-06",
    "summary": " This paper introduces ordered stick-breaking process (OSBP), where the atoms in a stick-breaking process (SBP) appear in order. The choice of weights on the atoms of OSBP ensure that; (1) probability of adding new atoms exponentially decrease, and (2) OSBP, though non-exchangeable, admit predictive probability functions (PPFs). In a Bayesian nonparametric (BNP) setting, OSBP serves as a natural prior over sequential mini-batches, facilitating exchange of relevant statistical information by sharing the atoms of OSBP. One of the major contributions of this paper is SUMO, an MCMC algorithm, for solving the inference problem arising from applying OSBP to BNP models. SUMO uses the PPFs of OSBP to obtain a Gibbs-sampling based truncation-free algorithm which applies generally to BNP models. For large scale inference problems existing algorithms such as particle filtering (PF) are not practical and variational procedures such as TSVI (Wang & Blei, 2012) are the only alternative. For Dirichlet process mixture model (DPMM), SUMO outperforms TSVI on perplexity by 33% on 3 datasets with million data points, which are beyond the scope of PF, using only 3GB RAM. ",
    "code_link": ""
  },
  "icml2015_main_onlinelearningofeigenvectors": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Online Learning of Eigenvectors",
    "authors": [
      "Dan Garber",
      "Elad Hazan",
      "Tengyu Ma"
    ],
    "page_url": "http://proceedings.mlr.press/v37/garberb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/garberb15.pdf",
    "published": "2015-06",
    "summary": " Computing the leading eigenvector of a symmetric real matrix is a fundamental primitive of numerical linear algebra with numerous applications. We consider a natural online extension of the leading eigenvector problem: a sequence of matrices is presented and the goal is to predict for each matrix a unit vector, with the overall goal of competing with the leading eigenvector of the cumulative matrix. Existing regret-minimization algorithms for this problem either require to compute an \\textiteigen decompostion every iteration, or suffer from a large dependency of the regret bound on the dimension. In both cases the algorithms are not practical for large scale applications. In this paper we present new algorithms that avoid both issues. On one hand they do not require any expensive matrix decompositions and on the other, they guarantee regret rates with a mild dependence on the dimension at most. In contrast to previous algorithms, our algorithms also admit implementations that enable to leverage sparsity in the data to further reduce computation. We extend our results to also handle non-symmetric matrices. ",
    "code_link": ""
  },
  "icml2015_main_aunifyingframeworkofanytimesparsegaussianprocessregressionmodelswithstochasticvariationalinferenceforbigdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Unifying Framework of Anytime Sparse Gaussian Process Regression Models with Stochastic Variational Inference for Big Data",
    "authors": [
      "Trong Nghia Hoang",
      "Quang Minh Hoang",
      "Bryan Kian Hsiang Low"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hoang15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hoang15.pdf",
    "published": "2015-06",
    "summary": " This paper presents a novel unifying framework of anytime sparse Gaussian process regression (SGPR) models that can produce good predictive performance fast and improve their predictive performance over time. Our proposed unifying framework reverses the variational inference procedure to theoretically construct a non-trivial, concave functional that is maximized at the predictive distribution of any SGPR model of our choice. As a result, a stochastic natural gradient ascent method can be derived that involves iteratively following the stochastic natural gradient of the functional to improve its estimate of the predictive distribution of the chosen SGPR model and is guaranteed to achieve asymptotic convergence to it. Interestingly, we show that if the predictive distribution of the chosen SGPR model satisfies certain decomposability conditions, then the stochastic natural gradient is an unbiased estimator of the exact natural gradient and can be computed in constant time (i.e., independent of data size) at each iteration. We empirically evaluate the trade-off between the predictive performance vs. time efficiency of the anytime SGPR models on two real-world million-sized datasets. ",
    "code_link": ""
  },
  "icml2015_main_yinyangk-meansadrop-inreplacementoftheclassick-meanswithconsistentspeedup": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup",
    "authors": [
      "Yufei Ding",
      "Yue Zhao",
      "Xipeng Shen",
      "Madanlal Musuvathi",
      "Todd Mytkowicz"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ding15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ding15.pdf",
    "published": "2015-06",
    "summary": " This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance\u2014plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does\u2014makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance. ",
    "code_link": ""
  },
  "icml2015_main_ordinalmixedmembershipmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Ordinal Mixed Membership Models",
    "authors": [
      "Seppo Virtanen",
      "Mark Girolami"
    ],
    "page_url": "http://proceedings.mlr.press/v37/virtanen15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/virtanen15.pdf",
    "published": "2015-06",
    "summary": " We present a novel class of mixed membership models for joint distributions of groups of observations that co-occur with ordinal response variables for each group for learning statistical associations between the ordinal response variables and the observation groups. The class of proposed models addresses a requirement for predictive and diagnostic methods in a wide range of practical contemporary applications. In this work, by way of illustration, we apply the models to a collection of consumer-generated reviews of mobile software applications, where each review contains unstructured text data accompanied with an ordinal rating, and demonstrate that the models infer useful and meaningful recurring patterns of consumer feedback. We also compare the developed models to relevant existing works, which rely on improper statistical assumptions for ordinal variables, showing significant improvements both in predictive ability and knowledge extraction. ",
    "code_link": ""
  },
  "icml2015_main_onlinetrackingbylearningdiscriminativesaliencymapwithconvolutionalneuralnetwork": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network",
    "authors": [
      "Seunghoon Hong",
      "Tackgeun You",
      "Suha Kwak",
      "Bohyung Han"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hong15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hong15.pdf",
    "published": "2015-06",
    "summary": " We propose an online visual tracking algorithm by learning discriminative saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained on a large-scale image repository in offline, our algorithm takes outputs from hidden layers of the network as feature descriptors since they show excellent representation performance in various general visual recognition problems. The features are used to learn discriminative target appearance models using an online Support Vector Machine (SVM). In addition, we construct target-specific saliency map by back-projecting CNN features with guidance of the SVM, and obtain the final tracking result in each frame based on the appearance model generatively constructed with the saliency map. Since the saliency map reveals spatial configuration of target effectively, it improves target localization accuracy and enables us to achieve pixel-level target segmentation. We verify the effectiveness of our tracking algorithm through extensive experiment on a challenging benchmark, where our method illustrates outstanding performance compared to the state-of-the-art tracking algorithms. ",
    "code_link": ""
  },
  "icml2015_main_fastkroneckerinferenceingaussianprocesseswithnon-gaussianlikelihoods": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods",
    "authors": [
      "Seth Flaxman",
      "Andrew Wilson",
      "Daniel Neill",
      "Hannes Nickisch",
      "Alex Smola"
    ],
    "page_url": "http://proceedings.mlr.press/v37/flaxman15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/flaxman15.pdf",
    "published": "2015-06",
    "summary": " Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs require O(n^3) computations and O(n^2) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning (Wilson et al., 2014). However, fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scalable Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requiring O(D n^(D+1)/D) operations and O(D n^2/D) storage, for n training data-points on a dense D > 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts. ",
    "code_link": ""
  },
  "icml2015_main_statisticalandalgorithmicperspectivesonrandomizedsketchingforordinaryleast-squares": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares",
    "authors": [
      "Garvesh Raskutti",
      "Michael Mahoney"
    ],
    "page_url": "http://proceedings.mlr.press/v37/raskutti15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/raskutti15.pdf",
    "published": "2015-06",
    "summary": " We consider statistical and algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. Prior results show that, from an \\emphalgorithmic perspective, when using sketching matrices constructed from random projections and leverage-score sampling, if the number of samples r much smaller than the original sample size n, then the worst-case (WC) error is the same as solving the original problem, up to a very small relative error. From a \\emphstatistical perspective, one typically considers the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling algorithms. Among other results, we show that the RE can be upper bounded when r is much smaller than n, while the PE typically requires the number of samples r to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE can not be improved. ",
    "code_link": ""
  },
  "icml2015_main_ontd(0)withfunctionapproximationconcentrationboundsandacenteredvariantwithexponentialconvergence": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence",
    "authors": [
      "Nathaniel Korda",
      "Prashanth La"
    ],
    "page_url": "http://proceedings.mlr.press/v37/korda15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/korda15.pdf",
    "published": "2015-06",
    "summary": " We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings. ",
    "code_link": ""
  },
  "icml2015_main_learningparametric-outputhmmswithtwoaliasedstates": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Parametric-Output HMMs with Two Aliased States",
    "authors": [
      "Roi Weiss",
      "Boaz Nadler"
    ],
    "page_url": "http://proceedings.mlr.press/v37/weiss15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/weiss15.pdf",
    "published": "2015-06",
    "summary": " In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations. ",
    "code_link": ""
  },
  "icml2015_main_latentgaussianprocessesfordistributionestimationofmultivariatecategoricaldata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data",
    "authors": [
      "Yarin Gal",
      "Yutian Chen",
      "Zoubin Ghahramani"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gala15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gala15.pdf",
    "published": "2015-06",
    "summary": " Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data. ",
    "code_link": ""
  },
  "icml2015_main_improvingthegaussianprocesssparsespectrumapproximationbyrepresentinguncertaintyinfrequencyinputs": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs",
    "authors": [
      "Yarin Gal",
      "Richard Turner"
    ],
    "page_url": "http://proceedings.mlr.press/v37/galb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/galb15.pdf",
    "published": "2015-06",
    "summary": " Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting. ",
    "code_link": "https://github.com/yaringal/VSSGP"
  },
  "icml2015_main_rankingfromstochasticpairwisepreferencesrecoveringcondorcetwinnersandtournamentsolutionsetsatthetop": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top",
    "authors": [
      "Arun Rajkumar",
      "Suprovat Ghoshal",
      "Lek-Heng Lim",
      "Shivani Agarwal"
    ],
    "page_url": "http://proceedings.mlr.press/v37/rajkumar15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/rajkumar15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of ranking n items from stochastically sampled pairwise preferences. It was shown recently that when the underlying pairwise preferences are acyclic, several algorithms including the Rank Centrality algorithm, the Matrix Borda algorithm, and the SVM-RankAggregation algorithm succeed in recovering a ranking that minimizes a global pairwise disagreement error (Rajkumar and Agarwal, 2014). In this paper, we consider settings where pairwise preferences can contain cycles. In such settings, one may still like to be able to recover \u2018good\u2019 items at the top of the ranking. For example, if a Condorcet winner exists that beats every other item, it is natural to ask that this be ranked at the top. More generally, several tournament solution concepts such as the top cycle, Copeland set, Markov set and others have been proposed in the social choice literature for choosing a set of winners in the presence of cycles. We show that existing algorithms can fail to perform well in terms of ranking Condorcet winners and various natural tournament solution sets at the top. We then give alternative ranking algorithms that provably rank Condorcet winners, top cycles, and other tournament solution sets of interest at the top. In all cases, we give finite sample complexity bounds for our algorithms to recover such winners. As a by-product of our analysis, we also obtain an improved sample complexity bound for the Rank Centrality algorithm to recover an optimal ranking under a Bradley-Terry-Luce (BTL) condition, which answers an open question of Rajkumar and Agarwal (2014). ",
    "code_link": ""
  },
  "icml2015_main_stochasticdualcoordinateascentwithadaptiveprobabilities": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities",
    "authors": [
      "Dominik Csiba",
      "Zheng Qu",
      "Peter Richtarik"
    ],
    "page_url": "http://proceedings.mlr.press/v37/csiba15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/csiba15.pdf",
    "published": "2015-06",
    "summary": " This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods. ",
    "code_link": ""
  },
  "icml2015_main_vector-spacemarkovrandomfieldsviaexponentialfamilies": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Vector-Space Markov Random Fields via Exponential Families",
    "authors": [
      "Wesley Tansey",
      "Oscar Hernan Madrid Padilla",
      "Arun Sai Suggala",
      "Pradeep Ravikumar"
    ],
    "page_url": "http://proceedings.mlr.press/v37/tansey15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/tansey15.pdf",
    "published": "2015-06",
    "summary": " We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent M-estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a real-world case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available. ",
    "code_link": "https://github.com/tansey/vsmrfs"
  },
  "icml2015_main_jump-meanssmall-varianceasymptoticsformarkovjumpprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes",
    "authors": [
      "Jonathan Huggins",
      "Karthik Narasimhan",
      "Ardavan Saeedi",
      "Vikash Mansinghka"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hugginsa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hugginsa15.pdf",
    "published": "2015-06",
    "summary": " Markov jump processes (MJPs) are used to model a wide range of phenomenon from disease progression to RNA path folding. However, existing methods suffer from a number of shortcomings: degenerate trajectories in the case of ML estimation of parametric models and poor inferential performance in the case of nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call \\emphJUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy. ",
    "code_link": ""
  },
  "icml2015_main_lowrankapproximationusingerrorcorrectingcodingmatrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Low Rank Approximation using Error Correcting Coding Matrices",
    "authors": [
      "Shashanka Ubaru",
      "Arya Mazumdar",
      "Yousef Saad"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ubaru15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ubaru15.pdf",
    "published": "2015-06",
    "summary": " Low-rank matrix approximation is an integral component of tools such as principal component analysis (PCA), as well as is an important instrument used in applications like web search models, text mining and computer vision, e.g., face recognition. Recently, randomized algorithms were proposed to effectively construct low rank approximations of large matrices. In this paper, we show how matrices from error correcting codes can be used to find such low rank approximations. The benefits of using these code matrices are the following: (i) They are easy to generate and they reduce randomness significantly. (ii) Code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors; (iii) Unlike Fourier transforms or Hadamard matrices, which require sampling O(k\\log k) columns for a rank-k approximation, the log factor is not necessary in the case of code matrices. (iv) Under certain conditions, the approximation errors can be better and the singular values obtained can be more accurate, than those obtained using Gaussian random matrices and other structured random matrices. ",
    "code_link": ""
  },
  "icml2015_main_off-policymodel-basedlearningunderunknownfactoreddynamics": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Off-policy Model-based Learning under Unknown Factored Dynamics",
    "authors": [
      "Assaf Hallak",
      "Francois Schnitzler",
      "Timothy Mann",
      "Shie Mannor"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hallak15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hallak15.pdf",
    "published": "2015-06",
    "summary": " Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computationally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples. ",
    "code_link": ""
  },
  "icml2015_main_log-euclideanmetriclearningonsymmetricpositivedefinitemanifoldwithapplicationtoimagesetclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification",
    "authors": [
      "Zhiwu Huang",
      "Ruiping Wang",
      "Shiguang Shan",
      "Xianqiu Li",
      "Xilin Chen"
    ],
    "page_url": "http://proceedings.mlr.press/v37/huanga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/huanga15.pdf",
    "published": "2015-06",
    "summary": " The manifold of Symmetric Positive Definite (SPD) matrices has been successfully used for data representation in image set classification. By endowing the SPD manifold with Log-Euclidean Metric, existing methods typically work on vector-forms of SPD matrix logarithms. This however not only inevitably distorts the geometrical structure of the space of SPD matrix logarithms but also brings low efficiency especially when the dimensionality of SPD matrix is high. To overcome this limitation, we propose a novel metric learning approach to work directly on logarithms of SPD matrices. Specifically, our method aims to learn a tangent map that can directly transform the matrix logarithms from the original tangent space to a new tangent space of more discriminability. Under the tangent map framework, the novel metric learning can then be formulated as an optimization problem of seeking a Mahalanobis-like matrix, which can take the advantage of traditional metric learning techniques. Extensive evaluations on several image set classification tasks demonstrate the effectiveness of our proposed metric learning method. ",
    "code_link": ""
  },
  "icml2015_main_asymmetrictransferlearningwithdeepgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Asymmetric Transfer Learning with Deep Gaussian Processes",
    "authors": [
      "Melih Kandemir"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kandemir15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kandemir15.pdf",
    "published": "2015-06",
    "summary": " We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images. ",
    "code_link": "https://github.com/melihkandemir/atldgp"
  },
  "icml2015_main_towardsalowersamplecomplexityforrobustone-bitcompressedsensing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing",
    "authors": [
      "Rongda Zhu",
      "Quanquan Gu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhua15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhua15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we propose a novel algorithm based on nonconvex sparsity-inducing penalty for one-bit compressed sensing. We prove that our algorithm has a sample complexity of O(s/\u03b5^2) for strong signals, and O(s\\log d/\u03b5^2) for weak signals, where s is the number of nonzero entries in the signal vector, d is the signal dimension and \u03b5is the recovery error. For general signals, the sample complexity of our algorithm lies between O(s/\u03b5^2) and O(s\\log d/\u03b5^2). This is a remarkable improvement over the existing best sample complexity O(s\\log d/\u03b5^2). Furthermore, we show that our algorithm achieves exact support recovery with high probability for strong signals. Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority of our algorithm for both approximate signal and support recovery in the noisy setting. ",
    "code_link": ""
  },
  "icml2015_main_bilbowafastbilingualdistributedrepresentationswithoutwordalignments": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments",
    "authors": [
      "Stephan Gouws",
      "Yoshua Bengio",
      "Greg Corrado"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gouws15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gouws15.pdf",
    "published": "2015-06",
    "summary": " We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperforms state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on the WMT11 data. ",
    "code_link": "https://github.com/gouwsmeister/bilbowa"
  },
  "icml2015_main_multi-viewsparseco-clusteringviaproximalalternatinglinearizedminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization",
    "authors": [
      "Jiangwen Sun",
      "Jin Lu",
      "Tingyang Xu",
      "Jinbo Bi"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sunb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sunb15.pdf",
    "published": "2015-06",
    "summary": " When multiple views of data are available for a set of subjects, co-clustering aims to identify subject clusters that agree across the different views. We explore the problem of co-clustering when the underlying clusters exist in different subspaces of each view. We propose a proximal alternating linearized minimization algorithm that simultaneously decomposes multiple data matrices into sparse row and columns vectors. This approach is able to group subjects consistently across the views and simultaneously identify the subset of features in each view that are associated with the clusters. The proposed algorithm can globally converge to a critical point of the problem. A simulation study validates that the proposed algorithm can identify the hypothesized clusters and their associated features. Comparison with several latest multi-view co-clustering methods on benchmark datasets demonstrates the superior performance of the proposed approach. ",
    "code_link": ""
  },
  "icml2015_main_cascadingbanditslearningtorankinthecascademodel": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Cascading Bandits: Learning to Rank in the Cascade Model",
    "authors": [
      "Branislav Kveton",
      "Csaba Szepesvari",
      "Zheng Wen",
      "Azin Ashkan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kveton15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kveton15.pdf",
    "published": "2015-06",
    "summary": " A search engine usually outputs a list of K web pages. The user examines this list, from the first web page to the last, and chooses the first attractive page. This model of user behavior is known as the cascade model. In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify K most attractive items. We formulate our problem as a stochastic combinatorial partial monitoring problem. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We experiment with our algorithms on several problems. The algorithms perform surprisingly well even when our modeling assumptions are violated. ",
    "code_link": ""
  },
  "icml2015_main_latenttopicnetworksaversatileprobabilisticprogrammingframeworkfortopicmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Latent Topic Networks: A Versatile Probabilistic Programming Framework for Topic Models",
    "authors": [
      "James Foulds",
      "Shachi Kumar",
      "Lise Getoor"
    ],
    "page_url": "http://proceedings.mlr.press/v37/foulds15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/foulds15.pdf",
    "published": "2015-06",
    "summary": " Topic models have become increasingly prominent text-analytic machine learning tools for research in the social sciences and the humanities. In particular, custom topic models can be developed to answer specific research questions. The design of these models requires a non-trivial amount of effort and expertise, motivating general-purpose topic modeling frameworks. In this paper we introduce latent topic networks, a flexible class of richly structured topic models designed to facilitate applied research. Custom models can straightforwardly be developed in our framework with an intuitive first-order logical probabilistic programming language. Latent topic networks admit scalable training via a parallelizable EM algorithm which leverages ADMM in the M-step. We demonstrate the broad applicability of the models with case studies on modeling influence in citation networks, and U.S. Presidential State of the Union addresses. ",
    "code_link": ""
  },
  "icml2015_main_randomcoordinatedescentmethodsforminimizingdecomposablesubmodularfunctions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions",
    "authors": [
      "Alina Ene",
      "Huy Nguyen"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ene15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ene15.pdf",
    "published": "2015-06",
    "summary": " Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of \u201csimple\u201d functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster \\emphlinear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations. ",
    "code_link": ""
  },
  "icml2015_main_alpha-betadivergencesdiscovermicroandmacrostructuresindata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Alpha-Beta Divergences Discover Micro and Macro Structures in Data",
    "authors": [
      "Karthik Narayan",
      "Ali Punjani",
      "Pieter Abbeel"
    ],
    "page_url": "http://proceedings.mlr.press/v37/narayan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/narayan15.pdf",
    "published": "2015-06",
    "summary": " Although recent work in non-linear dimensionality reduction investigates multiple choices of divergence measure during optimization\u00a0\\citeyang2013icml,bunte2012neuro, little work discusses the direct effects that divergence measures have on visualization. We study this relationship, theoretically and through an empirical analysis over 10 datasets. Our works shows how the \u03b1and \u03b2parameters of the generalized alpha-beta divergence can be chosen to discover hidden macro-structures (categories, e.g. birds) or micro-structures (fine-grained classes, e.g. toucans). Our method, which generalizes t-SNE\u00a0\\citetsne, allows us to discover such structure without extensive grid searches over (\u03b1, \u03b2) due to our theoretical analysis: such structure is apparent with particular choices of (\u03b1, \u03b2) that generalize across datasets. We also discuss efficient parallel CPU and GPU schemes which are non-trivial due to the tree-structures employed in optimization and the large datasets that do not fully fit into GPU memory. Our method runs 20x faster than the fastest published code\u00a0\\citefmm. We conclude with detailed case studies on the following very large datasets: ILSVRC 2012, a standard computer vision dataset with 1.2M images; SUSY, a particle physics dataset with 5M instances; and HIGGS, another particle physics dataset with 11M instances. This represents the largest published visualization attained by SNE methods. We have open-sourced our visualization code: \\texttthttp://rll.berkeley.edu/absne/. ",
    "code_link": ""
  },
  "icml2015_main_fictitiousself-playinextensive-formgames": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fictitious Self-Play in Extensive-Form Games",
    "authors": [
      "Johannes Heinrich",
      "Marc Lanctot",
      "David Silver"
    ],
    "page_url": "http://proceedings.mlr.press/v37/heinrich15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/heinrich15.pdf",
    "published": "2015-06",
    "summary": " Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria. ",
    "code_link": ""
  },
  "icml2015_main_counterfactualriskminimizationlearningfromloggedbanditfeedback": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
    "authors": [
      "Adith Swaminathan",
      "Thorsten Joachims"
    ],
    "page_url": "http://proceedings.mlr.press/v37/swaminathan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/swaminathan15.pdf",
    "published": "2015-06",
    "summary": " We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method \u2013 called Policy Optimizer for Exponential Models (POEM) \u2013 for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art. ",
    "code_link": ""
  },
  "icml2015_main_thehedgealgorithmonacontinuum": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Hedge Algorithm on a Continuum",
    "authors": [
      "Walid Krichene",
      "Maximilian Balandat",
      "Claire Tomlin",
      "Alexandre Bayen"
    ],
    "page_url": "http://proceedings.mlr.press/v37/krichene15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/krichene15.pdf",
    "published": "2015-06",
    "summary": " We consider an online optimization problem on a subset S of R^n (not necessarily convex), in which a decision maker chooses, at each iteration t, a probability distribution x^(t) over S, and seeks to minimize a cumulative expected loss, where each loss is a Lipschitz function revealed at the end of iteration t. Building on previous work, we propose a generalized Hedge algorithm and show a O(\\sqrtt \\log t) bound on the regret when the losses are uniformly Lipschitz and S is uniformly fat (a weaker condition than convexity). Finally, we propose a generalization to the dual averaging method on the set of Lebesgue-continuous distributions over S. ",
    "code_link": ""
  },
  "icml2015_main_alineardynamicalsystemmodelfortext": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Linear Dynamical System Model for Text",
    "authors": [
      "David Belanger",
      "Sham Kakade"
    ],
    "page_url": "http://proceedings.mlr.press/v37/belanger15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/belanger15.pdf",
    "published": "2015-06",
    "summary": " Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words\u2019 local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words\u2019 representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple co-occurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity. ",
    "code_link": ""
  },
  "icml2015_main_unsupervisedlearningofvideorepresentationsusinglstms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Unsupervised Learning of Video Representations using LSTMs",
    "authors": [
      "Nitish Srivastava",
      "Elman Mansimov",
      "Ruslan Salakhudinov"
    ],
    "page_url": "http://proceedings.mlr.press/v37/srivastava15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/srivastava15.pdf",
    "published": "2015-06",
    "summary": " We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \u2013 patches of image pixels and high-level representations (\u201cpercepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem \u2013 human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance. ",
    "code_link": ""
  },
  "icml2015_main_messagepassingforcollectivegraphicalmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Message Passing for Collective Graphical Models",
    "authors": [
      "Tao Sun",
      "Dan Sheldon",
      "Akshat Kumar"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sunc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sunc15.pdf",
    "published": "2015-06",
    "summary": " Collective graphical models (CGMs) are a formalism for inference and learning about a population of independent and identically distributed individuals when only noisy aggregate data are available. We highlight a close connection between approximate MAP inference in CGMs and marginal inference in standard graphical models. The connection leads us to derive a novel Belief Propagation (BP) style algorithm for collective graphical models. Mathematically, the algorithm is a strict generalization of BP\u2014it can be viewed as an extension to minimize the Bethe free energy plus additional energy terms that are non-linear functions of the marginals. For CGMs, the algorithm is much more efficient than previous approaches to inference. We demonstrate its performance on two synthetic experiments concerning bird migration and collective human mobility. ",
    "code_link": ""
  },
  "icml2015_main_dp-spacebayesiannonparametricsubspaceclusteringwithsmall-varianceasymptotics": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "DP-space: Bayesian Nonparametric Subspace Clustering with Small-variance Asymptotics",
    "authors": [
      "Yining Wang",
      "Jun Zhu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wanga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wanga15.pdf",
    "published": "2015-06",
    "summary": " Subspace clustering separates data points approximately lying on union of affine subspaces into several clusters. This paper presents a novel nonparametric Bayesian subspace clustering model that infers both the number of subspaces and the dimension of each subspace from the observed data. Though the posterior inference is hard, our model leads to a very efficient deterministic algorithm, DP-space, which retains the nonparametric ability under a small-variance asymptotic analysis. DP-space monotonically minimizes an intuitive objective with an explicit tradeoff between data fitness and model complexity. Experimental results demonstrate that DP-space outperforms various competitors in terms of clustering accuracy and at the same time it is highly efficient. ",
    "code_link": ""
  },
  "icml2015_main_hawkestopicajointmodelfornetworkinferenceandtopicmodelingfromtext-basedcascades": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "HawkesTopic: A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades",
    "authors": [
      "Xinran He",
      "Theodoros Rekatsinas",
      "James Foulds",
      "Lise Getoor",
      "Yan Liu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/he15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/he15.pdf",
    "published": "2015-06",
    "summary": " Understanding the diffusion of information in social network and social media requires modeling the text diffusion process. In this work, we develop the HawkesTopic model (HTM) for analyzing text-based cascades, such as \"retweeting a post\" or \"publishing a follow-up blog post\". HTM combines Hawkes processes and topic modeling to simultaneously reason about the information diffusion pathways and the topics characterizing the observed textual information. We show how to jointly infer them with a mean-field variational inference algorithm and validate our approach on both synthetic and real-world data sets, including a news media dataset for modeling information diffusion, and an ArXiv publication dataset for modeling scientific influence. The results show that HTM is significantly more accurate than several baselines for both tasks. ",
    "code_link": ""
  },
  "icml2015_main_mademaskedautoencoderfordistributionestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "MADE: Masked Autoencoder for Distribution Estimation",
    "authors": [
      "Mathieu Germain",
      "Karol Gregor",
      "Iain Murray",
      "Hugo Larochelle"
    ],
    "page_url": "http://proceedings.mlr.press/v37/germain15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/germain15.pdf",
    "published": "2015-06",
    "summary": " There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder\u2019s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators. ",
    "code_link": ""
  },
  "icml2015_main_anonlinelearningalgorithmforbilinearmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An Online Learning Algorithm for Bilinear Models",
    "authors": [
      "Yuanbin Wu",
      "Shiliang Sun"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wua15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wua15.pdf",
    "published": "2015-06",
    "summary": " We investigate the bilinear model, which is a matrix form linear model with the rank 1 constraint. A new online learning algorithm is proposed to train the model parameters. Our algorithm runs in the manner of online mirror descent, and gradients are computed by the power iteration. To analyze it, we give a new second order approximation of the squared spectral norm, which helps us to get a regret bound. Experiments on two sequential labelling tasks give positive results. ",
    "code_link": ""
  },
  "icml2015_main_adaptivebeliefpropagation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Adaptive Belief Propagation",
    "authors": [
      "Georgios Papachristoudis",
      "John Fisher"
    ],
    "page_url": "http://proceedings.mlr.press/v37/papachristoudis15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/papachristoudis15.pdf",
    "published": "2015-06",
    "summary": " Graphical models are widely used in inference problems. In practice, one may construct a single large-scale model to explain a phenomenon of interest, which may be utilized in a variety of settings. The latent variables of interest, which can differ in each setting, may only represent a small subset of all variables. The marginals of variables of interest may change after the addition of measurements at different time points. In such adaptive settings, naive algorithms, such as standard belief propagation (BP), may utilize many unnecessary computations by propagating messages over the entire graph. Here, we formulate an efficient inference procedure, termed adaptive BP (AdaBP), suitable for adaptive inference settings. We show that it gives exact results for trees in discrete and Gaussian Markov Random Fields (MRFs), and provide an extension to Gaussian loopy graphs. We also provide extensions on finding the most likely sequence of the entire latent graph. Lastly, we compare the proposed method to standard BP and to that of (Sumer et al., 2011), which tackles the same problem. We show in synthetic and real experiments that it outperforms standard BP by orders of magnitude and explore the settings that it is advantageous over (Sumer et al., 2011). ",
    "code_link": "https://github.com/geopapa11/adabp"
  },
  "icml2015_main_large-scalelog-determinantcomputationthroughstochasticchebyshevexpansions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Large-scale log-determinant computation through stochastic Chebyshev expansions",
    "authors": [
      "Insu Han",
      "Dmitry Malioutov",
      "Jinwoo Shin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hana15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hana15.pdf",
    "published": "2015-06",
    "summary": " Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids and metric and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables (i.e., the matrix dimension), which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Shur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables. ",
    "code_link": ""
  },
  "icml2015_main_differentiallyprivatebayesianoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Differentially Private Bayesian Optimization",
    "authors": [
      "Matt Kusner",
      "Jacob Gardner",
      "Roman Garnett",
      "Kilian Weinberger"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kusnera15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kusnera15.pdf",
    "published": "2015-06",
    "summary": " Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters of a wide variety of machine learning models. The success of machine learning has led practitioners in diverse real-world settings to learn classifiers for practical problems. As machine learning becomes commonplace, Bayesian optimization becomes an attractive method for practitioners to automate the process of classifier hyper-parameter tuning. A key observation is that the data used for tuning models in these settings is often sensitive. Certain data such as genetic predisposition, personal email statistics, and car accident history, if not properly private, may be at risk of being inferred from Bayesian optimization outputs. To address this, we introduce methods for releasing the best hyper-parameters and classifier accuracy privately. Leveraging the strong theoretical guarantees of differential privacy and known Bayesian optimization convergence bounds, we prove that under a GP assumption these private quantities are often near-optimal. Finally, even if this assumption is not satisfied, we can use different smoothness guarantees to protect privacy. ",
    "code_link": ""
  },
  "icml2015_main_anearly-lineartimeframeworkforgraph-structuredsparsity": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Nearly-Linear Time Framework for Graph-Structured Sparsity",
    "authors": [
      "Chinmay Hegde",
      "Piotr Indyk",
      "Ludwig Schmidt"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hegde15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hegde15.pdf",
    "published": "2015-06",
    "summary": " We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms improve on prior work also in practice. ",
    "code_link": ""
  },
  "icml2015_main_supportmatrixmachines": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Support Matrix Machines",
    "authors": [
      "Luo Luo",
      "Yubo Xie",
      "Zhihua Zhang",
      "Wu-Jun Li"
    ],
    "page_url": "http://proceedings.mlr.press/v37/luo15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/luo15.pdf",
    "published": "2015-06",
    "summary": " In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers algorithm for solving the problem. Experimental results on EEG and face image classification data show that our model is more robust and efficient than the state-of-the-art methods. ",
    "code_link": ""
  },
  "icml2015_main_rademacherobservations,privatedata,andboosting": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Rademacher Observations, Private Data, and Boosting",
    "authors": [
      "Richard Nock",
      "Giorgio Patrini",
      "Arik Friedman"
    ],
    "page_url": "http://proceedings.mlr.press/v37/nock15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/nock15.pdf",
    "published": "2015-06",
    "summary": " The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear classifiers, the minimization of the logistic loss is \\textitequivalent to the minimization of an exponential \\textitrado-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \\textitsame classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \\textitdirectly used to classify \\textitobservations. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \\textitlogistic loss (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \\textitcomplete set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados brings non-trivial privacy vs accuracy tradeoffs. ",
    "code_link": ""
  },
  "icml2015_main_fromwordembeddingstodocumentdistances": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "From Word Embeddings To Document Distances",
    "authors": [
      "Matt Kusner",
      "Yu Sun",
      "Nicholas Kolkin",
      "Kilian Weinberger"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kusnerb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kusnerb15.pdf",
    "published": "2015-06",
    "summary": " We present the Word Mover\u2019s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover\u2019s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates. ",
    "code_link": ""
  },
  "icml2015_main_bayesianandempiricalbayesianforests": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bayesian and Empirical Bayesian Forests",
    "authors": [
      "Taddy Matthew",
      "Chun-Sheng Chen",
      "Jun Yu",
      "Mitch Wyle"
    ],
    "page_url": "http://proceedings.mlr.press/v37/matthew15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/matthew15.pdf",
    "published": "2015-06",
    "summary": " We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view such ensembles as samples from a posterior distribution. This insight motivates a class of Bayesian Forest (BF) algorithms that provide small gains in performance and large gains in interpretability. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This motivates an empirical Bayesian Forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform sub-sampling based alternatives by a large margin. ",
    "code_link": ""
  },
  "icml2015_main_inferringgraphsfromcascadesasparserecoveryframework": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Inferring Graphs from Cascades: A Sparse Recovery Framework",
    "authors": [
      "Jean Pouget-Abadie",
      "Thibaut Horel"
    ],
    "page_url": "http://proceedings.mlr.press/v37/pouget-abadie15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/pouget-abadie15.pdf",
    "published": "2015-06",
    "summary": " In the Graph Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. In this paper, we approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graph\u2019s edges with high probability and O(s log m) measurements where s is the maximum degree of the graph and m is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound of \u03a9(s \\log m/s) and validate our approach empirically on synthetic graphs. ",
    "code_link": ""
  },
  "icml2015_main_distributedbox-constrainedquadraticoptimizationforduallinearsvm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Distributed Box-Constrained Quadratic Optimization for Dual Linear SVM",
    "authors": [
      "Ching-Pei Lee",
      "Dan Roth"
    ],
    "page_url": "http://proceedings.mlr.press/v37/leea15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/leea15.pdf",
    "published": "2015-06",
    "summary": " Training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine, motivating recent works on developing algorithms that train in a distributed fashion. This paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines (SVMs) with large data. Our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration, using an efficient method that requires only O(1) communication cost to ensure fast convergence. With this optimal step size, our approach is superior to other methods by possessing global linear convergence, or, equivalently, O(\\log(1/\u03b5)) iteration complexity for an epsilon-accurate solution, for distributedly solving the non-strongly-convex linear SVM dual problem. Experiments also show that our method is significantly faster than state-of- the-art distributed linear SVM algorithms including DSVM-AVE, DisDCA and TRON. ",
    "code_link": ""
  },
  "icml2015_main_safeexplorationforoptimizationwithgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Safe Exploration for Optimization with Gaussian Processes",
    "authors": [
      "Yanan Sui",
      "Alkis Gotovos",
      "Joel Burdick",
      "Andreas Krause"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sui15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sui15.pdf",
    "published": "2015-06",
    "summary": " We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified \"safety\" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation. ",
    "code_link": ""
  },
  "icml2015_main_theladderareliableleaderboardformachinelearningcompetitions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions",
    "authors": [
      "Avrim Blum",
      "Moritz Hardt"
    ],
    "page_url": "http://proceedings.mlr.press/v37/blum15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/blum15.pdf",
    "published": "2015-06",
    "summary": " The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever. ",
    "code_link": "https://github.com/mrtzh/Ladder.jl"
  },
  "icml2015_main_enablingscalablestochasticgradient-basedinferenceforgaussianprocessesbyemployingtheunbiasedlinearsystemsolver(ulisse)": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)",
    "authors": [
      "Maurizio Filippone",
      "Raphael Engler"
    ],
    "page_url": "http://proceedings.mlr.press/v37/filippone15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/filippone15.pdf",
    "published": "2015-06",
    "summary": " In applications of Gaussian processes where quantification of uncertainty is of primary interest, it is necessary to accurately characterize the posterior distribution over covariance parameters. This paper proposes an adaptation of the Stochastic Gradient Langevin Dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood. In Gaussian process regression, this has the enormous advantage that stochastic gradients can be computed by solving linear systems only. A novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients. The results demonstrate the possibility to enable scalable and exact (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors. ",
    "code_link": ""
  },
  "icml2015_main_findinggalaxiesintheshadowsofquasarswithgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Finding Galaxies in the Shadows of Quasars with Gaussian Processes",
    "authors": [
      "Roman Garnett",
      "Shirley Ho",
      "Jeff Schneider"
    ],
    "page_url": "http://proceedings.mlr.press/v37/garnett15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/garnett15.pdf",
    "published": "2015-06",
    "summary": " We develop an automated technique for detecting damped Lyman-\u03b1absorbers (DLAs) along spectroscopic sightlines to quasi-stellar objects (QSOs or quasars). The detection of DLAs in large-scale spectroscopic surveys such as SDSS\u2013III is critical to address outstanding cosmological questions, such as the nature of galaxy formation. We use nearly 50000 QSO spectra to learn a tailored Gaussian process model for quasar emission spectra, which we apply to the DLA detection problem via Bayesian model selection. We demonstrate our method\u2019s effectiveness with a large-scale validation experiment on over 100000 spectra, with excellent performance. ",
    "code_link": ""
  },
  "icml2015_main_followingtheperturbedleaderforonlinestructuredlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Following the Perturbed Leader for Online Structured Learning",
    "authors": [
      "Alon Cohen",
      "Tamir Hazan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/cohena15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/cohena15.pdf",
    "published": "2015-06",
    "summary": " We investigate a new Follow the Perturbed Leader (FTPL) algorithm for online structured prediction problems. We show a regret bound which is comparable to the state of the art of FTPL algorithms and is comparable with the best possible regret in some cases. To better understand FTPL algorithms for online structured learning, we present a lower bound on the regret for a large and natural class of FTPL algorithms that use logconcave perturbations. We complete our investigation with an online shortest path experiment and empirically show that our algorithm is both statistically and computationally efficient. ",
    "code_link": ""
  },
  "icml2015_main_reifiedcontextmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Reified Context Models",
    "authors": [
      "Jacob Steinhardt",
      "Percy Liang"
    ],
    "page_url": "http://proceedings.mlr.press/v37/steinhardta15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/steinhardta15.pdf",
    "published": "2015-06",
    "summary": " A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the choice of factors in a graphical model (the contexts) be random variables inside the model itself. In this sense, the contexts are reified and can be chosen in a data-dependent way. Empirically, we show that our approach obtains expressivity and coverage on three sequence modeling tasks. ",
    "code_link": ""
  },
  "icml2015_main_large-scalemarkovdecisionproblemswithklcontrolcostanditsapplicationtocrowdsourcing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing",
    "authors": [
      "Yasin Abbasi-Yadkori",
      "Peter Bartlett",
      "Xi Chen",
      "Alan Malek"
    ],
    "page_url": "http://proceedings.mlr.press/v37/abbasi-yadkori15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/abbasi-yadkori15.pdf",
    "published": "2015-06",
    "summary": " We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical costs of finding the optimal policy scale with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular, we show that for problems with a Kullback-Leibler divergence cost function, we can reduce policy optimization to a convex optimization and solve it approximately using a stochastic subgradient algorithm. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by controlling the important crowdsourcing application of budget allocation in crowd labeling. ",
    "code_link": ""
  },
  "icml2015_main_learningfast-mixingmodelsforstructuredprediction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Fast-Mixing Models for Structured Prediction",
    "authors": [
      "Jacob Steinhardt",
      "Percy Liang"
    ],
    "page_url": "http://proceedings.mlr.press/v37/steinhardtb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/steinhardtb15.pdf",
    "published": "2015-06",
    "summary": " Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the resulting approximate gradients can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks. ",
    "code_link": ""
  },
  "icml2015_main_aprobabilisticmodelfordirtymulti-taskfeatureselection": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Probabilistic Model for Dirty Multi-task Feature Selection",
    "authors": [
      "Daniel Hernandez-Lobato",
      "Jose Miguel Hernandez-Lobato",
      "Zoubin Ghahramani"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hernandez-lobatoa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hernandez-lobatoa15.pdf",
    "published": "2015-06",
    "summary": " Multi-task feature selection methods often make the hypothesis that learning tasks share relevant and irrelevant features. However, this hypothesis may be too restrictive in practice. For example, there may be a few tasks with specific relevant and irrelevant features (outlier tasks). Similarly, a few of the features may be relevant for only some of the tasks (outlier features). To account for this, we propose a model for multi-task feature selection based on a robust prior distribution that introduces a set of binary latent variables to identify outlier tasks and outlier features. Expectation propagation can be used for efficient approximate inference under the proposed prior. Several experiments show that a model based on the new robust prior provides better predictive performance than other benchmark methods. ",
    "code_link": ""
  },
  "icml2015_main_ondeepmulti-viewrepresentationlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On Deep Multi-View Representation Learning",
    "authors": [
      "Weiran Wang",
      "Raman Arora",
      "Karen Livescu",
      "Jeff Bilmes"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wangb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wangb15.pdf",
    "published": "2015-06",
    "summary": " We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). ",
    "code_link": ""
  },
  "icml2015_main_learningprogramembeddingstopropagatefeedbackonstudentcode": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Program Embeddings to Propagate Feedback on Student Code",
    "authors": [
      "Chris Piech",
      "Jonathan Huang",
      "Andy Nguyen",
      "Mike Phulsuksombati",
      "Mehran Sahami",
      "Leonidas Guibas"
    ],
    "page_url": "http://proceedings.mlr.press/v37/piech15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/piech15.pdf",
    "published": "2015-06",
    "summary": " Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University\u2019s CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions. ",
    "code_link": ""
  },
  "icml2015_main_safesubspacescreeningfornuclearnormregularizedleastsquaresproblems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems",
    "authors": [
      "Qiang Zhou",
      "Qi Zhao"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhoua15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhoua15.pdf",
    "published": "2015-06",
    "summary": " Nuclear norm regularization has been shown very promising for pursing a low rank matrix solution in various machine learning problems. Many efforts have been devoted to develop efficient algorithms for solving the optimization problem in nuclear norm regularization. Solving it for large-scale matrix variables, however, is still a challenging task since the complexity grows fast with the size of matrix variable. In this work, we propose a novel method called safe subspace screening (SSS), to improve the efficiency of the solver for nuclear norm regularized least squares problems. Motivated by the fact that the low rank solution can be represented by a few subspaces, the proposed method accurately discards a predominant percentage of inactive subspaces prior to solving the problem to reduce problem size. Consequently, a much smaller problem is required to solve, making it more efficient than optimizing the original problem. The proposed SSS is safe, in that its solution is identical to the solution from the solver. In addition, the proposed SSS can be used together with any existing nuclear norm solver since it is independent of the solver. Extensive results on several synthetic and real data sets show that the proposed SSS is very effective in inactive subspace screening. ",
    "code_link": ""
  },
  "icml2015_main_efficientlearninginlarge-scalecombinatorialsemi-bandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits",
    "authors": [
      "Zheng Wen",
      "Branislav Kveton",
      "Azin Ashkan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wen15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wen15.pdf",
    "published": "2015-06",
    "summary": " A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. ",
    "code_link": ""
  },
  "icml2015_main_sweptapproximatemessagepassingforsparseestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Swept Approximate Message Passing for Sparse Estimation",
    "authors": [
      "Andre Manoel",
      "Florent Krzakala",
      "Eric Tramel",
      "Lenka Zdeborov\u00e0"
    ],
    "page_url": "http://proceedings.mlr.press/v37/manoel15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/manoel15.pdf",
    "published": "2015-06",
    "summary": " Approximate Message Passing (AMP) has been shown to be a superior method for inference problems, such as the recovery of signals from sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy and in computational efficiency. However, AMP suffers from serious convergence issues in contexts that do not exactly match its assumptions. We propose a new approach to stabilizing AMP in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our results show that this change to the AMP iteration can provide theoretically expected, but hitherto unobtainable, performance for problems on which the standard AMP iteration diverges. Additionally, we find that the computational costs of this swept coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality. ",
    "code_link": ""
  },
  "icml2015_main_simpleregretforinfinitelymanyarmedbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Simple regret for infinitely many armed bandits",
    "authors": [
      "Alexandra Carpentier",
      "Michal Valko"
    ],
    "page_url": "http://proceedings.mlr.press/v37/carpentier15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/carpentier15.pdf",
    "published": "2015-06",
    "summary": " We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter \u03b2characterizing the distribution of the near-optimal arms. We prove that depending on \u03b2, our algorithm is minimax optimal either up to a multiplicative constant or up to a \\log(n) factor. We also provide extensions to several important cases: when \u03b2is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon. ",
    "code_link": ""
  },
  "icml2015_main_exponentialintegrationforhamiltonianmontecarlo": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Exponential Integration for Hamiltonian Monte Carlo",
    "authors": [
      "Wei-Lun Chao",
      "Justin Solomon",
      "Dominik Michels",
      "Fei Sha"
    ],
    "page_url": "http://proceedings.mlr.press/v37/chao15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/chao15.pdf",
    "published": "2015-06",
    "summary": " We investigate numerical integration of ordinary differential equations (ODEs) for Hamiltonian Monte Carlo (HMC). High-quality integration is crucial for designing efficient and effective proposals for HMC. While the standard method is leapfrog (Stormer-Verlet) integration, we propose the use of an exponential integrator, which is robust to stiff ODEs with highly-oscillatory components. This oscillation is difficult to reproduce using leapfrog integration, even with carefully selected integration parameters and preconditioning. Concretely, we use a Gaussian distribution approximation to segregate stiff components of the ODE. We integrate this term analytically for stability and account for deviation from the approximation using variation of constants. We consider various ways to derive Gaussian approximations and conduct extensive empirical studies applying the proposed \u201cexponential HMC\u201d to several benchmarked learning problems. We compare to state-of-the-art methods for improving leapfrog HMC and demonstrate the advantages of our method in generating many effective samples with high acceptance rates in short running times. ",
    "code_link": "https://github.com/pujols/Exponential-HMC"
  },
  "icml2015_main_optimalregretanalysisofthompsonsamplinginstochasticmulti-armedbanditproblemwithmultipleplays": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays",
    "authors": [
      "Junpei Komiyama",
      "Junya Honda",
      "Hiroshi Nakagawa"
    ],
    "page_url": "http://proceedings.mlr.press/v37/komiyama15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/komiyama15.pdf",
    "published": "2015-06",
    "summary": " We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al.\\,(1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance. ",
    "code_link": "https://github.com/jkomiyama/multiplaybanditlib"
  },
  "icml2015_main_fastercovertrees": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Faster cover trees",
    "authors": [
      "Mike Izbicki",
      "Christian Shelton"
    ],
    "page_url": "http://proceedings.mlr.press/v37/izbicki15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/izbicki15.pdf",
    "published": "2015-06",
    "summary": " The cover tree data structure speeds up exact nearest neighbor queries over arbitrary metric spaces. This paper makes cover trees even faster. In particular, we provide (1) a simpler definition of the cover tree that reduces the number of nodes from O(n) to exactly n, (2) an additional invariant that makes queries faster in practice, (3) algorithms for constructing and querying the tree in parallel on multiprocessor systems, and (4) a more cache efficient memory layout. On standard benchmark datasets, we reduce the number of distance computations by 10\u201350%. On a large-scale bioinformatics dataset, we reduce the number of distance computations by 71%. On a large-scale image dataset, our parallel algorithm with 16 cores reduces tree construction time from 3.5 hours to 12 minutes. ",
    "code_link": ""
  },
  "icml2015_main_blitzaprincipledmeta-algorithmforscalingsparseoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization",
    "authors": [
      "Tyler Johnson",
      "Carlos Guestrin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/johnson15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/johnson15.pdf",
    "published": "2015-06",
    "summary": " By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints. ",
    "code_link": "https://github.com/tqchen/rabit"
  },
  "icml2015_main_unsuperviseddomainadaptationbybackpropagation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Unsupervised Domain Adaptation by Backpropagation",
    "authors": [
      "Yaroslav Ganin",
      "Victor Lempitsky"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ganin15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ganin15.pdf",
    "published": "2015-06",
    "summary": " Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets. ",
    "code_link": ""
  },
  "icml2015_main_non-linearcross-domaincollaborativefilteringviahyper-structuretransfer": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer",
    "authors": [
      "Yan-Fu Liu",
      "Cheng-Yu Hsu",
      "Shan-Hung Wu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/liua15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/liua15.pdf",
    "published": "2015-06",
    "summary": " The Cross Domain Collaborative Filtering (CDCF) exploits the rating matrices from multiple domains to make better recommendations. Existing CDCF methods adopt the sub-structure sharing technique that can only transfer linearly correlated knowledge between domains. In this paper, we propose the notion of Hyper-Structure Transfer (HST) that requires the rating matrices to be explained by the projections of some more complex structure, called the hyper-structure, shared by all domains, and thus allows the non-linearly correlated knowledge between domains to be identified and transferred. Extensive experiments are conducted and the results demonstrate the effectiveness of our HST models empirically. ",
    "code_link": ""
  },
  "icml2015_main_manifold-valueddirichletprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Manifold-valued Dirichlet Processes",
    "authors": [
      "Hyunwoo Kim",
      "Jia Xu",
      "Baba Vemuri",
      "Vikas Singh"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kim15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kim15.pdf",
    "published": "2015-06",
    "summary": " Statistical models for manifold-valued data permit capturing the intrinsic nature of the curved spaces in which the data lie and have been a topic of research for several decades. Typically, these formulations use geodesic curves and distances defined locally for most cases - this makes it hard to design parametric models globally on smooth manifolds. Thus, most (manifold specific) parametric models available today assume that the data lie in a small neighborhood on the manifold. To address this \u2019locality\u2019 problem, we propose a novel nonparametric model which unifies multivariate general linear models (MGLMs) using multiple tangent spaces. Our framework generalizes existing work on (both Euclidean and non-Euclidean) general linear models providing a recipe to globally extend the locally-defined parametric models (using a mixture of local models). By grouping observations into sub-populations at multiple tangent spaces, our method provides insights into the hidden structure (geodesic relationships) in the data. This yields a framework to group observations and discover geodesic relationships between covariates X and manifold-valued responses Y, which we call Dirichlet process mixtures of multivariate general linear models (DP-MGLM) on Riemannian manifolds. Finally, we present proof of concept experiments to validate our model. ",
    "code_link": ""
  },
  "icml2015_main_multi-tasklearningforsubspacesegmentation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Multi-Task Learning for Subspace Segmentation",
    "authors": [
      "Yu Wang",
      "David Wipf",
      "Qing Ling",
      "Wei Chen",
      "Ian Wassell"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wangc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wangc15.pdf",
    "published": "2015-06",
    "summary": " Subspace segmentation is the process of clustering a set of data points that are assumed to lie on the union of multiple linear or affine subspaces, and is increasingly being recognized as a fundamental tool for data analysis in high dimensional settings. Arguably one of the most successful approaches is based on the observation that the sparsest representation of a given point with respect to a dictionary formed by the others involves nonzero coefficients associated with points originating in the same subspace. Such sparse representations are computed independently for each data point via \\ell_1-norm minimization and then combined into an affinity matrix for use by a final spectral clustering step. The downside of this procedure is two-fold. First, unlike canonical compressive sensing scenarios with ideally-randomized dictionaries, the data-dependent dictionaries here are unavoidably highly structured, disrupting many of the favorable properties of the \\ell_1 norm. Secondly, by treating each data point independently, we ignore useful relationships between points that can be leveraged for jointly computing such sparse representations. Consequently, we motivate a multi-task learning-based framework for learning coupled sparse representations leading to a segmentation pipeline that is both robust against correlation structure and tailored to generate an optimal affinity matrix. Theoretical analysis and empirical tests are provided to support these claims. ",
    "code_link": ""
  },
  "icml2015_main_markovchainmontecarloandvariationalinferencebridgingthegap": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Markov Chain Monte Carlo and Variational Inference: Bridging the Gap",
    "authors": [
      "Tim Salimans",
      "Diederik Kingma",
      "Max Welling"
    ],
    "page_url": "http://proceedings.mlr.press/v37/salimans15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/salimans15.pdf",
    "published": "2015-06",
    "summary": " Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results. ",
    "code_link": ""
  },
  "icml2015_main_scalablemodelselectionforlarge-scalefactorialrelationalmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Model Selection for Large-Scale Factorial Relational Models",
    "authors": [
      "Chunchen Liu",
      "Lu Feng",
      "Ryohei Fujimaki",
      "Yusuke Muraoka"
    ],
    "page_url": "http://proceedings.mlr.press/v37/liub15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/liub15.pdf",
    "published": "2015-06",
    "summary": " With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models. ",
    "code_link": "https://github.com/premgopalan/svinet"
  },
  "icml2015_main_thepowerofrandomizationdistributedsubmodularmaximizationonmassivedatasets": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Power of Randomization: Distributed Submodular Maximization on Massive Datasets",
    "authors": [
      "Rafael Barbosa",
      "Alina Ene",
      "Huy Nguyen",
      "Justin Ward"
    ],
    "page_url": "http://proceedings.mlr.press/v37/barbosa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/barbosa15.pdf",
    "published": "2015-06",
    "summary": " A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting. ",
    "code_link": ""
  },
  "icml2015_main_dealingwithsmalldataonthegeneralizationofcontexttrees": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dealing with small data: On the generalization of context trees",
    "authors": [
      "Ralf Eggeling",
      "Mikko Koivisto",
      "Ivo Grosse"
    ],
    "page_url": "http://proceedings.mlr.press/v37/eggeling15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/eggeling15.pdf",
    "published": "2015-06",
    "summary": " Context trees (CT) are a widely used tool in machine learning for representing context-specific independences in conditional probability distributions. Parsimonious context trees (PCTs) are a recently proposed generalization of CTs that can enable statistically more efficient learning due to a higher structural flexibility, which is particularly useful for small-data settings. However, this comes at the cost of a computationally expensive structure learning algorithm, which is feasible only for domains with small alphabets and tree depths. In this work, we investigate to which degree CTs can be generalized to increase statistical efficiency while still keeping the learning computationally feasible. Approaching this goal from two different angles, we (i) propose algorithmic improvements to the PCT learning algorithm, and (ii) study further generalizations of CTs, which are inspired by PCTs, but trade structural flexibility for computational efficiency. By empirical studies both on simulated and real-world data, we demonstrate that the synergy of combining of both orthogonal approaches yields a substantial improvement in obtaining statistically efficient and computationally feasible generalizations of CTs. ",
    "code_link": ""
  },
  "icml2015_main_non-gaussiandiscriminativefactormodelsviathemax-marginrank-likelihood": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood",
    "authors": [
      "Xin Yuan",
      "Ricardo Henao",
      "Ephraim Tsalik",
      "Raymond Langley",
      "Lawrence Carin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yuan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yuan15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a max-margin version of the rank-likelihood. A discriminative factor model is then developed, integrating the new max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology. ",
    "code_link": ""
  },
  "icml2015_main_abayesiannonparametricprocedureforcomparingalgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Bayesian nonparametric procedure for comparing algorithms",
    "authors": [
      "Alessio Benavoli",
      "Giorgio Corani",
      "Francesca Mangili",
      "Marco Zaffalon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/benavoli15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/benavoli15.pdf",
    "published": "2015-06",
    "summary": " A fundamental task in machine learning is to compare the performance of multiple algorithms. This is typically performed by frequentist tests (usually the Friedman test followed by a series of multiple pairwise comparisons). This implies dealing with null hypothesis significance tests and p-values, although the shortcomings of such methods are well known. First, we propose a nonparametric Bayesian version of the Friedman test using a Dirichlet process (DP) based prior. Our derivations show that, from a Bayesian perspective, the Friedman test is an inference for a multivariate mean based on an ellipsoid inclusion test. Second, we derive a joint procedure for the analysis of the multiple comparisons which accounts for their dependencies and which is based on the posterior probability computed through the DP. The proposed approach allows verifying the null hypothesis, not only rejecting it. Third, we apply our test to perform algorithms racing, i.e., the problem of identifying the best algorithm among a large set of candidates. We show by simulation that our approach is competitive both in terms of accuracy and speed in identifying the best algorithm. ",
    "code_link": ""
  },
  "icml2015_main_convergencerateofbayesiantensorestimatoranditsminimaxoptimality": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Convergence rate of Bayesian tensor estimator and its minimax optimality",
    "authors": [
      "Taiji Suzuki"
    ],
    "page_url": "http://proceedings.mlr.press/v37/suzuki15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/suzuki15.pdf",
    "published": "2015-06",
    "summary": " We investigate the statistical convergence rate of a Bayesian low-rank tensor estimator, and derive the minimax optimal rate for learning a low-rank tensor. Our problem setting is the regression problem where the regression coefficient forms a tensor structure. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate of the Bayes tensor estimator is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a fast learning rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori. Finally, we show the minimax optimal learning rate for the tensor estimation problem, and thus show that the derived bound of the Bayes estimator is tight and actually near minimax optimal. ",
    "code_link": ""
  },
  "icml2015_main_onidentifyinggoodoptionsundercombinatoriallystructuredfeedbackinfinitenoisyenvironments": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On Identifying Good Options under Combinatorially Structured Feedback in Finite Noisy Environments",
    "authors": [
      "Yifan Wu",
      "Andras Gyorgy",
      "Csaba Szepesvari"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wub15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wub15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of identifying a good option out of finite set of options under combinatorially structured, noisy feedback about the quality of the options in a sequential process: In each round, a subset of the options, from an available set of subsets, can be selected to receive noisy information about the quality of the options in the chosen subset. The goal is to identify the highest quality option, or a group of options of the highest quality, with a small error probability, while using the smallest number of measurements. The problem generalizes best-arm identification problems. By extending previous work, we design new algorithms that are shown to be able to exploit the combinatorial structure of the problem in a nontrivial fashion, while being unimprovable in special cases. The algorithms call a set multi-covering oracle, hence their performance and efficiency is strongly tied to whether the associated set multi-covering problem can be efficiently solved. ",
    "code_link": ""
  },
  "icml2015_main_nestedsequentialmontecarlomethods": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Nested Sequential Monte Carlo Methods",
    "authors": [
      "Christian Naesseth",
      "Fredrik Lindsten",
      "Thomas Schon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/naesseth15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/naesseth15.pdf",
    "published": "2015-06",
    "summary": " We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC can in itself be used to produce such properly weighted samples. Consequently, one NSMC sampler can be used to construct an efficient high-dimensional proposal distribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our approach on several filtering problems with dimensions in the order of 100 to 1000. ",
    "code_link": ""
  },
  "icml2015_main_sparsevariationalinferenceforgeneralizedgpmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Sparse Variational Inference for Generalized GP Models",
    "authors": [
      "Rishit Sheth",
      "Yuyang Wang",
      "Roni Khardon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sheth15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sheth15.pdf",
    "published": "2015-06",
    "summary": " Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference procedures. Sparse GP inference algorithms address the cubic complexity of GPs by focusing on a small set of pseudo-samples. To date, such approaches have focused on the simple case of Gaussian observation likelihoods. This paper develops a variational sparse solution for GPs under general likelihoods by providing a new characterization of the gradients required for inference in terms of individual observation likelihood terms. In addition, we propose a simple new approach for optimizing the sparse variational approximation using a fixed point computation. We demonstrate experimentally that the fixed point operator acts as a contraction in many cases and therefore leads to fast convergence. An experimental evaluation for count regression, classification, and ordinal regression illustrates the generality and advantages of the new approach. ",
    "code_link": ""
  },
  "icml2015_main_universalvaluefunctionapproximators": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Universal Value Function Approximators",
    "authors": [
      "Tom Schaul",
      "Daniel Horgan",
      "Karol Gregor",
      "David Silver"
    ],
    "page_url": "http://proceedings.mlr.press/v37/schaul15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/schaul15.pdf",
    "published": "2015-06",
    "summary": " Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals. ",
    "code_link": ""
  },
  "icml2015_main_approximatedynamicprogrammingfortwo-playerzero-summarkovgames": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games",
    "authors": [
      "Julien Perolat",
      "Bruno Scherrer",
      "Bilal Piot",
      "Olivier Pietquin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/perolat15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/perolat15.pdf",
    "published": "2015-06",
    "summary": " This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in L_p-norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is \\frac2\u03b3(1 - \u03b3)^2 \u03b5+ \\frac1(1 - \u03b3)^2\u03b5\u2019-optimal, where \u03b5is the value function approximation error and \u03b5\u2019 is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-Q) to solve infinite horizon \u03b3-discounted two-player zero-sum stochastic games in a batch setting. It is an extension of the Fitted-Q algorithm (which solves Markov Decisions Processes in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI-Q on a simultaneous two-player game, namely Alesia. ",
    "code_link": ""
  },
  "icml2015_main_ongreedymaximizationofentropy": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On Greedy Maximization of Entropy",
    "authors": [
      "Dravyansh Sharma",
      "Ashish Kapoor",
      "Amit Deshpande"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sharma15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sharma15.pdf",
    "published": "2015-06",
    "summary": " Submodular function maximization is one of the key problems that arise in many machine learning tasks. Greedy selection algorithms are the proven choice to solve such problems, where prior theoretical work guarantees (1 - 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main goal of this paper is to explore and answer why the greedy selection does significantly better than the theoretical guarantee of (1 - 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual information as a maximization criteria. We give a theoretical justification for the nearly optimal approximation ratio via detailed analysis of the curvature of these objective functions for Gaussian RBF kernels. ",
    "code_link": ""
  },
  "icml2015_main_metadatadependentmondrianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Metadata Dependent Mondrian Processes",
    "authors": [
      "Yi Wang",
      "Bin Li",
      "Yang Wang",
      "Fang Chen"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wangd15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wangd15.pdf",
    "published": "2015-06",
    "summary": " Stochastic partition processes in a product space play an important role in modeling relational data. Recent studies on the Mondrian process have introduced more flexibility into the block structure in relational models. A side-effect of such high flexibility is that, in data sparsity scenarios, the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network. In this paper, we propose a metadata dependent Mondrian process (MDMP) to incorporate meta information into the stochastic partition process in the product space and the entity allocation process on the resulting block structure. MDMP can not only encourage homogeneous relational interactions within blocks but also discourage meta-label diversity within blocks. Regularized by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more parsimonious model structure. ",
    "code_link": ""
  },
  "icml2015_main_complexeventdetectionusingsemanticsaliencyandnearly-isotonicsvm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Complex Event Detection using Semantic Saliency and Nearly-Isotonic SVM",
    "authors": [
      "Xiaojun Chang",
      "Yi Yang",
      "Eric Xing",
      "Yaoliang Yu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/changa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/changa15.pdf",
    "published": "2015-06",
    "summary": " We aim to detect complex events in long Internet videos that may last for hours. A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading. Instead of indifferently pooling the shots, we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest. We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector. Next, we propose a new isotonic regularizer that is able to exploit the semantic ordering information. The resulting nearly-isotonic SVM classifier exhibits higher discriminative power. Computationally, we develop an efficient implementation using the proximal gradient algorithm, and we prove new, closed-form proximal steps. We conduct extensive experiments on three real-world video datasets and confirm the effectiveness of the proposed approach. ",
    "code_link": ""
  },
  "icml2015_main_rebuildingfactorizedinformationcriterionasymptoticallyaccuratemarginallikelihood": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood",
    "authors": [
      "Kohei Hayashi",
      "Shin-ichi Maeda",
      "Ryohei Fujimaki"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hayashi15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hayashi15.pdf",
    "published": "2015-06",
    "summary": " Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover previously-unknown their relationship. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results. ",
    "code_link": ""
  },
  "icml2015_main_doublenystr\u00f6mmethodanefficientandaccuratenystr\u00f6mschemeforlarge-scaledatasets": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Double Nystr\u00f6m Method: An Efficient and Accurate Nystr\u00f6m Scheme for Large-Scale Data Sets",
    "authors": [
      "Woosang Lim",
      "Minhwan Kim",
      "Haesun Park",
      "Kyomin Jung"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lima15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lima15.pdf",
    "published": "2015-06",
    "summary": " The Nystr\u00f6m method has been one of the most effective techniques for kernel-based approach that scales well to large data sets. Since its introduction, there has been a large body of work that improves the approximation accuracy while maintaining computational efficiency. In this paper, we present a novel Nystr\u00f6m method that improves both accuracy and efficiency based on a new theoretical analysis. We first provide a generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace distance. We then present our double Nystr\u00f6m method that reduces the size of the decomposition in two stages. We show that our method is highly efficient and accurate compared to other state-of-the-art Nystr\u00f6m methods by evaluating them on a number of real data sets. ",
    "code_link": ""
  },
  "icml2015_main_thecompositiontheoremfordifferentialprivacy": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Composition Theorem for Differential Privacy",
    "authors": [
      "Peter Kairouz",
      "Sewoong Oh",
      "Pramod Viswanath"
    ],
    "page_url": "http://proceedings.mlr.press/v37/kairouz15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/kairouz15.pdf",
    "published": "2015-06",
    "summary": " Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature. ",
    "code_link": ""
  },
  "icml2015_main_convexformulationforlearningfrompositiveandunlabeleddata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Convex Formulation for Learning from Positive and Unlabeled Data",
    "authors": [
      "Marthinus Du Plessis",
      "Gang Niu",
      "Masashi Sugiyama"
    ],
    "page_url": "http://proceedings.mlr.press/v37/plessis15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/plessis15.pdf",
    "published": "2015-06",
    "summary": " We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost. ",
    "code_link": ""
  },
  "icml2015_main_thresholdinfluencemodelforallocatingadvertisingbudgets": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Threshold Influence Model for Allocating Advertising Budgets",
    "authors": [
      "Atsushi Miyauchi",
      "Yuni Iwamasa",
      "Takuro Fukunaga",
      "Naonori Kakimura"
    ],
    "page_url": "http://proceedings.mlr.press/v37/miyauchi15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/miyauchi15.pdf",
    "published": "2015-06",
    "summary": " We propose a new influence model for allocating budgets to advertising channels. Our model captures customer\u2019s sensitivity to advertisements as a threshold behavior; a customer is expected to be influenced if the influence he receives exceeds his threshold. Over the threshold model, we discuss two optimization problems. The first one is the budget-constrained influence maximization. We propose two greedy algorithms based on different strategies, and analyze the performance when the influence is submodular. We then introduce a new characteristic to measure the cost-effectiveness of a marketing campaign, that is, the proportion of the resulting influence to the cost spent. We design an almost linear-time approximation algorithm to maximize the cost-effectiveness. Furthermore, we design a better-approximation algorithm based on linear programming for a special case. We conduct thorough experiments to confirm that our algorithms outperform baseline algorithms. ",
    "code_link": ""
  },
  "icml2015_main_stronglyadaptiveonlinelearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Strongly Adaptive Online Learning",
    "authors": [
      "Amit Daniely",
      "Alon Gonen",
      "Shai Shalev-Shwartz"
    ],
    "page_url": "http://proceedings.mlr.press/v37/daniely15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/daniely15.pdf",
    "published": "2015-06",
    "summary": " Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems. ",
    "code_link": ""
  },
  "icml2015_main_curalgorithmforpartiallyobservedmatrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "CUR Algorithm for Partially Observed Matrices",
    "authors": [
      "Miao Xu",
      "Rong Jin",
      "Zhi-Hua Zhou"
    ],
    "page_url": "http://proceedings.mlr.press/v37/xua15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/xua15.pdf",
    "published": "2015-06",
    "summary": " CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they cannot deal with entries in a \\it partially observed matrix, while incomplete matrices are found in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(n r\\ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n\\times n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm. ",
    "code_link": ""
  },
  "icml2015_main_adeterministicanalysisofnoisysparsesubspaceclusteringfordimensionality-reduceddata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data",
    "authors": [
      "Yining Wang",
      "Yu-Xiang Wang",
      "Aarti Singh"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wange15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wange15.pdf",
    "published": "2015-06",
    "summary": " Subspace clustering groups data into several lowrank subspaces. In this paper, we propose a theoretical framework to analyze a popular optimization-based algorithm, Sparse Subspace Clustering (SSC), when the data dimension is compressed via some random projection algorithms. We show SSC provably succeeds if the random projection is a subspace embedding, which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our analysis applies to the most general deterministic setting and is able to handle both adversarial and stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering. ",
    "code_link": ""
  },
  "icml2015_main_mra-basedstatisticallearningfromincompleterankings": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "MRA-based Statistical Learning from Incomplete Rankings",
    "authors": [
      "Eric Sibony",
      "St\u00e9phan Clemen\u00e7on",
      "J\u00e9r\u00e9mie Jakubowicz"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sibony15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sibony15.pdf",
    "published": "2015-06",
    "summary": " Statistical analysis of rank data describing preferences over small and variable subsets of a potentially large ensemble of items 1, ..., n is a very challenging problem. It is motivated by a wide variety of modern applications, such as recommender systems or search engines. However, very few inference methods have been documented in the literature to learn a ranking model from such incomplete rank data. The goal of this paper is twofold: it develops a rigorous mathematical framework for the problem of learning a ranking model from incomplete rankings and introduces a novel general statistical method to address it. Based on an original concept of multi-resolution analysis (MRA) of incomplete rankings, it finely adapts to any observation setting, leading to a statistical accuracy and an algorithmic complexity that depend directly on the complexity of the observed data. Beyond theoretical guarantees, we also provide experimental results that show its statistical performance. ",
    "code_link": ""
  },
  "icml2015_main_riskandregretofhierarchicalbayesianlearners": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Risk and Regret of Hierarchical Bayesian Learners",
    "authors": [
      "Jonathan Huggins",
      "Josh Tenenbaum"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hugginsb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hugginsb15.pdf",
    "published": "2015-06",
    "summary": " Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater \u201crobustness\u201d and the ability to \u201cshare statistical strength.\u201d Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student\u2019s t and hierarchical Gaussian priors allow us to formalize the concepts of \u201crobustness\u201d and \u201csharing statistical strength.\u201d Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems. ",
    "code_link": ""
  },
  "icml2015_main_towardsalearningtheoryofcause-effectinference": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Towards a Learning Theory of Cause-Effect Inference",
    "authors": [
      "David Lopez-Paz",
      "Krikamol Muandet",
      "Bernhard Sch\u00f6lkopf",
      "Iliya Tolstikhin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lopez-paz15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lopez-paz15.pdf",
    "published": "2015-06",
    "summary": " We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection {(S_i,l_i)}_i=1^n, where each S_i is a sample drawn from the probability distribution of X_i \\times Y_i, and l_i is a binary label indicating whether \u201cX_i \\to Y_i\u201d or \u201cX_i \u2190Y_i\u201d. Given these data, we build a causal inference rule in two steps. First, we featurize each S_i using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables. ",
    "code_link": "https://github.com/lopezpaz/causation_learning_theory"
  },
  "icml2015_main_drawarecurrentneuralnetworkforimagegeneration": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "DRAW: A Recurrent Neural Network For Image Generation",
    "authors": [
      "Karol Gregor",
      "Ivo Danihelka",
      "Alex Graves",
      "Danilo Rezende",
      "Daan Wierstra"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gregor15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gregor15.pdf",
    "published": "2015-06",
    "summary": " This paper introduces the Deep Recurrent Attentive Writer (DRAW) architecture for image generation with neural networks. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it is able to generate images that are indistinguishable from real data with the naked eye. ",
    "code_link": ""
  },
  "icml2015_main_multiviewtripletembeddinglearningattributesinmultiplemaps": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Multiview Triplet Embedding: Learning Attributes in Multiple Maps",
    "authors": [
      "Ehsan Amid",
      "Antti Ukkonen"
    ],
    "page_url": "http://proceedings.mlr.press/v37/amid15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/amid15.pdf",
    "published": "2015-06",
    "summary": " For humans, it is usually easier to make statements about the similarity of objects in relative, rather than absolute terms. Moreover, subjective comparisons of objects can be based on a number of different and independent attributes. For example, objects can be compared based on their shape, color, etc. In this paper, we consider the problem of uncovering these hidden attributes given a set of relative distance judgments in the form of triplets. The attribute that was used to generate a particular triplet in this set is unknown. Such data occurs, e.g., in crowdsourcing applications where the triplets are collected from a large group of workers. We propose the Multiview Triplet Embedding (MVTE) algorithm that produces a number of low-dimensional maps, each corresponding to one of the hidden attributes. The method can be used to assess how many different attributes were used to create the triplets, as well as to assess the difficulty of a distance comparison task, and find objects that have multiple interpretations in relation to the other objects. ",
    "code_link": "https://github.com/eamid/mvte"
  },
  "icml2015_main_distributedgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Distributed Gaussian Processes",
    "authors": [
      "Marc Deisenroth",
      "Jun Wei Ng"
    ],
    "page_url": "http://proceedings.mlr.press/v37/deisenroth15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/deisenroth15.pdf",
    "published": "2015-06",
    "summary": " To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets. ",
    "code_link": ""
  },
  "icml2015_main_guaranteedtensordecompositionamomentapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Guaranteed Tensor Decomposition: A Moment Approach",
    "authors": [
      "Gongguo Tang",
      "Parikshit Shah"
    ],
    "page_url": "http://proceedings.mlr.press/v37/tanga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/tanga15.pdf",
    "published": "2015-06",
    "summary": " We develop a theoretical and computational framework to perform guaranteed tensor decomposition, which also has the potential to accomplish other tensor tasks such as tensor completion and denoising. We formulate tensor decomposition as a problem of measure estimation from moments. By constructing a dual polynomial, we demonstrate that measure optimization returns the correct CP decomposition under an incoherence condition on the rank-one factors. To address the computational challenge, we present a hierarchy of semidefinite programs based on sums-of-squares relaxations of the measure optimization problem. By showing that the constructed dual polynomial is a sum-of-squares modulo the sphere, we prove that the smallest SDP in the relaxation hierarchy is exact and the decomposition can be extracted from the solution under the same incoherence condition. One implication is that the tensor nuclear norm can be computed exactly using the smallest SDP as long as the rank-one factors of the tensor are incoherent. Numerical experiments are conducted to test the performance of the moment approach. ",
    "code_link": ""
  },
  "icml2015_main_\\ell_1,p-normregularizationerrorboundsandconvergencerateanalysisoffirst-ordermethods": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "\\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods",
    "authors": [
      "Zirui Zhou",
      "Qi Zhang",
      "Anthony Man-Cho So"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhoub15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhoub15.pdf",
    "published": "2015-06",
    "summary": " Recently, \\ell_1,p-regularization has been widely used to induce structured sparsity in the solutions to various optimization problems. Motivated by the desire to analyze the convergence rate of first-order methods, we show that for a large class of \\ell_1,p-regularized problems, an error bound condition is satisfied when p\u2208[1,2] or p=\u221ebut fails to hold for any p\u2208(2,\u221e). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to \\ell_1,p-regularized linear or logistic regression with p\u2208[1,2] or p=\u221e. By contrast, numerical experiments suggest that for the same class of problems with p\u2208(2,\u221e), the aforementioned methods may not converge linearly. ",
    "code_link": ""
  },
  "icml2015_main_consistentestimationofdynamicandmulti-layerblockmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Consistent estimation of dynamic and multi-layer block models",
    "authors": [
      "Qiuyi Han",
      "Kevin Xu",
      "Edoardo Airoldi"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hanb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hanb15.pdf",
    "published": "2015-06",
    "summary": " Significant progress has been made recently on theoretical analysis of estimators for the stochastic block model (SBM). In this paper, we consider the multi-graph SBM, which serves as a foundation for many application settings including dynamic and multi-layer networks. We explore the asymptotic properties of two estimators for the multi-graph SBM, namely spectral clustering and the maximum-likelihood estimate (MLE), as the number of layers of the multi-graph increases. We derive sufficient conditions for consistency of both estimators and propose a variational approximation to the MLE that is computationally feasible for large networks. We verify the sufficient conditions via simulation and demonstrate that they are practical. In addition, we apply the model to two real data sets: a dynamic social network and a multi-layer social network with several types of relations. ",
    "code_link": ""
  },
  "icml2015_main_ontherateofconvergenceanderrorboundsforlstd(\u03bb)": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Rate of Convergence and Error Bounds for LSTD(\u03bb)",
    "authors": [
      "Manel Tagorti",
      "Bruno Scherrer"
    ],
    "page_url": "http://proceedings.mlr.press/v37/tagorti15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/tagorti15.pdf",
    "published": "2015-06",
    "summary": " We consider LSTD(\u03bb), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a \u03b2-mixing assumption, we derive, for any value of \u03bb\u2208(0,1), a high-probability bound on the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where \u03bb=0. In the context of temporal-difference algorithms with value function approximation, this analysis is to our knowledge the first to provide insight on the choice of the eligibility-trace parameter \u03bbwith respect to the approximation quality of the space and the number of samples. ",
    "code_link": ""
  },
  "icml2015_main_variationalinferencewithnormalizingflows": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variational Inference with Normalizing Flows",
    "authors": [
      "Danilo Rezende",
      "Shakir Mohamed"
    ],
    "page_url": "http://proceedings.mlr.press/v37/rezende15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/rezende15.pdf",
    "published": "2015-06",
    "summary": " The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference. ",
    "code_link": ""
  },
  "icml2015_main_controversyinmechanisticmodellingwithgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Controversy in mechanistic modelling with Gaussian processes",
    "authors": [
      "Benn Macdonald",
      "Catherine Higham",
      "Dirk Husmeier"
    ],
    "page_url": "http://proceedings.mlr.press/v37/macdonald15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/macdonald15.pdf",
    "published": "2015-06",
    "summary": " Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by. ",
    "code_link": ""
  },
  "icml2015_main_convexlearningofmultipletasksandtheirstructure": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Convex Learning of Multiple Tasks and their Structure",
    "authors": [
      "Carlo Ciliberto",
      "Youssef Mroueh",
      "Tomaso Poggio",
      "Lorenzo Rosasco"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ciliberto15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ciliberto15.pdf",
    "published": "2015-06",
    "summary": " Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum. ",
    "code_link": ""
  },
  "icml2015_main_k-hyperplanehinge-minimaxclassifier": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "K-hyperplane Hinge-Minimax Classifier",
    "authors": [
      "Margarita Osadchy",
      "Tamir Hazan",
      "Daniel Keren"
    ],
    "page_url": "http://proceedings.mlr.press/v37/osadchy15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/osadchy15.pdf",
    "published": "2015-06",
    "summary": " We explore a novel approach to upper bound the misclassification error for problems with data comprising a small number of positive samples and a large number of negative samples. We assign the hinge-loss to upper bound the misclassification error of the positive examples and use the minimax risk to upper bound the misclassification error with respect to the worst case distribution that generates the negative examples. This approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, in contrast to kernel SVM which produces a very large number of support vectors in such settings. We derive empirical risk bounds for linear and non-linear classification and show that they are dimensionally independent and decay as 1/\\sqrtm for m samples. We propose an efficient algorithm for training an intersection of finite number of hyperplane and demonstrate its effectiveness on real data, including letter and scene recognition. ",
    "code_link": ""
  },
  "icml2015_main_non-stationaryapproximatemodifiedpolicyiteration": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Non-Stationary Approximate Modified Policy Iteration",
    "authors": [
      "Boris Lesner",
      "Bruno Scherrer"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lesner15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lesner15.pdf",
    "published": "2015-06",
    "summary": " We consider the infinite-horizon \u03b3-discounted optimal control problem formalized by Markov Decision Processes. Running any instance of Modified Policy Iteration\u2014a family of algorithms that can interpolate between Value and Policy Iteration\u2014with an error \u03b5at each iteration is known to lead to stationary policies that are at least \\frac2\u03b3\u03b5(1-\u03b3)^2-optimal. Variations of Value and Policy Iteration, that build \\ell-periodic non-stationary policies, have recently been shown to display a better \\frac2\u03b3\u03b5(1-\u03b3)(1-\u03b3^\\ell)-optimality guarantee. Our first contribution is to describe a new algorithmic scheme, Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers m \\ge 0 and \\ell \\ge 1 that generalizes all the above mentionned algorithms. While m allows to interpolate between Value-Iteration-style and Policy-Iteration-style updates, \\ell specifies the period of the non-stationary policy that is output. We show that this new family of algorithms also enjoys the improved \\frac2\u03b3\u03b5(1-\u03b3)(1-\u03b3^\\ell)-optimality guarantee. Perhaps more importantly, we show, by exhibiting an original problem instance, that this guarantee is tight for all m and \\ell; this tightness was to our knowledge only proved two specific cases, Value Iteration (m=0,\\ell=1) and Policy Iteration (m=\u221e,\\ell=1). ",
    "code_link": ""
  },
  "icml2015_main_entropyevaluationbasedonconfidenceintervalsoffrequencyestimatesapplicationtothelearningofdecisiontrees": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees",
    "authors": [
      "Mathieu Serrurier",
      "Henri Prade"
    ],
    "page_url": "http://proceedings.mlr.press/v37/serrurier15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/serrurier15.pdf",
    "published": "2015-06",
    "summary": " Entropy gain is widely used for learning decision trees. However, as we go deeper downward the tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading choices and over-fitting may occur and the tree has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these methods still depends on the choices previously made, which may be unsatisfactory. We propose a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability distribution and the uncertainty around the estimation of its parameters. This function takes advantage of the ability of a possibility distribution to upper bound a family of probabilities previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure has several advantages over the classical one. It performs significant choices of split and provides a statistically relevant stopping criterion that allows the learning of trees whose size is well-suited w.r.t. the available data. On the top of that, it also provides a reasonable estimator of the performances of a decision tree. Finally, we show that it can be used for designing a simple and efficient online learning algorithm. ",
    "code_link": ""
  },
  "icml2015_main_geometricconditionsforsubspace-sparserecovery": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Geometric Conditions for Subspace-Sparse Recovery",
    "authors": [
      "Chong You",
      "Rene Vidal"
    ],
    "page_url": "http://proceedings.mlr.press/v37/you15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/you15.pdf",
    "published": "2015-06",
    "summary": " Given a dictionary \\Pi and a signal \u03be= \\Pi \\mathbf x generated by a few \\textitlinearly independent columns of \\Pi, classical sparse recovery theory deals with the problem of uniquely recovering the sparse representation \\mathbf x of \u03be. In this work, we consider the more general case where \u03belies in a low-dimensional subspace spanned by a few columns of \\Pi, which are possibly \\textitlinearly dependent. In this case, \\mathbf x may not unique, and the goal is to recover any subset of the columns of \\Pi that spans the subspace containing \u03be. We call such a representation \\mathbf x \\textitsubspace-sparse. We study conditions under which existing pursuit methods recover a subspace-sparse representation. Such conditions reveal important geometric insights and have implications for the theory of classical sparse recovery as well as subspace clustering. ",
    "code_link": ""
  },
  "icml2015_main_anempiricalstudyofstochasticvariationalinferencealgorithmsforthebetabernoulliprocess": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process",
    "authors": [
      "Amar Shah",
      "David Knowles",
      "Zoubin Ghahramani"
    ],
    "page_url": "http://proceedings.mlr.press/v37/shahb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/shahb15.pdf",
    "published": "2015-06",
    "summary": " Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we also show that different posterior dependencies are important in BPFA relative to LDA. ",
    "code_link": ""
  },
  "icml2015_main_longshort-termmemoryoverrecursivestructures": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Long Short-Term Memory Over Recursive Structures",
    "authors": [
      "Xiaodan Zhu",
      "Parinaz Sobihani",
      "Hongyu Guo"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhub15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhub15.pdf",
    "published": "2015-06",
    "summary": " The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures. ",
    "code_link": ""
  },
  "icml2015_main_weightuncertaintyinneuralnetwork": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Weight Uncertainty in Neural Network",
    "authors": [
      "Charles Blundell",
      "Julien Cornebise",
      "Koray Kavukcuoglu",
      "Daan Wierstra"
    ],
    "page_url": "http://proceedings.mlr.press/v37/blundell15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/blundell15.pdf",
    "published": "2015-06",
    "summary": " We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning. ",
    "code_link": ""
  },
  "icml2015_main_learningsubmodularlosseswiththelovaszhinge": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Submodular Losses with the Lovasz Hinge",
    "authors": [
      "Jiaqian Yu",
      "Matthew Blaschko"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yub15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yub15.pdf",
    "published": "2015-06",
    "summary": " Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel convex surrogate loss function for submodular losses, the Lovasz hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through a real world image labeling task. ",
    "code_link": ""
  },
  "icml2015_main_coordinatedescentconvergesfasterwiththegauss-southwellrulethanrandomselection": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection",
    "authors": [
      "Julie Nutini",
      "Mark Schmidt",
      "Issam Laradji",
      "Michael Friedlander",
      "Hoyt Koepke"
    ],
    "page_url": "http://proceedings.mlr.press/v37/nutini15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/nutini15.pdf",
    "published": "2015-06",
    "summary": " There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of\u00a0 Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that\u2014except in extreme cases\u2014it\u2019s convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. ",
    "code_link": ""
  },
  "icml2015_main_hashingfordistributeddata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hashing for Distributed Data",
    "authors": [
      "Cong Leng",
      "Jiaxiang Wu",
      "Jian Cheng",
      "Xi Zhang",
      "Hanqing Lu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/leng15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/leng15.pdf",
    "published": "2015-06",
    "summary": " Recently, hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However, due to the large scale of many applications, the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper, we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method. ",
    "code_link": ""
  },
  "icml2015_main_large-scaledistributeddependentnonparametrictrees": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Large-scale Distributed Dependent Nonparametric Trees",
    "authors": [
      "Zhiting Hu",
      "Ho Qirong",
      "Avinava Dubey",
      "Eric Xing"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hu15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hu15.pdf",
    "published": "2015-06",
    "summary": " Practical applications of Bayesian nonparametric (BNP) models have been limited, due to their high computational complexity and poor scaling on large data. In this paper, we consider dependent nonparametric trees (DNTs), a powerful infinite model that captures time-evolving hierarchies, and develop a large-scale distributed training system. Our major contributions include: (1) an effective memoized variational inference for DNTs, with a novel birth-merge strategy for exploring the unbounded tree space; (2) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment, through conflict-free model partitioning and lightweight synchronization; (3) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data. Using 64 cores in 36 hours, our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and long-tail topics. Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process, and the near-linear scalability indicates great potential for even bigger problem sizes. ",
    "code_link": ""
  },
  "icml2015_main_qualitativemulti-armedbanditsaquantile-basedapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Qualitative Multi-Armed Bandits: A Quantile-Based Approach",
    "authors": [
      "Balazs Szorenyi",
      "Robert Busa-Fekete",
      "Paul Weng",
      "Eyke H\u00fcllermeier"
    ],
    "page_url": "http://proceedings.mlr.press/v37/szorenyi15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/szorenyi15.pdf",
    "published": "2015-06",
    "summary": " We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a finite (pure exploration) and infinite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of first experimental studies. ",
    "code_link": ""
  },
  "icml2015_main_deepedge-awarefilters": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deep Edge-Aware Filters",
    "authors": [
      "Li Xu",
      "Jimmy Ren",
      "Qiong Yan",
      "Renjie Liao",
      "Jiaya Jia"
    ],
    "page_url": "http://proceedings.mlr.press/v37/xub15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/xub15.pdf",
    "published": "2015-06",
    "summary": " There are many edge-aware filters varying in their construction forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a single framework. We made the attempt to learn a big and important family of edge-aware operators from data. Our method is based on a deep convolutional neural network with a gradient domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x acceleration, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters. ",
    "code_link": ""
  },
  "icml2015_main_aconvexoptimizationframeworkforbi-clustering": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Convex Optimization Framework for Bi-Clustering",
    "authors": [
      "Shiau Hong Lim",
      "Yudong Chen",
      "Huan Xu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/limb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/limb15.pdf",
    "published": "2015-06",
    "summary": " We present a framework for biclustering and clustering where the observations are general labels. Our approach is based on the maximum likelihood estimator and its convex relaxation, and generalizes recent works in graph clustering to the biclustering setting. In addition to standard biclustering setting where one seeks to discover clustering structure simultaneously in two domain sets, we show that the same algorithm can be as effective when clustering structure only occurs in one domain. This allows for an alternative approach to clustering that is more natural in some scenarios. We present theoretical results that provide sufficient conditions for the recovery of the true underlying clusters under a generalized stochastic block model. These are further validated by our empirical results on both synthetic and real data. ",
    "code_link": ""
  },
  "icml2015_main_isfeatureselectionsecureagainsttrainingdatapoisoning?": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Is Feature Selection Secure against Training Data Poisoning?",
    "authors": [
      "Huang Xiao",
      "Battista Biggio",
      "Gavin Brown",
      "Giorgio Fumera",
      "Claudia Eckert",
      "Fabio Roli"
    ],
    "page_url": "http://proceedings.mlr.press/v37/xiao15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/xiao15.pdf",
    "published": "2015-06",
    "summary": " Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures. ",
    "code_link": ""
  },
  "icml2015_main_predictiveentropysearchforbayesianoptimizationwithunknownconstraints": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Predictive Entropy Search for Bayesian Optimization with Unknown Constraints",
    "authors": [
      "Jose Miguel Hernandez-Lobato",
      "Michael Gelbart",
      "Matthew Hoffman",
      "Ryan Adams",
      "Zoubin Ghahramani"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hernandez-lobatob15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hernandez-lobatob15.pdf",
    "published": "2015-06",
    "summary": " Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraints\u2014i.e., when one can independently evaluate the objective or the constraints\u2014EI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization. ",
    "code_link": "https://github.com/nitishsrivastava/deepnet"
  },
  "icml2015_main_atheoreticalanalysisofmetrichypothesistransferlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Theoretical Analysis of Metric Hypothesis Transfer Learning",
    "authors": [
      "Micha\u00ebl Perrot",
      "Amaury Habrard"
    ],
    "page_url": "http://proceedings.mlr.press/v37/perrot15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/perrot15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of transferring some a priori knowledge in the context of supervised metric learning approaches. While this setting has been successfully applied in some empirical contexts, no theoretical evidence exists to justify this approach. In this paper, we provide a theoretical justification based on the notion of algorithmic stability adapted to the regularized metric learning setting. We propose an on-average-replace-two-stability model allowing us to prove fast generalization rates when an auxiliary source metric is used to bias the regularizer. Moreover, we prove a consistency result from which we show the interest of considering biased weighted regularized formulations and we provide a solution to estimate the associated weight. We also present some experiments illustrating the interest of the approach in standard metric learning tasks and in a transfer learning problem where few labelled data are available. ",
    "code_link": ""
  },
  "icml2015_main_generativemomentmatchingnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Generative Moment Matching Networks",
    "authors": [
      "Yujia Li",
      "Kevin Swersky",
      "Rich Zemel"
    ],
    "page_url": "http://proceedings.mlr.press/v37/li15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/li15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database. ",
    "code_link": "https://github.com/yujiali/gmmn"
  },
  "icml2015_main_stayonpathpcaalonggraphpaths": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stay on path: PCA along graph paths",
    "authors": [
      "Megasthenis Asteris",
      "Anastasios Kyrillidis",
      "Alex Dimakis",
      "Han-Gyol Yi",
      "Bharath Chandrasekaran"
    ],
    "page_url": "http://proceedings.mlr.press/v37/asteris15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/asteris15.pdf",
    "published": "2015-06",
    "summary": " We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model for sparse PCA. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets. ",
    "code_link": ""
  },
  "icml2015_main_deeplearningwithlimitednumericalprecision": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deep Learning with Limited Numerical Precision",
    "authors": [
      "Suyog Gupta",
      "Ankur Agrawal",
      "Kailash Gopalakrishnan",
      "Pritish Narayanan"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gupta15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gupta15.pdf",
    "published": "2015-06",
    "summary": " Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network\u2019s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding ",
    "code_link": ""
  },
  "icml2015_main_safescreeningformulti-taskfeaturelearningwithmultipledatamatrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices",
    "authors": [
      "Jie Wang",
      "Jieping Ye"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wangf15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wangf15.pdf",
    "published": "2015-06",
    "summary": " Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule\u2014that is based on the dual projection onto convex sets (DPC)\u2014to quickly identify the inactive features\u2014that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features\u2014especially for high dimensional data\u2014which leads to a speedup up to several orders of magnitude. ",
    "code_link": ""
  },
  "icml2015_main_harmonicexponentialfamiliesonmanifolds": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Harmonic Exponential Families on Manifolds",
    "authors": [
      "Taco Cohen",
      "Max Welling"
    ],
    "page_url": "http://proceedings.mlr.press/v37/cohenb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/cohenb15.pdf",
    "published": "2015-06",
    "summary": " In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train. ",
    "code_link": ""
  },
  "icml2015_main_trainingdeepconvolutionalneuralnetworkstoplaygo": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Training Deep Convolutional Neural Networks to Play Go",
    "authors": [
      "Christopher Clark",
      "Amos Storkey"
    ],
    "page_url": "http://proceedings.mlr.press/v37/clark15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/clark15.pdf",
    "published": "2015-06",
    "summary": " Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more \u2019humanlike\u2019 way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to \u2019hard code\u2019 symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time. ",
    "code_link": ""
  },
  "icml2015_main_kernelinterpolationforscalablestructuredgaussianprocesses(kiss-gp)": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)",
    "authors": [
      "Andrew Wilson",
      "Hannes Nickisch"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wilson15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wilson15.pdf",
    "published": "2015-06",
    "summary": " We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling. ",
    "code_link": ""
  },
  "icml2015_main_learningdeepstructuredmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Deep Structured Models",
    "authors": [
      "Liang-Chieh Chen",
      "Alexander Schwing",
      "Alan Yuille",
      "Raquel Urtasun"
    ],
    "page_url": "http://proceedings.mlr.press/v37/chenb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/chenb15.pdf",
    "published": "2015-06",
    "summary": " Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains. ",
    "code_link": ""
  },
  "icml2015_main_communitydetectionusingtime-dependentpersonalizedpagerank": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Community Detection Using Time-Dependent Personalized PageRank",
    "authors": [
      "Haim Avron",
      "Lior Horesh"
    ],
    "page_url": "http://proceedings.mlr.press/v37/avron15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/avron15.pdf",
    "published": "2015-06",
    "summary": " Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion - the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions. ",
    "code_link": ""
  },
  "icml2015_main_scalablevariationalinferenceinlog-supermodularmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Variational Inference in Log-supermodular Models",
    "authors": [
      "Josip Djolonga",
      "Andreas Krause"
    ],
    "page_url": "http://proceedings.mlr.press/v37/djolonga15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/djolonga15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field and variants. We show that a recently proposed variational approach to inference in log-supermodular models \u2013 L-Field \u2013 reduces to the widely studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and allows solving the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-Field, demonstrating that it exactly minimizes a specific type of Renyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-Field. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals and the benefit of incorporating higher order potentials. ",
    "code_link": ""
  },
  "icml2015_main_variationalinferenceforgaussianprocessmodulatedpoissonprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variational Inference for Gaussian Process Modulated Poisson Processes",
    "authors": [
      "Chris Lloyd",
      "Tom Gunter",
      "Michael Osborne",
      "Stephen Roberts"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lloyd15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lloyd15.pdf",
    "published": "2015-06",
    "summary": " We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya. ",
    "code_link": ""
  },
  "icml2015_main_scalabledeeppoissonfactoranalysisfortopicmodeling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Deep Poisson Factor Analysis for Topic Modeling",
    "authors": [
      "Zhe Gan",
      "Changyou Chen",
      "Ricardo Henao",
      "David Carlson",
      "Lawrence Carin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gan15.pdf",
    "published": "2015-06",
    "summary": " A new framework for topic modeling is developed, based on deep graphical models, where interactions between topics are inferred through deep latent binary hierarchies. The proposed multi-layer model employs a deep sigmoid belief network or restricted Boltzmann machine, the bottom binary layer of which selects topics for use in a Poisson factor analysis model. Under this setting, topics live on the bottom layer of the model, while the deep specification serves as a flexible prior for revealing topic structure. Scalable inference algorithms are derived by applying Bayesian conditional density filtering algorithm, in addition to extending recently proposed work on stochastic gradient thermostats. Experimental results on several corpora show that the proposed approach readily handles very large collections of text documents, infers structured topic representations, and obtains superior test perplexities when compared with related models. ",
    "code_link": ""
  },
  "icml2015_main_hiddenmarkovanomalydetection": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hidden Markov Anomaly Detection",
    "authors": [
      "Nico Goernitz",
      "Mikio Braun",
      "Marius Kloft"
    ],
    "page_url": "http://proceedings.mlr.press/v37/goernitz15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/goernitz15.pdf",
    "published": "2015-06",
    "summary": " We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter v can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM. ",
    "code_link": ""
  },
  "icml2015_main_robustestimationoftransitionmatricesinhighdimensionalheavy-tailedvectorautoregressiveprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes",
    "authors": [
      "Huitong Qiu",
      "Sheng Xu",
      "Fang Han",
      "Han Liu",
      "Brian Caffo"
    ],
    "page_url": "http://proceedings.mlr.press/v37/qiu15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/qiu15.pdf",
    "published": "2015-06",
    "summary": " Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction. ",
    "code_link": ""
  },
  "icml2015_main_convexcalibratedsurrogatesforhierarchicalclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Convex Calibrated Surrogates for Hierarchical Classification",
    "authors": [
      "Harish Ramaswamy",
      "Ambuj Tewari",
      "Shivani Agarwal"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ramaswamy15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ramaswamy15.pdf",
    "published": "2015-06",
    "summary": " Hierarchical classification problems are multiclass supervised learning problems with a pre-defined hierarchy over the set of class labels. In this work, we study the consistency of hierarchical classification algorithms with respect to a natural loss, namely the tree distance metric on the hierarchy tree of class labels, via the usage of calibrated surrogates. We first show that the Bayes optimal classifier for this loss classifies an instance according to the deepest node in the hierarchy such that the total conditional probability of the subtree rooted at the node is greater than \\frac12. We exploit this insight to develop new consistent algorithm for hierarchical classification, that makes use of an algorithm known to be consistent for the \u201cmulticlass classification with reject option (MCRO)\u201d problem as a sub-routine. Our experiments on a number of benchmark datasets show that the resulting algorithm, which we term OvA-Cascade, gives improved performance over other state-of-the-art hierarchical classification algorithms. ",
    "code_link": ""
  },
  "icml2015_main_probabilisticbackpropagationforscalablelearningofbayesianneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks",
    "authors": [
      "Jose Miguel Hernandez-Lobato",
      "Ryan Adams"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hernandez-lobatoc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf",
    "published": "2015-06",
    "summary": " Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights. ",
    "code_link": ""
  },
  "icml2015_main_activenearestneighborsinchangingenvironments": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Active Nearest Neighbors in Changing Environments",
    "authors": [
      "Christopher Berlind",
      "Ruth Urner"
    ],
    "page_url": "http://proceedings.mlr.press/v37/berlind15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/berlind15.pdf",
    "published": "2015-06",
    "summary": " While classic machine learning paradigms assume training and test data are generated from the same process, domain adaptation addresses the more realistic setting in which the learner has large quantities of labeled data from some source task but limited or no labeled data from the target task it is attempting to learn. In this work, we give the first formal analysis showing that using active learning for domain adaptation yields a way to address the statistical challenges inherent in this setting. We propose a novel nonparametric algorithm, ANDA, that combines an active nearest neighbor querying strategy with nearest neighbor prediction. We provide analyses of its querying behavior and of finite sample convergence rates of the resulting classifier under covariate shift. Our experiments show that ANDA successfully corrects for dataset bias in multi-class image categorization. ",
    "code_link": ""
  },
  "icml2015_main_bipartiteedgepredictionviatransductivelearningoverproductgraphs": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bipartite Edge Prediction via Transductive Learning over Product Graphs",
    "authors": [
      "Hanxiao Liu",
      "Yiming Yang"
    ],
    "page_url": "http://proceedings.mlr.press/v37/liuc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/liuc15.pdf",
    "published": "2015-06",
    "summary": " This paper addresses the problem of predicting the missing edges of a bipartite graph where each side of the vertices has its own intrinsic structure. We propose a new optimization framework to map the two sides of the intrinsic structures onto the manifold structure of the edges via a graph product, and to reduce the original problem to vertex label propagation over the product graph. This framework enjoys flexible choices in the formulation of graph products, and supports a rich family of graph transduction schemes with scalable inference. Experiments on benchmark datasets for collaborative filtering, citation network analysis and prerequisite prediction of online courses show advantageous performance of the proposed approach over other state-of-the-art methods. ",
    "code_link": ""
  },
  "icml2015_main_trustregionpolicyoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Trust Region Policy Optimization",
    "authors": [
      "John Schulman",
      "Sergey Levine",
      "Pieter Abbeel",
      "Michael Jordan",
      "Philipp Moritz"
    ],
    "page_url": "http://proceedings.mlr.press/v37/schulman15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/schulman15.pdf",
    "published": "2015-06",
    "summary": " In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters. ",
    "code_link": ""
  },
  "icml2015_main_discoveringtemporalcausalrelationsfromsubsampleddata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Discovering Temporal Causal Relations from Subsampled Data",
    "authors": [
      "Mingming Gong",
      "Kun Zhang",
      "Bernhard Schoelkopf",
      "Dacheng Tao",
      "Philipp Geiger"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gongb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gongb15.pdf",
    "published": "2015-06",
    "summary": " Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the high frequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches. ",
    "code_link": ""
  },
  "icml2015_main_preferencecompletionlarge-scalecollaborativerankingfrompairwisecomparisons": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons",
    "authors": [
      "Dohyung Park",
      "Joe Neeman",
      "Jin Zhang",
      "Sujay Sanghavi",
      "Inderjit Dhillon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/park15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/park15.pdf",
    "published": "2015-06",
    "summary": " In this paper we consider the collaborative ranking setting: a pool of users each provides a set of pairwise preferences over a small subset of the set of d possible items; from these we need to predict each user\u2019s preferences for items s/he has not yet seen. We do so via fitting a rank r score matrix to the pairwise data, and provide two main contributions: (a) We show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as O(r \\log^2 d) pairwise comparisons \u2014 essentially matching the sample complexity required in the related matrix completion setting (which uses actual numerical as opposed to pairwise information), and also matching a lower bound we establish here. (b) We develop a large-scale non-convex implementation, which we call AltSVM, which trains a factored form of the matrix via alternating minimization (which we show reduces to alternating SVM problems), and scales and parallelizes very well to large problem settings. It also outperforms common baselines on many moderately large popular collaborative filtering datasets in both NDCG and other measures of ranking performance. ",
    "code_link": ""
  },
  "icml2015_main_causalinferencebyidentificationofvectorautoregressiveprocesseswithhiddencomponents": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components",
    "authors": [
      "Philipp Geiger",
      "Kun Zhang",
      "Bernhard Schoelkopf",
      "Mingming Gong",
      "Dominik Janzing"
    ],
    "page_url": "http://proceedings.mlr.press/v37/geiger15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/geiger15.pdf",
    "published": "2015-06",
    "summary": " A widely applied approach to causal inference from a time series X, often referred to as \u201c(linear) Granger causal analysis\u201d, is to simply regress present on past and interpret the regression matrix \\hatB causally. However, if there is an unmeasured time series Z that influences X, then this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if one had additional information such as Z. In this paper we take a different approach: We assume that X together with some hidden Z forms a first order vector autoregressive (VAR) process with transition matrix A, and argue why it is more valid to interpret A causally instead of \\hatB. Then we examine under which conditions the most important parts of A are identifiable or almost identifiable from only X. Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from X to Z. We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using X. ",
    "code_link": ""
  },
  "icml2015_main_onsymmetricandasymmetriclshsforinnerproductsearch": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On Symmetric and Asymmetric LSHs for Inner Product Search",
    "authors": [
      "Behnam Neyshabur",
      "Nathan Srebro"
    ],
    "page_url": "http://proceedings.mlr.press/v37/neyshabur15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/neyshabur15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li (2014a) argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required. ",
    "code_link": ""
  },
  "icml2015_main_thekendallandmallowskernelsforpermutations": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Kendall and Mallows Kernels for Permutations",
    "authors": [
      "Yunlong Jiao",
      "Jean-Philippe Vert"
    ],
    "page_url": "http://proceedings.mlr.press/v37/jiao15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/jiao15.pdf",
    "published": "2015-06",
    "summary": " We show that the widely used Kendall tau correlation coefficient is a positive definite kernel for permutations. It offers a computationally attractive alternative to more complex kernels on the symmetric group to learn from rankings, or to learn to rank. We show how to extend it to partial rankings or rankings with uncertainty, and demonstrate promising results on high-dimensional classification problems in biomedical applications. ",
    "code_link": ""
  },
  "icml2015_main_bayesianmultipletargetlocalization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bayesian Multiple Target Localization",
    "authors": [
      "Purnima Rajan",
      "Weidong Han",
      "Raphael Sznitman",
      "Peter Frazier",
      "Bruno Jedynak"
    ],
    "page_url": "http://proceedings.mlr.press/v37/rajan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/rajan15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of quickly localizing multiple targets by asking questions of the form \u201cHow many targets are within this set\" while obtaining noisy answers. This setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried. We assume that the targets are points on the real line, or in a two dimensional plane for the experiments, drawn independently from a known distribution. We evaluate the performance of a policy using the expected entropy of the posterior distribution after a fixed number of questions with noisy answers. We derive a lower bound for the value of this problem and study a specific policy, named the dyadic policy. We show that this policy achieves a value which is no more than twice this lower bound when answers are noise-free, and show a more general constant factor approximation guarantee for the noisy setting. We present an empirical evaluation of this policy on simulated data for the problem of detecting multiple instances of the same object in an image. Finally, we present experiments on localizing multiple faces simultaneously on real images. ",
    "code_link": ""
  },
  "icml2015_main_submodularityindatasubsetselectionandactivelearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Submodularity in Data Subset Selection and Active Learning",
    "authors": [
      "Kai Wei",
      "Rishabh Iyer",
      "Jeff Bilmes"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wei15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wei15.pdf",
    "published": "2015-06",
    "summary": " We study the problem of selecting a subset of big data to train a classifier while incurring minimal performance loss. We show the connection of submodularity to the data likelihood functions for Naive Bayes (NB) and Nearest Neighbor (NN) classifiers, and formulate the data subset selection problems for these classifiers as constrained submodular maximization. Furthermore, we apply this framework to active learning and propose a novel scheme filtering active submodular selection (FASS), where we combine the uncertainty sampling method with a submodular data subset selection framework. We extensively evaluate the proposed framework on text categorization and handwritten digit recognition tasks with four different classifiers, including Deep Neural Network (DNN) based classifiers. Empirical results indicate that the proposed framework yields significant improvement over the state-of-the-art algorithms on all classifiers. ",
    "code_link": ""
  },
  "icml2015_main_variationalgenerativestochasticnetworkswithcollaborativeshaping": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variational Generative Stochastic Networks with Collaborative Shaping",
    "authors": [
      "Philip Bachman",
      "Doina Precup"
    ],
    "page_url": "http://proceedings.mlr.press/v37/bachman15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/bachman15.pdf",
    "published": "2015-06",
    "summary": " We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chain\u2019s trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view. ",
    "code_link": "https://github.com/Philip-Bachman/ICML-2015"
  },
  "icml2015_main_addingvs.averagingindistributedprimal-dualoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization",
    "authors": [
      "Chenxin Ma",
      "Virginia Smith",
      "Martin Jaggi",
      "Michael Jordan",
      "Peter Richtarik",
      "Martin Takac"
    ],
    "page_url": "http://proceedings.mlr.press/v37/mab15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/mab15.pdf",
    "published": "2015-06",
    "summary": " Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines. ",
    "code_link": "https://github.com/gingsmith/cocoa"
  },
  "icml2015_main_feature-budgetedrandomforest": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Feature-Budgeted Random Forest",
    "authors": [
      "Feng Nan",
      "Joseph Wang",
      "Venkatesh Saligrama"
    ],
    "page_url": "http://proceedings.mlr.press/v37/nan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/nan15.pdf",
    "published": "2015-06",
    "summary": " We seek decision rules for \\it prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified \\it average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms. ",
    "code_link": ""
  },
  "icml2015_main_entropicgraph-basedposteriorregularization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Entropic Graph-based Posterior Regularization",
    "authors": [
      "Maxwell Libbrecht",
      "Michael Hoffman",
      "Jeff Bilmes",
      "William Noble"
    ],
    "page_url": "http://proceedings.mlr.press/v37/libbrecht15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/libbrecht15.pdf",
    "published": "2015-06",
    "summary": " Graph smoothness objectives have achieved great success in semi-supervised learning but have not yet been applied extensively to unsupervised generative models. We define a new class of entropic graph-based posterior regularizers that augment a probabilistic model by encouraging pairs of nearby variables in a regularization graph to have similar posterior distributions. We present a three-way alternating optimization algorithm with closed-form updates for performing inference on this joint model and learning its parameters. This method admits updates linear in the degree of the regularization graph, exhibits monotone convergence and is easily parallelizable. We are motivated by applications in computational biology in which temporal models such as hidden Markov models are used to learn a human-interpretable representation of genomic data. On a synthetic problem, we show that our method outperforms existing methods for graph-based regularization and a comparable strategy for incorporating long-range interactions using existing methods for approximate inference. Using genome-scale functional genomics data, we integrate genome 3D interaction data into existing models for genome annotation and demonstrate significant improvements in predicting genomic activity. ",
    "code_link": ""
  },
  "icml2015_main_unsupervisedriemannianmetriclearningforhistogramsusingaitchisontransformations": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations",
    "authors": [
      "Tam Le",
      "Marco Cuturi"
    ],
    "page_url": "http://proceedings.mlr.press/v37/le15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/le15.pdf",
    "published": "2015-06",
    "summary": " Many applications in machine learning handle bags of features or histograms rather than simple vectors. In that context, defining a proper geometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon(2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multinomial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison(1982) transformations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimental evidence that the metric obtained under our proposal outperforms alternative approaches. ",
    "code_link": "https://github.com/probml/pmtk3"
  },
  "icml2015_main_low-rankmatrixrecoveryfromrow-and-columnaffinemeasurements": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements",
    "authors": [
      "Or Zuk",
      "Avishai Wagner"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zuk15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zuk15.pdf",
    "published": "2015-06",
    "summary": " We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X. This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SVD) and least-squares (LS), which we term alg. We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction. ",
    "code_link": "https://github.com/avishaiwa/SVLS"
  },
  "icml2015_main_algorithmsforthehardpre-imageproblemofstringkernelsandthegeneralproblemofstringprediction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Algorithms for the Hard Pre-Image Problem of String Kernels and the General Problem of String Prediction",
    "authors": [
      "S\u00e9bastien Gigu\u00e8re",
      "Am\u00e9lie Rolland",
      "Francois Laviolette",
      "Mario Marchand"
    ],
    "page_url": "http://proceedings.mlr.press/v37/giguere15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/giguere15.pdf",
    "published": "2015-06",
    "summary": " We address the pre-image problem encountered in structured output prediction and the one of finding a string maximizing the prediction function of various kernel-based classifiers and regressors. We demonstrate that these problems reduce to a common combinatorial problem valid for many string kernels. For this problem, we propose an upper bound on the prediction function which has low computational complexity and which can be used in a branch and bound search algorithm to obtain optimal solutions. We also show that for many string kernels, the complexity of the problem increases significantly when the kernel is normalized. On the optical word recognition task, the exact solution of the pre-image problem is shown to significantly improve the prediction accuracy in comparison with an approximation found by the best known heuristic. On the task of finding a string maximizing the prediction function of kernel-based classifiers and regressors, we highlight that existing methods can be biased toward long strings that contain many repeated symbols. We demonstrate that this bias is removed when using normalized kernels. Finally, we present results for the discovery of lead compounds in drug discovery. The source code can be found at https://github.com/a-ro/preimage ",
    "code_link": ""
  },
  "icml2015_main_amultitaskpointprocesspredictivemodel": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Multitask Point Process Predictive Model",
    "authors": [
      "Wenzhao Lian",
      "Ricardo Henao",
      "Vinayak Rao",
      "Joseph Lucas",
      "Lawrence Carin"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lian15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lian15.pdf",
    "published": "2015-06",
    "summary": " Point process data are commonly observed in fields like healthcare and social science. Designing predictive models for such event streams is an under-explored problem, due to often scarce training data. In this work we propose a multitask point process model, leveraging information from all tasks via a hierarchical Gaussian process (GP). Nonparametric learning functions implemented by a GP, which map from past events to future rates, allow analysis of flexible arrival patterns. To facilitate efficient inference, we propose a sparse construction for this hierarchical model, and derive a variational Bayes method for learning and inference. Experimental results are shown on both synthetic data and an application on real electronic health records. ",
    "code_link": ""
  },
  "icml2015_main_ahybridapproachforprobabilisticinferenceusingrandomprojections": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Hybrid Approach for Probabilistic Inference using Random Projections",
    "authors": [
      "Michael Zhu",
      "Stefano Ermon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/zhuc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/zhuc15.pdf",
    "published": "2015-06",
    "summary": " We introduce a new meta-algorithm for probabilistic inference in graphical models based on random projections. The key idea is to use approximate inference algorithms for an (exponentially) large number of samples, obtained by randomly projecting the original statistical model using universal hash functions. In the case where the approximate inference algorithm is a variational approximation, this approach can be viewed as interpolating between sampling-based and variational techniques. The number of samples used controls the trade-off between the accuracy of the approximate inference algorithm and the variance of the estimator. We show empirically that by using random projections, we can improve the accuracy of common approximate inference algorithms. ",
    "code_link": ""
  },
  "icml2015_main_show,attendandtellneuralimagecaptiongenerationwithvisualattention": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
    "authors": [
      "Kelvin Xu",
      "Jimmy Ba",
      "Ryan Kiros",
      "Kyunghyun Cho",
      "Aaron Courville",
      "Ruslan Salakhudinov",
      "Rich Zemel",
      "Yoshua Bengio"
    ],
    "page_url": "http://proceedings.mlr.press/v37/xuc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/xuc15.pdf",
    "published": "2015-06",
    "summary": " Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO. ",
    "code_link": ""
  },
  "icml2015_main_learningtosearchbetterthanyourteacher": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning to Search Better than Your Teacher",
    "authors": [
      "Kai-Wei Chang",
      "Akshay Krishnamurthy",
      "Alekh Agarwal",
      "Hal Daume",
      "John Langford"
    ],
    "page_url": "http://proceedings.mlr.press/v37/changb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/changb15.pdf",
    "published": "2015-06",
    "summary": " Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications. ",
    "code_link": ""
  },
  "icml2015_main_gatedfeedbackrecurrentneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Gated Feedback Recurrent Neural Networks",
    "authors": [
      "Junyoung Chung",
      "Caglar Gulcehre",
      "Kyunghyun Cho",
      "Yoshua Bengio"
    ],
    "page_url": "http://proceedings.mlr.press/v37/chung15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/chung15.pdf",
    "published": "2015-06",
    "summary": " In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions. ",
    "code_link": ""
  },
  "icml2015_main_context-basedunsuperviseddatafusionfordecisionmaking": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Context-based Unsupervised Data Fusion for Decision Making",
    "authors": [
      "Erfan Soltanmohammadi",
      "Mort Naraghi-Pour",
      "Mihaela Schaar"
    ],
    "page_url": "http://proceedings.mlr.press/v37/soltanmohammadi15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/soltanmohammadi15.pdf",
    "published": "2015-06",
    "summary": " Big Data received from sources such as social media, in-stream monitoring systems, networks, and markets is often mined for discovering patterns, detecting anomalies, and making decisions or predictions. In distributed learning and real-time processing of Big Data, ensemble-based systems in which a fusion center (FC) is used to combine the local decisions of several classifiers, have shown to be superior to single expert systems. However, optimal design of the FC requires knowledge of the accuracy of the individual classifiers which, in many cases, is not available. Moreover, in many applications supervised training of the FC is not feasible since the true labels of the data set are not available. In this paper, we propose an unsupervised joint estimation-detection scheme to estimate the accuracies of the local classifiers as functions of data context and to fuse the local decisions of the classifiers. Numerical results show the dramatic improvement of the proposed method as compared with the state of the art approaches. ",
    "code_link": ""
  },
  "icml2015_main_phrase-basedimagecaptioning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Phrase-based Image Captioning",
    "authors": [
      "Remi Lebret",
      "Pedro Pinheiro",
      "Ronan Collobert"
    ],
    "page_url": "http://proceedings.mlr.press/v37/lebret15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/lebret15.pdf",
    "published": "2015-06",
    "summary": " Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely linear model to embed an image representation (generated from a previously trained Convolutional Neural Network) into a multimodal space that is common to the images and the phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on the sentence description statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO. ",
    "code_link": ""
  },
  "icml2015_main_celestevariationalinferenceforagenerativemodelofastronomicalimages": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Celeste: Variational inference for a generative model of astronomical images",
    "authors": [
      "Jeffrey Regier",
      "Andrew Miller",
      "Jon McAuliffe",
      "Ryan Adams",
      "Matt Hoffman",
      "Dustin Lang",
      "David Schlegel",
      "Mr Prabhat"
    ],
    "page_url": "http://proceedings.mlr.press/v37/regier15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/regier15.pdf",
    "published": "2015-06",
    "summary": " We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors. ",
    "code_link": ""
  },
  "icml2015_main_distributionalrankaggregation,andanaxiomaticanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Distributional Rank Aggregation, and an Axiomatic Analysis",
    "authors": [
      "Adarsh Prasad",
      "Harsh Pareek",
      "Pradeep Ravikumar"
    ],
    "page_url": "http://proceedings.mlr.press/v37/prasad15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/prasad15.pdf",
    "published": "2015-06",
    "summary": " The rank aggregation problem has been studied with varying desiderata in varied communities such as Theoretical Computer Science, Statistics, Information Retrieval and Social Welfare Theory. We introduce a variant of this problem we call distributional rank aggregation, where the ranking data is only available via the induced distribution over the set of all permutations. We provide a novel translation of the usual social welfare theory axioms to this setting. As we show this allows for a more quantitative characterization of these axioms: which then are not only less prone to misinterpretation, but also allow simpler proofs for some key impossibility theorems. Most importantly, these quantitative characterizations lead to natural and novel relaxations of these axioms, which as we show, allow us to get around celebrated impossibility results in social choice theory. We are able to completely characterize the class of positional scoring rules with respect to our axioms and show that Borda Count is optimal in a certain sense. ",
    "code_link": ""
  },
  "icml2015_main_gradient-basedhyperparameteroptimizationthroughreversiblelearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Gradient-based Hyperparameter Optimization through Reversible Learning",
    "authors": [
      "Dougal Maclaurin",
      "David Duvenaud",
      "Ryan Adams"
    ],
    "page_url": "http://proceedings.mlr.press/v37/maclaurin15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/maclaurin15.pdf",
    "published": "2015-06",
    "summary": " Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum. ",
    "code_link": ""
  },
  "icml2015_main_bimodalmodellingofsourcecodeandnaturallanguage": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bimodal Modelling of Source Code and Natural Language",
    "authors": [
      "Miltos Allamanis",
      "Daniel Tarlow",
      "Andrew Gordon",
      "Yi Wei"
    ],
    "page_url": "http://proceedings.mlr.press/v37/allamanis15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/allamanis15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of building probabilistic models that jointly model short natural language utterances and source code snippets. The aim is to bring together recent work on statistical modelling of source code and work on bimodal models of images and natural language. The resulting models are useful for a variety of tasks that involve natural language and source code. We demonstrate their performance on two retrieval tasks: retrieving source code snippets given a natural language query, and retrieving natural language descriptions given a source code query (i.e., source code captioning). The experiments show there to be promise in this direction, and that modelling the structure of source code is helpful towards the retrieval tasks. ",
    "code_link": ""
  },
  "icml2015_main_cheapbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Cheap Bandits",
    "authors": [
      "Manjesh Hanawal",
      "Venkatesh Saligrama",
      "Michal Valko",
      "Remi Munos"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hanawal15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hanawal15.pdf",
    "published": "2015-06",
    "summary": " We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a \u03a9(\\sqrt(dT)) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d. ",
    "code_link": ""
  },
  "icml2015_main_subsamplingmethodsforpersistenthomology": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Subsampling Methods for Persistent Homology",
    "authors": [
      "Frederic Chazal",
      "Brittany Fasy",
      "Fabrizio Lecci",
      "Bertrand Michel",
      "Alessandro Rinaldo",
      "Larry Wasserman"
    ],
    "page_url": "http://proceedings.mlr.press/v37/chazal15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/chazal15.pdf",
    "published": "2015-06",
    "summary": " Persistent homology is a multiscale method for analyzing the shape of sets and functions from point cloud data arising from an unknown distribution supported on those sets. When the size of the sample is large, direct computation of the persistent homology is prohibitive due to the combinatorial nature of the existing algorithms. We propose to compute the persistent homology of several subsamples of the data and then combine the resulting estimates. We study the risk of two estimators and we prove that the subsampling approach carries stable topological information while achieving a great reduction in computational complexity. ",
    "code_link": ""
  },
  "icml2015_main_anembarrassinglysimpleapproachtozero-shotlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An embarrassingly simple approach to zero-shot learning",
    "authors": [
      "Bernardino Romera-Paredes",
      "Philip Torr"
    ],
    "page_url": "http://proceedings.mlr.press/v37/romera-paredes15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/romera-paredes15.pdf",
    "published": "2015-06",
    "summary": " Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalization error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17%. ",
    "code_link": ""
  },
  "icml2015_main_binaryembeddingfundamentallimitsandfastalgorithm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Binary Embedding: Fundamental Limits and Fast Algorithm",
    "authors": [
      "Xinyang Yi",
      "Constantine Caramanis",
      "Eric Price"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yi15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yi15.pdf",
    "published": "2015-06",
    "summary": " Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in \\mathbbS^p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to \u03b4uniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m =\u03a9(\\frac1\u03b4^2\\logN) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity m = O(\\frac1 \u03b4^2\\logN) and near linear running time O(p \\log p) whenever \\log N \u226a\u03b4\\sqrtp, with a slightly worse running time for larger \\log N; (3) we also provide an analytic result about embedding a general set of points K \u2286\\mathbbS^p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets. ",
    "code_link": ""
  },
  "icml2015_main_scalablebayesianoptimizationusingdeepneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Bayesian Optimization Using Deep Neural Networks",
    "authors": [
      "Jasper Snoek",
      "Oren Rippel",
      "Kevin Swersky",
      "Ryan Kiros",
      "Nadathur Satish",
      "Narayanan Sundaram",
      "Mostofa Patwary",
      "Mr Prabhat",
      "Ryan Adams"
    ],
    "page_url": "http://proceedings.mlr.press/v37/snoek15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/snoek15.pdf",
    "published": "2015-06",
    "summary": " Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models. ",
    "code_link": ""
  },
  "icml2015_main_howhardisinferenceforstructuredprediction?": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "How Hard is Inference for Structured Prediction?",
    "authors": [
      "Amir Globerson",
      "Tim Roughgarden",
      "David Sontag",
      "Cafer Yildirim"
    ],
    "page_url": "http://proceedings.mlr.press/v37/globerson15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/globerson15.pdf",
    "published": "2015-06",
    "summary": " Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. The goal of this paper is to develop a theoretical explanation of the empirical effectiveness of heuristic inference algorithms for solving such structured prediction problems. We study the minimum-achievable expected Hamming error in such problems, highlighting the case of 2D grid graphs, which are common in machine vision applications. Our main theorems provide tight upper and lower bounds on this error, as well as a polynomial-time algorithm that achieves the bound. ",
    "code_link": ""
  },
  "icml2015_main_onlinetimeseriespredictionwithmissingdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Online Time Series Prediction with Missing Data",
    "authors": [
      "Oren Anava",
      "Elad Hazan",
      "Assaf Zeevi"
    ],
    "page_url": "http://proceedings.mlr.press/v37/anava15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/anava15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of time series prediction in the presence of missing data. We cast the problem as an online learning problem in which the goal of the learner is to minimize prediction error. We then devise an efficient algorithm for the problem, which is based on autoregressive model, and does not assume any structure on the missing data nor on the mechanism that generates the time series. We show that our algorithm\u2019s performance asymptotically approaches the performance of the best AR predictor in hindsight, and corroborate the theoretic results with an empirical study on synthetic and real-world data. ",
    "code_link": ""
  },
  "icml2015_main_proteins,particles,andpseudo-max-marginalsasubmodularapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Proteins, Particles, and Pseudo-Max-Marginals: A Submodular Approach",
    "authors": [
      "Jason Pacheco",
      "Erik Sudderth"
    ],
    "page_url": "http://proceedings.mlr.press/v37/pacheco15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/pacheco15.pdf",
    "published": "2015-06",
    "summary": " Variants of max-product (MP) belief propagation effectively find modes of many complex graphical models, but are limited to discrete distributions. Diverse particle max-product (D-PMP) robustly approximates max-product updates in continuous MRFs using stochastically sampled particles, but previous work was specialized to tree-structured models. Motivated by the challenging problem of protein side chain prediction, we extend D-PMP in several key ways to create a generic MAP inference algorithm for loopy models. We define a modified diverse particle selection objective that is provably submodular, leading to an efficient greedy algorithm with rigorous optimality guarantees, and corresponding max-marginal error bounds. We further incorporate tree-reweighted variants of the MP algorithm to allow provable verification of global MAP recovery in many models. Our general-purpose Matlab library is applicable to a wide range of pairwise graphical models, and we validate our approach using optical flow benchmarks. We further demonstrate superior side chain prediction accuracy compared to baseline algorithms from the state-of-the-art Rosetta package. ",
    "code_link": ""
  },
  "icml2015_main_afastvariationalapproachforlearningmarkovrandomfieldlanguagemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Fast Variational Approach for Learning Markov Random Field Language Models",
    "authors": [
      "Yacine Jernite",
      "Alexander Rush",
      "David Sontag"
    ],
    "page_url": "http://proceedings.mlr.press/v37/jernite15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/jernite15.pdf",
    "published": "2015-06",
    "summary": " Language modelling is a fundamental building block of natural language processing. However, in practice the size of the vocabulary limits the distributions applicable for this task: specifically, one has to either resort to local optimization methods, such as those used in neural language models, or work with heavily constrained distributions. In this work, we take a step towards overcoming these difficulties. We present a method for global-likelihood optimization of a Markov random field language model exploiting long-range contexts in time independent of the corpus size. We take a variational approach to optimizing the likelihood and exploit underlying symmetries to greatly simplify learning. We demonstrate the efficiency of this method both for language modelling and for part-of-speech tagging. ",
    "code_link": ""
  },
  "icml2015_main_removingsystematicerrorsforexoplanetsearchvialatentcauses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Removing systematic errors for exoplanet search via latent causes",
    "authors": [
      "Bernhard Sch\u00f6lkopf",
      "David Hogg",
      "Dun Wang",
      "Dan Foreman-Mackey",
      "Dominik Janzing",
      "Carl-Johann Simon-Gabriel",
      "Jonas Peters"
    ],
    "page_url": "http://proceedings.mlr.press/v37/scholkopf15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/scholkopf15.pdf",
    "published": "2015-06",
    "summary": " We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as \\em half-sibling regression, is inspired by recent work in causal inference using additive noise models. We provide a theoretical justification and illustrate the potential of the method in a challenging astronomy application. ",
    "code_link": ""
  },
  "icml2015_main_scalablenonparametricbayesianinferenceonpointprocesseswithgaussianprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes",
    "authors": [
      "Yves-Laurent Kom Samo",
      "Stephen Roberts"
    ],
    "page_url": "http://proceedings.mlr.press/v37/samo15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/samo15.pdf",
    "published": "2015-06",
    "summary": " In this paper we propose an efficient, scalable non-parametric Gaussian process model for inference on Poisson point processes. Our model does not resort to gridding the domain or to introducing latent thinning points. Unlike competing models that scale as O(n^3) over n data points, our model has a complexity O(nk^2) where k << n. We propose a MCMC sampler and show that the model obtained is faster, more accurate and generates less correlated samples than competing approaches on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches. ",
    "code_link": ""
  },
  "icml2015_main_correlationclusteringindatastreams": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Correlation Clustering in Data Streams",
    "authors": [
      "KookJin Ahn",
      "Graham Cormode",
      "Sudipto Guha",
      "Andrew McGregor",
      "Anthony Wirth"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ahn15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ahn15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we address the problem of \\emphcorrelation clustering in the dynamic data stream model. The stream consists of updates to the edge weights of a graph on\u00a0n nodes and the goal is to find a node-partition such that the end-points of negative-weight edges are typically in different clusters whereas the end-points of positive-weight edges are typically in the same cluster. We present polynomial-time, O(n\u22c5\\textpolylog n)-space approximation algorithms for natural problems that arise. We first develop data structures based on linear sketches that allow the \u201cquality\u201d of a given node-partition to be measured. We then combine these data structures with convex programming and sampling techniques to solve the relevant approximation problem. However the standard LP and SDP formulations are not obviously solvable in O(n\u22c5\\textpolylog n)-space. Our work presents space-efficient algorithms for the convex programming required, as well as approaches to reduce the adaptivity of the sampling. Note that the improved space and running-time bounds achieved from streaming algorithms are also useful for offline settings such as MapReduce models. ",
    "code_link": ""
  },
  "icml2015_main_learningscale-freenetworksbydynamicnodespecificdegreeprior": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Scale-Free Networks by Dynamic Node Specific Degree Prior",
    "authors": [
      "Qingming Tang",
      "Siqi Sun",
      "Jinbo Xu"
    ],
    "page_url": "http://proceedings.mlr.press/v37/tangb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/tangb15.pdf",
    "published": "2015-06",
    "summary": " Learning network structure underlying data is an important problem in machine learning. This paper presents a novel degree prior to study the inference of scale-free networks, which are widely used to model social and biological networks. In particular, this paper formulates scale-free network inference using Gaussian Graphical model (GGM) regularized by a node degree prior. Our degree prior not only promotes a desirable global degree distribution, but also exploits the estimated degree of an individual node and the relative strength of all the edges of a single node. To fulfill this, this paper proposes a ranking-based method to dynamically estimate the degree of a node, which makes the resultant optimization problem challenging to solve. To deal with this, this paper presents a novel ADMM (alternating direction method of multipliers) procedure. Our experimental results on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than existing scale-free inducing prior, hub-inducing prior and the l_1 norm. ",
    "code_link": ""
  },
  "icml2015_main_deepunsupervisedlearningusingnonequilibriumthermodynamics": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": [
      "Jascha Sohl-Dickstein",
      "Eric Weiss",
      "Niru Maheswaranathan",
      "Surya Ganguli"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sohl-dickstein15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sohl-dickstein15.pdf",
    "published": "2015-06",
    "summary": " A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm. ",
    "code_link": ""
  },
  "icml2015_main_modelingorderinneuralwordembeddingsatscale": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Modeling Order in Neural Word Embeddings at Scale",
    "authors": [
      "Andrew Trask",
      "David Gilmore",
      "Matthew Russell"
    ],
    "page_url": "http://proceedings.mlr.press/v37/trask15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/trask15.pdf",
    "published": "2015-06",
    "summary": " Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network. ",
    "code_link": ""
  },
  "icml2015_main_distributedinferencefordirichletprocessmixturemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Distributed Inference for Dirichlet Process Mixture Models",
    "authors": [
      "Hong Ge",
      "Yutian Chen",
      "Moquan Wan",
      "Zoubin Ghahramani"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gea15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gea15.pdf",
    "published": "2015-06",
    "summary": " Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores. ",
    "code_link": ""
  },
  "icml2015_main_compressingneuralnetworkswiththehashingtrick": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Compressing Neural Networks with the Hashing Trick",
    "authors": [
      "Wenlin Chen",
      "James Wilson",
      "Stephen Tyree",
      "Kilian Weinberger",
      "Yixin Chen"
    ],
    "page_url": "http://proceedings.mlr.press/v37/chenc15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/chenc15.pdf",
    "published": "2015-06",
    "summary": " As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance. ",
    "code_link": ""
  },
  "icml2015_main_intersectingfacesnon-negativematrixfactorizationwithnewguarantees": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Intersecting Faces: Non-negative Matrix Factorization With New Guarantees",
    "authors": [
      "Rong Ge",
      "James Zou"
    ],
    "page_url": "http://proceedings.mlr.press/v37/geb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/geb15.pdf",
    "published": "2015-06",
    "summary": " Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems. ",
    "code_link": ""
  },
  "icml2015_main_scalingupnaturalgradientbysparselyfactorizingtheinversefishermatrix": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix",
    "authors": [
      "Roger Grosse",
      "Ruslan Salakhudinov"
    ],
    "page_url": "http://proceedings.mlr.press/v37/grosse15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/grosse15.pdf",
    "published": "2015-06",
    "summary": " Second-order optimization methods, such as natural gradient, are difficult to apply to high-dimensional problems, because they require approximately solving large linear systems. We present FActorized Natural Gradient (FANG), an approximation to natural gradient descent where the Fisher matrix is approximated with a Gaussian graphical model whose precision matrix can be computed efficiently. We analyze the Fisher matrix for a small RBM and derive an extremely sparse graphical model which is a good match to the covariance of the sufficient statistics. Our experiments indicate that FANG allows RBMs to be trained more efficiently compared with stochastic gradient descent. Additionally, our analysis yields insight into the surprisingly good performance of the \u201ccentering trick\u201d for training RBMs. ",
    "code_link": ""
  },
  "icml2015_main_adeeperlookatplanningaslearningfromreplay": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Deeper Look at Planning as Learning from Replay",
    "authors": [
      "Harm Vanseijen",
      "Rich Sutton"
    ],
    "page_url": "http://proceedings.mlr.press/v37/vanseijen15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/vanseijen15.pdf",
    "published": "2015-06",
    "summary": " In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation. ",
    "code_link": "https://github.com/vanseijen/singlestep-vs-multistep"
  },
  "icml2015_main_optimalandadaptivealgorithmsforonlineboosting": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Optimal and Adaptive Algorithms for Online Boosting",
    "authors": [
      "Alina Beygelzimer",
      "Satyen Kale",
      "Haipeng Luo"
    ],
    "page_url": "http://proceedings.mlr.press/v37/beygelzimer15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/beygelzimer15.pdf",
    "published": "2015-06",
    "summary": " We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal. ",
    "code_link": ""
  },
  "icml2015_main_globalconvergenceofstochasticgradientdescentforsomenon-convexmatrixproblems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems",
    "authors": [
      "Christopher De Sa",
      "Christopher Re",
      "Kunle Olukotun"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sa15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sa15.pdf",
    "published": "2015-06",
    "summary": " Stochastic gradient descent (SGD) on a low-rank factorization is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(\u03b5^-1 n \\log n) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show some experiments to illustrate the runtime and convergence of the algorithm. ",
    "code_link": ""
  },
  "icml2015_main_anempiricalexplorationofrecurrentnetworkarchitectures": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An Empirical Exploration of Recurrent Network Architectures",
    "authors": [
      "Rafal Jozefowicz",
      "Wojciech Zaremba",
      "Ilya Sutskever"
    ],
    "page_url": "http://proceedings.mlr.press/v37/jozefowicz15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/jozefowicz15.pdf",
    "published": "2015-06",
    "summary": " The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM\u2019s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM\u2019s forget gate closes the gap between the LSTM and the GRU. ",
    "code_link": ""
  },
  "icml2015_main_completedictionaryrecoveryusingnonconvexoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Complete Dictionary Recovery Using Nonconvex Optimization",
    "authors": [
      "Ju Sun",
      "Qing Qu",
      "John Wright"
    ],
    "page_url": "http://proceedings.mlr.press/v37/sund15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/sund15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of recovering a complete (i.e., square and invertible) dictionary mb A_0, from mb Y = mb A_0 mb X_0 with mb Y \u2208\\mathbb R^n \\times p. This recovery setting is central to the theoretical understanding of dictionary learning. We give the first efficient algorithm that provably recovers mb A_0 when mb X_0 has O(n) nonzeros per column, under suitable probability model for mb X_0. Prior results provide recovery guarantees when mb X_0 has only O(\\sqrtn) nonzeros per column. Our algorithm is based on nonconvex optimization with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. Our proofs give a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no spurious local minima. Experiments with synthetic data corroborate our theory. Full version of this paper is available online: \\urlhttp://arxiv.org/abs/1504.06785. ",
    "code_link": ""
  },
  "icml2015_main_safepolicysearchforlifelongreinforcementlearningwithsublinearregret": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret",
    "authors": [
      "Haitham Bou Ammar",
      "Rasul Tutunov",
      "Eric Eaton"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ammar15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ammar15.pdf",
    "published": "2015-06",
    "summary": " Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control. ",
    "code_link": ""
  },
  "icml2015_main_passcodeparallelasynchronousstochasticdualco-ordinatedescent": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent",
    "authors": [
      "Cho-Jui Hsieh",
      "Hsiang-Fu Yu",
      "Inderjit Dhillon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hsieha15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hsieha15.pdf",
    "published": "2015-06",
    "summary": " Stochastic Dual Coordinate Descent (DCD) is one of the most efficient ways to solve the family of L2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of parallel asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties of DCD when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present a novel error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with a perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers. ",
    "code_link": ""
  },
  "icml2015_main_highconfidencepolicyimprovement": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "High Confidence Policy Improvement",
    "authors": [
      "Philip Thomas",
      "Georgios Theocharous",
      "Mohammad Ghavamzadeh"
    ],
    "page_url": "http://proceedings.mlr.press/v37/thomas15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/thomas15.pdf",
    "published": "2015-06",
    "summary": " We present a batch reinforcement learning (RL) algorithm that provides probabilistic guarantees about the quality of each policy that it proposes, and which has no hyper-parameter that requires expert tuning. Specifically, the user may select any performance lower-bound and confidence level and our algorithm will ensure that the probability that it returns a policy with performance below the lower bound is at most the specified confidence level. We then propose an incremental algorithm that executes our policy improvement algorithm repeatedly to generate multiple policy improvements. We show the viability of our approach with a simple 4 x 4 gridworld and the standard mountain car problem, as well as with a digital marketing application that uses real world data. ",
    "code_link": ""
  },
  "icml2015_main_fixed-pointalgorithmsforlearningdeterminantalpointprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fixed-point algorithms for learning determinantal point processes",
    "authors": [
      "Zelda Mariet",
      "Suvrit Sra"
    ],
    "page_url": "http://proceedings.mlr.press/v37/mariet15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/mariet15.pdf",
    "published": "2015-06",
    "summary": " Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique. ",
    "code_link": ""
  },
  "icml2015_main_consistentmulticlassalgorithmsforcomplexperformancemeasures": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Consistent Multiclass Algorithms for Complex Performance Measures",
    "authors": [
      "Harikrishna Narasimhan",
      "Harish Ramaswamy",
      "Aadirupa Saha",
      "Shivani Agarwal"
    ],
    "page_url": "http://proceedings.mlr.press/v37/narasimhanb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/narasimhanb15.pdf",
    "published": "2015-06",
    "summary": " This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special case all loss-based performance measures, which are simply linear functions of the confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro F_1 measures. We give a general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices, and give two specific instantiations based on the Frank-Wolfe method for concave performance measures and on the bisection method for ratio-of-linear performance measures. The resulting algorithms are provably consistent and outperform a multiclass version of the state-of-the-art SVMperf method in experiments; for large multiclass problems, the algorithms are also orders of magnitude faster than SVMperf. ",
    "code_link": ""
  },
  "icml2015_main_optimizingneuralnetworkswithkronecker-factoredapproximatecurvature": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature",
    "authors": [
      "James Martens",
      "Roger Grosse"
    ],
    "page_url": "http://proceedings.mlr.press/v37/martens15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/martens15.pdf",
    "published": "2015-06",
    "summary": " We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network\u2019s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC\u2019s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix. ",
    "code_link": ""
  },
  "icml2015_main_aconvexexemplar-basedapproachtomad-bayesdirichletprocessmixturemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models",
    "authors": [
      "En-Hsu Yen",
      "Xin Lin",
      "Kai Zhong",
      "Pradeep Ravikumar",
      "Inderjit Dhillon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yen15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yen15.pdf",
    "published": "2015-06",
    "summary": " MAD-Bayes (MAP-based Asymptotic Derivations) has been recently proposed as a general technique to derive scalable algorithm for Bayesian Nonparametric models. However, the combinatorial nature of objective functions derived from MAD-Bayes results in hard optimization problem, for which current practice employs heuristic algorithms analogous to k-means to find local minimum. In this paper, we consider the exemplar-based version of MAD-Bayes formulation for DP and Hierarchical DP (HDP) mixture model. We show that an exemplar-based MAD-Bayes formulation can be relaxed to a convex structural-regularized program that, under cluster-separation conditions, shares the same optimal solution to its combinatorial counterpart. An algorithm based on Alternating Direction Method of Multiplier (ADMM) is then proposed to solve such program. In our experiments on several benchmark data sets, the proposed method finds optimal solution of the combinatorial problem and significantly improves existing methods in terms of the exemplar-based objective. ",
    "code_link": ""
  },
  "icml2015_main_multi-instancemulti-labellearninginthepresenceofnovelclassinstances": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Multi-instance multi-label learning in the presence of novel class instances",
    "authors": [
      "Anh Pham",
      "Raviv Raich",
      "Xiaoli Fern",
      "Jes\u00fas P\u00e9rez Arriaga"
    ],
    "page_url": "http://proceedings.mlr.press/v37/pham15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/pham15.pdf",
    "published": "2015-06",
    "summary": " Multi-instance multi-label learning (MIML) is a framework for learning in the presence of label ambiguity. In MIML, experts provide labels for groups of instances (bags), instead of directly providing a label for every instance. When labeling efforts are focused on a set of target classes, instances outside this set will not be appropriately modeled. For example, ornithologists label bird audio recordings with a list of species present. Other additional sound instances, e.g., a rain drop or a moving vehicle sound, are not labeled. The challenge is due to the fact that for a given bag, the presence or absence of novel instances is latent. In this paper, this problem is addressed using a discriminative probabilistic model that accounts for novel instances. We propose an exact and efficient implementation of the maximum likelihood approach to determine the model parameters and consequently learn an instance-level classifier for all classes including the novel class. Experiments on both synthetic and real datasets illustrate the effectiveness of the proposed approach. ",
    "code_link": ""
  },
  "icml2015_main_entropy-basedconcentrationinequalitiesfordependentvariables": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Entropy-Based Concentration Inequalities for Dependent Variables",
    "authors": [
      "Liva Ralaivola",
      "Massih-Reza Amini"
    ],
    "page_url": "http://proceedings.mlr.press/v37/ralaivola15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/ralaivola15.pdf",
    "published": "2015-06",
    "summary": " We provide new concentration inequalities for functions of dependent variables. The work extends that of Janson (2004), which proposes concentration inequalities using a combination of the Laplace transform and the idea of fractional graph coloring, as well as many works that derive concentration inequalities using the entropy method (see, e.g., (Boucheron et al., 2003)). We give inequalities for fractionally sub-additive and fractionally self-bounding functions. In the way, we prove a new Talagrand concentration inequality for fractionally sub-additive functions of dependent variables. The results allow us to envision the derivation of generalization bounds for various applications where dependent variables naturally appear, such as in bipartite ranking. ",
    "code_link": ""
  },
  "icml2015_main_pulearningformatrixcompletion": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PU Learning for Matrix Completion",
    "authors": [
      "Cho-Jui Hsieh",
      "Nagarajan Natarajan",
      "Inderjit Dhillon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/hsiehb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/hsiehb15.pdf",
    "published": "2015-06",
    "summary": " In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M , and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only \u201clikes\u201d or \u201cfriendships\u201d are observed. The problem is an instance of PU (positive-unlabeled) learning, i.e. learning from only positive and unlabeled examples that has been studied in the context of binary classification. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a \u201cshifted matrix completion\u201d method that recovers M using only a subset of indices corresponding to ones; for the second case, we propose a \u201cbiased matrix completion\u201d method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds \u2014 if M \u2208R^n \\times n, the error is bounded as O(1-\u03c1) , where 1-\u03c1denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our analysis to the inductive matrix completion problem, where rows and columns of M have associated features. We develop efficient and scalable optimization procedures for both the proposed methods and demonstrate their effectiveness for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks. ",
    "code_link": ""
  },
  "icml2015_main_anasynchronousdistributedproximalgradientmethodforcompositeconvexoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization",
    "authors": [
      "Necdet Aybat",
      "Zi Wang",
      "Garud Iyengar"
    ],
    "page_url": "http://proceedings.mlr.press/v37/aybat15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/aybat15.pdf",
    "published": "2015-06",
    "summary": " We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any eps > 0, an eps-optimal and eps-feasible solution can be computed within O(log(1/eps)) DFAL iterations, which require O(\\psi_\\textmax^1.5/d_\\textmin \u22c51/\u03b5) proximal gradient computations and communications per node in total, where \\psi_\\textmax denotes the largest eigenvalue of the graph Laplacian, and d_\\textmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems. ",
    "code_link": ""
  },
  "icml2015_main_sparsesubspaceclusteringwithmissingentries": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Sparse Subspace Clustering with Missing Entries",
    "authors": [
      "Congyuan Yang",
      "Daniel Robinson",
      "Rene Vidal"
    ],
    "page_url": "http://proceedings.mlr.press/v37/yangf15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/yangf15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of clustering incomplete data drawn from a union of subspaces. Classical subspace clustering methods are not applicable to this problem because the data are incomplete, while classical low-rank matrix completion methods may not be applicable because data in multiple subspaces may not be low rank. This paper proposes and evaluates two new approaches for subspace clustering and completion. The first one generalizes the sparse subspace clustering algorithm so that it can obtain a sparse representation of the data using only the observed entries. The second one estimates a suitable kernel matrix by assuming a random model for the missing entries and obtains the sparse representation from this kernel. Experiments on synthetic and real data show the advantages and disadvantages of the proposed methods, which all outperform the natural approach (low-rank matrix completion followed by sparse subspace clustering) when the data matrix is high-rank or the percentage of missing entries is large. ",
    "code_link": ""
  },
  "icml2015_main_moderatedanddriftinglineardynamicalsystems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Moderated and Drifting Linear Dynamical Systems",
    "authors": [
      "Jinyan Guan",
      "Kyle Simek",
      "Ernesto Brau",
      "Clayton Morrison",
      "Emily Butler",
      "Kobus Barnard"
    ],
    "page_url": "http://proceedings.mlr.press/v37/guan15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/guan15.pdf",
    "published": "2015-06",
    "summary": " We consider linear dynamical systems, particularly coupled linear oscillators, where the parameters represent meaningful values in a domain theory and thus learning what affects them contributes to explanation. Rather than allow perturbations of latent states, we assume that temporal variation beyond noise is explained by parameter drift, and variation across coupled systems is a function of moderating variables. This change of focus reduces opportunities for efficient inference, and we propose sampling procedures to learn and fit the models. We test our approach on a real dataset of physiological measures of heterosexual couples engaged in a conversation about a potentially emotional topic, with body mass index (BMI) being considered as a moderator. We evaluate several models on their ability to predict future conversation dynamics (the last 20% of the data for each test couple), with shared parameters being learned using held out data. As proof of concept, we validate the hypothesis that BMI affects the conversation dynamic in the experimentally chosen topic. ",
    "code_link": ""
  },
  "icml2015_main_boostedcategoricalrestrictedboltzmannmachineforcomputationalpredictionofsplicejunctions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions",
    "authors": [
      "Taehoon Lee",
      "Sungroh Yoon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/leeb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/leeb15.pdf",
    "published": "2015-06",
    "summary": " Splicing refers to the elimination of non-coding regions in transcribed pre-messenger ribonucleic acid (RNA). Discovering splice sites is an important machine learning task that helps us not only to identify the basic units of genetic heredity but also to understand how different proteins are produced. Existing methods for splicing prediction have produced promising results, but often show limited robustness and accuracy. In this paper, we propose a deep belief network-based methodology for computational splice junction prediction. Our proposal includes a novel method for training restricted Boltzmann machines for class-imbalanced prediction. The proposed method addresses the limitations of conventional contrastive divergence and provides regularization for datasets that have categorical features. We tested our approach using public human genome datasets and obtained significantly improved accuracy and reduced runtime compared to state-of-the-art alternatives. The proposed approach was less sensitive to the length of input sequences and more robust for handling false splicing signals. Furthermore, we could discover non-canonical splicing patterns that were otherwise difficult to recognize using conventional methods. Given the efficiency and robustness of our methodology, we anticipate that it can be extended to the discovery of primary structural patterns of other subtle genomic elements. ",
    "code_link": ""
  },
  "icml2015_main_privacyforfreeposteriorsamplingandstochasticgradientmontecarlo": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo",
    "authors": [
      "Yu-Xiang Wang",
      "Stephen Fienberg",
      "Alex Smola"
    ],
    "page_url": "http://proceedings.mlr.press/v37/wangg15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/wangg15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to \u201cdifferential privacy\u201d, a cryptographic approach to protect individual-level privacy while permitting database-level utility. Specifically, we show that under standard assumptions, getting one sample from a posterior distribution is differentially private \u201cfor free\u201d; and this sample as a statistical estimator is often consistent, near optimal, and computationally tractable. Similarly but separately, we show that a recent line of work that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an \u201canytime\u201d algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets. ",
    "code_link": ""
  },
  "icml2015_main_atrust-regionmethodforstochasticvariationalinferencewithapplicationstostreamingdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A trust-region method for stochastic variational inference with applications to streaming data",
    "authors": [
      "Lucas Theis",
      "Matt Hoffman"
    ],
    "page_url": "http://proceedings.mlr.press/v37/theis15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/theis15.pdf",
    "published": "2015-06",
    "summary": " Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance. ",
    "code_link": ""
  },
  "icml2015_main_inferenceinapartiallyobservedqueuingmodelwithapplicationsinecology": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Inference in a Partially Observed Queuing Model with Applications in Ecology",
    "authors": [
      "Kevin Winner",
      "Garrett Bernstein",
      "Dan Sheldon"
    ],
    "page_url": "http://proceedings.mlr.press/v37/winner15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/winner15.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of inference in a probabilistic model for transient populations where we wish to learn about arrivals, departures, and population size over all time, but the only available data are periodic counts of the population size at specific observation times. The underlying model arises in queueing theory (as an M/G/inf queue) and also in ecological models for short-lived animals such as insects. Our work applies to both systems. Previous work in the ecology literature focused on maximum likelihood estimation and made a simplifying independence assumption that prevents inference over unobserved random variables such as arrivals and departures. The contribution of this paper is to formulate a latent variable model and develop a novel Gibbs sampler based on Markov bases to perform inference using the correct, but intractable, likelihood function. We empirically validate the convergence behavior of our sampler and demonstrate the ability of our model to make much finer-grained inferences than the previous approach. ",
    "code_link": ""
  },
  "icml2015_main_deterministicindependentcomponentanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deterministic Independent Component Analysis",
    "authors": [
      "Ruitong Huang",
      "Andras Gyorgy",
      "Csaba Szepesv\u00e1ri"
    ],
    "page_url": "http://proceedings.mlr.press/v37/huangb15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/huangb15.pdf",
    "published": "2015-06",
    "summary": " We study independent component analysis with noisy observations. We present, for the first time in the literature, consistent, polynomial-time algorithms to recover non-Gaussian source signals and the mixing matrix with a reconstruction error that vanishes at a 1/\\sqrtT rate using T observations and scales only polynomially with the natural parameters of the problem. Our algorithms and analysis also extend to deterministic source signals whose empirical distributions are approximately independent. ",
    "code_link": ""
  },
  "icml2015_main_ontheoptimalityofmulti-labelclassificationundersubsetzero-onelossfordistributionssatisfyingthecompositionproperty": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property",
    "authors": [
      "Maxime Gasse",
      "Alexandre Aussem",
      "Haytham Elghazel"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gasse15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gasse15.pdf",
    "published": "2015-06",
    "summary": " The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factorization of the conditional distribution of the label set given the input features play a pivotal role for multi-label classification in the context of subset Zero-One loss minimization, as they divide the learning task into simpler independent multi-class problems. We establish theoretical results to characterize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data. ",
    "code_link": ""
  },
  "icml2015_main_un-regularizingapproximateproximalpointandfasterstochasticalgorithmsforempiricalriskminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization",
    "authors": [
      "Roy Frostig",
      "Rong Ge",
      "Sham Kakade",
      "Aaron Sidford"
    ],
    "page_url": "http://proceedings.mlr.press/v37/frostig15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/frostig15.pdf",
    "published": "2015-06",
    "summary": " We develop a family of accelerated stochastic algorithms that optimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework, based on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original ERM problem. ",
    "code_link": ""
  },
  "icml2015_main_anewgeneralizederrorpathalgorithmformodelselection": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A New Generalized Error Path Algorithm for Model Selection",
    "authors": [
      "Bin Gu",
      "Charles Ling"
    ],
    "page_url": "http://proceedings.mlr.press/v37/gu15.html",
    "pdf_url": "http://proceedings.mlr.press/v37/gu15.pdf",
    "published": "2015-06",
    "summary": " Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error.In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search. ",
    "code_link": ""
  },
  "icml2015_main_nooops,youwon\u2019tdoitagainmechanismsforself-correctionincrowdsourcing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "No Oops, You Won\u2019t Do It Again: Mechanisms for Self-correction in Crowdsourcing",
    "authors": [
      "Nihar Shah",
      "Dengyong Zhou"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shaha16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shaha16.pdf",
    "published": "2015-06",
    "summary": " Crowdsourcing is a very popular means of obtaining the large amounts of labeled data that modern machine learning methods require. Although cheap and fast to obtain, crowdsourced labels suffer from significant amounts of error, thereby degrading the performance of downstream machine learning tasks. With the goal of improving the quality of the labeled data, we seek to mitigate the many errors that occur due to silly mistakes or inadvertent errors by crowdsourcing workers. We propose a two-stage setting for crowdsourcing where the worker first answers the questions, and is then allowed to change her answers after looking at a (noisy) reference answer. We mathematically formulate this process and develop mechanisms to incentivize workers to act appropriately. Our mathematical guarantees show that our mechanism incentivizes the workers to answer honestly in both stages, and refrain from answering randomly in the first stage or simply copying in the second. Numerical experiments reveal a significant boost in performance that such \"self-correction\" can provide when using crowdsourcing to train machine learning algorithms. ",
    "code_link": ""
  },
  "icml2015_main_stochasticallytransitivemodelsforpairwisecomparisonsstatisticalandcomputationalissues": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues",
    "authors": [
      "Nihar Shah",
      "Sivaraman Balakrishnan",
      "Aditya Guntuboyina",
      "Martin Wainwright"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shahb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shahb16.pdf",
    "published": "2015-06",
    "summary": " There are various parametric models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations. ",
    "code_link": ""
  },
  "icml2015_main_uprootingandrerootinggraphicalmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Uprooting and Rerooting Graphical Models",
    "authors": [
      "Adrian Weller"
    ],
    "page_url": "https://proceedings.mlr.press/v48/weller16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/weller16.pdf",
    "published": "2015-06",
    "summary": " We show how any binary pairwise model may be \u201cuprooted\u201d to a fully symmetric model, wherein original singleton potentials are transformed to potentials on edges to an added variable, and then \u201crerooted\u201d to a new model on the original number of variables. The new model is essentially equivalent to the original model, with the same partition function and allowing recovery of the original marginals or a MAP configuration, yet may have very different computational properties that allow much more efficient inference. This meta-approach deepens our understanding, may be applied to any existing algorithm to yield improved methods in practice, generalizes earlier theoretical results, and reveals a remarkable interpretation of the triplet-consistent polytope. ",
    "code_link": ""
  },
  "icml2015_main_adeeplearningapproachtounsupervisedensemblelearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Deep Learning Approach to Unsupervised Ensemble Learning",
    "authors": [
      "Uri Shaham",
      "Xiuyuan Cheng",
      "Omer Dror",
      "Ariel Jaffe",
      "Boaz Nadler",
      "Joseph Chang",
      "Yuval Kluger"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shaham16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shaham16.pdf",
    "published": "2015-06",
    "summary": " We show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning. First, we prove that the popular model of Dawid and Skene, which assumes that all classifiers are conditionally independent, is \\em equivalent to a Restricted Boltzmann Machine (RBM) with a single hidden node. Hence, under this model, the posterior probabilities of the true labels can be instead estimated via a trained RBM. Next, to address the more general case, where classifiers may strongly violate the conditional independence assumption, we propose to apply RBM-based Deep Neural Net (DNN). Experimental results on various simulated and real-world datasets demonstrate that our proposed DNN approach outperforms other state-of-the-art methods, in particular when the data violates the conditional independence assumption. ",
    "code_link": "https://github.com/ushaham/RBMpaper"
  },
  "icml2015_main_revisitingsemi-supervisedlearningwithgraphembeddings": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Revisiting Semi-Supervised Learning with Graph Embeddings",
    "authors": [
      "Zhilin Yang",
      "William Cohen",
      "Ruslan Salakhudinov"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yanga16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yanga16.pdf",
    "published": "2015-06",
    "summary": " We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models. ",
    "code_link": "https://github.com/phanein/deepwalk"
  },
  "icml2015_main_guidedcostlearningdeepinverseoptimalcontrolviapolicyoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
    "authors": [
      "Chelsea Finn",
      "Sergey Levine",
      "Pieter Abbeel"
    ],
    "page_url": "https://proceedings.mlr.press/v48/finn16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/finn16.pdf",
    "published": "2015-06",
    "summary": " Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency. ",
    "code_link": ""
  },
  "icml2015_main_diversity-promotingbayesianlearningoflatentvariablemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Diversity-Promoting Bayesian Learning of Latent Variable Models",
    "authors": [
      "Pengtao Xie",
      "Jun Zhu",
      "Eric Xing"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xiea16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xiea16.pdf",
    "published": "2015-06",
    "summary": " In learning latent variable models (LVMs), it is important to effectively capture infrequent patterns and shrink model size without sacrificing modeling power. Various studies have been done to \u201cdiversify\u201d a LVM, which aim to learn a diverse set of latent components in LVMs. Most existing studies fall into a frequentist-style regularization framework, where the components are learned via point estimation. In this paper, we investigate how to \u201cdiversify\u201d LVMs in the paradigm of Bayesian learning, which has advantages complementary to point estimation, such as alleviating overfitting via model averaging and quantifying uncertainty. We propose two approaches that have complementary advantages. One is to define diversity-promoting mutual angular priors which assign larger density to components with larger mutual angles based on Bayesian network and von Mises-Fisher distribution and use these priors to affect the posterior via Bayes rule. We develop two efficient approximate posterior inference algorithms based on variational inference and Markov chain Monte Carlo sampling. The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components. These two methods are applied to the Bayesian mixture of experts model to encourage the \u201cexperts\u201d to be diverse and experimental results demonstrate the effectiveness and efficiency of our methods. ",
    "code_link": ""
  },
  "icml2015_main_additiveapproximationsinhighdimensionalnonparametricregressionviathesalsa": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Additive Approximations in High Dimensional Nonparametric Regression via the SALSA",
    "authors": [
      "Kirthevasan Kandasamy",
      "Yaoliang Yu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kandasamy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kandasamy16.pdf",
    "published": "2015-06",
    "summary": " High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \\emphfirst order, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose salsa, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. salsas minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on 15 real datasets, we show that our method is competitive against 21 other alternatives. ",
    "code_link": ""
  },
  "icml2015_main_hawkesprocesseswithstochasticexcitations": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hawkes Processes with Stochastic Excitations",
    "authors": [
      "Young Lee",
      "Kar Wai Lim",
      "Cheng Soon Ong"
    ],
    "page_url": "https://proceedings.mlr.press/v48/leea16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/leea16.pdf",
    "published": "2015-06",
    "summary": " We propose an extension to Hawkes processes by treating the levels of self-excitation as a stochastic differential equation. Our new point process allows better approximation in application domains where events and intensities accelerate each other with correlated levels of contagion. We generalize a recent algorithm for simulating draws from Hawkes processes whose levels of excitation are stochastic processes, and propose a hybrid Markov chain Monte Carlo approach for model fitting. Our sampling procedure scales linearly with the number of required events and does not require stationarity of the point process. A modular inference procedure consisting of a combination between Gibbs and Metropolis Hastings steps is put forward. We recover expectation maximization as a special case. Our general approach is illustrated for contagion following geometric Brownian motion and exponential Langevin dynamics. ",
    "code_link": ""
  },
  "icml2015_main_data-drivenrankbreakingforefficientrankaggregation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Data-driven Rank Breaking for Efficient Rank Aggregation",
    "authors": [
      "Ashish Khetan",
      "Sewoong Oh"
    ],
    "page_url": "https://proceedings.mlr.press/v48/khetan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/khetan16.pdf",
    "published": "2015-06",
    "summary": " Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. To reduce the computational complexity of learning the global ranking, a common practice is to use rank-breaking. Individuals\u2019 preferences are broken into pairwise comparisons and then applied to efficient algorithms tailored for independent pairwise comparisons. However, due to the ignored dependencies, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce unbiased and accurate estimates is to treat the paired comparisons outcomes unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity in some canonical scenarios. Further, we identify how the accuracy depends on the spectral gap of a corresponding comparison graph. ",
    "code_link": ""
  },
  "icml2015_main_dropoutdistillation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dropout distillation",
    "authors": [
      "Samuel Rota Bul\u00f2",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bulo16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bulo16.pdf",
    "published": "2015-06",
    "summary": " Dropout is a popular stochastic regularization technique for deep neural networks that works by randomly dropping (i.e. zeroing) units from the network during training. This randomization process allows to implicitly train an ensemble of exponentially many networks sharing the same parametrization, which should be averaged at test time to deliver the final prediction. A typical workaround for this intractable averaging operation consists in scaling the layers undergoing dropout randomization. This simple rule called \u2019standard dropout\u2019 is efficient, but might degrade the accuracy of the prediction. In this work we introduce a novel approach, coined \u2019dropout distillation\u2019, that allows us to train a predictor in a way to better approximate the intractable, but preferable, averaging process, while keeping under control its computational efficiency. We are thus able to construct models that are as efficient as standard dropout, or even more efficient, while being more accurate. Experiments on standard benchmark datasets demonstrate the validity of our method, yielding consistent improvements over conventional dropout. ",
    "code_link": ""
  },
  "icml2015_main_metadata-consciousanonymousmessaging": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Metadata-conscious anonymous messaging",
    "authors": [
      "Giulia Fanti",
      "Peter Kairouz",
      "Sewoong Oh",
      "Kannan Ramchandran",
      "Pramod Viswanath"
    ],
    "page_url": "https://proceedings.mlr.press/v48/fanti16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/fanti16.pdf",
    "published": "2015-06",
    "summary": " Anonymous messaging platforms like Whisper and Yik Yak allow users to spread messages over a network (e.g., a social network) without revealing message authorship to other users. The spread of messages on these platforms can be modeled by a diffusion process over a graph. Recent advances in network analysis have revealed that such diffusion processes are vulnerable to author deanonymization by adversaries with access to metadata, such as timing information. In this work, we ask the fundamental question of how to propagate anonymous messages over a graph to make it difficult for adversaries to infer the source. In particular, we study the performance of a message propagation protocol called adaptive diffusion introduced in (Fanti et al., 2015). We prove that when the adversary has access to metadata at a fraction of corrupted graph nodes, adaptive diffusion achieves asymptotically optimal source-hiding and significantly outperforms standard diffusion. We further demonstrate empirically that adaptive diffusion hides the source effectively on real social networks. ",
    "code_link": ""
  },
  "icml2015_main_theteachingdimensionoflinearlearners": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Teaching Dimension of Linear Learners",
    "authors": [
      "Ji Liu",
      "Xiaojin Zhu",
      "Hrag Ohannessian"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liua16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liua16.pdf",
    "published": "2015-06",
    "summary": " Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners. ",
    "code_link": ""
  },
  "icml2015_main_truthfulunivariateestimators": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Truthful Univariate Estimators",
    "authors": [
      "Ioannis Caragiannis",
      "Ariel Procaccia",
      "Nisarg Shah"
    ],
    "page_url": "https://proceedings.mlr.press/v48/caragiannis16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/caragiannis16.pdf",
    "published": "2015-06",
    "summary": " We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples, taking a game-theoretic viewpoint. In our setting, samples are supplied by strategic agents, who wish to pull the estimate as close as possible to their own value. In this setting, the sample mean gives rise to manipulation opportunities, whereas the sample median does not. Our key question is whether the sample median is the best (in terms of mean squared error) truthful estimator of the population mean. We show that when the underlying distribution is symmetric, there are truthful estimators that dominate the median. Our main result is a characterization of worst-case optimal truthful estimators, which provably outperform the median, for possibly asymmetric distributions with bounded support. ",
    "code_link": ""
  },
  "icml2015_main_whyregularizedauto-encoderslearnsparserepresentation?": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Why Regularized Auto-Encoders learn Sparse Representation?",
    "authors": [
      "Devansh Arpit",
      "Yingbo Zhou",
      "Hung Ngo",
      "Venu Govindaraju"
    ],
    "page_url": "https://proceedings.mlr.press/v48/arpita16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/arpita16.pdf",
    "published": "2015-06",
    "summary": " Sparse distributed representation is the key to learning useful features in deep learning algorithms, because not only it is an efficient mode of data representation, but also \u2013 more importantly \u2013 it captures the generation process of most real world data. While a number of regularized auto-encoders (AE) enforce sparsity explicitly in their learned representation and others don\u2019t, there has been little formal analysis on what encourages sparsity in these models in general. Our objective is to formally study this general problem for regularized auto-encoders. We provide sufficient conditions on both regularization and activation functions that encourage sparsity. We show that multiple popular models (de-noising and contractive auto encoders, e.g.) and activations (rectified linear and sigmoid, e.g.) satisfy these conditions; thus, our conditions help explain sparsity in their learned representation. Thus our theoretical and empirical analysis together shed light on the properties of regularization/activation that are conductive to sparsity and unify a number of existing auto-encoder models and activation functions under the same analytical framework. ",
    "code_link": ""
  },
  "icml2015_main_k-variates++moreplusesinthek-means++": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "k-variates++: more pluses in the k-means++",
    "authors": [
      "Richard Nock",
      "Raphael Canyasse",
      "Roksana Boreli",
      "Frank Nielsen"
    ],
    "page_url": "https://proceedings.mlr.press/v48/nock16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/nock16.pdf",
    "published": "2015-06",
    "summary": " k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, *and* a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a *bias+variance* approximation bound of the *global* optimum. This approximation exhibits a reduced dependency on the \"noise\" component with respect to the optimal potential \u2014 actually approaching the statistical lower bound. We show that k-variates++ *reduces* to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with *direct* approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds \u2014 state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is *no* closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art. ",
    "code_link": ""
  },
  "icml2015_main_multi-playerbandits\u2013amusicalchairsapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Multi-Player Bandits \u2013 a Musical Chairs Approach",
    "authors": [
      "Jonathan Rosenski",
      "Ohad Shamir",
      "Liran Szlak"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rosenski16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rosenski16.pdf",
    "published": "2015-06",
    "summary": " We consider a variant of the stochastic multi-armed bandit problem, where multiple players simultaneously choose from the same set of arms and may collide, receiving no reward. This setting has been motivated by problems arising in cognitive radio networks, and is especially challenging under the realistic assumption that communication between players is limited. We provide a communication-free algorithm (Musical Chairs) which attains constant regret with high probability, as well as a sublinear-regret, communication-free algorithm (Dynamic Musical Chairs) for the more difficult setting of players dynamically entering and leaving throughout the game. Moreover, both algorithms do not require prior knowledge of the number of players. To the best of our knowledge, these are the first communication-free algorithms with these types of formal guarantees. ",
    "code_link": ""
  },
  "icml2015_main_theinformationsieve": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Information Sieve",
    "authors": [
      "Greg Ver Steeg",
      "Aram Galstyan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/steeg16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/steeg16.pdf",
    "published": "2015-06",
    "summary": " We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis, lossy and lossless compression, and predicting missing values in data. ",
    "code_link": ""
  },
  "icml2015_main_deepspeech2end-to-endspeechrecognitioninenglishandmandarin": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin",
    "authors": [
      "Dario Amodei",
      "Sundaram Ananthanarayanan",
      "Rishita Anubhai",
      "Jingliang Bai",
      "Eric Battenberg",
      "Carl Case",
      "Jared Casper",
      "Bryan Catanzaro",
      "Qiang Cheng",
      "Guoliang Chen",
      "Jie Chen",
      "Jingdong Chen",
      "Zhijie Chen",
      "Mike Chrzanowski",
      "Adam Coates",
      "Greg Diamos",
      "Ke Ding",
      "Niandong Du",
      "Erich Elsen",
      "Jesse Engel",
      "Weiwei Fang",
      "Linxi Fan",
      "Christopher Fougner",
      "Liang Gao",
      "Caixia Gong",
      "Awni Hannun",
      "Tony Han",
      "Lappi Johannes",
      "Bing Jiang",
      "Cai Ju",
      "Billy Jun",
      "Patrick LeGresley",
      "Libby Lin",
      "Junjie Liu",
      "Yang Liu",
      "Weigao Li",
      "Xiangang Li",
      "Dongpeng Ma",
      "Sharan Narang",
      "Andrew Ng",
      "Sherjil Ozair",
      "Yiping Peng",
      "Ryan Prenger",
      "Sheng Qian",
      "Zongfeng Quan",
      "Jonathan Raiman",
      "Vinay Rao",
      "Sanjeev Satheesh",
      "David Seetapun",
      "Shubho Sengupta",
      "Kavya Srinet",
      "Anuroop Sriram",
      "Haiyuan Tang",
      "Liliang Tang",
      "Chong Wang",
      "Jidong Wang",
      "Kaifu Wang",
      "Yi Wang",
      "Zhijian Wang",
      "Zhiqian Wang",
      "Shuang Wu",
      "Likai Wei",
      "Bo Xiao",
      "Wen Xie",
      "Yan Xie",
      "Dani Yogatama",
      "Bin Yuan",
      "Jun Zhan",
      "Zhenyao Zhu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/amodei16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/amodei16.pdf",
    "published": "2015-06",
    "summary": " We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\u2013two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale. ",
    "code_link": ""
  },
  "icml2015_main_ontheconsistencyoffeatureselectionwithlassofornon-lineartargets": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Consistency of Feature Selection With Lasso for Non-linear Targets",
    "authors": [
      "Yue Zhang",
      "Weihong Guo",
      "Soumya Ray"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhanga16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhanga16.pdf",
    "published": "2015-06",
    "summary": " An important question in feature selection is whether a selection strategy recovers the \u201ctrue\u201d set of features, given enough data. We study this question in the context of the popular Least Absolute Shrinkage and Selection Operator (Lasso) feature selection strategy. In particular, we consider the scenario when the model is misspecified so that the learned model is linear while the underlying real target is nonlinear. Surprisingly, we prove that under certain conditions, Lasso is still able to recover the correct features in this case. We also carry out numerical studies to empirically verify the theoretical results and explore the necessity of the conditions under which the proof holds. ",
    "code_link": ""
  },
  "icml2015_main_minimumregretsearchforsingle-andmulti-taskoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Minimum Regret Search for Single- and Multi-Task Optimization",
    "authors": [
      "Jan Hendrik Metzen"
    ],
    "page_url": "https://proceedings.mlr.press/v48/metzen16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/metzen16.pdf",
    "published": "2015-06",
    "summary": " We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem. ",
    "code_link": "https://github.com/jmetzen/bayesian"
  },
  "icml2015_main_cryptonetsapplyingneuralnetworkstoencrypteddatawithhighthroughputandaccuracy": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy",
    "authors": [
      "Ran Gilad-Bachrach",
      "Nathan Dowlin",
      "Kim Laine",
      "Kristin Lauter",
      "Michael Naehrig",
      "John Wernsing"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gilad-bachrach16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gilad-bachrach16.pdf",
    "published": "2015-06",
    "summary": " Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions. ",
    "code_link": ""
  },
  "icml2015_main_thevariationalnystrommethodforlarge-scalespectralproblems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Variational Nystrom method for large-scale spectral problems",
    "authors": [
      "Max Vladymyrov",
      "Miguel Carreira-Perpinan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/vladymyrov16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/vladymyrov16.pdf",
    "published": "2015-06",
    "summary": " Spectral methods for dimensionality reduction and clustering require solving an eigenproblem defined by a sparse affinity matrix. When this matrix is large, one seeks an approximate solution. The standard way to do this is the Nystrom method, which first solves a small eigenproblem considering only a subset of landmark points, and then applies an out-of-sample formula to extrapolate the solution to the entire dataset. We show that by constraining the original problem to satisfy the Nystrom formula, we obtain an approximation that is computationally simple and efficient, but achieves a lower approximation error using fewer landmarks and less runtime. We also study the role of normalization in the computational cost and quality of the resulting solution. ",
    "code_link": ""
  },
  "icml2015_main_multi-biasnon-linearactivationindeepneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Multi-Bias Non-linear Activation in Deep Neural Networks",
    "authors": [
      "Hongyang Li",
      "Wanli Ouyang",
      "Xiaogang Wang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lia16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lia16.pdf",
    "published": "2015-06",
    "summary": " As a widely used non-linear activation, Rectified Linear Unit (ReLU) separates noise and signal in a feature map by learning a threshold or bias. However, we argue that the classification of noise and signal not only depends on the magnitude of responses, but also the context of how the feature responses would be used to detect more abstract patterns in higher layers. In order to output multiple response maps with magnitude in different ranges for a particular visual pattern, existing networks employing ReLU and its variants have to learn a large number of redundant filters. In this paper, we propose a multi-bias non-linear activation (MBA) layer to explore the information hidden in the magnitudes of responses. It is placed after the convolution layer to decouple the responses to a convolution kernel into multiple maps by multi-thresholding magnitudes, thus generating more patterns in the feature space at a low computational cost. It provides great flexibility of selecting responses to different visual patterns in different magnitude ranges to form rich representations in higher layers. Such a simple and yet effective scheme achieves the state-of-the-art performance on several benchmarks. ",
    "code_link": ""
  },
  "icml2015_main_asymmetricmulti-tasklearningbasedontaskrelatednessandloss": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Asymmetric Multi-task Learning Based on Task Relatedness and Loss",
    "authors": [
      "Giwoong Lee",
      "Eunho Yang",
      "Sung Hwang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/leeb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/leeb16.pdf",
    "published": "2015-06",
    "summary": " We propose a novel multi-task learning method that can minimize the effect of negative transfer by allowing asymmetric transfer between the tasks based on task relatedness as well as the amount of individual task losses, which we refer to as Asymmetric Multi-task Learning (AMTL). To tackle this problem, we couple multiple tasks via a sparse, directed regularization graph, that enforces each task parameter to be reconstructed as a sparse combination of other tasks, which are selected based on the task-wise loss. We present two different algorithms to solve this joint learning of the task predictors and the regularization graph. The first algorithm solves for the original learning objective using alternative optimization, and the second algorithm solves an approximation of it using curriculum learning strategy, that learns one task at a time. We perform experiments on multiple datasets for classification and regression, on which we obtain significant improvements in performance over the single task learning and symmetric multitask learning baselines. ",
    "code_link": ""
  },
  "icml2015_main_accuraterobustandefficienterrorestimationfordecisiontrees": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Accurate Robust and Efficient Error Estimation for Decision Trees",
    "authors": [
      "Lixin Fan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/fan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/fan16.pdf",
    "published": "2015-06",
    "summary": " This paper illustrates a novel approach to the estimation of generalization error of decision tree classifiers. We set out the study of decision tree errors in the context of consistency analysis theory, which proved that the Bayes error can be achieved only if when the number of data samples thrown into each leaf node goes to infinity. For the more challenging and practical case where the sample size is finite or small, a novel sampling error term is introduced in this paper to cope with the small sample problem effectively and efficiently. Extensive experimental results show that the proposed error estimate is superior to the well known K-fold cross validation methods in terms of robustness and accuracy. Moreover it is orders of magnitudes more efficient than cross validation methods. ",
    "code_link": ""
  },
  "icml2015_main_faststochasticalgorithmsforsvdandpcaconvergencepropertiesandconvexity": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity",
    "authors": [
      "Ohad Shamir"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shamira16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shamira16.pdf",
    "published": "2015-06",
    "summary": " We study the convergence properties of the VR-PCA algorithm introduced by (Shamir, 2015) for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the analysis, and what are the convexity and non-convexity properties of the underlying optimization problem. ",
    "code_link": ""
  },
  "icml2015_main_convergenceofstochasticgradientdescentforpca": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Convergence of Stochastic Gradient Descent for PCA",
    "authors": [
      "Ohad Shamir"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shamirb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shamirb16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in R^d. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this paper, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in (Hardt & Price, 2014). Moreover, under an eigengap assumption, we show that the same techniques lead to new SGD convergence guarantees with better dependence on the eigengap. ",
    "code_link": ""
  },
  "icml2015_main_dealbreakeranonlinearlatentvariablemodelforeducationaldata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dealbreaker: A Nonlinear Latent Variable Model for Educational Data",
    "authors": [
      "Andrew Lan",
      "Tom Goldstein",
      "Richard Baraniuk",
      "Christoph Studer"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lan16.pdf",
    "published": "2015-06",
    "summary": " Statistical models of student responses on assessment questions, such as those in homeworks and exams, enable educators and computer-based personalized learning systems to gain insights into students\u2019 knowledge using machine learning. Popular student-response models, including the Rasch model and item response theory models, represent the probability of a student answering a question correctly using an affine function of latent factors. While such models can accurately predict student responses, their ability to interpret the underlying knowledge structure (which is certainly nonlinear) is limited. In response, we develop a new, nonlinear latent variable model that we call the dealbreaker model, in which a student\u2019s success probability is determined by their weakest concept mastery. We develop efficient parameter inference algorithms for this model using novel methods for nonconvex optimization. We show that the dealbreaker model achieves comparable or better prediction performance as compared to affine models with real-world educational datasets. We further demonstrate that the parameters learned by the dealbreaker model are interpretable\u2014they provide key insights into which concepts are critical (i.e., the \u201cdealbreaker\u201d) to answering a question correctly. We conclude by reporting preliminary results for a movie-rating dataset, which illustrate the broader applicability of the dealbreaker model. ",
    "code_link": ""
  },
  "icml2015_main_akernelizedsteindiscrepancyforgoodness-of-fittests": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Kernelized Stein Discrepancy for Goodness-of-fit Tests",
    "authors": [
      "Qiang Liu",
      "Jason Lee",
      "Michael Jordan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liub16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liub16.pdf",
    "published": "2015-06",
    "summary": " We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein\u2019s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly. ",
    "code_link": ""
  },
  "icml2015_main_variableeliminationinthefourierdomain": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variable Elimination in the Fourier Domain",
    "authors": [
      "Yexiang Xue",
      "Stefano Ermon",
      "Ronan Le Bras",
      "Carla",
      "Bart Selman"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xue16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xue16.pdf",
    "published": "2015-06",
    "summary": " The ability to represent complex high dimensional probability distributions in a compact form is one of the key insights in the field of graphical models. Factored representations are ubiquitous in machine learning and lead to major computational advantages. We explore a different type of compact representation based on discrete Fourier representations, complementing the classical approach based on conditional independencies. We show that a large class of probabilistic graphical models have a compact Fourier representation. This theoretical result opens up an entirely new way of approximating a probability distribution. We demonstrate the significance of this approach by applying it to the variable elimination algorithm. Compared with the traditional bucket representation and other approximate inference algorithms, we obtain significant improvements. ",
    "code_link": ""
  },
  "icml2015_main_low-rankmatrixapproximationwithstability": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Low-Rank Matrix Approximation with Stability",
    "authors": [
      "Dongsheng Li",
      "Chao Chen",
      "Qin Lv",
      "Junchi Yan",
      "Li Shang",
      "Stephen Chu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lib16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lib16.pdf",
    "published": "2015-06",
    "summary": " Low-rank matrix approximation has been widely adopted in machine learning applications with sparse data, such as recommender systems. However, the sparsity of the data, incomplete and noisy, introduces challenges to the algorithm stability \u2013 small changes in the training data may significantly change the models. As a result, existing low-rank matrix approximation solutions yield low generalization performance, exhibiting high error variance on the training dataset, and minimizing the training error may not guarantee error reduction on the testing dataset. In this paper, we investigate the algorithm stability problem of low-rank matrix approximations. We present a new algorithm design framework, which (1) introduces new optimization objectives to guide stable matrix approximation algorithm design, and (2) solves the optimization problem to obtain stable low-rank approximation solutions with good generalization performance. Experimental results on real-world datasets demonstrate that the proposed work can achieve better prediction accuracy compared with both state-of-the-art low-rank matrix approximation methods and ensemble methods in recommendation task. ",
    "code_link": "https://github.com/ldscc/StableMA.git"
  },
  "icml2015_main_linkinglossesfordensityratioandclass-probabilityestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Linking losses for density ratio and class-probability estimation",
    "authors": [
      "Aditya Menon",
      "Cheng Soon Ong"
    ],
    "page_url": "https://proceedings.mlr.press/v48/menon16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/menon16.pdf",
    "published": "2015-06",
    "summary": " Given samples from two densities p and q, density ratio estimation (DRE) is the problem of estimating the ratio p/q. Two popular discriminative approaches to DRE are KL importance estimation (KLIEP), and least squares importance fitting (LSIF). In this paper, we show that KLIEP and LSIF both employ class-probability estimation (CPE) losses. Motivated by this, we formally relate DRE and CPE, and demonstrate the viability of using existing losses from one problem for the other. For the DRE problem, we show that essentially any CPE loss (eg logistic, exponential) can be used, as this equivalently minimises a Bregman divergence to the true density ratio. We show how different losses focus on accurately modelling different ranges of the density ratio, and use this to design new CPE losses for DRE. For the CPE problem, we argue that the LSIF loss is useful in the regime where one wishes to rank instances with maximal accuracy at the head of the ranking. In the course of our analysis, we establish a Bregman divergence identity that may be of independent interest. ",
    "code_link": ""
  },
  "icml2015_main_stochasticvariancereductionfornonconvexoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Variance Reduction for Nonconvex Optimization",
    "authors": [
      "Sashank J. Reddi",
      "Ahmed Hefny",
      "Suvrit Sra",
      "Barnabas Poczos",
      "Alex Smola"
    ],
    "page_url": "https://proceedings.mlr.press/v48/reddi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/reddi16.pdf",
    "published": "2015-06",
    "summary": " We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to minibatching in parallel settings. ",
    "code_link": ""
  },
  "icml2015_main_hierarchicalvariationalmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hierarchical Variational Models",
    "authors": [
      "Rajesh Ranganath",
      "Dustin Tran",
      "David Blei"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ranganath16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ranganath16.pdf",
    "published": "2015-06",
    "summary": " Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior. ",
    "code_link": ""
  },
  "icml2015_main_hierarchicalspan-basedconditionalrandomfieldsforlabelingandsegmentingeventsinwearablesensordatastreams": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hierarchical Span-Based Conditional Random Fields for Labeling and Segmenting Events in Wearable Sensor Data Streams",
    "authors": [
      "Roy Adams",
      "Nazir Saleheen",
      "Edison Thomaz",
      "Abhinav Parate",
      "Santosh Kumar",
      "Benjamin Marlin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/adams16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/adams16.pdf",
    "published": "2015-06",
    "summary": " The field of mobile health (mHealth) has the potential to yield new insights into health and behavior through the analysis of continuously recorded data from wearable health and activity sensors. In this paper, we present a hierarchical span-based conditional random field model for the key problem of jointly detecting discrete events in such sensor data streams and segmenting these events into high-level activity sessions. Our model includes higher-order cardinality factors and inter-event duration factors to capture domain-specific structure in the label space. We show that our model supports exact MAP inference in quadratic time via dynamic programming, which we leverage to perform learning in the structured support vector machine framework. We apply the model to the problems of smoking and eating detection using four real data sets. Our results show statistically significant improvements in segmentation performance relative to a hierarchical pairwise CRF. ",
    "code_link": ""
  },
  "icml2015_main_binaryembeddingswithstructuredhashedprojections": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Binary embeddings with structured hashed projections",
    "authors": [
      "Anna Choromanska",
      "Krzysztof Choromanski",
      "Mariusz Bojarski",
      "Tony Jebara",
      "Sanjiv Kumar",
      "Yann LeCun"
    ],
    "page_url": "https://proceedings.mlr.press/v48/choromanska16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/choromanska16.pdf",
    "published": "2015-06",
    "summary": " We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed \u201cbudget of randomness\u201d is distributed across the matrix. Such matrices can be edfficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier. ",
    "code_link": ""
  },
  "icml2015_main_avariationalanalysisofstochasticgradientalgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Variational Analysis of Stochastic Gradient Algorithms",
    "authors": [
      "Stephan Mandt",
      "Matthew Hoffman",
      "David Blei"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mandt16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mandt16.pdf",
    "published": "2015-06",
    "summary": " Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models. ",
    "code_link": ""
  },
  "icml2015_main_adaptivesamplingforsgdbyexploitingsideinformation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Adaptive Sampling for SGD by Exploiting Side Information",
    "authors": [
      "Siddharth Gopal"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gopal16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gopal16.pdf",
    "published": "2015-06",
    "summary": " This paper proposes a new mechanism for sampling training instances for stochastic gradient descent (SGD) methods by exploiting any side-information associated with the instances (for e.g. class-labels) to improve convergence. Previous methods have either relied on sampling from a distribution defined over training instances or from a static distribution that fixed before training. This results in two problems a) any distribution that is set apriori is independent of how the optimization progresses and b) maintaining a distribution over individual instances could be infeasible in large-scale scenarios. In this paper, we exploit the side information associated with the instances to tackle both problems. More specifically, we maintain a distribution over classes (instead of individual instances) that is adaptively estimated during the course of optimization to give the maximum reduction in the variance of the gradient. Intuitively, we sample more from those regions in space that have a \\textitlarger gradient contribution. Our experiments on highly multiclass datasets show that our proposal converge significantly faster than existing techniques. ",
    "code_link": ""
  },
  "icml2015_main_learningfrommultiwaydatasimpleandefficienttensorregression": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning from Multiway Data: Simple and Efficient Tensor Regression",
    "authors": [
      "Rose Yu",
      "Yan Liu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yu16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yu16.pdf",
    "published": "2015-06",
    "summary": " Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications. ",
    "code_link": ""
  },
  "icml2015_main_adistributedvariationalinferenceframeworkforunifyingparallelsparsegaussianprocessregressionmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Distributed Variational Inference Framework for Unifying Parallel Sparse Gaussian Process Regression Models",
    "authors": [
      "Trong Nghia Hoang",
      "Quang Minh Hoang",
      "Bryan Kian Hsiang Low"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hoang16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hoang16.pdf",
    "published": "2015-06",
    "summary": " This paper presents a novel distributed variational inference framework that unifies many parallel sparse Gaussian process regression (SGPR) models for scalable hyperparameter learning with big data. To achieve this, our framework exploits a structure of correlated noise process model that represents the observation noises as a finite realization of a high-order Gaussian Markov random process. By varying the Markov order and covariance function for the noise process model, different variational SGPR models result. This consequently allows the correlation structure of the noise process model to be characterized for which a particular variational SGPR model is optimal. We empirically evaluate the predictive performance and scalability of the distributed variational SGPR models unified by our framework on two real-world datasets. ",
    "code_link": ""
  },
  "icml2015_main_onlinestochasticlinearoptimizationunderone-bitfeedback": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Online Stochastic Linear Optimization under One-bit Feedback",
    "authors": [
      "Lijun Zhang",
      "Tianbao Yang",
      "Rong Jin",
      "Yichi Xiao",
      "Zhi-hua Zhou"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhangb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhangb16.pdf",
    "published": "2015-06",
    "summary": " In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world applications. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of O(d\\sqrtT), which matches the optimal result of stochastic linear bandits. ",
    "code_link": ""
  },
  "icml2015_main_adaptivealgorithmsforonlineconvexoptimizationwithlong-termconstraints": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Adaptive Algorithms for Online Convex Optimization with Long-term Constraints",
    "authors": [
      "Rodolphe Jenatton",
      "Jim Huang",
      "Cedric Archambeau"
    ],
    "page_url": "https://proceedings.mlr.press/v48/jenatton16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/jenatton16.pdf",
    "published": "2015-06",
    "summary": " We present an adaptive online gradient descent algorithm to solve online convex optimization problems with long-term constraints, which are constraints that need to be satisfied when accumulated over a finite number of rounds T, but can be violated in intermediate rounds. For some user-defined trade-off parameter \u03b2in (0, 1), the proposed algorithm achieves cumulative regret bounds of O(T^max\u03b2,1_\u03b2) and O(T^1_\u03b2/2), respectively for the loss and the constraint violations. Our results hold for convex losses, can handle arbitrary convex constraints and rely on a single computationally efficient algorithm. Our contributions improve over the best known cumulative regret bounds of Mahdavi et al. (2012), which are respectively O(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and O(T^2/3) when the domain is further restricted to be a polyhedral set. We supplement the analysis with experiments validating the performance of our algorithm in practice. ",
    "code_link": ""
  },
  "icml2015_main_activelylearninghemimetricswithapplicationstoelicitinguserpreferences": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Actively Learning Hemimetrics with Applications to Eliciting User Preferences",
    "authors": [
      "Adish Singla",
      "Sebastian Tschiatschek",
      "Andreas Krause"
    ],
    "page_url": "https://proceedings.mlr.press/v48/singla16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/singla16.pdf",
    "published": "2015-06",
    "summary": " Motivated by an application of eliciting users\u2019 preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of n items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires \u0398(n^2) queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into K tight clusters, the sample complexity of our algorithm reduces to O(n K). Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis. ",
    "code_link": ""
  },
  "icml2015_main_learningsimplealgorithmsfromexamples": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Simple Algorithms from Examples",
    "authors": [
      "Wojciech Zaremba",
      "Tomas Mikolov",
      "Armand Joulin",
      "Rob Fergus"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zaremba16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zaremba16.pdf",
    "published": "2015-06",
    "summary": " We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using Q-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by Q-learning. ",
    "code_link": ""
  },
  "icml2015_main_learningphysicalintuitionofblocktowersbyexample": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Physical Intuition of Block Towers by Example",
    "authors": [
      "Adam Lerer",
      "Sam Gross",
      "Rob Fergus"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lerer16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lerer16.pdf",
    "published": "2015-06",
    "summary": " Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the trajectories of the blocks. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects. ",
    "code_link": "https://github.com/facebook/UETorch"
  },
  "icml2015_main_structurelearningofpartitionedmarkovnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Structure Learning of Partitioned Markov Networks",
    "authors": [
      "Song Liu",
      "Taiji Suzuki",
      "Masashi Sugiyama",
      "Kenji Fukumizu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liuc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liuc16.pdf",
    "published": "2015-06",
    "summary": " We learn the structure of a Markov Network between two groups of random variables from joint observations. Since modelling and learning the full MN structure may be hard, learning the links between two groups directly may be a preferable option. We introduce a novel concept called the \\emphpartitioned ratio whose factorization directly associates with the Markovian properties of random variables across two groups. A simple one-shot convex optimization procedure is proposed for learning the \\emphsparse factorizations of the partitioned ratio and it is theoretically guaranteed to recover the correct inter-group structure under mild conditions. The performance of the proposed method is experimentally compared with the state of the art MN structure learning methods using ROC curves. Real applications on analyzing bipartisanship in US congress and pairwise DNA/time-series alignments are also reported. ",
    "code_link": ""
  },
  "icml2015_main_trackingslowlymovingclairvoyantoptimaldynamicregretofonlinelearningwithtrueandnoisygradient": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient",
    "authors": [
      "Tianbao Yang",
      "Lijun Zhang",
      "Rong Jin",
      "Jinfeng Yi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yangb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yangb16.pdf",
    "published": "2015-06",
    "summary": " This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are \\it optimal in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant\u2019s minimizers, to which we refer as path variation. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches that is achieved with full information. ",
    "code_link": ""
  },
  "icml2015_main_beyondccamomentmatchingformulti-viewmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Beyond CCA: Moment Matching for Multi-View Models",
    "authors": [
      "Anastasia Podosinnikova",
      "Francis Bach",
      "Simon Lacoste-Julien"
    ],
    "page_url": "https://proceedings.mlr.press/v48/podosinnikova16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/podosinnikova16.pdf",
    "published": "2015-06",
    "summary": " We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets. ",
    "code_link": "https://github.com/anastasia-podosinnikova/cca"
  },
  "icml2015_main_fastmethodsforestimatingthenumericalrankoflargematrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast methods for estimating the Numerical rank of large matrices",
    "authors": [
      "Shashanka Ubaru",
      "Yousef Saad"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ubaru16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ubaru16.pdf",
    "published": "2015-06",
    "summary": " We present two computationally inexpensive techniques for estimating the numerical rank of a matrix, combining powerful tools from computational linear algebra. These techniques exploit three key ingredients. The first is to approximate the projector on the non-null invariant subspace of the matrix by using a polynomial filter. Two types of filters are discussed, one based on Hermite interpolation and the other based on Chebyshev expansions. The second ingredient employs stochastic trace estimators to compute the rank of this wanted eigen-projector, which yields the desired rank of the matrix. In order to obtain a good filter, it is necessary to detect a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that correspond to the non-null invariant subspace. The third ingredient of the proposed approaches exploits the idea of spectral density, popular in physics, and the Lanczos spectroscopic method to locate this gap. ",
    "code_link": ""
  },
  "icml2015_main_unsuperviseddeepembeddingforclusteringanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Unsupervised Deep Embedding for Clustering Analysis",
    "authors": [
      "Junyuan Xie",
      "Ross Girshick",
      "Ali Farhadi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xieb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xieb16.pdf",
    "published": "2015-06",
    "summary": " Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods. ",
    "code_link": ""
  },
  "icml2015_main_efficientprivateempiricalriskminimizationforhigh-dimensionallearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Private Empirical Risk Minimization for High-dimensional Learning",
    "authors": [
      "Shiva Prasad Kasiviswanathan",
      "Hongxia Jin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kasiviswanathan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kasiviswanathan16.pdf",
    "published": "2015-06",
    "summary": " Dimensionality reduction is a popular approach for dealing with high dimensional data that leads to substantial computational savings. Random projections are a simple and effective method for universal dimensionality reduction with rigorous theoretical guarantees. In this paper, we theoretically study the problem of differentially private empirical risk minimization in the projected subspace (compressed domain). We ask: is it possible to design differentially private algorithms with small excess risk given access to only projected data? In this paper, we answer this question in affirmative, by showing that for the class of generalized linear functions, given only the projected data and the projection matrix, we can obtain excess risk bounds of $O(w(Theta)^2/3/n^1/3) under eps-differential privacy, and O((w(Theta)/n)^1/2)$ under (eps,delta)-differential privacy, where n is the sample size and w(Theta) is the Gaussian width of the parameter space that we optimize over. A simple consequence of these results is that, for a large class of ERM problems, in the traditional setting (i.e., with access to the original data), under eps-differential privacy, we improve the worst-case risk bounds of Bassily et al. (FOCS 2014). ",
    "code_link": ""
  },
  "icml2015_main_parameterestimationforgeneralizedthurstonechoicemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Parameter Estimation for Generalized Thurstone Choice Models",
    "authors": [
      "Milan Vojnovic",
      "Seyoung Yun"
    ],
    "page_url": "https://proceedings.mlr.press/v48/vojnovic16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/vojnovic16.pdf",
    "published": "2015-06",
    "summary": " We consider the maximum likelihood parameter estimation problem for a generalized Thurstone choice model, where choices are from comparison sets of two or more items. We provide tight characterizations of the mean square error, as well as necessary and sufficient conditions for correct classification when each item belongs to one of two classes. These results provide insights into how the estimation accuracy depends on the choice of a generalized Thurstone choice model and the structure of comparison sets. We find that for a priori unbiased structures of comparisons, e.g., when comparison sets are drawn independently and uniformly at random, the number of observations needed to achieve a prescribed estimation accuracy depends on the choice of a generalized Thurstone choice model. For a broad set of generalized Thurstone choice models, which includes all popular instances used in practice, the estimation error is shown to be largely insensitive to the cardinality of comparison sets. On the other hand, we found that there exist generalized Thurstone choice models for which the estimation error decreases much faster with the cardinality of comparison sets. ",
    "code_link": ""
  },
  "icml2015_main_large-marginsoftmaxlossforconvolutionalneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Large-Margin Softmax Loss for Convolutional Neural Networks",
    "authors": [
      "Weiyang Liu",
      "Yandong Wen",
      "Zhiding Yu",
      "Meng Yang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liud16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liud16.pdf",
    "published": "2015-06",
    "summary": " Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks. ",
    "code_link": ""
  },
  "icml2015_main_arandommatrixapproachtoecho-stateneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Random Matrix Approach to Echo-State Neural Networks",
    "authors": [
      "Romain Couillet",
      "Gilles Wainrib",
      "Hafiz Tiomoko Ali",
      "Harry Sevi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/couillet16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/couillet16.pdf",
    "published": "2015-06",
    "summary": " Recurrent neural networks, especially in their linear version, have provided many qualitative insights on their performance under different configurations. This article provides, through a novel random matrix framework, the quantitative counterpart of these performance results, specifically in the case of echo-state networks. Beyond mere insights, our approach conveys a deeper understanding on the core mechanism under play for both training and testing. ",
    "code_link": ""
  },
  "icml2015_main_supervisedandsemi-supervisedtextcategorizationusinglstmforregionembeddings": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings",
    "authors": [
      "Rie Johnson",
      "Tong Zhang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/johnson16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/johnson16.pdf",
    "published": "2015-06",
    "summary": " One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of \u2018text region embedding + pooling\u2019. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets. ",
    "code_link": ""
  },
  "icml2015_main_optimalityofbeliefpropagationforcrowdsourcedclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Optimality of Belief Propagation for Crowdsourced Classification",
    "authors": [
      "Jungseul Ok",
      "Sewoong Oh",
      "Jinwoo Shin",
      "Yung Yi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ok16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ok16.pdf",
    "published": "2015-06",
    "summary": " Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid (or even non-paid) workers. We study the problem of recovering the true labels from noisy crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. We close this gap under a simple but canonical scenario where each worker is assigned at most two tasks. In particular, we introduce a tighter lower bound on the fundamental limit and prove that Belief Propagation (BP) exactly matches this lower bound. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly la- bel a larger fraction of the tasks. In the general setting, when more than two tasks are assigned to each worker, we establish the dominance result on BP that it outperforms other existing algorithms with known provable guarantees. Experimental results suggest that BP is close to optimal for all regimes considered, while existing state-of-the-art algorithms exhibit suboptimal performances. ",
    "code_link": ""
  },
  "icml2015_main_stabilityofcontrollersforgaussianprocessforwardmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stability of Controllers for Gaussian Process Forward Models",
    "authors": [
      "Julia Vinogradska",
      "Bastian Bischoff",
      "Duy Nguyen-Tuong",
      "Anne Romer",
      "Henner Schmidt",
      "Jan Peters"
    ],
    "page_url": "https://proceedings.mlr.press/v48/vinogradska16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/vinogradska16.pdf",
    "published": "2015-06",
    "summary": " Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance guarantees which prevents its application in many real-world scenarios. As a step in this direction, we provide a stability analysis tool for controllers acting on dynamics represented by Gaussian processes (GPs). We consider arbitrary Markovian control policies and system dynamics given as (i) the mean of a GP, and (ii) the full GP distribution. For the first case, our tool finds a state space region, where the closed-loop system is provably stable. In the second case, it is well known that infinite horizon stability guarantees cannot exist. Instead, our tool analyzes finite time stability. Empirical evaluations on simulated benchmark problems support our theoretical results. ",
    "code_link": ""
  },
  "icml2015_main_learningprivatelyfrommultipartydata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning privately from multiparty data",
    "authors": [
      "Jihun Hamm",
      "Yingjun Cao",
      "Mikhail Belkin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hamm16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hamm16.pdf",
    "published": "2015-06",
    "summary": " Learning a classifier from private data distributed across multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party\u2019s private data? We propose to transfer the \u201cknowledge\u201d of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by O(\u03b5^-2 M^-2). This allows strong privacy without performance loss when the number of participating parties M is large, such as in crowdsensing applications. We demonstrate the performance of our framework with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection. ",
    "code_link": ""
  },
  "icml2015_main_networkmorphism": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Network Morphism",
    "authors": [
      "Tao Wei",
      "Changhu Wang",
      "Yong Rui",
      "Chang Wen Chen"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wei16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wei16.pdf",
    "published": "2015-06",
    "summary": " We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme. ",
    "code_link": ""
  },
  "icml2015_main_akronecker-factoredapproximatefishermatrixforconvolutionlayers": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Kronecker-factored approximate Fisher matrix for convolution layers",
    "authors": [
      "Roger Grosse",
      "James Martens"
    ],
    "page_url": "https://proceedings.mlr.press/v48/grosse16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/grosse16.pdf",
    "published": "2015-06",
    "summary": " Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting. ",
    "code_link": "https://github.com/TorontoDeepLearning/convnet"
  },
  "icml2015_main_experimentaldesignonabudgetforsparselinearmodelsandapplications": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Experimental Design on a Budget for Sparse Linear Models and Applications",
    "authors": [
      "Sathya Narayanan Ravi",
      "Vamsi Ithapu",
      "Sterling Johnson",
      "Vikas Singh"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ravi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ravi16.pdf",
    "published": "2015-06",
    "summary": " Budget constrained optimal design of experiments is a classical problem in statistics. Although the optimal design literature is very mature, few efficient strategies are available when these design problems appear in the context of sparse linear models commonly encountered in high dimensional machine learning and statistics. In this work, we study experimental design for the setting where the underlying regression model is characterized by a \\ell_1-regularized linear function. We propose two novel strategies: the first is motivated geometrically whereas the second is algebraic in nature. We obtain tractable algorithms for this problem and also hold for a more general class of sparse linear models. We perform an extensive set of experiments, on benchmarks and a large multi-site neuroscience study, showing that the proposed models are effective in practice. The latter experiment suggests that these ideas may play a small role in informing enrollment strategies for similar scientific studies in the short-to-medium term future. ",
    "code_link": "https://github.com/sraviuwmadison/Exp"
  },
  "icml2015_main_mindingthegapsforblockfrank-wolfeoptimizationofstructuredsvms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs",
    "authors": [
      "Anton Osokin",
      "Jean-Baptiste Alayrac",
      "Isabella Lukasewitz",
      "Puneet Dokania",
      "Simon Lacoste-Julien"
    ],
    "page_url": "https://proceedings.mlr.press/v48/osokin16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/osokin16.pdf",
    "published": "2015-06",
    "summary": " In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an *adaptive* criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gap-based sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets. ",
    "code_link": ""
  },
  "icml2015_main_exactexponentinoptimalratesforcrowdsourcing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Exact Exponent in Optimal Rates for Crowdsourcing",
    "authors": [
      "Chao Gao",
      "Yu Lu",
      "Dengyong Zhou"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gaoa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gaoa16.pdf",
    "published": "2015-06",
    "summary": " Crowdsourcing has become a popular tool for labeling large datasets. This paper studies the optimal error rate for aggregating crowdsourced labels provided by a collection of amateur workers. Under the Dawid-Skene probabilistic model, we establish matching upper and lower bounds with an exact exponent mI(\\pi), where m is the number of workers and I(\\pi) is the average Chernoff information that characterizes the workers\u2019 collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement m \\ge \\frac1I(\\pi)\\log\\frac1\u03b5 in order to achieve an \u03b5misclassification error. In addition, our results imply optimality of various forms of EM algorithms given accurate initializers of the model parameters. ",
    "code_link": ""
  },
  "icml2015_main_augmentingsupervisedneuralnetworkswithunsupervisedobjectivesforlarge-scaleimageclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification",
    "authors": [
      "Yuting Zhang",
      "Kibok Lee",
      "Honglak Lee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhangc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhangc16.pdf",
    "published": "2015-06",
    "summary": " Unsupervised learning and supervised learning are key research topics in deep learning. However, as high-capacity supervised neural networks trained with a large amount of labels have achieved remarkable success in many computer vision tasks, the availability of large-scale labeled images reduced the significance of unsupervised learning. Inspired by the recent trend toward revisiting the importance of unsupervised learning, we investigate joint supervised and unsupervised learning in a large-scale setting by augmenting existing neural networks with decoding pathways for reconstruction. First, we demonstrate that the intermediate activations of pretrained large-scale classification networks preserve almost all the information of input images except a portion of local spatial details. Then, by end-to-end training of the entire augmented architecture with the reconstructive objective, we show improvement of the network performance for supervised tasks. We evaluate several variants of autoencoders, including the recently proposed \u201cwhat-where\" autoencoder that uses the encoder pooling switches, to study the importance of the architecture design. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012 protocol as a strong baseline for image classification, our methods improve the validation-set accuracy by a noticeable margin. ",
    "code_link": ""
  },
  "icml2015_main_onlinelow-ranksubspaceclusteringbybasisdictionarypursuit": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Online Low-Rank Subspace Clustering by Basis Dictionary Pursuit",
    "authors": [
      "Jie Shen",
      "Ping Li",
      "Huan Xu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shen16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shen16.pdf",
    "published": "2015-06",
    "summary": " Low-Rank Representation\u00a0(LRR) has been a significant method for segmenting data that are generated from a union of subspaces. It is also known that solving LRR is challenging in terms of time complexity and memory footprint, in that the size of the nuclear norm regularized matrix is n-by-n (where n is the number of samples). In this paper, we thereby develop a novel online implementation of LRR that reduces the memory cost from O(n^2) to O(pd), with p being the ambient dimension and d being some estimated rank (d < p < n). We also establish the theoretical guarantee that the sequence of solutions produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Extensive experiments on synthetic and realistic datasets further substantiate that our algorithm is fast, robust and memory efficient. ",
    "code_link": ""
  },
  "icml2015_main_aself-correctingvariable-metricalgorithmforstochasticoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization",
    "authors": [
      "Frank Curtis"
    ],
    "page_url": "https://proceedings.mlr.press/v48/curtis16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/curtis16.pdf",
    "published": "2015-06",
    "summary": " An algorithm for stochastic (convex or nonconvex) optimization is presented. The algorithm is variable-metric in the sense that, in each iteration, the step is computed through the product of a symmetric positive definite scaling matrix and a stochastic (mini-batch) gradient of the objective function, where the sequence of scaling matrices is updated dynamically by the algorithm. A key feature of the algorithm is that it does not overly restrict the manner in which the scaling matrices are updated. Rather, the algorithm exploits fundamental self-correcting properties of BFGS-type updating\u2014properties that have been over-looked in other attempts to devise quasi-Newton methods for stochastic optimization. Numerical experiments illustrate that the method and a limited memory variant of it are stable and outperform (mini-batch) stochastic gradient and other quasi-Newton methods when employed to solve a few machine learning problems. ",
    "code_link": ""
  },
  "icml2015_main_stochasticquasi-newtonlangevinmontecarlo": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Quasi-Newton Langevin Monte Carlo",
    "authors": [
      "Umut Simsekli",
      "Roland Badeau",
      "Taylan Cemgil",
      "Ga\u00ebl Richard"
    ],
    "page_url": "https://proceedings.mlr.press/v48/simsekli16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/simsekli16.pdf",
    "published": "2015-06",
    "summary": " Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have been proposed for scaling up Monte Carlo computations to large data problems. Whilst these approaches have proven useful in many applications, vanilla SG-MCMC might suffer from poor mixing rates when random variables exhibit strong couplings under the target densities or big scale differences. In this study, we propose a novel SG-MCMC method that takes the local geometry into account by using ideas from Quasi-Newton optimization methods. These second order methods directly approximate the inverse Hessian by using a limited history of samples and their gradients. Our method uses dense approximations of the inverse Hessian while keeping the time and memory complexities linear with the dimension of the problem. We provide a formal theoretical analysis where we show that the proposed method is asymptotically unbiased and consistent with the posterior expectations. We illustrate the effectiveness of the approach on both synthetic and real datasets. Our experiments on two challenging applications show that our method achieves fast convergence rates similar to Riemannian approaches while at the same time having low computational requirements similar to diagonal preconditioning approaches. ",
    "code_link": ""
  },
  "icml2015_main_doublyrobustoff-policyvalueevaluationforreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning",
    "authors": [
      "Nan Jiang",
      "Lihong Li"
    ],
    "page_url": "https://proceedings.mlr.press/v48/jiang16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/jiang16.pdf",
    "published": "2015-06",
    "summary": " We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL to real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator\u2019s accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the inherent hardness of the problem, and show that our estimator can match the lower bound in certain scenarios. ",
    "code_link": ""
  },
  "icml2015_main_fastrateanalysisofsomestochasticoptimizationalgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast Rate Analysis of Some Stochastic Optimization Algorithms",
    "authors": [
      "Chao Qu",
      "Huan Xu",
      "Chong Ong"
    ],
    "page_url": "https://proceedings.mlr.press/v48/qua16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/qua16.pdf",
    "published": "2015-06",
    "summary": " In this paper, we revisit three fundamental and popular stochastic optimization algorithms (namely, Online Proximal Gradient, Regularized Dual Averaging method and ADMM with online proximal gradient) and analyze their convergence speed under conditions weaker than those in literature. In particular, previous works showed that these algorithms converge at a rate of O (\\ln T/T) when the loss function is strongly convex, and O (1 /\\sqrtT) in the weakly convex case. In contrast, we relax the strong convexity assumption of the loss function, and show that the algorithms converge at a rate O (\\ln T/T) if the \\em expectation of the loss function is \\em locally strongly convex. This is a much weaker assumption and is satisfied by many practical formulations including Lasso and Logistic Regression. Our analysis thus extends the applicability of these three methods, as well as provides a general recipe for improving analysis of convergence rate for stochastic and online optimization algorithms. ",
    "code_link": ""
  },
  "icml2015_main_fastk-nearestneighboursearchviadynamiccontinuousindexing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing",
    "authors": [
      "Ke Li",
      "Jitendra Malik"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lic16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lic16.pdf",
    "published": "2015-06",
    "summary": " Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency. ",
    "code_link": ""
  },
  "icml2015_main_smoothimitationlearningforonlinesequenceprediction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Smooth Imitation Learning for Online Sequence Prediction",
    "authors": [
      "Hoang Le",
      "Andrew Kang",
      "Yisong Yue",
      "Peter Carr"
    ],
    "page_url": "https://proceedings.mlr.press/v48/le16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/le16.pdf",
    "published": "2015-06",
    "summary": " We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches. ",
    "code_link": "https://github.com/hoangminhle/SIMILE"
  },
  "icml2015_main_communityrecoveryingraphswithlocality": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Community Recovery in Graphs with Locality",
    "authors": [
      "Yuxin Chen",
      "Govinda Kamath",
      "Changho Suh",
      "David Tse"
    ],
    "page_url": "https://proceedings.mlr.press/v48/chena16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/chena16.pdf",
    "published": "2015-06",
    "summary": " Motivated by applications in domains such as social networks and computational biology, we study the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all node pairs, as in most existing models. We present two algorithms that run nearly linearly in the number of measurements and which achieve the information limits for exact recovery. ",
    "code_link": ""
  },
  "icml2015_main_variancereductionforfasternon-convexoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variance Reduction for Faster Non-Convex Optimization",
    "authors": [
      "Zeyuan Allen-Zhu",
      "Elad Hazan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/allen-zhua16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/allen-zhua16.pdf",
    "published": "2015-06",
    "summary": " We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in O(1/\\varepsilon) iterations for smooth objectives, and stochastic gradient descent that converges in O(1/\\varepsilon^2) iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an O(1/\\varepsilon) rate, and is faster than full gradient descent by \u03a9(n^1/3). We demonstrate the effectiveness of our methods on empirical risk minimizations with non-convex loss functions and training neural nets. ",
    "code_link": ""
  },
  "icml2015_main_lossfactorization,weaklysupervisedlearningandlabelnoiserobustness": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Loss factorization, weakly supervised learning and label noise robustness",
    "authors": [
      "Giorgio Patrini",
      "Frank Nielsen",
      "Richard Nock",
      "Marcello Carioni"
    ],
    "page_url": "https://proceedings.mlr.press/v48/patrini16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/patrini16.pdf",
    "published": "2015-06",
    "summary": " We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator \u2014 the focal quantity of this work \u2014 which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. ",
    "code_link": ""
  },
  "icml2015_main_analysisofdeepneuralnetworkswithextendeddatajacobianmatrix": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Analysis of Deep Neural Networks with Extended Data Jacobian Matrix",
    "authors": [
      "Shengjie Wang",
      "Abdel-rahman Mohamed",
      "Rich Caruana",
      "Jeff Bilmes",
      "Matthai Plilipose",
      "Matthew Richardson",
      "Krzysztof Geras",
      "Gregor Urban",
      "Ozlem Aslan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wanga16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wanga16.pdf",
    "published": "2015-06",
    "summary": " Deep neural networks have achieved great successes on various machine learning tasks, however, there are many open fundamental questions to be answered. In this paper, we tackle the problem of quantifying the quality of learned wights of different networks with possibly different architectures, going beyond considering the final classification error as the only metric. We introduce \\emphExtended Data Jacobian Matrix to help analyze properties of networks of various structures, finding that, the spectrum of the extended data jacobian matrix is a strong discriminating factor for networks of different structures and performance. Based on such observation, we propose a novel regularization method, which manages to improve the network performance comparably to dropout, which in turn verifies the observation. ",
    "code_link": ""
  },
  "icml2015_main_doublydecomposingnonparametrictensorregression": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Doubly Decomposing Nonparametric Tensor Regression",
    "authors": [
      "Masaaki Imaizumi",
      "Kohei Hayashi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/imaizumi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/imaizumi16.pdf",
    "published": "2015-06",
    "summary": " Nonparametric extension of tensor regression is proposed. Nonlinearity in a high-dimensional tensor space is broken into simple local functions by incorporating low-rank tensor decomposition. Compared to naive nonparametric approaches, our formulation considerably improves the convergence rate of estimation while maintaining consistency with the same function class under specific conditions. To estimate local functions, we develop a Bayesian estimator with the Gaussian process prior. Experimental results show its theoretical properties and high performance in terms of predicting a summary statistic of a real complex network. ",
    "code_link": ""
  },
  "icml2015_main_hyperparameteroptimizationwithapproximategradient": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hyperparameter optimization with approximate gradient",
    "authors": [
      "Fabian Pedregosa"
    ],
    "page_url": "https://proceedings.mlr.press/v48/pedregosa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/pedregosa16.pdf",
    "published": "2015-06",
    "summary": " Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods. ",
    "code_link": "https://github.com/fabianp/hoag"
  },
  "icml2015_main_sdcawithoutduality,regularization,andindividualconvexity": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "SDCA without Duality, Regularization, and Individual Convexity",
    "authors": [
      "Shai Shalev-Shwartz"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shalev-shwartza16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shalev-shwartza16.pdf",
    "published": "2015-06",
    "summary": " Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex. ",
    "code_link": ""
  },
  "icml2015_main_heteroscedasticsequencesbeyondgaussianity": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Heteroscedastic Sequences: Beyond Gaussianity",
    "authors": [
      "Oren Anava",
      "Shie Mannor"
    ],
    "page_url": "https://proceedings.mlr.press/v48/anava16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/anava16.pdf",
    "published": "2015-06",
    "summary": " We address the problem of sequential prediction in the heteroscedastic setting, when both the signal and its variance are assumed to depend on explanatory variables. By applying regret minimization techniques, we devise an efficient online learning algorithm for the problem, without assuming that the error terms comply with a specific distribution. We show that our algorithm can be adjusted to provide confidence bounds for its predictions, and provide an application to ARCH models. The theoretic results are corroborated by an empirical study. ",
    "code_link": ""
  },
  "icml2015_main_aneuralautoregressiveapproachtocollaborativefiltering": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Neural Autoregressive Approach to Collaborative Filtering",
    "authors": [
      "Yin Zheng",
      "Bangsheng Tang",
      "Wenkui Ding",
      "Hanning Zhou"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zheng16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zheng16.pdf",
    "published": "2015-06",
    "summary": " This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance. ",
    "code_link": ""
  },
  "icml2015_main_onthequalityoftheinitialbasininoverspecifiedneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Quality of the Initial Basin in Overspecified Neural Networks",
    "authors": [
      "Itay Safran",
      "Ohad Shamir"
    ],
    "page_url": "https://proceedings.mlr.press/v48/safran16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/safran16.pdf",
    "published": "2015-06",
    "summary": " Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case. In this work, we study the \\emphgeometric structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value. A common theme in our results is that such properties are more likely to hold for larger (\u201coverspecified\u201d) networks, which accords with some recent empirical and theoretical observations. ",
    "code_link": ""
  },
  "icml2015_main_primal-dualratesandcertificates": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Primal-Dual Rates and Certificates",
    "authors": [
      "Celestine D\u00fcnner",
      "Simone Forte",
      "Martin Takac",
      "Martin Jaggi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/dunner16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/dunner16.pdf",
    "published": "2015-06",
    "summary": " We propose an algorithm-independent framework to equip existing optimization methods with primal-dual certificates. Such certificates and corresponding rate of convergence guarantees are important for practitioners to diagnose progress, in particular in machine learning applications. We obtain new primal-dual convergence rates, e.g., for the Lasso as well as many L1, Elastic Net, group Lasso and TV-regularized problems. The theory applies to any norm-regularized generalized linear model. Our approach provides efficiently computable duality gaps which are globally defined, without modifying the original problems in the region of interest. ",
    "code_link": ""
  },
  "icml2015_main_minimizingthemaximallosshowandwhy": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Minimizing the Maximal Loss: How and Why",
    "authors": [
      "Shai Shalev-Shwartz",
      "Yonatan Wexler"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shalev-shwartzb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shalev-shwartzb16.pdf",
    "published": "2015-06",
    "summary": " A commonly used learning rule is to approximately minimize the \\emphaverage loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the \\emphmaximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We show, theoretically and empirically, that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers. ",
    "code_link": ""
  },
  "icml2015_main_theinformation-theoreticrequirementsofsubspaceclusteringwithmissingdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Information-Theoretic Requirements of Subspace Clustering with Missing Data",
    "authors": [
      "Daniel Pimentel-Alarcon",
      "Robert Nowak"
    ],
    "page_url": "https://proceedings.mlr.press/v48/pimentel-alarcon16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/pimentel-alarcon16.pdf",
    "published": "2015-06",
    "summary": " Subspace clustering with missing data (SCMD) is a useful tool for analyzing incomplete datasets. Let d be the ambient dimension, and r the dimension of the subspaces. Existing theory shows that Nk = O(r d) columns per subspace are necessary for SCMD, and Nk =O(min d^(log d), d^(r+1) ) are sufficient. We close this gap, showing that Nk =O(r d) is also sufficient. To do this we derive deterministic sampling conditions for SCMD, which give precise information theoretic requirements and determine sampling regimes. These results explain the performance of SCMD algorithms from the literature. Finally, we give a practical algorithm to certify the output of any SCMD method deterministically. ",
    "code_link": ""
  },
  "icml2015_main_onlinelearningwithfeedbackgraphswithoutthegraphs": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Online Learning with Feedback Graphs Without the Graphs",
    "authors": [
      "Alon Cohen",
      "Tamir Hazan",
      "Tomer Koren"
    ],
    "page_url": "https://proceedings.mlr.press/v48/cohena16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/cohena16.pdf",
    "published": "2015-06",
    "summary": " We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is \\emphnever fully revealed to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves \\widetilde\u0398(\\sqrt\u03b1T) regret over T rounds, provided that the independence numbers of the hidden feedback graphs are at most \u03b1. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render the problem unlearnable. ",
    "code_link": ""
  },
  "icml2015_main_paclearningofprobabilisticautomatonbasedonthemethodofmoments": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PAC learning of Probabilistic Automaton based on the Method of Moments",
    "authors": [
      "Hadrien Glaude",
      "Olivier Pietquin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/glaude16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/glaude16.pdf",
    "published": "2015-06",
    "summary": " Probabilitic Finite Automata (PFA) are generative graphical models that define distributions with latent variables over finite sequences of symbols, a.k.a. stochastic languages. Traditionally, unsupervised learning of PFA is performed through algorithms that iteratively improves the likelihood like the Expectation-Maximization (EM) algorithm. Recently, learning algorithms based on the so-called Method of Moments (MoM) have been proposed as a much faster alternative that comes with PAC-style guarantees. However, these algorithms do not ensure the learnt automata to model a proper distribution, limiting their applicability and preventing them to serve as an initialization to iterative algorithms. In this paper, we propose a new MoM-based algorithm with PAC-style guarantees that learns automata defining proper distributions. We assess its performances on synthetic problems from the PAutomaC challenge and real datasets extracted from Wikipedia against previous MoM-based algorithms and EM algorithm. ",
    "code_link": ""
  },
  "icml2015_main_estimatingstructuredvectorautoregressivemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Estimating Structured Vector Autoregressive Models",
    "authors": [
      "Igor Melnyk",
      "Arindam Banerjee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/melnyk16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/melnyk16.pdf",
    "published": "2015-06",
    "summary": " While considerable advances have been made in estimating high-dimensional structured models from independent data using Lasso-type models, limited progress has been made for settings when the samples are dependent. We consider estimating structured VAR (vector auto-regressive model), where the structure can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted Lasso, etc. In VAR setting with correlated noise, although there is strong dependence over time and covariates, we establish bounds on the non-asymptotic estimation error of structured VAR parameters. The estimation error is of the same order as that of the corresponding Lasso-type estimator with independent samples, and the analysis holds for any norm. Our analysis relies on results in generic chaining, sub-exponential martingales, and spectral representation of VAR models. Experimental results on synthetic and real data with a variety of structures are presented, validating theoretical results. ",
    "code_link": ""
  },
  "icml2015_main_mixingratesforthealternatinggibbssampleroverrestrictedboltzmannmachinesandfriends": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Mixing Rates for the Alternating Gibbs Sampler over Restricted Boltzmann Machines and Friends",
    "authors": [
      "Christopher Tosh"
    ],
    "page_url": "https://proceedings.mlr.press/v48/tosh16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/tosh16.pdf",
    "published": "2015-06",
    "summary": " Alternating Gibbs sampling is a modification of classical Gibbs sampling where several variables are simultaneously sampled from their joint conditional distribution. In this work, we investigate the mixing rate of alternating Gibbs sampling with a particular emphasis on Restricted Boltzmann Machines (RBMs) and variants. ",
    "code_link": ""
  },
  "icml2015_main_polynomialnetworksandfactorizationmachinesnewinsightsandefficienttrainingalgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms",
    "authors": [
      "Mathieu Blondel",
      "Masakazu Ishihata",
      "Akinori Fujino",
      "Naonori Ueda"
    ],
    "page_url": "https://proceedings.mlr.press/v48/blondel16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/blondel16.pdf",
    "published": "2015-06",
    "summary": " Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks. ",
    "code_link": ""
  },
  "icml2015_main_anewpac-bayesianperspectiveondomainadaptation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A New PAC-Bayesian Perspective on Domain Adaptation",
    "authors": [
      "Pascal Germain",
      "Amaury Habrard",
      "Fran\u00e7ois Laviolette",
      "Emilie Morvant"
    ],
    "page_url": "https://proceedings.mlr.press/v48/germain16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/germain16.pdf",
    "published": "2015-06",
    "summary": " We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions\u2019 divergence - expressed as a ratio - controls the trade-off between a source error measure and the target voters\u2019 disagreement. Our bound suggests that one has to focus on regions where the source data is informative. From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithm and perform experiments on real data. ",
    "code_link": ""
  },
  "icml2015_main_correlationclusteringandbiclusteringwithlocallyboundederrors": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Correlation Clustering and Biclustering with Locally Bounded Errors",
    "authors": [
      "Gregory Puleo",
      "Olgica Milenkovic"
    ],
    "page_url": "https://proceedings.mlr.press/v48/puleo16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/puleo16.pdf",
    "published": "2015-06",
    "summary": " We consider a generalized version of the correlation clustering problem, defined as follows. Given a complete graph G whose edges are labeled with + or -, we wish to partition the graph into clusters while trying to avoid errors: + edges between clusters or - edges within clusters. Classically, one seeks to minimize the total number of such errors. We introduce a new framework that allows the objective to be a more general function of the number of errors at each vertex (for example, we may wish to minimize the number of errors at the worst vertex) and provide a rounding algorithm which converts \u201cfractional clusterings\u201d into discrete clusterings while causing only a constant-factor blowup in the number of errors at each vertex. This rounding algorithm yields constant-factor approximation algorithms for the discrete problem under a wide variety of objective functions. ",
    "code_link": ""
  },
  "icml2015_main_paclowerboundsandefficientalgorithmsforthemaxk-armedbanditproblem": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PAC Lower Bounds and Efficient Algorithms for The Max K-Armed Bandit Problem",
    "authors": [
      "Yahel David",
      "Nahum Shimkin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/david16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/david16.pdf",
    "published": "2015-06",
    "summary": " We consider the Max K-Armed Bandit problem, where a learning agent is faced with several stochastic arms, each a source of i.i.d. rewards of unknown distribution. At each time step the agent chooses an arm, and observes the reward of the obtained sample. Each sample is considered here as a separate item with the reward designating its value, and the goal is to find an item with the highest possible value. Our basic assumption is a known lower bound on the \\em tail function of the reward distributions. Under the PAC framework, we provide a lower bound on the sample complexity of any (\u03b5,\u03b4)-correct algorithm, and propose an algorithm that attains this bound up to logarithmic factors. We provide an analysis of the robustness of the proposed algorithm to the model assumptions, and further compare its performance to the simple non-adaptive variant, in which the arms are chosen randomly at each stage. ",
    "code_link": ""
  },
  "icml2015_main_acomparativeanalysisandstudyofmultiviewcnnmodelsforjointobjectcategorizationandposeestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimation",
    "authors": [
      "Mohamed Elhoseiny",
      "Tarek El-Gaaly",
      "Amr Bakry",
      "Ahmed Elgammal"
    ],
    "page_url": "https://proceedings.mlr.press/v48/elhoseiny16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/elhoseiny16.pdf",
    "published": "2015-06",
    "summary": " In the Object Recognition task, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose estimation using these approaches has received relatively less attention. In this work, we study how Convolutional Neural Networks (CNN) architectures can be adapted to the task of simultaneous object recognition and pose estimation. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art. ",
    "code_link": ""
  },
  "icml2015_main_bascapplyingbayesianoptimizationtothesearchforglobalminimaonpotentialenergysurfaces": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "BASC: Applying Bayesian Optimization to the Search for Global Minima on Potential Energy Surfaces",
    "authors": [
      "Shane Carr",
      "Roman Garnett",
      "Cynthia Lo"
    ],
    "page_url": "https://proceedings.mlr.press/v48/carr16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/carr16.pdf",
    "published": "2015-06",
    "summary": " We present a novel application of Bayesian optimization to the field of surface science: rapidly and accurately searching for the global minimum on potential energy surfaces. Controlling molecule-surface interactions is key for applications ranging from environmental catalysis to gas sensing. We present pragmatic techniques, including exploration/exploitation scheduling and a custom covariance kernel that encodes the properties of our objective function. Our method, the Bayesian Active Site Calculator (BASC), outperforms differential evolution and constrained minima hopping \u2013 two state-of-the-art approaches \u2013 in trial examples of carbon monoxide adsorption on a hematite substrate, both with and without a defect. ",
    "code_link": "https://gitlab.com/caml/basc"
  },
  "icml2015_main_ontheiterationcomplexityofobliviousfirst-orderoptimizationalgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Iteration Complexity of Oblivious First-Order Optimization Algorithms",
    "authors": [
      "Yossi Arjevani",
      "Ohad Shamir"
    ],
    "page_url": "https://proceedings.mlr.press/v48/arjevani16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/arjevani16.pdf",
    "published": "2015-06",
    "summary": " We consider a broad class of first-order optimization algorithms which are \\emphoblivious, in the sense that their step sizes are scheduled regardless of the function under consideration, except for limited side-information such as smoothness or strong convexity parameters. With the knowledge of these two parameters, we show that any such algorithm attains an iteration complexity lower bound of \u03a9(\\sqrtL/\u03b5) for L-smooth convex functions, and \\tilde\u03a9(\\sqrtL/\u03bc\\ln(1/\u03b5)) for L-smooth \u03bc-strongly convex functions. These lower bounds are stronger than those in the traditional oracle model, as they hold independently of the dimension. To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in Arjevani et al. (2015). We further show that without knowing the strong convexity parameter, it is impossible to attain an iteration complexity better than \\tilde\u03a9\\sqrt(L/\u03bc)\\ln(1/\u03b5). This result is then used to formalize an observation regarding L-smooth convex functions, namely, that the iteration complexity of algorithms employing time-invariant step sizes must be at least \u03a9(L/\u03b5). ",
    "code_link": ""
  },
  "icml2015_main_stochasticvariancereducedoptimizationfornonconvexsparselearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning",
    "authors": [
      "Xingguo Li",
      "Tuo Zhao",
      "Raman Arora",
      "Han Liu",
      "Jarvis Haupt"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lid16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lid16.pdf",
    "published": "2015-06",
    "summary": " We propose a stochastic variance reduced optimization algorithm for solving a class of large-scale nonconvex optimization problems with cardinality constraints, and provide sufficient conditions under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. Numerical experiments demonstrate the efficiency of our method in terms of both parameter estimation and computational performance. ",
    "code_link": ""
  },
  "icml2015_main_analysisofvariationalbayesianfactorizationsforsparseandlow-rankestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Analysis of Variational Bayesian Factorizations for Sparse and Low-Rank Estimation",
    "authors": [
      "David Wipf"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wipf16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wipf16.pdf",
    "published": "2015-06",
    "summary": " Variational Bayesian (VB) approximations anchor a wide variety of probabilistic models, where tractable posterior inference is almost never possible. Typically based on the so-called VB mean-field approximation to the Kullback-Leibler divergence, a posterior distribution is sought that factorizes across groups of latent variables such that, with the distributions of all but one group of variables held fixed, an optimal closed-form distribution can be obtained for the remaining group, with differing algorithms distinguished by how different variables are grouped and ultimately factored. This basic strategy is particularly attractive when estimating structured low-dimensional models of high-dimensional data, exemplified by the search for minimal rank and/or sparse approximations to observed data. To this end, VB models are frequently deployed across applications including multi-task learning, robust PCA, subspace clustering, matrix completion, affine rank minimization, source localization, compressive sensing, and assorted combinations thereof. Perhaps surprisingly however, there exists almost no attendant theoretical explanation for how various VB factorizations operate, and in which situations one may be preferable to another. We address this relative void by comparing arguably two of the most popular factorizations, one built upon Gaussian scale mixture priors, the other bilinear Gaussian priors, both of which can favor minimal rank or sparsity depending on the context. More specifically, by reexpressing the respective VB objective functions, we weigh multiple factors related to local minima avoidance, feature transformation invariance and correlation, and computational complexity to arrive at insightful conclusions useful in explaining performance and deciding which VB flavor is advantageous. We also envision that the principles explored here are quite relevant to other structured inverse problems where VB serves as a viable solution. ",
    "code_link": ""
  },
  "icml2015_main_fastk-meanswithaccuratebounds": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast k-means with accurate bounds",
    "authors": [
      "James Newling",
      "Francois Fleuret"
    ],
    "page_url": "https://proceedings.mlr.press/v48/newling16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/newling16.pdf",
    "published": "2015-06",
    "summary": " We propose a novel accelerated exact k-means algorithm, which outperforms the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, obtaining speedups in 36 of 44 experiments, of up to 1.8 times. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments. ",
    "code_link": "https://github.com/idiap/eakmeans"
  },
  "icml2015_main_booleanmatrixfactorizationandnoisycompletionviamessagepassing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Boolean Matrix Factorization and Noisy Completion via Message Passing",
    "authors": [
      "Siamak Ravanbakhsh",
      "Barnabas Poczos",
      "Russell Greiner"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ravanbakhsha16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ravanbakhsha16.pdf",
    "published": "2015-06",
    "summary": " Boolean matrix factorization and Boolean matrix completion from noisy observations are desirable unsupervised data-analysis methods due to their interpretability, but hard to perform due to their NP-hardness. We treat these problems as maximum a posteriori inference problems in a graphical model and present a message passing approach that scales linearly with the number of observations and factors. Our empirical study demonstrates that message passing is able to recover low-rank Boolean matrices, in the boundaries of theoretically possible recovery and compares favorably with state-of-the-art in real-world applications, such collaborative filtering with large-scale Boolean data. ",
    "code_link": "https://github.com/mravanba/BooleanFactorization"
  },
  "icml2015_main_convolutionalrectifiernetworksasgeneralizedtensordecompositions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions",
    "authors": [
      "Nadav Cohen",
      "Amnon Shashua"
    ],
    "page_url": "https://proceedings.mlr.press/v48/cohenb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/cohenb16.pdf",
    "published": "2015-06",
    "summary": " Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On the other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits possess the property of \"complete depth efficiency\", meaning that besides a negligible set, all functions realizable by a deep network of polynomial size, require exponential size in order to be realized (or approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners. ",
    "code_link": ""
  },
  "icml2015_main_low-ranksolutionsoflinearmatrixequationsviaprocrustesflow": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Low-rank Solutions of Linear Matrix Equations via Procrustes Flow",
    "authors": [
      "Stephen Tu",
      "Ross Boczar",
      "Max Simchowitz",
      "Mahdi Soltanolkotabi",
      "Ben Recht"
    ],
    "page_url": "https://proceedings.mlr.press/v48/tu16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/tu16.pdf",
    "published": "2015-06",
    "summary": " In this paper we study the problem of recovering a low-rank matrix from linear measurements. Our algorithm, which we call Procrustes Flow, starts from an initial estimate obtained by a thresholding scheme followed by gradient descent on a non-convex objective. We show that as long as the measurements obey a standard restricted isometry property, our algorithm converges to the unknown matrix at a geometric rate. In the case of Gaussian measurements, such convergence occurs for a n1 \\times n2 matrix of rank r when the number of measurements exceeds a constant times (n1 + n2)r. ",
    "code_link": ""
  },
  "icml2015_main_anytimeexplorationformulti-armedbanditsusingconfidenceinformation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Anytime Exploration for Multi-armed Bandits using Confidence Information",
    "authors": [
      "Kwang-Sung Jun",
      "Robert Nowak"
    ],
    "page_url": "https://proceedings.mlr.press/v48/jun16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/jun16.pdf",
    "published": "2015-06",
    "summary": " We introduce anytime Explore-m, a pure exploration problem for multi-armed bandits (MAB) that requires making a prediction of the top-m arms at every time step. Anytime Explore-m is more practical than fixed budget or fixed confidence formulations of the top-m problem, since many applications involve a finite, but unpredictable, budget. However, the development and analysis of anytime algorithms present many challenges. We propose AT-LUCB (AnyTime Lower and Upper Confidence Bound), the first nontrivial algorithm that provably solves anytime Explore-m. Our analysis shows that the sample complexity of AT-LUCB is competitive to anytime variants of existing algorithms. Moreover, our empirical evaluation on AT-LUCB shows that AT-LUCB performs as well as or better than state-of-the-art baseline methods for anytime Explore-m. ",
    "code_link": ""
  },
  "icml2015_main_structuredpredictionenergynetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Structured Prediction Energy Networks",
    "authors": [
      "David Belanger",
      "Andrew McCallum"
    ],
    "page_url": "https://proceedings.mlr.press/v48/belanger16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/belanger16.pdf",
    "published": "2015-06",
    "summary": " We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of structured outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction. ",
    "code_link": ""
  },
  "icml2015_main_l1-regularizedneuralnetworksareimproperlylearnableinpolynomialtime": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "L1-regularized Neural Networks are Improperly Learnable in Polynomial Time",
    "authors": [
      "Yuchen Zhang",
      "Jason D. Lee",
      "Michael I. Jordan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhangd16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhangd16.pdf",
    "published": "2015-06",
    "summary": " We study the improper learning of multi-layer neural networks. Suppose that the neural network to be learned has k hidden layers and that the \\ell_1-norm of the incoming weights of any neuron is bounded by L. We present a kernel-based method, such that with probability at least 1 - \u03b4, it learns a predictor whose generalization error is at most \u03b5worse than that of the neural network. The sample complexity and the time complexity of the presented method are polynomial in the input dimension and in (1/\u03b5,\\log(1/\u03b4),F(k,L)), where F(k,L) is a function depending on (k,L) and on the activation function, independent of the number of neurons. The algorithm applies to both sigmoid-like activation functions and ReLU-like activation functions. It implies that any sufficiently sparse neural network is learnable in polynomial time. ",
    "code_link": ""
  },
  "icml2015_main_compressivespectralclustering": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Compressive Spectral Clustering",
    "authors": [
      "Nicolas Tremblay",
      "Gilles Puy",
      "Remi Gribonval",
      "Pierre Vandergheynst"
    ],
    "page_url": "https://proceedings.mlr.press/v48/tremblay16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/tremblay16.pdf",
    "published": "2015-06",
    "summary": " Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data. ",
    "code_link": ""
  },
  "icml2015_main_low-ranktensorcompletionariemannianmanifoldpreconditioningapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Low-rank tensor completion: a Riemannian manifold preconditioning approach",
    "authors": [
      "Hiroyuki Kasai",
      "Bamdev Mishra"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kasai16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kasai16.pdf",
    "published": "2015-06",
    "summary": " We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms in batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets. ",
    "code_link": ""
  },
  "icml2015_main_provablenon-convexphaseretrievalwithoutliersmediantruncatedwirtingerflow": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Provable Non-convex Phase Retrieval with Outliers: Median TruncatedWirtinger Flow",
    "authors": [
      "Huishuai Zhang",
      "Yuejie Chi",
      "Yingbin Liang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhange16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhange16.pdf",
    "published": "2015-06",
    "summary": " Solving systems of quadratic equations is a central problem in machine learning and signal processing. One important example is phase retrieval, which aims to recover a signal from only magnitudes of its linear measurements. This paper focuses on the situation when the measurements are corrupted by arbitrary outliers, for which the recently developed non-convex gradient descent Wirtinger flow (WF) and truncated Wirtinger flow (TWF) algorithms likely fail. We develop a novel median-TWF algorithm that exploits robustness of sample median to resist arbitrary outliers in the initialization and the gradient update in each iteration. We show that such a non-convex algorithm provably recovers the signal from a near-optimal number of measurements composed of i.i.d. Gaussian entries, up to a logarithmic factor, even when a constant portion of the measurements are corrupted by arbitrary outliers. We further show that median-TWF is also robust when measurements are corrupted by both arbitrary outliers and bounded noise. Our analysis of performance guarantee is accomplished by development of non-trivial concentration measures of median-related quantities, which may be of independent interest. We further provide numerical experiments to demonstrate the effectiveness of the approach. ",
    "code_link": ""
  },
  "icml2015_main_estimatingmaximumexpectedvaluethroughgaussianapproximation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Estimating Maximum Expected Value through Gaussian Approximation",
    "authors": [
      "Carlo D\u2019Eramo",
      "Marcello Restelli",
      "Alessandro Nuara"
    ],
    "page_url": "https://proceedings.mlr.press/v48/deramo16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/deramo16.pdf",
    "published": "2015-06",
    "summary": " This paper is about the estimation of the maximum expected value of a set of independent random variables. The performance of several learning algorithms (e.g., Q-learning) is affected by the accuracy of such estimation. Unfortunately, no unbiased estimator exists. The usual approach of taking the maximum of the sample means leads to large overestimates that may significantly harm the performance of the learning algorithm. Recent works have shown that the cross validation estimator\u2014which is negatively biased\u2014outperforms the maximum estimator in many sequential decision-making scenarios. On the other hand, the relative performance of the two estimators is highly problem-dependent. In this paper, we propose a new estimator for the maximum expected value, based on a weighted average of the sample means, where the weights are computed using Gaussian approximations for the distributions of the sample means. We compare the proposed estimator with the other state-of-the-art methods both theoretically, by deriving upper bounds to the bias and the variance of the estimator, and empirically, by testing the performance on different sequential learning problems. ",
    "code_link": ""
  },
  "icml2015_main_representationalsimilaritylearningwithapplicationtobrainnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Representational Similarity Learning with Application to Brain Networks",
    "authors": [
      "Urvashi Oswal",
      "Christopher Cox",
      "Matthew Lambon-Ralph",
      "Timothy Rogers",
      "Robert Nowak"
    ],
    "page_url": "https://proceedings.mlr.press/v48/oswal16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/oswal16.pdf",
    "published": "2015-06",
    "summary": " Representational Similarity Learning (RSL) aims to discover features that are important in representing (human-judged) similarities among objects. RSL can be posed as a sparsity-regularized multi-task regression problem. Standard methods, like group lasso, may not select important features if they are strongly correlated with others. To address this shortcoming we present a new regularizer for multitask regression called Group Ordered Weighted \\ell_1 (GrOWL). Another key contribution of our paper is a novel application to fMRI brain imaging. Representational Similarity Analysis (RSA) is a tool for testing whether localized brain regions encode perceptual similarities. Using GrOWL, we propose a new approach called Network RSA that can discover arbitrarily structured brain networks (possibly widely distributed and non-local) that encode similarity information. We show, in theory and fMRI experiments, how GrOWL deals with strongly correlated covariates. ",
    "code_link": ""
  },
  "icml2015_main_dropoutasabayesianapproximationrepresentingmodeluncertaintyindeeplearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
    "authors": [
      "Yarin Gal",
      "Zoubin Ghahramani"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gal16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gal16.pdf",
    "published": "2015-06",
    "summary": " Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \u2013 extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout\u2019s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout\u2019s uncertainty in deep reinforcement learning. ",
    "code_link": "https://github.com/JasperSnoek/spearmint"
  },
  "icml2015_main_generativeadversarialtexttoimagesynthesis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Generative Adversarial Text to Image Synthesis",
    "authors": [
      "Scott Reed",
      "Zeynep Akata",
      "Xinchen Yan",
      "Lajanugen Logeswaran",
      "Bernt Schiele",
      "Honglak Lee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/reed16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/reed16.pdf",
    "published": "2015-06",
    "summary": " Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions. ",
    "code_link": "https://github.com/soumith/dcgan.torch"
  },
  "icml2015_main_dirichletprocessmixturemodelforcorrectingtechnicalvariationinsingle-cellgeneexpressiondata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dirichlet Process Mixture Model for Correcting Technical Variation in Single-Cell Gene Expression Data",
    "authors": [
      "Sandhya Prabhakaran",
      "Elham Azizi",
      "Ambrose Carr",
      "Dana Pe\u2019er"
    ],
    "page_url": "https://proceedings.mlr.press/v48/prabhakaran16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/prabhakaran16.pdf",
    "published": "2015-06",
    "summary": " We introduce an iterative normalization and clustering method for single-cell gene expression data. The emerging technology of single-cell RNA-seq gives access to gene expression measurements for thousands of cells, allowing discovery and characterization of cell types. However, the data is confounded by technical variation emanating from experimental errors and cell type-specific biases. Current approaches perform a global normalization prior to analyzing biological signals, which does not resolve missing data or variation dependent on latent cell types. Our model is formulated as a hierarchical Bayesian mixture model with cell-specific scalings that aid the iterative normalization and clustering of cells, teasing apart technical variation from biological signals. We demonstrate that this approach is superior to global normalization followed by clustering. We show identifiability and weak convergence guarantees of our method and present a scalable Gibbs inference algorithm. This method improves cluster inference in both synthetic and real single-cell data compared with previous methods, and allows easy interpretation and recovery of the underlying structure and cell types. ",
    "code_link": ""
  },
  "icml2015_main_improvedsvrgfornon-strongly-convexorsum-of-non-convexobjectives": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives",
    "authors": [
      "Zeyuan Allen-Zhu",
      "Yang Yuan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/allen-zhub16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/allen-zhub16.pdf",
    "published": "2015-06",
    "summary": " Many classical algorithms are found until several years later to outlive the confines in which they were conceived, and continue to be relevant in unforeseen settings. In this paper, we show that SVRG is one such method: being originally designed for strongly convex objectives, it is also very robust in non-strongly convex or sum-of-non-convex settings. More precisely, we provide new analysis to improve the state-of-the-art running times in both settings by either applying SVRG or its novel variant. Since non-strongly convex objectives include important examples such as Lasso or logistic regression, and sum-of-non-convex objectives include famous examples such as stochastic PCA and is even believed to be related to training deep neural nets, our results also imply better performances in these applications. ",
    "code_link": ""
  },
  "icml2015_main_sparseparameterrecoveryfromaggregateddata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Sparse Parameter Recovery from Aggregated Data",
    "authors": [
      "Avradeep Bhowmik",
      "Joydeep Ghosh",
      "Oluwasanmi Koyejo"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bhowmik16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bhowmik16.pdf",
    "published": "2015-06",
    "summary": " Data aggregation is becoming an increasingly common technique for sharing sensitive information, and for reducing data size when storage and/or communication costs are high. Aggregate quantities such as group-average are a form of semi-supervision as they do not directly provide information of individual values, but despite their wide-spread use, prior literature on learning individual-level models from aggregated data is extremely limited. This paper investigates the effect of data aggregation on parameter recovery for a sparse linear model, when known results are no longer applicable. In particular, we consider a scenario where the data are collected into groups e.g. aggregated patient records, and first-order empirical moments are available only at the group level. Despite this obfuscation of individual data values, we can show that the true parameter is recoverable with high probability using these aggregates when the collection of true group moments is an incoherent matrix, and the empirical moment estimates have been computed from a sufficiently large number of samples. To the best of our knowledge, ours are the first results on structured parameter recovery using only aggregated data. Experimental results on synthetic data are provided in support of these theoretical claims. We also show that parameter estimation from aggregated data approaches the accuracy of parameter estimation obtainable from non-aggregated or \u201cindividual\" samples, when applied to two real world healthcare applications- predictive modeling of CMS Medicare reimbursement claims, and modeling of Texas State healthcare charges. ",
    "code_link": ""
  },
  "icml2015_main_deepstructuredenergybasedmodelsforanomalydetection": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deep Structured Energy Based Models for Anomaly Detection",
    "authors": [
      "Shuangfei Zhai",
      "Yu Cheng",
      "Weining Lu",
      "Zhongfei Zhang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhai16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhai16.pdf",
    "published": "2015-06",
    "summary": " In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We hence propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyvarinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark anomaly detection tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods. ",
    "code_link": ""
  },
  "icml2015_main_evenfasteracceleratedcoordinatedescentusingnon-uniformsampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling",
    "authors": [
      "Zeyuan Allen-Zhu",
      "Zheng Qu",
      "Peter Richtarik",
      "Yang Yuan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/allen-zhuc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/allen-zhuc16.pdf",
    "published": "2015-06",
    "summary": " Accelerated coordinate descent is widely used in optimization due to its cheap per-iteration cost and scalability to large-scale problems. Up to a primal-dual transformation, it is also the same as accelerated stochastic gradient descent that is one of the central methods used in machine learning. In this paper, we improve the best known running time of accelerated coordinate descent by a factor up to \\sqrtn. Our improvement is based on a clean, novel non-uniform sampling that selects each coordinate with a probability proportional to the square root of its smoothness parameter. Our proof technique also deviates from the classical estimation sequence technique used in prior work. Our speed-up applies to important problems such as empirical risk minimization and solving linear systems, both in theory and in practice. ",
    "code_link": ""
  },
  "icml2015_main_unitaryevolutionrecurrentneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Unitary Evolution Recurrent Neural Networks",
    "authors": [
      "Martin Arjovsky",
      "Amar Shah",
      "Yoshua Bengio"
    ],
    "page_url": "https://proceedings.mlr.press/v48/arjovsky16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/arjovsky16.pdf",
    "published": "2015-06",
    "summary": " Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies. ",
    "code_link": ""
  },
  "icml2015_main_markovlatentfeaturemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Markov Latent Feature Models",
    "authors": [
      "Aonan Zhang",
      "John Paisley"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhangf16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhangf16.pdf",
    "published": "2015-06",
    "summary": " We introduce Markov latent feature models (MLFM), a sparse latent feature model that arises naturally from a simple sequential construction. The key idea is to interpret each state of a sequential process as corresponding to a latent feature, and the set of states visited between two null-state visits as picking out features for an observation. We show that, given some natural constraints, we can represent this stochastic process as a mixture of recurrent Markov chains. In this way we can perform correlated latent feature modeling for the sparse coding problem. We demonstrate two cases in which we define finite and infinite latent feature models constructed from first-order Markov chains, and derive their associated scalable inference algorithms. We show empirical results on a genome analysis task and an image denoising task. ",
    "code_link": ""
  },
  "icml2015_main_theknowledgegradientforsequentialdecisionmakingwithstochasticbinaryfeedbacks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Knowledge Gradient for Sequential Decision Making with Stochastic Binary Feedbacks",
    "authors": [
      "Yingfei Wang",
      "Chu Wang",
      "Warren Powell"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wangb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wangb16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of sequentially making decisions that are rewarded by \u201csuccesses\u201d and \u201cfailures\u201d which can be predicted through an unknown relationship that depends on a partially controllable vector of attributes for each instance. The learner takes an active role in selecting samples from the instance pool. The goal is to maximize the probability of success, either after the offline training phase or minimizing regret in online learning. Our problem is motivated by real-world applications where observations are time consuming and/or expensive. With the adaptation of an online Bayesian linear classifier, we develop a knowledge-gradient type policy to guide the experiment by maximizing the expected value of information of labeling each alternative, in order to reduce the number of expensive physical experiments. We provide a finite-time analysis of the estimated error and demonstrate the performance of the proposed algorithm on both synthetic problems and benchmark UCI datasets. ",
    "code_link": ""
  },
  "icml2015_main_asimpleandprovablealgorithmforsparsediagonalcca": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Simple and Provable Algorithm for Sparse Diagonal CCA",
    "authors": [
      "Megasthenis Asteris",
      "Anastasios Kyrillidis",
      "Oluwasanmi Koyejo",
      "Russell Poldrack"
    ],
    "page_url": "https://proceedings.mlr.press/v48/asteris16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/asteris16.pdf",
    "published": "2015-06",
    "summary": " Given two sets of variables, derived from a common set of samples, sparse Canonical Correlation Analysis (CCA) seeks linear combinations of a small number of variables in each set, such that the induced \\emphcanonical variables are maximally correlated. Sparse CCA is NP-hard. We propose a novel combinatorial algorithm for sparse diagonal CCA, \\textiti.e., sparse CCA under the additional assumption that variables within each set are standardized and uncorrelated. Our algorithm operates on a low rank approximation of the input data and its computational complexity scales linearly with the number of input variables. It is simple to implement, and parallelizable. In contrast to most existing approaches, our algorithm administers precise control on the sparsity of the extracted canonical vectors, and comes with theoretical data-dependent global approximation guarantees, that hinge on the spectrum of the input data. Finally, it can be straightforwardly adapted to other constrained variants of CCA enforcing structure beyond sparsity. We empirically evaluate the proposed scheme and apply it on a real neuroimaging dataset to investigate associations between brain activity and behavior measurements. ",
    "code_link": ""
  },
  "icml2015_main_quadraticoptimizationwithorthogonalityconstraintsexplicitlojasiewiczexponentandlinearconvergenceofline-searchmethods": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods",
    "authors": [
      "Huikang Liu",
      "Weijie Wu",
      "Anthony Man-Cho So"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liue16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liue16.pdf",
    "published": "2015-06",
    "summary": " A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rates of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. This not only allows us to establish the linear convergence of a large class of line-search methods but also answers an important and intriguing problem in mathematical analysis and numerical optimization. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest. ",
    "code_link": ""
  },
  "icml2015_main_normalizationpropagationaparametrictechniqueforremovinginternalcovariateshiftindeepnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks",
    "authors": [
      "Devansh Arpit",
      "Yingbo Zhou",
      "Bhargava Kota",
      "Venu Govindaraju"
    ],
    "page_url": "https://proceedings.mlr.press/v48/arpitb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/arpitb16.pdf",
    "published": "2015-06",
    "summary": " While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks\u2013 \\textitInternal Covariate Shift\u2013 the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size1during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \\textitNormalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. ",
    "code_link": ""
  },
  "icml2015_main_learningtogeneratewithmemory": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning to Generate with Memory",
    "authors": [
      "Chongxuan Li",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lie16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lie16.pdf",
    "published": "2015-06",
    "summary": " Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs on various tasks, including density estimation, image generation, and missing value imputation, and DGMs with memory can achieve state-of-the-art quantitative results. ",
    "code_link": "https://github.com/zhenxuan00/MEM_DGM"
  },
  "icml2015_main_learningend-to-endvideoclassificationwithrank-pooling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning End-to-end Video Classification with Rank-Pooling",
    "authors": [
      "Basura Fernando",
      "Stephen Gould"
    ],
    "page_url": "https://proceedings.mlr.press/v48/fernando16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/fernando16.pdf",
    "published": "2015-06",
    "summary": " We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks. ",
    "code_link": ""
  },
  "icml2015_main_learningtofilterwithpredictivestateinferencemachines": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning to Filter with Predictive State Inference Machines",
    "authors": [
      "Wen Sun",
      "Arun Venkatraman",
      "Byron Boots",
      "J.Andrew Bagnell"
    ],
    "page_url": "https://proceedings.mlr.press/v48/sun16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/sun16.pdf",
    "published": "2015-06",
    "summary": " Latent state space models are a fundamental and widely used tool for modeling dynamical systems. However, they are difficult to learn from data and learned models often lack performance guarantees on inference tasks such as filtering and prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE (PSIM), a data-driven method that considers the inference procedure on a dynamical system as a composition of predictors. The key idea is that rather than first learning a latent state space model, and then using the learned model for inference, PSIM directly learns predictors for inference in predictive state space. We provide theoretical guarantees for inference, in both realizable and agnostic settings, and showcase practical performance on a variety of simulated and real world robotics benchmarks. ",
    "code_link": ""
  },
  "icml2015_main_asubspacelearningapproachforhighdimensionalmatrixdecompositionwithefficientcolumn/rowsampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Subspace Learning Approach for High Dimensional Matrix Decomposition with Efficient Column/Row Sampling",
    "authors": [
      "Mostafa Rahmani",
      "Geroge Atia"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rahmani16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rahmani16.pdf",
    "published": "2015-06",
    "summary": " This paper presents a new randomized approach to high-dimensional low rank (LR) plus sparse matrix decomposition. For a data matrix D \u2208R^N_1 \\times N_2, the complexity of conventional decomposition methods is O(N_1 N_2 r), which limits their usefulness in big data settings (r is the rank of the LR component). In addition, the existing randomized approaches rely for the most part on uniform random sampling, which may be inefficient for many real world data matrices. The proposed subspace learning based approach recovers the LR component using only a small subset of the columns/rows of data and reduces complexity to O(\\max(N_1,N_2) r^2). Even when the columns/rows are sampled uniformly at random, the sufficient number of sampled columns/rows is shown to be roughly O(r \u03bc), where \u03bcis the coherency parameter of the LR component. In addition, efficient sampling algorithms are proposed to address the problem of column/row sampling from structured data. ",
    "code_link": ""
  },
  "icml2015_main_dcmbanditslearningtorankwithmultipleclicks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "DCM Bandits: Learning to Rank with Multiple Clicks",
    "authors": [
      "Sumeet Katariya",
      "Branislav Kveton",
      "Csaba Szepesvari",
      "Zheng Wen"
    ],
    "page_url": "https://proceedings.mlr.press/v48/katariya16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/katariya16.pdf",
    "published": "2015-06",
    "summary": " A search engine recommends to the user a list of web pages. The user examines this list, from the first page to the last, and clicks on all attractive pages until the user is satisfied. This behavior of the user can be described by the dependent click model (DCM). We propose DCM bandits, an online learning variant of the DCM where the goal is to maximize the probability of recommending satisfactory items, such as web pages. The main challenge of our learning problem is that we do not observe which attractive item is satisfactory. We propose a computationally-efficient learning algorithm for solving our problem, dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable assumptions; and also prove a matching lower bound up to logarithmic factors. We evaluate our algorithm on synthetic and real-world problems, and show that it performs well even when our model is misspecified. This work presents the first practical and regret-optimal online algorithm for learning to rank with multiple clicks in a cascade-like click model. ",
    "code_link": ""
  },
  "icml2015_main_trainfaster,generalizebetterstabilityofstochasticgradientdescent": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Train faster, generalize better: Stability of stochastic gradient descent",
    "authors": [
      "Moritz Hardt",
      "Ben Recht",
      "Yoram Singer"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hardt16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hardt16.pdf",
    "published": "2015-06",
    "summary": " We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit. ",
    "code_link": ""
  },
  "icml2015_main_copelandduelingbanditproblemregretlowerbound,optimalalgorithm,andcomputationallyefficientalgorithm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm",
    "authors": [
      "Junpei Komiyama",
      "Junya Honda",
      "Hiroshi Nakagawa"
    ],
    "page_url": "https://proceedings.mlr.press/v48/komiyama16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/komiyama16.pdf",
    "published": "2015-06",
    "summary": " We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Deterministic Minimum Empirical Divergence (CW-RMED), an algorithm inspired by the DMED algorithm (Honda and Takemura, 2010), and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones. ",
    "code_link": ""
  },
  "icml2015_main_contextualcombinatorialcascadingbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Contextual Combinatorial Cascading Bandits",
    "authors": [
      "Shuai Li",
      "Baoxiang Wang",
      "Shengyu Zhang",
      "Wei Chen"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lif16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lif16.pdf",
    "published": "2015-06",
    "summary": " We propose the contextual combinatorial cascading bandits, a combinatorial online learning game, where at each time step a learning agent is given a set of contextual information, then selects a list of items, and observes stochastic outcomes of a prefix in the selected items by some stopping criterion. In online recommendation, the stopping criterion might be the first item a user selects; in network routing, the stopping criterion might be the first edge blocked in a path. We consider position discounts in the list order, so that the agent\u2019s reward is discounted depending on the position where the stopping criterion is met. We design a UCB-type algorithm, C^3-UCB, for this problem, prove an n-step regret bound \\tildeO(\\sqrtn) in the general setting, and give finer analysis for two special cases. Our work generalizes existing studies in several directions, including contextual information, position discounts, and a more general cascading bandit model. Experiments on synthetic and real datasets demonstrate the advantage of involving contextual information and position discounts. ",
    "code_link": ""
  },
  "icml2015_main_conservativebandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Conservative Bandits",
    "authors": [
      "Yifan Wu",
      "Roshan Shariff",
      "Tor Lattimore",
      "Csaba Szepesvari"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wu16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wu16.pdf",
    "published": "2015-06",
    "summary": " We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the design of those algorithms makes them unsuitable under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose natural yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings. ",
    "code_link": ""
  },
  "icml2015_main_variance-reducedandprojection-freestochasticoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variance-Reduced and Projection-Free Stochastic Optimization",
    "authors": [
      "Elad Hazan",
      "Haipeng Luo"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hazana16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hazana16.pdf",
    "published": "2015-06",
    "summary": " The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve 1-\u03b5accuracy. For example, we improve from O(\\frac1\u03b5) to O(\\ln\\frac1\u03b5) if the objective function is smooth and strongly convex, and from O(\\frac1\u03b5^2) to O(\\frac1\u03b5^1.5) if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application. ",
    "code_link": ""
  },
  "icml2015_main_factoredtemporalsigmoidbeliefnetworksforsequencelearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Factored Temporal Sigmoid Belief Networks for Sequence Learning",
    "authors": [
      "Jiaming Song",
      "Zhe Gan",
      "Lawrence Carin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/songa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/songa16.pdf",
    "published": "2015-06",
    "summary": " Deep conditional generative models are developed to simultaneously learn the temporal dependencies of multiple sequences. The model is designed by introducing a three-way weight tensor to capture the multiplicative interactions between side information and sequences. The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). The transition matrices are further factored to reduce the number of parameters and improve generalization. When side information is not available, a general framework for semi-supervised learning based on the proposed model is constituted, allowing robust sequence classification. Experimental results show that the proposed approach achieves state-of-the-art predictive and classification performance on sequential data, and has the capacity to synthesize sequences, with controlled style transitioning and blending. ",
    "code_link": ""
  },
  "icml2015_main_falsediscoveryratecontrolandstatisticalqualityassessmentofannotatorsincrowdsourcedranking": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking",
    "authors": [
      "QianQian Xu",
      "Jiechao Xiong",
      "Xiaochun Cao",
      "Yuan Yao"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xua16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xua16.pdf",
    "published": "2015-06",
    "summary": " With the rapid growth of crowdsourcing platforms it has become easy and relatively inexpensive to collect a dataset labeled by multiple annotators in a short time. However due to the lack of control over the quality of the annotators, some abnormal annotators may be affected by position bias which can potentially degrade the quality of the final consensus labels. In this paper we introduce a statistical framework to model and detect annotator\u2019s position bias in order to control the false discovery rate (FDR) without a prior knowledge on the amount of biased annotators\u2013the expected fraction of false discoveries among all discoveries being not too high, in order to assure that most of the discoveries are indeed true and replicable. The key technical development relies on some new knockoff filters adapted to our problem and new algorithms based on the Inverse Scale Space dynamics whose discretization is potentially suitable for large scale crowdsourcing data analysis. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a useful tool for quantitatively studying annotator\u2019s abnormal behavior in crowdsourcing. ",
    "code_link": ""
  },
  "icml2015_main_strongly-typedrecurrentneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Strongly-Typed Recurrent Neural Networks",
    "authors": [
      "David Balduzzi",
      "Muhammad Ghifary"
    ],
    "page_url": "https://proceedings.mlr.press/v48/balduzzi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/balduzzi16.pdf",
    "published": "2015-06",
    "summary": " Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics, we introduce type constraints, analogous to the constraints that forbids adding meters to seconds. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and state-dependent firmware, reducing the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on one-dimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training and comparable generalization error to classical architectures. ",
    "code_link": ""
  },
  "icml2015_main_distributedclusteringoflinearbanditsinpeertopeernetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Distributed Clustering of Linear Bandits in Peer to Peer Networks",
    "authors": [
      "Nathan Korda",
      "Balazs Szorenyi",
      "Shuai Li"
    ],
    "page_url": "https://proceedings.mlr.press/v48/korda16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/korda16.pdf",
    "published": "2015-06",
    "summary": " We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art. ",
    "code_link": ""
  },
  "icml2015_main_collapsedvariationalinferenceforsum-productnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Collapsed Variational Inference for Sum-Product Networks",
    "authors": [
      "Han Zhao",
      "Tameem Adel",
      "Geoff Gordon",
      "Brandon Amos"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhaoa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhaoa16.pdf",
    "published": "2015-06",
    "summary": " Sum-Product Networks (SPNs) are probabilistic inference machines that admit exact inference in linear time in the size of the network. Existing parameter learning approaches for SPNs are largely based on the maximum likelihood principle and are subject to overfitting compared to more Bayesian approaches. Exact Bayesian posterior inference for SPNs is computationally intractable. Even approximation techniques such as standard variational inference and posterior sampling for SPNs are computationally infeasible even for networks of moderate size due to the large number of local latent variables per instance. In this work, we propose a novel deterministic collapsed variational inference algorithm for SPNs that is computationally efficient, easy to implement and at the same time allows us to incorporate prior information into the optimization formulation. Extensive experiments show a significant improvement in accuracy compared with a maximum likelihood based approach. ",
    "code_link": ""
  },
  "icml2015_main_ontheanalysisofcomplexbackupstrategiesinmontecarlotreesearch": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Analysis of Complex Backup Strategies in Monte Carlo Tree Search",
    "authors": [
      "Piyush Khandelwal",
      "Elad Liebman",
      "Scott Niekum",
      "Peter Stone"
    ],
    "page_url": "https://proceedings.mlr.press/v48/khandelwal16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/khandelwal16.pdf",
    "published": "2015-06",
    "summary": " Over the past decade, Monte Carlo Tree Search (MCTS) and specifically Upper Confidence Bound in Trees (UCT) have proven to be quite effective in large probabilistic planning domains. In this paper, we focus on how values are backpropagated in the MCTS tree, and apply complex return strategies from the Reinforcement Learning (RL) literature to MCTS, producing 4 new MCTS variants. We demonstrate that in some probabilistic planning benchmarks from the International Planning Competition (IPC), selecting a MCTS variant with a backup strategy different from Monte Carlo averaging can lead to substantially better results. We also propose a hypothesis for why different backup strategies lead to different performance in particular environments, and manipulate a carefully structured grid-world domain to provide empirical evidence supporting our hypothesis. ",
    "code_link": "https://github.com/ssanner/rddlsim"
  },
  "icml2015_main_benchmarkingdeepreinforcementlearningforcontinuouscontrol": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
    "authors": [
      "Yan Duan",
      "Xi Chen",
      "Rein Houthooft",
      "John Schulman",
      "Pieter Abbeel"
    ],
    "page_url": "https://proceedings.mlr.press/v48/duan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/duan16.pdf",
    "published": "2015-06",
    "summary": " Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers. ",
    "code_link": "https://github.com/rllab/rllab"
  },
  "icml2015_main_k-meansclusteringwithdistributeddimensions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "K-Means Clustering with Distributed Dimensions",
    "authors": [
      "Hu Ding",
      "Yu Liu",
      "Lingxiao Huang",
      "Jian Li"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ding16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ding16.pdf",
    "published": "2015-06",
    "summary": " Distributed clustering has attracted significant attention in recent years. In this paper, we study the k-means problem in the distributed dimension setting, where the dimensions of the data are partitioned across multiple machines. We provide new approximation algorithms, which incur low communication costs and achieve constant approximation ratios. The communication complexity of our algorithms significantly improve on existing algorithms. We also provide the first communication lower bound, which nearly matches our upper bound in a certain range of parameter setting. Our experimental results show that our algorithms outperform existing algorithms on real data-sets in the distributed dimension setting. ",
    "code_link": ""
  },
  "icml2015_main_texturenetworksfeed-forwardsynthesisoftexturesandstylizedimages": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Texture Networks: Feed-forward Synthesis of Textures and Stylized Images",
    "authors": [
      "Dmitry Ulyanov",
      "Vadim Lebedev",
      "Andrea",
      "Victor Lempitsky"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ulyanov16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ulyanov16.pdf",
    "published": "2015-06",
    "summary": " Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys et al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions. ",
    "code_link": ""
  },
  "icml2015_main_fastconstrainedsubmodularmaximizationpersonalizeddatasummarization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast Constrained Submodular Maximization: Personalized Data Summarization",
    "authors": [
      "Baharan Mirzasoleiman",
      "Ashwinkumar Badanidiyuru",
      "Amin Karbasi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mirzasoleiman16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mirzasoleiman16.pdf",
    "published": "2015-06",
    "summary": " Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains. It achieves a (1 + \u03b5)(p + 1)(2p + 2l + 1)/p approximation guarantee with only O(nrp log(n)/\u03b5) query complexity (n and r indicate the size of the ground set and the size of the largest feasible solution, respectively). We then show how we can use FANTOM for personalized data summarization. In particular, a p-system can model different aspects of data, such as categories or time stamps, from which the users choose. In addition, knapsacks encode users\u2019 constraints including budget or time. In our set of experiments, we consider several concrete applications: movie recommendation over 11K movies, personalized image summarization with 10K images, and revenue maximization on the YouTube social networks with 5000 communities. We observe that FANTOM constantly provides the highest utility against all the baselines. ",
    "code_link": ""
  },
  "icml2015_main_onthestatisticallimitsofconvexrelaxations": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Statistical Limits of Convex Relaxations",
    "authors": [
      "Zhaoran Wang",
      "Quanquan Gu",
      "Han Liu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wangc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wangc16.pdf",
    "published": "2015-06",
    "summary": " Many high dimensional sparse learning problems are formulated as nonconvex optimization. A popular approach to solve these nonconvex optimization problems is through convex relaxations such as linear and semidefinite programming. In this paper, we study the statistical limits of convex relaxations. Particularly, we consider two problems: Mean estimation for sparse principal submatrix and edge probability estimation for stochastic block model. We exploit the sum-of-squares relaxation hierarchy to sharply characterize the limits of a broad class of convex relaxations. Our result shows statistical optimality needs to be compromised for achieving computational tractability using convex relaxations. Compared with existing results on computational lower bounds for statistical problems, which consider general polynomial-time algorithms and rely on computational hardness hypotheses on problems like planted clique detection, our theory focuses on a broad class of convex relaxations and does not rely on unproven hypotheses. ",
    "code_link": ""
  },
  "icml2015_main_askmeanythingdynamicmemorynetworksfornaturallanguageprocessing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
    "authors": [
      "Ankit Kumar",
      "Ozan Irsoy",
      "Peter Ondruska",
      "Mohit Iyyer",
      "James Bradbury",
      "Ishaan Gulrajani",
      "Victor Zhong",
      "Romain Paulus",
      "Richard Socher"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kumar16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kumar16.pdf",
    "published": "2015-06",
    "summary": " Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook\u2019s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets. ",
    "code_link": ""
  },
  "icml2015_main_gossipdualaveragingfordecentralizedoptimizationofpairwisefunctions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions",
    "authors": [
      "Igor Colin",
      "Aurelien Bellet",
      "Joseph Salmon",
      "St\u00e9phan Cl\u00e9men\u00e7on"
    ],
    "page_url": "https://proceedings.mlr.press/v48/colin16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/colin16.pdf",
    "published": "2015-06",
    "summary": " In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach. ",
    "code_link": ""
  },
  "icml2015_main_solvingridgeregressionusingsketchedpreconditionedsvrg": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Solving Ridge Regression using Sketched Preconditioned SVRG",
    "authors": [
      "Alon Gonen",
      "Francesco Orabona",
      "Shai Shalev-Shwartz"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gonen16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gonen16.pdf",
    "published": "2015-06",
    "summary": " We develop a novel preconditioning method for ridge regression, based on recent linear sketching methods. By equipping Stochastic Variance Reduced Gradient (SVRG) with this preconditioning process, we obtain a significant speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG. ",
    "code_link": ""
  },
  "icml2015_main_cumulativeprospecttheorymeetsreinforcementlearningpredictionandcontrol": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control",
    "authors": [
      "Prashanth L.A.",
      "Cheng Jie",
      "Michael Fu",
      "Steve Marcus",
      "Csaba Szepesvari"
    ],
    "page_url": "https://proceedings.mlr.press/v48/la16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/la16.pdf",
    "published": "2015-06",
    "summary": " Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The RL setting presents two particular challenges when CPT is applied: estimating the CPT objective requires estimations of the entire distribution of the value function and finding a randomized optimal policy. The estimation scheme that we propose uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of a CPT-value optimization procedure that is based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA). We provide theoretical convergence guarantees for all the proposed algorithms and also empirically demonstrate the usefulness of our algorithms. ",
    "code_link": ""
  },
  "icml2015_main_estimatingaccuracyfromunlabeleddataabayesianapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Estimating Accuracy from Unlabeled Data: A Bayesian Approach",
    "authors": [
      "Emmanouil Antonios Platanios",
      "Avinava Dubey",
      "Tom Mitchell"
    ],
    "page_url": "https://proceedings.mlr.press/v48/platanios16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/platanios16.pdf",
    "published": "2015-06",
    "summary": " We consider the question of how unlabeled data can be used to estimate the true accuracy of learned classifiers, and the related question of how outputs from several classifiers performing the same task can be combined based on their estimated accuracies. To answer these questions, we first present a simple graphical model that performs well in practice. We then provide two nonparametric extensions to it that improve its performance. Experiments on two real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. ",
    "code_link": ""
  },
  "icml2015_main_non-negativematrixfactorizationunderheavynoise": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Non-negative Matrix Factorization under Heavy Noise",
    "authors": [
      "Chiranjib Bhattacharya",
      "Navin Goyal",
      "Ravindran Kannan",
      "Jagdeep Pani"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bhattacharya16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bhattacharya16.pdf",
    "published": "2015-06",
    "summary": " The Noisy Non-negative Matrix factorization (NMF) is: given a data matrix A (d x n), find non-negative matrices B;C (d x k, k x n respy.) so that A = BC +N, where N is a noise matrix. Existing polynomial time algorithms with proven error guarantees require EACH column N_\u22c5j to have l1 norm much smaller than ||(BC)_\u22c5j ||_1, which could be very restrictive. In important applications of NMF such as Topic Modeling as well as theoretical noise models (e.g. Gaussian with high sigma), almost EVERY column of N_.j violates this condition. We introduce the heavy noise model which only requires the average noise over large subsets of columns to be small. We initiate a study of Noisy NMF under the heavy noise model. We show that our noise model subsumes noise models of theoretical and practical interest (for e.g. Gaussian noise of maximum possible sigma). We then devise an algorithm TSVDNMF which under certain assumptions on B,C, solves the problem under heavy noise. Our error guarantees match those of previous algorithms. Our running time of O(k.(d+n)^2) is substantially better than the O(d.n^3) for the previous best. Our assumption on B is weaker than the \u201cSeparability\u201d assumption made by all previous results. We provide empirical justification for our assumptions on C. We also provide the first proof of identifiability (uniqueness of B) for noisy NMF which is not based on separability and does not use hard to check geometric conditions. Our algorithm outperforms earlier polynomial time algorithms both in time and error, particularly in the presence of high noise. ",
    "code_link": ""
  },
  "icml2015_main_extremef-measuremaximizationusingsparseprobabilityestimates": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Extreme F-measure Maximization using Sparse Probability Estimates",
    "authors": [
      "Kalina Jasinska",
      "Krzysztof Dembczynski",
      "Robert Busa-Fekete",
      "Karlson Pfannschmidt",
      "Timo Klerx",
      "Eyke Hullermeier"
    ],
    "page_url": "https://proceedings.mlr.press/v48/jasinska16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/jasinska16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of (macro) F-measure maximization in the context of extreme multi-label classification (XMLC), i.e., multi-label classification with extremely large label spaces. We investigate several approaches based on recent results on the maximization of complex performance measures in binary classification. According to these results, the F-measure can be maximized by properly thresholding conditional class probability estimates. We show that a naive adaptation of this approach can be very costly for XMLC and propose to solve the problem by classifiers that efficiently deliver sparse probability estimates (SPEs), that is, probability estimates restricted to the most probable labels. Empirical results provide evidence for the strong practical performance of this approach. ",
    "code_link": "https://github.com/busarobi/XMLC"
  },
  "icml2015_main_auxiliarydeepgenerativemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Auxiliary Deep Generative Models",
    "authors": [
      "Lars Maal\u00f8e",
      "Casper Kaae S\u00f8nderby",
      "S\u00f8ren Kaae S\u00f8nderby",
      "Ole Winther"
    ],
    "page_url": "https://proceedings.mlr.press/v48/maaloe16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/maaloe16.pdf",
    "published": "2015-06",
    "summary": " Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets. ",
    "code_link": ""
  },
  "icml2015_main_importancesamplingtreeforlarge-scaleempiricalexpectation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Importance Sampling Tree for Large-scale Empirical Expectation",
    "authors": [
      "Olivier Canevet",
      "Cijo Jose",
      "Francois Fleuret"
    ],
    "page_url": "https://proceedings.mlr.press/v48/canevet16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/canevet16.pdf",
    "published": "2015-06",
    "summary": " We propose a tree-based procedure inspired by the Monte-Carlo Tree Search that dynamically modulates an importance-based sampling to prioritize computation, while getting unbiased estimates of weighted sums. We apply this generic method to learning on very large training sets, and to the evaluation of large-scale SVMs. The core idea is to reformulate the estimation of a score - whether a loss or a prediction estimate - as an empirical expectation, and to use such a tree whose leaves carry the samples to focus efforts over the problematic \"heavy weight\" ones. We illustrate the potential of this approach on three problems: to improve Adaboost and a multi-layer perceptron on 2D synthetic tasks with several million points, to train a large-scale convolution network on several millions deformations of the CIFAR data-set, and to compute the response of a SVM with several hundreds of thousands of support vectors. In each case, we show how it either cuts down computation by more than one order of magnitude and/or allows to get better loss estimates. ",
    "code_link": "https://github.com/nagadomi/kaggle-cifar10-torch7"
  },
  "icml2015_main_startingsmall-learningwithadaptivesamplesizes": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Starting Small - Learning with Adaptive Sample Sizes",
    "authors": [
      "Hadi Daneshmand",
      "Aurelien Lucchi",
      "Thomas Hofmann"
    ],
    "page_url": "https://proceedings.mlr.press/v48/daneshmand16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/daneshmand16.pdf",
    "published": "2015-06",
    "summary": " For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show - theoretically and empirically - how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an n-sample in 2n, instead of n log n steps. ",
    "code_link": ""
  },
  "icml2015_main_deepgaussianprocessesforregressionusingapproximateexpectationpropagation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation",
    "authors": [
      "Thang Bui",
      "Daniel Hernandez-Lobato",
      "Jose Hernandez-Lobato",
      "Yingzhen Li",
      "Richard Turner"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bui16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bui16.pdf",
    "published": "2015-06",
    "summary": " Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks. ",
    "code_link": ""
  },
  "icml2015_main_dr-abcapproximatebayesiancomputationwithkernel-baseddistributionregression": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression",
    "authors": [
      "Jovana Mitrovic",
      "Dino Sejdinovic",
      "Yee-Whye Teh"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mitrovic16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mitrovic16.pdf",
    "published": "2015-06",
    "summary": " Performing exact posterior inference in complex generative models is often difficult or impossible due to an expensive to evaluate or intractable likelihood function. Approximate Bayesian computation (ABC) is an inference framework that constructs an approximation to the true likelihood based on the similarity between the observed and simulated data as measured by a predefined set of summary statistics. Although the choice of informative problem-specific summary statistics crucially influences the quality of the likelihood approximation and hence also the quality of the posterior sample in ABC, there are only few principled general-purpose approaches to the selection or construction of such summary statistics. In this paper, we develop a novel framework for solving this problem. We model the functional relationship between the data and the optimal choice (with respect to a loss function) of summary statistics using kernel-based distribution regression. Furthermore, we extend our approach to incorporate kernel-based regression from conditional distributions, thus appropriately taking into account the specific structure of the posited generative model. We show that our approach can be implemented in a computationally and statistically efficient way using the random Fourier features framework for large-scale kernel learning. In addition to that, our framework outperforms related methods by a large margin on toy and real-world data, including hierarchical and time series models. ",
    "code_link": "https://github.com/jovana-mitrovic/dr-abc"
  },
  "icml2015_main_predictiveentropysearchformulti-objectivebayesianoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Predictive Entropy Search for Multi-objective Bayesian Optimization",
    "authors": [
      "Daniel Hernandez-Lobato",
      "Jose Hernandez-Lobato",
      "Amar Shah",
      "Ryan Adams"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hernandez-lobatoa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hernandez-lobatoa16.pdf",
    "published": "2015-06",
    "summary": " We present \\small PESMO, a Bayesian method for identifying the Pareto set of multi-objective optimization problems, when the functions are expensive to evaluate. \\small PESMO chooses the evaluation points to maximally reduce the entropy of the posterior distribution over the Pareto set. The \\small PESMO acquisition function is decomposed as a sum of objective-specific acquisition functions, which makes it possible to use the algorithm in \\emphdecoupled scenarios in which the objectives can be evaluated separately and perhaps with different costs. This decoupling capability is useful to identify difficult objectives that require more evaluations. \\small PESMO also offers gains in efficiency, as its cost scales linearly with the number of objectives, in comparison to the exponential cost of other methods. We compare \\small PESMO with other methods on synthetic and real-world problems. The results show that \\small PESMO produces better recommendations with a smaller number of evaluations, and that a decoupled evaluation can lead to improvements in performance, particularly when the number of objectives is large. ",
    "code_link": ""
  },
  "icml2015_main_richcomponentanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Rich Component Analysis",
    "authors": [
      "Rong Ge",
      "James Zou"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gea16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gea16.pdf",
    "published": "2015-06",
    "summary": " In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don\u2019t have samples from the true model but only samples after complex perturbations. ",
    "code_link": ""
  },
  "icml2015_main_black-boxalphadivergenceminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Black-Box Alpha Divergence Minimization",
    "authors": [
      "Jose Hernandez-Lobato",
      "Yingzhen Li",
      "Mark Rowland",
      "Thang Bui",
      "Daniel Hernandez-Lobato",
      "Richard Turner"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hernandez-lobatob16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hernandez-lobatob16.pdf",
    "published": "2015-06",
    "summary": " Black-box alpha (BB-\u03b1) is a new approximate inference method based on the minimization of \u03b1-divergences. BB-\u03b1scales to large datasets because it can be implemented using stochastic gradient descent. BB-\u03b1can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter \u03b1, the method is able to interpolate between variational Bayes (VB) (\u03b1\u21920) and an algorithm similar to expectation propagation (EP) (\u03b1= 1). Experiments on probit regression and neural network regression and classification problems show that BB-\u03b1with non-standard settings of \u03b1, such as \u03b1= 0.5, usually produces better predictions than with \u03b1\u21920 (VB) or \u03b1= 1 (EP). ",
    "code_link": "https://github.com/HIPS/autograd"
  },
  "icml2015_main_one-shotgeneralizationindeepgenerativemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "One-Shot Generalization in Deep Generative Models",
    "authors": [
      "Danilo Rezende",
      "Shakir",
      "Ivo Danihelka",
      "Karol Gregor",
      "Daan Wierstra"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rezende16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rezende16.pdf",
    "published": "2015-06",
    "summary": " Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples\u2014having seen new examples just once\u2014providing an important class of general-purpose models for one-shot machine learning. ",
    "code_link": ""
  },
  "icml2015_main_optimalclassificationwithmultivariatelosses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Optimal Classification with Multivariate Losses",
    "authors": [
      "Nagarajan Natarajan",
      "Oluwasanmi Koyejo",
      "Pradeep Ravikumar",
      "Inderjit Dhillon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/natarajan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/natarajan16.pdf",
    "published": "2015-06",
    "summary": " Multivariate loss functions are extensively employed in several prediction tasks arising in Information Retrieval. Often, the goal in the tasks is to minimize expected loss when retrieving relevant items from a presented set of items, where the expectation is with respect to the joint distribution over item sets. Our key result is that for most multivariate losses, the expected loss is provably optimized by sorting the items by the conditional probability of label being positive and then selecting top k items. Such a result was previously known only for the F-measure. Leveraging on the optimality characterization, we give an algorithm for estimating optimal predictions in practice with runtime quadratic in size of item sets for many losses. We provide empirical results on benchmark datasets, comparing the proposed algorithm to state-of-the-art methods for optimizing multivariate losses. ",
    "code_link": ""
  },
  "icml2015_main_arankingapproachtoglobaloptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A ranking approach to global optimization",
    "authors": [
      "Cedric Malherbe",
      "Emile Contal",
      "Nicolas Vayatis"
    ],
    "page_url": "https://proceedings.mlr.press/v48/malherbe16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/malherbe16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of maximizing an unknown function f over a compact and convex set using as few observations f(x) as possible. We observe that the optimization of the function f essentially relies on learning the induced bipartite ranking rule of f. Based on this idea, we relate global optimization to bipartite ranking which allows to address problems with high dimensional input space, as well as cases of functions with weak regularity properties. The paper introduces novel meta-algorithms for global optimization which rely on the choice of any bipartite ranking method. Theoretical properties are provided as well as convergence guarantees and equivalences between various optimization methods are obtained as a by-product. Eventually, numerical evidence is given to show that the main algorithm of the paper which adapts empirically to the underlying ranking structure essentially outperforms existing state-of-the-art global optimization algorithms in typical benchmarks. ",
    "code_link": ""
  },
  "icml2015_main_parallelanddistributedblock-coordinatefrank-wolfealgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Parallel and Distributed Block-Coordinate Frank-Wolfe Algorithms",
    "authors": [
      "Yu-Xiang Wang",
      "Veeranjaneyulu Sadhanala",
      "Wei Dai",
      "Willie Neiswanger",
      "Suvrit Sra",
      "Eric Xing"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wangd16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wangd16.pdf",
    "published": "2015-06",
    "summary": " We study parallel and distributed Frank-Wolfe algorithms; the former on shared memory machines with mini-batching, and the latter in a delayed update framework. In both cases, we perform computations asynchronously whenever possible. We assume block-separable constraints as in Block-Coordinate Frank-Wolfe (BCFW) method (Lacoste et. al., 2013) , but our analysis subsumes BCFW and reveals problem-dependent quantities that govern the speedups of our methods over BCFW. A notable feature of our algorithms is that they do not depend on worst-case bounded delays, but only (mildly) on **expected** delays, making them robust to stragglers and faulty worker threads. We present experiments on structural SVM and Group Fused Lasso, and observe significant speedups over competing state-of-the-art (and synchronous) methods. ",
    "code_link": ""
  },
  "icml2015_main_autoencodingbeyondpixelsusingalearnedsimilaritymetric": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Autoencoding beyond pixels using a learned similarity metric",
    "authors": [
      "Anders Boesen Lindbo Larsen",
      "S\u00f8ren Kaae S\u00f8nderby",
      "Hugo Larochelle",
      "Ole Winther"
    ],
    "page_url": "https://proceedings.mlr.press/v48/larsen16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/larsen16.pdf",
    "published": "2015-06",
    "summary": " We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic. ",
    "code_link": "https://github.com/andersbll/deeppy"
  },
  "icml2015_main_ensuringrapidmixingandlowbiasforasynchronousgibbssampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling",
    "authors": [
      "Christopher De Sa",
      "Chris Re",
      "Kunle Olukotun"
    ],
    "page_url": "https://proceedings.mlr.press/v48/sa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/sa16.pdf",
    "published": "2015-06",
    "summary": " Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes. ",
    "code_link": ""
  },
  "icml2015_main_simultaneoussafescreeningoffeaturesandsamplesindoublysparsemodeling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling",
    "authors": [
      "Atsushi Shibagaki",
      "Masayuki Karasuyama",
      "Kohei Hatano",
      "Ichiro Takeuchi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shibagaki16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shibagaki16.pdf",
    "published": "2015-06",
    "summary": " The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of non-active features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A significant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and vice-versa. We first theoretically investigate the synergy effect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples. ",
    "code_link": "https://github.com/takeuchi-lab/s3fs"
  },
  "icml2015_main_anytimeoptimalalgorithmsinstochasticmulti-armedbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Anytime optimal algorithms in stochastic multi-armed bandits",
    "authors": [
      "R\u00e9my Degenne",
      "Vianney Perchet"
    ],
    "page_url": "https://proceedings.mlr.press/v48/degenne16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/degenne16.pdf",
    "published": "2015-06",
    "summary": " We introduce an anytime algorithm for stochastic multi-armed bandit with optimal distribution free and distribution dependent bounds (for a specific family of parameters). The performances of this algorithm (as well as another one motivated by the conjectured optimal bound) are evaluated empirically. A similar analysis is provided with full information, to serve as a benchmark. ",
    "code_link": ""
  },
  "icml2015_main_boundedoff-policyevaluationwithmissingdataforcourserecommendationandcurriculumdesign": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bounded Off-Policy Evaluation with Missing Data for Course Recommendation and Curriculum Design",
    "authors": [
      "William Hoiles",
      "Mihaela Schaar"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hoiles16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hoiles16.pdf",
    "published": "2015-06",
    "summary": " Successfully recommending personalized course schedules is a difficult problem given the diversity of students knowledge, learning behaviour, and goals. This paper presents personalized course recommendation and curriculum design algorithms that exploit logged student data. The algorithms are based on the regression estimator for contextual multi-armed bandits with a penalized variance term. Guarantees on the predictive performance of the algorithms are provided using empirical Bernstein bounds. We also provide guidelines for including expert domain knowledge into the recommendations. Using undergraduate engineering logged data from a post-secondary institution we illustrate the performance of these algorithms. ",
    "code_link": ""
  },
  "icml2015_main_oncollapsedrepresentationofhierarchicalcompletelyrandommeasures": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On collapsed representation of hierarchical Completely Random Measures",
    "authors": [
      "Gaurav Pandey",
      "Ambedkar Dukkipati"
    ],
    "page_url": "https://proceedings.mlr.press/v48/pandey16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/pandey16.pdf",
    "published": "2015-06",
    "summary": " The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures\u00a0(CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topic-modelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP. ",
    "code_link": ""
  },
  "icml2015_main_fromsoftmaxtosparsemaxasparsemodelofattentionandmulti-labelclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification",
    "authors": [
      "Andre Martins",
      "Ramon Astudillo"
    ],
    "page_url": "https://proceedings.mlr.press/v48/martins16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/martins16.pdf",
    "published": "2015-06",
    "summary": " We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus. ",
    "code_link": ""
  },
  "icml2015_main_black-boxoptimizationwithapolitician": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Black-box Optimization with a Politician",
    "authors": [
      "Sebastien Bubeck",
      "Yin Tat Lee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bubeck16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bubeck16.pdf",
    "published": "2015-06",
    "summary": " We propose a new framework for black-box convex optimization which is well-suited for situations where gradient computations are expensive. We derive a new method for this framework which leverages several concepts from convex optimization, from standard first-order methods (e.g. gradient descent or quasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant barriers). We demonstrate empirically that our new technique compares favorably with state of the art algorithms (such as BFGS). ",
    "code_link": ""
  },
  "icml2015_main_gaussianprocessnonparametrictensorestimatoranditsminimaxoptimality": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Gaussian process nonparametric tensor estimator and its minimax optimality",
    "authors": [
      "Heishiro Kanagawa",
      "Taiji Suzuki",
      "Hayato Kobayashi",
      "Nobuyuki Shimizu",
      "Yukihiro Tagami"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kanagawa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kanagawa16.pdf",
    "published": "2015-06",
    "summary": " We investigate the statistical efficiency of a nonparametric Gaussian process method for a nonlinear tensor estimation problem. Low-rank tensor estimation has been used as a method to learn higher order relations among several data sources in a wide range of applications, such as multi-task learning, recommendation systems, and spatiotemporal analysis. We consider a general setting where a common linear tensor learning is extended to a nonlinear learning problem in reproducing kernel Hilbert space and propose a nonparametric Bayesian method based on the Gaussian process method. We prove its statistical convergence rate without assuming any strong convexity, such as restricted strong convexity. Remarkably, it is shown that our convergence rate achieves the minimax optimal rate. We apply our proposed method to multi-task learning and show that our method significantly outperforms existing methods through numerical experiments on real-world data sets. ",
    "code_link": ""
  },
  "icml2015_main_no-regretalgorithmsforheavy-tailedlinearbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "No-Regret Algorithms for Heavy-Tailed Linear Bandits",
    "authors": [
      "Andres Munoz Medina",
      "Scott Yang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/medina16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/medina16.pdf",
    "published": "2015-06",
    "summary": " We analyze the problem of linear bandits under heavy tailed noise. Most of of the work on linear bandits has been based on the assumption of bounded or sub-Gaussian noise. However, this assumption is often violated in common scenarios such as financial markets. We present two algorithms to tackle this problem: one based on dynamic truncation and one based on a median of means estimator. We show that, when the noise admits admits only a 1 + \u03b5moment, these algorithms are still able to achieve regret in \\widetildeO(T^\\frac2 + \u03b52(1 + \u03b5)) and \\widetildeO(T^\\frac1+ 2\u03b51 + 3 \u03b5) respectively. In particular, they guarantee sublinear regret as long as the noise has finite variance. We also present empirical results showing that our algorithms achieve a better performance than the current state of the art for bounded noise when the L_\u221ebound on the noise is large yet the 1 + \u03b5moment of the noise is small. ",
    "code_link": ""
  },
  "icml2015_main_extendedandunscentedkitchensinks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Extended and Unscented Kitchen Sinks",
    "authors": [
      "Edwin Bonilla",
      "Daniel Steinberg",
      "Alistair Reid"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bonilla16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bonilla16.pdf",
    "published": "2015-06",
    "summary": " We propose a scalable multiple-output generalization of unscented and extended Gaussian processes. These algorithms have been designed to handle general likelihood models by linearizing them using a Taylor series or the Unscented Transform in a variational inference framework. We build upon random feature approximations of Gaussian process covariance functions and show that, on small-scale single-task problems, our methods can attain similar performance as the original algorithms while having less computational cost. We also evaluate our methods at a larger scale on MNIST and on a seismic inversion which is inherently a multi-task problem. ",
    "code_link": ""
  },
  "icml2015_main_matrixeigen-decompositionviadoublystochasticriemannianoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Matrix Eigen-decomposition via Doubly Stochastic Riemannian Optimization",
    "authors": [
      "Zhiqiang Xu",
      "Peilin Zhao",
      "Jianneng Cao",
      "Xiaoli Li"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xub16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xub16.pdf",
    "published": "2015-06",
    "summary": " Matrix eigen-decomposition is a classic and long-standing problem that plays a fundamental role in scientific computing and machine learning. Despite some existing algorithms for this inherently non-convex problem, the study remains inadequate for the need of large data nowadays. To address this gap, we propose a Doubly Stochastic Riemannian Gradient EIGenSolver, DSRG-EIGS, where the double stochasticity comes from the generalization of the stochastic Euclidean gradient ascent and the stochastic Euclidean coordinate ascent to Riemannian manifolds. As a result, it induces a greatly reduced complexity per iteration, enables the algorithm to completely avoid the matrix inversion, and consequently makes it well-suited to large-scale applications. We theoretically analyze its convergence properties and empirically validate it on real-world datasets. Encouraging experimental results demonstrate its advantages over the deterministic counterparts. ",
    "code_link": ""
  },
  "icml2015_main_recommendationsastreatmentsdebiasinglearningandevaluation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Recommendations as Treatments: Debiasing Learning and Evaluation",
    "authors": [
      "Tobias Schnabel",
      "Adith Swaminathan",
      "Ashudeep Singh",
      "Navin Chandak",
      "Thorsten Joachims"
    ],
    "page_url": "https://proceedings.mlr.press/v48/schnabel16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/schnabel16.pdf",
    "published": "2015-06",
    "summary": " Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handle selection biases by adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, and find that it is highly practical and scalable. ",
    "code_link": ""
  },
  "icml2015_main_forecasticuaprognosticdecisionsupportsystemfortimelypredictionofintensivecareunitadmission": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "ForecastICU: A Prognostic Decision Support System for Timely Prediction of Intensive Care Unit Admission",
    "authors": [
      "Jinsung Yoon",
      "Ahmed Alaa",
      "Scott Hu",
      "Mihaela Schaar"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yoon16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yoon16.pdf",
    "published": "2015-06",
    "summary": " We develop ForecastICU: a prognostic decision support system that monitors hospitalized patients and prompts alarms for intensive care unit (ICU) admissions. ForecastICU is first trained in an offline stage by constructing a Bayesian belief system that corresponds to its belief about how trajectories of physiological data streams of the patient map to a clinical status. After that, ForecastICU monitors a new patient in real-time by observing her physiological data stream, updating its belief about her status over time, and prompting an alarm whenever its belief process hits a predefined threshold (confidence). Using a real-world dataset obtained from UCLA Ronald Reagan Medical Center, we show that ForecastICU can predict ICU admissions 9 hours before a physician\u2019s decision (for a sensitivity of 40% and a precision of 50%). Also, ForecastICU performs consistently better than other state-of-the-art machine learning algorithms in terms of sensitivity, precision, and timeliness: it can predict ICU admissions 3 hours earlier, and offers a 7.8% gain in sensitivity and a 5.1% gain in precision compared to the best state-of-the-art algorithm. Moreover, ForecastICU offers an area under curve (AUC) gain of 22.3% compared to the Rothman index, which is the currently deployed technology in most hospital wards. ",
    "code_link": ""
  },
  "icml2015_main_anoptimalalgorithmforthethresholdingbanditproblem": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "An optimal algorithm for the Thresholding Bandit Problem",
    "authors": [
      "Andrea Locatelli",
      "Maurilio Gutzeit",
      "Alexandra Carpentier"
    ],
    "page_url": "https://proceedings.mlr.press/v48/locatelli16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/locatelli16.pdf",
    "published": "2015-06",
    "summary": " We study a specific combinatorial pure exploration stochastic bandit problem where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and for a fixed time horizon. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with fixed budget for which provably optimal strategies are constructed. ",
    "code_link": ""
  },
  "icml2015_main_fastparameterinferenceinnonlineardynamicalsystemsusingiterativegradientmatching": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast Parameter Inference in Nonlinear Dynamical Systems using Iterative Gradient Matching",
    "authors": [
      "Mu Niu",
      "Simon Rogers",
      "Maurizio Filippone",
      "Dirk Husmeier"
    ],
    "page_url": "https://proceedings.mlr.press/v48/niu16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/niu16.pdf",
    "published": "2015-06",
    "summary": " Parameter inference in mechanistic models of coupled differential equations is a topical and challenging problem. We propose a new method based on kernel ridge regression and gradient matching, and an objective function that simultaneously encourages goodness of fit and penalises inconsistencies with the differential equations. Fast minimisation is achieved by exploiting partial convexity inherent in this function, and setting up an iterative algorithm in the vein of the EM algorithm. An evaluation of the proposed method on various benchmark data suggests that it compares favourably with state-of-the-art alternatives. ",
    "code_link": ""
  },
  "icml2015_main_structuredandefficientvariationaldeeplearningwithmatrixgaussianposteriors": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors",
    "authors": [
      "Christos Louizos",
      "Max Welling"
    ],
    "page_url": "https://proceedings.mlr.press/v48/louizos16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/louizos16.pdf",
    "published": "2015-06",
    "summary": " We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian (Gupta & Nagar \u201999) parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the \u201clocal reprarametrization trick\" (Kingma & Welling \u201915) on this posterior distribution we arrive at a Gaussian Process (Rasmussen \u201906) interpretation of the hidden units in each layer and we, similarly with (Gal & Ghahramani \u201915), provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate \u201cpseudo-data\u201d (Snelson & Ghahramani \u201905) in our model, which in turn allows for more efficient posterior sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments. ",
    "code_link": ""
  },
  "icml2015_main_learninggrangercausalityforhawkesprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Granger Causality for Hawkes Processes",
    "authors": [
      "Hongteng Xu",
      "Mehrdad Farajtabar",
      "Hongyuan Zha"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xuc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xuc16.pdf",
    "published": "2015-06",
    "summary": " Learning Granger causality for general point processes is a very challenging task. We propose an effective method learning Granger causality for a special but significant type of point processes \u2014 Hawkes processes. Focusing on Hawkes processes, we reveal the relationship between Hawkes process\u2019s impact functions and its Granger causality graph. Specifically, our model represents impact functions using a series of basis functions and recovers the Granger causality graph via group sparsity of the impact functions\u2019 coefficients. We propose an effective learning algorithm combining a maximum likelihood estimator (MLE) with a sparse-group-lasso (SGL) regularizer. Additionally, the pairwise similarity between the dimensions of the process is considered when their clustering structure is available. We analyze our learning method and discuss the selection of the basis functions. Experiments on synthetic data and real-world data show that our method can learn the Granger causality graph and the triggering patterns of Hawkes processes simultaneously. ",
    "code_link": ""
  },
  "icml2015_main_neuralvariationalinferencefortextprocessing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Neural Variational Inference for Text Processing",
    "authors": [
      "Yishu Miao",
      "Lei Yu",
      "Phil Blunsom"
    ],
    "page_url": "https://proceedings.mlr.press/v48/miao16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/miao16.pdf",
    "published": "2015-06",
    "summary": " Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks. ",
    "code_link": ""
  },
  "icml2015_main_dictionarylearningformassivematrixfactorization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dictionary Learning for Massive Matrix Factorization",
    "authors": [
      "Arthur Mensch",
      "Julien Mairal",
      "Bertrand Thirion",
      "Gael Varoquaux"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mensch16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mensch16.pdf",
    "published": "2015-06",
    "summary": " Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factorization method that scales gracefully to terabyte-scale datasets. Those could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods. ",
    "code_link": ""
  },
  "icml2015_main_pixelrecurrentneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Pixel Recurrent Neural Networks",
    "authors": [
      "A\u00e4ron van den Oord",
      "Nal Kalchbrenner",
      "Koray Kavukcuoglu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/oord16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/oord16.pdf",
    "published": "2015-06",
    "summary": " Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent. ",
    "code_link": ""
  },
  "icml2015_main_whymostdecisionsareeasyintetris\u2014andperhapsinothersequentialdecisionproblems,aswell": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Why Most Decisions Are Easy in Tetris\u2014And Perhaps in Other Sequential Decision Problems, As Well",
    "authors": [
      "\u00d6zg\u00fcr \u015eim\u015fek",
      "Sim\u00f3n Algorta",
      "Amit Kothiyal"
    ],
    "page_url": "https://proceedings.mlr.press/v48/simsek16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/simsek16.pdf",
    "published": "2015-06",
    "summary": " We examined the sequence of decision problems that are encountered in the game of Tetris and found that most of the problems are easy in the following sense: One can choose well among the available actions without knowing an evaluation function that scores well in the game. This is a consequence of three conditions that are prevalent in the game: simple dominance, cumulative dominance, and noncompensation. These conditions can be exploited to develop faster and more effective learning algorithms. In addition, they allow certain types of domain knowledge to be incorporated with ease into a learning algorithm. Among the sequential decision problems we encounter, it is unlikely that Tetris is unique or rare in having these properties. ",
    "code_link": ""
  },
  "icml2015_main_gaussianquadratureformatrixinverseformswithapplications": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Gaussian quadrature for matrix inverse forms with applications",
    "authors": [
      "Chengtao Li",
      "Suvrit Sra",
      "Stefanie Jegelka"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lig16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lig16.pdf",
    "published": "2015-06",
    "summary": " We present a framework for accelerating a spectrum of machine learning algorithms that require computation of \\emphbilinear inverse forms u^T A^-1u, where A is a positive definite matrix and u a given vector. Our framework is built on Gauss-type quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on u^T A^-1u, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances. ",
    "code_link": ""
  },
  "icml2015_main_trainandtesttightnessoflprelaxationsinstructuredprediction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Train and Test Tightness of LP Relaxations in Structured Prediction",
    "authors": [
      "Ofer Meshi",
      "Mehrdad Mahdavi",
      "Adrian Weller",
      "David Sontag"
    ],
    "page_url": "https://proceedings.mlr.press/v48/meshi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/meshi16.pdf",
    "published": "2015-06",
    "summary": " Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data. ",
    "code_link": ""
  },
  "icml2015_main_stochasticoptimizationformultiviewrepresentationlearningusingpartialleastsquares": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Optimization for Multiview Representation Learning using Partial Least Squares",
    "authors": [
      "Raman Arora",
      "Poorya Mianjy",
      "Teodor Marinov"
    ],
    "page_url": "https://proceedings.mlr.press/v48/aroraa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/aroraa16.pdf",
    "published": "2015-06",
    "summary": " Partial Least Squares (PLS) is a ubiquitous statistical technique for bilinear factor analysis. It is used in many data analysis, machine learning, and information retrieval applications to model the covariance structure between a pair of data matrices. In this paper, we consider PLS for representation learning in a multiview setting where we have more than one view in data at training time. Furthermore, instead of framing PLS as a problem about a fixed given data set, we argue that PLS should be studied as a stochastic optimization problem, especially in a \"big data\" setting, with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation (SA) approaches, such as Stochastic Gradient Descent (SGD) and enables a rigorous analysis of their benefits. In this paper, we develop SA approaches to PLS and provide iteration complexity bounds for the proposed algorithms. ",
    "code_link": ""
  },
  "icml2015_main_hierarchicalcompoundpoissonfactorization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hierarchical Compound Poisson Factorization",
    "authors": [
      "Mehmet Basbug",
      "Barbara Engelhardt"
    ],
    "page_url": "https://proceedings.mlr.press/v48/basbug16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/basbug16.pdf",
    "published": "2015-06",
    "summary": " Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF. ",
    "code_link": ""
  },
  "icml2015_main_opponentmodelingindeepreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Opponent Modeling in Deep Reinforcement Learning",
    "authors": [
      "He He",
      "Jordan Boyd-Graber",
      "Kevin Kwok",
      "Hal Daum\u00e9 III"
    ],
    "page_url": "https://proceedings.mlr.press/v48/he16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/he16.pdf",
    "published": "2015-06",
    "summary": " Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because of strategies\u2019 complex interaction and the non-stationary nature. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent\u2019s action, we encode observation of the opponents into a deep Q-Network (DQN), while retaining explicit modeling under multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents even without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants. ",
    "code_link": ""
  },
  "icml2015_main_nopenaltynotearsleastsquaresinhigh-dimensionallinearmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "No penalty no tears: Least squares in high-dimensional linear models",
    "authors": [
      "Xiangyu Wang",
      "David Dunson",
      "Chenlei Leng"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wange16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wange16.pdf",
    "published": "2015-06",
    "summary": " Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms. ",
    "code_link": ""
  },
  "icml2015_main_sdnastochasticdualnewtonascentforempiricalriskminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization",
    "authors": [
      "Zheng Qu",
      "Peter Richtarik",
      "Martin Takac",
      "Olivier Fercoq"
    ],
    "page_url": "https://proceedings.mlr.press/v48/qub16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/qub16.pdf",
    "published": "2015-06",
    "summary": " We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all local curvature information contained in the examples, which leads to striking improvements in both theory and practice \u2013 sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method. ",
    "code_link": ""
  },
  "icml2015_main_ongraduatedoptimizationforstochasticnon-convexproblems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On Graduated Optimization for Stochastic Non-Convex Problems",
    "authors": [
      "Elad Hazan",
      "Kfir Yehuda Levy",
      "Shai Shalev-Shwartz"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hazanb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hazanb16.pdf",
    "published": "2015-06",
    "summary": " The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade.Despite being popular, very little is known in terms of its theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a family of non-convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an \u03b5-approximate solution within O(1 / \u03b5^2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of \u201czero-order optimization\", and devise a variant of our algorithm which converges at rate of O(d^2/ \u03b5^4). ",
    "code_link": ""
  },
  "icml2015_main_meta-learningwithmemory-augmentedneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Meta-Learning with Memory-Augmented Neural Networks",
    "authors": [
      "Adam Santoro",
      "Sergey Bartunov",
      "Matthew Botvinick",
      "Daan Wierstra",
      "Timothy Lillicrap"
    ],
    "page_url": "https://proceedings.mlr.press/v48/santoro16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/santoro16.pdf",
    "published": "2015-06",
    "summary": " Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms. ",
    "code_link": ""
  },
  "icml2015_main_theknockofffilterforfdrcontrolingroup-sparseandmultitaskregression": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The knockoff filter for FDR control in group-sparse and multitask regression",
    "authors": [
      "Ran Dai",
      "Rina Barber"
    ],
    "page_url": "https://proceedings.mlr.press/v48/daia16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/daia16.pdf",
    "published": "2015-06",
    "summary": " We propose the group knockoff filter, a method for false discovery rate control in a linear regression setting where the features are grouped, and we would like to select a set of relevant groups which have a nonzero effect on the response. By considering the set of true and false discoveries at the group level, this method gains power relative to sparse regression methods. We also apply our method to the multitask regression problem where multiple response variables share similar sparsity patterns across the set of possible features. Empirically, the group knockoff filter successfully controls false discoveries at the group level in both settings, with substantially more discoveries made by leveraging the group structure. ",
    "code_link": ""
  },
  "icml2015_main_softenedapproximatepolicyiterationformarkovgames": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Softened Approximate Policy Iteration for Markov Games",
    "authors": [
      "Julien P\u00e9rolat",
      "Bilal Piot",
      "Matthieu Geist",
      "Bruno Scherrer",
      "Olivier Pietquin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/perolat16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/perolat16.pdf",
    "published": "2015-06",
    "summary": " This paper reports theoretical and empirical investigations on the use of quasi-Newton methods to minimize the Optimal Bellman Residual (OBR) of zero-sum two-player Markov Games. First, it reveals that state-of-the-art algorithms can be derived by the direct application of Newton\u2019s method to different norms of the OBR. More precisely, when applied to the norm of the OBR, Newton\u2019s method results in the Bellman Residual Minimization Policy Iteration (BRMPI) and, when applied to the norm of the Projected OBR (POBR), it results into the standard Least Squares Policy Iteration (LSPI) algorithm. Consequently, new algorithms are proposed, making use of quasi-Newton methods to minimize the OBR and the POBR so as to take benefit of enhanced empirical performances at low cost. Indeed, using a quasi-Newton method approach introduces slight modifications in term of coding of LSPI and BRMPI but improves significantly both the stability and the performance of those algorithms. These phenomena are illustrated on an experiment conducted on artificially constructed games called Garnets. ",
    "code_link": ""
  },
  "icml2015_main_stochasticblockbfgssqueezingmorecurvatureoutofdata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Block BFGS: Squeezing More Curvature out of Data",
    "authors": [
      "Robert Gower",
      "Donald Goldfarb",
      "Peter Richtarik"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gower16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gower16.pdf",
    "published": "2015-06",
    "summary": " We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods. ",
    "code_link": ""
  },
  "icml2015_main_differentialgeometricregularizationforsupervisedlearningofclassifiers": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Differential Geometric Regularization for Supervised Learning of Classifiers",
    "authors": [
      "Qinxun Bai",
      "Steven Rosenberg",
      "Zheng Wu",
      "Stan Sclaroff"
    ],
    "page_url": "https://proceedings.mlr.press/v48/baia16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/baia16.pdf",
    "published": "2015-06",
    "summary": " We study the problem of supervised learning for both binary and multiclass classification from a unified geometric perspective. In particular, we propose a geometric regularization technique to find the submanifold corresponding to an estimator of the class probability P(y|\\vec x). The regularization term measures the volume of this submanifold, based on the intuition that overfitting produces rapid local oscillations and hence large volume of the estimator. This technique can be applied to regularize any classification function that satisfies two requirements: firstly, an estimator of the class probability can be obtained; secondly, first and second derivatives of the class probability estimator can be calculated. In experiments, we apply our regularization technique to standard loss functions for classification, our RBF-based implementation compares favorably to widely used regularization methods for both binary and multiclass classification. ",
    "code_link": ""
  },
  "icml2015_main_exploitingcyclicsymmetryinconvolutionalneuralnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Exploiting Cyclic Symmetry in Convolutional Neural Networks",
    "authors": [
      "Sander Dieleman",
      "Jeffrey De Fauw",
      "Koray Kavukcuoglu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/dieleman16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/dieleman16.pdf",
    "published": "2015-06",
    "summary": " Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models. ",
    "code_link": ""
  },
  "icml2015_main_grayingtheblackboxunderstandingdqns": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Graying the black box: Understanding DQNs",
    "authors": [
      "Tom Zahavy",
      "Nir Ben-Zrihem",
      "Shie Mannor"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zahavy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zahavy16.pdf",
    "published": "2015-06",
    "summary": " In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning. ",
    "code_link": ""
  },
  "icml2015_main_thesum-producttheoremafoundationforlearningtractablemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Sum-Product Theorem: A Foundation for Learning Tractable Models",
    "authors": [
      "Abram Friesen",
      "Pedro Domingos"
    ],
    "page_url": "https://proceedings.mlr.press/v48/friesen16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/friesen16.pdf",
    "published": "2015-06",
    "summary": " Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization. ",
    "code_link": ""
  },
  "icml2015_main_paretofrontierlearningwithexpensivecorrelatedobjectives": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Pareto Frontier Learning with Expensive Correlated Objectives",
    "authors": [
      "Amar Shah",
      "Zoubin Ghahramani"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shahc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shahc16.pdf",
    "published": "2015-06",
    "summary": " There has been a surge of research interest in developing tools and analysis for Bayesian optimization, the task of finding the global maximizer of an unknown, expensive function through sequential evaluation using Bayesian decision theory. However, many interesting problems involve optimizing multiple, expensive to evaluate objectives simultaneously, and relatively little research has addressed this setting from a Bayesian theoretic standpoint. A prevailing choice when tackling this problem, is to model the multiple objectives as being independent, typically for ease of computation. In practice, objectives are correlated to some extent. In this work, we incorporate the modelling of inter-task correlations, developing an approximation to overcome intractable integrals. We illustrate the power of modelling dependencies between objectives on a range of synthetic and real world multi-objective optimization problems. ",
    "code_link": ""
  },
  "icml2015_main_asynchronousmethodsfordeepreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Asynchronous Methods for Deep Reinforcement Learning",
    "authors": [
      "Volodymyr Mnih",
      "Adria Puigdomenech Badia",
      "Mehdi Mirza",
      "Alex Graves",
      "Timothy Lillicrap",
      "Tim Harley",
      "David Silver",
      "Koray Kavukcuoglu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mniha16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mniha16.pdf",
    "published": "2015-06",
    "summary": " We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input. ",
    "code_link": ""
  },
  "icml2015_main_asimpleandstrongly-localflow-basedmethodforcutimprovement": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Simple and Strongly-Local Flow-Based Method for Cut Improvement",
    "authors": [
      "Nate Veldt",
      "David Gleich",
      "Michael Mahoney"
    ],
    "page_url": "https://proceedings.mlr.press/v48/veldt16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/veldt16.pdf",
    "published": "2015-06",
    "summary": " Many graph-based learning problems can be cast as finding a good set of vertices nearby a seed set, and a powerful methodology for these problems is based on minimum cuts and maximum flows. We introduce and analyze a new method for locally-biased graph-based learning called SimpleLocal, which finds good conductance cuts near a set of seed vertices. An important feature of our algorithm is that it is strongly-local, meaning it does not need to explore the entire graph to find cuts that are locally optimal. This method is related to other strongly-local flow-based methods, but it enables a simple implementation. We also show how it achieves localization through an implicit l1-norm penalty term. As a flow-based method, our algorithm exhibits several advantages in terms of cut optimality and accurate identification of target regions in a graph. We demonstrate the power of SimpleLocal solving segmentation problems on a 467 million edge graph based on an MRI scan. ",
    "code_link": ""
  },
  "icml2015_main_nonlinearstatisticallearningwithtruncatedgaussiangraphicalmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Nonlinear Statistical Learning with Truncated Gaussian Graphical Models",
    "authors": [
      "Qinliang Su",
      "Xuejun Liao",
      "Changyou Chen",
      "Lawrence Carin"
    ],
    "page_url": "https://proceedings.mlr.press/v48/su16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/su16.pdf",
    "published": "2015-06",
    "summary": " We introduce the truncated Gaussian graphical model (TGGM) as a novel framework for designing statistical models for nonlinear learning. A TGGM is a Gaussian graphical model (GGM) with a subset of variables truncated to be nonnegative. The truncated variables are assumed latent and integrated out to induce a marginal model. We show that the variables in the marginal model are non-Gaussian distributed and their expected relations are nonlinear. We use expectation-maximization to break the inference of the nonlinear model into a sequence of TGGM inference problems, each of which is efficiently solved by using the properties and numerical methods of multivariate Gaussian distributions. We use the TGGM to design models for nonlinear regression and classification, with the performances of these models demonstrated on extensive benchmark datasets and compared to state-of-the-art competing results. ",
    "code_link": ""
  },
  "icml2015_main_barronandcover\u2019stheoryinsupervisedlearninganditsapplicationtolasso": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Barron and Cover\u2019s Theory in Supervised Learning and its Application to Lasso",
    "authors": [
      "Masanori Kawakita",
      "Jun\u2019ichi Takeuchi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kawakita16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kawakita16.pdf",
    "published": "2015-06",
    "summary": " We study Barron and Cover\u2019s theory (BC theory) in supervised learning. The original BC theory can be applied to supervised learning only approximately and limitedly. Though Barron (2008) and Chatterjee and Barron (2014) succeeded in removing the approximation, their idea cannot be essentially applied to supervised learning in general. By solving this issue, we propose an extension of BC theory to supervised learning. The extended theory has several advantages inherited from the original BC theory. First, it holds for finite sample number n. Second, it requires remarkably few assumptions. Third, it gives a justification of the MDL principle in supervised learning. We also derive new risk and regret bounds of lasso with random design as its application. The derived risk bound hold for any finite n without boundedness of features in contrast to past work. Behavior of the regret bound is investigated by numerical simulations. We believe that this is the first extension of BC theory to general supervised learning without approximation. ",
    "code_link": ""
  },
  "icml2015_main_nonparametriccanonicalcorrelationanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Nonparametric Canonical Correlation Analysis",
    "authors": [
      "Tomer Michaeli",
      "Weiran Wang",
      "Karen Livescu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/michaeli16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/michaeli16.pdf",
    "published": "2015-06",
    "summary": " Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster\u2019s theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA. ",
    "code_link": ""
  },
  "icml2015_main_bistroanefficientrelaxation-basedmethodforcontextualbandits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits",
    "authors": [
      "Alexander Rakhlin",
      "Karthik Sridharan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rakhlin16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rakhlin16.pdf",
    "published": "2015-06",
    "summary": " We present efficient algorithms for the problem of contextual bandits with i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of policies. Our algorithm BISTRO requires d calls to the empirical risk minimization (ERM) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the ERM problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the ERM. The integrality gap of the relaxation only enters in the regret bound rather than the benchmark. Finally, we show that the adversarial version of the contextual bandit problem is learnable (and efficient) whenever the full-information supervised online learning problem has a non-trivial regret bound (and efficient). ",
    "code_link": ""
  },
  "icml2015_main_associativelongshort-termmemory": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Associative Long Short-Term Memory",
    "authors": [
      "Ivo Danihelka",
      "Greg Wayne",
      "Benigno Uria",
      "Nal Kalchbrenner",
      "Alex Graves"
    ],
    "page_url": "https://proceedings.mlr.press/v48/danihelka16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/danihelka16.pdf",
    "published": "2015-06",
    "summary": " We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks. ",
    "code_link": ""
  },
  "icml2015_main_duelingnetworkarchitecturesfordeepreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dueling Network Architectures for Deep Reinforcement Learning",
    "authors": [
      "Ziyu Wang",
      "Tom Schaul",
      "Matteo Hessel",
      "Hado Hasselt",
      "Marc Lanctot",
      "Nando Freitas"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wangf16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wangf16.pdf",
    "published": "2015-06",
    "summary": " In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain. ",
    "code_link": ""
  },
  "icml2015_main_persistenceweightedgaussiankernelfortopologicaldataanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Persistence weighted Gaussian kernel for topological data analysis",
    "authors": [
      "Genki Kusano",
      "Yasuaki Hiraoka",
      "Kenji Fukumizu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kusano16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kusano16.pdf",
    "published": "2015-06",
    "summary": " Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complex data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, and can distinguish robust and noisy topological properties. This paper proposes a kernel method on persistence diagrams to develop a statistical framework in TDA. The proposed kernel satisfies the stability property and provides explicit control on the effect of persistence. Furthermore, the method allows a fast approximation technique. The method is applied into practical data on proteins and oxide glasses, and the results show the advantage of our method compared to other relevant methods on persistence diagrams. ",
    "code_link": ""
  },
  "icml2015_main_learningconvolutionalneuralnetworksforgraphs": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Convolutional Neural Networks for Graphs",
    "authors": [
      "Mathias Niepert",
      "Mohamed Ahmed",
      "Konstantin Kutzkov"
    ],
    "page_url": "https://proceedings.mlr.press/v48/niepert16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/niepert16.pdf",
    "published": "2015-06",
    "summary": " Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient. ",
    "code_link": ""
  },
  "icml2015_main_persistentrnnsstashingrecurrentweightson-chip": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Persistent RNNs: Stashing Recurrent Weights On-Chip",
    "authors": [
      "Greg Diamos",
      "Shubho Sengupta",
      "Bryan Catanzaro",
      "Mike Chrzanowski",
      "Adam Coates",
      "Erich Elsen",
      "Jesse Engel",
      "Awni Hannun",
      "Sanjeev Satheesh"
    ],
    "page_url": "https://proceedings.mlr.press/v48/diamos16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/diamos16.pdf",
    "published": "2015-06",
    "summary": " This paper introduces a new technique for mapping Deep Recurrent Neural Networks (RNN) efficiently onto GPUs. We show how it is possi- ble to achieve substantially higher computational throughput at low mini-batch sizes than direct implementations of RNNs based on matrix multiplications. The key to our approach is the use of persistent computational kernels that exploit the GPU\u2019s inverted memory hierarchy to reuse network weights over multiple timesteps. Our initial implementation sustains 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU. This provides a 16x reduction in activation memory footprint, enables model training with 12x more parameters on the same hardware, allows us to strongly scale RNN training to 128 GPUs, and allows us to efficiently explore end-to-end speech recognition models with over 100 layers. ",
    "code_link": ""
  },
  "icml2015_main_recurrentorthogonalnetworksandlong-memorytasks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Recurrent Orthogonal Networks and Long-Memory Tasks",
    "authors": [
      "Mikael Henaff",
      "Arthur Szlam",
      "Yann LeCun"
    ],
    "page_url": "https://proceedings.mlr.press/v48/henaff16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/henaff16.pdf",
    "published": "2015-06",
    "summary": " Although RNNs have been shown to be power- ful tools for processing sequential data, finding architectures or optimization strategies that al- low them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets orig- inally outlined in (Hochreiter & Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illumi- nate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions fur- thermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices. ",
    "code_link": ""
  },
  "icml2015_main_thearrowoftimeinmultivariatetimeseries": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Arrow of Time in Multivariate Time Series",
    "authors": [
      "Stefan Bauer",
      "Bernhard Sch\u00f6lkopf",
      "Jonas Peters"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bauer16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bauer16.pdf",
    "published": "2015-06",
    "summary": " We prove that a time series satisfying a (linear) multivariate autoregressive moving average (VARMA) model satisfies the same model assumption in the reversed time direction, too, if all innovations are normally distributed. This reversibility breaks down if the innovations are non-Gaussian. This means that under the assumption of a VARMA process with non-Gaussian noise, the arrow of time becomes detectable. Our work thereby provides a theoretic justification of an algorithm that has been used for inferring the direction of video snippets. We present a slightly modified practical algorithm that estimates the time direction for a given sample and prove its consistency. We further investigate how the performance of the algorithm depends on sample size, number of dimensions of the time series and the order of the process. An application to real world data from economics shows that considering multivariate processes instead of univariate processes can be beneficial for estimating the time direction. Our result extends earlier work on univariate time series. It relates to the concept of causal inference, where recent methods exploit non-Gaussianity of the error terms for causal structure learning. ",
    "code_link": ""
  },
  "icml2015_main_mixtureproportionestimationviakernelembeddingsofdistributions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Mixture Proportion Estimation via Kernel Embeddings of Distributions",
    "authors": [
      "Harish Ramaswamy",
      "Clayton Scott",
      "Ambuj Tewari"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ramaswamy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ramaswamy16.pdf",
    "published": "2015-06",
    "summary": " Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key part in many \"weakly supervised learning\" problems like learning with positive and unlabelled samples, learning with label noise, anomaly detection and crowdsourcing. While there have been several methods proposed to solve this problem, to the best of our knowledge no efficient algorithm with a proven convergence rate towards the true proportion exists for this problem. We fill this gap by constructing a provably correct algorithm for MPE, and derive convergence rates under certain assumptions on the distribution. Our method is based on embedding distributions onto an RKHS, and implementing it only requires solving a simple convex quadratic programming problem a few times. We run our algorithm on several standard classification datasets, and demonstrate that it performs comparably to or better than other algorithms on most datasets. ",
    "code_link": ""
  },
  "icml2015_main_fastdppsamplingfornystromwithapplicationtokernelmethods": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast DPP Sampling for Nystrom with Application to Kernel Methods",
    "authors": [
      "Chengtao Li",
      "Stefanie Jegelka",
      "Suvrit Sra"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lih16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lih16.pdf",
    "published": "2015-06",
    "summary": " The Nystrom method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystrom using Determinantal Point Processes (DPPs), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via DPPs guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of DPP sampling, we show that (under certain conditions) Markov chain DPP sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of DPP-based landmark selection compared with existing approaches. ",
    "code_link": ""
  },
  "icml2015_main_complexembeddingsforsimplelinkprediction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Complex Embeddings for Simple Link Prediction",
    "authors": [
      "Th\u00e9o Trouillon",
      "Johannes Welbl",
      "Sebastian Riedel",
      "Eric Gaussier",
      "Guillaume Bouchard"
    ],
    "page_url": "https://proceedings.mlr.press/v48/trouillon16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/trouillon16.pdf",
    "published": "2015-06",
    "summary": " In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks. ",
    "code_link": "https://github.com/ttrouill/complex"
  },
  "icml2015_main_interactivebayesianhierarchicalclustering": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Interactive Bayesian Hierarchical Clustering",
    "authors": [
      "Sharad Vikram",
      "Sanjoy Dasgupta"
    ],
    "page_url": "https://proceedings.mlr.press/v48/vikram16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/vikram16.pdf",
    "published": "2015-06",
    "summary": " Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user\u2019s needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data. ",
    "code_link": ""
  },
  "icml2015_main_aconvolutionalattentionnetworkforextremesummarizationofsourcecode": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
    "authors": [
      "Miltiadis Allamanis",
      "Hao Peng",
      "Charles Sutton"
    ],
    "page_url": "https://proceedings.mlr.press/v48/allamanis16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/allamanis16.pdf",
    "published": "2015-06",
    "summary": " Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model\u2019s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network\u2019s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. ",
    "code_link": ""
  },
  "icml2015_main_howtofakemultiplybyagaussianmatrix": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "How to Fake Multiply by a Gaussian Matrix",
    "authors": [
      "Michael Kapralov",
      "Vamsi Potluru",
      "David Woodruff"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kapralov16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kapralov16.pdf",
    "published": "2015-06",
    "summary": " Have you ever wanted to multiply an n \\times d matrix X, with n \u226bd, on the left by an m \\times n matrix \\tilde G of i.i.d. Gaussian random variables, but could not afford to do it because it was too slow? In this work we propose a new randomized m \\times n matrix T, for which one can compute T \u22c5X in only O(nnz(X)) + \\tilde O(m^1.5 \u22c5d^3) time, for which the total variation distance between the distributions T \u22c5X and \\tilde G \u22c5X is as small as desired, i.e., less than any positive constant. Here nnz(X) denotes the number of non-zero entries of X. Assuming nnz(X) \u226bm^1.5 \u22c5d^3, this is a significant savings over the na\u00efve O(nnz(X) m) time to compute \\tilde G \u22c5X. Moreover, since the total variation distance is small, we can provably use T \u22c5X in place of \\tilde G \u22c5X in any application and have the same guarantees as if we were using \\tilde G \u22c5X, up to a small positive constant in error probability. We apply this transform to nonnegative matrix factorization (NMF) and support vector machines (SVM). ",
    "code_link": "https://github.com/marinkaz/nimfa"
  },
  "icml2015_main_differentiallyprivatechi-squaredhypothesistestinggoodnessoffitandindependencetesting": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit and Independence Testing",
    "authors": [
      "Marco Gaboardi",
      "Hyun Lim",
      "Ryan Rogers",
      "Salil Vadhan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rogers16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rogers16.pdf",
    "published": "2015-06",
    "summary": " Hypothesis testing is a useful statistical tool in determining whether a given model should be rejected based on a sample from the population. Sample data may contain sensitive information about individuals, such as medical information. Thus it is important to design statistical tests that guarantee the privacy of subjects in the data. In this work, we study hypothesis testing subject to differential privacy, specifically chi-squared tests for goodness of fit for multinomial data and independence between two categorical variables. ",
    "code_link": ""
  },
  "icml2015_main_pliablerejectionsampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Pliable Rejection Sampling",
    "authors": [
      "Akram Erraqabi",
      "Michal Valko",
      "Alexandra Carpentier",
      "Odalric Maillard"
    ],
    "page_url": "https://proceedings.mlr.press/v48/erraqabi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/erraqabi16.pdf",
    "published": "2015-06",
    "summary": " Rejection sampling is a technique for sampling from difficult distributions. However, its use is limited due to a high rejection rate. Common adaptive rejection sampling methods either work only for very specific distributions or without performance guarantees. In this paper, we present pliable rejection sampling (PRS), a new approach to rejection sampling, where we learn the sampling proposal using a kernel estimator. Since our method builds on rejection sampling, the samples obtained are with high probability i.i.d. and distributed according to f. Moreover, PRS comes with a guarantee on the number of accepted samples. ",
    "code_link": ""
  },
  "icml2015_main_differentiallyprivatepolicyevaluation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Differentially Private Policy Evaluation",
    "authors": [
      "Borja Balle",
      "Maziar Gomrokchi",
      "Doina Precup"
    ],
    "page_url": "https://proceedings.mlr.press/v48/balle16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/balle16.pdf",
    "published": "2015-06",
    "summary": " We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples. ",
    "code_link": ""
  },
  "icml2015_main_data-efficientoff-policypolicyevaluationforreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning",
    "authors": [
      "Philip Thomas",
      "Emma Brunskill"
    ],
    "page_url": "https://proceedings.mlr.press/v48/thomasa16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/thomasa16.pdf",
    "published": "2015-06",
    "summary": " In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods\u2014it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang & Li, 2015), and a new way to mix between model based and importance sampling based estimates. ",
    "code_link": ""
  },
  "icml2015_main_discretedeepfeatureextractionatheoryandnewarchitectures": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Discrete Deep Feature Extraction: A Theory and New Architectures",
    "authors": [
      "Thomas Wiatowski",
      "Michael Tschannen",
      "Aleksandar Stanic",
      "Philipp Grohs",
      "Helmut Boelcskei"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wiatowski16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wiatowski16.pdf",
    "published": "2015-06",
    "summary": " First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made\u2014for the continuous-time case\u2014in Mallat, 2012, and Wiatowski and B\u00f6lcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection\u2014including feature importance evaluation\u2014complement the theoretical findings. ",
    "code_link": ""
  },
  "icml2015_main_efficientalgorithmsforadversarialcontextuallearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Algorithms for Adversarial Contextual Learning",
    "authors": [
      "Vasilis Syrgkanis",
      "Akshay Krishnamurthy",
      "Robert Schapire"
    ],
    "page_url": "https://proceedings.mlr.press/v48/syrgkanis16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/syrgkanis16.pdf",
    "published": "2015-06",
    "summary": " We provide the first oracle efficient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently on one of the contexts in the set. Our algorithms fall into the Follow-The-Perturbed-Leader family (Kalai and Vempala, 2005) and achieve regret O(T^3/4\\sqrtK\\log(N)) in the transductive setting and O(T^2/3 d^3/4 K\\sqrt\\log(N)) in the separator setting, where T is the number of rounds, K is the number of actions, N is the number of baseline policies, and d is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences. ",
    "code_link": ""
  },
  "icml2015_main_trainingdeepneuralnetworksviadirectlossminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Training Deep Neural Networks via Direct Loss Minimization",
    "authors": [
      "Yang Song",
      "Alexander Schwing",
      "Richard",
      "Raquel Urtasun"
    ],
    "page_url": "https://proceedings.mlr.press/v48/songb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/songb16.pdf",
    "published": "2015-06",
    "summary": " Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on metrics specific to the application. In this paper we propose a direct loss minimization approach to train deep neural networks, which provably minimizes the application-specific loss function. This is often non-trivial, since these functions are neither smooth nor decomposable and thus are not amenable to optimization with standard gradient-based methods. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we develop a novel dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection, especially in the presence of label noise. ",
    "code_link": ""
  },
  "icml2015_main_sequencetosequencetrainingofctc-rnnswithpartialwindowing": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Sequence to Sequence Training of CTC-RNNs with Partial Windowing",
    "authors": [
      "Kyuyeon Hwang",
      "Wonyong Sung"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hwanga16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hwanga16.pdf",
    "published": "2015-06",
    "summary": " Connectionist temporal classification (CTC) based supervised sequence training of recurrent neural networks (RNNs) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition. For the CTC training, however, it is required to unroll (or unfold) the RNN by the length of an input sequence. This unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation. Furthermore, the length of training sequences is usually not uniform, which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs). In this work, we introduce an expectation-maximization (EM) based online CTC algorithm that enables unidirectional RNNs to learn sequences that are longer than the amount of unrolling. The RNNs can also be trained to process an infinitely long input sequence without pre-segmentation or external reset. Moreover, the proposed approach allows efficient parallel training on GPUs. Our approach achieves 20.7% phoneme error rate (PER) on the very long input sequence that is generated by concatenating all 192 utterances in the TIMIT core test set. In the end-to-end speech recognition task on the Wall Street Journal corpus, a network can be trained with only 64 times of unrolling with little performance loss. ",
    "code_link": ""
  },
  "icml2015_main_variationalinferenceformontecarloobjectives": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Variational Inference for Monte Carlo Objectives",
    "authors": [
      "Andriy Mnih",
      "Danilo Rezende"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mnihb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mnihb16.pdf",
    "published": "2015-06",
    "summary": " Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators. ",
    "code_link": ""
  },
  "icml2015_main_hierarchicaldecisionmakinginelectricitygridmanagement": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Hierarchical Decision Making In Electricity Grid Management",
    "authors": [
      "Gal Dalal",
      "Elad Gilboa",
      "Shie Mannor"
    ],
    "page_url": "https://proceedings.mlr.press/v48/dalal16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/dalal16.pdf",
    "published": "2015-06",
    "summary": " The power grid is a complex and vital system that necessitates careful reliability management. Managing the grid is a difficult problem with multiple time scales of decision making and stochastic behavior due to renewable energy generations, variable demand and unplanned outages. Solving this problem in the face of uncertainty requires a new methodology with tractable algorithms. In this work, we introduce a new model for hierarchical decision making in complex systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a level of abstraction, for real-time power grid reliability. We devise an algorithm that alternates between slow time-scale policy improvement, and fast time-scale value function approximation. We compare our results to prevailing heuristics, and show the strength of our method. ",
    "code_link": "https://github.com/galdl/icml16_iapi"
  },
  "icml2015_main_learningsparsecombinatorialrepresentationsviatwo-stagesubmodularmaximization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Sparse Combinatorial Representations via Two-stage Submodular Maximization",
    "authors": [
      "Eric Balkanski",
      "Baharan Mirzasoleiman",
      "Andreas Krause",
      "Yaron Singer"
    ],
    "page_url": "https://proceedings.mlr.press/v48/balkanski16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/balkanski16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of learning sparse representations of data sets, where the goal is to reduce a data set in manner that optimizes multiple objectives. Motivated by applications of data summarization, we develop a new model which we refer to as the two-stage submodular maximization problem. This task can be viewed as a combinatorial analogue of representation learning problems such as dictionary learning and sparse regression. The two-stage problem strictly generalizes the problem of cardinality constrained submodular maximization, though the objective function is not submodular and the techniques for submodular maximization cannot be applied. We describe a continuous optimization method which achieves an approximation ratio which asymptotically approaches 1-1/e. For instances where the asymptotics do not kick in, we design a local-search algorithm whose approximation ratio is arbitrarily close to 1/2. We empirically demonstrate the effectiveness of our methods on two multi-objective data summarization tasks, where the goal is to construct summaries via sparse representative subsets w.r.t. to predefined objectives. ",
    "code_link": ""
  },
  "icml2015_main_understandingandimprovingconvolutionalneuralnetworksviaconcatenatedrectifiedlinearunits": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units",
    "authors": [
      "Wenling Shang",
      "Kihyuk Sohn",
      "Diogo Almeida",
      "Honglak Lee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/shang16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/shang16.pdf",
    "published": "2015-06",
    "summary": " Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CReLU) and theoretically analyze its reconstruction property in CNNs. We integrate CReLU into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification. ",
    "code_link": ""
  },
  "icml2015_main_isotonichawkesprocesses": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Isotonic Hawkes Processes",
    "authors": [
      "Yichen Wang",
      "Bo Xie",
      "Nan Du",
      "Le Song"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wangg16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wangg16.pdf",
    "published": "2015-06",
    "summary": " Hawkes processes are powerful tools for modeling the mutual-excitation phenomena commonly observed in event data from a variety of domains, such as social networks, quantitative finance and healthcare records. The intensity function of a Hawkes process is typically assumed to be linear in the sum of triggering kernels, rendering it inadequate to capture nonlinear effects present in real-world data. To address this shortcoming, we propose an Isotonic-Hawkes process whose intensity function is modulated by an additional nonlinear link function. We also developed a novel iterative algorithm which learns both the nonlinear link function and other parameters provably. We showed that Isotonic-Hawkes processes can fit a variety of nonlinear patterns which cannot be captured by conventional Hawkes processes, and achieve superior empirical performance in real world applications. ",
    "code_link": ""
  },
  "icml2015_main_cross-graphlearningofmulti-relationalassociations": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Cross-Graph Learning of Multi-Relational Associations",
    "authors": [
      "Hanxiao Liu",
      "Yiming Yang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liuf16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liuf16.pdf",
    "published": "2015-06",
    "summary": " Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a constant time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly. ",
    "code_link": ""
  },
  "icml2015_main_markov-modulatedmarkedpoissonprocessesforcheck-indata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Markov-modulated Marked Poisson Processes for Check-in Data",
    "authors": [
      "Jiangwei Pan",
      "Vinayak Rao",
      "Pankaj Agarwal",
      "Alan Gelfand"
    ],
    "page_url": "https://proceedings.mlr.press/v48/pana16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/pana16.pdf",
    "published": "2015-06",
    "summary": " We develop continuous-time probabilistic models to study trajectory data consisting of times and locations of user \u201ccheck-ins\u201d. We model the data as realizations of a marked point process, with intensity and mark-distribution modulated by a latent Markov jump process (MJP). We also include user-heterogeneity in our model by assigning each user a vector of \u201cpreferred locations\u201d. Our model extends latent Dirichlet allocation by dropping the bag-of-words assumption and operating in continuous time. We show how an appropriate choice of priors allows efficient posterior inference. Our experiments demonstrate the usefulness of our approach by comparing with various baselines on a variety of tasks. ",
    "code_link": ""
  },
  "icml2015_main_beyondparityconstraintsfourieranalysisofhashfunctionsforinference": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Beyond Parity Constraints: Fourier Analysis of Hash Functions for Inference",
    "authors": [
      "Tudor Achim",
      "Ashish Sabharwal",
      "Stefano Ermon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/achim16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/achim16.pdf",
    "published": "2015-06",
    "summary": " Random projections have played an important role in scaling up machine learning and data mining algorithms. Recently they have also been applied to probabilistic inference to estimate properties of high-dimensional distributions; however, they all rely on the same class of projections based on universal hashing. We provide a general framework to analyze random projections which relates their statistical properties to their Fourier spectrum, which is a well-studied area of theoretical computer science. Using this framework we introduce two new classes of hash functions for probabilistic inference and model counting that show promising performance on synthetic and real-world benchmarks. ",
    "code_link": ""
  },
  "icml2015_main_onthepowerandlimitsofdistance-basedlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "On the Power and Limits of Distance-Based Learning",
    "authors": [
      "Periklis Papakonstantinou",
      "Jia Xu",
      "Guang Yang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/papakonstantinou16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/papakonstantinou16.pdf",
    "published": "2015-06",
    "summary": " We initiate the study of low-distortion finite metric embeddings in multi-class (and multi-label) classification where (i) both the space of input instances and the space of output classes have combinatorial metric structure and (ii) the concepts we wish to learn are low-distortion embeddings. We develop new geometric techniques and prove strong learning lower bounds. These provable limits hold even when we allow learners and classifiers to get advice by one or more experts. Our study overwhelmingly indicates that post-geometry assumptions are necessary in multi-class classification, as in natural language processing (NLP). Technically, the mathematical tools we developed in this work could be of independent interest to NLP. To the best of our knowledge, this is the first work which formally studies classification problems in combinatorial spaces. and where the concepts are low-distortion embeddings. ",
    "code_link": ""
  },
  "icml2015_main_aconvexatomic-normapproachtomultiplesequencealignmentandmotifdiscovery": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Convex Atomic-Norm Approach to Multiple Sequence Alignment and Motif Discovery",
    "authors": [
      "Ian En-Hsu Yen",
      "Xin Lin",
      "Jiong Zhang",
      "Pradeep Ravikumar",
      "Inderjit Dhillon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yena16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yena16.pdf",
    "published": "2015-06",
    "summary": " Multiple Sequence Alignment and Motif Discovery, known as NP-hard problems, are two fundamental tasks in Bioinformatics. Existing approaches to these two problems are based on either local search methods such as Expectation Maximization (EM), Gibbs Sampling or greedy heuristic methods. In this work, we develop a convex relaxation approach to both problems based on the recent concept of atomic norm and develop a new algorithm, termed Greedy Direction Method of Multiplier, for solving the convex relaxation with two convex atomic constraints. Experiments show that our convex relaxation approach produces solutions of higher quality than those standard tools widely-used in Bioinformatics community on the Multiple Sequence Alignment and Motif Discovery problems. ",
    "code_link": ""
  },
  "icml2015_main_generalizeddirectchangeestimationinisingmodelstructure": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Generalized Direct Change Estimation in Ising Model Structure",
    "authors": [
      "Farideh Fazayeli",
      "Arindam Banerjee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/fazayeli16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/fazayeli16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of estimating change in the dependency structure of two p-dimensional Ising models, based on respectively n_1 and n_2 samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say n_2, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as \\fracc\\sqrt\\min(n_1,n_2), where c depends on the Gaussian width of the unit norm ball. For example, for \\ell_1 norm applied to s-sparse change, the change can be accurately estimated with \\min(n_1,n_2)=O(s \\log p) which is sharper than an existing result n_1= O(s^2 \\log p) and n_2 = O(n_1^2). Experimental results illustrating the effectiveness of the proposed estimator are presented. ",
    "code_link": ""
  },
  "icml2015_main_robustprincipalcomponentanalysiswithsideinformation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Robust Principal Component Analysis with Side Information",
    "authors": [
      "Kai-Yang Chiang",
      "Cho-Jui Hsieh",
      "Inderjit Dhillon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/chiang16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/chiang16.pdf",
    "published": "2015-06",
    "summary": " The robust principal component analysis (robust PCA) problem has been considered in many machine learning applications, where the goal is to decompose the data matrix as a low rank part plus a sparse residual. While current approaches are developed by only considering the low rank plus sparse structure, in many applications, side information of row and/or column entities may also be given, and it is still unclear to what extent could such information help robust PCA. Thus, in this paper, we study the problem of robust PCA with side information, where both prior structure and features of entities are exploited for recovery. We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions. In particular, our guarantee suggests that a substantial amount of low rank matrices, which cannot be recovered by standard robust PCA, become recoverable by our proposed method. The result theoretically justifies the effectiveness of features in robust PCA. In addition, we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information. ",
    "code_link": ""
  },
  "icml2015_main_towardsfasterratesandoraclepropertyforlow-rankmatrixestimation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation",
    "authors": [
      "Huan Gui",
      "Jiawei Han",
      "Quanquan Gu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gui16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gui16.pdf",
    "published": "2015-06",
    "summary": " We present a unified framework for low-rank matrix estimation with a nonconvex penalty. A proximal gradient homotopy algorithm is proposed to solve the proposed optimization problem. Theoretically, we first prove that the proposed estimator attains a faster statistical rate than the traditional low-rank matrix estimator with nuclear norm penalty. Moreover, we rigorously show that under a certain condition on the magnitude of the nonzero singular values, the proposed estimator enjoys oracle property (i.e., exactly recovers the true rank of the matrix), besides attaining a faster rate. Extensive numerical experiments on both synthetic and real world datasets corroborate our theoretical findings. ",
    "code_link": ""
  },
  "icml2015_main_earlyandreliableeventdetectionusingproximityspacerepresentation": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Early and Reliable Event Detection Using Proximity Space Representation",
    "authors": [
      "Maxime Sangnier",
      "Jerome Gauthier",
      "Alain Rakotomamonjy"
    ],
    "page_url": "https://proceedings.mlr.press/v48/sangnier16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/sangnier16.pdf",
    "published": "2015-06",
    "summary": " Let us consider a specific action or situation (called event) that takes place within a time series. The objective in early detection is to build a decision function that is able to go off as soon as possible from the onset of an occurrence of this event. This implies making a decision with an incomplete information. This paper proposes a novel framework that i) guarantees that a detection made with a partial observation will also occur at full observation of the time-series; ii) incorporates in a consistent manner the lack of knowledge about the minimal amount of information needed to make a decision. The proposed detector is based on mapping the temporal sequences to a landmarking space thanks to appropriately designed similarity functions. As a by-product, the framework benefits from a scalable training algorithm and a theoretical guarantee concerning its generalization ability. We also discuss an important improvement of our framework in which decision function can still be made reliable while being more expressive. Our experimental studies provide compelling results on toy data, presenting the trade-off that occurs when aiming at accuracy, earliness and reliability. Results on real physiological and video datasets show that our proposed approach is as accurate and early as state-of-the-art algorithm, while ensuring reliability and being far more efficient to learn. ",
    "code_link": ""
  },
  "icml2015_main_stratifiedsamplingmeetsmachinelearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stratified Sampling Meets Machine Learning",
    "authors": [
      "Edo Liberty",
      "Kevin Lang",
      "Konstantin Shmakov"
    ],
    "page_url": "https://proceedings.mlr.press/v48/liberty16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/liberty16.pdf",
    "published": "2015-06",
    "summary": " This paper solves a specialized regression problem to obtain sampling probabilities for records in databases. The goal is to sample a small set of records over which evaluating aggregate queries can be done both efficiently and accurately. We provide a principled and provable solution for this problem; it is parameterless and requires no data insights. Unlike standard regression problems, the loss is inversely proportional to the regressed-to values. Moreover, a cost zero solution always exists and can only be excluded by hard budget constraints. A unique form of regularization is also needed. We provide an efficient and simple regularized Empirical Risk Minimization (ERM) algorithm along with a theoretical generalization result. Our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards. ",
    "code_link": ""
  },
  "icml2015_main_efficientmulti-instancelearningforactivityrecognitionfromtimeseriesdatausinganauto-regressivehiddenmarkovmodel": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model",
    "authors": [
      "Xinze Guan",
      "Raviv Raich",
      "Weng-Keen Wong"
    ],
    "page_url": "https://proceedings.mlr.press/v48/guan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/guan16.pdf",
    "published": "2015-06",
    "summary": " Activity recognition from sensor data has spurred a great deal of interest due to its impact on health care. Prior work on activity recognition from multivariate time series data has mainly applied supervised learning techniques which require a high degree of annotation effort to produce training data with the start and end times of each activity. In order to reduce the annotation effort, we present a weakly supervised approach based on multi-instance learning. We introduce a generative graphical model for multi-instance learning on time series data based on an auto-regressive hidden Markov model. Our model has a number of advantages, including the ability to produce both bag and instance-level predictions as well as an efficient exact inference algorithm based on dynamic programming. ",
    "code_link": ""
  },
  "icml2015_main_generalizationpropertiesandimplicitregularizationformultiplepassessgm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Generalization Properties and Implicit Regularization for Multiple Passes SGM",
    "authors": [
      "Junhong Lin",
      "Raffaello Camoriano",
      "Lorenzo Rosasco"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lina16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lina16.pdf",
    "published": "2015-06",
    "summary": " We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings. ",
    "code_link": ""
  },
  "icml2015_main_principalcomponentprojectionwithoutprincipalcomponentanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Principal Component Projection Without Principal Component Analysis",
    "authors": [
      "Roy Frostig",
      "Cameron Musco",
      "Christopher Musco",
      "Aaron Sidford"
    ],
    "page_url": "https://proceedings.mlr.press/v48/frostig16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/frostig16.pdf",
    "published": "2015-06",
    "summary": " We show how to efficiently project a vector onto the top principal components of a matrix, *without explicitly computing these components*. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a \"smooth projection\" onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision. ",
    "code_link": ""
  },
  "icml2015_main_recoveryguaranteeofweightedlow-rankapproximationviaalternatingminimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization",
    "authors": [
      "Yuanzhi Li",
      "Yingyu Liang",
      "Andrej Risteski"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lii16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lii16.pdf",
    "published": "2015-06",
    "summary": " Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries, which in practice is typically formulated as a weighted low-rank approximation problem and solved by non-convex optimization heuristics such as alternating minimization. In this paper, we provide provable recovery guarantee of weighted low-rank via a simple alternating minimization algorithm. In particular, for a natural class of matrices and weights and without any assumption on the noise, we bound the spectral norm of the difference between the recovered matrix and the ground truth, by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. These provide the first theoretical results for weighted low-rank approximation via alternating minimization with non-binary deterministic weights, significantly generalizing those for matrix completion, the special case with binary weights, since our assumptions are similar or weaker than those made in existing works. Furthermore, this is achieved by a very simple algorithm that improves the vanilla alternating minimization with a simple clipping step. ",
    "code_link": ""
  },
  "icml2015_main_deconstructingtheladdernetworkarchitecture": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Deconstructing the Ladder Network Architecture",
    "authors": [
      "Mohammad Pezeshki",
      "Linxi Fan",
      "Philemon Brakel",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "page_url": "https://proceedings.mlr.press/v48/pezeshki16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/pezeshki16.pdf",
    "published": "2015-06",
    "summary": " The Ladder Network is a recent new approach to semi-supervised learning that turned out to be very successful. While showing impressive performance, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connections, followed by the application of noise, and the choice of what we refer to as the \u2018combinator function\u2019. As the number of labeled training examples increases, the lateral connections and the reconstruction criterion become less important, with most of the generalization improvement coming from the injection of noise in each layer. Finally, we introduce a combinator function that reduces test error rates on Permutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples, respectively. ",
    "code_link": ""
  },
  "icml2015_main_generalizationandexplorationviarandomizedvaluefunctions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Generalization and Exploration via Randomized Value Functions",
    "authors": [
      "Ian Osband",
      "Benjamin Van Roy",
      "Zheng Wen"
    ],
    "page_url": "https://proceedings.mlr.press/v48/osband16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/osband16.pdf",
    "published": "2015-06",
    "summary": " We propose randomized least-squares value iteration (RLSVI) \u2013 a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization. ",
    "code_link": ""
  },
  "icml2015_main_evasionandhardeningoftreeensembleclassifiers": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Evasion and Hardening of Tree Ensemble Classifiers",
    "authors": [
      "Alex Kantchelian",
      "J. D. Tygar",
      "Anthony Joseph"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kantchelian16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kantchelian16.pdf",
    "published": "2015-06",
    "summary": " Classifier evasion consists in finding for a given instance x the \u201cnearest\u201d instance x\u2019 such that the classifier predictions of x and x\u2019 are different. We present two novel algorithms for systematically computing evasions for tree ensembles such as boosted trees and random forests. Our first algorithm uses a Mixed Integer Linear Program solver and finds the optimal evading instance under an expressive set of constraints. Our second algorithm trades off optimality for speed by using symbolic prediction, a novel algorithm for fast finite differences on tree ensembles. On a digit recognition task, we demonstrate that both gradient boosted trees and random forests are extremely susceptible to evasions. Finally, we harden a boosted tree model without loss of predictive accuracy by augmenting the training set of each boosting round with evading instances, a technique we call adversarial boosting. ",
    "code_link": "https://github.com/dmlc/xgboost"
  },
  "icml2015_main_dynamicmemorynetworksforvisualandtextualquestionanswering": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dynamic Memory Networks for Visual and Textual Question Answering",
    "authors": [
      "Caiming Xiong",
      "Stephen Merity",
      "Richard Socher"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xiong16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xiong16.pdf",
    "published": "2015-06",
    "summary": " Neural network architectures with memory and attention mechanisms exhibit certain reason- ing capabilities required for question answering. One such architecture, the dynamic memory net- work (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision. ",
    "code_link": ""
  },
  "icml2015_main_estimatingcosmologicalparametersfromthedarkmatterdistribution": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Estimating Cosmological Parameters from the Dark Matter Distribution",
    "authors": [
      "Siamak Ravanbakhsh",
      "Junier Oliva",
      "Sebastian Fromenteau",
      "Layne Price",
      "Shirley Ho",
      "Jeff Schneider",
      "Barnabas Poczos"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ravanbakhshb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ravanbakhshb16.pdf",
    "published": "2015-06",
    "summary": " A grand challenge of the 21st century cosmology is to accurately estimate the cosmological parameters of our Universe. A major approach in estimating the cosmological parameters is to use the large scale matter distribution of the Universe. Galaxy surveys provide the means to map out cosmic large-scale structure in three dimensions. Information about galaxy locations is typically summarized in a \"single\" function of scale, such as the galaxy correlation function or power-spectrum. We show that it is possible to estimate these cosmological parameters directly from the distribution of matter. This paper presents the application of deep 3D convolutional networks to volumetric representation of dark matter simulations as well as the results obtained using a recently proposed distribution regression framework, showing that machine learning techniques are comparable to, and can sometimes outperform, maximum-likelihood point estimates using \"cosmological models\". This opens the way to estimating the parameters of our Universe with higher accuracy. ",
    "code_link": ""
  },
  "icml2015_main_learningpopulation-leveldiffusionswithgenerativernns": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Population-Level Diffusions with Generative RNNs",
    "authors": [
      "Tatsunori Hashimoto",
      "David Gifford",
      "Tommi Jaakkola"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hashimoto16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hashimoto16.pdf",
    "published": "2015-06",
    "summary": " We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the \u2018epigenetic landscape\u2019 from existing biological assays. ",
    "code_link": ""
  },
  "icml2015_main_expressivenessofrectifiernetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Expressiveness of Rectifier Networks",
    "authors": [
      "Xingyuan Pan",
      "Vivek Srikumar"
    ],
    "page_url": "https://proceedings.mlr.press/v48/panb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/panb16.pdf",
    "published": "2015-06",
    "summary": " Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient backpropagation, and empirically promote sparsity in the learned parameters. They have led to state-of-the-art results in a variety of applications. However, unlike threshold and sigmoid networks, ReLU networks are less explored from the perspective of their expressiveness. This paper studies the expressiveness of ReLU networks. We characterize the decision boundary of two-layer ReLU networks by constructing functionally equivalent threshold networks. We show that while the decision boundary of a two-layer ReLU network can be captured by a threshold network, the latter may require an exponentially larger number of hidden units. We also formulate sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, we experimentally compare threshold networks and their much smaller ReLU counterparts with respect to their ability to learn from synthetically generated data. ",
    "code_link": ""
  },
  "icml2015_main_discretedistributionestimationunderlocalprivacy": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Discrete Distribution Estimation under Local Privacy",
    "authors": [
      "Peter Kairouz",
      "Keith Bonawitz",
      "Daniel Ramage"
    ],
    "page_url": "https://proceedings.mlr.press/v48/kairouz16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/kairouz16.pdf",
    "published": "2015-06",
    "summary": " The collection and analysis of user data drives improvements in the app and web ecosystems, but comes with risks to privacy. This paper examines discrete distribution estimation under local privacy, a setting wherein service providers can learn the distribution of a categorical statistic of interest without collecting the underlying data. We present new mechanisms, including hashed k-ary Randomized Response (KRR), that empirically meet or exceed the utility of existing mechanisms at all privacy levels. New theoretical results demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at different privacy regimes. ",
    "code_link": ""
  },
  "icml2015_main_squarerootgraphicalmodelsmultivariategeneralizationsofunivariateexponentialfamiliesthatpermitpositivedependencies": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive Dependencies",
    "authors": [
      "David Inouye",
      "Pradeep Ravikumar",
      "Inderjit Dhillon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/inouye16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/inouye16.pdf",
    "published": "2015-06",
    "summary": " We develop Square Root Graphical Models (SQR), a novel class of parametric graphical models that provides multivariate generalizations of univariate exponential family distributions. Previous multivariate graphical models [Yang et al. 2015] did not allow positive dependencies for the exponential and Poisson generalizations. However, in many real-world datasets, variables clearly have positive dependencies. For example, the airport delay time in New York\u2014modeled as an exponential distribution\u2014is positively related to the delay time in Boston. With this motivation, we give an example of our model class derived from the univariate exponential distribution that allows for almost arbitrary positive and negative dependencies with only a mild condition on the parameter matrix\u2014a condition akin to the positive definiteness of the Gaussian covariance matrix. Our Poisson generalization allows for both positive and negative dependencies without any constraints on the parameter values. We also develop parameter estimation methods using node-wise regressions with \\ell_1 regularization and likelihood approximation methods using sampling. Finally, we demonstrate our exponential generalization on a synthetic dataset and a real-world dataset of airport delay times. ",
    "code_link": ""
  },
  "icml2015_main_abox-constrainedapproachforhardpermutationproblems": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Box-Constrained Approach for Hard Permutation Problems",
    "authors": [
      "Cong Han Lim",
      "Steve Wright"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lim16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lim16.pdf",
    "published": "2015-06",
    "summary": " We describe the use of sorting networks to form relaxations of problems involving permutations of n objects. This approach is an alternative to relaxations based on the Birkhoff polytope (the set of n \\times n doubly stochastic matrices), providing a more compact formulation in which the only constraints are box constraints. Using this approach, we form a variant of the relaxation of the quadratic assignment problem recently studied in Vogelstein et al. (2015), and show that the continuation method applied to this formulation can be quite effective. We develop a coordinate descent algorithm that achieves a per-cycle complexity of O(n^2 \\log^2 n). We compare this method with Fast Approximate QAP (FAQ) algorithm introduced in Vogelstein et al. (2015), which uses a conditional-gradient method whose per-iteration complexity is O(n^3). We demonstrate that for most problems in QAPLIB and for a class of synthetic QAP problems, the sorting-network formulation returns solutions that are competitive with the FAQ algorithm, often in significantly less computing time. ",
    "code_link": "https://github.com/jovo/FastApproximateQAP"
  },
  "icml2015_main_geometricmeanmetriclearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Geometric Mean Metric Learning",
    "authors": [
      "Pourya Zadeh",
      "Reshad Hosseini",
      "Suvrit Sra"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zadeh16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zadeh16.pdf",
    "published": "2015-06",
    "summary": " We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy. ",
    "code_link": ""
  },
  "icml2015_main_sparsenonlinearregressionparameterestimationundernonconvexity": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Sparse Nonlinear Regression: Parameter Estimation under Nonconvexity",
    "authors": [
      "Zhuoran Yang",
      "Zhaoran Wang",
      "Han Liu",
      "Yonina Eldar",
      "Tong Zhang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yangc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yangc16.pdf",
    "published": "2015-06",
    "summary": " We study parameter estimation for sparse nonlinear regression. More specifically, we assume the data are given by y = f( \\bf x^T \\bf \u03b2^* ) + \u03b5, where f is nonlinear. To recover \\bf \u03b2s, we propose an \\ell_1-regularized least-squares estimator. Unlike classical linear regression, the corresponding optimization problem is nonconvex because of the nonlinearity of f. In spite of the nonconvexity, we prove that under mild conditions, every stationary point of the objective enjoys an optimal statistical rate of convergence. Detailed numerical results are provided to back up our theory. ",
    "code_link": ""
  },
  "icml2015_main_conditionalbernoullimixturesformulti-labelclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Conditional Bernoulli Mixtures for Multi-label Classification",
    "authors": [
      "Cheng Li",
      "Bingyu Wang",
      "Virgil Pavlu",
      "Javed Aslam"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lij16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lij16.pdf",
    "published": "2015-06",
    "summary": " Multi-label classification is an important machine learning task wherein one assigns a subset of candidate labels to an object. In this paper, we propose a new multi-label classification method based on Conditional Bernoulli Mixtures. Our proposed method has several attractive properties: it captures label dependencies; it reduces the multi-label problem to several standard binary and multi-class problems; it subsumes the classic independent binary prediction and power-set subset prediction methods as special cases; and it exhibits accuracy and/or computational complexity advantages over existing approaches. We demonstrate two implementations of our method using logistic regressions and gradient boosted trees, together with a simple training procedure based on Expectation Maximization. We further derive an efficient prediction procedure based on dynamic programming, thus avoiding the cost of examining an exponential number of potential label subsets. Experimental results show the effectiveness of the proposed method against competitive alternatives on benchmark datasets. ",
    "code_link": ""
  },
  "icml2015_main_scalablediscretesamplingasamulti-armedbanditproblem": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Discrete Sampling as a Multi-Armed Bandit Problem",
    "authors": [
      "Yutian Chen",
      "Zoubin Ghahramani"
    ],
    "page_url": "https://proceedings.mlr.press/v48/chenb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/chenb16.pdf",
    "published": "2015-06",
    "summary": " Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems. ",
    "code_link": ""
  },
  "icml2015_main_recyclingrandomnesswithstructureforsublineartimekernelexpansions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Recycling Randomness with Structure for Sublinear time Kernel Expansions",
    "authors": [
      "Krzysztof Choromanski",
      "Vikas Sindhwani"
    ],
    "page_url": "https://proceedings.mlr.press/v48/choromanski16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/choromanski16.pdf",
    "published": "2015-06",
    "summary": " We propose a scheme for recycling Gaussian random vectors into structured matrices to ap- proximate various kernel functions in sublin- ear time via random embeddings. Our frame- work includes the Fastfood construction of Le et al. (2013) as a special case, but also ex- tends to Circulant, Toeplitz and Hankel matri- ces, and the broader family of structured matri- ces that are characterized by the concept of low- displacement rank. We introduce notions of co- herence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of ran- dom feature maps that arise within our frame- work. For the case of low-displacement matri- ces, we show how the degree of structure and randomness can be controlled to reduce statis- tical variance at the cost of increased computa- tion and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scal- ing up kernel methods using random features. ",
    "code_link": ""
  },
  "icml2015_main_bidirectionalhelmholtzmachines": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bidirectional Helmholtz Machines",
    "authors": [
      "Jorg Bornschein",
      "Samira Shabanian",
      "Asja Fischer",
      "Yoshua Bengio"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bornschein16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bornschein16.pdf",
    "published": "2015-06",
    "summary": " Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine or Variational Autoencoder, involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference. Recent results indicate that better generative models can be obtained with better approximate inference procedures. Instead of improving the inference procedure, we here propose a new model, the bidirectional Helmholtz machine, which guarantees that the top-down and bottom-up distributions can efficiently invert each other. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. This approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient likelihood estimation. ",
    "code_link": "https://github.com/jbornschein/bihm"
  },
  "icml2015_main_fasterconvexoptimizationsimulatedannealingwithanefficientuniversalbarrier": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Faster Convex Optimization: Simulated Annealing with an Efficient Universal Barrier",
    "authors": [
      "Jacob Abernethy",
      "Elad Hazan"
    ],
    "page_url": "https://proceedings.mlr.press/v48/abernethy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/abernethy16.pdf",
    "published": "2015-06",
    "summary": " This paper explores a surprising equivalence between two seemingly-distinct convex optimization methods. We show that simulated annealing, a well-studied random walk algorithms, is *directly equivalent*, in a certain sense, to the central path interior point algorithm for the the entropic universal barrier function. This connection exhibits several benefits. First, we are able improve the state of the art time complexity for convex optimization under the membership oracle model by devising a new temperature schedule for simulated annealing motivated by central path following interior point methods. Second, we get an efficient randomized interior point method with an efficiently computable universal barrier for any convex set described by a membership oracle. Previously, efficiently computable barriers were known only for particular convex sets. ",
    "code_link": ""
  },
  "icml2015_main_preconditioningkernelmatrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Preconditioning Kernel Matrices",
    "authors": [
      "Kurt Cutajar",
      "Michael Osborne",
      "John Cunningham",
      "Maurizio Filippone"
    ],
    "page_url": "https://proceedings.mlr.press/v48/cutajar16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/cutajar16.pdf",
    "published": "2015-06",
    "summary": " The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget. ",
    "code_link": "https://github.com/mauriziofilippone/preconditioned_GPs"
  },
  "icml2015_main_greedycolumnsubsetselectionnewboundsanddistributedalgorithms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Greedy Column Subset Selection: New Bounds and Distributed Algorithms",
    "authors": [
      "Jason Altschuler",
      "Aditya Bhaskara",
      "Gang Fu",
      "Vahab Mirrokni",
      "Afshin Rostamizadeh",
      "Morteza Zadimoghaddam"
    ],
    "page_url": "https://proceedings.mlr.press/v48/altschuler16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/altschuler16.pdf",
    "published": "2015-06",
    "summary": " The problem of column subset selection has recently attracted a large body of research, with feature selection serving as one obvious and important application. Among the techniques that have been applied to solve this problem, the greedy algorithm has been shown to be quite effective in practice. However, theoretical guarantees on its performance have not been explored thoroughly, especially in a distributed setting. In this paper, we study the greedy algorithm for the column subset selection problem from a theoretical and empirical perspective and show its effectiveness in a distributed setting. In particular, we provide an improved approximation guarantee for the greedy algorithm which we show is tight up to a constant factor, and present the first distributed implementation with provable approximation factors. We use the idea of randomized composable core-sets, developed recently in the context of submodular maximization. Finally, we validate the effectiveness of this distributed algorithm via an empirical study. ",
    "code_link": ""
  },
  "icml2015_main_dynamiccapacitynetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Dynamic Capacity Networks",
    "authors": [
      "Amjad Almahairi",
      "Nicolas Ballas",
      "Tim Cooijmans",
      "Yin Zheng",
      "Hugo Larochelle",
      "Aaron Courville"
    ],
    "page_url": "https://proceedings.mlr.press/v48/almahairi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/almahairi16.pdf",
    "published": "2015-06",
    "summary": " We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance. ",
    "code_link": ""
  },
  "icml2015_main_pricingalow-regretseller": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Pricing a Low-regret Seller",
    "authors": [
      "Hoda Heidari",
      "Mohammad Mahdian",
      "Umar Syed",
      "Sergei Vassilvitskii",
      "Sadra Yazdanbod"
    ],
    "page_url": "https://proceedings.mlr.press/v48/heidari16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/heidari16.pdf",
    "published": "2015-06",
    "summary": " As the number of ad exchanges has grown, publishers have turned to low regret learning algorithms to decide which exchange offers the best price for their inventory. This in turn opens the following question for the exchange: how to set prices to attract as many sellers as possible and maximize revenue. In this work we formulate this precisely as a learning problem, and present algorithms showing that by simply knowing that the counterparty is using a low regret algorithm is enough for the exchange to have its own low regret learning algorithm to find the optimal price. ",
    "code_link": ""
  },
  "icml2015_main_estimationfromindirectsupervisionwithlinearmoments": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Estimation from Indirect Supervision with Linear Moments",
    "authors": [
      "Aditi Raghunathan",
      "Roy Frostig",
      "John Duchi",
      "Percy Liang"
    ],
    "page_url": "https://proceedings.mlr.press/v48/raghunathan16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/raghunathan16.pdf",
    "published": "2015-06",
    "summary": " In structured prediction problems where we have indirect supervision of the output, maximum marginal likelihood faces two computational obstacles: non-convexity of the objective and intractability of even a single gradient computation. In this paper, we bypass both obstacles for a class of what we call linear indirectly-supervised problems. Our approach is simple: we solve a linear system to estimate sufficient statistics of the model, which we then use to estimate parameters via convex optimization. We analyze the statistical properties of our approach and show empirically that it is effective in two settings: learning with local privacy constraints and learning from low-cost count-based annotations. ",
    "code_link": ""
  },
  "icml2015_main_speedingupk-meansbyapproximatingeuclideandistancesviablockvectors": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Speeding up k-means by approximating Euclidean distances via block vectors",
    "authors": [
      "Thomas Bottesch",
      "Thomas B\u00fchler",
      "Markus K\u00e4chele"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bottesch16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bottesch16.pdf",
    "published": "2015-06",
    "summary": " This paper introduces a new method to approximate Euclidean distances between points using block vectors in combination with the H\u00f6lder inequality. By defining lower bounds based on the proposed approximation, cluster algorithms can be considerably accelerated without loss of quality. In extensive experiments, we show a considerable reduction in terms of computational time in comparison to standard methods and the recently proposed Yinyang k-means. Additionally we show that the memory consumption of the presented clustering algorithm does not depend on the number of clusters, which makes the approach suitable for large scale problems. ",
    "code_link": ""
  },
  "icml2015_main_learningandinferenceviamaximuminnerproductsearch": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning and Inference via Maximum Inner Product Search",
    "authors": [
      "Stephen Mussmann",
      "Stefano Ermon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/mussmann16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/mussmann16.pdf",
    "published": "2015-06",
    "summary": " A large class of commonly used probabilistic models known as log-linear models are defined up to a normalization constant.Typical learning algorithms for such models require solving a sequence of probabilistic inference queries. These inferences are typically intractable, and are a major bottleneck for learning models with large output spaces. In this paper, we provide a new approach for amortizing the cost of a sequence of related inference queries, such as the ones arising during learning. Our technique relies on a surprising connection with algorithms developed in the past two decades for similarity search in large data bases. Our approach achieves improved running times with provable approximation guarantees. We show that it performs well both on synthetic data and neural language models with large output spaces. ",
    "code_link": ""
  },
  "icml2015_main_asuperlinearly-convergentproximalnewton-typemethodfortheoptimizationoffinitesums": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Superlinearly-Convergent Proximal Newton-type Method for the Optimization of Finite Sums",
    "authors": [
      "Anton Rodomanov",
      "Dmitry Kropotov"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rodomanov16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rodomanov16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of minimizing the strongly convex sum of a finite number of convex functions. Standard algorithms for solving this problem in the class of incremental/stochastic methods have at most a linear convergence rate. We propose a new incremental method whose convergence rate is superlinear \u2013 the Newton-type incremental method (NIM). The idea of the method is to introduce a model of the objective with the same sum-of-functions structure and further update a single component of the model per iteration. We prove that NIM has a superlinear local convergence rate and linear global convergence rate. Experiments show that the method is very effective for problems with a large number of functions and a small number of variables. ",
    "code_link": ""
  },
  "icml2015_main_akerneltestofgoodnessoffit": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Kernel Test of Goodness of Fit",
    "authors": [
      "Kacper Chwialkowski",
      "Heiko Strathmann",
      "Arthur Gretton"
    ],
    "page_url": "https://proceedings.mlr.press/v48/chwialkowski16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/chwialkowski16.pdf",
    "published": "2015-06",
    "summary": " We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein\u2019s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation. ",
    "code_link": ""
  },
  "icml2015_main_interactingparticlemarkovchainmontecarlo": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Interacting Particle Markov Chain Monte Carlo",
    "authors": [
      "Tom Rainforth",
      "Christian Naesseth",
      "Fredrik Lindsten",
      "Brooks Paige",
      "Jan-Willem Vandemeent",
      "Arnaud Doucet",
      "Frank Wood"
    ],
    "page_url": "https://proceedings.mlr.press/v48/rainforth16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/rainforth16.pdf",
    "published": "2015-06",
    "summary": " We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC method based on an interacting pool of standard and conditional sequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte Carlo sampler on an extended space. We present empirical results that show significant improvements in mixing rates relative to both non-interacting PMCMC samplers and a single PMCMC sampler with an equivalent memory and computational budget. An additional advantage of the iPMCMC method is that it is suitable for distributed and multi-core architectures. ",
    "code_link": ""
  },
  "icml2015_main_fastereigenvectorcomputationviashift-and-invertpreconditioning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Faster Eigenvector Computation via Shift-and-Invert Preconditioning",
    "authors": [
      "Dan Garber",
      "Elad Hazan",
      "Chi Jin",
      "Sham",
      "Cameron Musco",
      "Praneeth Netrapalli",
      "Aaron Sidford"
    ],
    "page_url": "https://proceedings.mlr.press/v48/garber16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/garber16.pdf",
    "published": "2015-06",
    "summary": " We give faster algorithms and improved sample complexities for the fundamental problem of estimating the top eigenvector. Given an explicit matrix $A \\in \\mathbb{R}^{n \\times d}$, we show how to compute an $\\epsilon$-approximate top eigenvector of $A^TA$ in time $\\tilde O\\left( \\left[\\text{nnz}(A) + \\frac{d \\text{sr}(A)}{\\text{gap}^2} \\right] \\cdot \\log 1/\\epsilon\\right)$. Here $\\text{nnz}(A)$ is the number of nonzeros in $A$, $\\text{sr}(A)$ is the stable rank, and gap is the relative eigengap. We also consider an online setting in which, given a stream of i.i.d. samples from a distribution D with covariance matrix $\\Sigma$ and a vector $x_0$ which is an $O(\\text{gap})$ approximate top eigenvector for $\\Sigma$, we show how to refine $x_0$ to an $\\epsilon$ approximation using$O \\left( \\frac{\\text{var}(\\mathcal{D})}{\\text{gap}-\\epsilon}\\right)$ samples from $\\mathcal{D}$. Here $\\text{var}(\\mathcal{D})$ is a natural notion of variance. Combining our algorithm with previous work to initialize $x_0$, we obtain improved sample complexities and runtimes under a variety of assumptions on D. We achieve our results via a robust analysis of the classic shift-and-invert preconditioning method. This technique lets us reduce eigenvector computation to approximately solving a series of linear systems with fast stochastic gradient methods. ",
    "code_link": ""
  },
  "icml2015_main_atheoryofgenerativeconvnet": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "A Theory of Generative ConvNet",
    "authors": [
      "Jianwen Xie",
      "Yang Lu",
      "Song-Chun Zhu",
      "Yingnian Wu"
    ],
    "page_url": "https://proceedings.mlr.press/v48/xiec16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/xiec16.pdf",
    "published": "2015-06",
    "summary": " We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns. ",
    "code_link": ""
  },
  "icml2015_main_efficientlearningwithafamilyofnonconvexregularizersbyredistributingnonconvexity": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity",
    "authors": [
      "Quanming Yao",
      "James Kwok"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yao16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yao16.pdf",
    "published": "2015-06",
    "summary": " The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers. ",
    "code_link": ""
  },
  "icml2015_main_computationallyefficientnystr\u00f6mapproximationusingfasttransforms": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Computationally Efficient Nystr\u00f6m Approximation using Fast Transforms",
    "authors": [
      "Si Si",
      "Cho-Jui Hsieh",
      "Inderjit Dhillon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/si16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/si16.pdf",
    "published": "2015-06",
    "summary": " Our goal is to improve the \\it training and \\it prediction time of Nystr\u00f6m method, which is a widely-used technique for generating low-rank kernel matrix approximations. When applying the Nystr\u00f6m approximation for large-scale applications, both training and prediction time is dominated by computing kernel values between a data point and all landmark points. With m landmark points, this computation requires \u0398(md) time (flops), where d is the input dimension. In this paper, we propose the use of a family of fast transforms to generate structured landmark points for Nystr\u00f6m approximation. By exploiting fast transforms, e.g., Haar transform and Hadamard transform, our modified Nystr\u00f6m method requires only \u0398(m) or \u0398(m\\log d) time to compute the kernel values between a given data point and m landmark points. This improvement in time complexity can significantly speed up kernel approximation and benefit prediction speed in kernel machines. For instance, on the webspam data (more than 300,000 data points), our proposed algorithm enables kernel SVM prediction to deliver 98% accuracy and the resulting prediction time is 1000 times faster than LIBSVM and only 10 times slower than linear SVM prediction (which yields only 91% accuracy). ",
    "code_link": ""
  },
  "icml2015_main_gromov-wassersteinaveragingofkernelanddistancematrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Gromov-Wasserstein Averaging of Kernel and Distance Matrices",
    "authors": [
      "Gabriel Peyr\u00e9",
      "Marco Cuturi",
      "Justin Solomon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/peyre16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/peyre16.pdf",
    "published": "2015-06",
    "summary": " This paper presents a new technique for computing the barycenter of a set of distance or kernel matrices. These matrices, which define the inter-relationships between points sampled from individual domains, are not required to have the same size or to be in row-by-row correspondence. We compare these matrices using the softassign criterion, which measures the minimum distortion induced by a probabilistic map from the rows of one similarity matrix to the rows of another; this criterion amounts to a regularized version of the Gromov-Wasserstein (GW) distance between metric-measure spaces. The barycenter is then defined as a Fr\u00e9chet mean of the input matrices with respect to this criterion, minimizing a weighted sum of softassign values. We provide a fast iterative algorithm for the resulting nonconvex optimization problem, built upon state-of- the-art tools for regularized optimal transportation. We demonstrate its application to the computation of shape barycenters and to the prediction of energy levels from molecular configurations in quantum chemistry. ",
    "code_link": ""
  },
  "icml2015_main_robustmontecarlosamplingusingriemanniannos\u00e9-poincar\u00e9hamiltoniandynamics": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Robust Monte Carlo Sampling using Riemannian Nos\u00e9-Poincar\u00e9 Hamiltonian Dynamics",
    "authors": [
      "Anirban Roychowdhury",
      "Brian Kulis",
      "Srinivasan Parthasarathy"
    ],
    "page_url": "https://proceedings.mlr.press/v48/roychowdhury16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/roychowdhury16.pdf",
    "published": "2015-06",
    "summary": " We present a Monte Carlo sampler using a modified Nos\u00e9-Poincar\u00e9 Hamiltonian along with Riemannian preconditioning. Hamiltonian Monte Carlo samplers allow better exploration of the state space as opposed to random walk-based methods, but, from a molecular dynamics perspective, may not necessarily provide samples from the canonical ensemble. Nos\u00e9-Hoover samplers rectify that shortcoming, but the resultant dynamics are not Hamiltonian. Furthermore, usage of these algorithms on large real-life datasets necessitates the use of stochastic gradients, which acts as another potentially destabilizing source of noise. In this work, we propose dynamics based on a modified Nos\u00e9-Poincar\u00e9 Hamiltonian augmented with Riemannian manifold corrections. The resultant symplectic sampling algorithm samples from the canonical ensemble while using structural cues from the Riemannian preconditioning matrices to efficiently traverse the parameter space. We also propose a stochastic variant using additional terms in the Hamiltonian to correct for the noise from the stochastic gradients. We show strong performance of our algorithms on synthetic datasets and high-dimensional Poisson factor analysis-based topic modeling scenarios. ",
    "code_link": ""
  },
  "icml2015_main_thesegmentedihmmasimple,efficienthierarchicalinfinitehmm": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM",
    "authors": [
      "Ardavan Saeedi",
      "Matthew Hoffman",
      "Matthew Johnson",
      "Ryan Adams"
    ],
    "page_url": "https://proceedings.mlr.press/v48/saeedi16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/saeedi16.pdf",
    "published": "2015-06",
    "summary": " We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov model (iHMM) that supports a simple, efficient inference scheme. The siHMM is well suited to segmentation problems, where the goal is to identify points at which a time series transitions from one relatively stable regime to a new regime. Conventional iHMMs often struggle with such problems, since they have no mechanism for distinguishing between high-and low-level dynamics. Hierarchical HMMs (HHMMs) can do better, but they require much more complex and expensive inference algorithms. The siHMM retains the simplicity and efficiency of the iHMM, but outperforms it on a variety of segmentation problems, achieving performance that matches or exceeds that of a more complicated HHMM. ",
    "code_link": ""
  },
  "icml2015_main_meta\u2013gradientboosteddecisiontreemodelforweightandtargetlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Meta\u2013Gradient Boosted Decision Tree Model for Weight and Target Learning",
    "authors": [
      "Yury Ustinovskiy",
      "Valentina Fedorova",
      "Gleb Gusev",
      "Pavel Serdyukov"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ustinovskiy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ustinovskiy16.pdf",
    "published": "2015-06",
    "summary": " Labeled training data is an essential part of any supervised machine learning framework. In practice, there is a trade-off between the quality of a label and its cost. In this paper, we consider a problem of learning to rank on a large-scale dataset with low-quality relevance labels aiming at maximizing the quality of a trained ranker on a small validation dataset with high-quality ground truth relevance labels. Motivated by the classical Gauss-Markov theorem for the linear regression problem, we formulate the problems of (1) reweighting training instances and (2) remapping learning targets. We propose meta\u2013gradient decision tree learning framework for optimizing weight and target functions by applying gradient-based hyperparameter optimization. Experiments on a large-scale real-world dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework. ",
    "code_link": "https://github.com/dmlc/xgboost"
  },
  "icml2015_main_discriminativeembeddingsoflatentvariablemodelsforstructureddata": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Discriminative Embeddings of Latent Variable Models for Structured Data",
    "authors": [
      "Hanjun Dai",
      "Bo Dai",
      "Le Song"
    ],
    "page_url": "https://proceedings.mlr.press/v48/daib16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/daib16.pdf",
    "published": "2015-06",
    "summary": " Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10,000 times smaller, while at the same time achieving the state-of-the-art predictive performance. ",
    "code_link": "https://github.com/Hanjun-Dai/graphnn"
  },
  "icml2015_main_robustrandomcutforestbasedanomalydetectiononstreams": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Robust Random Cut Forest Based Anomaly Detection on Streams",
    "authors": [
      "Sudipto Guha",
      "Nina Mishra",
      "Gourav Roy",
      "Okke Schrijvers"
    ],
    "page_url": "https://proceedings.mlr.press/v48/guha16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/guha16.pdf",
    "published": "2015-06",
    "summary": " In this paper we focus on the anomaly detection problem for dynamic data streams through the lens of random cut forests. We investigate a robust random cut data structure that can be used as a sketch or synopsis of the input stream. We provide a plausible definition of non-parametric anomalies based on the influence of an unseen point on the remainder of the data, i.e., the externality imposed by that point. We show how the sketch can be efficiently updated in a dynamic data stream. We demonstrate the viability of the algorithm on publicly available real data. ",
    "code_link": ""
  },
  "icml2015_main_trainingneuralnetworkswithoutgradientsascalableadmmapproach": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Training Neural Networks Without Gradients: A Scalable ADMM Approach",
    "authors": [
      "Gavin Taylor",
      "Ryan Burmeister",
      "Zheng Xu",
      "Bharat Singh",
      "Ankit Patel",
      "Tom Goldstein"
    ],
    "page_url": "https://proceedings.mlr.press/v48/taylor16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/taylor16.pdf",
    "published": "2015-06",
    "summary": " With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don\u2019t scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization sub-steps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly non-convex problems. In addition, the method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores. ",
    "code_link": ""
  },
  "icml2015_main_clusteringhighdimensionalcategoricaldataviatopographicalfeatures": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Clustering High Dimensional Categorical Data via Topographical Features",
    "authors": [
      "Chao Chen",
      "Novi Quadrianto"
    ],
    "page_url": "https://proceedings.mlr.press/v48/chenc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/chenc16.pdf",
    "published": "2015-06",
    "summary": " Analysis of categorical data is a challenging task. In this paper, we propose to compute topographical features of high-dimensional categorical data. We propose an efficient algorithm to extract modes of the underlying distribution and their attractive basins. These topographical features provide a geometric view of the data and can be applied to visualization and clustering of real world challenging datasets. Experiments show that our principled method outperforms state-of-the-art clustering methods while also admits an embarrassingly parallel property. ",
    "code_link": ""
  },
  "icml2015_main_efficientalgorithmsforlarge-scalegeneralizedeigenvectorcomputationandcanonicalcorrelationanalysis": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis",
    "authors": [
      "Rong Ge",
      "Chi Jin",
      "Sham",
      "Praneeth Netrapalli",
      "Aaron Sidford"
    ],
    "page_url": "https://proceedings.mlr.press/v48/geb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/geb16.pdf",
    "published": "2015-06",
    "summary": " This paper considers the problem of canonical-correlation analysis (CCA) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics. We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-k generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of \\order\\fracz k \\sqrt\u03ba\u03c1 \\log(1/\u03b5) \\log \\left(k\u03ba/\u03c1\\right) where z is the total number of nonzero entries, \u03bais the condition number and \u03c1is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components k up to a \\log(k) factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets. ",
    "code_link": ""
  },
  "icml2015_main_algorithmsforoptimizingtheratioofsubmodularfunctions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Algorithms for Optimizing the Ratio of Submodular Functions",
    "authors": [
      "Wenruo Bai",
      "Rishabh Iyer",
      "Kai Wei",
      "Jeff Bilmes"
    ],
    "page_url": "https://proceedings.mlr.press/v48/baib16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/baib16.pdf",
    "published": "2015-06",
    "summary": " We investigate a new optimization problem involving minimizing the Ratio of Submodular (RS) functions. We argue that this problem occurs naturally in several real world applications. We then show the connection between this problem and several related problems, including minimizing the difference of submodular functions, and to submodular optimization subject to submodular constraints. We show RS that optimization can be solved within bounded approximation factors. We also provide a hardness bound and show that our tightest algorithm matches the lower bound up to a \\log factor. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms. ",
    "code_link": ""
  },
  "icml2015_main_model-freeimitationlearningwithpolicyoptimization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Model-Free Imitation Learning with Policy Optimization",
    "authors": [
      "Jonathan Ho",
      "Jayesh Gupta",
      "Stefano Ermon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/ho16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/ho16.pdf",
    "published": "2015-06",
    "summary": " In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima. ",
    "code_link": ""
  },
  "icml2015_main_adiosarchitecturesdeepinoutputspace": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "ADIOS: Architectures Deep In Output Space",
    "authors": [
      "Moustapha Cisse",
      "Maruan Al-Shedivat",
      "Samy Bengio"
    ],
    "page_url": "https://proceedings.mlr.press/v48/cisse16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/cisse16.pdf",
    "published": "2015-06",
    "summary": " Multi-label classification is a generalization of binary classification where the task consists in predicting \\emphsets of labels. With the availability of ever larger datasets, the multi-label setting has become a natural one in many applications, and the interest in solving multi-label problems has grown significantly. As expected, deep learning approaches are now yielding state-of-the-art performance for this class of problems. Unfortunately, they usually do not take into account the often unknown but nevertheless rich relationships between labels. In this paper, we propose to make use of this underlying structure by learning to partition the labels into a Markov Blanket Chain and then applying a novel deep architecture that exploits the partition. Experiments on several popular and large multi-label datasets demonstrate that our approach not only yields significant improvements, but also helps to overcome trade-offs specific to the multi-label classification setting. ",
    "code_link": "https://github.com/alshedivat/adios"
  },
  "icml2015_main_conditionaldependenceviashannoncapacityaxioms,estimatorsandapplications": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications",
    "authors": [
      "Weihao Gao",
      "Sreeram Kannan",
      "Sewoong Oh",
      "Pramod Viswanath"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gaob16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gaob16.pdf",
    "published": "2015-06",
    "summary": " We consider axiomatically the problem of estimating the strength of a conditional dependence relationship P_Y|X from a random variables X to a random variable Y. This has applications in determining the strength of a known causal relationship, where the strength depends only on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause). Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-k nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity. ",
    "code_link": ""
  },
  "icml2015_main_controlofmemory,activeperception,andactioninminecraft": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Control of Memory, Active Perception, and Action in Minecraft",
    "authors": [
      "Junhyuk Oh",
      "Valliappa Chockalingam",
      "Satinder",
      "Honglak Lee"
    ],
    "page_url": "https://proceedings.mlr.press/v48/oh16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/oh16.pdf",
    "published": "2015-06",
    "summary": " In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures. ",
    "code_link": ""
  },
  "icml2015_main_thelabelcomplexityofmixed-initiativeclassifiertraining": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "The Label Complexity of Mixed-Initiative Classifier Training",
    "authors": [
      "Jina Suh",
      "Xiaojin Zhu",
      "Saleema Amershi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/suh16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/suh16.pdf",
    "published": "2015-06",
    "summary": " Mixed-initiative classifier training, where the human teacher can choose which items to label or to label items chosen by the computer, has enjoyed empirical success but without a rigorous statistical learning theoretical justification. We analyze the label complexity of a simple mixed-initiative training mechanism using teach- ing dimension and active learning. We show that mixed-initiative training is advantageous com- pared to either computer-initiated (represented by active learning) or human-initiated classifier training. The advantage exists across all human teaching abilities, from optimal to completely unhelpful teachers. We further improve classifier training by educating the human teachers. This is done by showing, or explaining, optimal teaching sets to the human teachers. We conduct Mechanical Turk human experiments on two stylistic classifier training tasks to illustrate our approach. ",
    "code_link": ""
  },
  "icml2015_main_bayesianpoissontuckerdecompositionforlearningthestructureofinternationalrelations": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations",
    "authors": [
      "Aaron Schein",
      "Mingyuan Zhou",
      "David Blei",
      "Hanna Wallach"
    ],
    "page_url": "https://proceedings.mlr.press/v48/schein16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/schein16.pdf",
    "published": "2015-06",
    "summary": " We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country\u2013country interaction event data. These data consist of interaction events of the form \u201ccountry i took action a toward country j at time t.\u201d BPTD discovers overlapping country\u2013community memberships, including the number of latent communities. In addition, it discovers directed community\u2013community interaction networks that are specific to \u201ctopics\u201d of action types and temporal \u201cregimes.\u201d We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations. ",
    "code_link": ""
  },
  "icml2015_main_tensordecompositionviajointmatrixschurdecomposition": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Tensor Decomposition via Joint Matrix Schur Decomposition",
    "authors": [
      "Nicolo Colombo",
      "Nikos Vlassis"
    ],
    "page_url": "https://proceedings.mlr.press/v48/colombo16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/colombo16.pdf",
    "published": "2015-06",
    "summary": " We describe an approach to tensor decomposition that involves extracting a set of observable matrices from the tensor and applying an approximate joint Schur decomposition on those matrices, and we establish the corresponding first-order perturbation bounds. We develop a novel iterative Gauss-Newton algorithm for joint matrix Schur decomposition, which minimizes a nonconvex objective over the manifold of orthogonal matrices, and which is guaranteed to converge to a global optimum under certain conditions. We empirically demonstrate that our algorithm is faster and at least as accurate and robust than state-of-the-art algorithms for this problem. ",
    "code_link": ""
  },
  "icml2015_main_continuousdeepq-learningwithmodel-basedacceleration": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Continuous Deep Q-Learning with Model-based Acceleration",
    "authors": [
      "Shixiang Gu",
      "Timothy Lillicrap",
      "Ilya Sutskever",
      "Sergey Levine"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gu16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gu16.pdf",
    "published": "2015-06",
    "summary": " Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable. ",
    "code_link": ""
  },
  "icml2015_main_domainadaptationwithconditionaltransferablecomponents": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Domain Adaptation with Conditional Transferable Components",
    "authors": [
      "Mingming Gong",
      "Kun Zhang",
      "Tongliang Liu",
      "Dacheng Tao",
      "Clark Glymour",
      "Bernhard Sch\u00f6lkopf"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gong16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gong16.pdf",
    "published": "2015-06",
    "summary": " Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let X and Y denote the features and target, respectively, previous work on domain adaptation considers the covariate shift situation where the distribution of the features P(X) changes across domains while the conditional distribution P(Y|X) stays the same. To reduce domain discrepancy, recent methods try to find invariant components \\mathcalT(X) that have similar P(\\mathcalT(X)) by explicitly minimizing a distribution discrepancy measure. However, it is not clear if P(Y|\\mathcalT(X)) in different domains is also similar when P(Y|X) changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identifiable, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where P(X|Y) and P(Y) both change in a causal system in which Y is the cause for X. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution P(\\mathcalT(X)|Y) is invariant after proper location-scale (LS) transformations, and identify how P(Y) changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method. ",
    "code_link": ""
  },
  "icml2015_main_fixedpointquantizationofdeepconvolutionalnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fixed Point Quantization of Deep Convolutional Networks",
    "authors": [
      "Darryl Lin",
      "Sachin Talathi",
      "Sreekanth Annapureddy"
    ],
    "page_url": "https://proceedings.mlr.press/v48/linb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/linb16.pdf",
    "published": "2015-06",
    "summary": " In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we propose a quantizer design for fixed point implementation of DCNs. We formulate and solve an optimization problem to identify optimal fixed point bit-width allocation across DCN layers. Our experiments show that in comparison to equal bit-width settings, the fixed point DCNs with optimized bit width allocation offer >20% reduction in the model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark. ",
    "code_link": ""
  },
  "icml2015_main_provablealgorithmsforinferenceintopicmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Provable Algorithms for Inference in Topic Models",
    "authors": [
      "Sanjeev Arora",
      "Rong Ge",
      "Frederic Koehler",
      "Tengyu Ma",
      "Ankur Moitra"
    ],
    "page_url": "https://proceedings.mlr.press/v48/arorab16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/arorab16.pdf",
    "published": "2015-06",
    "summary": " Recently, there has been considerable progress on designing algorithms with provable guarantees \u2014typically using linear algebraic methods\u2014for parameter learning in latent variable models. Designing provable algorithms for inference has proved more difficult. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a single iteration of Gibbs sampling. ",
    "code_link": ""
  },
  "icml2015_main_epigraphprojectionsforfastgeneralconvexprogramming": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Epigraph projections for fast general convex programming",
    "authors": [
      "Po-Wei Wang",
      "Matt Wytock",
      "Zico Kolter"
    ],
    "page_url": "https://proceedings.mlr.press/v48/wangh16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/wangh16.pdf",
    "published": "2015-06",
    "summary": " This paper develops an approach for efficiently solving general convex optimization problems specified as disciplined convex programs (DCP), a common general-purpose modeling framework. Specifically we develop an algorithm based upon fast epigraph projections, projections onto the epigraph of a convex function, an approach closely linked to proximal operator methods. We show that by using these operators, we can solve any disciplined convex program without transforming the problem to a standard cone form, as is done by current DCP libraries. We then develop a large library of efficient epigraph projection operators, mirroring and extending work on fast proximal algorithms, for many common convex functions. Finally, we evaluate the performance of the algorithm, and show it often achieves order of magnitude speedups over existing general-purpose optimization solvers. ",
    "code_link": ""
  },
  "icml2015_main_fastalgorithmsforsegmentedregression": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Fast Algorithms for Segmented Regression",
    "authors": [
      "Jayadev Acharya",
      "Ilias Diakonikolas",
      "Jerry Li",
      "Ludwig Schmidt"
    ],
    "page_url": "https://proceedings.mlr.press/v48/acharya16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/acharya16.pdf",
    "published": "2015-06",
    "summary": " We study the fixed design segmented regression problem: Given noisy samples from a piecewise linear function f, we want to recover f up to a desired accuracy in mean-squared error. Previous rigorous approaches for this problem rely on dynamic programming (DP) and, while sample efficient, have running time quadratic in the sample size. As our main contribution, we provide new sample near-linear time algorithms for the problem that - while not being minimax optimal - achieve a significantly better sample-time tradeoff on large datasets compared to the DP approach. Our experimental evaluation shows that, compared with the DP approach, our algorithms provide a convergence rate that is only off by a factor of 2 to 4, while achieving speedups of three orders of magnitude. ",
    "code_link": ""
  },
  "icml2015_main_energeticnaturalgradientdescent": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Energetic Natural Gradient Descent",
    "authors": [
      "Philip Thomas",
      "Bruno Castro Silva",
      "Christoph Dann",
      "Emma Brunskill"
    ],
    "page_url": "https://proceedings.mlr.press/v48/thomasb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/thomasb16.pdf",
    "published": "2015-06",
    "summary": " We propose a new class of algorithms for minimizing or maximizing functions of parametric probabilistic models. These new algorithms are natural gradient algorithms that leverage more information than prior methods by using a new metric tensor in place of the commonly used Fisher information matrix. This new metric tensor is derived by computing directions of steepest ascent where the distance between distributions is measured using an approximation of energy distance (as opposed to Kullback-Leibler divergence, which produces the Fisher information matrix), and so we refer to our new ascent direction as the energetic natural gradient. ",
    "code_link": ""
  },
  "icml2015_main_partitionfunctionsfromrao-blackwellizedtemperedsampling": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Partition Functions from Rao-Blackwellized Tempered Sampling",
    "authors": [
      "David Carlson",
      "Patrick Stinson",
      "Ari Pakman",
      "Liam Paninski"
    ],
    "page_url": "https://proceedings.mlr.press/v48/carlson16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/carlson16.pdf",
    "published": "2015-06",
    "summary": " Partition functions of probability distributions are important quantities for model evaluation and comparisons. We present a new method to compute partition functions of complex and multimodal distributions. Such distributions are often sampled using simulated tempering, which augments the target space with an auxiliary inverse temperature variable. Our method exploits the multinomial probability law of the inverse temperatures, and provides estimates of the partition function in terms of a simple quotient of Rao-Blackwellized marginal inverse temperature probability estimates, which are updated while sampling. We show that the method has interesting connections with several alternative popular methods, and offers some significant advantages. In particular, we empirically find that the new method provides more accurate estimates than Annealed Importance Sampling when calculating partition functions of large Restricted Boltzmann Machines (RBM); moreover, the method is sufficiently accurate to track training and validation log-likelihoods during learning of RBMs, at minimal computational cost. ",
    "code_link": ""
  },
  "icml2015_main_learningmixturesofplackett-lucemodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Mixtures of Plackett-Luce Models",
    "authors": [
      "Zhibing Zhao",
      "Peter Piech",
      "Lirong Xia"
    ],
    "page_url": "https://proceedings.mlr.press/v48/zhaob16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/zhaob16.pdf",
    "published": "2015-06",
    "summary": " In this paper we address the identifiability and efficient learning problems of finite mixtures of Plackett-Luce models for rank data. We prove that for any k\u22652, the mixture of k Plackett-Luce models for no more than 2k-1 alternatives is non-identifiable and this bound is tight for k=2. For generic identifiability, we prove that the mixture of k Plackett-Luce models over m alternatives is \\em generically identifiable if k\u2264\u230a\\frac m-2 2\u230b!. We also propose an efficient generalized method of moments (GMM) algorithm to learn the mixture of two Plackett-Luce models and show that the algorithm is consistent. Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm by Gormley & Murphy (2008), while achieving competitive statistical efficiency. ",
    "code_link": ""
  },
  "icml2015_main_nearoptimalbehaviorviaapproximatestateabstraction": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Near Optimal Behavior via Approximate State Abstraction",
    "authors": [
      "David Abel",
      "David Hershkowitz",
      "Michael Littman"
    ],
    "page_url": "https://proceedings.mlr.press/v48/abel16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/abel16.pdf",
    "published": "2015-06",
    "summary": " The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments. ",
    "code_link": "https://github.com/david-abel/state_abstraction"
  },
  "icml2015_main_poweroforderedhypothesistesting": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Power of Ordered Hypothesis Testing",
    "authors": [
      "Lihua Lei",
      "William Fithian"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lei16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lei16.pdf",
    "published": "2015-06",
    "summary": " Ordered testing procedures are multiple testing procedures that exploit a pre-specified ordering of the null hypotheses, from most to least promising. We analyze and compare the power of several recent proposals using the asymptotic framework of Li & Barber (2015). While accumulation tests including ForwardStop can be quite powerful when the ordering is very informative, they are asymptotically powerless when the ordering is weaker. By contrast, Selective SeqStep, proposed by Barber & Candes (2015), is much less sensitive to the quality of the ordering. We compare the power of these procedures in different regimes, concluding that Selective SeqStep dominates accumulation tests if either the ordering is weak or non-null hypotheses are sparse or weak. Motivated by our asymptotic analysis, we derive an improved version of Selective SeqStep which we call Adaptive SeqStep, analogous to Storey\u2019s improvement on the Benjamini-Hochberg procedure. We compare these methods using the GEO-Query data set analyzed by (Li & Barber, 2015) and find Adaptive SeqStep has favorable performance for both good and bad prior orderings. ",
    "code_link": ""
  },
  "icml2015_main_phogprobabilisticmodelforcode": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PHOG: Probabilistic Model for Code",
    "authors": [
      "Pavol Bielik",
      "Veselin Raychev",
      "Martin Vechev"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bielik16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bielik16.pdf",
    "published": "2015-06",
    "summary": " We introduce a new generative model for code called probabilistic higher order grammar (PHOG). PHOG generalizes probabilistic context free grammars (PCFGs) by allowing conditioning of a production rule beyond the parent non-terminal, thus capturing rich contexts relevant to programs. Even though PHOG is more powerful than a PCFG, it can be learned from data just as efficiently. We trained a PHOG model on a large JavaScript code corpus and show that it is more precise than existing models, while similarly fast. As a result, PHOG can immediately benefit existing programming tools based on probabilistic models of code. ",
    "code_link": ""
  },
  "icml2015_main_shiftingregret,mirrordescent,andmatrices": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Shifting Regret, Mirror Descent, and Matrices",
    "authors": [
      "Andras Gyorgy",
      "Csaba Szepesvari"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gyorgy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gyorgy16.pdf",
    "published": "2015-06",
    "summary": " We consider the problem of online prediction in changing environments. In this framework the performance of a predictor is evaluated as the loss relative to an arbitrarily changing predictor, whose individual components come from a base class of predictors. Typical results in the literature consider different base classes (experts, linear predictors on the simplex, etc.) separately. Introducing an arbitrary mapping inside the mirror decent algorithm, we provide a framework that unifies and extends existing results. As an example, we prove new shifting regret bounds for matrix prediction problems. ",
    "code_link": ""
  },
  "icml2015_main_scalablegradient-basedtuningofcontinuousregularizationhyperparameters": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters",
    "authors": [
      "Jelena Luketina",
      "Mathias Berglund",
      "Klaus Greff",
      "Tapani Raiko"
    ],
    "page_url": "https://proceedings.mlr.press/v48/luketina16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/luketina16.pdf",
    "published": "2015-06",
    "summary": " Hyperparameter selection generally relies on running multiple full training trials, with selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels are within the optimal regions. The additional computational cost depends on how frequently the hyperparameters are trained, but the tested scheme adds only 30% computational overhead regardless of the model size. Since the method is significantly less computationally demanding compared to similar gradient-based approaches to hyperparameter optimization, and consistently finds good hyperparameter values, it can be a useful tool for training neural network models. ",
    "code_link": "https://github.com/jelennal/t1t2"
  },
  "icml2015_main_model-freetrajectoryoptimizationforreinforcementlearning": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Model-Free Trajectory Optimization for Reinforcement Learning",
    "authors": [
      "Riad Akrour",
      "Gerhard Neumann",
      "Hany Abdulsamad",
      "Abbas Abdolmaleki"
    ],
    "page_url": "https://proceedings.mlr.press/v48/akrour16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/akrour16.pdf",
    "published": "2015-06",
    "summary": " Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics. ",
    "code_link": ""
  },
  "icml2015_main_controllingthedistancetoakemenyconsensuswithoutcomputingit": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Controlling the distance to a Kemeny consensus without computing it",
    "authors": [
      "Yunlong Jiao",
      "Anna Korba",
      "Eric Sibony"
    ],
    "page_url": "https://proceedings.mlr.press/v48/korba16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/korba16.pdf",
    "published": "2015-06",
    "summary": " Due to its numerous applications, rank aggregation has become a problem of major interest across many fields of the computer science literature. In the vast majority of situations, Kemeny consensus(es) are considered as the ideal solutions. It is however well known that their computation is NP-hard. Many contributions have thus established various results to apprehend this complexity. In this paper we introduce a practical method to predict, for a ranking and a dataset, how close the Kemeny consensus(es) are to this ranking. A major strength of this method is its generality: it does not require any assumption on the dataset nor the ranking. Furthermore, it relies on a new geometric interpretation of Kemeny aggregation that, we believe, could lead to many other results. ",
    "code_link": ""
  },
  "icml2015_main_horizontallyscalablesubmodularmaximization": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Horizontally Scalable Submodular Maximization",
    "authors": [
      "Mario Lucic",
      "Olivier Bachem",
      "Morteza Zadimoghaddam",
      "Andreas Krause"
    ],
    "page_url": "https://proceedings.mlr.press/v48/lucic16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/lucic16.pdf",
    "published": "2015-06",
    "summary": " A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity - number of instances that can fit in memory - must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution. ",
    "code_link": ""
  },
  "icml2015_main_groupequivariantconvolutionalnetworks": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Group Equivariant Convolutional Networks",
    "authors": [
      "Taco Cohen",
      "Max Welling"
    ],
    "page_url": "https://proceedings.mlr.press/v48/cohenc16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/cohenc16.pdf",
    "published": "2015-06",
    "summary": " We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. ",
    "code_link": ""
  },
  "icml2015_main_stochasticdiscreteclenshaw-curtisquadrature": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Stochastic Discrete Clenshaw-Curtis Quadrature",
    "authors": [
      "Nico Piatkowski",
      "Katharina Morik"
    ],
    "page_url": "https://proceedings.mlr.press/v48/piatkowski16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/piatkowski16.pdf",
    "published": "2015-06",
    "summary": " The partition function is fundamental for probabilistic graphical models\u2014it is required for inference, parameter estimation, and model selection. Evaluating this function corresponds to discrete integration, namely a weighted sum over an exponentially large set. This task quickly becomes intractable as the dimensionality of the problem increases. We propose an approximation scheme that, for any discrete graphical model whose parameter vector has bounded norm, estimates the partition function with arbitrarily small error. Our algorithm relies on a near minimax optimal polynomial approximation to the potential function and a Clenshaw-Curtis style quadrature. Furthermore, we show that this algorithm can be randomized to split the computation into a high-complexity part and a low-complexity part, where the latter may be carried out on small computational devices. Experiments confirm that the new randomized algorithm is highly accurate if the parameter norm is small, and is otherwise comparable to methods with unbounded error. ",
    "code_link": ""
  },
  "icml2015_main_correctingforecastswithmultifactorneuralattention": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Correcting Forecasts with Multifactor Neural Attention",
    "authors": [
      "Matthew Riemer",
      "Aditya Vempaty",
      "Flavio Calmon",
      "Fenno Heath",
      "Richard Hull",
      "Elham Khabiri"
    ],
    "page_url": "https://proceedings.mlr.press/v48/riemer16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/riemer16.pdf",
    "published": "2015-06",
    "summary": " Automatic forecasting of time series data is a challenging problem in many industries. Current forecast models adopted by businesses do not provide adequate means for including data representing external factors that may have a significant impact on the time series, such as weather, national events, local events, social media trends, promotions, etc. This paper introduces a novel neural network attention mechanism that naturally incorporates data from multiple external sources without the feature engineering needed to get other techniques to work. We demonstrate empirically that the proposed model achieves superior performance for predicting the demand of 20 commodities across 107 stores of one of America\u2019s largest retailers when compared to other baseline models, including neural networks, linear models, certain kernel methods, Bayesian regression, and decision trees. Our method ultimately accounts for a 23.9% relative improvement as a result of the incorporation of external data sources, and provides an unprecedented level of descriptive ability for a neural network forecasting model. ",
    "code_link": ""
  },
  "icml2015_main_learningrepresentationsforcounterfactualinference": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Learning Representations for Counterfactual Inference",
    "authors": [
      "Fredrik Johansson",
      "Uri Shalit",
      "David Sontag"
    ],
    "page_url": "https://proceedings.mlr.press/v48/johansson16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/johansson16.pdf",
    "published": "2015-06",
    "summary": " Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \u201cWould this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art. ",
    "code_link": "https://github.com/vdorie/npci"
  },
  "icml2015_main_automaticconstructionofnonparametricrelationalregressionmodelsformultipletimeseries": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Automatic Construction of Nonparametric Relational Regression Models for Multiple Time Series",
    "authors": [
      "Yunseong Hwang",
      "Anh Tong",
      "Jaesik Choi"
    ],
    "page_url": "https://proceedings.mlr.press/v48/hwangb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/hwangb16.pdf",
    "published": "2015-06",
    "summary": " Gaussian Processes (GPs) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. The Automatic Bayesian Covariance Discovery (ABCD) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using GP with a composite covariance kernel function. Unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. We address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. We show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; US stock data, US house price index data and currency exchange rate data. ",
    "code_link": ""
  },
  "icml2015_main_inferencenetworksforsequentialmontecarloingraphicalmodels": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Inference Networks for Sequential Monte Carlo in Graphical Models",
    "authors": [
      "Brooks Paige",
      "Frank Wood"
    ],
    "page_url": "https://proceedings.mlr.press/v48/paige16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/paige16.pdf",
    "published": "2015-06",
    "summary": " We introduce a new approach for amortizing inference in directed graphical models by learning heuristic approximations to stochastic inverses, designed specifically for use as proposal distributions in sequential Monte Carlo methods. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation to the distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output of these networks can be used as automatically-learned high-quality proposal distributions to accelerate sequential Monte Carlo across a diverse range of problem settings. ",
    "code_link": ""
  },
  "icml2015_main_slicesamplingonhamiltoniantrajectories": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Slice Sampling on Hamiltonian Trajectories",
    "authors": [
      "Benjamin Bloem-Reddy",
      "John Cunningham"
    ],
    "page_url": "https://proceedings.mlr.press/v48/bloem-reddy16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/bloem-reddy16.pdf",
    "published": "2015-06",
    "summary": " Hamiltonian Monte Carlo and slice sampling are amongst the most widely used and studied classes of Markov Chain Monte Carlo samplers. We connect these two methods and present Hamiltonian slice sampling, which allows slice sampling to be carried out along Hamiltonian trajectories, or transformations thereof. Hamiltonian slice sampling clarifies a class of model priors that induce closed-form slice samplers. More pragmatically, inheriting properties of slice samplers, it offers advantages over Hamiltonian Monte Carlo, in that it has fewer tunable hyperparameters and does not require gradient information. We demonstrate the utility of Hamiltonian slice sampling out of the box on problems ranging from Gaussian process regression to Pitman-Yor based mixture models. ",
    "code_link": ""
  },
  "icml2015_main_noisyactivationfunctions": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "Noisy Activation Functions",
    "authors": [
      "Caglar Gulcehre",
      "Marcin Moczulski",
      "Misha Denil",
      "Yoshua Bengio"
    ],
    "page_url": "https://proceedings.mlr.press/v48/gulcehre16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/gulcehre16.pdf",
    "published": "2015-06",
    "summary": " Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradients. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to explore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps optimization in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results. ",
    "code_link": ""
  },
  "icml2015_main_pd-sparseaprimalanddualsparseapproachtoextrememulticlassandmultilabelclassification": {
    "conf_id": "ICML2015",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "ICML2015",
    "title": "PD-Sparse : A Primal and Dual Sparse Approach to Extreme Multiclass and Multilabel Classification",
    "authors": [
      "Ian En-Hsu Yen",
      "Xiangru Huang",
      "Pradeep Ravikumar",
      "Kai Zhong",
      "Inderjit Dhillon"
    ],
    "page_url": "https://proceedings.mlr.press/v48/yenb16.html",
    "pdf_url": "http://proceedings.mlr.press/v48/yenb16.pdf",
    "published": "2015-06",
    "summary": " We consider Multiclass and Multilabel classification with extremely large number of classes, of which only few are labeled to each instance. In such setting, standard methods that have training, prediction cost linear to the number of classes become intractable. State-of-the-art methods thus aim to reduce the complexity by exploiting correlation between labels under assumption that the similarity between labels can be captured by structures such as low-rank matrix or balanced tree. However, as the diversity of labels increases in the feature space, structural assumption can be easily violated, which leads to degrade in the testing performance. In this work, we show that a margin-maximizing loss with l1 penalty, in case of Extreme Classification, yields extremely sparse solution both in primal and in dual without sacrificing the expressive power of predictor. We thus propose a Fully-Corrective Block-Coordinate Frank-Wolfe (FC-BCFW) algorithm that exploits both primal and dual sparsity to achieve a complexity sublinear to the number of primal and dual variables. A bi-stochastic search method is proposed to further improve the efficiency. In our experiments on both Multiclass and Multilabel problems, the proposed method achieves significant higher accuracy than existing approaches of Extreme Classification with very competitive training and prediction time. ",
    "code_link": ""
  }
}