{
  "aaai2022_main_learningunseenemotionsfromgesturesviasemantically-conditionedzero-shotperceptionwithadversarialautoencoders": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Unseen Emotions from Gestures via Semantically-Conditioned Zero-Shot Perception with Adversarial Autoencoders",
    "authors": [
      "Abhishek Banerjee",
      "Uttaran Bhattacharya",
      "Aniket Bera"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19873",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19873/19632",
    "published": "2022-02",
    "summary": "We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial autoencoder-based representation learning that correlates 3D motion-captured gesture sequences with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of 58.43%. We see an improvement in performance compared to current state-of-the-art algorithms for generalized zero-shot learning by an absolute 25-27%. We also demonstrate our approach on publicly available online videos and movie scenes, where the actors' pose has been extracted and map to their respective emotive states.",
    "code_link": ""
  },
  "aaai2022_main_optimizedpotentialinitializationforlow-latencyspikingneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimized Potential Initialization for Low-Latency Spiking Neural Networks",
    "authors": [
      "Tong Bu",
      "Jianhao Ding",
      "Zhaofei Yu",
      "Tiejun Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19874",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19874/19633",
    "published": "2022-02",
    "summary": "Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness. The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets. However, there is a trade-off between accuracy and latency. In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN. In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps). We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization. Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer. Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10 dataset and CIFAR-100 dataset and achieve state-of-the-art accuracy, using fewer time-steps. For example, we reach top-1 accuracy of 93.38% on CIFAR-10 with 16 time-steps. Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small.",
    "code_link": ""
  },
  "aaai2022_main_planningwithbiologicalneuronsandsynapses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Planning with Biological Neurons and Synapses",
    "authors": [
      "Francesco d'Amore",
      "Daniel Mitropolsky",
      "Pierluigi Crescenzi",
      "Emanuele\n      Natale",
      "Christos H. Papadimitriou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19875",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19875/19634",
    "published": "2022-02",
    "summary": "We revisit the planning problem in the blocks world, and we implement a known heuristic for this task. Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons. Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind. The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as \"put the top block in stack 1 on the table\". The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function. Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience. A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task. The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic --- if idealized --- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs.",
    "code_link": ""
  },
  "aaai2022_main_backprop-freereinforcementlearningwithactiveneuralgenerativecoding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Backprop-Free Reinforcement Learning with Active Neural Generative Coding",
    "authors": [
      "Alexander G. Ororbia",
      "Ankur Mali"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19876",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19876/19635",
    "published": "2022-02",
    "summary": "In humans, perceptual awareness facilitates the fast recognition and extraction of information from sensory input. This awareness largely depends on how the human agent interacts with the environment. In this work, we propose active neural generative coding, a computational framework for learning action-driven generative models without backpropagation of errors (backprop) in dynamic environments. Specifically, we develop an intelligent agent that operates even with sparse rewards, drawing inspiration from the cognitive theory of planning as inference. We demonstrate on several simple control problems that our framework performs competitively with deep Q-learning. The robust performance of our agent offers promising evidence that a backprop-free approach for neural inference and learning can drive goal-directed behavior.",
    "code_link": ""
  },
  "aaai2022_main_vecaanewbenchmarkandtoolkitforgeneralcognitivedevelopment": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "VECA: A New Benchmark and Toolkit for General Cognitive Development",
    "authors": [
      "Kwanyoung Park",
      "Hyunseok Oh",
      "Youngki Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19877",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19877/19636",
    "published": "2022-02",
    "summary": "The developmental approach, simulating a cognitive development of a human, arises as a way to nurture a human-level commonsense and overcome the limitations of data-driven approaches. However, neither a virtual environment nor an evaluation platform exists for the overall development of core cognitive skills. We present the VECA(Virtual Environment for Cognitive Assessment), which consists of two main components: (i) a first benchmark to assess the overall cognitive development of an AI agent, and (ii) a novel toolkit to generate diverse and distinct cognitive tasks. VECA benchmark virtually implements the cognitive scale of Bayley Scales of Infant and Toddler Development-IV(Bayley-4), the gold-standard developmental assessment for human infants and toddlers. Our VECA toolkit provides a human toddler-like embodied agent with various human-like perceptual features crucial to human cognitive development, e.g., binocular vision, 3D-spatial audio, and tactile receptors. We compare several modern RL algorithms on our VECA benchmark and seek their limitations in modeling human-like cognitive development. We further analyze the validity of the VECA benchmark, as well as the effect of human-like sensory characteristics on cognitive skills.",
    "code_link": "https://github.com/snuhcs/veca"
  },
  "aaai2022_main_bridgingbetweencognitiveprocessingsignalsandlinguisticfeaturesviaaunifiedattentionalnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network",
    "authors": [
      "Yuqi Ren",
      "Deyi Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19878",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19878/19637",
    "published": "2022-02",
    "summary": "Cognitive processing signals can be used to improve natural language processing (NLP) tasks. However, it is not clear how these signals correlate with linguistic information. Bridging between human language processing and linguistic features has been widely studied in neurolinguistics, usually via single-variable controlled experiments with highly-controlled stimuli. Such methods not only compromises the authenticity of natural reading, but also are time-consuming and expensive. In this paper, we propose a data-driven method to investigate the relationship between cognitive processing signals and linguistic features. Specifically, we present a unified attentional framework that is composed of embedding, attention, encoding and predicting layers to selectively map cognitive processing signals to linguistic features. We define the mapping procedure as a bridging task and develop 12 bridging tasks for lexical, syntactic and semantic features. The proposed framework only requires cognitive processing signals recorded under natural reading as inputs, and can be used to detect a wide range of linguistic features with a single cognitive dataset. Observations from experiment results resonate with previous neuroscience findings. In addition to this, our experiments also reveal a number of interesting findings, such as the correlation between contextual eye-tracking features and tense of sentence.",
    "code_link": ""
  },
  "aaai2022_main_multi-sacledynamiccodingimprovedspikingactornetworkforreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Sacle Dynamic Coding Improved Spiking Actor Network for Reinforcement Learning",
    "authors": [
      "Duzhen Zhang",
      "Tielin Zhang",
      "Shuncheng Jia",
      "Bo Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19879",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19879/19638",
    "published": "2022-02",
    "summary": "With the help of deep neural networks (DNNs), deep reinforcement learning (DRL) has achieved great success on many complex tasks, from games to robotic control. Compared to DNNs with partial brain-inspired structures and functions, spiking neural networks (SNNs) consider more biological features, including spiking neurons with complex dynamics and learning paradigms with biologically plausible plasticity principles. Inspired by the efficient computation of cell assembly in the biological brain, whereby memory-based coding is much more complex than readout, we propose a multiscale dynamic coding improved spiking actor network (MDC-SAN) for reinforcement learning to achieve effective decision-making. The population coding at the network scale is integrated with the dynamic neurons coding (containing 2nd-order neuronal dynamics) at the neuron scale towards a powerful spatial-temporal state representation. Extensive experimental results show that our MDC-SAN performs better than its counterpart deep actor network (based on DNNs) on four continuous control tasks from OpenAI gym. We think this is a significant attempt to improve SNNs from the perspective of efficient coding towards effective decision-making, just like that in biological networks.",
    "code_link": ""
  },
  "aaai2022_main_jointhumanposeestimationandinstancesegmentationwithposeplusseg": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Joint Human Pose Estimation and Instance Segmentation with PosePlusSeg",
    "authors": [
      "Niaz Ahmad",
      "Jawad Khan",
      "Jeremy Yuhyun Kim",
      "Youngmoon Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19880",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19880/19639",
    "published": "2022-02",
    "summary": "Despite the advances in multi-person pose estimation, state-of-the-art techniques only deliver the human pose structure.Yet, they do not leverage the keypoints of human pose to deliver whole-body shape information for human instance segmentation. This paper presents PosePlusSeg, a joint model designed for both human pose estimation and instance segmentation. For pose estimation, PosePlusSeg first takes a bottom-up approach to detect the soft and hard keypoints of individuals by producing a strong keypoint heat map, then improves the keypoint detection confidence score by producing a body heat map. For instance segmentation, PosePlusSeg generates a mask offset where keypoint is defined as a centroid for the pixels in the embedding space, enabling instance-level segmentation for the human class. Finally, we propose a new pose and instance segmentation algorithm that enables PosePlusSeg to determine the joint structure of the human pose and instance segmentation. Experiments using the COCO challenging dataset demonstrate that PosePlusSeg copes better with challenging scenarios, like occlusions, en-tangled limbs, and overlapped people. PosePlusSeg outperforms state-of-the-art detection-based approaches achieving a 0.728 mAP for human pose estimation and a 0.445 mAP for instance segmentation. Code has been made available at:https://github.com/RaiseLab/PosePlusSeg.",
    "code_link": "https://github.com/RaiseLab/PosePlusSeg"
  },
  "aaai2022_main_logicruleguidedattributionwithdynamicablation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Logic Rule Guided Attribution with Dynamic Ablation",
    "authors": [
      "Jianqiao An",
      "Yuandu Lai",
      "Yahong Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19881",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19881/19640",
    "published": "2022-02",
    "summary": "With the increasing demands for understanding the internal behaviors of deep networks, Explainable AI (XAI) has been made remarkable progress in interpreting the model's decision. A family of attribution techniques has been proposed, highlighting whether the input pixels are responsible for the model's prediction. However, the existing attribution methods suffer from the lack of rule guidance and require further human interpretations. In this paper, we construct the 'if-then' logic rules that are sufficiently precise locally. Moreover, a novel rule-guided method, dynamic ablation (DA), is proposed to find a minimal bound sufficient in an input image to justify the network's prediction and aggregate iteratively to reach a complete attribution. Both qualitative and quantitative experiments are conducted to evaluate the proposed DA. We demonstrate the advantages of our method in providing clear and explicit explanations that are also easy for human experts to understand. Besides, through the attribution on a series of trained networks with different architectures, we show that more complex networks require less information to make a specific prediction.",
    "code_link": ""
  },
  "aaai2022_main_neuralmarionetteunsupervisedlearningofmotionskeletonandlatentdynamicsfromvolumetricvideo": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video",
    "authors": [
      "Jinseok Bae",
      "Hojun Jang",
      "Cheol-Hui Min",
      "Hyungun Choi",
      "Young Min Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19882",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19882/19641",
    "published": "2022-02",
    "summary": "We present Neural Marionette, an unsupervised approach that discovers the skeletal structure from a dynamic sequence and learns to generate diverse motions that are consistent with the observed motion dynamics. Given a video stream of point cloud observation of an articulated body under arbitrary motion, our approach discovers the unknown low-dimensional skeletal relationship that can effectively represent the movement. Then the discovered structure is utilized to encode the motion priors of dynamic sequences in a latent structure, which can be decoded to the relative joint rotations to represent the full skeletal motion. Our approach works without any prior knowledge of the underlying motion or skeletal structure, and we demonstrate that the discovered structure is even comparable to the hand-labeled ground truth skeleton in representing a 4D sequence of motion. The skeletal structure embeds the general semantics of possible motion space that can generate motions for diverse scenarios. We verify that the learned motion prior is generalizable to the multi-modal sequence generation, interpolation of two poses, and motion retargeting to a different skeletal structure.",
    "code_link": ""
  },
  "aaai2022_main_deformablepartregionlearningforobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deformable Part Region Learning for Object Detection",
    "authors": [
      "Seung-Hwan Bae"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19883",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19883/19642",
    "published": "2022-02",
    "summary": "In a convolutional object detector, the detection accuracy can be degraded often due to the low feature discriminability caused by geometric variation or transformation of an object. In this paper, we propose a deformable part region learning in order to allow decomposed part regions to be deformable according to geometric transformation of an object. To this end, we introduce trainable geometric parameters for the location of each part model. Because the ground truth of the part models is not available, we design classification and mask losses for part models, and learn the geometric parameters by minimizing an integral loss including those part losses. As a result, we can train a deformable part region network without extra super-vision and make each part model deformable according to object scale variation. Furthermore, for improving cascade object detection and instance segmentation, we present a Cascade deformable part region architecture which can refine whole and part detections iteratively in the cascade manner. Without bells and whistles, our implementation of a Cascade deformable part region detector achieves better detection and segmentation mAPs on COCO and VOC datasets, compared to the recent cascade and other state-of-the-art detectors.",
    "code_link": ""
  },
  "aaai2022_main_towardsend-to-endimagecompressionandanalysiswithtransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards End-to-End Image Compression and Analysis with Transformers",
    "authors": [
      "Yuanchao Bai",
      "Xu Yang",
      "Xianming Liu",
      "Junjun Jiang",
      "Yaowei Wang",
      "Xiangyang\n      Ji",
      "Wen Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19884",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19884/19643",
    "published": "2022-02",
    "summary": "We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application. Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer. Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network. The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance. The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy. Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks.",
    "code_link": ""
  },
  "aaai2022_main_handwrittenmathematicalexpressionrecognitionviaattentionaggregationbasedbi-directionalmutuallearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Handwritten Mathematical Expression Recognition via Attention Aggregation Based Bi-directional Mutual Learning",
    "authors": [
      "Xiaohang Bian",
      "Bo Qin",
      "Xiaozhe Xin",
      "Jianwu Li",
      "Xuefeng Su",
      "Yanfeng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19885",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19885/19644",
    "published": "2022-02",
    "summary": "Handwritten mathematical expression recognition aims to automatically generate LaTeX sequences from given images. Currently, attention-based encoder-decoder models are widely used in this task. They typically generate target sequences in a left-to-right (L2R) manner, leaving the right-to-left (R2L) contexts unexploited. In this paper, we propose an Attention aggregation based Bi-directional Mutual learning Network (ABM) which consists of one shared encoder and two parallel inverse decoders (L2R and R2L). The two decoders are enhanced via mutual distillation, which involves one-to-one knowledge transfer at each training step, making full use of the complementary information from two inverse directions. Moreover, in order to deal with mathematical symbols in diverse scales, an Attention Aggregation Module (AAM) is proposed to effectively integrate multi-scale coverage attentions. Notably, in the inference phase, given that the model already learns knowledge from two inverse directions, we only use the L2R branch for inference, keeping the original parameter size and inference speed. Extensive experiments demonstrate that our proposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014, 52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation and model ensembling, substantially outperforming the state-of-the-art methods. The source code is available in https://github.com/XH-B/ABM.",
    "code_link": "https://github.com/XH-B/ABM"
  },
  "aaai2022_main_addfrequencyattentionandmulti-viewbasedknowledgedistillationtodetectlow-qualitycompresseddeepfakeimages": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ADD: Frequency Attention and Multi-View Based Knowledge Distillation to Detect Low-Quality Compressed Deepfake Images",
    "authors": [
      "Le Minh Binh",
      "Simon Woo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19886",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19886/19645",
    "published": "2022-02",
    "summary": "Despite significant advancements of deep learning-based forgery detectors for distinguishing manipulated deepfake images, most detection approaches suffer from moderate to significant performance degradation with low-quality compressed deepfake images. Because of the limited information in low-quality images, detecting low-quality deepfake remains an important challenge. In this work, we apply frequency domain learning and optimal transport theory in knowledge distillation (KD) to specifically improve the detection of low-quality compressed deepfake images. We explore transfer learning capability in KD to enable a student network to learn discriminative features from low-quality images effectively. In particular, we propose the Attention-based Deepfake detection Distiller (ADD), which consists of two novel distillations: 1) frequency attention distillation that effectively retrieves the removed high-frequency components in the student network, and 2) multi-view attention distillation that creates multiple attention vectors by slicing the teacher\u2019s and student\u2019s tensors under different views to transfer the teacher tensor\u2019s distribution to the student more efficiently. Our extensive experimental results demonstrate that our approach outperforms state-of-the-art baselines in detecting low-quality compressed deepfake images.",
    "code_link": ""
  },
  "aaai2022_main_lunalocalizingunfamiliaritynearacquaintanceforopen-setlong-tailedrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LUNA: Localizing Unfamiliarity Near Acquaintance for Open-Set Long-Tailed Recognition",
    "authors": [
      "Jiarui Cai",
      "Yizhou Wang",
      "Hung-Min Hsu",
      "Jenq-Neng Hwang",
      "Kelsey Magrane",
      "Craig S Rose"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19887",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19887/19646",
    "published": "2022-02",
    "summary": "The predefined artificially-balanced training classes in object recognition have limited capability in modeling real-world scenarios where objects are imbalanced-distributed with unknown classes. In this paper, we discuss a promising solution to the Open-set Long-Tailed Recognition (OLTR) task utilizing metric learning. Firstly, we propose a distribution-sensitive loss, which weighs more on the tail classes to decrease the intra-class distance in the feature space. Building upon these concentrated feature clusters, a local-density-based metric is introduced, called Localizing Unfamiliarity Near Acquaintance (LUNA), to measure the novelty of a testing sample. LUNA is flexible with different cluster sizes and is reliable on the cluster boundary by considering neighbors of different properties. Moreover, contrary to most of the existing works that alleviate the open-set detection as a simple binary decision, LUNA is a quantitative measurement with interpretable meanings. Our proposed method exceeds the state-of-the-art algorithm by 4-6% in the closed-set recognition accuracy and 4% in F-measure under the open-set on the public benchmark datasets, including our own newly introduced fine-grained OLTR dataset about marine species (MS-LT), which is the first naturally-distributed OLTR dataset revealing the genuine genetic relationships of the classes.",
    "code_link": ""
  },
  "aaai2022_main_priorgradientmaskguidedpruning-awarefine-tuning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Prior Gradient Mask Guided Pruning-Aware Fine-Tuning",
    "authors": [
      "Linhang Cai",
      "Zhulin An",
      "Chuanguang Yang",
      "Yangchun Yan",
      "Yongjun Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19888",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19888/19647",
    "published": "2022-02",
    "summary": "We proposed a Prior Gradient Mask Guided Pruning-aware Fine-Tuning (PGMPF) framework to accelerate deep Convolutional Neural Networks (CNNs). In detail, the proposed PGMPF selectively suppresses the gradient of those \u201dunimportant\u201d parameters via a prior gradient mask generated by the pruning criterion during fine-tuning. PGMPF has three charming characteristics over previous works: (1) Pruning-aware network fine-tuning. A typical pruning pipeline consists of training, pruning and fine-tuning, which are relatively independent, while PGMPF utilizes a variant of the pruning mask as a prior gradient mask to guide fine-tuning, withoutcomplicated pruning criteria. (2) An excellent tradeoff between large model capacity during fine-tuning and stable convergence speed to obtain the final compact model. Previous works preserve more training information of pruned parameters during fine-tuning to pursue better performance, which would incur catastrophic non-convergence of the pruned model for relatively large pruning rates, while our PGMPF greatly stabilizes the fine-tuning phase by gradually constraining the learning rate of those \u201dunimportant\u201d parameters. (3) Channel-wise random dropout of the prior gradient mask to impose some gradient noise to fine-tuning to further improve the robustness of final compact model. Experimental results on three image classification benchmarks CIFAR10/ 100 and ILSVRC-2012 demonstrate the effectiveness of our method for various CNN architectures, datasets and pruning rates. Notably, on ILSVRC-2012, PGMPF reduces 53.5% FLOPs on ResNet-50 with only 0.90% top-1 accuracy drop and 0.52% top-5 accuracy drop, which has advanced the state-of-the-art with negligible extra computational cost.",
    "code_link": ""
  },
  "aaai2022_main_context-awaretransferattacksforobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Context-Aware Transfer Attacks for Object Detection",
    "authors": [
      "Zikui Cai",
      "Xinxin Xie",
      "Shasha Li",
      "Mingjun Yin",
      "Chengyu Song",
      "Srikanth V.\n      Krishnamurthy",
      "Amit K. Roy-Chowdhury",
      "M. Salman Asif"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19889",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19889/19648",
    "published": "2022-02",
    "summary": "Blackbox transfer attacks for image classifiers have been extensively studied in recent years. In contrast, little progress has been made on transfer attacks for object detectors. Object detectors take a holistic view of the image and the detection of one object (or lack thereof) often depends on other objects in the scene. This makes such detectors inherently context-aware and adversarial attacks in this space are more challenging than those targeting image classifiers. In this paper, we present a new approach to generate context-aware attacks for object detectors. We show that by using co-occurrence of objects and their relative locations and sizes as context information, we can successfully generate targeted mis-categorization attacks that achieve higher transfer success rates on blackbox object detectors than the state-of-the-art. We test our approach on a variety of object detectors with images from PASCAL VOC and MS COCO datasets and demonstrate up to 20 percentage points improvement in performance compared to the other state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_oodhdr-codecout-of-distributiongeneralizationforhdrimagecompression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "OoDHDR-Codec: Out-of-Distribution Generalization for HDR Image Compression",
    "authors": [
      "Linfeng Cao",
      "Aofan Jiang",
      "Wei Li",
      "Huaying Wu",
      "Nanyang Ye"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19890",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19890/19649",
    "published": "2022-02",
    "summary": "Recently, deep learning has been proven to be a promising approach in standard dynamic range (SDR) image compression. However, due to the wide luminance distribution of high dynamic range (HDR) images and the lack of large standard datasets, developing a deep model for HDR image compression is much more challenging. To tackle this issue, we view HDR data as distributional shifts of SDR data and the HDR image compression can be modeled as an out-of-distribution generalization (OoD) problem. Herein, we propose a novel out-of-distribution (OoD) HDR image compression framework (OoDHDR-codec). It learns the general representation across HDR and SDR environments, and allows the model to be trained effectively using a large set of SDR datases supplemented with much fewer HDR samples. Specifically, OoDHDR-codec consists of two branches to process the data from two environments. The SDR branch is a standard blackbox network. For the HDR branch, we develop a hybrid system that models luminance masking and tone mapping with white-box modules and performs content compression with black-box neural networks. To improve the generalization from SDR training data on HDR data, we introduce an invariance regularization term to learn the common representation for both SDR and HDR compression. Extensive experimental results show that the OoDHDR codec achieves strong competitive in-distribution performance and state-of-the-art OoD performance. To the best of our knowledge, our proposed approach is the first work to model HDR compression as OoD generalization problems and our OoD generalization algorithmic framework can be applied to any deep compression model in addition to the network architectural choice demonstrated in the paper. Code available at https://github.com/caolinfeng/OoDHDR-codec.",
    "code_link": "https://github.com/caolinfeng/OoDHDR-codec"
  },
  "aaai2022_main_visualconsensusmodelingforvideo-textretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Visual Consensus Modeling for Video-Text Retrieval",
    "authors": [
      "Shuqiang Cao",
      "Bairui Wang",
      "Wei Zhang",
      "Lin Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19891",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19891/19650",
    "published": "2022-02",
    "summary": "In this paper, we propose a novel method to mine the commonsense knowledge shared between the video and text modalities for video-text retrieval, namely visual consensus modeling. Different from the existing works, which learn the video and text representations and their complicated relationships solely based on the pairwise video-text data, we make the first attempt to model the visual consensus by mining the visual concepts from videos and exploiting their co-occurrence patterns within the video and text modalities with no reliance on any additional concept annotations. Specifically, we build a shareable and learnable graph as the visual consensus, where the nodes denoting the mined visual concepts and the edges connecting the nodes representing the co-occurrence relationships between the visual concepts. Extensive experimental results on the public benchmark datasets demonstrate that our proposed method, with the ability to effectively model the visual consensus,achieves state-of-the-art performances on the bidirectional video-text retrieval task. Our code is available at https://github.com/sqiangcao99/VCM.",
    "code_link": "https://github.com/sqiangcao99/VCM"
  },
  "aaai2022_main_proximalpannetamodel-baseddeepnetworkforpansharpening": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Proximal PanNet: A Model-Based Deep Network for Pansharpening",
    "authors": [
      "Xiangyong Cao",
      "Yang Chen",
      "Wenfei Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19892",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19892/19651",
    "published": "2022-02",
    "summary": "Recently, deep learning techniques have been extensively studied for pansharpening, which aims to generate a high resolution multispectral (HRMS) image by fusing a low resolution multispectral (LRMS) image with a high resolution panchromatic (PAN) image. However, existing deep learning-based pansharpening methods directly learn the mapping from LRMS and PAN to HRMS. These network architectures always lack sufficient interpretability, which limits further performance improvements. To alleviate this issue, we propose a novel deep network for pansharpening by combining the model-based methodology with the deep learning method. Firstly, we build an observation model for pansharpening using the convolutional sparse coding (CSC) technique and design a proximal gradient algorithm to solve this model. Secondly, we unfold the iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning the proximal operators using convolutional neural networks. Finally, all the learnable modules can be automatically learned in an end-to-end manner. Experimental results on some benchmark datasets show that our network performs better than other advanced methods both quantitatively and qualitatively.",
    "code_link": ""
  },
  "aaai2022_main_cf-detrcoarse-to-finetransformersforend-to-endobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CF-DETR: Coarse-to-Fine Transformers for End-to-End Object Detection",
    "authors": [
      "Xipeng Cao",
      "Peng Yuan",
      "Bailan Feng",
      "Kun Niu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19893",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19893/19652",
    "published": "2022-02",
    "summary": "The recently proposed DEtection TRansformer (DETR) achieves promising performance for end-to-end object detection. However, it has relatively lower detection performance on small objects and suffers from slow convergence. This paper observed that DETR performs surprisingly well even on small objects when measuring Average Precision (AP) at decreased Intersection-over-Union (IoU) thresholds. Motivated by this observation, we propose a simple way to improve DETR by refining the coarse features and predicted locations. Specifically, we propose a novel Coarse-to-Fine (CF) decoder layer constituted of a coarse layer and a carefully designed fine layer. Within each CF decoder layer, the extracted local information (region of interest feature) is introduced into the flow of global context information from the coarse layer to refine and enrich the object query features via the fine layer. In the fine layer, the multi-scale information can be fully explored and exploited via the Adaptive Scale Fusion(ASF) module and Local Cross-Attention (LCA) module. The multi-scale information can also be enhanced by another proposed Transformer Enhanced FPN (TEF) module to further improve the performance. With our proposed framework (named CF-DETR), the localization accuracy of objects (especially for small objects) can be largely improved. As a byproduct, the slow convergence issue of DETR can also be addressed. The effectiveness of CF-DETR is validated via extensive experiments on the coco benchmark. CF-DETR achieves state-of-the-art performance among end-to-end detectors, e.g., achieving 47.8 AP using ResNet-50 with 36 epochs in the standard 3x training schedule.",
    "code_link": ""
  },
  "aaai2022_main_arandomcnnseesobjectsoneinductivebiasofcnnanditsapplications": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications",
    "authors": [
      "Yun-Hao Cao",
      "Jianxin Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19894",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19894/19653",
    "published": "2022-02",
    "summary": "This paper starts by revealing a surprising finding: without any learning, a randomly initialized CNN can localize objects surprisingly well. That is, a CNN has an inductive bias to naturally focus on objects, named as Tobias (\"The object is at sight\") in this paper. This empirical inductive bias is further analyzed and successfully applied to self-supervised learning (SSL). A CNN is encouraged to learn representations that focus on the foreground object, by transforming every image into various versions with different backgrounds, where the foreground and background separation is guided by Tobias. Experimental results show that the proposed Tobias significantly improves downstream tasks, especially for object detection. This paper also shows that Tobias has consistent improvements on training sets of different sizes, and is more resilient to changes in image augmentations.",
    "code_link": ""
  },
  "aaai2022_main_texturegenerationusingdual-domainfeatureflowwithmulti-viewhallucinations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Texture Generation Using Dual-Domain Feature Flow with Multi-View Hallucinations",
    "authors": [
      "Seunggyu Chang",
      "Jungchan Cho",
      "Songhwai Oh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19895",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19895/19654",
    "published": "2022-02",
    "summary": "We propose a dual-domain generative model to estimate a texture map from a single image for colorizing a 3D human model. When estimating a texture map, a single image is insufficient as it reveals only one facet of a 3D object. To provide sufficient information for estimating a complete texture map, the proposed model simultaneously generates multi-view hallucinations in the image domain and an estimated texture map in the texture domain. During the generating process, each domain generator exchanges features to the other by a flow-based local attention mechanism. In this manner, the proposed model can estimate a texture map utilizing abundant multi-view image features from which multiview hallucinations are generated. As a result, the estimated texture map contains consistent colors and patterns over the entire region. Experiments show the superiority of our model for estimating a directly render-able texture map, which is applicable to 3D animation rendering. Furthermore, our model also improves an overall generation quality in the image domain for pose and viewpoint transfer tasks.",
    "code_link": ""
  },
  "aaai2022_main_resistancetrainingusingpriorbiastowardunbiasedscenegraphgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Resistance Training Using Prior Bias: Toward Unbiased Scene Graph Generation",
    "authors": [
      "Chao Chen",
      "Yibing Zhan",
      "Baosheng Yu",
      "Liu Liu",
      "Yong Luo",
      "Bo Du"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19896",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19896/19655",
    "published": "2022-02",
    "summary": "Scene Graph Generation (SGG) aims to build a structured representation of a scene using objects and pairwise relationships, which benefits downstream tasks. However, current SGG methods usually suffer from sub-optimal scene graph generation because of the long-tailed distribution of training data. To address this problem, we propose Resistance Training using Prior Bias (RTPB) for the scene graph generation. Specifically, RTPB uses a distributed-based prior bias to improve models' detecting ability on less frequent relationships during training, thus improving the model generalizability on tail categories. In addition, to further explore the contextual information of objects and relationships, we design a contextual encoding backbone network, termed as Dual Transformer (DTrans). We perform extensive experiments on a very popular benchmark, VG150, to demonstrate the effectiveness of our method for the unbiased scene graph generation. In specific, our RTPB achieves an improvement of over 10% under the mean recall when applied to current SGG methods. Furthermore, DTrans with RTPB outperforms nearly all state-of-the-art methods with a large margin. Code is available at https://github.com/ChCh1999/RTPB",
    "code_link": "https://github.com/ChCh1999/RTPB"
  },
  "aaai2022_main_sasasemantics-augmentedsetabstractionforpoint-based3dobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SASA: Semantics-Augmented Set Abstraction for Point-Based 3D Object Detection",
    "authors": [
      "Chen Chen",
      "Zhe Chen",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19897",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19897/19656",
    "published": "2022-02",
    "summary": "Although point-based networks are demonstrated to be accurate for 3D point cloud modeling, they are still falling behind their voxel-based competitors in 3D detection. We observe that the prevailing set abstraction design for down-sampling points may maintain too much unimportant background information that can affect feature learning for detecting objects. To tackle this issue, we propose a novel set abstraction method named Semantics-Augmented Set Abstraction (SASA). Technically, we first add a binary segmentation module as the side output to help identify foreground points. Based on the estimated point-wise foreground scores, we then propose a semantics-guided point sampling algorithm to help retain more important foreground points during down-sampling. In practice, SASA shows to be effective in identifying valuable points related to foreground objects and improving feature learning for point-based 3D detection. Additionally, it is an easy-to-plug-in module and able to boost various point-based detectors, including single-stage and two-stage ones. Extensive experiments on the popular KITTI and nuScenes datasets validate the superiority of SASA, lifting point-based detection models to reach comparable performance to state-of-the-art voxel-based methods. Code is available at https://github.com/blakechen97/SASA.",
    "code_link": "https://github.com/blakechen97/SASA"
  },
  "aaai2022_main_comprehensiveregularizationinabi-directionalpredictivenetworkforvideoanomalydetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Comprehensive Regularization in a Bi-directional Predictive Network for Video Anomaly Detection",
    "authors": [
      "Chengwei Chen",
      "Yuan Xie",
      "Shaohui Lin",
      "Angela Yao",
      "Guannan Jiang",
      "Wei\n      Zhang",
      "Yanyun Qu",
      "Ruizhi Qiao",
      "Bo Ren",
      "Lizhuang Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19898",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19898/19657",
    "published": "2022-02",
    "summary": "Video anomaly detection aims to automatically identify unusual objects or behaviours by learning from normal videos. Previous methods tend to use simplistic reconstruction or prediction constraints, which leads to the insufficiency of learned representations for normal data. As such, we propose a novel bi-directional architecture with three consistency constraints to comprehensively regularize the prediction task from pixel-wise, cross-modal, and temporal-sequence levels. First, predictive consistency is proposed to consider the symmetry property of motion and appearance in forwards and backwards time, which ensures the highly realistic appearance and motion predictions at the pixel-wise level. Second, association consistency considers the relevance between different modalities and uses one modality to regularize the prediction of another one. Finally, temporal consistency utilizes the relationship of the video sequence and ensures that the predictive network generates temporally consistent frames. During inference, the pattern of abnormal frames is unpredictable and will therefore cause higher prediction errors. Experiments show that our method outperforms advanced anomaly detectors and achieves state-of-the-art results on UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets.",
    "code_link": ""
  },
  "aaai2022_main_keypointmessagepassingforvideo-basedpersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Keypoint Message Passing for Video-Based Person Re-identification",
    "authors": [
      "Di Chen",
      "Andreas Doering",
      "Shanshan Zhang",
      "Jian Yang",
      "Juergen Gall",
      "Bernt\n      Schiele"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19899",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19899/19658",
    "published": "2022-02",
    "summary": "Video-based person re-identification~(re-ID) is an important technique in visual surveillance systems which aims to match video snippets of people captured by different cameras. Existing methods are mostly based on convolutional neural networks~(CNNs), whose building blocks either process local neighbor pixels at a time, or, when 3D convolutions are used to model temporal information, suffer from the misalignment problem caused by person movement. In this paper, we propose to overcome the limitations of normal convolutions with a human-oriented graph method. Specifically, features located at person joint keypoints are extracted and connected as a spatial-temporal graph. These keypoint features are then updated by message passing from their connected nodes with a graph convolutional network~(GCN). During training, the GCN can be attached to any CNN-based person re-ID model to assist representation learning on feature maps, whilst it can be dropped after training for better inference speed. Our method brings significant improvements over the CNN-based baseline model on the MARS dataset with generated person keypoints and a newly annotated dataset: PoseTrackReID. It also defines a new state-of-the-art method in terms of top-1 accuracy and mean average precision in comparison to prior works.",
    "code_link": "https://github.com/DeanChan/KeypointMessagePassing"
  },
  "aaai2022_main_dcanimprovingtemporalactiondetectionviadualcontextaggregation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DCAN: Improving Temporal Action Detection via Dual Context Aggregation",
    "authors": [
      "Guo Chen",
      "Yin-Dong Zheng",
      "Limin Wang",
      "Tong Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19900",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19900/19659",
    "published": "2022-02",
    "summary": "Temporal action detection aims to locate the boundaries of action in the video. The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals. However, these methods neglect the long-range context aggregation in boundary prediction. At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination. In this paper, we propose the end-to-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection. Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries. For matching evaluation, Coarse-to-fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine. We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.14% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance. We release the code at https://github.com/cg1177/DCAN.",
    "code_link": "https://github.com/cg1177/DCAN"
  },
  "aaai2022_main_geometry-contrastivetransformerforgeneralized3dposetransfer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Geometry-Contrastive Transformer for Generalized 3D Pose Transfer",
    "authors": [
      "Haoyu Chen",
      "Hao Tang",
      "Zitong Yu",
      "Nicu Sebe",
      "Guoying Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19901",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19901/19660",
    "published": "2022-02",
    "summary": "We present a customized 3D mesh Transformer model for the pose transfer task. As the 3D pose transfer essentially is a deformation procedure dependent on the given meshes, the intuition of this work is to perceive the geometric inconsistency between the given meshes with the powerful self-attention mechanism. Specifically, we propose a novel geometry-contrastive Transformer that has an efficient 3D structured perceiving ability to the global geometric inconsistencies across the given meshes. Moreover, locally, a simple yet efficient central geodesic contrastive loss is further proposed to improve the regional geometric-inconsistency learning. At last, we present a latent isometric regularization module together with a novel semi-synthesized dataset for the cross-dataset 3D pose transfer task towards unknown spaces. The massive experimental results prove the efficacy of our approach by showing state-of-the-art quantitative performances on SMPL-NPT, FAUST and our new proposed dataset SMG-3D datasets, as well as promising qualitative results on MG-cloth and SMAL datasets. It's demonstrated that our method can achieve robust 3D pose transfer and be generalized to challenging meshes from unknown spaces on cross-dataset tasks. The code and dataset are made available. Code is available: https://github.com/mikecheninoulu/CGT.",
    "code_link": "https://github.com/mikecheninoulu/CGT"
  },
  "aaai2022_main_exploreinter-contrastbetweenvideosviacompositionforweaklysupervisedtemporalsentencegrounding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Explore Inter-contrast between Videos via Composition for Weakly Supervised Temporal Sentence Grounding",
    "authors": [
      "Jiaming Chen",
      "Weixin Luo",
      "Wei Zhang",
      "Lin Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19902",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19902/19661",
    "published": "2022-02",
    "summary": "Weakly supervised temporal sentence grounding aims to temporally localize the target segment corresponding to a given natural language query, where it provides video-query pairs without temporal annotations during training. Most existing methods use the fused visual-linguistic feature to reconstruct the query, where the least reconstruction error determines the target segment. This work introduces a novel approach that explores the inter-contrast between videos in a composed video by selecting components from two different videos and fusing them into a single video. Such a straightforward yet effective composition strategy provides the temporal annotations at multiple composed positions, resulting in numerous videos with temporal ground-truths for training the temporal sentence grounding task. A transformer framework is introduced with multi-tasks training to learn a compact but efficient visual-linguistic space. The experimental results on the public Charades-STA and ActivityNet-Caption dataset demonstrate the effectiveness of the proposed method, where our approach achieves comparable performance over the state-of-the-art weakly-supervised baselines. The code is available at https://github.com/PPjmchen/Composition_WSTG.",
    "code_link": ""
  },
  "aaai2022_main_adaptiveimage-to-videoscenegraphgenerationviaknowledgereasoningandadversariallearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Image-to-Video Scene Graph Generation via Knowledge Reasoning and Adversarial Learning",
    "authors": [
      "Jin Chen",
      "Xiaofeng Ji",
      "Xinxiao Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19903",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19903/19662",
    "published": "2022-02",
    "summary": "Scene graph in a video conveys a wealth of information about objects and their relationships in the scene, thus benefiting many downstream tasks such as video captioning and visual question answering. Existing methods of scene graph generation require large-scale training videos annotated with objects and relationships in each frame to learn a powerful model. However, such comprehensive annotation is time-consuming and labor-intensive. On the other hand, it is much easier and less cost to annotate images with scene graphs, so we investigate leveraging annotated images to facilitate training a scene graph generation model for unannotated videos, namely image-to-video scene graph generation. This task presents two challenges: 1) infer unseen dynamic relationships in videos from static relationships in images due to the absence of motion information in images; 2) adapt objects and static relationships from images to video frames due to the domain shift between them. To address the first challenge, we exploit external commonsense knowledge to infer the unseen dynamic relationship from the temporal evolution of static relationships. We tackle the second challenge by hierarchical adversarial learning to reduce the data distribution discrepancy between images and video frames. Extensive experiment results on two benchmark video datasets demonstrate the effectiveness of our method.",
    "code_link": ""
  },
  "aaai2022_main_textgestaltstroke-awarescenetextimagesuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Text Gestalt: Stroke-Aware Scene Text Image Super-resolution",
    "authors": [
      "Jingye Chen",
      "Haiyang Yu",
      "Jianqi Ma",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19904",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19904/19663",
    "published": "2022-02",
    "summary": "In the last decade, the blossom of deep learning has witnessed the rapid development of scene text recognition. However, the recognition of low-resolution scene text images remains a challenge. Even though some super-resolution methods have been proposed to tackle this problem, they usually treat text images as general images while ignoring the fact that the visual quality of strokes (the atomic unit of text) plays an essential role for text recognition. According to Gestalt Psychology, humans are capable of composing parts of details into the most similar objects guided by prior knowledge. Likewise, when humans observe a low-resolution text image, they will inherently use partial stroke-level details to recover the appearance of holistic characters. Inspired by Gestalt Psychology, we put forward a Stroke-Aware Scene Text Image Super-Resolution method containing a Stroke-Focused Module (SFM) to concentrate on stroke-level internal structures of characters in text images. Specifically, we attempt to design rules for decomposing English characters and digits at stroke-level, then pre-train a text recognizer to provide stroke-level attention maps as positional clues with the purpose of controlling the consistency between the generated super-resolution image and high-resolution ground truth. The extensive experimental results validate that the proposed method can indeed generate more distinguishable images on TextZoom and manually constructed Chinese character dataset Degraded-IC13. Furthermore, since the proposed SFM is only used to provide stroke-level guidance when training, it will not bring any time overhead during the test phase. Code is available at https://github.com/FudanVI/FudanOCR/tree/main/text-gestalt.",
    "code_link": "https://github.com/FudanVI/FudanOCR"
  },
  "aaai2022_main_towardshigh-fidelityfaceself-occlusionrecoveryviamulti-viewresidual-basedganinversion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion",
    "authors": [
      "Jinsong Chen",
      "Hu Han",
      "Shiguang Shan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19905",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19905/19664",
    "published": "2022-02",
    "summary": "Face self-occlusions are inevitable due to the 3D nature of the human face and the loss of information in the projection process from 3D to 2D images. While recovering face self-occlusions based on 3D face reconstruction, e.g., 3D Morphable Model (3DMM) and its variants provides an effective solution, most of the existing methods show apparent limitations in expressing high-fidelity, natural, and diverse facial details. To overcome these limitations, we propose in this paper a new generative adversarial network (MvInvert) for natural face self-occlusion recovery without using paired image-texture data. We design a coarse-to-fine generator for photorealistic texture generation. A coarse texture is computed by inpainting the invisible areas in the photorealistic but incomplete texture sampled directly from the 2D image using the unrealistic but complete statistical texture from 3DMM. Then, we design a multi-view Residual-based GAN Inversion, which re-renders and refines multi-view 2D images, which are used for extracting multiple high-fidelity textures. Finally, these high-fidelity textures are fused based on their visibility maps via Poisson blending. To perform adversarial learning to assure the quality of the recovered texture, we design a discriminator consisting of two heads, i.e., one for global and local discrimination between the recovered texture and a small set of real textures in UV space, and the other for discrimination between the input image and the re-rendered 2D face images via pixel-wise, identity, and adversarial losses. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in face self-occlusion recovery under unconstrained scenarios.",
    "code_link": ""
  },
  "aaai2022_main_progressivemotionsegmutuallyreinforcedframeworkforevent-basedmotionsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation",
    "authors": [
      "Jinze Chen",
      "Yang Wang",
      "Yang Cao",
      "Feng Wu",
      "Zheng-Jun Zha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19906",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19906/19665",
    "published": "2022-02",
    "summary": "Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting apparent motion of objects with microsecond resolution, and shows great application potential in monitoring and other fields. However, the output event stream of existing DVS inevitably contains background activity noise (BA noise) due to dark current and junction leakage current, which will affect the temporal correlation of objects, resulting in deteriorated motion estimation performance. Particularly, the existing filter-based denoising methods cannot be directly applied to suppress the noise in event stream, since there is no spatial correlation. To address this issue, this paper presents a novel progressive framework, in which a Motion Estimation (ME) module and an Event Denoising (ED) module are jointly optimized in a mutually reinforced manner. Specifically, based on the maximum sharpness criterion, ME module divides the input event into several segments by adaptive clustering in a motion compensating warp field, and captures the temporal correlation of event stream according to the clustered motion parameters. Taking temporal correlation as guidance, ED module calculates the confidence that each event belongs to real activity events, and transmits it to ME module to update energy function of motion segmentation for noise suppression. The two steps are iteratively updated until stable motion segmentation results are obtained. Extensive experimental results on both synthetic and real datasets demonstrate the superiority of our proposed approaches against the State-Of-The-Art (SOTA) methods.",
    "code_link": ""
  },
  "aaai2022_main_attackingvideorecognitionmodelswithbullet-screencomments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Attacking Video Recognition Models with Bullet-Screen Comments",
    "authors": [
      "Kai Chen",
      "Zhipeng Wei",
      "Jingjing Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19907",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19907/19666",
    "published": "2022-02",
    "summary": "Recent research has demonstrated that Deep Neural Networks (DNNs) are vulnerable to adversarial patches which introduce perceptible but localized changes to the input. Nevertheless, existing approaches have focused on generating adversarial patches on images, their counterparts in videos have been less explored. Compared with images, attacking videos is much more challenging as it needs to consider not only spatial cues but also temporal cues. To close this gap, we introduce a novel adversarial attack in this paper, the bullet-screen comment (BSC) attack, which attacks video recognition models with BSCs. Specifically, adversarial BSCs are generated with a Reinforcement Learning (RL) framework, where the environment is set as the target model and the agent plays the role of selecting the position and transparency of each BSC. By continuously querying the target models and receiving feedback, the agent gradually adjusts its selection strategies in order to achieve a high fooling rate with non-overlapping BSCs. As BSCs can be regarded as a kind of meaningful patch, adding it to a clean video will not affect people\u2019s understanding of the video content, nor will arouse people\u2019s suspicion. We conduct extensive experiments to verify the effectiveness of the proposed method. On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve about 90% fooling rate when attacking three mainstream video recognition models, while only occluding < 8% areas in the video. Our code is available at https://github.com/kay-ck/BSC-attack.",
    "code_link": "https://github.com/kay-ck/BSC-attack"
  },
  "aaai2022_main_vitaamulti-sourcevicinaltransferaugmentationmethodforout-of-distributiongeneralization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "VITA: A Multi-Source Vicinal Transfer Augmentation Method for Out-of-Distribution Generalization",
    "authors": [
      "Minghui Chen",
      "Cheng Wen",
      "Feng Zheng",
      "Fengxiang He",
      "Ling Shao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19908",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19908/19667",
    "published": "2022-02",
    "summary": "Invariance to diverse types of image corruption, such as noise, blurring, or colour shifts, is essential to establish robust models in computer vision. Data augmentation has been the major approach in improving the robustness against common corruptions. However, the samples produced by popular augmentation strategies deviate significantly from the underlying data manifold. As a result, performance is skewed toward certain types of corruption. To address this issue, we propose a multi-source vicinal transfer augmentation (VITA) method for generating diverse on-manifold samples. The proposed VITA consists of two complementary parts: tangent transfer and integration of multi-source vicinal samples. The tangent transfer creates initial augmented samples for improving corruption robustness. The integration employs a generative model to characterize the underlying manifold built by vicinal samples, facilitating the generation of on-manifold samples. Our proposed VITA significantly outperforms the current state-of-the-art augmentation methods, demonstrated in extensive experiments on corruption benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_transzeroattribute-guidedtransformerforzero-shotlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TransZero: Attribute-Guided Transformer for Zero-Shot Learning",
    "authors": [
      "Shiming Chen",
      "Ziming Hong",
      "Yang Liu",
      "Guo-Sen Xie",
      "Baigui Sun",
      "Hao Li",
      "Qinmu Peng",
      "Ke Lu",
      "Xinge You"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19909",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19909/19668",
    "published": "2022-02",
    "summary": "Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen ones. Semantic knowledge is learned from attribute descriptions shared between different classes, which are strong prior for localization of object attribute for representing discriminative region features enabling significant visual-semantic interaction. Although few attention-based models have attempted to learn such region features in a single image, the transferability and discriminative attribute localization of visual features are typically neglected. In this paper, we propose an attribute-guided Transformer network to learn the attribute localization for discriminative visual-semantic embedding representations in ZSL, termed TransZero. Specifically, TransZero takes a feature augmentation encoder to alleviate the cross-dataset bias between ImageNet and ZSL benchmarks and improve the transferability of visual features by reducing the entangled relative geometry relationships among region features. To learn locality-augmented visual features, TransZero employs a visual-semantic decoder to localize the most relevant image regions to each attributes from a given image under the guidance of attribute semantic information. Then, the locality-augmented visual features and semantic vectors are used for conducting effective visual-semantic interaction in a visual-semantic embedding network. Extensive experiments show that TransZero achieves a new state-of-the-art on three ZSL benchmarks. The codes are available at: https://github.com/shiming-chen/TransZero.",
    "code_link": "https://github.com/shiming-chen/TransZero"
  },
  "aaai2022_main_structuredsemantictransferformulti-labelrecognitionwithpartiallabels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Structured Semantic Transfer for Multi-Label Recognition with Partial Labels",
    "authors": [
      "Tianshui Chen",
      "Tao Pu",
      "Hefeng Wu",
      "Yuan Xie",
      "Liang Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19910",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19910/19669",
    "published": "2022-02",
    "summary": "Multi-label image recognition is a fundamental yet practical task because real-world images inherently possess multiple semantic labels. However, it is difficult to collect large-scale multi-label annotations due to the complexity of both the input images and output label spaces. To reduce the annotation cost, we propose a structured semantic transfer (SST) framework that enables training multi-label recognition models with partial labels, i.e., merely some labels are known while other labels are missing (also called unknown labels) per image. The framework consists of two complementary transfer modules that explore within-image and cross-image semantic correlations to transfer knowledge of known labels to generate pseudo labels for unknown labels. Specifically, an intra-image semantic transfer module learns image-specific label co-occurrence matrix and maps the known labels to complement unknown labels based on this matrix. Meanwhile, a cross-image transfer module learns category-specific feature similarities and helps complement unknown labels with high similarities. Finally, both known and generated labels are used to train the multi-label recognition models. Extensive experiments on the Microsoft COCO, Visual Genome and Pascal VOC datasets show that the proposed SST framework obtains superior performance over current state-of-the-art algorithms. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.",
    "code_link": "https://github.com/HCPLabSYSU/HCP-MLR-PL"
  },
  "aaai2022_main_sjdl-vehiclesemi-supervisedjointdefogginglearningforfoggyvehiclere-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SJDL-Vehicle: Semi-supervised Joint Defogging Learning for Foggy Vehicle Re-identification",
    "authors": [
      "Wei-Ting Chen",
      "I-Hsiang Chen",
      "Chih-Yuan Yeh",
      "Hao-Hsiang Yang",
      "Jian-Jiun\n      Ding",
      "Sy-Yen Kuo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19911",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19911/19670",
    "published": "2022-02",
    "summary": "Vehicle re-identification (ReID) has attracted considerable attention in computer vision. Although several methods have been proposed to achieve state-of-the-art performance on this topic, re-identifying vehicle in foggy scenes remains a great challenge due to the degradation of visibility. To our knowledge, this problem is still not well-addressed so far. In this paper, to address this problem, we propose a novel training framework called Semi-supervised Joint Defogging Learning (SJDL) framework. First, the fog removal branch and the re-identification branch are integrated to perform simultaneous training. With the collaborative training scheme, defogged features generated by the defogging branch from input images can be shared to learn better representation for the re-identification branch. However, since the fog-free image of real-world data is intractable, this architecture can only be trained on the synthetic data, which may cause the domain gap problem between real-world and synthetic scenarios. To solve this problem, we design a semi-supervised defogging training scheme that can train two kinds of data alternatively in each iteration. Due to the lack of a dataset specialized for vehicle ReID in the foggy weather, we construct a dataset called FVRID which consists of real-world and synthetic foggy images to train and evaluate the performance. Experimental results show that the proposed method is effective and outperforms other existing vehicle ReID methods in the foggy weather. The code and dataset are available in https://github.com/Cihsaing/SJDL-Foggy-Vehicle-Re-Identification--AAAI2022.",
    "code_link": "https://github.com/Cihsaing/SJDLFoggy-Vehicle-Re-Identification--AAAI2022"
  },
  "aaai2022_main_imaginebyreasoningareasoning-basedimplicitsemanticdataaugmentationforlong-tailedclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification",
    "authors": [
      "Xiaohua Chen",
      "Yucan Zhou",
      "Dayan Wu",
      "Wanqian Zhang",
      "Yu Zhou",
      "Bo Li",
      "Weiping\n      Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19912",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19912/19671",
    "published": "2022-02",
    "summary": "Real-world data often follows a long-tailed distribution, which makes the performance of existing classification algorithms degrade heavily. A key issue is that the samples in tail categories fail to depict their intra-class diversity. Humans can imagine a sample in new poses, scenes and view angles with their prior knowledge even if it is the first time to see this category. Inspired by this, we propose a novel reasoning-based implicit semantic data augmentation method to borrow transformation directions from other classes. Since the covariance matrix of each category represents the feature transformation directions, we can sample new directions from similar categories to generate definitely different instances. Specifically, the long-tailed distributed data is first adopted to train a backbone and a classifier. Then, a covariance matrix for each category is estimated, and a knowledge graph is constructed to store the relations of any two categories. Finally, tail samples are adaptively enhanced via propagating information from all the similar categories in the knowledge graph. Experimental results on CIFAR-LT-100, ImageNet-LT, and iNaturalist 2018 have demonstrated the effectiveness of our proposed method compared with the state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_guidelocalfeaturematchingbyoverlapestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Guide Local Feature Matching by Overlap Estimation",
    "authors": [
      "Ying Chen",
      "Dihe Huang",
      "Shang Xu",
      "Jianlin Liu",
      "Yong Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19913",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19913/19672",
    "published": "2022-02",
    "summary": "Local image feature matching under large appearance, viewpoint, and distance changes is challenging yet important. Conventional methods detect and match tentative local features across the whole images, with heuristic consistency checks to guarantee reliable matches. In this paper, we introduce a novel Overlap Estimation method conditioned on image pairs with TRansformer, named OETR, to constrain local feature matching in the commonly visible region. OETR performs overlap estimation in a two step process of feature correlation and then overlap regression. As a preprocessing module, OETR can be plugged into any existing local feature detection and matching pipeline, to mitigate potential view angle or scale variance. Intensive experiments show that OETR can boost state of the art local feature matching performance substantially, especially for image pairs with small shared regions. The code will be publicly available at https://github.com/AbyssGaze/OETR.",
    "code_link": "https://github.com/AbyssGaze/OETR"
  },
  "aaai2022_main_causalinterventionforsubject-deconfoundedfacialactionunitrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition",
    "authors": [
      "Yingjie Chen",
      "Diqi Chen",
      "Tao Wang",
      "Yizhou Wang",
      "Yun Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19914",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19914/19673",
    "published": "2022-02",
    "summary": "Subject-invariant facial action unit (AU) recognition remains challenging for the reason that the data distribution varies among subjects. In this paper, we propose a causal inference framework for subject-invariant facial action unit recognition. To illustrate the causal effect existing in AU recognition task, we formulate the causalities among facial images, subjects, latent AU semantic relations, and estimated AU occurrence probabilities via a structural causal model. By constructing such a causal diagram, we clarify the causal-effect among variables and propose a plug-in causal intervention module, CIS, to deconfound the confounder Subject in the causal diagram. Extensive experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of our CIS, and the model with CIS inserted, CISNet, has achieved state-of-the-art performance.",
    "code_link": ""
  },
  "aaai2022_main_deepone-classclassificationviainterpolatedgaussiandescriptor": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep One-Class Classification via Interpolated Gaussian Descriptor",
    "authors": [
      "Yuanhong Chen",
      "Yu Tian",
      "Guansong Pang",
      "Gustavo Carneiro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19915",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19915/19674",
    "published": "2022-02",
    "summary": "One-class classification (OCC) aims to learn an effective data description to enclose all normal training samples and detect anomalies based on the deviation from the data description. Current state-of-the-art OCC models learn a compact normality description by hyper-sphere minimisation, but they often suffer from overfitting the training data, especially when the training set is small or contaminated with anomalous samples. To address this issue, we introduce the interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples. The Gaussian anomaly classifier differentiates the training samples based on their distance to the Gaussian centre and the standard deviation of these distances, offering the model a discriminability w.r.t. the given samples during training. The adversarial interpolation is enforced to consistently learn a smooth Gaussian descriptor, even when the training data is small or contaminated with anomalous samples. This enables our model to learn the data description based on the representative normal samples rather than fringe or anomalous samples, resulting in significantly improved normality description. In extensive experiments on diverse popular benchmarks, including MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves better detection accuracy than current state-of-the-art models. IGD also shows better robustness in problems with small or contaminated training sets.",
    "code_link": ""
  },
  "aaai2022_main_towardsultra-resolutionneuralstyletransferviathumbnailinstancenormalization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization",
    "authors": [
      "Zhe Chen",
      "Wenhai Wang",
      "Enze Xie",
      "Tong Lu",
      "Ping Luo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19916",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19916/19675",
    "published": "2022-02",
    "summary": "We present an extremely simple Ultra-Resolution Style Transfer framework, termed URST, to flexibly process arbitrary high-resolution images (e.g., 10000x10000 pixels) style transfer for the first time. Most of the existing state-of-the-art methods would fall short due to massive memory cost and small stroke size when processing ultra-high resolution images. URST completely avoids the memory problem caused by ultra-high resolution images by (1) dividing the image into small patches and (2) performing patch-wise style transfer with a novel Thumbnail Instance Normalization (TIN). Specifically, TIN can extract thumbnail features' normalization statistics and apply them to small patches, ensuring the style consistency among different patches.Overall, the URST framework has three merits compared to prior arts. (1) We divide input image into small patches and adopt TIN, successfully transferring image style with arbitrary high-resolution. (2) Experiments show that our URST surpasses existing SOTA methods on ultra-high resolution images benefiting from the effectiveness of the proposed stroke perceptual loss in enlarging the stroke size. (3) Our URST can be easily plugged into most existing style transfer methods and directly improve their performance even without training. Code is available at https://git.io/URST.",
    "code_link": ""
  },
  "aaai2022_main_detarnetdecouplingtranslationandrotationbysiamesenetworkforpointcloudregistration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeTarNet: Decoupling Translation and Rotation by Siamese Network for Point Cloud Registration",
    "authors": [
      "Zhi Chen",
      "Fan Yang",
      "Wenbing Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19917",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19917/19676",
    "published": "2022-02",
    "summary": "Point cloud registration is a fundamental step for many tasks. In this paper, we propose a neural network named DetarNet to decouple the translation t and rotation R, so as to overcome the performance degradation due to their mutual interference in point cloud registration. First, a Siamese Network based Progressive and Coherent Feature Drift (PCFD) module is proposed to align the source and target points in high-dimensional feature space, and accurately recover translation from the alignment process. Then we propose a Consensus Encoding Unit (CEU) to construct more distinguishable features for a set of putative correspondences. After that, a Spatial and Channel Attention (SCA) block is adopted to build a classification network for finding good correspondences. Finally, the rotation is obtained by Singular Value Decomposition (SVD). In this way, the proposed network decouples the estimation of translation and rotation, resulting in better performance for both of them. Experimental results demonstrate that the proposed DetarNet improves registration performance on both indoor and outdoor scenes. Our code will be available in https://github.com/ZhiChen902/DetarNet.",
    "code_link": "https://github.com/ZhiChen902/DetarNet"
  },
  "aaai2022_main_lctronawakeningthelocalcontinuityoftransformerforweaklysupervisedobjectlocalization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization",
    "authors": [
      "Zhiwei Chen",
      "Changan Wang",
      "Yabiao Wang",
      "Guannan Jiang",
      "Yunhang Shen",
      "Ying\n      Tai",
      "Chengjie Wang",
      "Wei Zhang",
      "Liujuan Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19918",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19918/19677",
    "published": "2022-02",
    "summary": "Weakly supervised object localization (WSOL) aims to learn object localizer solely by using image-level labels. The convolution neural network (CNN) based techniques often result in highlighting the most discriminative part of objects while ignoring the entire object extent. Recently, the transformer architecture has been deployed to WSOL to capture the long-range feature dependencies with self-attention mechanism and multilayer perceptron structure. Nevertheless, transformers lack the locality inductive bias inherent to CNNs and therefore may deteriorate local feature details in WSOL. In this paper, we propose a novel framework built upon the transformer, termed LCTR (Local Continuity TRansformer), which targets at enhancing the local perception capability of global features among long-range feature dependencies. To this end, we propose a relational patch-attention module (RPAM), which considers cross-patch information on a global basis. We further design a cue digging module (CDM), which utilizes local features to guide the learning trend of the model for highlighting the weak local responses. Finally, comprehensive experiments are carried out on two widely used datasets, ie, CUB-200-2011 and ILSVRC, to verify the effectiveness of our method.",
    "code_link": ""
  },
  "aaai2022_main_efficientvirtualviewselectionfor3dhandposeestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Virtual View Selection for 3D Hand Pose Estimation",
    "authors": [
      "Jian Cheng",
      "Yanguang Wan",
      "Dexin Zuo",
      "Cuixia Ma",
      "Jian Gu",
      "Ping Tan",
      "Hongan\n      Wang",
      "Xiaoming Deng",
      "Yinda Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19919",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19919/19678",
    "published": "2022-02",
    "summary": "3D hand pose estimation from single depth is a fundamental problem in computer vision, and has wide applications. However, the existing methods still can not achieve satisfactory hand pose estimation results due to view variation and occlusion of human hand. In this paper, we propose a new virtual view selection and fusion module for 3D hand pose estimation from single depth. We propose to automatically select multiple virtual viewpoints for pose estimation and fuse the results of all and find this empirically delivers accurate and robust pose estimation. In order to select most effective virtual views for pose fusion, we evaluate the virtual views based on the confidence of virtual views using a light-weight network via network distillation. Experiments on three main benchmark datasets including NYU, ICVL and Hands2019 demonstrate that our method outperforms the state-of-the-arts on NYU and ICVL, and achieves very competitive performance on Hands2019-Task1, and our proposed virtual view selection and fusion module is both effective for 3D hand pose estimation.",
    "code_link": ""
  },
  "aaai2022_main_poseadaptivedualmixupforfew-shotsingle-view3dreconstruction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction",
    "authors": [
      "Ta-Ying Cheng",
      "Hsuan-Ru Yang",
      "Niki Trigoni",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19920",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19920/19679",
    "published": "2022-02",
    "summary": "We present a pose adaptive few-shot learning procedure and a two-stage data interpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for single-image 3D reconstruction. While augmentations via interpolating feature-label pairs are effective in classification tasks, they fall short in shape predictions potentially due to inconsistencies between interpolated products of two images and volumes when rendering viewpoints are unknown. PADMix targets this issue with two sets of mixup procedures performed sequentially. We first perform an input mixup which, combined with a pose adaptive learning procedure, is helpful in learning 2D feature extraction and pose adaptive latent encoding. The stagewise training allows us to build upon the pose invariant representations to perform a follow-up latent mixup under one-to-one correspondences between features and ground-truth volumes. PADMix significantly outperforms previous literature on few-shot settings over the ShapeNet dataset and sets new benchmarks on the more challenging real-world Pix3D dataset.",
    "code_link": ""
  },
  "aaai2022_main_puregazepurifyinggazefeatureforgeneralizablegazeestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PureGaze: Purifying Gaze Feature for Generalizable Gaze Estimation",
    "authors": [
      "Yihua Cheng",
      "Yiwei Bao",
      "Feng Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19921",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19921/19680",
    "published": "2022-02",
    "summary": "Gaze estimation methods learn eye gaze from facial features. However, among rich information in the facial image, real gaze-relevant features only correspond to subtle changes in eye region, while other gaze-irrelevant features like illumination, personal appearance and even facial expression may affect the learning in an unexpected way. This is a major reason why existing methods show significant performance degradation in cross-domain/dataset evaluation. In this paper, we tackle the cross-domain problem in gaze estimation. Different from common domain adaption methods, we propose a domain generalization method to improve the cross-domain performance without touching target samples. The domain generalization is realized by gaze feature purification. We eliminate gaze-irrelevant factors such as illumination and identity to improve the cross-domain performance. We design a plug-and-play self-adversarial framework for the gaze feature purification. The framework enhances not only our baseline but also existing gaze estimation methods directly and significantly. To the best of our knowledge, we are the first to propose domain generalization methods in gaze estimation. Our method achieves not only state-of-the-art performance among typical gaze estimation methods but also competitive results among domain adaption methods. The code is released in https://github.com/yihuacheng/PureGaze.",
    "code_link": "https://github.com/yihuacheng/PureGaze"
  },
  "aaai2022_main_(2.5+1)dspatio-temporalscenegraphsforvideoquestionanswering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering",
    "authors": [
      "Anoop Cherian",
      "Chiori Hori",
      "Tim K. Marks",
      "Jonathan Le Roux"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19922",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19922/19681",
    "published": "2022-02",
    "summary": "Spatio-temporal scene-graph approaches to video-based reasoning tasks, such as video question-answering (QA), typically construct such graphs for every video frame. These approaches often ignore the fact that videos are essentially sequences of 2D ``views'' of events happening in a 3D space, and that the semantics of the 3D scene can thus be carried over from frame to frame. Leveraging this insight, we propose a (2.5+1)D scene graph representation to better capture the spatio-temporal information flows inside the videos. Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D transformation module, following which we register the video frames into a shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it. Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic sub-graph, corresponding to whether the objects within them usually move in the world. The nodes in the dynamic graph are enriched with motion features capturing their interactions with other graph nodes. Next, for the video QA task, we present a novel transformer-based reasoning pipeline that embeds the (2.5+1)D graph into a spatio-temporal hierarchical latent space, where the sub-graphs and their interactions are captured at varied granularity. To demonstrate the effectiveness of our approach, we present experiments on the NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D representation leads to faster training and inference, while our hierarchical model showcases superior performance on the video QA task versus the state of the art.",
    "code_link": ""
  },
  "aaai2022_main_event-imagefusionstereousingcross-modalityfeaturepropagation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Event-Image Fusion Stereo Using Cross-Modality Feature Propagation",
    "authors": [
      "Hoonhee Cho",
      "Kuk-Jin Yoon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19923",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19923/19682",
    "published": "2022-02",
    "summary": "Event cameras asynchronously output the polarity values of pixel-level log intensity alterations. They are robust against motion blur and can be adopted in challenging light conditions. Owing to these advantages, event cameras have been employed in various vision tasks such as depth estimation, visual odometry, and object detection. In particular, event cameras are effective in stereo depth estimation to \ufb01nd correspondence points between two cameras under challenging illumination conditions and/or fast motion. However, because event cameras provide spatially sparse event stream data, it is dif\ufb01cult to obtain a dense disparity map. Although it is possible to estimate disparity from event data at the edge of a structure where intensity changes are likely to occur, estimating the disparity in a region where event occurs rarely is challenging. In this study, we propose a deep network that combines the features of an image with the features of an event to generate a dense disparity map. The proposed network uses images to obtain spatially dense features that are lacking in events. In addition, we propose a spatial multi-scale correlation between two fused feature maps for an accurate disparity map. To validate our method, we conducted experiments using synthetic and real-world datasets.",
    "code_link": ""
  },
  "aaai2022_main_style-guidedanddisentangledrepresentationforrobustimage-to-imagetranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Style-Guided and Disentangled Representation for Robust Image-to-Image Translation",
    "authors": [
      "Jaewoong Choi",
      "Daeha Kim",
      "Byung Cheol Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19924",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19924/19683",
    "published": "2022-02",
    "summary": "Recently, various image-to-image translation (I2I) methods have improved mode diversity and visual quality in terms of neural networks or regularization terms. However, conventional I2I methods relies on a static decision boundary and the encoded representations in those methods are entangled with each other, so they often face with \u2018mode collapse\u2019 phenomenon. To mitigate mode collapse, 1) we design a so-called style-guided discriminator that guides an input image to the target image style based on the strategy of flexible decision boundary. 2) Also, we make the encoded representations include independent domain attributes. Based on two ideas, this paper proposes Style-Guided and Disentangled Representation for Robust Image-to-Image Translation (SRIT). SRIT showed outstanding FID by 8%, 22.8%, and 10.1% for CelebA-HQ, AFHQ, and Yosemite datasets, respectively. The translated images of SRIT reflect the styles of target domain successfully. This indicates that SRIT shows better mode diversity than previous works.",
    "code_link": "https://github.com/jaewoong1/SRIT"
  },
  "aaai2022_main_denoisedmaximumclassi\ufb01erdiscrepancyforsource-freeunsuperviseddomainadaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Denoised Maximum Classi\ufb01er Discrepancy for Source-Free Unsupervised Domain Adaptation",
    "authors": [
      "Tong Chu",
      "Yahao Liu",
      "Jinhong Deng",
      "Wen Li",
      "Lixin Duan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19925",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19925/19684",
    "published": "2022-02",
    "summary": "Source-Free Unsupervised Domain Adaptation(SFUDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to the original labeled source domain samples. Many existing SFUDA approaches apply the self-training strategy, which involves iteratively selecting confidently predicted target samples as pseudo-labeled samples used to train the model to fit the target domain. However, the self-training strategy may also suffer fromsample selection bias and be impacted by the label noise of the pseudo-labeled samples. In this work, we provide a rigorous theoretical analysis on how these two issues affect the model generalization ability when applying the self-training strategy for the SFUDA problem. Based on this theoretical analysis, we then propose a new Denoised Maximum Classifier Discrepancy (D-MCD) method for SFUDA to effectively address these two issues. In particular, we first minimize the distribution mismatch between the selected pseudo-labeled samples and the remaining target domain samples to alleviate the sample selection bias. Moreover, we design a strong-weak self-training paradigm to denoise the selected pseudo-labeled samples, where the strong network is used to select pseudo-labeled samples while the weak network helps the strong network to filter out hard samples to avoid incorrect labels. In this way, we are able to ensure both the quality of the pseudo-labels and the generalization ability of the trained model on the target domain. We achieve state-of-the-art results on three domain adaptation benchmark datasets, which clearly validates the effectiveness of our proposed approach. Full code is available at https://github.com/kkkkkkon/D-MCD.",
    "code_link": "https://github.com/kkkkkkon/D-MCD"
  },
  "aaai2022_main_model-basedimagesignalprocessorsvialearnabledictionaries": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Model-Based Image Signal Processors via Learnable Dictionaries",
    "authors": [
      "Marcos V. Conde",
      "Steven McDonagh",
      "Matteo Maggioni",
      "Ales Leonardis",
      "Eduardo\n      P\u00e9rez-Pellitero"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19926",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19926/19685",
    "published": "2022-02",
    "summary": "Digital cameras transform sensor RAW readings into RGB images by means of their Image Signal Processor (ISP). Computational photography tasks such as image denoising and colour constancy are commonly performed in the RAW domain, in part due to the inherent hardware design, but also due to the appealing simplicity of noise statistics that result from the direct sensor readings. Despite this, the availability of RAW images is limited in comparison with the abundance and diversity of available RGB data. Recent approaches have attempted to bridge this gap by estimating the RGB to RAW mapping: handcrafted model-based methods that are interpretable and controllable usually require manual parameter fine-tuning, while end-to-end learnable neural networks require large amounts of training data, at times with complex training procedures, and generally lack interpretability and parametric control. Towards addressing these existing limitations, we present a novel hybrid model-based and data-driven ISP that builds on canonical ISP operations and is both learnable and interpretable. Our proposed invertible model, capable of bidirectional mapping between RAW and RGB domains, employs end-to-end learning of rich parameter representations, i.e. dictionaries, that are free from direct parametric supervision and additionally enable simple and plausible data augmentation. We evidence the value of our data generation process by extensive experiments under both RAW image reconstruction and RAW image denoising tasks, obtaining state-of-the-art performance in both. Additionally, we show that our ISP can learn meaningful mappings from few data samples, and that denoising models trained with our dictionary-based data augmentation are competitive despite having only few or zero ground-truth labels.",
    "code_link": ""
  },
  "aaai2022_main_mmamulti-camerabasedglobalmotionaveraging": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MMA: Multi-Camera Based Global Motion Averaging",
    "authors": [
      "Hainan Cui",
      "Shuhan Shen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19927",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19927/19686",
    "published": "2022-02",
    "summary": "In order to fully perceive the surrounding environment, many intelligent robots and self-driving cars are equipped with a multi-camera system. Based on this system, the structure-from-motion (SfM) technology is used to realize scene reconstruction, but the fixed relative poses between cameras in the multi-camera system are usually not considered. This paper presents a tailor-made multi-camera based motion averaging system, where the fixed relative poses are utilized to improve the accuracy and robustness of SfM. Our approach starts by dividing the images into reference images and non-reference images, and edges in view-graph are divided into four categories accordingly. Then, a multi-camera based rotating averaging problem is formulated and solved in two stages, where an iterative re-weighted least squares scheme is used to deal with outliers. Finally, a multi-camera based translation averaging problem is formulated and a l1-norm based optimization scheme is proposed to compute the relative translations of multi-camera system and reference camera positions simultaneously. Experiments demonstrate that our algorithm achieves superior accuracy and robustness on various data sets compared to the state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_gencogenerativeco-trainingforgenerativeadversarialnetworkswithlimiteddata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GenCo: Generative Co-training for Generative Adversarial Networks with Limited Data",
    "authors": [
      "Kaiwen Cui",
      "Jiaxing Huang",
      "Zhipeng Luo",
      "Gongjie Zhang",
      "Fangneng Zhan",
      "Shijian Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19928",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19928/19687",
    "published": "2022-02",
    "summary": "Training effective Generative Adversarial Networks (GANs) requires large amounts of training data, without which the trained models are usually sub-optimal with discriminator over-fitting. Several prior studies address this issue by expanding the distribution of the limited training data via massive and hand-crafted data augmentation. We handle data-limited image generation from a very different perspective. Specifically, we design GenCo, a Generative Co-training network that mitigates the discriminator over-fitting issue by introducing multiple complementary discriminators that provide diverse supervision from multiple distinctive views in training. We instantiate the idea of GenCo in two ways. The first way is Weight-Discrepancy Co-training (WeCo) which co-trains multiple distinctive discriminators by diversifying their parameters. The second way is Data-Discrepancy Co-training (DaCo) which achieves co-training by feeding discriminators with different views of the input images. Extensive experiments over multiple benchmarks show that GenCo achieves superior generation with limited training data. In addition, GenCo also complements the augmentation approach with consistent and clear performance gains when combined.",
    "code_link": ""
  },
  "aaai2022_main_unbiasediouforsphericalimageobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unbiased IoU for Spherical Image Object Detection",
    "authors": [
      "Feng Dai",
      "Bin Chen",
      "Hang Xu",
      "Yike Ma",
      "Xiaodong Li",
      "Bailan Feng",
      "Peng Yuan",
      "Chenggang Yan",
      "Qiang Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19929",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19929/19688",
    "published": "2022-02",
    "summary": "As one of the fundamental components of object detection, intersection-over-union (IoU) calculations between two bounding boxes play an important role in samples selection, NMS operation and evaluation of object detection algorithms. This procedure is well-defined and solved for planar images, while it is challenging for spherical ones. Some existing methods utilize planar bounding boxes to represent spherical objects. However, they are biased due to the distortions of spherical objects. Others use spherical rectangles as unbiased representations, but they adopt excessive approximate algorithms when computing the IoU. In this paper, we propose an unbiased IoU as a novel evaluation criterion for spherical image object detection, which is based on the unbiased representations and utilize unbiased analytical method for IoU calculation. This is the first time that the absolutely accurate IoU calculation is applied to the evaluation criterion, thus object detection algorithms can be correctly evaluated for spherical images. With the unbiased representation and calculation, we also present Spherical CenterNet, an anchor free object detection algorithm for spherical images. The experiments show that our unbiased IoU gives accurate results and the proposed Spherical CenterNet achieves better performance on one real-world and two synthetic spherical object detection datasets than existing methods.",
    "code_link": ""
  },
  "aaai2022_main_insclrimprovinginstanceretrievalwithself-supervision": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "InsCLR: Improving Instance Retrieval with Self-Supervision",
    "authors": [
      "Zelu Deng",
      "Yujie Zhong",
      "Sheng Guo",
      "Weilin Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19930",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19930/19689",
    "published": "2022-02",
    "summary": "This work aims at improving instance retrieval with self-supervision. We find that fine-tuning using the recently developed self-supervised learning (SSL) methods, such as SimCLR and MoCo, fails to improve the performance of instance retrieval. In this work, we identify that the learnt representations for instance retrieval should be invariant to large variations in viewpoint and background etc., whereas self-augmented positives applied by the current SSL methods can not provide strong enough signals for learning robust instance-level representations. To overcome this problem, we propose InsCLR, a new SSL method that builds on the instance-level contrast, to learn the intra-class invariance by dynamically mining meaningful pseudo positive samples from both mini-batches and a memory bank during training. Extensive experiments demonstrate that InsCLR achieves similar or even better performance than the state-of-the-art SSL methods on instance retrieval. Code is available at https://github.com/zeludeng/insclr.",
    "code_link": "https://github.com/zeludeng/insclr"
  },
  "aaai2022_main_spatio-temporalrecurrentnetworksforevent-basedopticalflowestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation",
    "authors": [
      "Ziluo Ding",
      "Rui Zhao",
      "Jiyuan Zhang",
      "Tianxiao Gao",
      "Ruiqin Xiong",
      "Zhaofei\n      Yu",
      "Tiejun Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19931",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19931/19690",
    "published": "2022-02",
    "summary": "Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin.",
    "code_link": ""
  },
  "aaai2022_main_constructeffectivegeometryawarefeaturepyramidnetworkformulti-scaleobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Construct Effective Geometry Aware Feature Pyramid Network for Multi-Scale Object Detection",
    "authors": [
      "Jinpeng Dong",
      "Yuhao Huang",
      "Songyi Zhang",
      "Shitao Chen",
      "Nanning Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19932",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19932/19691",
    "published": "2022-02",
    "summary": "Feature Pyramid Network (FPN) has been widely adopted to exploit multi-scale features for scale variation in object detection. However, intrinsic defects in most of the current methods with FPN make it difficult to adapt to the feature of different geometric objects. To address this issue, we introduce geometric prior into FPN to obtain more discriminative features. In this paper, we propose Geometry-aware Feature Pyramid Network (GaFPN), which mainly consists of the novel Geometry-aware Mapping Module and Geometry-aware Predictor Head.The Geometry-aware Mapping Module is proposed to make full use of all pyramid features to obtain better proposal features by the weight-generation subnetwork. The weights generation subnetwork generates fusion weight for each layer proposal features by using the geometric information of the proposal. The Geometry-aware Predictor Head introduces geometric prior into predictor head by the embedding generation network to strengthen feature representation for classification and regression. Our GaFPN can be easily extended to other two-stage object detectors with feature pyramid and applied to instance segmentation task. The proposed GaFPN significantly improves detection performance compared to baseline detectors with ResNet-50-FPN: +1.9, +2.0, +1.7, +1.3, +0.8 points Average Precision (AP) on Faster-RCNN, Cascade R-CNN, Dynamic R-CNN, SABL, and AugFPN respectively on MS COCO dataset.",
    "code_link": ""
  },
  "aaai2022_main_complementaryattentiongatednetworkforpedestriantrajectoryprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Complementary Attention Gated Network for Pedestrian Trajectory Prediction",
    "authors": [
      "Jinghai Duan",
      "Le Wang",
      "Chengjiang Long",
      "Sanping Zhou",
      "Fang Zheng",
      "Liushuai\n      Shi",
      "Gang Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19933",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19933/19692",
    "published": "2022-02",
    "summary": "Pedestrian trajectory prediction is crucial in many practical applications due to the diversity of pedestrian movements, such as social interactions and individual motion behaviors. With similar observable trajectories and social environments, different pedestrians may make completely different future decisions. However, most existing methods only focus on the frequent modal of the trajectory and thus are difficult to generalize to the peculiar scenario, which leads to the decline of the multimodal fitting ability when facing similar scenarios. In this paper, we propose a complementary attention gated network (CAGN) for pedestrian trajectory prediction, in which a dual-path architecture including normal and inverse attention is proposed to capture both frequent and peculiar modals in spatial and temporal patterns, respectively. Specifically, a complementary block is proposed to guide normal and inverse attention, which are then be summed with learnable weights to get attention features by a gated network. Finally, multiple trajectory distributions are estimated based on the fused spatio-temporal attention features due to the multimodality of future trajectory. Experimental results on benchmark datasets, i.e., the ETH, and the UCY, demonstrate that our method outperforms state-of-the-art methods by 13.8% in Average Displacement Error (ADE) and 10.4% in Final Displacement Error (FDE). Code will be available at https://github.com/jinghaiD/CAGN",
    "code_link": "https://github.com/jinghaiD/CAGN"
  },
  "aaai2022_main_svt-netsuperlight-weightsparsevoxeltransformerforlargescaleplacerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale Place Recognition",
    "authors": [
      "Zhaoxin Fan",
      "Zhenbo Song",
      "Hongyan Liu",
      "Zhiwu Lu",
      "Jun He",
      "Xiaoyong Du"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19934",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19934/19693",
    "published": "2022-02",
    "summary": "Simultaneous Localization and Mapping (SLAM) and Autonomous Driving are becoming increasingly more important in recent years. Point cloud-based large scale place recognition is the spine of them. While many models have been proposed and have achieved acceptable performance by learning short-range local features, they always skip long-range contextual properties. Moreover, the model size also becomes a serious shackle for their wide applications. To overcome these challenges, we propose a super light-weight network model termed SVT-Net. On top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based Sparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer (CSVT) are proposed respectively to learn both short-range local features and long-range contextual features. Consisting of ASVT and CSVT, SVT-Net can achieve state-of-the-art performance in terms of both recognition accuracy and running speed with a super-light model size (0.9M parameters). Meanwhile, for the purpose of further boosting efficiency, we introduce two simplified versions, which also achieve state-of-the-art performance and further reduce the model size to 0.8M and 0.4M respectively.",
    "code_link": ""
  },
  "aaai2022_main_backdoorattacksonthednninterpretationsystem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Backdoor Attacks on the DNN Interpretation System",
    "authors": [
      "Shihong Fang",
      "Anna Choromanska"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19935",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19935/19694",
    "published": "2022-02",
    "summary": "Interpretability is crucial to understand the inner workings of deep neural networks (DNNs). Many interpretation methods help to understand the decision-making of DNNs by generating saliency maps that highlight parts of the input image that contribute the most to the prediction made by the DNN. In this paper we design a backdoor attack that alters the saliency map produced by the network for an input image with a specific trigger pattern while not losing the prediction performance significantly. The saliency maps are incorporated in the penalty term of the objective function that is used to train a deep model and its influence on model training is conditioned upon the presence of a trigger. We design two types of attacks: a targeted attack that enforces a specific modification of the saliency map and a non-targeted attack when the importance scores of the top pixels from the original saliency map are significantly reduced. We perform empirical evaluations of the proposed backdoor attacks on gradient-based interpretation methods, Grad-CAM and SimpleGrad, and a gradient-free scheme, VisualBackProp, for a variety of deep learning architectures. We show that our attacks constitute a serious security threat to the reliability of the interpretation methods when deploying models developed by untrusted sources. We furthermore show that existing backdoor defense mechanisms are ineffective in detecting our attacks. Finally, we demonstrate that the proposed methodology can be used in an inverted setting, where the correct saliency map can be obtained only in the presence of a trigger (key), effectively making the interpretation system available only to selected users.",
    "code_link": ""
  },
  "aaai2022_main_learningtolearntransferableattack": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Learn Transferable Attack",
    "authors": [
      "Shuman Fang",
      "Jie Li",
      "Xianming Lin",
      "Rongrong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19936",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19936/19695",
    "published": "2022-02",
    "summary": "Transfer adversarial attack is a non-trivial black-box adversarial attack that aims to craft adversarial perturbations on the surrogate model and then apply such perturbations to the victim model. However, the transferability of perturbations from existing methods is still limited, since the adversarial perturbations are easily over\ufb01tting with a single surrogate model and speci\ufb01c data pattern. In this paper, we propose a Learning to Learn Transferable Attack (LLTA) method, which makes the adversarial perturbations more generalized via learning from both data and model augmentation. For data augmentation, we adopt simple random resizing and padding. For model augmentation, we randomly alter the back propagation instead of the forward propagation to eliminate the effect on the model prediction. By treating the attack of both speci\ufb01c data and a modi\ufb01ed model as a task, we expect the adversarial perturbations to adopt enough tasks for generalization. To this end, the meta-learning algorithm is further introduced during the iteration of perturbation generation. Empirical results on the widely-used dataset demonstrate the effectiveness of our attack method with a 12.85% higher success rate of transfer attack compared with the state-of-the-art methods. We also evaluate our method on the real-world online system, i.e., Google Cloud Vision API, to further show the practical potentials of our method.",
    "code_link": "https://github.com/JHL-HUST/SI-NI-FGSM"
  },
  "aaai2022_main_perceptualqualityassessmentofomnidirectionalimages": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Perceptual Quality Assessment of Omnidirectional Images",
    "authors": [
      "Yuming Fang",
      "Liping Huang",
      "Jiebin Yan",
      "Xuelin Liu",
      "Yang Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19937",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19937/19696",
    "published": "2022-02",
    "summary": "Omnidirectional images, also called 360\u25e6images, have attracted extensive attention in recent years, due to the rapid development of virtual reality (VR) technologies. During omnidirectional image processing including capture, transmission, consumption, and so on, measuring the perceptual quality of omnidirectional images is highly desired, since it plays a great role in guaranteeing the immersive quality of experience (IQoE). In this paper, we conduct a comprehensive study on the perceptual quality of omnidirectional images from both subjective and objective perspectives. Specifically, we construct the largest so far subjective omnidirectional image quality database, where we consider several key influential elements, i.e., realistic non-uniform distortion, viewing condition, and viewing behavior, from the user view. In addition to subjective quality scores, we also record head and eye movement data. Besides, we make the first attempt by using the proposed database to train a convolutional neural network (CNN) for blind omnidirectional image quality assessment. To be consistent with the human viewing behavior in the VR device, we extract viewports from each omnidirectional image and incorporate the user viewing conditions naturally in the proposed model. The proposed model is composed of two parts, including a multi-scale CNN-based feature extraction module and a perceptual quality prediction module. The feature extraction module is used to incorporate the multi-scale features, and the perceptual quality prediction module is designed to regress them to perceived quality scores. The experimental results on our database verify that the proposed model achieves the competing performance compared with the state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_patchupafeature-spaceblock-levelregularizationtechniqueforconvolutionalneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PatchUp: A Feature-Space Block-Level Regularization Technique for Convolutional Neural Networks",
    "authors": [
      "Mojtaba Faramarzi",
      "Mohammad Amini",
      "Akilesh Badrinaaraayanan",
      "Vikas Verma",
      "Sarath Chandar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19938",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19938/19697",
    "published": "2022-02",
    "summary": "Large capacity deep learning models are often prone to a high generalization gap when trained with a limited amount of labeled training data. A recent class of methods to address this problem uses various ways to construct a new training sample by mixing a pair (or more) of training samples. We propose PatchUp, a hidden state block-level regularization technique for Convolutional Neural Networks (CNNs), that is applied on selected contiguous blocks of feature maps from a random pair of samples. Our approach improves the robustness of CNN models against the manifold intrusion problem that may occur in other state-of-the-art mixing approaches. Moreover, since we are mixing the contiguous block of features in the hidden space, which has more dimensions than the input space, we obtain more diverse samples for training towards different dimensions. Our experiments on CIFAR10/100, SVHN, Tiny-ImageNet, and ImageNet using ResNet architectures including PreActResnet18/34, WRN-28-10, ResNet101/152 models show that PatchUp improves upon, or equals, the performance of current state-of-the-art regularizers for CNNs. We also show that PatchUp can provide a better generalization to deformed samples and is more robust against adversarial attacks.",
    "code_link": "https://github.com/chandar-lab/PatchUp"
  },
  "aaai2022_main_dumlp-pinadual-mlp-dot-productpermutation-invariantnetworkforsetfeatureextraction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DuMLP-Pin: A Dual-MLP-Dot-Product Permutation-Invariant Network for Set Feature Extraction",
    "authors": [
      "Jiajun Fei",
      "Ziyu Zhu",
      "Wenlei Liu",
      "Zhidong Deng",
      "Mingyang Li",
      "Huanjun Deng",
      "Shuo Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19939",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19939/19698",
    "published": "2022-02",
    "summary": "Existing permutation-invariant methods can be divided into two categories according to the aggregation scope, i.e. global aggregation and local one. Although the global aggregation methods, e. g., PointNet and Deep Sets, get involved in simpler structures, their performance is poorer than the local aggregation ones like PointNet++ and Point Transformer. It remains an open problem whether there exists a global aggregation method with a simple structure, competitive performance, and even much fewer parameters. In this paper, we propose a novel global aggregation permutation-invariant network based on dual MLP dot-product, called DuMLP-Pin, which is capable of being employed to extract features for set inputs, including unordered or unstructured pixel, attribute, and point cloud data sets. We strictly prove that any permutation-invariant function implemented by DuMLP-Pin can be decomposed into two or more permutation-equivariant ones in a dot-product way as the cardinality of the given input set is greater than a threshold. We also show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints under certain conditions. The performance of DuMLP-Pin is evaluated on several different tasks with diverse data sets. The experimental results demonstrate that our DuMLP-Pin achieves the best results on the two classification problems for pixel sets and attribute sets. On both the point cloud classification and the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far best-performing local aggregation method with only a 1-2% difference, while the number of required parameters is significantly reduced by more than 85% in classification and 69% in segmentation, respectively. The code is publicly available on https://github.com/JaronTHU/DuMLP-Pin.",
    "code_link": ""
  },
  "aaai2022_main_attention-alignedtransformerforimagecaptioning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Attention-Aligned Transformer for Image Captioning",
    "authors": [
      "Zhengcong Fei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19940",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19940/19699",
    "published": "2022-02",
    "summary": "Recently, attention-based image captioning models, which are expected to ground correct image regions for proper word generations, have achieved remarkable performance. However, some researchers have argued \u201cdeviated focus\u201d problem of existing attention mechanisms in determining the effective and influential image features. In this paper, we present A2 - an attention-aligned Transformer for image captioning, which guides attention learning in a perturbation-based self-supervised manner, without any annotation overhead. Specifically, we add mask operation on image regions through a learnable network to estimate the true function in ultimate description generation. We hypothesize that the necessary image region features, where small disturbance causes an obvious performance degradation, deserve more attention weight. Then, we propose four aligned strategies to use this information to refine attention weight distribution. Under such a pattern, image regions are attended correctly with the output words. Extensive experiments conducted on the MS COCO dataset demonstrate that the proposed A2 Transformer consistently outperforms baselines in both automatic metrics and human evaluation. Trained models and code for reproducing the experiments are publicly available.",
    "code_link": ""
  },
  "aaai2022_main_modeldoctorasimplegradientaggregationstrategyfordiagnosingandtreatingcnnclassifiers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Model Doctor: A Simple Gradient Aggregation Strategy for Diagnosing and Treating CNN Classifiers",
    "authors": [
      "Zunlei Feng",
      "Jiacong Hu",
      "Sai Wu",
      "XiaoTian Yu",
      "Jie Song",
      "Mingli Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19941",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19941/19700",
    "published": "2022-02",
    "summary": "Recently, Convolutional Neural Network (CNN) has achieved excellent performance in the classification task. It is widely known that CNN is deemed as a 'blackbox', which is hard for understanding the prediction mechanism and debugging the wrong prediction. Some model debugging and explanation works are developed for solving the above drawbacks. However, those methods focus on explanation and diagnosing possible causes for model prediction, based on which the researchers handle the following optimization of models manually. In this paper, we propose the first completely automatic model diagnosing and treating tool, termed as Model Doctor. Based on two discoveries that 1) each category is only correlated with sparse and specific convolution kernels, and 2) adversarial samples are isolated while normal samples are successive in the feature space, a simple aggregate gradient constraint is devised for effectively diagnosing and optimizing CNN classifiers. The aggregate gradient strategy is a versatile module for mainstream CNN classifiers. Extensive experiments demonstrate that the proposed Model Doctor applies to all existing CNN classifiers, and improves the accuracy of 16 mainstream CNN classifiers by 1%~5%.",
    "code_link": ""
  },
  "aaai2022_main_octattentionoctree-basedlarge-scalecontextsmodelforpointcloudcompression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "OctAttention: Octree-Based Large-Scale Contexts Model for Point Cloud Compression",
    "authors": [
      "Chunyang Fu",
      "Ge Li",
      "Rui Song",
      "Wei Gao",
      "Shan Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19942",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19942/19701",
    "published": "2022-02",
    "summary": "In point cloud compression, sufficient contexts are significant for modeling the point cloud distribution. However, the contexts gathered by the previous voxel-based methods decrease when handling sparse point clouds. To address this problem, we propose a multiple-contexts deep learning framework called OctAttention employing the octree structure, a memory-efficient representation for point clouds. Our approach encodes octree symbol sequences in a lossless way by gathering the information of sibling and ancestor nodes. Expressly, we first represent point clouds with octree to reduce spatial redundancy, which is robust for point clouds with different resolutions. We then design a conditional entropy model with a large receptive field that models the sibling and ancestor contexts to exploit the strong dependency among the neighboring nodes and employ an attention mechanism to emphasize the correlated nodes in the context. Furthermore, we introduce a mask operation during training and testing to make a trade-off between encoding time and performance. Compared to the previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate gain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset (e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based baseline. The code is available at https://github.com/zb12138/OctAttention.",
    "code_link": "https://github.com/zb12138/OctAttention"
  },
  "aaai2022_main_doc2pptautomaticpresentationslidesgenerationfromscientificdocuments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents",
    "authors": [
      "Tsu-Jui Fu",
      "William Yang Wang",
      "Daniel McDuff",
      "Yale Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19943",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19943/19702",
    "published": "2022-02",
    "summary": "Creating presentation materials requires complex multimodal reasoning skills to summarize key concepts and arrange them in a logical and visually pleasing manner. Can machines learn to emulate this laborious process? We present a novel task and approach for document-to-slide generation. Solving this involves document summarization, image and text retrieval, slide structure and layout prediction to arrange key elements in a form suitable for presentation. We propose a hierarchical sequence-to-sequence approach to tackle our task in an end-to-end manner. Our approach exploits the inherent structures within documents and slides and incorporates paraphrasing and layout prediction modules to generate slides. To help accelerate research in this domain, we release a dataset about 6K paired documents and slide decks used in our experiments. We show that our approach outperforms strong baselines and produces slides with rich content and aligned imagery.",
    "code_link": ""
  },
  "aaai2022_main_unsupervisedunderwaterimagerestorationfromahomologyperspective": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Underwater Image Restoration: From a Homology Perspective",
    "authors": [
      "Zhenqi Fu",
      "Huangxing Lin",
      "Yan Yang",
      "Shu Chai",
      "Liyan Sun",
      "Yue Huang",
      "Xinghao Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19944",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19944/19703",
    "published": "2022-02",
    "summary": "Underwater images suffer from degradation due to light scattering and absorption. It remains challenging to restore such degraded images using deep neural networks since real-world paired data is scarcely available while synthetic paired data cannot approximate real-world data perfectly. In this paper, we propose an UnSupervised Underwater Image Restoration method (USUIR) by leveraging the homology property between a raw underwater image and a re-degraded image. Specifically, USUIR first estimates three latent components of the raw underwater image, i.e., the global background light, the transmission map, and the scene radiance (the clean image). Then, a re-degraded image is generated by randomly mixing up the estimated scene radiance and the raw underwater image. We demonstrate that imposing a homology constraint between the raw underwater image and the re-degraded image is equivalent to minimizing the restoration error and hence can be used for the unsupervised restoration. Extensive experiments show that USUIR achieves promising performance in both inference time and restoration quality.",
    "code_link": "https://github.com/zhenqifu/USUIR"
  },
  "aaai2022_main_playinglotteryticketswithvisionandlanguage": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Playing Lottery Tickets with Vision and Language",
    "authors": [
      "Zhe Gan",
      "Yen-Chun Chen",
      "Linjie Li",
      "Tianlong Chen",
      "Yu Cheng",
      "Shuohang Wang",
      "Jingjing Liu",
      "Lijuan Wang",
      "Zicheng Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19945",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19945/19704",
    "published": "2022-02",
    "summary": "Large-scale pre-training has recently revolutionized vision-and-language (VL) research. Models such as LXMERT and UNITER have significantly lifted the state of the art over a wide range of VL tasks. However, the large number of parameters in such models hinders their application in practice. In parallel, work on the lottery ticket hypothesis (LTH) has shown that deep neural networks contain small matching subnetworks that can achieve on par or even better performance than the dense networks when trained in isolation. In this work, we perform the first empirical study to assess whether such trainable subnetworks also exist in pre-trained VL models. We use UNITER as the main testbed (also test on LXMERT and ViLT), and consolidate 7 representative VL tasks for experiments, including visual question answering, visual commonsense reasoning, visual entailment, referring expression comprehension, image-text retrieval, GQA, and NLVR2. Through comprehensive analysis, we summarize our main findings as follows. (i) It is difficult to find subnetworks that strictly match the performance of the full model. However, we can find relaxed winning tickets at 50%-70% sparsity that maintain 99% of the full accuracy. (ii) Subnetworks found by task-specific pruning transfer reasonably well to the other tasks, while those found on the pre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96% of the full accuracy on average over all the tasks. (iii) Besides UNITER, other models such as LXMERT and ViLT can also play lottery tickets. However, the highest sparsity we can achieve for ViLT is far lower than LXMERT and UNITER (30% vs. 70%). (iv) LTH also remains relevant when using other training methods (e.g., adversarial training).",
    "code_link": ""
  },
  "aaai2022_main_featuredistillationinteractionweightingnetworkforlightweightimagesuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Feature Distillation Interaction Weighting Network for Lightweight Image Super-resolution",
    "authors": [
      "Guangwei Gao",
      "Wenjie Li",
      "Juncheng Li",
      "Fei Wu",
      "Huimin Lu",
      "Yi Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19946",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19946/19705",
    "published": "2022-02",
    "summary": "Convolutional neural networks based single-image superresolution (SISR) has made great progress in recent years. However, it is difficult to apply these methods to real-world scenarios due to the computational and memory cost. Meanwhile, how to take full advantage of the intermediate features under the constraints of limited parameters and calculations is also a huge challenge. To alleviate these issues, we propose a lightweight yet efficient Feature Distillation Interaction Weighted Network (FDIWN). Specifically, FDIWN utilizes a series of specially designed Feature Shuffle Weighted Groups (FSWG) as the backbone, and several novel mutual Wide-residual Distillation Interaction Blocks (WDIB) form an FSWG. In addition, Wide Identical Residual Weighting (WIRW) units and Wide Convolutional Residual Weighting (WCRW) units are introduced into WDIB for better feature distillation. Moreover, a Wide-Residual Distillation Connection (WRDC) framework and a Self-Calibration Fusion (SCF) unit are proposed to interact features with different scales more flexibly and efficiently. Extensive experiments show that our FDIWN is superior to other models to strike a good balance between model performance and efficiency. The code is available at https://github.com/IVIPLab/FDIWN.",
    "code_link": "https://github.com/IVIPLab/FDIWN"
  },
  "aaai2022_main_weakly-supervisedsalientobjectdetectionusingpointsupervison": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Weakly-Supervised Salient Object Detection Using Point Supervison",
    "authors": [
      "Shuyong Gao",
      "Wei Zhang",
      "Yan Wang",
      "Qianyu Guo",
      "Chenglong Zhang",
      "Yangji He",
      "Wenqiang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19947",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19947/19706",
    "published": "2022-02",
    "summary": "Current state-of-the-art saliency detection models rely heavily on large datasets of accurate pixel-wise annotations, but manually labeling pixels is time-consuming and labor-intensive. There are some weakly supervised methods developed for alleviating the problem, such as image label, bounding box label, and scribble label, while point label still has not been explored in this field. In this paper, we propose a novel weakly-supervised salient object detection method using point supervision. To infer the saliency map, we first design an adaptive masked flood filling algorithm to generate pseudo labels. Then we develop a transformer-based point-supervised saliency detection model to produce the first round of saliency maps. However, due to the sparseness of the label, the weakly supervised model tends to degenerate into a general foreground detection model. To address this issue, we propose a Non-Salient Suppression (NSS) method to optimize the erroneous saliency maps generated in the first round and leverage them for the second round of training. Moreover, we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS dataset. In P-DUTS, there is only one labeled point for each salient object. Comprehensive experiments on five largest benchmark datasets demonstrate our method outperforms the previous state-of-the-art methods trained with the stronger supervision and even surpass several fully supervised state-of-the-art models. The code is available at: https://github.com/shuyonggao/PSOD.",
    "code_link": "https://github.com/shuyonggao/PSOD"
  },
  "aaai2022_main_latentspaceexplanationbyintervention": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Latent Space Explanation by Intervention",
    "authors": [
      "Itai Gat",
      "Guy Lorberbom",
      "Idan Schwartz",
      "Tamir Hazan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19948",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19948/19707",
    "published": "2022-02",
    "summary": "The success of deep neural nets heavily relies on their ability to encode complex relations between their input and their output. While this property serves to fit the training data well, it also obscures the mechanism that drives prediction. This study aims to reveal hidden concepts by employing an intervention mechanism that shifts the predicted class based on discrete variational autoencoders. An explanatory model then visualizes the encoded information from any hidden layer and its corresponding intervened representation. By the assessment of differences between the original representation and the intervened representation, one can determine the concepts that can alter the class, hence providing interpretability. We demonstrate the effectiveness of our approach on CelebA, where we show various visualizations for bias in the data and suggest different interventions to reveal and change bias.",
    "code_link": ""
  },
  "aaai2022_main_lifelongpersonre-identificationbypseudotaskknowledgepreservation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Lifelong Person Re-identification by Pseudo Task Knowledge Preservation",
    "authors": [
      "Wenhang Ge",
      "Junlong Du",
      "Ancong Wu",
      "Yuqiao Xian",
      "Ke Yan",
      "Feiyue Huang",
      "Wei-Shi Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19949",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19949/19708",
    "published": "2022-02",
    "summary": "In real world, training data for person re-identification (Re-ID) is collected discretely with spatial and temporal variations, which requires a model to incrementally learn new knowledge without forgetting old knowledge. This problem is called lifelong person re-identification (LReID). Variations of illumination and background for images of each task exhibit task-specific image style and lead to task-wise domain gap. In addition to missing data from the old tasks, task-wise domain gap is a key factor for catastrophic forgetting in LReID, which is ignored in existing approaches for LReID. The model tends to learn task-specific knowledge with task-wise domain gap, which results in stability and plasticity dilemma. To overcome this problem, we cast LReID as a domain adaptation problem and propose a pseudo task knowledge preservation framework to alleviate the domain gap. Our framework is based on a pseudo task transformation module which maps the features of the new task into the feature space of the old tasks to complement the limited saved exemplars of the old tasks. With extra transformed features in the task-specific feature space, we propose a task-specific domain consistency loss to implicitly alleviate the task-wise domain gap for learning task-shared knowledge instead of task-specific one. Furthermore, to guide knowledge preservation with the feature distributions of the old tasks, we propose to preserve knowledge on extra pseudo tasks which jointly distills knowledge and discriminates identity, in order to achieve a better trade-off between stability and plasticity for lifelong learning with task-wise domain gap. Extensive experiments demonstrate the superiority of our method as compared with the state-of-the-art lifelong learning and LReID methods.",
    "code_link": "https://github.com/g3956/PTKP"
  },
  "aaai2022_main_adversarialrobustnessinmulti-tasklearningpromisesandillusions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Robustness in Multi-Task Learning: Promises and Illusions",
    "authors": [
      "Salah Ghamizi",
      "Maxime Cordy",
      "Mike Papadakis",
      "Yves Le Traon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19950",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19950/19709",
    "published": "2022-02",
    "summary": "Vulnerability to adversarial attacks is a well-known weakness of Deep Neural networks. While most of the studies focus on single-task neural networks with computer vision datasets, very little research has considered complex multi-task models that are common in real applications. In this paper, we evaluate the design choices that impact the robustness of multi-task deep learning networks. We provide evidence that blindly adding auxiliary tasks, or weighing the tasks provides a false sense of robustness. Thereby, we tone down the claim made by previous research and study the different factors which may affect robustness. In particular, we show that the choice of the task to incorporate in the loss function are important factors that can be leveraged to yield more robust models. We provide the appendix, all our algorithms, models, and open source-code at https://github.com/yamizi/taskaugment",
    "code_link": "https://github.com/yamizi/taskaugment"
  },
  "aaai2022_main_deepconfidenceguideddistancefor3dpartialshaperegistration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Confidence Guided Distance for 3D Partial Shape Registration",
    "authors": [
      "Dvir Ginzburg",
      "Dan Raviv"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19951",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19951/19710",
    "published": "2022-02",
    "summary": "We present a novel non-iterative learnable method for partial-to-partial 3D shape registration. The partial alignment task is extremely complex, as it jointly tries to match between points, and identify which points do not appear in the corresponding shape, causing the solution to be non-unique and ill-posed in most cases. Until now, two main methodologies have been suggested to solve this problem: sample a subset of points that are likely to have correspondences, or perform soft alignment between the point clouds and try to avoid a match to an occluded part. These heuristics work when the partiality is mild or when the transformation is small but fails for severe occlusions, or when outliers are present. We present a unique approach named Confidence Guided Distance Network (CGD-net), where we fuse learnable similarity between point embeddings and spatial distance between point clouds, inducing an optimized solution for the overlapping points while ignoring parts that only appear in one of the shapes. The point feature generation is done by a self-supervised architecture that repels far points to have different embeddings, therefore succeeds to align partial views of shapes, even with excessive internal symmetries, or acute rotations. We compare our network to recently presented learning-based and axiomatic methods and report a fundamental boost in performance.",
    "code_link": ""
  },
  "aaai2022_main_predictingphysicalworlddestinationsforcommandsgiventoself-drivingcars": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Predicting Physical World Destinations for Commands Given to Self-Driving Cars",
    "authors": [
      "Dusan Grujicic",
      "Thierry Deruyttere",
      "Marie-Francine Moens",
      "Matthew B.\n      Blaschko"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19952",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19952/19711",
    "published": "2022-02",
    "summary": "In recent years, we have seen significant steps taken in the development of self-driving cars. Multiple companies are starting to roll out impressive systems that work in a variety of settings. These systems can sometimes give the impression that full self-driving is just around the corner and that we would soon build cars without even a steering wheel. The increase in the level of autonomy and control given to an AI provides an opportunity for new modes of human-vehicle interaction. However, surveys have shown that giving more control to an AI in self-driving cars is accompanied by a degree of uneasiness by passengers. In an attempt to alleviate this issue, recent works have taken a natural language-oriented approach by allowing the passenger to give commands that refer to specific objects in the visual scene. Nevertheless, this is only half the task as the car should also understand the physical destination of the command, which is what we focus on in this paper. We propose an extension in which we annotate the 3D destination that the car needs to reach after executing the given command and evaluate multiple different baselines on predicting this destination location. Additionally, we introduce a model that outperforms the prior works adapted for this particular setting.",
    "code_link": ""
  },
  "aaai2022_main_towardslight-weightandreal-timelinesegmentdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Light-Weight and Real-Time Line Segment Detection",
    "authors": [
      "Geonmo Gu",
      "Byungsoo Ko",
      "SeoungHyun Go",
      "Sung-Hyun Lee",
      "Jingeun Lee",
      "Minchul\n      Shin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19953",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19953/19712",
    "published": "2022-02",
    "summary": "Previous deep learning-based line segment detection (LSD) suffers from the immense model size and high computational cost for line prediction. This constrains them from real-time inference on computationally restricted environments. In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments named Mobile LSD (M-LSD). We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction found in previous methods. To maintain competitive performance with a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation, matching and geometric loss. SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process. Moreover, the matching and geometric loss allow a model to capture additional geometric cues. Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSD-tiny) achieves competitive performance with 2.5% of model size and an increase of 130.5% in inference speed on GPU. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively. To the best of our knowledge, this is the first real-time deep LSD available on mobile devices.",
    "code_link": "https://github.com/navervision/mlsd"
  },
  "aaai2022_main_exploitingfine-grainedfaceforgerycluesviaprogressiveenhancementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploiting Fine-Grained Face Forgery Clues via Progressive Enhancement Learning",
    "authors": [
      "Qiqi Gu",
      "Shen Chen",
      "Taiping Yao",
      "Yang Chen",
      "Shouhong Ding",
      "Ran Yi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19954",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19954/19713",
    "published": "2022-02",
    "summary": "With the rapid development of facial forgery techniques, forgery detection has attracted more and more attention due to security concerns. Existing approaches attempt to use frequency information to mine subtle artifacts under high-quality forged faces. However, the exploitation of frequency information is coarse-grained, and more importantly, their vanilla learning process struggles to extract fine-grained forgery traces. To address this issue, we propose a progressive enhancement learningframework to exploit both the RGB and fine-grained frequency clues. Specifically, we perform a fine-grained decomposition of RGB images to completely decouple the real and fake traces in the frequency space. Subsequently, we propose a progressive enhancement learning framework based on a two-branch network, combined with self-enhancement and mutual-enhancement modules. The self-enhancement module captures the traces in different input spaces based on spatial noise enhancement and channel attention. The Mutual-enhancement module concurrently enhances RGB and frequency features by communicating in the shared spatial dimension. The progressive enhancement process facilitates the learning of discriminative features with fine-grained face forgery clues. Extensive experiments on several datasets show that our method outperforms the state-of-the-art face forgery detection methods.",
    "code_link": ""
  },
  "aaai2022_main_delvingintothelocaldynamicinconsistencylearningfordeepfakevideodetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Delving into the Local: Dynamic Inconsistency Learning for DeepFake Video Detection",
    "authors": [
      "Zhihao Gu",
      "Yang Chen",
      "Taiping Yao",
      "Shouhong Ding",
      "Jilin Li",
      "Lizhuang Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19955",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19955/19714",
    "published": "2022-02",
    "summary": "The rapid development of facial manipulation techniques has aroused public concerns in recent years. Existing deepfake video detection approaches attempt to capture the discrim- inative features between real and fake faces based on tem- poral modelling. However, these works impose supervisions on sparsely sampled video frames but overlook the local mo- tions among adjacent frames, which instead encode rich in- consistency information that can serve as an efficient indica- tor for DeepFake video detection. To mitigate this issue, we delves into the local motion and propose a novel sampling unit named snippet which contains a few successive videos frames for local temporal inconsistency learning. Moreover, we elaborately design an Intra-Snippet Inconsistency Module (Intra-SIM) and an Inter-Snippet Interaction Module (Inter- SIM) to establish a dynamic inconsistency modelling frame- work. Specifically, the Intra-SIM applies bi-directional tem- poral difference operations and a learnable convolution ker- nel to mine the short-term motions within each snippet. The Inter-SIM is then devised to promote the cross-snippet infor- mation interaction to form global representations. The Intra- SIM and Inter-SIM work in an alternate manner and can be plugged into existing 2D CNNs. Our method outperforms the state of the art competitors on four popular benchmark dataset, i.e., FaceForensics++, Celeb-DF, DFDC and Wild- Deepfake. Besides, extensive experiments and visualizations are also presented to further illustrate its effectiveness.",
    "code_link": ""
  },
  "aaai2022_main_assessingasingleimageinreference-guidedimagesynthesis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Assessing a Single Image in Reference-Guided Image Synthesis",
    "authors": [
      "Jiayi Guo",
      "Chaoqun Du",
      "Jiangshan Wang",
      "Huijuan Huang",
      "Pengfei Wan",
      "Gao\n      Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19956",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19956/19715",
    "published": "2022-02",
    "summary": "Assessing the performance of Generative Adversarial Networks (GANs) has been an important topic due to its practical significance. Although several evaluation metrics have been proposed, they generally assess the quality of the whole generated image distribution. For Reference-guided Image Synthesis (RIS) tasks, i.e., rendering a source image in the style of another reference image, where assessing the quality of a single generated image is crucial, these metrics are not applicable. In this paper, we propose a general learning-based framework, Reference-guided Image Synthesis Assessment (RISA) to quantitatively evaluate the quality of a single generated image. Notably, the training of RISA does not require human annotations. In specific, the training data for RISA are acquired by the intermediate models from the training procedure in RIS, and weakly annotated by the number of models' iterations, based on the positive correlation between image quality and iterations. As this annotation is too coarse as a supervision signal, we introduce two techniques: 1) a pixel-wise interpolation scheme to refine the coarse labels, and 2) multiple binary classifiers to replace a na\u00efve regressor. In addition, an unsupervised contrastive loss is introduced to effectively capture the style similarity between a generated image and its reference image. Empirical results on various datasets demonstrate that RISA is highly consistent with human preference and transfers well across models.",
    "code_link": ""
  },
  "aaai2022_main_contrastivelearningfromextremelyaugmentedskeletonsequencesforself-supervisedactionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contrastive Learning from Extremely Augmented Skeleton Sequences for Self-Supervised Action Recognition",
    "authors": [
      "Tianyu Guo",
      "Hong Liu",
      "Zhan Chen",
      "Mengyuan Liu",
      "Tao Wang",
      "Runwei Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19957",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19957/19716",
    "published": "2022-02",
    "summary": "In recent years, self-supervised representation learning for skeleton-based action recognition has been developed with the advance of contrastive learning methods. The existing contrastive learning methods use normal augmentations to construct similar positive samples, which limits the ability to explore novel movement patterns. In this paper, to make better use of the movement patterns introduced by extreme augmentations, a Contrastive Learning framework utilizing Abundant Information Mining for self-supervised action Representation (AimCLR) is proposed. First, the extreme augmentations and the Energy-based Attention-guided Drop Module (EADM) are proposed to obtain diverse positive samples, which bring novel movement patterns to improve the universality of the learned representations. Second, since directly using extreme augmentations may not be able to boost the performance due to the drastic changes in original identity, the Dual Distributional Divergence Minimization Loss (D3M Loss) is proposed to minimize the distribution divergence in a more gentle way. Third, the Nearest Neighbors Mining (NNM) is proposed to further expand positive samples to make the abundant information mining process more reasonable. Exhaustive experiments on NTU RGB+D 60, PKU-MMD, NTU RGB+D 120 datasets have verified that our AimCLR can significantly perform favorably against state-of-the-art methods under a variety of evaluation protocols with observed higher quality action representations. Our code is available at https://github.com/Levigty/AimCLR.",
    "code_link": "https://github.com/Levigty/AimCLR"
  },
  "aaai2022_main_convolutionalneuralnetworkcompressionthroughgeneralizedkroneckerproductdecomposition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition",
    "authors": [
      "Marawan Gamal Abdel Hameed",
      "Marzieh S. Tahaei",
      "Ali Mosleh",
      "Vahid Partovi\n      Nia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19958",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19958/19717",
    "published": "2022-02",
    "summary": "Modern Convolutional Neural Network (CNN) architectures, despite their superiority in solving various problems, are generally too large to be deployed on resource constrained edge devices. In this paper, we reduce memory usage and floating-point operations required by convolutional layers in CNNs. We compress these layers by generalizing the Kronecker Product Decomposition to apply to multidimensional tensors, leading to the Generalized Kronecker Product Decomposition (GKPD). Our approach yields a plug-and-play module that can be used as a drop-in replacement for any convolutional layer. Experimental results for image classification on CIFAR-10 and ImageNet datasets using ResNet, MobileNetv2 and SeNet architectures substantiate the effectiveness of our proposed approach. We find that GKPD outperforms state-of-the-art decomposition methods including Tensor-Train and Tensor-Ring as well as other relevant compression methods such as pruning and knowledge distillation.",
    "code_link": ""
  },
  "aaai2022_main_metafasterr-cnntowardsaccuratefew-shotobjectdetectionwithattentivefeaturealignment": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment",
    "authors": [
      "Guangxing Han",
      "Shiyuan Huang",
      "Jiawei Ma",
      "Yicheng He",
      "Shih-Fu Chang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19959",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19959/19718",
    "published": "2022-02",
    "summary": "Few-shot object detection (FSOD) aims to detect objects using only a few examples. How to adapt state-of-the-art object detectors to the few-shot domain remains challenging. Object proposal is a key ingredient in modern object detectors. However, the quality of proposals generated for few-shot classes using existing methods is far worse than that of many-shot classes, e.g., missing boxes for few-shot classes due to misclassification or inaccurate spatial locations with respect to true objects. To address the noisy proposal problem, we propose a novel meta-learning based FSOD model by jointly optimizing the few-shot proposal generation and fine-grained few-shot proposal classification. To improve proposal generation for few-shot classes, we propose to learn a lightweight metric-learning based prototype matching network, instead of the conventional simple linear object/nonobject classifier, e.g., used in RPN. Our non-linear classifier with the feature fusion network could improve the discriminative prototype matching and the proposal recall for few-shot classes. To improve the fine-grained few-shot proposal classification, we propose a novel attentive feature alignment method to address the spatial misalignment between the noisy proposals and few-shot classes, thus improving the performance of few-shot object detection. Meanwhile we learn a separate Faster R-CNN detection head for many-shot base classes and show strong performance of maintaining base-classes knowledge. Our model achieves state-of-the-art performance on multiple FSOD benchmarks over most of the shots and metrics.",
    "code_link": "https://github.com/YoungXIAO13/FewShotDetection"
  },
  "aaai2022_main_delvingintoprobabilisticuncertaintyforunsuperviseddomainadaptivepersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-identification",
    "authors": [
      "Jian Han",
      "Ya-Li Li",
      "Shengjin Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19960",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19960/19719",
    "published": "2022-02",
    "summary": "Clustering-based unsupervised domain adaptive (UDA) person re-identification (ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature embedding and imperfect clustering, pseudo labels for target domain data inherently contain an unknown proportion of wrong ones, which would mislead feature learning. In this paper, we propose an approach named probabilistic uncertainty guided progressive label refinery (P2LR) for domain adaptive person re-identification. First, we propose to model the labeling uncertainty with the probabilistic distance along with ideal single-peak distributions. A quantitative criterion is established to measure the uncertainty of pseudo labels and facilitate the network training. Second, we explore a progressive strategy for refining pseudo labels. With the uncertainty-guided alternative optimization, we balance between the exploration of target domain data and the negative effects of noisy labeling. On top of a strong baseline, we obtain significant improvements and achieve the state-of-the-art performance on four UDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5% mAP on the Duke2Market task, while surpassing the state-of-the-art method by 2.5% mAP on the Market2MSMT task. Code is available at: https://github.com/JeyesHan/P2LR.",
    "code_link": "https://github.com/JeyesHan/P2LR"
  },
  "aaai2022_main_laneformerobject-awarerow-columntransformersforlanedetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Laneformer: Object-Aware Row-Column Transformers for Lane Detection",
    "authors": [
      "Jianhua Han",
      "Xiajun Deng",
      "Xinyue Cai",
      "Zhen Yang",
      "Hang Xu",
      "Chunjing Xu",
      "Xiaodan Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19961",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19961/19720",
    "published": "2022-02",
    "summary": "We present Laneformer, a conceptually simple yet powerful transformer-based architecture tailored for lane detection that is a long-standing research topic for visual perception in autonomous driving. The dominant paradigms rely on purely CNN-based architectures which often fail in incorporating relations of long-range lane points and global contexts induced by surrounding objects (e.g., pedestrians, vehicles). Inspired by recent advances of the transformer encoder-decoder architecture in various vision tasks, we move forwards to design a new end-to-end Laneformer architecture that revolutionizes the conventional transformers into better capturing the shape and semantic characteristics of lanes, with minimal overhead in latency. First, coupling with deformable pixel-wise self-attention in the encoder, Laneformer presents two new row and column self-attention operations to efficiently mine point context along with the lane shapes. Second, motivated by the appearing objects would affect the decision of predicting lane segments, Laneformer further includes the detected object instances as extra inputs of multi-head attention blocks in the encoder and decoder to facilitate the lane point detection by sensing semantic contexts. Specifically, the bounding box locations of objects are added into Key module to provide interaction with each pixel and query while the ROI-aligned features are inserted into Value module. Extensive experiments demonstrate our Laneformer achieves state-of-the-art performances on CULane benchmark, in terms of 77.1% F1 score. We hope our simple and effective Laneformer will serve as a strong baseline for future research in self-attention models for lane detection.",
    "code_link": ""
  },
  "aaai2022_main_modifyself-attentionviaskeletondecompositionforeffectivepointcloudtransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Modify Self-Attention via Skeleton Decomposition for Effective Point Cloud Transformer",
    "authors": [
      "Jiayi Han",
      "Longbin Zeng",
      "Liang Du",
      "Xiaoqing Ye",
      "Weiyang Ding",
      "Jianfeng\n      Feng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19962",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19962/19721",
    "published": "2022-02",
    "summary": "Although considerable progress has been achieved regarding the transformers in recent years, the large number of parameters, quadratic computational complexity, and memory cost conditioned on long sequences make the transformers hard to train and implement, especially in edge computing configurations. In this case, a dizzying number of works have sought to make improvements around computational and memory efficiency upon the original transformer architecture. Nevertheless, many of them restrict the context in the attention to seek a trade-off between cost and performance with prior knowledge of orderly stored data. It is imperative to dig deep into an efficient feature extractor for point clouds due to their irregularity and a large number of points. In this paper, we propose a novel skeleton decomposition-based self-attention (SD-SA) which has no sequence length limit and exhibits favorable scalability in long-sequence models. Due to the numerical low-rank nature of self-attention, we approximate it by the skeleton decomposition method while maintaining its effectiveness. At this point, we have shown that the proposed method works for the proposed approach on point cloud classification, segmentation, and detection tasks on the ModelNet40, ShapeNet, and KITTI datasets, respectively. Our approach significantly improves the efficiency of the point cloud transformer and exceeds other efficient transformers on point cloud tasks in terms of the speed at comparable performance.",
    "code_link": "https://github.com/yanx27/Pointnet"
  },
  "aaai2022_main_generalizablepersonre-identificationviaself-supervisedbatchnormtest-timeadaption": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generalizable Person Re-identification via Self-Supervised Batch Norm Test-Time Adaption",
    "authors": [
      "Ke Han",
      "Chenyang Si",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19963",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19963/19722",
    "published": "2022-02",
    "summary": "In this paper, we investigate the generalization problem of person re-identification (re-id), whose major challenge is the distribution shift on an unseen domain. As an important tool of regularizing the distribution, batch normalization (BN) has been widely used in existing methods. However, they neglect that BN is severely biased to the training domain and inevitably suffers the performance drop if directly generalized without being updated. To tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel re-id framework that applies the self-supervised strategy to update BN parameters adaptively. Specifically, BNTA quickly explores the domain-aware information within unlabeled target data before inference, and accordingly modulates the feature distribution normalized by BN to adapt to the target domain. This is accomplished by two designed self-supervised auxiliary tasks, namely part positioning and part nearest neighbor matching, which help the model mine the domain-aware information with respect to the structure and identity of body parts, respectively. To demonstrate the effectiveness of our method, we conduct extensive experiments on three re-id datasets and confirm the superior performance to the state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_rrlregionalrotatelayerinconvolutionalneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "RRL: Regional Rotate Layer in Convolutional Neural Networks",
    "authors": [
      "Zongbo Hao",
      "Tao Zhang",
      "Mingwang Chen",
      "Zou Kaixu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19964",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19964/19723",
    "published": "2022-02",
    "summary": "Convolutional Neural Networks (CNNs) perform very well in image classification and object detection in recent years, but even the most advanced models have limited rotation invariance. Known solutions include the enhancement of training data and the increase of rotation invariance by globally merging the rotation equivariant features. These methods either increase the workload of training or increase the number of model parameters. To address this problem, this paper proposes a module that can be inserted into the existing networks, and directly incorporates the rotation invariance into the feature extraction layers of the CNNs. This module does not have learnable parameters and will not increase the complexity of the model. At the same time, only by training the upright data, it can perform well on the rotated testing set. These ad-vantages will be suitable for fields such as biomedicine and astronomy where it is difficult to obtain upright samples or the target has no directionality. Evaluate our module with LeNet-5, ResNet-18 and tiny-yolov3, we get impressive results.",
    "code_link": ""
  },
  "aaai2022_main_querypropobjectquerypropagationforhigh-performancevideoobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "QueryProp: Object Query Propagation for High-Performance Video Object Detection",
    "authors": [
      "Fei He",
      "Naiyu Gao",
      "Jian Jia",
      "Xin Zhao",
      "Kaiqi Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19965",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19965/19724",
    "published": "2022-02",
    "summary": "Video object detection has been an important yet challenging topic in computer vision. Traditional methods mainly focus on designing the image-level or box-level feature propagation strategies to exploit temporal information. This paper argues that with a more effective and efficient feature propagation framework, video object detectors can gain improvement in terms of both accuracy and speed. For this purpose, this paper studies object-level feature propagation, and proposes an object query propagation (QueryProp) framework for high-performance video object detection. The proposed QueryProp contains two propagation strategies: 1) query propagation is performed from sparse key frames to dense non-key frames to reduce the redundant computation on non-key frames; 2) query propagation is performed from previous key frames to the current key frame to improve feature representation by temporal context modeling. To further facilitate query propagation, an adaptive propagation gate is designed to achieve flexible key frame selection. We conduct extensive experiments on the ImageNet VID dataset. QueryProp achieves comparable accuracy with state-of-the-art methods and strikes a decent accuracy/speed trade-off.",
    "code_link": ""
  },
  "aaai2022_main_flow-basedunconstrainedliptospeechgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Flow-Based Unconstrained Lip to Speech Generation",
    "authors": [
      "Jinzheng He",
      "Zhou Zhao",
      "Yi Ren",
      "Jinglin Liu",
      "Baoxing Huai",
      "Nicholas Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19966",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19966/19725",
    "published": "2022-02",
    "summary": "Unconstrained lip-to-speech aims to generate corresponding speeches based on silent facial videos with no restriction to head pose or vocabulary. It is desirable to generate intelligible and natural speech with a fast speed in unconstrained settings. Currently, to handle the more complicated scenarios, most existing methods adopt the autoregressive architecture, which is optimized with the MSE loss. Although these methods have achieved promising performance, they are prone to bring issues including high inference latency and mel-spectrogram over-smoothness.To tackle these problems, we propose a novelflow-based non-autoregressive lip-to-speech model (GlowLTS) to break autoregressive constraints and achieve faster inference. Concretely, we adopt a flow-based decoder which is optimized by maximizing the likelihood of the training data and is capable of more natural and fast speech generation. Moreover, we devise a condition module to improve the intelligibility of generated speech.We demonstrate the superiority of our proposed method through objective and subjective evaluation on Lip2Wav-Chemistry-Lectures and Lip2Wav-Chess-Analysis datasets. Our demo video can be found at https://glowlts.github.io/.",
    "code_link": ""
  },
  "aaai2022_main_transfgatransformerarchitectureforfine-grainedrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TransFG: A Transformer Architecture for Fine-Grained Recognition",
    "authors": [
      "Ju He",
      "Jie-Neng Chen",
      "Shuai Liu",
      "Adam Kortylewski",
      "Cheng Yang",
      "Yutong Bai",
      "Changhu Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19967",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19967/19726",
    "published": "2022-02",
    "summary": "Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedrobustsceneflowestimationviathealignmentofprobabilitydensityfunctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Robust Scene Flow Estimation via the Alignment of Probability Density Functions",
    "authors": [
      "Pan He",
      "Patrick Emami",
      "Sanjay Ranka",
      "Anand Rangarajan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19968",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19968/19727",
    "published": "2022-02",
    "summary": "In this paper, we present a new self-supervised scene flow estimation approach for a pair of consecutive point clouds. The key idea of our approach is to represent discrete point clouds as continuous probability density functions using Gaussian mixture models. Scene flow estimation is therefore converted into the problem of recovering motion from the alignment of probability density functions, which we achieve using a closed-form expression of the classic Cauchy-Schwarz divergence. Unlike existing nearest-neighbor-based approaches that use hard pairwise correspondences, our proposed approach establishes soft and implicit point correspondences between point clouds and generates more robust and accurate scene flow in the presence of missing correspondences and outliers. Comprehensive experiments show that our method makes noticeable gains over the Chamfer Distance and the Earth Mover\u2019s Distance in real-world environments and achieves state-of-the-art performance among self-supervised learning methods on FlyingThings3D and KITTI, even outperforming some supervised methods with ground truth annotations.",
    "code_link": ""
  },
  "aaai2022_main_svga-netsparsevoxel-graphattentionnetworkfor3dobjectdetectionfrompointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds",
    "authors": [
      "Qingdong He",
      "Zhengning Wang",
      "Hao Zeng",
      "Yi Zeng",
      "Yijun Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19969",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19969/19728",
    "published": "2022-02",
    "summary": "Accurate 3D object detection from point clouds has become a crucial component in autonomous driving. However, the volumetric representations and the projection methods in previous works fail to establish the relationships between the local point sets. In this paper, we propose Sparse Voxel-Graph Attention Network (SVGA-Net), a novel end-to-end trainable network which mainly contains voxel-graph module and sparse-to-dense regression module to achieve comparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net constructs the local complete graph within each divided 3D spherical voxel and global KNN graph through all voxels. The local and global graphs serve as the attention mechanism to enhance the extracted features. In addition, the novel sparse-to-dense regression module enhances the 3D box estimation accuracy through feature maps aggregation at different levels. Experiments on KITTI detection benchmark and Waymo Open dataset demonstrate the efficiency of extending the graph representation to 3D object detection and the proposed SVGA-Net can achieve decent detection accuracy.",
    "code_link": ""
  },
  "aaai2022_main_secretself-consistentpseudolabelrefinementforunsuperviseddomainadaptivepersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SECRET: Self-Consistent Pseudo Label Refinement for Unsupervised Domain Adaptive Person Re-identification",
    "authors": [
      "Tao He",
      "Leqi Shen",
      "Yuchen Guo",
      "Guiguang Ding",
      "Zhenhua Guo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19970",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19970/19729",
    "published": "2022-02",
    "summary": "Unsupervised domain adaptive person re-identification aims at learning on an unlabeled target domain with only labeled data in source domain. Currently, the state-of-the-arts usually solve this problem by pseudo-label-based clustering and fine-tuning in target domain. However, the reason behind the noises of pseudo labels is not sufficiently explored, especially for the popular multi-branch models. We argue that the consistency between different feature spaces is the key to the pseudo labels\u2019 quality. Then a SElf-Consistent pseudo label RefinEmenT method, termed as SECRET, is proposed to improve consistency by mutually refining the pseudo labels generated from different feature spaces. The proposed SECRET gradually encourages the improvement of pseudo labels\u2019 quality during training process, which further leads to better cross-domain Re-ID performance. Extensive experiments on benchmark datasets show the superiority of our method. Specifically, our method outperforms the state-of-the-arts by 6.3% in terms of mAP on the challenging dataset MSMT17. In the purely unsupervised setting, our method also surpasses existing works by a large margin. Code is available at https://github.com/LunarShen/SECRET.",
    "code_link": "https://github.com/LunarShen/SECRET"
  },
  "aaai2022_main_visualsemanticsallowfortextualreasoningbetterinscenetextrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition",
    "authors": [
      "Yue He",
      "Chen Chen",
      "Jing Zhang",
      "Juhua Liu",
      "Fengxiang He",
      "Chaoyue Wang",
      "Bo\n      Du"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19971",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19971/19730",
    "published": "2022-02",
    "summary": "Existing Scene Text Recognition (STR) methods typically use a language model to optimize the joint probability of the 1D character sequence predicted by a visual recognition (VR) model, which ignore the 2D spatial context of visual semantics within and between character instances, making them not generalize well to arbitrary shape scene text. To address this issue, we make the first attempt to perform textual reasoning based on visual semantics in this paper. Technically, given the character segmentation maps predicted by a VR model, we construct a subgraph for each instance, where nodes represent the pixels in it and edges are added between nodes based on their spatial similarity. Then, these subgraphs are sequentially connected by their root nodes and merged into a complete graph. Based on this graph, we devise a graph convolutional network for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR can be easily plugged in representative STR models to improve their performance owing to better textual reasoning. Specifically, we construct our model, namely S-GTR, by paralleling GTR to the language model in a segmentation-based STR baseline, which can effectively exploit the visual-linguistic complementarity via mutual learning. S-GTR sets new state-of-the-art on six challenging STR benchmarks and generalizes well to multi-linguistic datasets. Code is available at https://github.com/adeline-cs/GTR.",
    "code_link": "https://github.com/adeline-cs/GTR"
  },
  "aaai2022_main_rankinginfonoisecontrastiveestimationboostingcontrastivelearningviarankedpositives": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives",
    "authors": [
      "David T. Hoffmann",
      "Nadine Behrmann",
      "Juergen Gall",
      "Thomas Brox",
      "Mehdi\n      Noroozi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19972",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19972/19731",
    "published": "2022-02",
    "summary": "This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a new member in the family of InfoNCE losses that preserves a ranked ordering of positive samples. In contrast to the standard InfoNCE loss, which requires a strict binary separation of the training pairs into similar and dissimilar samples, RINCE can exploit information about a similarity ranking for learning a corresponding embedding space. We show that the proposed loss function learns favorable embeddings compared to the standard InfoNCE whenever at least noisy ranking information can be obtained or when the definition of positives and negatives is blurry. We demonstrate this for a supervised classification task with additional superclass labels and noisy similarity scores. Furthermore, we show that RINCE can also be applied to unsupervised training with experiments on unsupervised representation learning from videos. In particular, the embedding yields higher classification accuracy, retrieval rates and performs better on out-of-distribution detection than the standard InfoNCE loss.",
    "code_link": "https://github.com/boschresearch/rince"
  },
  "aaai2022_main_uncertainty-drivendehazingnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Uncertainty-Driven Dehazing Network",
    "authors": [
      "Ming Hong",
      "Jianzhuang Liu",
      "Cuihua Li",
      "Yanyun Qu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19973",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19973/19732",
    "published": "2022-02",
    "summary": "Deep learning has made remarkable achievements for single image haze removal. However, existing deep dehazing models only give deterministic results without discussing the uncertainty of them. There exist two types of uncertainty in the dehazing models: aleatoric uncertainty that comes from noise inherent in the observations and epistemic uncertainty that accounts for uncertainty in the model.In this paper, we propose a novel uncertainty-driven dehazing network (UDN) that improves the dehazing results by exploiting the relationship between the uncertain and confident representations. We first introduce an Uncertainty Estimation Block (UEB) to predict the aleatoric and epistemic uncertainty together. Then, we propose an Uncertainty-aware Feature Modulation (UFM) block to adaptively enhance the learned features. UFM predicts a convolution kernel and channel-wise modulation cofficients conitioned on the uncertainty weighted representation.Moreover, we develop an uncertainty-driven self-distillation loss to improve the uncertain representation by transferring the knowledge from the confident one.Extensive experimental results on synthetic datasets and real-world images show that UDN achieves significant quantitative and qualitative improvements, outperforming the state-of-the-arts.",
    "code_link": ""
  },
  "aaai2022_main_shadowgenerationforcompositeimageinreal-worldscenes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shadow Generation for Composite Image in Real-World Scenes",
    "authors": [
      "Yan Hong",
      "Li Niu",
      "Jianfu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19974",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19974/19733",
    "published": "2022-02",
    "summary": "Image composition targets at inserting a foreground object into a background image. Most previous image composition methods focus on adjusting the foreground to make it compatible with background while ignoring the shadow effect of foreground on the background. In this work, we focus on generating plausible shadow for the foreground object in the composite image. First, we contribute a real-world shadow generation dataset DESOBA by generating synthetic composite images based on paired real images and deshadowed images.Then, we propose a novel shadow generation network SGRNet, which consists of a shadow mask prediction stage and a shadow filling stage. In the shadow mask prediction stage, foreground and background information are thoroughly interacted to generate foreground shadow mask. In the shadow filling stage, shadow parameters are predicted to fill the shadow area. Extensive experiments on our DESOBA dataset and real composite images demonstrate the effectiveness of our proposed method. Our dataset and code are available at https://github.com/bcmi/Object-Shadow-Generation- Dataset-DESOBA.",
    "code_link": "https://github.com/bcmi/Object-Shadow-GenerationDataset-DESOBA"
  },
  "aaai2022_main_shape-adaptiveselectionandmeasurementfororientedobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shape-Adaptive Selection and Measurement for Oriented Object Detection",
    "authors": [
      "Liping Hou",
      "Ke Lu",
      "Jian Xue",
      "Yuqiu Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19975",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19975/19734",
    "published": "2022-02",
    "summary": "The development of detection methods for oriented object detection remains a challenging task. A considerable obstacle is the wide variation in the shape (e.g., aspect ratio) of objects. Sample selection in general object detection has been widely studied as it plays a crucial role in the performance of the detection method and has achieved great progress. However, existing sample selection strategies still overlook some issues: (1) most of them ignore the object shape information; (2) they do not make a potential distinction between selected positive samples; and (3) some of them can only be applied to either anchor-free or anchor-based methods and cannot be used for both of them simultaneously. In this paper, we propose novel flexible shape-adaptive selection (SA-S) and shape-adaptive measurement (SA-M) strategies for oriented object detection, which comprise an SA-S strategy for sample selection and SA-M strategy for the quality estimation of positive samples. Specifically, the SA-S strategy dynamically selects samples according to the shape information and characteristics distribution of objects. The SA-M strategy measures the localization potential and adds quality information on the selected positive samples. The experimental results on both anchor-free and anchor-based baselines and four publicly available oriented datasets (DOTA, HRSC2016, UCAS-AOD, and ICDAR2015) demonstrate the effectiveness of the proposed method.",
    "code_link": "https://github.com/houliping/SASM"
  },
  "aaai2022_main_h2-milexploringhierarchicalrepresentationwithheterogeneousmultipleinstancelearningforwholeslideimageanalysis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "H^2-MIL: Exploring Hierarchical Representation with Heterogeneous Multiple Instance Learning for Whole Slide Image Analysis",
    "authors": [
      "Wentai Hou",
      "Lequan Yu",
      "Chengxuan Lin",
      "Helong Huang",
      "Rongshan Yu",
      "Jing Qin",
      "Liansheng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19976",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19976/19735",
    "published": "2022-02",
    "summary": "Current representation learning methods for whole slide image (WSI) with pyramidal resolutions are inherently homogeneous and flat, which cannot fully exploit the multiscale and heterogeneous diagnostic information of different structures for comprehensive analysis. This paper presents a novel graph neural network-based multiple instance learning framework (i.e., H^2-MIL) to learn hierarchical representation from a heterogeneous graph with different resolutions for WSI analysis. A heterogeneous graph with the \u201cresolution\u201d attribute is constructed to explicitly model the feature and spatial-scaling relationship of multi-resolution patches. We then design a novel resolution-aware attention convolution (RAConv) block to learn compact yet discriminative representation from the graph, which tackles the heterogeneity of node neighbors with different resolutions and yields more reliable message passing. More importantly, to explore the task-related structured information of WSI pyramid, we elaborately design a novel iterative hierarchical pooling (IHPool) module to progressively aggregate the heterogeneous graph based on scaling relationships of different nodes. We evaluated our method on two public WSI datasets from the TCGA project, i.e., esophageal cancer and kidney cancer. Experimental results show that our method clearly outperforms the state-of-the-art methods on both tumor typing and staging tasks.",
    "code_link": "https://github.com/lin-lcx/H2-MIL"
  },
  "aaai2022_main_elastic-linkforbinarizedneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Elastic-Link for Binarized Neural Networks",
    "authors": [
      "Jie Hu",
      "Ziheng Wu",
      "Vince Tan",
      "Zhilin Lu",
      "Mengze Zeng",
      "Enhua Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19977",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19977/19736",
    "published": "2022-02",
    "summary": "Recent work has shown that Binarized Neural Networks (BNNs) are able to greatly reduce computational costs and memory footprints, facilitating model deployment on resource-constrained devices. However, in comparison to their full-precision counterparts, BNNs suffer from severe accuracy degradation. Research aiming to reduce this accuracy gap has thus far largely focused on specific network architectures with few or no 1 \u00d7 1 convolutional layers, for which standard binarization methods do not work well. Because 1 \u00d7 1 convolutions are common in the design of modern architectures (e.g. GoogleNet, ResNet, DenseNet), it is crucial to develop a method to binarize them effectively for BNNs to be more widely adopted. In this work, we propose an \u201cElastic-Link\u201d (EL) module to enrich information flow within a BNN by adaptively adding real-valued input features to the subsequent convolutional output features. The proposed EL module is easily implemented and can be used in conjunction with other methods for BNNs. We demonstrate that adding EL to BNNs produces a significant improvement on the challenging large-scale ImageNet dataset. For example, we raise the top-1 accuracy of binarized ResNet26 from 57.9% to 64.0%. EL also aids con-vergence in the training of binarized MobileNet, for which a top-1 accuracy of 56.4% is achieved. Finally, with the integration of ReActNet, it yields a new state-of-the-art result of 71.9% top-1 accuracy.",
    "code_link": ""
  },
  "aaai2022_main_finferframeinference-baseddeepfakedetectionforhigh-visual-qualityvideos": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FInfer: Frame Inference-Based Deepfake Detection for High-Visual-Quality Videos",
    "authors": [
      "Juan Hu",
      "Xin Liao",
      "Jinwen Liang",
      "Wenbo Zhou",
      "Zheng Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19978",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19978/19737",
    "published": "2022-02",
    "summary": "Deepfake has ignited hot research interests in both academia and industry due to its potential security threats. Many countermeasures have been proposed to mitigate such risks. Current Deepfake detection methods achieve superior performances in dealing with low-visual-quality Deepfake media which can be distinguished by the obvious visual artifacts. However, with the development of deep generative models, the realism of Deepfake media has been significantly improved and becomes tough challenging to current detection models. In this paper, we propose a frame inference-based detection framework (FInfer) to solve the problem of high-visual-quality Deepfake detection. Specifically, we first learn the referenced representations of the current and future frames\u2019 faces. Then, the current frames\u2019 facial representations are utilized to predict the future frames\u2019 facial representations by using an autoregressive model. Finally, a representation-prediction loss is devised to maximize the discriminability of real videos and fake videos. We demonstrate the effectiveness of our FInfer framework through information theory analyses. The entropy and mutual information analyses indicate the correlation between the predicted representations and referenced representations in real videos is higher than that of high-visual-quality Deepfake videos. Extensive experiments demonstrate the performance of our method is promising in terms of in-dataset detection performance, detection efficiency, and cross-dataset detection performance in high-visual-quality Deepfake videos.",
    "code_link": "https://github.com/deepfakes/faceswap"
  },
  "aaai2022_main_bi-volutionastaticanddynamiccoupledfilter": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bi-volution: A Static and Dynamic Coupled Filter",
    "authors": [
      "Xiwei Hu",
      "Xuanhong Chen",
      "Bingbing Ni",
      "Teng Li",
      "Yutian Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19979",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19979/19738",
    "published": "2022-02",
    "summary": "Dynamic convolution has achieved significant gain in performance and computational complexity, thanks to its powerful representation capability given limited filter number/layers.However, SOTA dynamic convolution operators are sensitive to input noises (e.g., Gaussian noise, shot noise, e.t.c.) and lack sufficient spatial contextual information in filter generation.To alleviate this inherent weakness, we propose a lightweight and heterogeneous-structure (i.e., static and dynamic) operator, named Bi-volution.On the one hand, Bi-volution is designed as a dual-branch structure to fully leverage complementary properties of static/dynamic convolution, which endows Bi-volution more robust properties and higher performance.On the other hand, the Spatial Augmented Kernel Generation module is proposed to improve the dynamic convolution, realizing the learning of spatial context information with negligible additional computational complexity.Extensive experiments illustrate that the ResNet-50 equipped with Bi-volution achieves a highly competitive boost in performance (+2.8% top-1 accuracy on ImageNet classification, +2.4% box AP and +2.2% mask AP on COCO detection and instance segmentation) while maintaining extremely low FLOPs (i.e., ResNet50@2.7 GFLOPs). Furthermore, our Bi-volution shows better robustness than dynamic convolution against various noise and input corruptions. Our code is available at https://github.com/neuralchen/Bivolution.",
    "code_link": "https://github.com/neuralchen/Bivolution"
  },
  "aaai2022_main_afdetv2rethinkingthenecessityofthesecondstageforobjectdetectionfrompointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds",
    "authors": [
      "Yihan Hu",
      "Zhuangzhuang Ding",
      "Runzhou Ge",
      "Wenxin Shao",
      "Li Huang",
      "Kun Li",
      "Qiang Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19980",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19980/19739",
    "published": "2022-02",
    "summary": "There have been two streams in the 3D detection from point clouds: single-stage methods and two-stage methods. While the former is more computationally efficient, the latter usually provides better detection accuracy. By carefully examining the two-stage approaches, we have found that if appropriately designed, the first stage can produce accurate box regression. In this scenario, the second stage mainly rescores the boxes such that the boxes with better localization get selected. From this observation, we have devised a single-stage anchor-free network that can fulfill these requirements. This network, named AFDetV2, extends the previous work by incorporating a self-calibrated convolution block in the backbone, a keypoint auxiliary supervision, and an IoU prediction branch in the multi-task head. We take a simple product of the predicted IoU score with the classification heatmap to form the final classification confidence. The enhanced backbone strengthens the box localization capability, and the rescoring approach effectively joins the object presence confidence and the box regression accuracy. As a result, the detection accuracy is drastically boosted in the single-stage. To evaluate our approach, we have conducted extensive experiments on the Waymo Open Dataset and the nuScenes Dataset. We have observed that our AFDetV2 achieves the state-of-the-art results on these two datasets, superior to all the prior arts, including both the single-stage and the two-stage 3D detectors. AFDetV2 won the 1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge 2021. In addition, a variant of our model AFDetV2-Base was entitled the \"Most Efficient Model\" by the Challenge Sponsor, showing a superior computational efficiency. To demonstrate the generality of this single-stage method, we have also applied it to the first stage of the two-stage networks. Without exception, the results show that with the strengthened backbone and the rescoring approach, the second stage refinement is no longer needed.",
    "code_link": "https://github.com/open-mmlab/OpenPCDet"
  },
  "aaai2022_main_divide-and-regroupclusteringfordomainadaptivepersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Divide-and-Regroup Clustering for Domain Adaptive Person Re-identification",
    "authors": [
      "Zhengdong Hu",
      "Yifan Sun",
      "Yi Yang",
      "Jianguang Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19981",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19981/19740",
    "published": "2022-02",
    "summary": "Clustering is important for domain adaptive person re-identification(re-ID). A majority of unsupervised domain adaptation (UDA) methods conduct clustering on the target domain and then use the generated pseudo labels for adaptive training. Albeit important, the clustering pipeline adopted by current literature is quite standard and lacks consideration for two characteristics of re-ID, i.e., 1) a single person has various feature distribution in multiple cameras. 2) a person\u2019s occurrence in the same camera are usually temporally continuous. We argue that the multi-camera distribution hinders clustering because it enlarges the intra-class distances. In contrast, the temporal continuity prior is beneficial, because it offers clue for distinguishing some look-alike person (who are temporally far away from each other). These two insight motivate us to propose a novel Divide-And-Regroup Clustering (DARC) pipeline for re-ID UDA. Specifically, DARC divides the unlabeled data into multiple camera-specific groups and conducts local clustering within each camera. Afterwards, it regroups those local clusters potentially belonging to the same person into a unity. Through this divide-and-regroup pipeline, DARC avoids directly clustering across multiple cameras and focuses on the feature distribution within each individual camera. Moreover, during the local clustering, DARC uses the temporal continuity prior to distinguish some look-alike person and thus reduces false positive pseudo labels. Consequentially, DARC effectively reduces clustering errors and improves UDA. Importantly, we show that DARC is compatible to many pseudo label-based UDA methods and brings general improvement. Based on a recent UDA method, DARC advances the state of the art (e.g, 85.1% mAP on MSMT-to-Market and 83.1% mAP on PersonX-to-Market).",
    "code_link": ""
  },
  "aaai2022_main_cmua-watermarkacross-modeluniversaladversarialwatermarkforcombatingdeepfakes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes",
    "authors": [
      "Hao Huang",
      "Yongtao Wang",
      "Zhaoyu Chen",
      "Yuze Zhang",
      "Yuheng Li",
      "Zhi Tang",
      "Wei\n      Chu",
      "Jingdong Chen",
      "Weisi Lin",
      "Kai-Kuang Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19982",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19982/19741",
    "published": "2022-02",
    "summary": "Malicious applications of deepfakes (i.e., technologies generating target facial attributes or entire faces from facial images) have posed a huge threat to individuals' reputation and security. To mitigate these threats, recent studies have proposed adversarial watermarks to combat deepfake models, leading them to generate distorted outputs. Despite achieving impressive results, these adversarial watermarks have low image-level and model-level transferability, meaning that they can protect only one facial image from one specific deepfake model. To address these issues, we propose a novel solution that can generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a large number of facial images from multiple deepfake models. Specifically, we begin by proposing a cross-model universal attack pipeline that attacks multiple deepfake models iteratively. Then, we design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models. Moreover, we address the key problem in cross-model optimization with a heuristic approach to automatically find the suitable attack step sizes for different models, further weakening the model-level conflict. Finally, we introduce a more reasonable and comprehensive evaluation method to fully test the proposed method and compare it with existing ones. Extensive experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by multiple deepfake models while achieving a better performance than existing methods. Our code is available at https://github.com/VDIGPKU/CMUA-Watermark.",
    "code_link": ""
  },
  "aaai2022_main_deconfoundedvisualgrounding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deconfounded Visual Grounding",
    "authors": [
      "Jianqiang Huang",
      "Yu Qin",
      "Jiaxin Qi",
      "Qianru Sun",
      "Hanwang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19983",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19983/19742",
    "published": "2022-02",
    "summary": "We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial languagelocation association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin. Code is available at: https://github.com/JianqiangH/Deconfounded_VG.",
    "code_link": ""
  },
  "aaai2022_main_learningtomodelpixel-embeddedaffinityforhomogeneousinstancesegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Model Pixel-Embedded Affinity for Homogeneous Instance Segmentation",
    "authors": [
      "Wei Huang",
      "Shiyu Deng",
      "Chang Chen",
      "Xueyang Fu",
      "Zhiwei Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19984",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19984/19743",
    "published": "2022-02",
    "summary": "Homogeneous instance segmentation aims to identify each instance in an image where all interested instances belong to the same category, such as plant leaves and microscopic cells. Recently, proposal-free methods, which straightforwardly generate instance-aware information to group pixels into different instances, have received increasing attention due to their efficient pipeline. However, they often fail to distinguish adjacent instances due to similar appearances, dense distribution and ambiguous boundaries of instances in homogeneous images. In this paper, we propose a pixel-embedded affinity modeling method for homogeneous instance segmentation, which is able to preserve the semantic information of instances and improve the distinguishability of adjacent instances. Instead of predicting affinity directly, we propose a self-correlation module to explicitly model the pairwise relationships between pixels, by estimating the similarity between embeddings generated from the input image through CNNs. Based on the self-correlation module, we further design a cross-correlation module to maintain the semantic consistency between instances. Specifically, we map the transformed input images with different views and appearances into the same embedding space, and then mutually estimate the pairwise relationships of embeddings generated from the original input and its transformed variants. In addition, to integrate the global instance information, we introduce an embedding pyramid module to model affinity on different scales. Extensive experiments demonstrate the versatile and superior performance of our method on three representative datasets. Code and models are available at https://github.com/weih527/Pixel-Embedded-Affinity.",
    "code_link": ""
  },
  "aaai2022_main_channelizedaxialattention\u2013consideringchannelrelationwithinspatialattentionforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Channelized Axial Attention \u2013 considering Channel Relation within Spatial Attention for Semantic Segmentation",
    "authors": [
      "Ye Huang",
      "Di Kang",
      "Wenjing Jia",
      "Liu Liu",
      "Xiangjian He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19985",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19985/19744",
    "published": "2022-02",
    "summary": "Spatial and channel attentions, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation. However, computing spatial and channel attentions separately sometimes causes errors, especially for those difficult cases. In this paper, we propose Channelized Axial Attention (CAA) to seamlessly integrate channel attention and spatial attention into a single operation with negligible computation overhead. Specifically, we break down the dot-product operation of the spatial attention into two parts and insert channel relation in between, allowing for independently optimized channel attention on each spatial location. We further develop grouped vectorization, which allows our model to run with very little memory consumption without slowing down the running speed. Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate that our CAA outperforms many state-of-the-art segmentation models (including dual attention) on all tested datasets.",
    "code_link": ""
  },
  "aaai2022_main_ufpmp-dettowardaccurateandefficientobjectdetectionondroneimagery": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "UFPMP-Det:Toward Accurate and Efficient Object Detection on Drone Imagery",
    "authors": [
      "Yecheng Huang",
      "Jiaxin Chen",
      "Di Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19986",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19986/19745",
    "published": "2022-02",
    "summary": "This paper proposes a novel approach to object detection on drone imagery, namely Multi-Proxy Detection Network with Unified Foreground Packing (UFPMP-Det). To deal with the numerous instances of very small scales, different from the common solution that divides the high-resolution input image into quite a number of chips with low foreground ratios to perform detection on them each, the Unified Foreground Packing (UFP) module is designed, where the sub-regions given by a coarse detector are initially merged through clustering to suppress background and the resulting ones are subsequently packed into a mosaic for a single inference, thus significantly reducing overall time cost. Furthermore, to address the more serious confusion between inter-class similarities and intra-class variations of instances, which deteriorates detection performance but is rarely discussed, the Multi-Proxy Detection Network (MP-Det) is presented to model object distributions in a fine-grained manner by employing multiple proxy learning, and the proxies are enforced to be diverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport loss. By such means, UFPMP-Det largely promotes both the detection accuracy and efficiency. Extensive experiments are carried out on the widely used VisDrone and UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much higher speed, highlighting its advantages. The code is available at https://github.com/PuAnysh/UFPMP-Det.",
    "code_link": ""
  },
  "aaai2022_main_modality-adaptivemixupandinvariantdecompositionforrgb-infraredpersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-identification",
    "authors": [
      "Zhipeng Huang",
      "Jiawei Liu",
      "Liang Li",
      "Kecheng Zheng",
      "Zheng-Jun Zha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19987",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19987/19746",
    "published": "2022-02",
    "summary": "RGB-infrared person re-identification is an emerging cross-modality re-identification task, which is very challenging due to significant modality discrepancy between RGB and infrared images. In this work, we propose a novel modality-adaptive mixup and invariant decomposition (MID) approach for RGB-infrared person re-identification towards learning modality-invariant and discriminative representations. MID designs a modality-adaptive mixup scheme to generate suitable mixed modality images between RGB and infrared images for mitigating the inherent modality discrepancy at the pixel-level. It formulates modality mixup procedure as Markov decision process, where an actor-critic agent learns dynamical and local linear interpolation policy between different regions of cross-modality images under a deep reinforcement learning framework. Such policy guarantees modality-invariance in a more continuous latent space and avoids manifold intrusion by the corrupted mixed modality samples. Moreover, to further counter modality discrepancy and enforce invariant visual semantics at the feature-level, MID employs modality-adaptive convolution decomposition to disassemble a regular convolution layer into modality-specific basis layers and a modality-shared coefficient layer. Extensive experimental results on two challenging benchmarks demonstrate superior performance of MID over state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_mumucooperativemultitasklearning-basedguidedmultimodalfusion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion",
    "authors": [
      "Md Mofijul Islam",
      "Tariq Iqbal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19988",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19988/19747",
    "published": "2022-02",
    "summary": "Multimodal sensors (visual, non-visual, and wearable) can provide complementary information to develop robust perception systems for recognizing activities accurately. However, it is challenging to extract robust multimodal representations due to the heterogeneous characteristics of data from multimodal sensors and disparate human activities, especially in the presence of noisy and misaligned sensor data. In this work, we propose a cooperative multitask learning-based guided multimodal fusion approach, MuMu, to extract robust multimodal representations for human activity recognition (HAR). MuMu employs an auxiliary task learning approach to extract features specific to each set of activities with shared characteristics (activity-group). MuMu then utilizes activity-group-specific features to direct our proposed Guided Multimodal Fusion Approach (GM-Fusion) for extracting complementary multimodal representations, designed as the target task. We evaluated MuMu by comparing its performance to state-of-the-art multimodal HAR approaches on three activity datasets. Our extensive experimental results suggest that MuMu outperforms all the evaluated approaches across all three datasets. Additionally, the ablation study suggests that MuMu significantly outperforms the baseline models (p<0.05), which do not use our guided multimodal fusion. Finally, the robust performance of MuMu on noisy and misaligned sensor data posits that our approach is suitable for HAR in real-world settings.",
    "code_link": ""
  },
  "aaai2022_main_anunsupervisedwaytounderstandartifactgeneratinginternalunitsingenerativeneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Unsupervised Way to Understand Artifact Generating Internal Units in Generative Neural Networks",
    "authors": [
      "Haedong Jeong",
      "Jiyeon Han",
      "Jaesik Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19989",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19989/19748",
    "published": "2022-02",
    "summary": "Despite significant improvements on the image generation performance of Generative Adversarial Networks (GANs), generations with low visual fidelity still have been observed. As widely used metrics for GANs focus more on the overall performance of the model, evaluation on the quality of individual generations or detection of defective generations is challenging. While recent studies try to detect featuremap units that cause artifacts and evaluate individual samples, these approaches require additional resources such as external networks or a number of training data to approximate the real data manifold. In this work, we propose the concept of local activation, and devise a metric on the local activation to detect artifact generations without additional supervision.We empirically verify that our approach can detect and correct artifact generations from GANs with various datasets. Finally, we discuss a geometrical analysis to partially reveal the relation between the proposed concept and low visual fidelity.",
    "code_link": ""
  },
  "aaai2022_main_frepganrobustdeepfakedetectionusingfrequency-levelperturbations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FrePGAN: Robust Deepfake Detection Using Frequency-Level Perturbations",
    "authors": [
      "Yonghyun Jeong",
      "Doyeon Kim",
      "Youngmin Ro",
      "Jongwon Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19990",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19990/19749",
    "published": "2022-02",
    "summary": "Various deepfake detectors have been proposed, but challenges still exist to detect images of unknown categories or GAN models outside of the training settings.Such issues arise from the overfitting issue, which we discover from our own analysis and the previous studies to originate from the frequency-level artifacts in generated images. We find that ignoring the frequency-level artifacts can improve the detector's generalization across various GAN models, but it can reduce the model's performance for the trained GAN models. Thus, we design a framework to generalize the deepfake detector for both the known and unseen GAN models. Our framework generates the frequency-level perturbation maps to make the generated images indistinguishable from the real images. By updating the deepfake detector along with the training of the perturbation generator, our model is trained to detect the frequency-level artifacts at the initial iterations and consider the image-level irregularities at the last iterations. For experiments, we design new test scenarios varying from the training settings in GAN models, color manipulations, and object categories. Numerous experiments validate the state-of-the-art performance of our deepfake detector.",
    "code_link": ""
  },
  "aaai2022_main_learningdisentangledattributerepresentationsforrobustpedestrianattributerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Disentangled Attribute Representations for Robust Pedestrian Attribute Recognition",
    "authors": [
      "Jian Jia",
      "Naiyu Gao",
      "Fei He",
      "Xiaotang Chen",
      "Kaiqi Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19991",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19991/19750",
    "published": "2022-02",
    "summary": "Although various methods have been proposed for pedestrian attribute recognition, most studies follow the same feature learning mechanism, \\ie, learning a shared pedestrian image feature to classify multiple attributes. However, this mechanism leads to low-confidence predictions and non-robustness of the model in the inference stage. In this paper, we investigate why this is the case. We mathematically discover that the central cause is that the optimal shared feature cannot maintain high similarities with multiple classifiers simultaneously in the context of minimizing classification loss. In addition, this feature learning mechanism ignores the spatial and semantic distinctions between different attributes. To address these limitations, we propose a novel disentangled attribute feature learning (DAFL) framework to learn a disentangled feature for each attribute, which exploits the semantic and spatial characteristics of attributes. The framework mainly consists of learnable semantic queries, a cascaded semantic-spatial cross-attention (SSCA) module, and a group attention merging (GAM) module. Specifically, based on learnable semantic queries, the cascaded SSCA module iteratively enhances the spatial localization of attribute-related regions and aggregates region features into multiple disentangled attribute features, used for classification and updating learnable semantic queries. The GAM module splits attributes into groups based on spatial distribution and utilizes reliable group attention to supervise query attention maps. Experiments on PETA, RAPv1, PA100k, and RAPv2 show that the proposed method performs favorably against state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_degradeisupgradelearningdegradationforlow-lightimageenhancement": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Degrade Is Upgrade: Learning Degradation for Low-Light Image Enhancement",
    "authors": [
      "Kui Jiang",
      "Zhongyuan Wang",
      "Zheng Wang",
      "Chen Chen",
      "Peng Yi",
      "Tao Lu",
      "Chia-Wen Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19992",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19992/19751",
    "published": "2022-02",
    "summary": "Low-light image enhancement aims to improve an image's visibility while keeping its visual naturalness. Different from existing methods, which tend to accomplish the relighting task directly, we investigate the intrinsic degradation and relight the low-light image while refining the details and color in two steps. Inspired by the color image formulation (diffuse illumination color plus environment illumination color), we first estimate the degradation from low-light inputs to simulate the distortion of environment illumination color, and then refine the content to recover the loss of diffuse illumination color. To this end, we propose a novel Degradation-to-Refinement Generation Network (DRGN). Its distinctive features can be summarized as 1) A novel two-step generation network for degradation learning and content refinement. It is not only superior to one-step methods, but also capable of synthesizing sufficient paired samples to benefit the model training; 2) A multi-resolution fusion network to represent the target information (degradation or contents) in a multi-scale cooperative manner, which is more effective to address the complex unmixing problems. Extensive experiments on both the enhancement task and the joint detection task have verified the effectiveness and efficiency of our proposed method, surpassing the SOTA by 1.59dB on average and 3.18\\% in mAP on the ExDark dataset. The code will be available soon.",
    "code_link": ""
  },
  "aaai2022_main_harmoflharmonizinglocalandglobaldriftsinfederatedlearningonheterogeneousmedicalimages": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images",
    "authors": [
      "Meirui Jiang",
      "Zirui Wang",
      "Qi Dou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19993",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19993/19752",
    "published": "2022-02",
    "summary": "Multiple medical institutions collaboratively training a model using federated learning (FL) has become a promising solution for maximizing the potential of data-driven models, yet the non-independent and identically distributed (non-iid) data in medical images is still an outstanding challenge in real-world practice. The feature heterogeneity caused by diverse scanners or protocols introduces a drift in the learning process, in both local (client) and global (server) optimizations, which harms the convergence as well as model performance. Many previous works have attempted to address the non-iid issue by tackling the drift locally or globally, but how to jointly solve the two essentially coupled drifts is still unclear. In this work, we concentrate on handling both local and global drifts and introduce a new harmonizing framework called HarmoFL. First, we propose to mitigate the local update drift by normalizing amplitudes of images transformed into the frequency domain to mimic a unified imaging setting, in order to generate a harmonized feature space across local clients. Second, based on harmonized features, we design a client weight perturbation guiding each local model to reach a flat optimum, where a neighborhood area of the local optimal solution has a uniformly low loss. Without any extra communication cost, the perturbation assists the global model to optimize towards a converged optimal solution by aggregating several local flat optima. We have theoretically analyzed the proposed method and empirically conducted extensive experiments on three medical image classification and segmentation tasks, showing that HarmoFL outperforms a set of recent state-of-the-art methods with promising convergence behavior. Code is available at: https://github.com/med-air/HarmoFL",
    "code_link": "https://github.com/med-air/HarmoFL"
  },
  "aaai2022_main_coarse-to-finegenerativemodelingforgraphiclayouts": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Coarse-to-Fine Generative Modeling for Graphic Layouts",
    "authors": [
      "Zhaoyun Jiang",
      "Shizhao Sun",
      "Jihua Zhu",
      "Jian-Guang Lou",
      "Dongmei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19994",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19994/19753",
    "published": "2022-02",
    "summary": "Even though graphic layout generation has attracted growing attention recently, it is still challenging to synthesis realistic and diverse layouts, due to the complicated element relationships and varied element arrangements. In this work, we seek to improve the performance of layout generation by incorporating the concept of regions, which consist of a smaller number of elements and appears like a simple layout, into the generation process. Specifically, we leverage Variational Autoencoder (VAE) as the overall architecture and decompose the decoding process into two stages. The first stage predicts representations for regions, and the second stage fills in the detailed position for each element within the region based on the predicted region representation. Compared to prior studies that merely abstract the layout into a list of elements and generate all the element positions in one go, our approach has at least two advantages. First, by the two-stage decoding, our approach decouples the complex layout generation task into several simple layout generation tasks, which reduces the problem difficulty. Second, the predicted regions can help the model roughly know what the graphic layout looks like and serve as global context to improve the generation of detailed element positions. Qualitative and quantitative experiments demonstrate that our approach significantly outperforms the existing methods, especially on the complex graphic layouts.",
    "code_link": ""
  },
  "aaai2022_main_darkvisionnetlow-lightimagingviargb-nirfusionwithdeepinconsistencyprior": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior",
    "authors": [
      "Shuangping Jin",
      "Bingbing Yu",
      "Minhao Jing",
      "Yi Zhou",
      "Jiajun Liang",
      "Renhe Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19995",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19995/19754",
    "published": "2022-02",
    "summary": "RGB-NIR fusion is a promising method for low-light imaging. However, high-intensity noise in low-light images amplifies the effect of structure inconsistency between RGB-NIR images, which fails existing algorithms. To handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net (DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior (DIP). The Deep Structure extracts clear structure details in deep multiscale feature space rather than raw input space, which is more robust to noisy inputs. Based on the deep structures from both RGB and NIR domains, we introduce the DIP to leverage the structure inconsistency to guide the fusion of RGB-NIR. Benefits from this, the proposed DVN obtains high-quality low-light images without the visual artifacts. We also propose a new dataset called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as the first public RGB-NIR fusion benchmark. Quantitative and qualitative results on the proposed benchmark show that DVN significantly outperforms other comparison algorithms in PSNR and SSIM, especially in extremely low light conditions.",
    "code_link": ""
  },
  "aaai2022_main_lagconvlocal-contextadaptiveconvolutionkernelswithglobalharmonicbiasforpansharpening": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LAGConv: Local-Context Adaptive Convolution Kernels with Global Harmonic Bias for Pansharpening",
    "authors": [
      "Zi-Rong Jin",
      "Tian-Jing Zhang",
      "Tai-Xiang Jiang",
      "Gemine Vivone",
      "Liang-Jian\n      Deng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19996",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19996/19755",
    "published": "2022-02",
    "summary": "Pansharpening is a critical yet challenging low-level vision task that aims to obtain a higher-resolution image by fusing a multispectral (MS) image and a panchromatic (PAN) image. While most pansharpening methods are based on convolutional neural network (CNN) architectures with standard convolution operations, few attempts have been made with context-adaptive/dynamic convolution, which delivers impressive results on high-level vision tasks. In this paper, we propose a novel strategy to generate local-context adaptive (LCA) convolution kernels and introduce a new global harmonic (GH) bias mechanism, exploiting image local specificity as well as integrating global information, dubbed LAGConv. The proposed LAGConv can replace the standard convolution that is context-agnostic to fully perceive the particularity of each pixel for the task of remote sensing pansharpening. Furthermore, by applying the LAGConv, we provide an image fusion network architecture, which is more effective than conventional CNN-based pansharpening approaches. The superiority of the proposed method is demonstrated by extensive experiments implemented on a wide range of datasets compared with state-of-the-art pansharpening methods. Besides, more discussions testify that the proposed LAGConv outperforms recent adaptive convolution techniques for pansharpening.",
    "code_link": "https://github.com/liangjiandeng/LAGConv"
  },
  "aaai2022_main_learningthedynamicsofvisualrelationalreasoningviareinforcedpathrouting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning the Dynamics of Visual Relational Reasoning via Reinforced Path Routing",
    "authors": [
      "Chenchen Jing",
      "Yunde Jia",
      "Yuwei Wu",
      "Chuanhao Li",
      "Qi Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19997",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19997/19756",
    "published": "2022-02",
    "summary": "Reasoning is a dynamic process. In cognitive theories, the dynamics of reasoning refers to reasoning states over time after successive state transitions. Modeling the cognitive dynamics is of utmost importance to simulate human reasoning capability. In this paper, we propose to learn the reasoning dynamics of visual relational reasoning by casting it as a path routing task. We present a reinforced path routing method that represents an input image via a structured visual graph and introduces a reinforcement learning based model to explore paths (sequences of nodes) over the graph based on an input sentence to infer reasoning results. By exploring such paths, the proposed method represents reasoning states clearly and characterizes state transitions explicitly to fully model the reasoning dynamics for accurate and transparent visual relational reasoning. Extensive experiments on referring expression comprehension and visual question answering demonstrate the effectiveness of our method.",
    "code_link": ""
  },
  "aaai2022_main_towardsto-a-tspatio-temporalfocusforskeleton-basedactionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards To-a-T Spatio-Temporal Focus for Skeleton-Based Action Recognition",
    "authors": [
      "Lipeng Ke",
      "Kuan-Chuan Peng",
      "Siwei Lyu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19998",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19998/19757",
    "published": "2022-02",
    "summary": "Graph Convolutional Networks (GCNs) have been widely used to model the high-order dynamic dependencies for skeleton-based action recognition. Most existing approaches do not explicitly embed the high-order spatio-temporal importance to joints\u2019 spatial connection topology and intensity, and they do not have direct objectives on their attention module to jointly learn when and where to focus on in the action sequence. To address these problems, we propose the To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition framework that utilizes the spatio-temporal gradient to focus on relevant spatio-temporal features. We first propose the STF modules with learnable gradient-enforced and instance-dependent adjacency matrices to model the high-order spatio-temporal dynamics. Second, we propose three loss terms defined on the gradient-based spatio-temporal focus to explicitly guide the classifier when and where to look at, distinguish confusing classes, and optimize the stacked STF modules. STF outperforms the state-of-the-art methods on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all 15 settings over different views, subjects, setups, and input modalities, and STF also shows better accuracy on scarce data and dataset shifting settings.",
    "code_link": ""
  },
  "aaai2022_main_modnetreal-timetrimap-freeportraitmattingviaobjectivedecomposition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition",
    "authors": [
      "Zhanghan Ke",
      "Jiayu Sun",
      "Kaican Li",
      "Qiong Yan",
      "Rynson W.H. Lau"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19999",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/19999/19758",
    "published": "2022-02",
    "summary": "Existing portrait matting methods either require auxiliary inputs that are costly to obtain or involve multiple stages that are computationally expensive, making them less suitable for real-time applications. In this work, we present a light-weight matting objective decomposition network (MODNet) for portrait matting in real-time with a single input image. The key idea behind our efficient design is by optimizing a series of sub-objectives simultaneously via explicit constraints. In addition, MODNet includes two novel techniques for improving model efficiency and robustness. First, an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for semantic estimation. Second, a self-supervised sub-objectives consistency (SOC) strategy is proposed to adapt MODNet to real-world data to address the domain shift problem common to trimap-free methods. MODNet is easy to be trained in an end-to-end manner. It is much faster than contemporaneous methods and runs at 67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms prior trimap-free methods by a large margin on both Adobe Matting Dataset and a carefully designed photographic portrait matting (PPM-100) benchmark proposed by us. Further, MODNet achieves remarkable results on daily photos and videos.",
    "code_link": ""
  },
  "aaai2022_main_learningmixtureofdomain-specificexpertsviadisentangledfactorsforautonomousdriving": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Mixture of Domain-Specific Experts via Disentangled Factors for Autonomous Driving",
    "authors": [
      "Inhan Kim",
      "Joonyeong Lee",
      "Daijin Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20000",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20000/19759",
    "published": "2022-02",
    "summary": "Since human drivers only consider the driving-related factors that affect vehicle control depending on the situation, they can drive safely even in diverse driving environments. To mimic this behavior, we propose an autonomous driving framework based on the two-stage representation learning that initially splits the latent features as domain-specific features and domain-general features. Subsequently, the dynamic-object features, which contain information of dynamic objects, are disentangled from latent features using mutual information estimator. In this study, the problem in behavior cloning is divided into several domain-specific subspaces, with experts becoming specialized on each domain-specific policy. The proposed mixture of domain-specific experts (MoDE) model predicts the final control values through the cooperation of experts using a gating function. The domain-specific features are used to calculate the importance weight of the domain-specific experts, and the disentangled domain-general and dynamic-object features are applied in estimating the control values. To validate the proposed MoDE model, we conducted several experiments and achieved a higher success rate on the CARLA benchmarks under several conditions and tasks than state-of-the-art approaches.",
    "code_link": ""
  },
  "aaai2022_main_towardsversatilepedestriandetectorwithmultisensory-matchingandmultispectralrecallingmemory": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Versatile Pedestrian Detector with Multisensory-Matching and Multispectral Recalling Memory",
    "authors": [
      "Jung Uk Kim",
      "Sungjune Park",
      "Yong Man Ro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20001",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20001/19760",
    "published": "2022-02",
    "summary": "Recently, automated surveillance cameras can change a visible sensor and a thermal sensor for all-day operation. However, existing single-modal pedestrian detectors mainly focus on detecting pedestrians in only one specific modality (i.e., visible or thermal), so they cannot cope with other modal inputs. In addition, recent multispectral pedestrian detectors have shown remarkable performance by adopting multispectral modalities, but they also have limitations in practical applications (e.g., different Field-of-View (FoV) and frame rate). In this paper, we introduce a versatile pedestrian detector that shows robust detection performance in any single modality. We propose a multisensory-matching contrastive loss to reduce the difference between the visual representation of pedestrians in the visible and thermal modalities. Moreover, for the robust detection on a single modality, we design a Multispectral Recalling (MSR) Memory. The MSR Memory enhances the visual representation of the single modal features by recalling that of the multispectral modalities. To guide the MSR Memory to store the multispectral modal contexts, we introduce a multispectral recalling loss. It enables the pedestrian detector to encode more discriminative features with a single input modality. We believe our method is a step forward detector that can be applied to a variety of real-world applications. The comprehensive experimental results verify the effectiveness of the proposed method.",
    "code_link": ""
  },
  "aaai2022_main_semanticfeatureextractionforgeneralizedzero-shotlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semantic Feature Extraction for Generalized Zero-Shot Learning",
    "authors": [
      "Junhan Kim",
      "Kyuhong Shim",
      "Byonghyo Shim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20002",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20002/19761",
    "published": "2022-02",
    "summary": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the attribute. In this paper, we put forth a new GZSL technique that improves the GZSL classification performance greatly.Key idea of the proposed approach, henceforth referred to as semantic feature extraction-based GZSL (SE-GZSL), is to use the semantic feature containing only attribute-related information in learning the relationship between the image and the attribute.In doing so, we can remove the interference, if any, caused by the attribute-irrelevant information contained in the image feature.To train a network extracting the semantic feature, we present two novel loss functions, 1) mutual information-based loss to capture all the attribute-related information in the image feature and 2) similarity-based loss to remove unwanted attribute-irrelevant information.From extensive experiments using various datasets, we show that the proposed SE-GZSL technique outperforms conventional GZSL approaches by a large margin.",
    "code_link": ""
  },
  "aaai2022_main_distinguishinghomophenesusingmulti-headvisual-audiomemoryforlipreading": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading",
    "authors": [
      "Minsu Kim",
      "Jeong Hun Yeo",
      "Yong Man Ro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20003",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20003/19762",
    "published": "2022-02",
    "summary": "Recognizing speech from silent lip movement, which is called lip reading, is a challenging task due to 1) the inherent information insufficiency of lip movement to fully represent the speech, and 2) the existence of homophenes that have similar lip movement with different pronunciations. In this paper, we try to alleviate the aforementioned two challenges in lip reading by proposing a Multi-head Visual-audio Memory (MVM). Firstly, MVM is trained with audio-visual datasets and remembers audio representations by modelling the inter-relationships of paired audio-visual representations. At the inference stage, visual input alone can extract the saved audio representation from the memory by examining the learned inter-relationships. Therefore, the lip reading model can complement the insufficient visual information with the extracted audio representations. Secondly, MVM is composed of multi-head key memories for saving visual features and one value memory for saving audio knowledge, which is designed to distinguish the homophenes. With the multi-head key memories, MVM extracts possible candidate audio features from the memory, which allows the lip reading model to consider the possibility of which pronunciations can be represented from the input lip movement. This also can be viewed as an explicit implementation of the one-to-many mapping of viseme-to-phoneme. Moreover, MVM is employed in multi-temporal levels to consider the context when retrieving the memory and distinguish the homophenes. Extensive experimental results verify the effectiveness of the proposed method in lip reading and in distinguishing the homophenes.",
    "code_link": ""
  },
  "aaai2022_main_deeptranslationpriortest-timetrainingforphotorealisticstyletransfer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Translation Prior: Test-Time Training for Photorealistic Style Transfer",
    "authors": [
      "Sunwoo Kim",
      "Soohyun Kim",
      "Seungryong Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20004",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20004/19763",
    "published": "2022-02",
    "summary": "Recent techniques to solve photorealistic style transfer within deep convolutional neural networks (CNNs) generally require intensive training from large-scale datasets, thus having limited applicability and poor generalization ability to unseen images or styles. To overcome this, we propose a novel framework, dubbed Deep Translation Prior (DTP), to accomplish photorealistic style transfer through test-time training on given input image pair with untrained networks, which learns an image pair-specific translation prior and thus yields better performance and generalization. Tailored for such test-time training for style transfer, we present novel network architectures, with two sub-modules of correspondence and generation modules, and loss functions consisting of contrastive content, style, and cycle consistency losses. Our framework does not require offline training phase for style transfer, which has been one of the main challenges in existing methods, but the networks are to be solely learned during test time. Experimental results prove that our framework has a better generalization ability to unseen image pairs and even outperforms the state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_privatesnnprivacy-preservingspikingneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PrivateSNN: Privacy-Preserving Spiking Neural Networks",
    "authors": [
      "Youngeun Kim",
      "Yeshwanth Venkatesha",
      "Priyadarshini Panda"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20005",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20005/19764",
    "published": "2022-02",
    "summary": "How can we bring both privacy and energy-efficiency to a neural system? In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset. Here, we tackle two types of leakage problems: 1) Data leakage is caused when the networks access real training data during an ANN-SNN conversion process. 2) Class leakage is caused when class-related features can be reconstructed from network parameters. In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using the generated images. However, converted SNNs remain vulnerable to class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule. Updating weight parameters with temporal data makes SNNs difficult to be interpreted in the spatial domain. We observe that the encrypted PrivateSNN eliminates data and class leakage issues with a slight performance drop (less than ~2%) and significant energy-efficiency gain (about 55x) compared to the standard ANN. We conduct extensive experiments on various datasets includingCIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training.",
    "code_link": ""
  },
  "aaai2022_main_naturalinversiondata-freeimagesynthesisimprovingreal-worldconsistency": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency",
    "authors": [
      "Yujin Kim",
      "Dogyun Park",
      "Dohee Kim",
      "Suhyun Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20006",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20006/19765",
    "published": "2022-02",
    "summary": "We introduce NaturalInversion, a novel model inversion-based method to synthesize images that agrees well with the original data distribution without using real data. In NaturalInversion, we propose: (1) a Feature Transfer Pyramid which uses enhanced image prior of the original data by combining the multi-scale feature maps extracted from the pre-trained classifier, (2) a one-to-one approach generative model where only one batch of images are synthesized by one generator to bring the non-linearity to optimization and to ease the overall optimizing process, (3) learnable Adaptive Channel Scaling parameters which are end-to-end trained to scale the output image channel to utilize the original image prior further. With our NaturalInversion, we synthesize images from classifiers trained on CIFAR-10/100 and show that our images are more consistent with original data distribution than prior works by visualization and additional analysis. Furthermore, our synthesized images outperform prior works on various applications such as knowledge distillation and pruning, demonstrating the effectiveness of our proposed method.",
    "code_link": ""
  },
  "aaai2022_main_joint3dobjectdetectionandtrackingusingspatio-temporalrepresentationofcameraimageandlidarpointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Joint 3D Object Detection and Tracking Using Spatio-Temporal Representation of Camera Image and LiDAR Point Clouds",
    "authors": [
      "Junho Koh",
      "Jaekyum Kim",
      "Jin Hyeok Yoo",
      "Yecheol Kim",
      "Dongsuk Kum",
      "Jun Won\n      Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20007",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20007/19766",
    "published": "2022-02",
    "summary": "In this paper, we propose a new joint object detection and tracking (JoDT) framework for 3D object detection and tracking based on camera and LiDAR sensors. The proposed method, referred to as 3D DetecTrack, enables the detector and tracker to cooperate to generate a spatio-temporal representation of the camera and LiDAR data, with which 3D object detection and tracking are then performed. The detector constructs the spatio-temporal features via the weighted temporal aggregation of the spatial features obtained by the camera and LiDAR fusion. Then, the detector reconfigures the initial detection results using information from the tracklets maintained up to the previous time step. Based on the spatio-temporal features generated by the detector, the tracker associates the detected objects with previously tracked objects using a graph neural network (GNN). We devise a fully-connected GNN facilitated by a combination of rule-based edge pruning and attention-based edge gating, which exploits both spatial and temporal object contexts to improve tracking performance. The experiments conducted on both KITTI and nuScenes benchmarks demonstrate that the proposed 3D DetecTrack achieves significant improvements in both detection and tracking performances over baseline methods and achieves state-of-the-art performance among existing methods through collaboration between the detector and tracker.",
    "code_link": ""
  },
  "aaai2022_main_amplitudespectrumtransformationforopencompounddomainadaptivesemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation",
    "authors": [
      "Jogendra Nath Kundu",
      "Akshay R Kulkarni",
      "Suvaansh Bhambri",
      "Varun Jampani",
      "Venkatesh Babu Radhakrishnan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20008",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20008/19767",
    "published": "2022-02",
    "summary": "Open compound domain adaptation (OCDA) has emerged as a practical adaptation setting which considers a single labeled source domain against a compound of multi-modal unlabeled target data in order to generalize better on novel unseen domains. We hypothesize that an improved disentanglement of domain-related and task-related factors of dense intermediate layer features can greatly aid OCDA. Prior-arts attempt this indirectly by employing adversarial domain discriminators on the spatial CNN output. However, we find that latent features derived from the Fourier-based amplitude spectrum of deep CNN features hold a more tractable mapping with domain discrimination. Motivated by this, we propose a novel feature space Amplitude Spectrum Transformation (AST). During adaptation, we employ the AST auto-encoder for two purposes. First, carefully mined source-target instance pairs undergo a simulation of cross-domain feature stylization (AST-Sim) at a particular layer by altering the AST-latent. Second, AST operating at a later layer is tasked to normalize (AST-Norm) the domain content by fixing its latent to a mean prototype. Our simplified adaptation technique is not only clustering-free but also free from complex adversarial alignment. We achieve leading performance against the prior arts on the OCDA scene segmentation benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_siamesenetworkwithinteractivetransformerforvideoobjectsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Siamese Network with Interactive Transformer for Video Object Segmentation",
    "authors": [
      "Meng Lan",
      "Jing Zhang",
      "Fengxiang He",
      "Lefei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20009",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20009/19768",
    "published": "2022-02",
    "summary": "Semi-supervised video object segmentation (VOS) refers to segmenting the target object in remaining frames given its annotation in the first frame, which has been actively studied in recent years. The key challenge lies in finding effective ways to exploit the spatio-temporal context of past frames to help learn discriminative target representation of current frame. In this paper, we propose a novel Siamese network with a specifically designed interactive transformer, called SITVOS, to enable effective context propagation from historical to current frames. Technically, we use the transformer encoder and decoder to handle the past frames and current frame separately, i.e., the encoder encodes robust spatio-temporal context of target object from the past frames, while the decoder takes the feature embedding of current frame as the query to retrieve the target from the encoder output. To further enhance the target representation, a feature interaction module (FIM) is devised to promote the information flow between the encoder and decoder. Moreover, we employ the Siamese architecture to extract backbone features of both past and current frames, which enables feature reuse and is more efficient than existing methods. Experimental results on three challenging benchmarks validate the superiority of SITVOS over state-of-the-art methods. Code is available at https://github.com/LANMNG/SITVOS.",
    "code_link": "https://github.com/LANMNG/SITVOS"
  },
  "aaai2022_main_adversarialattackforasynchronousevent-baseddata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Attack for Asynchronous Event-Based Data",
    "authors": [
      "Wooju Lee",
      "Hyun Myung"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20010",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20010/19769",
    "published": "2022-02",
    "summary": "Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes. Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited. Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving. However, the given adversarial events make the current deep learning model vulnerable to safety issues. In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time. Our algorithm shifts the time of the original events and generates additional adversarial events. Additional adversarial events are generated in two stages. First, null events are added to the event-based data to generate additional adversarial events. The perturbation size can be controlled with the number of null events. Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack. Our algorithm achieves an attack success rate of 97.95% on the N-Caltech101 dataset. Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model.",
    "code_link": ""
  },
  "aaai2022_main_iterativelyselectinganeasyreferenceframemakesunsupervisedvideoobjectsegmentationeasier": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Iteratively Selecting an Easy Reference Frame Makes Unsupervised Video Object Segmentation Easier",
    "authors": [
      "Youngjo Lee",
      "Hongje Seong",
      "Euntai Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20011",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20011/19770",
    "published": "2022-02",
    "summary": "Unsupervised video object segmentation (UVOS) is a per-pixel binary labeling problem which aims at separating the foreground object from the background in the video without using the ground truth (GT) mask of the foreground object. Most of the previous UVOS models use the first frame or the entire video as a reference frame to specify the mask of the foreground object. Our question is why the first frame should be selected as a reference frame or why the entire video should be used to specify the mask. We believe that we can select a better reference frame to achieve the better UVOS performance than using only the first frame or the entire video as a reference frame. In our paper, we propose Easy Frame Selector (EFS). The EFS enables us to select an \"easy\" reference frame that makes the subsequent VOS become easy, thereby improving the VOS performance. Furthermore, we propose a new framework named as Iterative Mask Prediction (IMP). In the framework, we repeat applying EFS to the given video and selecting an \"easier\" reference frame from the video than the previous iteration, increasing the VOS performance incrementally. The IMP consists of EFS, Bi-directional Mask Prediction (BMP), and Temporal Information Updating (TIU). From the proposed framework, we achieve state-of-the-art performance in three UVOS benchmark sets: DAVIS16, FBMS, and SegTrack-V2.",
    "code_link": ""
  },
  "aaai2022_main_sctnsparseconvolution-transformernetworkforsceneflowestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation",
    "authors": [
      "Bing Li",
      "Cheng Zheng",
      "Silvio Giancola",
      "Bernard Ghanem"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20012",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20012/19771",
    "published": "2022-02",
    "summary": "We propose a novel scene flow estimation approach to capture and infer 3D motions from point clouds. Estimating 3D motions for point clouds is challenging, since a point cloud is unordered and its density is significantly non-uniform. Such unstructured data poses difficulties in matching corresponding points between point clouds, leading to inaccurate flow estimation. We propose a novel architecture named Sparse Convolution-Transformer Network (SCTN) that equips the sparse convolution with the transformer. Specifically, by leveraging the sparse convolution, SCTN transfers irregular point cloud into locally consistent flow features for estimating spatially consistent motions within an object/local object part. We further propose to explicitly learn point relations using a point transformer module, different from exiting methods. We show that the learned relation-based contextual information is rich and helpful for matching corresponding points, benefiting scene flow estimation. In addition, a novel loss function is proposed to adaptively encourage flow consistency according to feature similarity. Extensive experiments demonstrate that our proposed approach achieves a new state of the art in scene flow estimation. Our approach achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene Flow respectively, which significantly outperforms previous methods by large margins.",
    "code_link": ""
  },
  "aaai2022_main_shrinkingtemporalattentionintransformersforvideoactionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shrinking Temporal Attention in Transformers for Video Action Recognition",
    "authors": [
      "Bonan Li",
      "Pengfei Xiong",
      "Congying Han",
      "Tiande Guo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20013",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20013/19772",
    "published": "2022-02",
    "summary": "Spatiotemporal modeling in an unified architecture is key for video action recognition. This paper proposes a Shrinking Temporal Attention Transformer (STAT), which efficiently builts spatiotemporal attention maps considering the attenuation of spatial attention in short and long temporal sequences. Specifically, for short-term temporal tokens, query token interacts with them in a fine-grained manner in dealing with short-range motion. It then shrinks to a coarse attention in neighborhood for long-term tokens, to provide larger receptive field for long-range spatial aggregation. Both of them are composed in a short-long temporal integrated block to build visual appearances and temporal structure concurrently with lower costly in computation. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple action recognition benchmarks including Kinetics400 and Something-Something v2, outperforming prior methods with 50% less FLOPs and without any pretrained model.",
    "code_link": ""
  },
  "aaai2022_main_danceformermusicconditioned3ddancegenerationwithparametricmotiontransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer",
    "authors": [
      "Buyu Li",
      "Yongchi Zhao",
      "Shi Zhelun",
      "Lu Sheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20014",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20014/19773",
    "published": "2022-02",
    "summary": "Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, i.e., a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset also encodes dances as key poses and parametric motion curves apart from pose sequences, thus benefiting the training of our DanceFormer. Extensive experiments demonstrate that the proposed method, even trained by existing datasets, can generate fluent, performative, and music-matched 3D dances that surpass previous works quantitatively and qualitatively. Moreover, the proposed DanceFormer, together with the PhantomDance dataset, are seamlessly compatible with industrial animation software, thus facilitating the adaptation for various downstream applications.",
    "code_link": ""
  },
  "aaai2022_main_interpretablegenerativeadversarialnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Interpretable Generative Adversarial Networks",
    "authors": [
      "Chao Li",
      "Kelu Yao",
      "Jin Wang",
      "Boyu Diao",
      "Yongjun Xu",
      "Quanshi Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20015",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20015/19774",
    "published": "2022-02",
    "summary": "Learning a disentangled representation is still a challenge in the field of the interpretability of generative adversarial networks (GANs). This paper proposes a generic method to modify a traditional GAN into an interpretable GAN, which ensures that filters in an intermediate layer of the generator encode disentangled localized visual concepts. Each filter in the layer is supposed to consistently generate image regions corresponding to the same visual concept when generating different images. The interpretable GAN learns to automatically discover meaningful visual concepts without any annotations of visual concepts. The interpretable GAN enables people to modify a specific visual concept on generated images by manipulating feature maps of the corresponding filters in the layer. Our method can be broadly applied to different types of GANs. Experiments have demonstrated the effectiveness of our method.",
    "code_link": "https://github.com/denis19973/faceshifter"
  },
  "aaai2022_main_cross-modalobjecttrackingmodality-awarerepresentationsandaunifiedbenchmark": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Modal Object Tracking: Modality-Aware Representations and a Unified Benchmark",
    "authors": [
      "Chenglong Li",
      "Tianhao Zhu",
      "Lei Liu",
      "Xiaonan Si",
      "Zilin Fan",
      "Sulan Zhai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20016",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20016/19775",
    "published": "2022-02",
    "summary": "In many visual systems, visual tracking often bases on RGB image sequences, in which some targets are invalid in low-light conditions, and tracking performance is thus affected significantly. Introducing other modalities such as depth and infrared data is an effective way to handle imaging limitations of individual sources, but multi-modal imaging platforms usually require elaborate designs and cannot be applied in many real-world applications at present. Near-infrared (NIR) imaging becomes an essential part of many surveillance cameras, whose imaging is switchable between RGB and NIR based on the light intensity. These two modalities are heterogeneous with very different visual properties and thus bring big challenges for visual tracking. However, existing works have not studied this challenging problem. In this work, we address the cross-modal object tracking problem and contribute a new video dataset, including 654 cross-modal image sequences with over 481K frames in total, and the average video length is more than 735 frames. To promote the research and development of cross-modal object tracking, we propose a new algorithm, which learns the modality-aware target representation to mitigate the appearance gap between RGB and NIR modalities in the tracking process. It is plug-and-play and could thus be flexibly embedded into different tracking frameworks. Extensive experiments on the dataset are conducted, and we demonstrate the effectiveness of the proposed algorithm in two representative tracking frameworks against 19 state-of-the-art tracking methods. Dataset, code, model and results are available at https://github.com/mmic-lcl/source-code.",
    "code_link": "https://github.com/mmiclcl/source-code"
  },
  "aaai2022_main_youonlyinferoncecross-modalmeta-transferforreferringvideoobjectsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation",
    "authors": [
      "Dezhuang Li",
      "Ruoqi Li",
      "Lijun Wang",
      "Yifan Wang",
      "Jinqing Qi",
      "Lu Zhang",
      "Ting\n      Liu",
      "Qingquan Xu",
      "Huchuan Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20017",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20017/19776",
    "published": "2022-02",
    "summary": "We present YOFO (You Only inFer Once), a new paradigm for referring video object segmentation (RVOS) that operates in an one-stage manner. Our key insight is that the language descriptor should serve as target-specific guidance to identify the target object, while a direct feature fusion of image and language can increase feature complexity and thus may be sub-optimal for RVOS. To this end, we propose a meta-transfer module, which is trained in a learning-to-learn fashion and aims to transfer the target-specific information from the language domain to the image domain, while discarding the uncorrelated complex variations of language description. To bridge the gap between the image and language domains, we develop a multi-scale cross-modal feature mining block that aggregates all the essential features required by RVOS from both domains and generates regression labels for the meta-transfer module. The whole system can be trained in an end-to-end manner and shows competitive performance against state-of-the-art two-stage approaches.",
    "code_link": ""
  },
  "aaai2022_main_knowledgedistillationforobjectdetectionviarankmimickingandprediction-guidedfeatureimitation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-Guided Feature Imitation",
    "authors": [
      "Gang Li",
      "Xiang Li",
      "Yujie Wang",
      "Shanshan Zhang",
      "Yichao Wu",
      "Ding Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20018",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20018/19777",
    "published": "2022-02",
    "summary": "Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods.",
    "code_link": ""
  },
  "aaai2022_main_rethinkingpseudolabelsforsemi-supervisedobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Rethinking Pseudo Labels for Semi-supervised Object Detection",
    "authors": [
      "Hengduo Li",
      "Zuxuan Wu",
      "Abhinav Shrivastava",
      "Larry S. Davis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20019",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20019/19778",
    "published": "2022-02",
    "summary": "Recent advances in semi-supervised object detection (SSOD) are largely driven by consistency-based pseudo-labeling methods for image classification tasks, producing pseudo labels as supervisory signals. However, when using pseudo labels, there is a lack of consideration in localization precision and amplified class imbalance, both of which are critical for detection tasks. In this paper, we introduce certainty-aware pseudo labels tailored for object detection, which can effectively estimate the classification and localization quality of derived pseudo labels. This is achieved by converting conventional localization as a classification task followed by refinement. Conditioned on classification and localization quality scores, we dynamically adjust the thresholds used to generate pseudo labels and reweight loss functions for each category to alleviate the class imbalance problem. Extensive experiments demonstrate that our method improves state-of-the-art SSOD performance by 1-2% AP on COCO and PASCAL VOC while being orthogonal and complementary to most existing methods. In the limited-annotation regime, our approach improves supervised baselines by up to 10% AP using only 1-10% labeled data from COCO.",
    "code_link": ""
  },
  "aaai2022_main_action-awareembeddingenhancementforimage-textretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Action-Aware Embedding Enhancement for Image-Text Retrieval",
    "authors": [
      "Jiangtong Li",
      "Li Niu",
      "Liqing Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20020",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20020/19779",
    "published": "2022-02",
    "summary": "Image-text retrieval plays a central role in bridging vision and language, which aims to reduce the semantic discrepancy between images and texts. Most of existing works rely on refined words and objects representation through the data-oriented method to capture the word-object cooccurrence. Such approaches are prone to ignore the asymmetric action relation between images and texts, that is, the text has explicit action representation (i.e., verb phrase) while the image only contains implicit action information. In this paper, we propose Action-aware Memory-Enhanced embedding (AME) method for image-text retrieval, which aims to emphasize the action information when mapping the images and texts into a shared embedding space. Specifically, we integrate action prediction along with an action-aware memory bank to enrich the image and text features with action-similar text features. The effectiveness of our proposed AME method is verified by comprehensive experimental results on two benchmark datasets.",
    "code_link": ""
  },
  "aaai2022_main_retinomorphicobjectdetectioninasynchronousvisualstreams": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Retinomorphic Object Detection in Asynchronous Visual Streams",
    "authors": [
      "Jianing Li",
      "Xiao Wang",
      "Lin Zhu",
      "Jia Li",
      "Tiejun Huang",
      "Yonghong Tian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20021",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20021/19780",
    "published": "2022-02",
    "summary": "Due to high-speed motion blur and challenging illumination, conventional frame-based cameras have encountered an important challenge in object detection tasks. Neuromorphic cameras that output asynchronous visual streams instead of intensity frames, by taking the advantage of high temporal resolution and high dynamic range, have brought a new perspective to address the challenge. In this paper, we propose a novel problem setting, retinomorphic object detection, which is the first trial that integrates foveal-like and peripheral-like visual streams. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-Vidar-DVS) over 215.5k spatio-temporal synchronized labels. Then, we design temporal aggregation representations to preserve the spatio-temporal information from asynchronous visual streams. Finally, we present a novel bio-inspired unifying framework to fuse two sensing modalities via a dynamic interaction mechanism. Our experimental evaluation shows that our approach has significant improvements over the state-of-the-art methods with the single-modality, especially in high-speed motion and low-light scenarios. We hope that our work will attract further research into this newly identified, yet crucial research direction. Our dataset can be available at https://www.pkuml.org/resources/pku-vidar-dvs.html.",
    "code_link": ""
  },
  "aaai2022_main_learningfromweakly-labeledwebvideosviaexploringsub-concepts": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning from Weakly-Labeled Web Videos via Exploring Sub-concepts",
    "authors": [
      "Kunpeng Li",
      "Zizhao Zhang",
      "Guanhang Wu",
      "Xuehan Xiong",
      "Chen-Yu Lee",
      "Zhichao\n      Lu",
      "Yun Fu",
      "Tomas Pfister"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20022",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20022/19781",
    "published": "2022-02",
    "summary": "Learning visual knowledge from massive weakly-labeled web videos has attracted growing research interests thanks to the large corpus of easily accessible video data on the Internet. However, for video action recognition, the action of interest might only exist in arbitrary clips of untrimmed web videos, resulting in high label noises in the temporal space. To address this challenge, we introduce a new method for pre-training video action recognition models using queried web videos. Instead of trying to filter out potential noises, we propose to provide fine-grained supervision signals by defining the concept of Sub-Pseudo Label (SPL). Specifically, SPL spans out a new set of meaningful \"middle ground\" label space constructed by extrapolating the original weak labels during video querying and the prior knowledge distilled from a teacher model. Consequently, SPL provides enriched supervision for video models to learn better representations and improves data utilization efficiency of untrimmed videos. We validate the effectiveness of our method on four video action recognition datasets and a weakly-labeled image dataset. Experiments show that SPL outperforms several existing pre-training strategies and the learned representations lead to competitive results on several benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_learninguniversaladversarialperturbationbyadversarialexample": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Universal Adversarial Perturbation by Adversarial Example",
    "authors": [
      "Maosen Li",
      "Yanhua Yang",
      "Kun Wei",
      "Xu Yang",
      "Heng Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20023",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20023/19782",
    "published": "2022-02",
    "summary": "Deep learning models have shown to be susceptible to universal adversarial perturbation (UAP), which has aroused wide concerns in the community. Compared with the conventional adversarial attacks that generate adversarial samples at the instance level, UAP can fool the target model for different instances with only a single perturbation, enabling us to evaluate the robustness of the model from a more effective and accurate perspective. The existing universal attack methods fail to exploit the differences and connections between the instance and universal levels to produce dominant perturbations. To address this challenge, we propose a new universal attack method that unifies instance-specific and universal attacks from a feature perspective to generate a more dominant UAP. Specifically, we reformulate the UAP generation task as a minimax optimization problem and then utilize the instance-specific attack method to solve the minimization problem thereby obtaining better training data for generating UAP. At the same time, we also introduce a consistency regularizer to explore the relationship between training data, thus further improving the dominance of the generated UAP. Furthermore, our method is generic with no additional assumptions about the training data and hence can be applied to both data-dependent (supervised) and data-independent (unsupervised) manners. Extensive experiments demonstrate that the proposed method improves the performance by a significant margin over the existing methods in both data-dependent and data-independent settings. Code is available at https://github.com/lisenxd/AT-UAP.",
    "code_link": "https://github.com/lisenxd/AT-UAP"
  },
  "aaai2022_main_logitperturbation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Logit Perturbation",
    "authors": [
      "Mengyang Li",
      "Fengguang Su",
      "Ou Wu",
      "Ji Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20024",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20024/19783",
    "published": "2022-02",
    "summary": "Features, logits, and labels are the three primary data when a sample passes through a deep neural network. Feature perturbation and label perturbation receive increasing attention in recent years. They have been proven to be useful in various deep learning approaches. For example, (adversarial) feature perturbation can improve the robustness or even generalization capability of learned models. However, limited studies have explicitly explored for the perturbation of logit vectors. This work discusses several existing methods related to logit perturbation. Based on a unified viewpoint between positive/negative data augmentation and loss variations incurred by logit perturbation, a new method is proposed to explicitly learn to perturb logits. A comparative analysis is conducted for the perturbations used in our and existing methods. Extensive experiments on benchmark image classification data sets and their long-tail versions indicated the competitive performance of our learning method. In addition, existing methods can be further improved by utilizing our method.",
    "code_link": "https://github.com/limengyang1992/lpl"
  },
  "aaai2022_main_neighborhood-adaptivestructureaugmentedmetriclearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neighborhood-Adaptive Structure Augmented Metric Learning",
    "authors": [
      "Pandeng Li",
      "Yan Li",
      "Hongtao Xie",
      "Lei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20025",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20025/19784",
    "published": "2022-02",
    "summary": "Most metric learning techniques typically focus on sample embedding learning, while implicitly assume a homogeneous local neighborhood around each sample, based on the metrics used in training ( e.g., hypersphere for Euclidean distance or unit hyperspherical crown for cosine distance). As real-world data often lies on a low-dimensional manifold curved in a high-dimensional space, it is unlikely that everywhere of the manifold shares the same local structures in the input space. Besides, considering the non-linearity of neural networks, the local structure in the output embedding space may not be homogeneous as assumed. Therefore, representing each sample simply with its embedding while ignoring its individual neighborhood structure would have limitations in Embedding-Based Retrieval (EBR). By exploiting the heterogeneity of local structures in the embedding space, we propose a Neighborhood-Adaptive Structure Augmented metric learning framework (NASA), where the neighborhood structure is realized as astructure embedding, and learned along with the sample embedding in a self-supervised manner. In this way, without any modifications, most indexing techniques can be used to support large-scale EBR with NASA embeddings. Experiments on six standard benchmarks with two kinds of embeddings, i.e., binary embeddings and real-valued embeddings, show that our method significantly improves and outperforms the state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_stereoneuralverniercaliper": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stereo Neural Vernier Caliper",
    "authors": [
      "Shichao Li",
      "Zechun Liu",
      "Zhiqiang Shen",
      "Kwang-Ting Cheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20026",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20026/19785",
    "published": "2022-02",
    "summary": "We propose a new object-centric framework for learning-based stereo 3D object detection. Previous studies build scene-centric representations that do not consider the significant variation among outdoor instances and thus lack the flexibility and functionalities that an instance-level model can offer. We build such an instance-level model by formulating and tackling a local update problem, i.e., how to predict a refined update given an initial 3D cuboid guess. We demonstrate how solving this problem can complement scene-centric approaches in (i) building a coarse-to-fine multi-resolution system, (ii) performing model-agnostic object location refinement, and (iii) conducting stereo 3D tracking-by-detection. Extensive experiments demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on the KITTI benchmark. Code and pre-trained models are available at https://github.com/Nicholasli1995/SNVC.",
    "code_link": "https://github.com/Nicholasli1995/SNVC"
  },
  "aaai2022_main_editvaeunsupervisedparts-awarecontrollable3dpointcloudshapegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "EditVAE: Unsupervised Parts-Aware Controllable 3D Point Cloud Shape Generation",
    "authors": [
      "Shidi Li",
      "Miaomiao Liu",
      "Christian Walder"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20027",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20027/19786",
    "published": "2022-02",
    "summary": "This paper tackles the problem of parts-aware point cloud generation. Unlike existing works which require the point cloud to be segmented into parts a priori, our parts-aware editing and generation are performed in an unsupervised manner. We achieve this with a simple modification of the Variational Auto-Encoder which yields a joint model of the point cloud itself along with a schematic representation of it as a combination of shape primitives. In particular, we introduce a latent representation of the point cloud which can be decomposed into a disentangled representation for each part of the shape. These parts are in turn disentangled into both a shape primitive and a point cloud representation, along with a standardising transformation to a canonical coordinate system. The dependencies between our standardising transformations preserve the spatial dependencies between the parts in a manner that allows meaningful parts-aware point cloud generation and shape editing. In addition to the flexibility afforded by our disentangled representation, the inductive bias introduced by our joint modeling approach yields state-of-the-art experimental results on the ShapeNet dataset.",
    "code_link": ""
  },
  "aaai2022_main_self-trainingmulti-sequencelearningwithtransformerforweaklysupervisedvideoanomalydetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection",
    "authors": [
      "Shuo Li",
      "Fang Liu",
      "Licheng Jiao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20028",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20028/19787",
    "published": "2022-02",
    "summary": "Weakly supervised Video Anomaly Detection (VAD) using Multi-Instance Learning (MIL) is usually based on the fact that the anomaly score of an abnormal snippet is higher than that of a normal snippet. In the beginning of training, due to the limited accuracy of the model, it is easy to select the wrong abnormal snippet. In order to reduce the probability of selection errors, we first propose a Multi-Sequence Learning (MSL) method and a hinge-based MSL ranking loss that uses a sequence composed of multiple snippets as an optimization unit. We then design a Transformer-based MSL network to learn both video-level anomaly probability and snippet-level anomaly scores. In the inference stage, we propose to use the video-level anomaly probability to suppress the fluctuation of snippet-level anomaly scores. Finally, since VAD needs to predict the snippet-level anomaly scores, by gradually reducing the length of selected sequence, we propose a self-training strategy to gradually refine the anomaly scores. Experimental results show that our method achieves significant improvements on ShanghaiTech, UCF-Crime, and XD-Violence.",
    "code_link": ""
  },
  "aaai2022_main_ta2ntwo-stageactionalignmentnetworkforfew-shotactionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TA2N: Two-Stage Action Alignment Network for Few-Shot Action Recognition",
    "authors": [
      "Shuyuan Li",
      "Huabin Liu",
      "Rui Qian",
      "Yuxi Li",
      "John See",
      "Mengjuan Fei",
      "Xiaoyuan Yu",
      "Weiyao Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20029",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20029/19788",
    "published": "2022-02",
    "summary": "Few-shot action recognition aims to recognize novel action classes (query) using just a few samples (support). The majority of current approaches follow the metric learning paradigm, which learns to compare the similarity between videos. Recently, it has been observed that directly measuring this similarity is not ideal since different action instances may show distinctive temporal distribution, resulting in severe misalignment issues across query and support videos. In this paper, we arrest this problem from two distinct aspects -- action duration misalignment and action evolution misalignment. We address them sequentially through a Two-stage Action Alignment Network (TA2N). The first stage locates the action by learning a temporal affine transform, which warps each video feature to its action duration while dismissing the action-irrelevant feature (e.g. background). Next, the second stage coordinates query feature to match the spatial-temporal action evolution of support by performing temporally rearrange and spatially offset prediction. Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition.",
    "code_link": ""
  },
  "aaai2022_main_best-buddygansforhighlydetailedimagesuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Best-Buddy GANs for Highly Detailed Image Super-resolution",
    "authors": [
      "Wenbo Li",
      "Kun Zhou",
      "Lu Qi",
      "Liying Lu",
      "Jiangbo Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20030",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20030/19789",
    "published": "2022-02",
    "summary": "We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the ill-posed SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the rigid one-to-one constraint, we allow the estimated patches to dynamically seek trustworthy surrogates of supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research.",
    "code_link": ""
  },
  "aaai2022_main_scancrossdomainobjectdetectionwithsemanticconditionedadaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SCAN: Cross Domain Object Detection with Semantic Conditioned Adaptation",
    "authors": [
      "Wuyang Li",
      "Xinyu Liu",
      "Xiwen Yao",
      "Yixuan Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20031",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20031/19790",
    "published": "2022-02",
    "summary": "The domain gap severely limits the transferability and scalability of object detectors trained in a specific domain when applied to a novel one. Most existing works bridge the domain gap by minimizing the domain discrepancy in the category space and aligning category-agnostic global features. Though great success, these methods model domain discrepancy with prototypes within a batch, yielding a biased estimation of domain-level distribution. Besides, the category-agnostic alignment leads to the disagreement of class-specific distributions in the two domains, further causing inevitable classification errors. To overcome these two challenges, we propose a novel Semantic Conditioned AdaptatioN (SCAN) framework such that well-modeled unbiased semantics can support semantic conditioned adaptation for precise domain adaptive object detection. Specifically, class-specific semantics crossing different images in the source domain are graphically aggregated as the input to learn an unbiased semantic paradigm incrementally. The paradigm is then sent to a lightweight manifestation module to obtain conditional kernels to serve as the role of extracting semantics from the target domain for better adaptation. Subsequently, conditional kernels are integrated into global alignment to support the class-specific adaptation in a well-designed Conditional Kernel guided Alignment (CKA) module. Meanwhile, rich knowledge of the unbiased paradigm is transferred to the target domain with a novel Graph-based Semantic Transfer (GST) mechanism, yielding the adaptation in the category-based feature space. Comprehensive experiments conducted on three adaptation benchmarks demonstrate that SCAN outperforms existing works by a large margin.",
    "code_link": "https://github.com/CityU-AIM-Group/SCAN"
  },
  "aaai2022_main_hybridinstance-awaretemporalfusionforonlinevideoinstancesegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hybrid Instance-Aware Temporal Fusion for Online Video Instance Segmentation",
    "authors": [
      "Xiang Li",
      "Jinglu Wang",
      "Xiao Li",
      "Yan Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20032",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20032/19791",
    "published": "2022-02",
    "summary": "Recently, transformer-based image segmentation methods have achieved notable success against previous solutions. While for video domains, how to effectively model temporal context with the attention of object instances across frames remains an open problem. In this paper, we propose an online video instance segmentation framework with a novel instance-aware temporal fusion method. We first leverage the representation, \\ie, a latent code in the global context (instance code) and CNN feature maps to represent instance- and pixel-level features. Based on this representation, we introduce a cropping-free temporal fusion approach to model the temporal consistency between video frames. Specifically, we encode global instance-specific information in the instance code and build up inter-frame contextual fusion with hybrid attentions between the instance codes and CNN feature maps. Inter-frame consistency between the instance codes is further enforced with order constraints. By leveraging the learned hybrid temporal consistency, we are able to directly retrieve and maintain instance identities across frames, eliminating the complicated frame-wise instance matching in prior methods. Extensive experiments have been conducted on popular VIS datasets, i.e. Youtube-VIS-19/21. Our model achieves the best performance among all online VIS methods. Notably, our model also eclipses all offline methods when using the ResNet-50 backbone.",
    "code_link": ""
  },
  "aaai2022_main_closetheloopaunifiedbottom-upandtop-downparadigmforjointimagederainingandsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Close the Loop: A Unified Bottom-Up and Top-Down Paradigm for Joint Image Deraining and Segmentation",
    "authors": [
      "Yi Li",
      "Yi Chang",
      "Changfeng Yu",
      "Luxin Yan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20033",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20033/19792",
    "published": "2022-02",
    "summary": "In this work, we focus on a very practical problem: image segmentation under rain conditions. Image deraining is a classic low-level restoration task, while image segmentation is a typical high-level understanding task. Most of the existing methods intuitively employ the bottom-up paradigm by taking deraining as a preprocessing step for subsequent segmentation. However, our statistical analysis indicates that not only deraining would benefit segmentation (bottom-up), but also segmentation would further improve deraining performance (top-down) in turn. This motivates us to solve the rainy image segmentation task within a novel top-down and bottom-up unified paradigm, in which two sub-tasks are alternatively performed and collaborated with each other. Specifically, the bottom-up procedure yields both clearer images and rain-robust features from both image and feature domains, so as to ease the segmentation ambiguity caused by rain streaks. The top-down procedure adopts semantics to adaptively guide the restoration for different contents via a novel multi-path semantic attentive module (SAM). Thus the deraining and segmentation could boost the performance of each other cooperatively and progressively. Extensive experiments and ablations demonstrate that the proposed method outperforms the state-of-the-art on rainy image segmentation.",
    "code_link": ""
  },
  "aaai2022_main_uncertaintyestimationviaresponsescalingforpseudo-masknoisemitigationinweakly-supervisedsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Uncertainty Estimation via Response Scaling for Pseudo-Mask Noise Mitigation in Weakly-Supervised Semantic Segmentation",
    "authors": [
      "Yi Li",
      "Yiqun Duan",
      "Zhanghui Kuang",
      "Yimin Chen",
      "Wayne Zhang",
      "Xiaomeng Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20034",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20034/19793",
    "published": "2022-02",
    "summary": "Weakly-Supervised Semantic Segmentation (WSSS) segments objects without heavy burden of dense annotation. While as a price, generated pseudo-masks exist obvious noisy pixels, which result in sub-optimal segmentation models trained over these pseudo-masks. But rare studies notice or work on this problem, even these noisy pixels are inevitable after their improvements on pseudo-mask. So we try to improve WSSS in the aspect of noise mitigation. And we observe that many noisy pixels are of high confidences, especially when the response range is too wide or narrow, presenting an uncertain status. Thus, in this paper, we simulate noisy variations of response by scaling the prediction map in multiple times for uncertainty estimation. The uncertainty is then used to weight the segmentation loss to mitigate noisy supervision signals. We call this method URN, abbreviated from Uncertainty estimation via Response scaling for Noise mitigation. Experiments validate the benefits of URN, and our method achieves state-of-the-art results at 71.2% and 41.5% on PASCAL VOC 2012 and MS COCO 2014 respectively, without extra models like saliency detection. Code is available at https://github.com/XMed-Lab/URN.",
    "code_link": "https://github.com/XMed-Lab/URN"
  },
  "aaai2022_main_multi-modalperceptionattentionnetworkwithself-supervisedlearningforaudio-visualspeakertracking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking",
    "authors": [
      "Yidi Li",
      "Hong Liu",
      "Hao Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20035",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20035/19794",
    "published": "2022-02",
    "summary": "Multi-modal fusion is proven to be an effective method to improve the accuracy and robustness of speaker tracking, especially in complex scenarios. However, how to combine the heterogeneous information and exploit the complementarity of multi-modal signals remains a challenging issue. In this paper, we propose a novel Multi-modal Perception Tracker (MPT) for speaker tracking using both audio and visual modalities. Specifically, a novel acoustic map based on spatial-temporal Global Coherence Field (stGCF) is first constructed for heterogeneous signal fusion, which employs a camera model to map audio cues to the localization space consistent with the visual cues. Then a multi-modal perception attention network is introduced to derive the perception weights that measure the reliability and effectiveness of intermittent audio and video streams disturbed by noise. Moreover, a unique cross-modal self-supervised learning method is presented to model the confidence of audio and visual observations by leveraging the complementarity and consistency between different modalities. Experimental results show that the proposed MPT achieves 98.6% and 78.3% tracking accuracy on the standard and occluded datasets, respectively, which demonstrates its robustness under adverse conditions and outperforms the current state-of-the-art methods.",
    "code_link": "https://github.com/liyidi/MPT"
  },
  "aaai2022_main_defendingagainstmodelstealingviaverifyingembeddedexternalfeatures": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Defending against Model Stealing via Verifying Embedded External Features",
    "authors": [
      "Yiming Li",
      "Linghui Zhu",
      "Xiaojun Jia",
      "Yong Jiang",
      "Shu-Tao Xia",
      "Xiaochun Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20036",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20036/19795",
    "published": "2022-02",
    "summary": "Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property. Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by tempering a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. We examine our method on both CIFAR-10 and ImageNet datasets. Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process. The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification).",
    "code_link": "https://github.com/zlh-thu/StealingVerification"
  },
  "aaai2022_main_towardsaneffectiveorthogonaldictionaryconvolutionstrategy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards an Effective Orthogonal Dictionary Convolution Strategy",
    "authors": [
      "Yishi Li",
      "Kunran Xu",
      "Rui Lai",
      "Lin Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20037",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20037/19796",
    "published": "2022-02",
    "summary": "Orthogonality regularization has proven effective in improving the precision, convergence speed and the training stability of CNNs. Here, we propose a novel Orthogonal Dictionary Convolution Strategy (ODCS) on CNNs to improve orthogonality effect by optimizing the network architecture and changing the regularized object. Specifically, we remove the nonlinear layer in typical convolution block \u201cConv(BN) + Nonlinear + Pointwise Conv(BN)\u201d, and only impose orthogonal regularization on the front Conv. The structure, \u201cConv(BN) + Pointwise Conv(BN)\u201d, is then equivalent to a pair of dictionary and encoding, defined in sparse dictionary learning. Thanks to the exact and efficient representation of signal with dictionaries in low-dimensional projections, our strategy could reduce the superfluous information in dictionary Conv kernels. Meanwhile, the proposed strategy relieves the too strict orthogonality regularization in training, which makes hyper-parameters tuning of model to be more flexible. In addition, our ODCS can modify the state-of-the-art models easily without any extra consumption in inference phase. We evaluate it on a variety of CNNs in small-scale (CIFAR), large-scale (ImageNet) and fine-grained (CUB-200-2011) image classification tasks, respectively. The experimental results show that our method achieve a stable and superior improvement.",
    "code_link": ""
  },
  "aaai2022_main_elmaenergy-basedlearningformulti-agentactivityforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ELMA: Energy-Based Learning for Multi-Agent Activity Forecasting",
    "authors": [
      "Yuke Li",
      "Pin Wang",
      "Lixiong Chen",
      "Zheng Wang",
      "Ching-Yao Chan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20038",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20038/19797",
    "published": "2022-02",
    "summary": "This paper describes an energy-based learning method that predicts the activities of multiple agents simultaneously. It aims to forecast both upcoming actions and paths of all agents in a scene based on their past activities, which can be jointly formulated by a probabilistic model over time. Learning this model is challenging because: 1) it has a large number of time-dependent variables that must scale with the forecast horizon and the number of agents; 2) distribution functions have to contain multiple modes in order to capture the spatio-temporal complexities of each agent's activities. To address these challenges, we put forth a novel Energy-based Learning approach for Multi-Agent activity forecasting (ELMA) to estimate this complex model via maximum log-likelihood estimation. Specifically, by sampling from a sequence of factorized marginalized multi-model distributions, ELMA generates most possible future actions efficiently. Moreover, by graph-based representations, ELMA also explicitly resolves the spatio-temporal dependencies of all agents' activities in a single pass. Our experiments on two large-scale datasets prove that ELMA outperforms recent leading studies by an obvious margin.",
    "code_link": ""
  },
  "aaai2022_main_equalbitsenforcingequallydistributedbinarynetworkweights": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Equal Bits: Enforcing Equally Distributed Binary Network Weights",
    "authors": [
      "Yunqiang Li",
      "Silvia-Laura Pintea",
      "Jan C van Gemert"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20039",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20039/19798",
    "published": "2022-02",
    "summary": "Binary networks are extremely efficient as they use only two symbols to define the network: {+1, \u22121}. One can make the prior distribution of these symbols a design choice. The recent IR-Net of Qin et al. argues that imposing a Bernoulli distribution with equal priors (equal bit ratios) over the binary weights leads to maximum entropy and thus minimizes information loss. However, prior work cannot precisely control the binary weight distribution during training, and therefore cannot guarantee maximum entropy. Here, we show that quantizing using optimal transport can guarantee any bit ratio, including equal ratios. We investigate experimentally that equal bit ratios are indeed preferable and show that our method leads to optimization benefits. We show that our quantization method is effective when compared to state-of-the-art binarization methods, even when using binary weight pruning. Our code is available at https://github.com/liyunqianggyn/Equal-Bits-BNN.",
    "code_link": ""
  },
  "aaai2022_main_simipusimple2dimageand3dpointcloudunsupervisedpre-trainingforspatial-awarevisualrepresentations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-training for Spatial-Aware Visual Representations",
    "authors": [
      "Zhenyu Li",
      "Zehui Chen",
      "Ang Li",
      "Liangji Fang",
      "Qinhong Jiang",
      "Xianming Liu",
      "Junjun Jiang",
      "Bolei Zhou",
      "Hang Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20040",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20040/19799",
    "published": "2022-02",
    "summary": "Pre-training has become a standard paradigm in many computer vision tasks. However, most of the methods are generally designed on the RGB image domain. Due to the discrepancy between the two-dimensional image plane and the three-dimensional space, such pre-trained models fail to perceive spatial information and serve as sub-optimal solutions for 3D-related tasks. To bridge this gap, we aim to learn a spatial-aware visual representation that can describe the three-dimensional space and is more suitable and effective for these tasks. To leverage point clouds, which are much more superior in providing spatial information compared to images, we propose a simple yet effective 2D Image and 3D Point cloud Unsupervised pre-training strategy, called SimIPU. Specifically, we develop a multi-modal contrastive learning framework that consists of an intra-modal spatial perception module to learn a spatial-aware representation from point clouds and an inter-modal feature interaction module to transfer the capability of perceiving spatial information from the point cloud encoder to the image encoder, respectively. Positive pairs for contrastive losses are established by the matching algorithm and the projection matrix. The whole framework is trained in an unsupervised end-to-end fashion. To the best of our knowledge, this is the first study to explore contrastive learning pre-training strategies for outdoor multi-modal datasets, containing paired camera images and LIDAR point clouds.",
    "code_link": "https://github.com/open-mmlab/mmdetection3d"
  },
  "aaai2022_main_improvinghuman-objectinteractiondetectionviaphraselearningandlabelcomposition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Human-Object Interaction Detection via Phrase Learning and Label Composition",
    "authors": [
      "Zhimin Li",
      "Cheng Zou",
      "Yu Zhao",
      "Boxun Li",
      "Sheng Zhong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20041",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20041/19800",
    "published": "2022-02",
    "summary": "Human-Object Interaction (HOI) detection is a fundamental task in high-level human-centric scene understanding. We propose PhraseHOI, containing a HOI branch and a novel phrase branch, to leverage language prior and improve relation expression. Specifically, the phrase branch is supervised by semantic embeddings, whose ground truths are automatically converted from the original HOI annotations without extra human efforts. Meanwhile, a novel label composition method is proposed to deal with the long-tailed problem in HOI, which composites novel phrase labels by semantic neighbors. Further, to optimize the phrase branch, a loss composed of a distilling loss and a balanced triplet loss is proposed. Extensive experiments are conducted to prove the effectiveness of the proposed PhraseHOI, which achieves significant improvement over the baseline and surpasses previous state-of-the-art methods on Full and NonRare on the challenging HICO-DET benchmark.",
    "code_link": ""
  },
  "aaai2022_main_rethinkingtheoptimizationofaverageprecisiononlypenalizingnegativeinstancesbeforepositiveonesisenough": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Rethinking the Optimization of Average Precision: Only Penalizing Negative Instances before Positive Ones Is Enough",
    "authors": [
      "Zhuo Li",
      "Weiqing Min",
      "Jiajun Song",
      "Yaohui Zhu",
      "Liping Kang",
      "Xiaoming Wei",
      "Xiaolin Wei",
      "Shuqiang Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20042",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20042/19801",
    "published": "2022-02",
    "summary": "Optimising the approximation of Average Precision (AP) has been widely studied for image retrieval. Limited by the definition of AP, such methods consider both negative and positive instances ranking before each positive instance. However, we claim that only penalizing negative instances before positive ones is enough, because the loss only comes from these negative instances. To this end, we propose a novel loss, namely Penalizing Negative instances before Positive ones (PNP),which can directly minimize the number of negative instances before each positive one. In addition, AP-based methods adopt a fixed and sub-optimal gradient assignment strategy. Therefore, we systematically investigate different gradient assignment solutions via constructing derivative functions of the loss, resulting in PNP-I with increasing derivative functions and PNP-D with decreasing ones. PNP-I focuses more on the hard positive instances by assigning larger gradients to them and tries to make all relevant instances closer. In contrast,PNP-D pays less attention to such instances and slowly corrects them. For most real-world data, one class usually contains several local clusters. PNP-I blindly gathers these clusters while PNP-D keeps them as they were. Therefore, PNP-D is more superior. Experiments on three standard retrieval datasets show consistent results with the above analysis. Extensive evaluations demonstrate that PNP-D achieves the state-of-the-art performance. Code is available at https://github.com/interestingzhuo/PNPloss",
    "code_link": "https://github.com/interestingzhuo/PNPloss"
  },
  "aaai2022_main_reliabilityexplorationwithself-ensemblelearningfordomainadaptivepersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reliability Exploration with Self-Ensemble Learning for Domain Adaptive Person Re-identification",
    "authors": [
      "Zongyi Li",
      "Yuxuan Shi",
      "Hefei Ling",
      "Jiazhong Chen",
      "Qian Wang",
      "Fengfan Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20043",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20043/19802",
    "published": "2022-02",
    "summary": "Person re-identifcation (Re-ID) based on unsupervised domain adaptation (UDA) aims to transfer the pre-trained model from one labeled source domain to an unlabeled target domain. Existing methods tackle this problem by using clustering methods to generate pseudo labels. However, pseudo labels produced by these techniques may be unstable and noisy, substantially deteriorating models\u2019 performance. In this paper, we propose a Reliability Exploration with Self-ensemble Learning (RESL) framework for domain adaptive person ReID. First, to increase the feature diversity, multiple branches are presented to extract features from different data augmentations. Taking the temporally average model as a mean teacher model, online label refning is conducted by using its dynamic ensemble predictions from different branches as soft labels. Second, to combat the adverse effects of unreliable samples in clusters, sample reliability is estimated by evaluating the consistency of different clusters\u2019 results, followed by selecting reliable instances for training and re-weighting sample contribution within Re-ID losses. A contrastive loss is also utilized with cluster-level memory features which are updated by the mean feature. The experiments demonstrate that our method can signifcantly surpass the state-of-the-art performance on the unsupervised domain adaptive person ReID.",
    "code_link": ""
  },
  "aaai2022_main_deconfoundingphysicaldynamicswithglobalcausalrelationandconfoundertransmissionforcounterfactualprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deconfounding Physical Dynamics with Global Causal Relation and Confounder Transmission for Counterfactual Prediction",
    "authors": [
      "Zongzhao Li",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Zhaoxiang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20044",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20044/19803",
    "published": "2022-02",
    "summary": "Discovering the underneath causal relations is the fundamental ability for reasoning about the surrounding environment and predicting the future states in the physical world. Counterfactual prediction from visual input, which requires simulating future states based on unrealized situations in the past, is a vital component in causal relation tasks. In this paper, we work on the confounders that have effect on the physical dynamics, including masses, friction coefficients, etc., to bridge relations between the intervened variable and the affected variable whose future state may be altered. We propose a neural network framework combining Global Causal Relation Attention (GCRA) and Confounder Transmission Structure (CTS). The GCRA looks for the latent causal relations between different variables and estimates the confounders by capturing both spatial and temporal information. The CTS integrates and transmits the learnt confounders in a residual way, so that the estimated confounders can be encoded into the network as a constraint for object positions when performing counterfactual prediction. Without any access to ground truth information about confounders, our model outperforms the state-of-the-art method on various benchmarks by fully utilizing the constraints of confounders. Extensive experiments demonstrate that our model can generalize to unseen environments and maintain good performance.",
    "code_link": ""
  },
  "aaai2022_main_onemorecheckmaking\u201cfakebackground\u201dbetrackedagain": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "One More Check: Making \u201cFake Background\u201d Be Tracked Again",
    "authors": [
      "Chao Liang",
      "Zhipeng Zhang",
      "Xue Zhou",
      "Bing Li",
      "Weiming Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20045",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20045/19804",
    "published": "2022-02",
    "summary": "The one-shot multi-object tracking, which integrates object detection and ID embedding extraction into a unified network, has achieved groundbreaking results in recent years. However, current one-shot trackers solely rely on single-frame detections to predict candidate bounding boxes, which may be unreliable when facing disastrous visual degradation, e.g., motion blur, occlusions. Once a target bounding box is mistakenly classified as background by the detector, the temporal consistency of its corresponding tracklet will be no longer maintained. In this paper, we set out to restore the bounding boxes misclassified as ``fake background'' by proposing a re-check network. The re-check network innovatively expands the role of ID embedding from data association to motion forecasting by effectively propagating previous tracklets to the current frame with a small overhead. Note that the propagation results are yielded by an independent and efficient embedding search, preventing the model from over-relying on detection results. Eventually, it helps to reload the ``fake background'' and repair the broken tracklets. Building on a strong baseline CSTrack, we construct a new one-shot tracker and achieve favorable gains by 70.7 \u27a1 76.4, 70.6 \u27a1 76.3 MOTA on MOT16 and MOT17, respectively. It also reaches a new state-of-the-art MOTA and IDF1 performance. Code is released at https://github.com/JudasDie/SOTS.",
    "code_link": "https://github.com/JudasDie/SOTS"
  },
  "aaai2022_main_semanticallycontrastivelearningforlow-lightimageenhancement": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semantically Contrastive Learning for Low-Light Image Enhancement",
    "authors": [
      "Dong Liang",
      "Ling Li",
      "Mingqiang Wei",
      "Shuo Yang",
      "Liyan Zhang",
      "Wenhan Yang",
      "Yun Du",
      "Huiyu Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20046",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20046/19805",
    "published": "2022-02",
    "summary": "Low-light image enhancement (LLE) remains challenging due to the unfavorable prevailing low-contrast and weak-visibility problems of single RGB images. In this paper, we respond to the intriguing learning-related question -- if leveraging both accessible unpaired over/underexposed images and high-level semantic guidance, can improve the performance of cutting-edge LLE models? Here, we propose an effective semantically contrastive learning paradigm for LLE (namely SCL-LLE). Beyond the existing LLE wisdom, it casts the image enhancement task as multi-task joint learning, where LLE is converted into three constraints of contrastive learning, semantic brightness consistency, and feature preservation for simultaneously ensuring the exposure, texture, and color consistency. SCL-LLE allows the LLE model to learn from unpaired positives (normal-light)/negatives (over/underexposed), and enables it to interact with the scene semantics to regularize the image enhancement network, yet the interaction of high-level semantic knowledge and the low-level signal prior is seldom investigated in previous methods. Training on readily available open data, extensive experiments demonstrate that our method surpasses the state-of-the-arts LLE models over six independent cross-scenes datasets. Moreover, SCL-LLE's potential to benefit the downstream semantic segmentation under extremely dark conditions is discussed. Source Code: https://github.com/LingLIx/SCL-LLE.",
    "code_link": "https://github.com/LingLIx/SCL-LLE"
  },
  "aaai2022_main_self-supervisedspatiotemporalrepresentationlearningbyexploitingvideocontinuity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Spatiotemporal Representation Learning by Exploiting Video Continuity",
    "authors": [
      "Hanwen Liang",
      "Niamul Quader",
      "Zhixiang Chi",
      "Lizhe Chen",
      "Peng Dai",
      "Juwei Lu",
      "Yang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20047",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20047/19806",
    "published": "2022-02",
    "summary": "Recent self-supervised video representation learning methods have found significant success by exploring essential properties of videos, e.g. speed, temporal order, etc. This work exploits an essential yet under-explored property of videos, the \\textit{video continuity}, to obtain supervision signals for self-supervised representation learning. Specifically, we formulate three novel continuity-related pretext tasks, i.e. continuity justification, discontinuity localization, and missing section approximation, that jointly supervise a shared backbone for video representation learning.This self-supervision approach, termed as Continuity Perception Network (CPNet), solves the three tasks altogether and encourages the backbone network to learn local and long-ranged motion and context representations. It outperforms prior arts on multiple downstream tasks, such as action recognition, video retrieval, and action localization. Additionally, the video continuity can be complementary to other coarse-grained video properties for representation learning, and integrating the proposed pretext task to prior arts can yield much performance gains.",
    "code_link": ""
  },
  "aaai2022_main_inharmoniousregionlocalizationbymagnifyingdomaindiscrepancy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Inharmonious Region Localization by Magnifying Domain Discrepancy",
    "authors": [
      "Jing Liang",
      "Li Niu",
      "Penghao Wu",
      "Fengjun Guo",
      "Teng Long"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20048",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20048/19807",
    "published": "2022-02",
    "summary": "Inharmonious region localization aims to localize the region in a synthetic image which is incompatible with surrounding background. The inharmony issue is mainly attributed to the color and illumination inconsistency produced by image editing techniques. In this work, we tend to transform the input image to another color space to magnify the domain discrepancy between inharmonious region and background, so that the model can identify the inharmonious region more easily. To this end, we present a novel framework consisting of a color mapping module and an inharmonious region localization network, in which the former is equipped with a novel domain discrepancy magnification loss and the latter could be an arbitrary localization network. Extensive experiments on image harmonization dataset show the superiority of our designed framework.",
    "code_link": ""
  },
  "aaai2022_main_distributionawarevotenetfor3dobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Distribution Aware VoteNet for 3D Object Detection",
    "authors": [
      "Junxiong Liang",
      "Pei An",
      "Jie Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20049",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20049/19808",
    "published": "2022-02",
    "summary": "Occlusion is common in the actual 3D scenes, causing the boundary ambiguity of the targeted object. This uncertainty brings difficulty for labeling and learning. Current 3D detectors predict the bounding box directly, regarding it as Dirac delta distribution. However, it does not fully consider such ambiguity. To deal with it, distribution learning is used to efficiently represent the boundary ambiguity. In this paper, we revise the common regression method by predicting the distribution of the 3D box and then present a distribution-aware regression (DAR) module for box refinement and localization quality estimation. It contains scale adaptive (SA) encoder and joint localization quality estimator (JLQE). With the adaptive receptive field, SA encoder refines discriminative features for precise distribution learning. JLQE provides a reliable location score by further leveraging the distribution statistics, correlating with the localization quality of the targeted object. Combining DAR module and the baseline VoteNet, we propose a novel 3D detector called DAVNet. Extensive experiments on both ScanNet V2 and SUN RGB-D datasets demonstrate that the proposed DAVNet achieves significant improvement and outperforms state-of-the-art 3D detectors.",
    "code_link": ""
  },
  "aaai2022_main_contrastiveinstruction-trajectorylearningforvision-languagenavigation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contrastive Instruction-Trajectory Learning for Vision-Language Navigation",
    "authors": [
      "Xiwen Liang",
      "Fengda Zhu",
      "Yi Zhu",
      "Bingqian Lin",
      "Bing Wang",
      "Xiaodan Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20050",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20050/19809",
    "published": "2022-02",
    "summary": "The vision-language navigation (VLN) task requires an agent to reach a target with the guidance of natural language instruction. Previous works learn to navigate step-by-step following an instruction. However, these works may fail to discriminate the similarities and discrepancies across instruction-trajectory pairs and ignore the temporal continuity of sub-instructions. These problems hinder agents from learning distinctive vision-and-language representations, harming the robustness and generalizability of the navigation policy. In this paper, we propose a Contrastive Instruction-Trajectory Learning (CITL) framework that explores invariance across similar data samples and variance across different ones to learn distinctive representations for robust navigation. Specifically, we propose: (1) a coarse-grained contrastive learning objective to enhance vision-and-language representations by contrasting semantics of full trajectory observations and instructions, respectively; (2) a fine-grained contrastive learning objective to perceive instructions by leveraging the temporal information of the sub-instructions; (3) a pairwise sample-reweighting mechanism for contrastive learning to mine hard samples and hence mitigate the influence of data sampling bias in contrastive learning. Our CITL can be easily integrated with VLN backbones to form a new learning paradigm and achieve better generalizability in unseen environments. Extensive experiments show that the model with CITL surpasses the previous state-of-the-art methods on R2R, R4R, and RxR.",
    "code_link": ""
  },
  "aaai2022_main_interventionalmulti-instancelearningwithdeconfoundedinstance-levelprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction",
    "authors": [
      "Tiancheng Lin",
      "Hongteng Xu",
      "Canqian Yang",
      "Yi Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20051",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20051/19810",
    "published": "2022-02",
    "summary": "When applying multi-instance learning (MIL) to make predictions for bags of instances, the prediction accuracy of an instance often depends on not only the instance itself but also its context in the corresponding bag. From the viewpoint of causal inference, such bag contextual prior works as a confounder and may result in model robustness and interpretability issues. Focusing on this problem, we propose a novel interventional multi-instance learning (IMIL) framework to achieve deconfounded instance-level prediction. Unlike traditional likelihood-based strategies, we design an Expectation-Maximization (EM) algorithm based on causal intervention, providing a robust instance selection in the training phase and suppressing the bias caused by the bag contextual prior. Experiments on pathological image analysis demonstrate that our IMIL method substantially reduces false positives and outperforms state-of-the-art MIL methods.",
    "code_link": ""
  },
  "aaai2022_main_acausaldebiasingframeworkforunsupervisedsalientobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Causal Debiasing Framework for Unsupervised Salient Object Detection",
    "authors": [
      "Xiangru Lin",
      "Ziyi Wu",
      "Guanqi Chen",
      "Guanbin Li",
      "Yizhou Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20052",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20052/19811",
    "published": "2022-02",
    "summary": "Unsupervised Salient Object Detection (USOD) is a promising yet challenging task that aims to learn a salient object detection model without any ground-truth labels. Self-supervised learning based methods have achieved remarkable success recently and have become the dominant approach in USOD. However, we observed that two distribution biases of salient objects limit further performance improvement of the USOD methods, namely, contrast distribution bias and spatial distribution bias. Concretely, contrast distribution bias is essentially a confounder that makes images with similar high-level semantic contrast and/or low-level visual appearance contrast spuriously dependent, thus forming data-rich contrast clusters and leading the training process biased towards the data-rich contrast clusters in the data. Spatial distribution bias means that the position distribution of all salient objects in a dataset is concentrated on the center of the image plane, which could be harmful to off-center objects prediction. This paper proposes a causal based debiasing framework to disentangle the model from the impact of such biases. Specifically, we use causal intervention to perform de-confounded model training to minimize the contrast distribution bias and propose an image-level weighting strategy that softly weights each image's importance according to the spatial distribution bias map. Extensive experiments on 6 benchmark datasets show that our method significantly outperforms previous unsupervised state-of-the-art methods and even surpasses some of the supervised methods, demonstrating our debiasing framework's effectiveness.",
    "code_link": ""
  },
  "aaai2022_main_acausalinferencelookatunsupervisedvideoanomalydetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Causal Inference Look at Unsupervised Video Anomaly Detection",
    "authors": [
      "Xiangru Lin",
      "Yuyang Chen",
      "Guanbin Li",
      "Yizhou Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20053",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20053/19812",
    "published": "2022-02",
    "summary": "Unsupervised video anomaly detection, a task that requires no labeled normal/abnormal training data in any form, is challenging yet of great importance to both industrial applications and academic research. Existing methods typically follow an iterative pseudo label generation process. However, they lack a principled analysis of the impact of such pseudo label generation on training. Furthermore, the long-range temporal dependencies also has been overlooked, which is unreasonable since the definition of an abnormal event depends on the long-range temporal context. To this end, first, we propose a causal graph to analyze the confounding effect of the pseudo label generation process. Then, we introduce a simple yet effective causal inference based framework to disentangle the noisy pseudo label's impact. Finally, we perform counterfactual based model ensemble that blends long-range temporal context with local image context in inference to make final anomaly detection. Extensive experiments on six standard benchmark datasets show that our proposed method significantly outperforms previous state-of-the-art methods, demonstrating our framework's effectiveness.",
    "code_link": ""
  },
  "aaai2022_main_unpairedmulti-domainstaintransferforkidneyhistopathologicalimages": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unpaired Multi-Domain Stain Transfer for Kidney Histopathological Images",
    "authors": [
      "Yiyang Lin",
      "Bowei Zeng",
      "Yifeng Wang",
      "Yang Chen",
      "Zijie Fang",
      "Jian Zhang",
      "Xiangyang Ji",
      "Haoqian Wang",
      "Yongbing Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20054",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20054/19813",
    "published": "2022-02",
    "summary": "As an essential step in the pathological diagnosis, histochemical staining can show specific tissue structure information and, consequently, assist pathologists in making accurate diagnoses. Clinical kidney histopathological analyses usually employ more than one type of staining: H&E, MAS, PAS, PASM, etc. However, due to the interference of colors among multiple stains, it is not easy to perform multiple staining simultaneously on one biological tissue. To address this problem, we propose a network based on unpaired training data to virtually generate multiple types of staining from one staining. Our method can preserve the content of input images while transferring them to multiple target styles accurately. To efficiently control the direction of stain transfer, we propose a style guided normalization (SGN). Furthermore, a multiple style encoding (MSE) is devised to represent the relationship among different staining styles dynamically. An improved one-hot label is also proposed to enhance the generalization ability and extendibility of our method. Vast experiments have demonstrated that our model can achieve superior performance on a tiny dataset. The results exhibit not only good performance but also great visualization and interpretability. Especially, our method also achieves satisfactory results over cross-tissue, cross-staining as well as cross-task. We believe that our method will significantly influence clinical stain transfer and reduce the workload greatly for pathologists. Our code and Supplementary materials are available at https://github.com/linyiyang98/UMDST.",
    "code_link": "https://github.com/linyiyang98/UMDST"
  },
  "aaai2022_main_dynamicspatialpropagationnetworkfordepthcompletion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dynamic Spatial Propagation Network for Depth Completion",
    "authors": [
      "Yuankai Lin",
      "Tao Cheng",
      "Qi Zhong",
      "Wending Zhou",
      "Hua Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20055",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20055/19814",
    "published": "2022-02",
    "summary": "Image-guided depth completion aims to generate dense depth maps with sparse depth measurements and corresponding RGB images. Currently, spatial propagation networks (SPNs) are the most popular affinity-based methods in depth completion, but they still suffer from the representation limitation of the fixed affinity and the over smoothing during iterations. Our solution is to estimate independent affinity matrices in each SPN iteration, but it is over-parameterized and heavy calculation.This paper introduces an efficient model that learns the affinity among neighboring pixels with an attention-based, dynamic approach. Specifically, the Dynamic Spatial Propagation Network (DySPN) we proposed makes use of a non-linear propagation model (NLPM). It decouples the neighborhood into parts regarding to different distances and recursively generates independent attention maps to refine these parts into adaptive affinity matrices. Furthermore, we adopt a diffusion suppression (DS) operation so that the model converges at an early stage to prevent over-smoothing of dense depth. Finally, in order to decrease the computational cost required, we also introduce three variations that reduce the amount of neighbors and attentions needed while still retaining similar accuracy. In practice, our method requires less iteration to match the performance of other SPNs and yields better results overall. DySPN outperforms other state-of-the-art (SoTA) methods on KITTI Depth Completion (DC) evaluation by the time of submission and is able to yield SoTA performance in NYU Depth v2 dataset as well.",
    "code_link": ""
  },
  "aaai2022_main_localsimilaritypatternandcostself-reassemblingfordeepstereomatchingnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Local Similarity Pattern and Cost Self-Reassembling for Deep Stereo Matching Networks",
    "authors": [
      "Biyang Liu",
      "Huimin Yu",
      "Yangqi Long"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20056",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20056/19815",
    "published": "2022-02",
    "summary": "Although convolutional neural network based stereo matching architectures have made impressive achievements, there are still some limitations: 1) Convolutional Feature (CF) tends to capture appearance information, which is inadequate for accurate matching. 2) Due to the static filters, current convolution based disparity refinement modules often produce over-smooth results. In this paper, we present two schemes to address these issues, where some traditional wisdoms are integrated. Firstly, we introduce a pairwise feature for deep stereo matching networks, named LSP (Local Similarity Pattern). Through explicitly revealing the neighbor relationships, LSP contains rich structural information, which can be leveraged to aid CF for more discriminative feature description. Secondly, we design a dynamic self-reassembling refinement strategy and apply it to the cost distribution and the disparity map respectively. The former could be equipped with the unimodal distribution constraint to alleviate the over-smoothing problem, and the latter is more practical. The effectiveness of the proposed methods is demonstrated via incorporating them into two well-known basic architectures, GwcNet and GANet-deep. Experimental results on the SceneFlow and KITTI benchmarks show that our modules significantly improve the performance of the model. Code is available at https://github.com/SpadeLiu/Lac-GwcNet.",
    "code_link": "https://github.com/SpadeLiu/Lac-GwcNet"
  },
  "aaai2022_main_fedfrjointoptimizationfederatedframeworkforgenericandpersonalizedfacerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition",
    "authors": [
      "Chih-Ting Liu",
      "Chien-Yi Wang",
      "Shao-Yi Chien",
      "Shang-Hong Lai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20057",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20057/19816",
    "published": "2022-02",
    "summary": "Current state-of-the-art deep learning based face recognition (FR) models require a large number of face identities for central training. However, due to the growing privacy awareness, it is prohibited to access the face images on user devices to continually improve face recognition models. Federated Learning (FL) is a technique to address the privacy issue, which can collaboratively optimize the model without sharing the data between clients. In this work, we propose a FL based framework called FedFR to improve the generic face representation in a privacy-aware manner. Besides, the framework jointly optimizes personalized models for the corresponding clients via the proposed Decoupled Feature Customization module. The client-specific personalized model can serve the need of optimized face recognition experience for registered identities at the local device. To the best of our knowledge, we are the first to explore the personalized face recognition in FL setup. The proposed framework is validated to be superior to previous approaches on several generic and personalized face recognition benchmarks with diverse FL scenarios. The source codes and our proposed personalized FR benchmark under FL setup are available at https://github.com/jackie840129/FedFR.",
    "code_link": "https://github.com/jackie840129/FedFR"
  },
  "aaai2022_main_memory-guidedsemanticlearningnetworkfortemporalsentencegrounding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Memory-Guided Semantic Learning Network for Temporal Sentence Grounding",
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Xing Di",
      "Yu Cheng",
      "Zichuan Xu",
      "Pan Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20058",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20058/19817",
    "published": "2022-02",
    "summary": "Temporal sentence grounding (TSG) is crucial and fundamental for video understanding. Although existing methods train well-designed deep networks with large amount of data, we find that they can easily forget the rarely appeared cases during training due to the off-balance data distribution, which influences the model generalization and leads to unsatisfactory performance. To tackle this issue, we propose a memory-augmented network, called Memory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes the rarely appeared content in TSG task. Specifically, our proposed model consists of three main parts: cross-modal interaction module, memory augmentation module, and heterogeneous attention module. We first align the given video-query pair by a cross-modal graph convolutional network, and then utilize memory module to record the cross-modal shared semantic features in the domain-specific persistent memory. During training, the memory slots are dynamically associated with both common and rare cases, alleviating the forgetting issue. In testing, the rare cases can thus be enhanced by retrieving the stored memories, leading to better generalization. At last, the heterogeneous attention module is utilized to integrate the enhanced multi-modal features in both video and query domains. Experimental results on three benchmarks show the superiority of our method on both effectiveness and efficiency, which substantially improves the accuracy not only on the entire dataset but also on the rare cases.",
    "code_link": ""
  },
  "aaai2022_main_exploringmotionandappearanceinformationfortemporalsentencegrounding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploring Motion and Appearance Information for Temporal Sentence Grounding",
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Pan Zhou",
      "Yang Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20059",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20059/19818",
    "published": "2022-02",
    "summary": "This paper addresses temporal sentence grounding. Previous works typically solve this task by learning frame-level video features and align them with the textual information. A major limitation of these works is that they fail to distinguish ambiguous video frames with subtle appearance differences due to frame-level feature extraction. Recently, a few methods adopt Faster R-CNN to extract detailed object features in each frame to differentiate the fine-grained appearance similarities. However, the object-level features extracted by Faster R-CNN suffer from missing motion analysis since the object detection model lacks temporal modeling. To solve this issue, we propose a novel Motion-Appearance Reasoning Network (MARN), which incorporates both motion-aware and appearance-aware object features to better reason object relations for modeling the activity among successive frames. Specifically, we first introduce two individual video encoders to embed the video into corresponding motion-oriented and appearance-aspect object representations. Then, we develop separate motion and appearance branches to learn motion-guided and appearance-guided object relations, respectively. At last, both motion and appearance information from two branches are associated to generate more representative features for final grounding. Extensive experiments on two challenging datasets (Charades-STA and TACoS) show that our proposed MARN significantly outperforms previous state-of-the-art methods by a large margin.",
    "code_link": ""
  },
  "aaai2022_main_unsupervisedtemporalvideogroundingwithdeepsemanticclustering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Temporal Video Grounding with Deep Semantic Clustering",
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Yinzhen Wang",
      "Xing Di",
      "Kai Zou",
      "Yu Cheng",
      "Zichuan\n      Xu",
      "Pan Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20060",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20060/19819",
    "published": "2022-02",
    "summary": "Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive to collect in real-world scenarios. In this paper, we explore whether a video grounding model can be learned without any paired annotations. To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting. Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding. Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set. Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module. Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results. To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets. The results demonstrate that our DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches.",
    "code_link": ""
  },
  "aaai2022_main_spikeconverteranefficientconversionframeworkzippingthegapbetweenartificialneuralnetworksandspikingneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SpikeConverter: An Efficient Conversion Framework Zipping the Gap between Artificial Neural Networks and Spiking Neural Networks",
    "authors": [
      "Fangxin Liu",
      "Wenbo Zhao",
      "Yongbiao Chen",
      "Zongwu Wang",
      "Li Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20061",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20061/19820",
    "published": "2022-02",
    "summary": "Spiking Neural Networks (SNNs) have recently attracted enormous research interest since their event-driven and brain-inspired structure enables low-power computation. In image recognition tasks, the best results are achieved by SNN so far utilizing ANN-SNN conversion methods that replace activation functions in artificial neural networks~(ANNs) with integrate-and-fire neurons. Compared to source ANNs, converted SNNs usually suffer from accuracy loss and require a considerable number of time steps to achieve competitive accuracy. We find that the performance degradation of converted SNN stems from the fact that the information capacity of spike trains in transferred networks is smaller than that of activation values in source ANN, resulting in less information being passed during SNN inference.To better correlate ANN and SNN for better performance, we propose a conversion framework to mitigate the gap between the activation value of source ANN and the generated spike train of target SNN. The conversion framework originates from exploring an identical relation in the conversion and exploits temporal separation scheme and novel neuron model for the relation to hold. We demonstrate almost lossless ANN-SNN conversion using SpikeConverter for VGG-16, ResNet-20/34, and MobileNet-v2 SNNs on challenging datasets including CIFAR-10, CIFAR-100, and ImageNet. Our results also show that SpikeConverter achieves the abovementioned accuracy across different network architectures and datasets using 32X - 512X fewer inference time-steps than state-of-the-art ANN-SNN conversion methods.",
    "code_link": ""
  },
  "aaai2022_main_perceivingstroke-semanticcontexthierarchicalcontrastivelearningforrobustscenetextrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition",
    "authors": [
      "Hao Liu",
      "Bin Wang",
      "Zhimin Bao",
      "Mobai Xue",
      "Sheng Kang",
      "Deqiang Jiang",
      "Yinsong Liu",
      "Bo Ren"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20062",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20062/19821",
    "published": "2022-02",
    "summary": "We introduce Perceiving Stroke-Semantic Context (PerSec), a new approach to self-supervised representation learning tailored for Scene Text Recognition (STR) task. Considering scene text images carry both visual and semantic properties, we equip our PerSec with dual context perceivers which can contrast and learn latent representations from low-level stroke and high-level semantic contextual spaces simultaneously via hierarchical contrastive learning on unlabeled text image data. Experiments in un- and semi-supervised learning settings on STR benchmarks demonstrate our proposed framework can yield a more robust representation for both CTC-based and attention-based decoders than other contrastive learning methods. To fully investigate the potential of our method, we also collect a dataset of 100 million unlabeled text images, named UTI-100M, covering 5 scenes and 4 languages. By leveraging hundred-million-level unlabeled data, our PerSec shows significant performance improvement when fine-tuning the learned representation on the labeled data. Furthermore, we observe that the representation learned by PerSec presents great generalization, especially under few labeled data scenes.",
    "code_link": ""
  },
  "aaai2022_main_anchorfaceboostingtar@farforpracticalfacerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AnchorFace: Boosting TAR@FAR for Practical Face Recognition",
    "authors": [
      "Jiaheng Liu",
      "Haoyu Qin",
      "Yichao Wu",
      "Ding Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20063",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20063/19822",
    "published": "2022-02",
    "summary": "Within the field of face recognition (FR), it is widely accepted that the key objective is to optimize the entire feature space in the training process and acquire robust feature representations. However, most real-world FR systems tend to operate at a pre-defined False Accept Rate (FAR), and the corresponding True Accept Rate (TAR) represents the performance of the FR systems, which indicates that the optimization on the pre-defined FAR is more meaningful and important in the practical evaluation process. In this paper, we call the predefined FAR as Anchor FAR, and we argue that the existing FR loss functions cannot guarantee the optimal TAR under the Anchor FAR, which impedes further improvements of FR systems. To this end, we propose AnchorFace to bridge the aforementioned gap between the training and practical evaluation process for FR. Given the Anchor FAR, AnchorFace can boost the performance of FR systems by directly optimizing the non-differentiable FR evaluation metrics. Specifically, in AnchorFace, we first calculate the similarities of the positive and negative pairs based on both the features of the current batch and the stored features in the maintained online-updating set. Then, we generate the differentiable TAR loss and FAR loss using a soften strategy. Our AnchorFace can be readily integrated into most existing FR loss functions, and extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of AnchorFace.",
    "code_link": ""
  },
  "aaai2022_main_memory-basedjitterimprovingvisualrecognitiononlong-taileddatawithdiversityinmemory": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Memory-Based Jitter: Improving Visual Recognition on Long-Tailed Data with Diversity in Memory",
    "authors": [
      "Jialun Liu",
      "Wenhui Li",
      "Yifan Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20064",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20064/19823",
    "published": "2022-02",
    "summary": "This paper considers deep visual recognition on long-tailed data. To make our method general, we tackle two applied scenarios, i.e. , deep classification and deep metric learning. Under the long-tailed data distribution, the most classes (i.e., tail classes) only occupy relatively few samples and are prone to lack of within-class diversity. A radical solution is to augment the tail classes with higher diversity. To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ). We observe that during training, the deep model constantly changes its parameters after every iteration, yielding the phenomenon of weight jitters. Consequentially, given a same image as the input, two historical editions of the model generate two different features in the deeply-embedded space, resulting in feature jitters. Using a memory bank, we collect these (model or feature) jitters across multiple training iterations and get the so-called Memory-based Jitter. The accumulated jitters enhance the within-class diversity for the tail classes and consequentially improves long-tailed visual recognition. With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, i.e., deep image classification and deep metric learning (on long-tailed data). Extensive experiments on five long-tailed classification benchmarks and two deep metric learning benchmarks demonstrate significant improvement. Moreover, the achieved performance are on par with the state of the art on both tasks.",
    "code_link": ""
  },
  "aaai2022_main_debiasedbatchnormalizationviagaussianprocessforgeneralizablepersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-identification",
    "authors": [
      "Jiawei Liu",
      "Zhipeng Huang",
      "Liang Li",
      "Kecheng Zheng",
      "Zheng-Jun Zha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20065",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20065/19824",
    "published": "2022-02",
    "summary": "Generalizable person re-identification aims to learn a model with only several labeled source domains that can perform well on unseen domains. Without access to the unseen domain, the feature statistics of the batch normalization (BN) layer learned from a limited number of source domains is doubtlessly biased for unseen domain. This would mislead the feature representation learning for unseen domain and deteriorate the generalizaiton ability of the model. In this paper, we propose a novel Debiased Batch Normalization via Gaussian Process approach (GDNorm) for generalizable person re-identification, which models the feature statistic estimation from BN layers as a dynamically self-refining Gaussian process to alleviate the bias to unseen domain for improving the generalization. Specifically, we establish a lightweight model with multiple set of domain-specific BN layers to capture the discriminability of individual source domain, and learn the corresponding parameters of the domain-specific BN layers. These parameters of different source domains are employed to deduce a Gaussian process. We randomly sample several paths from this Gaussian process served as the BN estimations of potential new domains outside of existing source domains, which can further optimize these learned parameters from source domains, and estimate more accurate Gaussian process by them in return, tending to real data distribution. Even without a large number of source domains, GDNorm can still provide debiased BN estimation by using the mean path of the Gaussian process, while maintaining low computational cost during testing. Extensive experiments demonstrate that our GDNorm effectively improves the generalization ability of the model on unseen domain.",
    "code_link": ""
  },
  "aaai2022_main_parallelandhigh-fidelitytext-to-lipgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Parallel and High-Fidelity Text-to-Lip Generation",
    "authors": [
      "Jinglin Liu",
      "Zhiying Zhu",
      "Yi Ren",
      "Wencan Huang",
      "Baoxing Huai",
      "Nicholas\n      Yuan",
      "Zhou Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20066",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20066/19825",
    "published": "2022-02",
    "summary": "As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video. Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner. However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation. This encourages the research of parallel T2L generation. In this work, we propose a parallel decoding model for fast and high-fidelity text-to-lip generation (ParaLip). Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner. Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem. Extensive experiments conducted on GRID and TCD-TIMIT datasets demonstrate the superiority of proposed methods.",
    "code_link": ""
  },
  "aaai2022_main_siamtranszero-shotmulti-frameimagerestorationwithpre-trainedsiamesetransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-trained Siamese Transformers",
    "authors": [
      "Lin Liu",
      "Shanxin Yuan",
      "Jianzhuang Liu",
      "Xin Guo",
      "Youliang Yan",
      "Qi Tian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20067",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20067/19826",
    "published": "2022-02",
    "summary": "We propose a novel zero-shot multi-frame image restoration method for removing unwanted obstruction elements (such as rains, snow, and moire patterns) that vary in successive frames. It has three stages: transformer pre-training, zero-shot restoration, and hard patch refinement. Using the pre-trained transformers, our model is able to tell the motion difference between the true image information and the obstructing elements. For zero-shot image restoration, we design a novel model, termed SiamTrans, which is constructed by Siamese transformers, encoders, and decoders. Each transformer has a temporal attention layer and several self-attention layers, to capture both temporal and spatial information of multiple frames. Only self-supervisedly pre-trained on the denoising task, SiamTrans is tested on three different low-level vision tasks (deraining, demoireing, and desnowing). Compared with related methods, SiamTrans achieves the best performances, even outperforming those with supervised learning.",
    "code_link": ""
  },
  "aaai2022_main_single-domaingeneralizationinmedicalimagesegmentationviatest-timeadaptationfromshapedictionary": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Single-Domain Generalization in Medical Image Segmentation via Test-Time Adaptation from Shape Dictionary",
    "authors": [
      "Quande Liu",
      "Cheng Chen",
      "Qi Dou",
      "Pheng-Ann Heng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20068",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20068/19827",
    "published": "2022-02",
    "summary": "Domain generalization typically requires data from multiple source domains for model learning. However, such strong assumption may not always hold in practice, especially in medical field where the data sharing is highly concerned and sometimes prohibitive due to privacy issue. This paper studies the important yet challenging single domain generalization problem, in which a model is learned under the worst-case scenario with only one source domain to directly generalize to different unseen target domains. We present a novel approach to address this problem in medical image segmentation, which extracts and integrates the semantic shape prior information of segmentation that are invariant across domains and can be well-captured even from single domain data to facilitate segmentation under distribution shifts. Besides, a test-time adaptation strategy with dual-consistency regularization is further devised to promote dynamic incorporation of these shape priors under each unseen domain to improve model generalizability. Extensive experiments on two medical image segmentation tasks demonstrate the consistent improvements of our method across various unseen domains, as well as its superiority over state-of-the-art approaches in addressing domain generalization under the worst-case scenario.",
    "code_link": ""
  },
  "aaai2022_main_learningtopredict3dlaneshapeandcameraposefromasingleimageviageometryconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints",
    "authors": [
      "Ruijin Liu",
      "Dapeng Chen",
      "Tie Liu",
      "Zhiliang Xiong",
      "Zejian Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20069",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20069/19828",
    "published": "2022-02",
    "summary": "Detecting 3D lanes from the camera is a rising problem for autonomous vehicles. In this task, the correct camera pose is the key to generating accurate lanes, which can transform an image from perspective-view to the top-view. With this transformation, we can get rid of the perspective effects so that 3D lanes would look similar and can accurately be fitted by low-order polynomials. However, mainstream 3D lane detectors rely on perfect camera poses provided by other sensors, which is expensive and encounters multi-sensor calibration issues. To overcome this problem, we propose to predict 3D lanes by estimating camera pose from a single image with a two-stage framework. The first stage aims at the camera pose task from perspective-view images. To improve pose estimation, we introduce an auxiliary 3D lane task and geometry constraints to benefit from multi-task learning, which enhances consistencies between 3D and 2D, as well as compatibility in the above two tasks. The second stage targets the 3D lane task. It uses previously estimated pose to generate top-view images containing distance-invariant lane appearances for predicting accurate 3D lanes. Experiments demonstrate that, without ground truth camera pose, our method outperforms the state-of-the-art perfect-camera-pose-based methods and has the fewest parameters and computations. Codes are available at https://github.com/liuruijin17/CLGo.",
    "code_link": ""
  },
  "aaai2022_main_ovisopen-vocabularyvisualinstancesearchviavisual-semanticalignedrepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning",
    "authors": [
      "Sheng Liu",
      "Kevin Lin",
      "Lijuan Wang",
      "Junsong Yuan",
      "Zicheng Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20070",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20070/19829",
    "published": "2022-02",
    "summary": "We introduce the task of open-vocabulary visual instance search (OVIS). Given an arbitrary textual search query, Open-vocabulary Visual Instance Search (OVIS) aims to return a ranked list of visual instances, i.e., image patches, that satisfies the search intent from an image database. The term ``open vocabulary'' means that there are neither restrictions to the visual instance to be searched nor restrictions to the word that can be used to compose the textual search query. We propose to address such a search challenge via visual-semantic aligned representation learning (ViSA). ViSA leverages massive image-caption pairs as weak image-level (not instance-level) supervision to learn a rich cross-modal semantic space where the representations of visual instances (not images) and those of textual queries are aligned, thus allowing us to measure the similarities between any visual instance and an arbitrary textual query. To evaluate the performance of ViSA, we build two datasets named OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through extensive experiments on the two datasets, we demonstrate ViSA's ability to search for visual instances in images not available during training given a wide range of textual queries including those composed of uncommon words. Experimental results show that ViSA achieves an mAP@50 of 27.8% on OVIS40 and achieves a recall@30 of 21.3% on OVIS1400 dataset under the most challenging settings.",
    "code_link": ""
  },
  "aaai2022_main_featuregenerationandhypothesisverificationforreliablefaceanti-spoofing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Feature Generation and Hypothesis Verification for Reliable Face Anti-spoofing",
    "authors": [
      "Shice Liu",
      "Shitao Lu",
      "Hongyi Xu",
      "Jing Yang",
      "Shouhong Ding",
      "Lizhuang Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20071",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20071/19830",
    "published": "2022-02",
    "summary": "Although existing face anti-spoofing (FAS) methods achieve high accuracy in intra-domain experiments, their effects drop severely in cross-domain scenarios because of poor generalization. Recently, multifarious techniques have been explored, such as domain generalization and representation disentanglement. However, the improvement is still limited by two issues: 1) It is difficult to perfectly map all faces to a shared feature space. If faces from unknown domains are not mapped to the known region in the shared feature space, accidentally inaccurate predictions will be obtained. 2) It is hard to completely consider various spoof traces for disentanglement. In this paper, we propose a Feature Generation and Hypothesis Verification framework to alleviate the two issues. Above all, feature generation networks which generate hypotheses of real faces and known attacks are introduced for the first time in the FAS task. Subsequently, two hypothesis verification modules are applied to judge whether the input face comes from the real-face space and the real-face distribution respectively. Furthermore, some analyses of the relationship between our framework and Bayesian uncertainty estimation are given, which provides theoretical support for reliable defense in unknown domains. Experimental results show our framework achieves promising results and outperforms the state-of-the-art approaches on extensive public datasets.",
    "code_link": "https://github.com/lustoo/FGHV"
  },
  "aaai2022_main_image-adaptiveyoloforobjectdetectioninadverseweatherconditions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions",
    "authors": [
      "Wenyu Liu",
      "Gaofeng Ren",
      "Runsheng Yu",
      "Shi Guo",
      "Jianke Zhu",
      "Lei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20072",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20072/19831",
    "published": "2022-02",
    "summary": "Though deep learning-based object detection methods have achieved promising results on the conventional datasets, it is still challenging to locate objects from the low-quality images captured in adverse weather conditions. The existing methods either have difficulties in balancing the tasks of image enhancement and object detection, or often ignore the latent information beneficial for detection. To alleviate this problem, we propose a novel Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively enhanced for better detection performance. Specifically, a differentiable image processing (DIP) module is presented to take into account the adverse weather conditions for YOLO detector, whose parameters are predicted by a small convolutional neural network (CNN-PP). We learn CNN-PP and YOLOv3 jointly in an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP to enhance the image for detection in a weakly supervised manner. Our proposed IA-YOLO approach can adaptively process images in both normal and adverse weather conditions. The experimental results are very encouraging, demonstrating the effectiveness of our proposed IA-YOLO method in both foggy and low-light scenarios. The source code can be found at https://github.com/wenyyu/Image-Adaptive-YOLO.",
    "code_link": "https://github.com/wenyyu/ImageAdaptive-YOLO"
  },
  "aaai2022_main_visualsoundlocalizationinthewildbycross-modalinterferenceerasing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Visual Sound Localization in the Wild by Cross-Modal Interference Erasing",
    "authors": [
      "Xian Liu",
      "Rui Qian",
      "Hang Zhou",
      "Di Hu",
      "Weiyao Lin",
      "Ziwei Liu",
      "Bolei Zhou",
      "Xiaowei Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20073",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20073/19832",
    "published": "2022-02",
    "summary": "The task of audiovisual sound source localization has been well studied under constrained scenes, where the audio recordings are clean. However, in real world scenarios, audios are usually contaminated by off screen sound and background noise. They will interfere with the procedure of identifying desired sources and building visual sound connections, making previous studies nonapplicable. In this work, we propose the Interference Eraser (IEr) framework, which tackles the problem of audiovisual sound source localization in the wild. The key idea is to eliminate the interference by redefining and carving discriminative audio representations. Specifically, we observe that the previous practice of learning only a single audio representation is insufficient due to the additive nature of audio signals. We thus extend the audio representation with our Audio Instance Identifier module, which clearly distinguishes sounding instances when audio signals of different volumes are unevenly mixed. Then we erase the influence of the audible but off screen sounds and the silent but visible objects by a Cross modal Referrer module with cross modality distillation. Quantitative and qualitative evaluations demonstrate that our framework achieves superior results on sound localization tasks, especially under real world scenarios.",
    "code_link": ""
  },
  "aaai2022_main_learningauxiliarymonocularcontextshelpsmonocular3dobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection",
    "authors": [
      "Xianpeng Liu",
      "Nan Xue",
      "Tianfu Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20074",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20074/19833",
    "published": "2022-02",
    "summary": "Monocular 3D object detection aims to localize 3D bounding boxes in an input single 2D image. It is a highly challenging problem and remains open, especially when no extra information (e.g., depth, lidar and/or multi-frames) can be leveraged in training and/or inference.This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information. It presents the MonoCon method which learns Monocular Contexts, as auxiliary tasks in training, to help monocular 3D object detection. The key idea is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the center of 2D bounding box, which should be exploited as auxiliary tasks in training.The proposed MonoCon is motivated by the Cramer\u2013Wold theorem in measure theory at a high level.In implementation, it utilizes a very simple end-to-end design to justify the effectiveness of learning auxiliary monocular contexts, whichconsists of three components: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts. After training, the auxiliary context regression branches are discarded for better inference efficiency. In experiments, the proposed MonoCon is tested in the KITTI benchmark (car, pedestrian and cyclist). It outperforms all prior arts in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy. Thanks to the simple design, the proposed MonoCon method obtains the fastest inference speed with 38.7 fps in comparisons. Our code is released at https://git.io/MonoCon.",
    "code_link": ""
  },
  "aaai2022_main_highlightingobjectcategoryimmunityforthegeneralizationofhuman-objectinteractiondetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection",
    "authors": [
      "Xinpeng Liu",
      "Yong-Lu Li",
      "Cewu Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20075",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20075/19834",
    "published": "2022-02",
    "summary": "Human-Object Interaction (HOI) detection plays a core role in activity understanding. As a compositional learning problem (human-verb-object), studying its generalization matters. However, widely-used metric mean average precision (mAP) fails to model the compositional generalization well. Thus, we propose a novel metric, mPD (mean Performance Degradation), as a complementary of mAP to evaluate the performance gap among compositions of different objects and the same verb. Surprisingly, mPD reveals that previous methods usually generalize poorly. With mPD as a cue, we propose Object Category (OC) Immunity to boost HOI generalization. The idea is to prevent model from learning spurious object-verb correlations as a short-cut to over-fit the train set. To achieve OC-immunity, we propose an OC-immune network that decouples the inputs from OC, extracts OC-immune representations, and leverages uncertainty quantification to generalize to unseen objects. In both conventional and zero-shot experiments, our method achieves decent improvements. To fully evaluate the generalization, we design a new and more difficult benchmark, on which we present significant advantage. The code is available at https://github.com/Foruck/OC-Immunity.",
    "code_link": "https://github.com/Foruck/OC-Immunity"
  },
  "aaai2022_main_dmn4few-shotlearningviadiscriminativemutualnearestneighborneuralnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DMN4: Few-Shot Learning via Discriminative Mutual Nearest Neighbor Neural Network",
    "authors": [
      "Yang Liu",
      "Tu Zheng",
      "Jie Song",
      "Deng Cai",
      "Xiaofei He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20076",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20076/19835",
    "published": "2022-02",
    "summary": "Few-shot learning (FSL) aims to classify images under low-data regimes, where the conventional pooled global feature is likely to lose useful local characteristics. Recent work has achieved promising performances by using deep descriptors. They generally take all deep descriptors from neural networks into consideration while ignoring that some of them are useless in classification due to their limited receptive field, e.g., task-irrelevant descriptors could be misleading and multiple aggregative descriptors from background clutter could even overwhelm the object's presence. In this paper, we argue that a Mutual Nearest Neighbor (MNN) relation should be established to explicitly select the query descriptors that are most relevant to each task and discard less relevant ones from aggregative clutters in FSL. Specifically, we propose Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive experiments demonstrate that our method outperforms the existing state-of-the-arts on both fine-grained and generalized datasets.",
    "code_link": ""
  },
  "aaai2022_main_multi-knowledgeaggregationandtransferforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Knowledge Aggregation and Transfer for Semantic Segmentation",
    "authors": [
      "Yuang Liu",
      "Wei Zhang",
      "Jun Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20077",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20077/19836",
    "published": "2022-02",
    "summary": "As a popular deep neural networks (DNN) compression technique, knowledge distillation (KD) has attracted increasing attentions recently. Existing KD methods usually utilize one kind of knowledge in an intermediate layer of DNN for classification tasks to transfer useful information from cumbersome teacher networks to compact student networks. However, this paradigm is not very suitable for semantic segmentation, a comprehensive vision task based on both pixel-level and contextual information, since it cannot provide rich information for distillation. In this paper, we propose a novel multi-knowledge aggregation and transfer (MKAT) framework to comprehensively distill knowledge within an intermediate layer for semantic segmentation. Specifically, the proposed framework consists of three parts: Independent Transformers and Encoders module (ITE), Auxiliary Prediction Branch (APB), and Mutual Label Calibration (MLC) mechanism, which can take advantage of abundant knowledge from intermediate features. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on three segmentation datasets: Pascal VOC, Cityscapes, and CamVid, showing that MKAT outperforms the other KD methods.",
    "code_link": ""
  },
  "aaai2022_main_unsupervisedcoherentvideocartoonizationwithperceptualmotionconsistency": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency",
    "authors": [
      "Zhenhuan Liu",
      "Liang Li",
      "Huajie Jiang",
      "Xin Jin",
      "Dandan Tu",
      "Shuhui Wang",
      "Zheng-Jun Zha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20078",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20078/19837",
    "published": "2022-02",
    "summary": "In recent years, creative content generations like style transfer and neural photo editing have attracted more and more attention. Among these, cartoonization of real-world scenes has promising applications in entertainment and industry. Different from image translations focusing on improving the style effect of generated images, video cartoonization has additional requirements on the temporal consistency. In this paper, we propose a spatially-adaptive semantic alignment framework with perceptual motion consistency for coherent video cartoonization in an unsupervised manner. The semantic alignment module is designed to restore deformation of semantic structure caused by spatial information lost in the encoder-decoder architecture. Furthermore, we introduce the spatio-temporal correlative map as a style-independent, global-aware regularization on perceptual motion consistency. Deriving from similarity measurement of high-level features in photo and cartoon frames, it captures global semantic information beyond raw pixel-value of optical flow. Besides, the similarity measurement disentangles temporal relationship from domain-specific style properties, which helps regularize the temporal consistency without hurting style effects of cartoon images. Qualitative and quantitative experiments demonstrate our method is able to generate highly stylistic and temporal consistent cartoon videos.",
    "code_link": "https://github.com/PyTorchLightning/pytorch-lightning"
  },
  "aaai2022_main_task-customizedself-supervisedpre-trainingwithscalabledynamicrouting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing",
    "authors": [
      "Zhili LIU",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Kai Chen",
      "Chunjing Xu",
      "Zhenguo Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20079",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20079/19838",
    "published": "2022-02",
    "summary": "Self-supervised learning (SSL), especially contrastive methods, has raised attraction recently as it learns effective transferable representations without semantic annotations. A common practice for self-supervised pre-training is to use as much data as possible. For a specific downstream task, however, involving irrelevant data in pre-training may degenerate the downstream performance, observed from our extensive experiments. On the other hand, for existing SSL methods, it is burdensome and infeasible to use different downstream-task-customized datasets in pre-training for different tasks.To address this issue, we propose a novel SSL paradigm called Scalable Dynamic Routing (SDR), which can be trained once and deployed efficiently to different downstream tasks with task-customized pre-trained models. Specifically, we construct the SDRnet with various sub-nets and train each sub-net with only one subset of the data by data-aware progressive training. When a downstream task arrives, we route among all the pre-trained sub-nets to get the best along with its corresponding weights. Experiment results show that our SDR can train 256 sub-nets on ImageNet simultaneously, which provides better transfer performance than a unified model trained on the full ImageNet, achieving state-of-the-art (SOTA) averaged accuracy over 11 downstream classification tasks and AP on PASCAL VOC detection task.",
    "code_link": ""
  },
  "aaai2022_main_poseguidedimagegenerationfrommisalignedsourcesviaresidualflowbasedcorrection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pose Guided Image Generation from Misaligned Sources via Residual Flow Based Correction",
    "authors": [
      "Jiawei Lu",
      "He Wang",
      "Tianjia Shao",
      "Yin Yang",
      "Kun Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20080",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20080/19839",
    "published": "2022-02",
    "summary": "Generating new images with desired properties (e.g. new view/poses) from source images has been enthusiastically pursued recently, due to its wide range of potential applications. One way to ensure high-quality generation is to use multiple sources with complementary information such as different views of the same object. However, as source images are often misaligned due to the large disparities among the camera settings, strong assumptions have been made in the past with respect to the camera(s) or/and the object in interest, limiting the application of such techniques. Therefore, we propose a new general approach which models multiple types of variations among sources, such as view angles, poses, facial expressions, in a unified framework, so that it can be employed on datasets of vastly different nature. We verify our approach on a variety of data including humans bodies, faces, city scenes and 3D objects. Both the qualitative and quantitative results demonstrate the better performance of our method than the state of the art.",
    "code_link": ""
  },
  "aaai2022_main_pmalopensetrecognitionviarobustprototypemining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PMAL: Open Set Recognition via Robust Prototype Mining",
    "authors": [
      "Jing Lu",
      "Yunlu Xu",
      "Hao Li",
      "Zhanzhan Cheng",
      "Yi Niu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20081",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20081/19840",
    "published": "2022-02",
    "summary": "Open Set Recognition (OSR) has been an emerging topic. Besides recognizing predefined classes, the system needs to reject the unknowns. Prototype learning is a potential manner to handle the problem, as its ability to improve intra-class compactness of representations is much needed in discrimination between the known and the unknowns. In this work, we propose a novel Prototype Mining And Learning (PMAL) framework. It has a prototype mining mechanism before the phase of optimizing embedding space, explicitly considering two crucial properties, namely high-quality and diversity of the prototype set. Concretely, a set of high-quality candidates are firstly extracted from training samples based on data uncertainty learning, avoiding the interference from unexpected noise. Considering the multifarious appearance of objects even in a single category, a diversity-based strategy for prototype set filtering is proposed. Accordingly, the embedding space can be better optimized to discriminate therein the predefined classes and between known and unknowns. Extensive experiments verify the two good characteristics (i.e., high-quality and diversity) embraced in prototype mining, and show the remarkable performance of the proposed framework compared to state-of-the-arts.",
    "code_link": ""
  },
  "aaai2022_main_barely-supervisedlearningsemi-supervisedlearningwithveryfewlabeledimages": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Barely-Supervised Learning: Semi-supervised Learning with Very Few Labeled Images",
    "authors": [
      "Thomas Lucas",
      "Philippe Weinzaepfel",
      "Gregory Rogez"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20082",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20082/19841",
    "published": "2022-02",
    "summary": "This paper tackles the problem of semi-supervised learning when the set of labeled samples is limited to a small number of images per class, typically less than 10, problem that we refer to as barely-supervised learning. We analyze in depth the behavior of a state-of-the-art semi-supervised method, FixMatch, which relies on a weakly-augmented version of an image to obtain supervision signal for a more strongly-augmented version. We show that it frequently fails in barely-supervised scenarios, due to a lack of training signal when no pseudo-label can be predicted with high confidence. We propose a method to leverage self-supervised methods that provides training signal in the absence of confident pseudo-labels. We then propose two methods to refine the pseudo-label selection process which lead to further improvements.The first one relies on a per-sample history of the model predictions, akin to a voting scheme. The second iteratively up-dates class-dependent confidence thresholds to better explore classes that are under-represented in the pseudo-labels. Our experiments show that our approach performs significantly better on STL-10 in the barely-supervised regime,e.g. with 4 or 8 labeled images per class.",
    "code_link": ""
  },
  "aaai2022_main_learningopticalflowwithadaptivegraphreasoning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Optical Flow with Adaptive Graph Reasoning",
    "authors": [
      "Ao Luo",
      "Fan Yang",
      "Kunming Luo",
      "Xin Li",
      "Haoqiang Fan",
      "Shuaicheng Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20083",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20083/19842",
    "published": "2022-02",
    "summary": "Estimating per-pixel motion between video frames, known as optical flow, is a long-standing problem in video understanding and analysis. Most contemporary optical flow techniques largely focus on addressing the cross-image matching with feature similarity, with few methods considering how to explicitly reason over the given scene for achieving a holistic motion understanding. In this work, taking a fresh perspective, we introduce a novel graph-based approach, called adaptive graph reasoning for optical flow (AGFlow), to emphasize the value of scene/context information in optical flow. Our key idea is to decouple the context reasoning from the matching procedure, and exploit scene information to effectively assist motion estimation by learning to reason over the adaptive graph. The proposed AGFlow can effectively exploit the context information and incorporate it within the matching procedure, producing more robust and accurate results. On both Sintel clean and final passes, our AGFlow achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming state-of-the-art approaches by 11.2% and 13.6%, respectively. Code is publicly available at https://github.com/megvii-research/AGFlow.",
    "code_link": ""
  },
  "aaai2022_main_afusion-denoisingattackoninstahidewithdataaugmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Fusion-Denoising Attack on InstaHide with Data Augmentation",
    "authors": [
      "Xinjian Luo",
      "Xiaokui Xiao",
      "Yuncheng Wu",
      "Juncheng Liu",
      "Beng Chin Ooi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20084",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20084/19843",
    "published": "2022-02",
    "summary": "InstaHide is a state-of-the-art mechanism for protecting private training images, by mixing multiple private images and modifying them such that their visual features are indistinguishable to the naked eye. In recent work, however, Carlini et al. show that it is possible to reconstruct private images from the encrypted dataset generated by InstaHide. Nevertheless, we demonstrate that Carlini et al.\u2019s attack can be easily defeated by incorporating data augmentation into InstaHide. This leads to a natural question: is InstaHide with data augmentation secure? In this paper, we provide a negative answer to this question, by devising an attack for recovering private images from the outputs of InstaHide even when data augmentation is present. The basic idea is to use a comparative network to identify encrypted images that are likely to correspond to the same private image, and then employ a fusion-denoising network for restoring the private image from the encrypted ones, taking into account the effects of data augmentation. Extensive experiments demonstrate the effectiveness of the proposed attack in comparison to Carlini et al.\u2019s attack.",
    "code_link": ""
  },
  "aaai2022_main_deepneuralnetworkslearnmeta-structuresfromnoisylabelsinsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation",
    "authors": [
      "Yaoru Luo",
      "Guole Liu",
      "Yuanhao Guo",
      "Ge Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20085",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20085/19844",
    "published": "2022-02",
    "summary": "How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.",
    "code_link": ""
  },
  "aaai2022_main_stochasticplanner-actor-criticforunsuperviseddeformableimageregistration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration",
    "authors": [
      "Ziwei Luo",
      "Jing Hu",
      "Xin Wang",
      "Shu Hu",
      "Bin Kong",
      "Youbing Yin",
      "Qi Song",
      "Xi\n      Wu",
      "Siwei Lyu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20086",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20086/19845",
    "published": "2022-02",
    "summary": "Large deformations of organs, caused by diverse shapes and nonlinear shape changes, pose a significant challenge for medical image registration. Traditional registration methods need to iteratively optimize an objective function via a specific deformation model along with meticulous parameter tuning, but which have limited capabilities in registering images with large deformations. While deep learning-based methods can learn the complex mapping from input images to their respective deformation field, it is regression-based and is prone to be stuck at local minima, particularly when large deformations are involved. To this end, we present Stochastic Planner-Actor-Critic (spac), a novel reinforcement learning-based framework that performs step-wise registration. The key notion is warping a moving image successively by each time step to finally align to a fixed image. Considering that it is challenging to handle high dimensional continuous action and state spaces in the conventional reinforcement learning (RL) framework, we introduce a new concept `Plan' to the standard Actor-Critic model, which is of low dimension and can facilitate the actor to generate a tractable high dimensional action. The entire framework is based on unsupervised training and operates in an end-to-end manner. We evaluate our method on several 2D and 3D medical image datasets, some of which contain large deformations. Our empirical results highlight that our work achieves consistent, significant gains and outperforms state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_adaptivepoincar\u00e9pointtosetdistanceforfew-shotclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Poincar\u00e9 Point to Set Distance for Few-Shot Classification",
    "authors": [
      "Rongkai Ma",
      "Pengfei Fang",
      "Tom Drummond",
      "Mehrtash Harandi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20087",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20087/19846",
    "published": "2022-02",
    "summary": "Learning and generalizing from limited examples, i.e., few-shot learning, is of core importance to many real-world vision applications. A principal way of achieving few-shot learning is to realize an embedding where samples from different classes are distinctive. Recent studies suggest that embedding via hyperbolic geometry enjoys low distortion for hierarchical and structured data, making it suitable for few-shot learning. In this paper, we propose to learn a context-aware hyperbolic metric to characterize the distance between a point and a set associated with a learned set to set distance. To this end, we formulate the metric as a weighted sum on the tangent bundle of the hyperbolic space and develop a mechanism to obtain the weights adaptively, based on the constellation of the points. This not only makes the metric local but also dependent on the task in hand, meaning that the metric will adapt depending on the samples that it compares. We empirically show that such metric yields robustness in the presence of outliers and achieves a tangible improvement over baseline models. This includes the state-of-the-art results on five popular few-shot classification benchmarks, namely mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds-200-2011(CUB), CIFAR-FS, and FC100.",
    "code_link": ""
  },
  "aaai2022_main_generativeadaptiveconvolutionsforreal-worldnoisyimagedenoising": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generative Adaptive Convolutions for Real-World Noisy Image Denoising",
    "authors": [
      "Ruijun Ma",
      "Shuyi Li",
      "Bob Zhang",
      "Zhengming Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20088",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20088/19847",
    "published": "2022-02",
    "summary": "Recently, deep learning techniques are soaring and have shown dramatic improvements in real-world noisy image denoising. However, the statistics of real noise generally vary with different camera sensors and in-camera signal processing pipelines. This will induce problems of most deep denoisers for the overfitting or degrading performance due to the noise discrepancy between the training and test sets. To remedy this issue, we propose a novel flexible and adaptive denoising network, coined as FADNet. Our FADNet is equipped with a plane dynamic filter module, which generates weight filters with flexibility that can adapt to the specific input and thereby impedes the FADNet from overfitting to the training data. Specifically, we exploit the advantage of the spatial and channel attention, and utilize this to devise a decoupling filter generation scheme. The generated filters are conditioned on the input and collaboratively applied to the decoded features for representation capability enhancement. We additionally introduce the Fourier transform and its inverse to guide the predicted weight filters to adapt to the noisy input with respect to the image contents. Experimental results demonstrate the superior denoising performances of the proposed FADNet versus the state-of-the-art. In contrast to the existing deep denoisers, our FADNet is not only flexible and efficient, but also exhibits a compelling generalization capability, enjoying tremendous potential for practical usage.",
    "code_link": ""
  },
  "aaai2022_main_remotereinforcedmotiontransformationnetworkforsemi-supervised2dposeestimationinvideos": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "REMOTE: Reinforced Motion Transformation Network for Semi-supervised 2D Pose Estimation in Videos",
    "authors": [
      "Xianzheng Ma",
      "Hossein Rahmani",
      "Zhipeng Fan",
      "Bin Yang",
      "Jun Chen",
      "Jun Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20089",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20089/19848",
    "published": "2022-02",
    "summary": "Existing approaches for 2D pose estimation in videos often require a large number of dense annotations, which are costly and labor intensive to acquire. In this paper, we propose a semi-supervised REinforced MOtion Transformation nEtwork (REMOTE) to leverage a few labeled frames and temporal pose variations in videos, which enables effective learning of 2D pose estimation in sparsely annotated videos. Specifically, we introduce a Motion Transformer (MT) module to perform cross frame reconstruction, aiming to learn motion dynamic knowledge in videos. Besides, a novel reinforcement learning-based Frame Selection Agent (FSA) is designed within our framework, which is able to harness informative frame pairs on the fly to enhance the pose estimator under our cross reconstruction mechanism. We conduct extensive experiments that show the efficacy of our proposed REMOTE framework.",
    "code_link": ""
  },
  "aaai2022_main_learningfromthetargetdualprototypenetworkforfewshotsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning from the Target: Dual Prototype Network for Few Shot Semantic Segmentation",
    "authors": [
      "Binjie Mao",
      "Xinbang Zhang",
      "Lingfeng Wang",
      "Qian Zhang",
      "Shiming Xiang",
      "Chunhong Pan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20090",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20090/19849",
    "published": "2022-02",
    "summary": "Due to the scarcity of annotated samples, the diversity between support set and query set becomes the main obstacle for few shot semantic segmentation. Most existing prototype-based approaches only exploit the prototype from the support feature and ignore the information from the query sample, failing to remove this obstacle.In this paper, we proposes a dual prototype network (DPNet) to dispose of few shot semantic segmentation from a new perspective. Along with the prototype extracted from the support set, we propose to build the pseudo-prototype based on foreground features in the query image. To achieve this goal, the cycle comparison module is developed to select reliable foreground features and generate the pseudo-prototype with them. Then, a prototype interaction module is utilized to integrate the information of the prototype and the pseudo-prototype based on their underlying correlation. Finally, a multi-scale fusion module is introduced to capture contextual information during the dense comparison between prototype (pseudo-prototype) and query feature. Extensive experiments conducted on two benchmarks demonstrate that our method exceeds previous state-of-the-arts with a sizable margin, verifying the effectiveness of the proposed method.",
    "code_link": ""
  },
  "aaai2022_main_most-gan3dmorphablestyleganfordisentangledfaceimagemanipulation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation",
    "authors": [
      "Safa C. Medin",
      "Bernhard Egger",
      "Anoop Cherian",
      "Ye Wang",
      "Joshua B.\n      Tenenbaum",
      "Xiaoming Liu",
      "Tim K. Marks"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20091",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20091/19850",
    "published": "2022-02",
    "summary": "Recent advances in generative adversarial networks (GANs) have led to remarkable achievements in face image synthesis. While methods that use style-based GANs can generate strikingly photorealistic face images, it is often difficult to control the characteristics of the generated faces in a meaningful and disentangled way. Prior approaches aim to achieve such semantic control and disentanglement within the latent space of a previously trained GAN. In contrast, we propose a framework that a priori models physical attributes of the face such as 3D shape, albedo, pose, and lighting explicitly, thus providing disentanglement by design. Our method, MOST-GAN, integrates the expressive power and photorealism of style-based GANs with the physical disentanglement and flexibility of nonlinear 3D morphable models, which we couple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves photorealistic manipulation of portrait images with fully disentangled 3D control over their physical attributes, enabling extreme manipulation of lighting, facial expression, and pose variations up to full profile view.",
    "code_link": ""
  },
  "aaai2022_main_towardsbridgingsamplecomplexityandmodelcapacity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Bridging Sample Complexity and Model Capacity",
    "authors": [
      "Shibin Mei",
      "Chenglong Zhao",
      "Shengchao Yuan",
      "Bingbing Ni"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20092",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20092/19851",
    "published": "2022-02",
    "summary": "In this paper, we give a new definition for sample complexity, and further develop a theoretical analysis to bridge the gap between sample complexity and model capacity. In contrast to previous works which study on some toy samples, we conduct our analysis on more general data space, and build a qualitative relationship from sample complexity to model capacity required to achieve comparable performance. Besides, we introduce a simple indicator to evaluate the sample complexity based on continuous mapping. Moreover, we further analysis the relationship between sample complexity and data distribution, which paves the way to understand the present representation learning. Extensive experiments on several datasets well demonstrate the effectiveness of our evaluation method.",
    "code_link": ""
  },
  "aaai2022_main_towardsaccuratefacialmotionretargetingwithidentity-consistentandexpression-exclusiveconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Accurate Facial Motion Retargeting with Identity-Consistent and Expression-Exclusive Constraints",
    "authors": [
      "Langyuan Mo",
      "Haokun Li",
      "Chaoyang Zou",
      "Yubing Zhang",
      "Ming Yang",
      "Yihong\n      Yang",
      "Mingkui Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20093",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20093/19852",
    "published": "2022-02",
    "summary": "We address the problem of facial motion retargeting that aims to transfer facial motion from a 2D face image to 3D characters. Existing methods often formulate this problem as a 3D face reconstruction problem, which estimates the face attributes such as face identity and expression from face images. However, due to the lack of ground-truth labels for both identity and expression, most 3D-face reconstruction-based methods fail to capture the facial identity and expression accurately. As a result, these methods may not achieve promising performance. To address this, we propose an identity-consistent constraint to learn accurate identities by encouraging consistent identity prediction across multiple frames. Based on a more accurate identity, we are able to obtain a more accurate facial expression. Moreover, we further propose an expression-exclusive constraint to improve performance by avoiding the co-occurrence of contradictory expression units (e.g., ``brow lower'' vs. ``brow raise''). Extensive experiments on facial motion retargeting and 3D face reconstruction tasks demonstrate the superiority of the proposed method over existing methods. Our code andsupplementary materials are available at https://github.com/deepmo24/CPEM.",
    "code_link": "https://github.com/deepmo24/CPEM"
  },
  "aaai2022_main_canvisiontransformerslearnwithoutnaturalimages?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Can Vision Transformers Learn without Natural Images?",
    "authors": [
      "Kodai Nakashima",
      "Hirokatsu Kataoka",
      "Asato Matsumoto",
      "Kenji Iwata",
      "Nakamasa\n      Inoue",
      "Yutaka Satoh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20094",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20094/19853",
    "published": "2022-02",
    "summary": "Is it possible to complete Vision Transformer (ViT) pre-training without natural images and human-annotated labels? This question has become increasingly relevant in recent months because while current ViT pre-training tends to rely heavily on a large number of natural images and human-annotated labels, the recent use of natural images has resulted in problems related to privacy violation, inadequate fairness protection, and the need for labor-intensive annotations. In this paper, we experimentally verify that the results of formula-driven supervised learning (FDSL) framework are comparable with, and can even partially outperform, sophisticated self-supervised learning (SSL) methods like SimCLRv2 and MoCov2 without using any natural images in the pre-training phase. We also consider ways to reorganize FractalDB generation based on our tentative conclusion that there is room for configuration improvements in the iterated function system (IFS) parameter settings of such databases. Moreover, we show that while ViTs pre-trained without natural images produce visualizations that are somewhat different from ImageNet pre-trained ViTs, they can still interpret natural image datasets to a large extent. Finally, in experiments using the CIFAR-10 dataset, we show that our model achieved a performance rate of 97.8, which is comparable to the rate of 97.4 achieved with SimCLRv2 and 98.0 achieved with ImageNet.",
    "code_link": ""
  },
  "aaai2022_main_federatedlearningforfacerecognitionwithgradientcorrection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Federated Learning for Face Recognition with Gradient Correction",
    "authors": [
      "Yifan Niu",
      "Weihong Deng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20095",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20095/19854",
    "published": "2022-02",
    "summary": "With increasing appealing to privacy issues in face recognition, federated learning has emerged as one of the most prevalent approaches to study the unconstrained face recognition problem with private decentralized data. However, conventional decentralized federated algorithm sharing whole parameters of networks among clients suffers from privacy leakage in face recognition scene. In this work, we introduce a framework, FedGC, to tackle federated learning for face recognition and guarantees higher privacy. We explore a novel idea of correcting gradients from the perspective of backward propagation and propose a softmax-based regularizer to correct gradients of class embeddings by precisely injecting a cross-client gradient term. Theoretically, we show that FedGC constitutes a valid loss function similar to standard softmax. Extensive experiments have been conducted to validate the superiority of FedGC which can match the performance of conventional centralized methods utilizing full training dataset on several popular benchmark datasets.",
    "code_link": ""
  },
  "aaai2022_main_restorableimageoperatorswithquasi-invertiblenetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Restorable Image Operators with Quasi-Invertible Networks",
    "authors": [
      "Hao Ouyang",
      "Tengfei Wang",
      "Qifeng Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20096",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20096/19855",
    "published": "2022-02",
    "summary": "Image operators have been extensively applied to create visually attractive photos for users to share processed images on social media. However, most image operators often smooth out details or generate textures after the processing, which removes the original content and raises challenges for restoring the original image. To resolve this issue, we propose a quasi-invertible model that learns common image processing operators in a restorable fashion: the learned image operators can generate visually pleasing results with the original content embedded. Our model is trained on input-output pairs that represent an image processing operator's behavior and uses a network that consists of an invertible branch and a non-invertible branch to increase our model's approximation capability. We evaluate the proposed model on ten image operators, including detail enhancement, abstraction, blur,photographic style, and non-photorealistic style. Extensive experiments show that our approach outperforms relevant baselines in the restoration quality, and the learned restorable operator is fast in inference and robust to compression. Furthermore, we demonstrate that the invertible operator can be easily applied to practical applications such as restorable human face retouching and highlight preserved exposure adjustment.",
    "code_link": ""
  },
  "aaai2022_main_teachtask-drivenembodiedagentsthatchat": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TEACh: Task-Driven Embodied Agents That Chat",
    "authors": [
      "Aishwarya Padmakumar",
      "Jesse Thomason",
      "Ayush Shrivastava",
      "Patrick Lange",
      "Anjali Narayan-Chen",
      "Spandana Gella",
      "Robinson Piramuthu",
      "Gokhan Tur",
      "Dilek\n      Hakkani-Tur"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20097",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20097/19856",
    "published": "2022-02",
    "summary": "Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\", asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution.",
    "code_link": "https://github.com/alexa/teach"
  },
  "aaai2022_main_label-efficienthybrid-supervisedlearningformedicalimagesegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Label-Efficient Hybrid-Supervised Learning for Medical Image Segmentation",
    "authors": [
      "Junwen Pan",
      "Qi Bi",
      "Yanzhan Yang",
      "Pengfei Zhu",
      "Cheng Bian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20098",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20098/19857",
    "published": "2022-02",
    "summary": "Due to the lack of expertise for medical image annotation, the investigation of label-efficient methodology for medical image segmentation becomes a heated topic. Recent progresses focus on the efficient utilization of weak annotations together with few strongly-annotated labels so as to achieve comparable segmentation performance in many unprofessional scenarios. However, these approaches only concentrate on the supervision inconsistency between strongly- and weakly-annotated instances but ignore the instance inconsistency inside the weakly-annotated instances, which inevitably leads to performance degradation.To address this problem, we propose a novel label-efficient hybrid-supervised framework, which considers each weakly-annotated instance individually and learns its weight guided by the gradient direction of the strongly-annotated instances, so that the high-quality prior in the strongly-annotated instances is better exploited and the weakly-annotated instances are depicted more precisely. Specially, our designed dynamic instance indicator (DII) realizes the above objectives, and is adapted to our dynamic co-regularization (DCR) framework further to alleviate the erroneous accumulation from distortions of weak annotations. Extensive experiments on two hybrid-supervised medical segmentation datasets demonstrate that with only 10% strong labels, the proposed framework can leverage the weak labels efficiently and achieve competitive performance against the 100% strong-label supervised scenario.",
    "code_link": ""
  },
  "aaai2022_main_lessismorepaylessattentioninvisiontransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Less Is More: Pay Less Attention in Vision Transformers",
    "authors": [
      "Zizheng Pan",
      "Bohan Zhuang",
      "Haoyu He",
      "Jing Liu",
      "Jianfei Cai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20099",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20099/19858",
    "published": "2022-02",
    "summary": "Transformers have become one of the dominant architectures in deep learning, particularly as a powerful alternative to convolutional neural networks (CNNs) in computer vision. However, Transformer training and inference in previous works can be prohibitively expensive due to the quadratic complexity of self-attention over a long sequence of representations, especially for high-resolution dense prediction tasks. To this end, we present a novel Less attention vIsion Transformer (LIT), building upon the fact that the early self-attention layers in Transformers still focus on local patterns and bring minor benefits in recent hierarchical vision Transformers. Specifically, we propose a hierarchical Transformer where we use pure multi-layer perceptrons (MLPs) to encode rich local patterns in the early stages while applying self-attention modules to capture longer dependencies in deeper layers. Moreover, we further propose a learned deformable token merging module to adaptively fuse informative patches in a non-uniform manner. The proposed LIT achieves promising performance on image recognition tasks, including image classification, object detection and instance segmentation, serving as a strong backbone for many vision tasks. Code is available at https://github.com/zip-group/LIT.",
    "code_link": "https://github.com/zip-group/LIT"
  },
  "aaai2022_main_unsupervisedrepresentationforsemanticsegmentationbyimplicitcycle-attentioncontrastivelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Representation for Semantic Segmentation by Implicit Cycle-Attention Contrastive Learning",
    "authors": [
      "Bo Pang",
      "Yizhuo Li",
      "Yifan Zhang",
      "Gao Peng",
      "Jiajun Tang",
      "Kaiwen Zha",
      "Jiefeng Li",
      "Cewu Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20100",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20100/19859",
    "published": "2022-02",
    "summary": "We study the unsupervised representation learning for the semantic segmentation task. Different from previous works that aim at providing unsupervised pre-trained backbones for segmentation models which need further supervised fine-tune, here, we focus on providing representation that is only trained by unsupervised methods. This means models need to directly generate pixel-level, linearly separable semantic results.We first explore and present two factors that have significant effects on segmentation under the contrastive learning framework: 1) the difficulty and diversity of the positive contrastive pairs, 2) the balance of global and local features. With the intention of optimizing these factors, we propose the cycle-attention contrastive learning (CACL). CACL makes use of semantic continuity of video frames, adopting unsupervised cycle-consistent attention mechanism to implicitly conduct contrastive learning with difficult, global-local-balanced positive pixel pairs. Compared with baseline model MoCo-v2 and other unsupervised methods, CACL demonstrates consistently superior performance on PASCAL VOC (+4.5 mIoU) and Cityscapes (+4.5 mIoU) datasets.",
    "code_link": ""
  },
  "aaai2022_main_graph-basedpointtrackerfor3dobjecttrackinginpointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph-Based Point Tracker for 3D Object Tracking in Point Clouds",
    "authors": [
      "Minseong Park",
      "Hongje Seong",
      "Wonje Jang",
      "Euntai Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20101",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20101/19860",
    "published": "2022-02",
    "summary": "In this paper, a new deep learning network named as graph-based point tracker (GPT) is proposed for 3D object tracking in point clouds. GPT is not based on Siamese network applied to template and search area, but it is based on the transfer of target clue from the template to the search area. GPT is end-to-end trainable. GPT has two new modules: graph feature augmentation (GFA) and improved target clue (ITC) module. The key idea of GFA is to exploit one-to-many relationship between template and search area points using a bipartite graph. In GFA, edge features of the bipartite graph are generated by transferring the target clues of template points to search area points through edge convolution. It captures the relationship between template and search area points effectively from the perspective of geometry and shape of two point clouds. The second module is ITC. The key idea of ITC is to embed the information of the center of the target into the edges of the bipartite graph via Hough voting, strengthening the discriminative power of GFA. Both modules significantly contribute to the improvement of GPT by transferring geometric and shape information including target center from target template to search area effectively. Experiments on the KITTI tracking dataset show that GPT achieves state-of-the-art performance and can run in real-time.",
    "code_link": ""
  },
  "aaai2022_main_synctalkfacetalkingfacegenerationwithpreciselip-syncingviaaudio-lipmemory": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory",
    "authors": [
      "Se Jin Park",
      "Minsu Kim",
      "Joanna Hong",
      "Jeongsoo Choi",
      "Yong Man Ro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20102",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20102/19861",
    "published": "2022-02",
    "summary": "The challenge of talking face generation from speech lies in aligning two different modal information, audio and video, such that the mouth region corresponds to input audio. Previous methods either exploit audio-visual representation learning or leverage intermediate structural information such as landmarks and 3D models. However, they struggle to synthesize fine details of the lips varying at the phoneme level as they do not sufficiently provide visual information of the lips at the video synthesis step. To overcome this limitation, our work proposes Audio-Lip Memory that brings in visual information of the mouth region corresponding to input audio and enforces fine-grained audio-visual coherence. It stores lip motion features from sequential ground truth images in the value memory and aligns them with corresponding audio features so that they can be retrieved using audio input at inference time. Therefore, using the retrieved lip motion features as visual hints, it can easily correlate audio with visual dynamics in the synthesis step. By analyzing the memory, we demonstrate that unique lip features are stored in each memory slot at the phoneme level, capturing subtle lip motion based on memory addressing. In addition, we introduce visual-visual synchronization loss which can enhance lip-syncing performance when used along with audio-visual synchronization loss in our model. Extensive experiments are performed to verify that our method generates high-quality video with mouth shapes that best align with the input audio, outperforming previous state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_visiontransformersarerobustlearners": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Vision Transformers Are Robust Learners",
    "authors": [
      "Sayak Paul",
      "Pin-Yu Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20103",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20103/19862",
    "published": "2022-02",
    "summary": "Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) (Dosovitskiy et al. 2021) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT(Dosovitskiy et al. 2021) models and SOTA convolutional neural networks (CNNs), Big-Transfer (Kolesnikov et al. 2020). Through a series of six systematically designed experiments, we then present analyses that provide both quantitative andqualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedcategory-level6dobjectposeestimationwithdeepimplicitshaperepresentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Category-Level 6D Object Pose Estimation with Deep Implicit Shape Representation",
    "authors": [
      "Wanli Peng",
      "Jianhang Yan",
      "Hongtao Wen",
      "Yi Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20104",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20104/19863",
    "published": "2022-02",
    "summary": "Category-level 6D pose estimation can be better generalized to unseen objects in a category compared with instance-level 6D pose estimation. However, existing category-level 6D pose estimation methods usually require supervised training with a sufficient number of 6D pose annotations of objects which makes them difficult to be applied in real scenarios. To address this problem, we propose a self-supervised framework for category-level 6D pose estimation in this paper. We leverage DeepSDF as a 3D object representation and design several novel loss functions based on DeepSDF to help the self-supervised model predict unseen object poses without any 6D object pose labels and explicit 3D models in real scenarios. Experiments demonstrate that our method achieves comparable performance with the state-of-the-art fully supervised methods on the category-level NOCS benchmark.",
    "code_link": "https://github.com/swords123/SSC-6D"
  },
  "aaai2022_main_semantic-awarerepresentationblendingformulti-labelimagerecognitionwithpartiallabels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels",
    "authors": [
      "Tao Pu",
      "Tianshui Chen",
      "Hefeng Wu",
      "Liang Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20105",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20105/19864",
    "published": "2022-02",
    "summary": "Training the multi-label image recognition models with partial labels, in which merely some labels are known while others are unknown for each image, is a considerably challenging and practical task. To address this task, current algorithms mainly depend on pre-training classification or similarity models to generate pseudo labels for the unknown labels. However, these algorithms depend on sufficient multi-label annotations to train the models, leading to poor performance especially with low known label proportion. In this work, we propose to blend category-specific representation across different images to transfer information of known labels to complement unknown labels, which can get rid of pre-training models and thus does not depend on sufficient annotations. To this end, we design a unified semantic-aware representation blending (SARB) framework that exploits instance-level and prototype-level semantic representation to complement unknown labels by two complementary modules: 1) an instance-level representation blending (ILRB) module blends the representations of the known labels in an image to the representations of the unknown labels in another image to complement these unknown labels. 2) a prototype-level representation blending (PLRB) module learns more stable representation prototypes for each category and blends the representation of unknown labels with the prototypes of corresponding labels to complement these labels. Extensive experiments on the MS-COCO, Visual Genome, Pascal VOC 2007 datasets show that the proposed SARB framework obtains superior performance over current leading competitors on all known label proportion settings, i.e., with the mAP improvement of 4.6%, 4.6%, 2.2% on these three datasets when the known label proportion is 10%. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.",
    "code_link": "https://github.com/HCPLab-SYSU/HCP-MLR-PL"
  },
  "aaai2022_main_rexanefficientapproachtoreducingmemorycostinimageclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ReX: An Efficient Approach to Reducing Memory Cost in Image Classification",
    "authors": [
      "Xuwei Qian",
      "Renlong Hang",
      "Qingshan Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20106",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20106/19865",
    "published": "2022-02",
    "summary": "Exiting simple samples in adaptive multi-exit networks through early modules is an effective way to achieve high computational efficiency. One can observe that deployments of multi-exit architectures on resource-constrained devices are easily limited by high memory footprint of early modules. In this paper, we propose a novel approach named recurrent aggregation operator (ReX), which uses recurrent neural networks (RNNs) to effectively aggregate intra-patch features within a large receptive field to get delicate local representations, while bypassing large early activations. The resulting model, named ReXNet, can be easily extended to dynamic inference by introducing a novel consistency-based early exit criteria, which is based on the consistency of classification decisions over several modules, rather than the entropy of the prediction distribution. Extensive experiments on two benchmark datasets, i.e., Visual Wake Words, ImageNet-1k, demonstrate that our method consistently reduces the peak RAM and average latency of a wide variety of adaptive models on low-power devices.",
    "code_link": ""
  },
  "aaai2022_main_cpralcollaborativepanoptic-regionalactivelearningforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CPRAL: Collaborative Panoptic-Regional Active Learning for Semantic Segmentation",
    "authors": [
      "Yu Qiao",
      "Jincheng Zhu",
      "Chengjiang Long",
      "Zeyao Zhang",
      "Yuxin Wang",
      "Zhenjun\n      Du",
      "Xin Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20107",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20107/19866",
    "published": "2022-02",
    "summary": "Acquiring the most representative examples via active learning (AL) can benefit many data-dependent computer vision tasks by minimizing efforts of image-level or pixel-wise annotations. In this paper, we propose a novel Collaborative Panoptic-Regional Active Learning framework (CPRAL) to address the semantic segmentation task. For a small batch of images initially sampled with pixel-wise annotations, we employ panoptic information to initially select unlabeled samples. Considering the class imbalance in the segmentation dataset, we import a Regional Gaussian Attention module (RGA) to achieve semantics-biased selection. The subset is highlighted by vote entropy and then attended by Gaussian kernels to maximize the biased regions. We also propose a Contextual Labels Extension (CLE) to boost regional annotations with contextual attention guidance. With the collaboration of semantics-agnostic panoptic matching and region-biased selection and extension, our CPRAL can strike a balance between labeling efforts and performance and compromise the semantics distribution. We perform extensive experiments on Cityscapes and BDD10K datasets and show that CPRAL outperforms the cutting-edge methods with impressive results and less labeling proportion.",
    "code_link": ""
  },
  "aaai2022_main_activationmodulationandrecalibrationschemeforweaklysupervisedsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Activation Modulation and Recalibration Scheme for Weakly Supervised Semantic Segmentation",
    "authors": [
      "Jie Qin",
      "Jie Wu",
      "Xuefeng Xiao",
      "Lujun Li",
      "Xingang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20108",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20108/19867",
    "published": "2022-02",
    "summary": "Image-level weakly supervised semantic segmentation (WSSS) is a fundamental yet challenging computer vision task facilitating scene understanding and automatic driving. Most existing methods resort to classification-based Class Activation Maps (CAMs) to play as the initial pseudo labels, which tend to focus on the discriminative image regions and lack customized characteristics for the segmentation task. To alleviate this issue, we propose a novel activation modulation and recalibration (AMR) scheme, which leverages a spotlight branch and a compensation branch to obtain weighted CAMs that can provide recalibration supervision and task-specific concepts. Specifically, an attention modulation module (AMM) is employed to rearrange the distribution of feature importance from the channel-spatial sequential perspective, which helps to explicitly model channel-wise interdependencies and spatial encodings to adaptively modulate segmentation-oriented activation responses. Furthermore, we introduce a cross pseudo supervision for dual branches, which can be regarded as a semantic similar regularization to mutually refine two branches. Extensive experiments show that AMR establishes a new state-of-the-art performance on the PASCAL VOC 2012 dataset, surpassing not only current methods trained with the image-level of supervision but also some methods relying on stronger supervision, such as saliency label. Experiments also reveal that our scheme is plug-and-play and can be incorporated with other approaches to boost their performance. Our code is available at: https://github.com/jieqin-ai/AMR.",
    "code_link": "https://github.com/jieqin-ai/AMR"
  },
  "aaai2022_main_transmefatransformer-basedmulti-exposureimagefusionframeworkusingself-supervisedmulti-tasklearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework Using Self-Supervised Multi-Task Learning",
    "authors": [
      "Linhao Qu",
      "Shaolei Liu",
      "Manning Wang",
      "Zhijian Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20109",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20109/19868",
    "published": "2022-02",
    "summary": "In this paper, we propose TransMEF, a transformer-based multi-exposure image fusion framework that uses self-supervised multi-task learning. The framework is based on an encoder-decoder network, which can be trained on large natural image datasets and does not require ground truth fusion images. We design three self-supervised reconstruction tasks according to the characteristics of multi-exposure images and conduct these tasks simultaneously using multi-task learning; through this process, the network can learn the characteristics of multi-exposure images and extract more generalized features. In addition, to compensate for the defect in establishing long-range dependencies in CNN-based architectures, we design an encoder that combines a CNN module with a transformer module. This combination enables the network to focus on both local and global information. We evaluated our method and compared it to 11 competitive traditional and deep learning-based methods on the latest released multi-exposure image fusion benchmark dataset, and our method achieved the best performance in both subjective and objective evaluations. Code will be available at https://github.com/miccaiif/TransMEF.",
    "code_link": "https://github.com/miccaiif/TransMEF"
  },
  "aaai2022_main_deepimplicitstatisticalshapemodelsfor3dmedicalimagedelineation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Implicit Statistical Shape Models for 3D Medical Image Delineation",
    "authors": [
      "Ashwin Raju",
      "Shun Miao",
      "Dakai Jin",
      "Le Lu",
      "Junzhou Huang",
      "Adam P. Harrison"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20110",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20110/19869",
    "published": "2022-02",
    "summary": "3D delineation of anatomical structures is a cardinal goal in medical imaging analysis. Prior to deep learning, statistical shape models (SSMs) that imposed anatomical constraints and produced high quality surfaces were a core technology. Today\u2019s fully-convolutional networks (FCNs), while dominant, do not offer these capabilities. We present deep implicit statistical shape models (DISSMs), a new approach that marries the representation power of deep networks with the benefits of SSMs. DISSMs use an implicit representation to produce compact and descriptive deep surface embeddings that permit statistical models of anatomical variance. To reliably fit anatomically plausible shapes to an image, we introduce a novel rigid and non-rigid pose estimation pipeline that is modelled as a Markov decision process (MDP). Intra-dataset experiments on the task of pathological liver segmentation demonstrate that DISSMs can perform more robustly than four leading FCN models, including nnU-Net + an adversarial prior: reducing the mean Hausdorff distance (HD) by 7.5-14.3 mm and improving the worst case Dice-S\u00f8rensen coefficient (DSC) by 1.2-2.3%. More critically, cross-dataset experiments on an external and highly challenging clinical dataset demonstrate that DISSMs improve the mean DSC and HD by 2.1-5.9% and 9.9-24.5 mm, respectively, and the worst-case DSC by 5.4-7.3%. Supplemental validation on a highly challenging and low-contrast larynx dataset further demonstrate DISSM\u2019s improvements. These improvements are over and above any benefits from representing delineations with high-quality surfaces.",
    "code_link": ""
  },
  "aaai2022_main_decomposethesoundsandpixels,recomposetheevents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Decompose the Sounds and Pixels, Recompose the Events",
    "authors": [
      "Varshanth R. Rao",
      "Md Ibrahim Khalil",
      "Haoda Li",
      "Peng Dai",
      "Juwei Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20111",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20111/19870",
    "published": "2022-02",
    "summary": "In this paper, we propose a framework centering around a novel architecture called the Event Decomposition Recomposition Network (EDRNet) to tackle the Audio-Visual Event (AVE) localization problem in the supervised and weakly supervised settings. AVEs in the real world exhibit common unraveling patterns (termed as Event Progress Checkpoints(EPC)), which humans can perceive through the cooperation of their auditory and visual senses. Unlike earlier methods which attempt to recognize entire event sequences, the EDRNet models EPCs and inter-EPC relationships using stacked temporal convolutions. Based on the postulation that EPC representations are theoretically consistent for an event category, we introduce the State Machine Based Video Fusion, a novel augmentation technique that blends source videos using different EPC template sequences. Additionally, we design a new loss function called the Land-Shore-Sea loss to compactify continuous foreground and background representations. Lastly, to alleviate the issue of confusing events during weak supervision, we propose a prediction stabilization method called Bag to Instance Label Correction. Experiments on the AVE dataset show that our collective framework outperforms the state-of-the-art by a sizable margin.",
    "code_link": ""
  },
  "aaai2022_main_learningfromlabelproportionswithprototypicalcontrastiveclustering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning from Label Proportions with Prototypical Contrastive Clustering",
    "authors": [
      "Laura Elena Cu\u00e9 La Rosa",
      "D\u00e1rio Augusto Borges Oliveira"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20112",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20112/19871",
    "published": "2022-02",
    "summary": "The use of priors to avoid manual labeling for training machine learning methods has received much attention in the last few years. One of the critical subthemes in this regard is Learning from Label Proportions (LLP), where only the information about class proportions is available for training the models. While various LLP training settings verse in the literature, most approaches focus on bag-level label proportions errors, often leading to suboptimal solutions. This paper proposes a new model that jointly uses prototypical contrastive learning and bag-level cluster proportions to implement efficient LLP classification. Our proposal explicitly relaxes the equipartition constraint commonly used in prototypical contrastive learning methods and incorporates the exact cluster proportions into the optimal transport algorithm used for cluster assignments. At inference time, we compute the clusters' assignment, delivering instance-level classification. We experimented with our method on two widely used image classification benchmarks and report a new state-of-art LLP performance, achieving results close to fully supervised methods.",
    "code_link": ""
  },
  "aaai2022_main_beyondlearningfeaturestrainingafully-functionalclassifierwithzeroinstance-levellabels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Beyond Learning Features: Training a Fully-Functional Classifier with ZERO Instance-Level Labels",
    "authors": [
      "Deepak Babu Sam",
      "Abhinav Agarwalla",
      "Venkatesh Babu Radhakrishnan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20113",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20113/19872",
    "published": "2022-02",
    "summary": "We attempt to train deep neural networks for classification without using any labeled data. Existing unsupervised methods, though mine useful clusters or features, require some annotated samples to facilitate the final task-specific predictions. This defeats the true purpose of unsupervised learning and hence we envisage a paradigm of `true' self-supervision, where absolutely no annotated instances are used for training a classifier. The proposed method first pretrains a deep network through self-supervision and performs clustering on the learned features. A classifier layer is then appended to the self-supervised network and is trained by matching the distribution of the predictions to that of a predefined prior. This approach leverages the distribution of labels for supervisory signals and consequently, no image-label pair is needed. Experiments reveal that the method works on major nominal as well as ordinal classification datasets and delivers significant performance.",
    "code_link": "https://github.com/explosion/spaCy"
  },
  "aaai2022_main_reference-guidedpseudo-labelgenerationformedicalsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reference-Guided Pseudo-Label Generation for Medical Semantic Segmentation",
    "authors": [
      "Constantin Marc Seibold",
      "Simon Rei\u00df",
      "Jens Kleesiek",
      "Rainer Stiefelhagen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20114",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20114/19873",
    "published": "2022-02",
    "summary": "Producing densely annotated data is a difficult and tedious task for medical imaging applications.To address this problem, we propose a novel approach to generate supervision for semi-supervised semantic segmentation.We argue that visually similar regions between labeled and unlabeled images likely contain the same semantics and therefore should share their label. Following this thought, we use a small number of labeled images as reference material and match pixels in an unlabeled image to the semantic of the best fitting pixel in a reference set. This way, we avoid pitfalls such as confirmation bias, common in purely prediction-based pseudo-labeling. Since our method does not require any architectural changes or accompanying networks, one can easily insert it into existing frameworks. We achieve the same performance as a standard fully supervised model on X-ray anatomy segmentation, albeit using 95% fewer labeled images. Aside from an in-depth analysis of different aspects of our proposed method, we further demonstrate the effectiveness of our reference-guided learning paradigm by comparing our approach against existing methods for retinal fluid segmentation with competitive performance as we improve upon recent work by up to 15% mean IoU.",
    "code_link": ""
  },
  "aaai2022_main_information-theoreticbiasreductionviacausalviewofspuriouscorrelation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Information-Theoretic Bias Reduction via Causal View of Spurious Correlation",
    "authors": [
      "Seonguk Seo",
      "Joon-Young Lee",
      "Bohyung Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20115",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20115/19874",
    "published": "2022-02",
    "summary": "We propose an information-theoretic bias measurement technique through a causal interpretation of spurious correlation, which is effective to identify the feature-level algorithmic bias by taking advantage of conditional mutual information. Although several bias measurement methods have been proposed and widely investigated to achieve algorithmic fairness in various tasks such as face recognition, their accuracy- or logit-based metrics are susceptible to leading to trivial prediction score adjustment rather than fundamental bias reduction. Hence, we design a novel debiasing framework against the algorithmic bias, which incorporates a bias regularization loss derived by the proposed information-theoretic bias measurement approach. In addition, we present a simple yet effective unsupervised debiasing technique based on stochastic label noise, which does not require the explicit supervision of bias information. The proposed bias measurement and debiasing approaches are validated in diverse realistic scenarios through extensive experiments on multiple standard benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_improvingscenegraphclassificationbyexploitingknowledgefromtexts": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Scene Graph Classification by Exploiting Knowledge from Texts",
    "authors": [
      "Sahand Sharifzadeh",
      "Sina Moayed Baharlou",
      "Martin Schmitt",
      "Hinrich Sch\u00fctze",
      "Volker Tresp"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20116",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20116/19875",
    "published": "2022-02",
    "summary": "Training scene graph classification models requires a large amount of annotated image data. Meanwhile, scene graphs represent relational knowledge that can be modeled with symbolic data from texts or knowledge graphs. While image annotation demands extensive labor, collecting textual descriptions of natural scenes requires less effort. In this work, we investigate whether textual scene descriptions can substitute for annotated image data. To this end, we employ a scene graph classification framework that is trained not only from annotated images but also from symbolic data. In our architecture, the symbolic entities are first mapped to their correspondent image-grounded representations and then fed into the relational reasoning pipeline. Even though a structured form of knowledge, such as the form in knowledge graphs, is not always available, we can generate it from unstructured texts using a transformer-based language model. We show that by fine-tuning the classification pipeline with the extracted knowledge from texts, we can achieve ~8x more accurate results in scene graph classification, ~3x in object classification, and ~1.5x in predicate classification, compared to the supervised baselines with only 1% of the annotated images.",
    "code_link": ""
  },
  "aaai2022_main_reliableinlierevaluationforunsupervisedpointcloudregistration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reliable Inlier Evaluation for Unsupervised Point Cloud Registration",
    "authors": [
      "Yaqi Shen",
      "Le Hui",
      "Haobo Jiang",
      "Jin Xie",
      "Jian Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20117",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20117/19876",
    "published": "2022-02",
    "summary": "Unsupervised point cloud registration algorithm usually suffers from the unsatisfied registration precision in the partially overlapping problem due to the lack of effective inlier evaluation. In this paper, we propose a neighborhood consensus based reliable inlier evaluation method for robust unsupervised point cloud registration. It is expected to capture the discriminative geometric difference between the source neighborhood and the corresponding pseudo target neighborhood for effective inlier distinction. Specifically, our model consists of a matching map refinement module and an inlier evaluation module. In our matching map refinement module, we improve the point-wise matching map estimation by integrating the matching scores of neighbors into it. The aggregated neighborhood information potentially facilitates the discriminative map construction so that high-quality correspondences can be provided for generating the pseudo target point cloud. Based on the observation that the outlier has the significant structure-wise difference between its source neighborhood and corresponding pseudo target neighborhood while this difference for inlier is small, the inlier evaluation module exploits this difference to score the inlier confidence for each estimated correspondence. In particular, we construct an effective graph representation for capturing this geometric difference between the neighborhoods. Finally, with the learned correspondences and the corresponding inlier confidence, we use the weighted SVD algorithm for transformation estimation.Under the unsupervised setting, we exploit the Huber function based global alignment loss, the local neighborhood consensus loss and spatial consistency loss for model optimization. The experimental results on extensive datasets demonstrate that our unsupervised point cloud registration method can yield comparable performance.",
    "code_link": ""
  },
  "aaai2022_main_explainablesurvivalanalysiswithconvolution-involvedvisiontransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
    "authors": [
      "Yifan Shen",
      "Li Liu",
      "Zhihao Tang",
      "Zongyi Chen",
      "Guixiang Ma",
      "Jiyan Dong",
      "Xi\n      Zhang",
      "Lin Yang",
      "Qingfeng Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20118",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20118/19877",
    "published": "2022-02",
    "summary": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
    "code_link": "https://github.com/CamDavidsonPilon/lifelines"
  },
  "aaai2022_main_un-mixrethinkingimagemixturesforunsupervisedvisualrepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Un-mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning",
    "authors": [
      "Zhiqiang Shen",
      "Zechun Liu",
      "Zhuang Liu",
      "Marios Savvides",
      "Trevor Darrell",
      "Eric Xing"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20119",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20119/19878",
    "published": "2022-02",
    "summary": "The recently advanced unsupervised learning approaches use the siamese-like framework to compare two \"views\" from the same image for learning representations. Making the two views distinctive is a core to guarantee that unsupervised methods can learn meaningful information. However, such frameworks are sometimes fragile on overfitting if the augmentations used for generating two views are not strong enough, causing the over-confident issue on the training data. This drawback hinders the model from learning subtle variance and fine-grained information. To address this, in this work we aim to involve the soft distance concept on label space in the contrastive-based unsupervised learning task and let the model be aware of the soft degree of similarity between positive or negative pairs through mixing the input data space, to further work collaboratively for the input and loss spaces. Despite its conceptual simplicity, we show empirically that with the solution -- Unsupervised image mixtures (Un-Mix), we can learn subtler, more robust and generalized representations from the transformed input and corresponding new label space. Extensive experiments are conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet-1K with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, SwAV, etc. Our proposed image mixture and label assignment strategy can obtain consistent improvement by 1~3% following exactly the same hyperparameters and training procedures of the base methods. Code is publicly available at https://github.com/szq0214/Un-Mix.",
    "code_link": ""
  },
  "aaai2022_main_ontheefficacyofsmallself-supervisedcontrastivemodelswithoutdistillationsignals": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals",
    "authors": [
      "Haizhou Shi",
      "Youcai Zhang",
      "Siliang Tang",
      "Wenjie Zhu",
      "Yaqian Li",
      "Yandong\n      Guo",
      "Yueting Zhuang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20120",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20120/19879",
    "published": "2022-02",
    "summary": "It is a consensus that small models perform quite poorly under the paradigm of self-supervised contrastive learning. Existing methods usually adopt a large off-the-shelf model to transfer knowledge to the small one via distillation. Despite their effectiveness, distillation-based methods may not be suitable for some resource-restricted scenarios due to the huge computational expenses of deploying a large model. In this paper, we study the issue of training self-supervised small models without distillation signals. We first evaluate the representation spaces of the small models and make two non-negligible observations: (i) the small models can complete the pretext task without overfitting despite their limited capacity and (ii) they universally suffer the problem of over clustering. Then we verify multiple assumptions that are considered to alleviate the over-clustering phenomenon. Finally, we combine the validated techniques and improve the baseline performances of five small architectures with considerable margins, which indicates that training small self-supervised contrastive models is feasible even without distillation signals. The code is available at https://github.com/WOWNICE/ssl-small.",
    "code_link": "https://github.com/WOWNICE/ssl-small"
  },
  "aaai2022_main_socialinterpretabletreeforpedestriantrajectoryprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Social Interpretable Tree for Pedestrian Trajectory Prediction",
    "authors": [
      "Liushuai Shi",
      "Le Wang",
      "Chengjiang Long",
      "Sanping Zhou",
      "Fang Zheng",
      "Nanning\n      Zheng",
      "Gang Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20121",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20121/19880",
    "published": "2022-02",
    "summary": "Understanding the multiple socially-acceptable future behaviors is an essential task for many vision applications. In this paper, we propose a tree-based method, termed as Social Interpretable Tree (SIT), to address this multi-modal prediction task, where a hand-crafted tree is built depending on the prior information of observed trajectory to model multiple future trajectories. Specifically, a path in the tree from the root to leaf represents an individual possible future trajectory. SIT employs a coarse-to-fine optimization strategy, in which the tree is first built by high-order velocity to balance the complexity and coverage of the tree and then optimized greedily to encourage multimodality. Finally, a teacher-forcing refining operation is used to predict the final fine trajectory. Compared with prior methods which leverage implicit latent variables to represent possible future trajectories, the path in the tree can explicitly explain the rough moving behaviors (e.g., go straight and then turn right), and thus provides better interpretability. Despite the hand-crafted tree, the experimental results on ETH-UCY and Stanford Drone datasets demonstrate that our method is capable of matching or exceeding the performance of state-of-the-art methods. Interestingly, the experiments show that the raw built tree without training outperforms many prior deep neural network based approaches. Meanwhile, our method presents sufficient flexibility in long-term prediction and different best-of-K predictions.",
    "code_link": ""
  },
  "aaai2022_main_p3-netpartmobilityparsingfrompointcloudsequencesvialearningexplicitpointcorrespondence": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "P^3-Net: Part Mobility Parsing from Point Cloud Sequences via Learning Explicit Point Correspondence",
    "authors": [
      "Yahao Shi",
      "Xinyu Cao",
      "Feixiang Lu",
      "Bin Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20122",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20122/19881",
    "published": "2022-02",
    "summary": "Understanding an articulated 3D object with its movable parts is an essential skill for an intelligent agent. This paper presents a novel approach to parse 3D part mobility from point cloud sequences. The key innovation is learning explicit point correspondence from a raw unordered point cloud sequence. We propose a novel deep network called P^3-Net to parallelize trajectory feature extraction and point correspondence establishment, performing joint optimization between them. Specifically, we design a Match-LSTM module to reaggregate point features among different frames by a point correspondence matrix, a.k.a. the matching matrix. To obtain this matrix, an attention module is proposed to calculate the point correspondence. Moreover, we implement a Gumbel-Sinkhorn module to reduce the many-to-one relationship for better point correspondence. We conduct comprehensive evaluations on public benchmarks, including the motion dataset and the PartNet dataset. Results demonstrate that our approach outperforms SOTA methods on various 3D parsing tasks of part mobility, including motion flow prediction, motion part segmentation, and motion attribute (i.e. axis & range) estimation. Moreover, we integrate our approach into a robot perception module to validate its robustness.",
    "code_link": ""
  },
  "aaai2022_main_improvingzero-shotphrasegroundingviareasoningonexternalknowledgeandspatialrelations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Zero-Shot Phrase Grounding via Reasoning on External Knowledge and Spatial Relations",
    "authors": [
      "Zhan Shi",
      "Yilin Shen",
      "Hongxia Jin",
      "Xiaodan Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20123",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20123/19882",
    "published": "2022-02",
    "summary": "Phrase grounding is a multi-modal problem that localizes a particular noun phrase in an image referred to by a text query. In the challenging zero-shot phrase grounding setting, the existing state-of-the-art grounding models have limited capacity in handling the unseen phrases. Humans, however, can ground novel types of objects in images with little effort, significantly benefiting from reasoning with commonsense. In this paper, we design a novel phrase grounding architecture that builds multi-modal knowledge graphs using external knowledge and then performs graph reasoning and spatial relation reasoning to localize the referred nouns phrases. We perform extensive experiments on different zero-shot grounding splits sub-sampled from the Flickr30K Entity and Visual Genome dataset, demonstrating that the proposed framework is orthogonal to backbone image encoders and outperforms the baselines by 2~3% in accuracy, resulting in a significant improvement under the standard evaluation metrics.",
    "code_link": ""
  },
  "aaai2022_main_iterativecontrast-classifyforsemi-supervisedtemporalactionsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Iterative Contrast-Classify for Semi-supervised Temporal Action Segmentation",
    "authors": [
      "Dipika Singhania",
      "Rahul Rahaman",
      "Angela Yao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20124",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20124/19883",
    "published": "2022-02",
    "summary": "Temporal action segmentation classifies the action of each frame in (long) video sequences. Due to the high cost of frame-wise labeling, we propose the first semi-supervised method for temporal action segmentation. Our method hinges on unsupervised representation learning, which, for temporal action segmentation, poses unique challenges. Actions in untrimmed videos vary in length and have unknown labels and start/end times. Ordering of actions across videos may also vary. We propose a novel way to learn frame-wise representations from temporal convolutional networks (TCNs) by clustering input features with added time-proximity conditions and multi-resolution similarity. By merging representation learning with conventional supervised learning, we develop an \"Iterative Contrast-Classify (ICC)'' semi-supervised learning scheme. With more labelled data, ICC progressively improves in performance; ICC semi-supervised learning, with 40% labelled videos, performs similarly to fully-supervised counterparts. Our ICC improves MoF by {+1.8, +5.6, +2.5}% on Breakfast, 50Salads, and GTEA respectively for 100% labelled videos.",
    "code_link": ""
  },
  "aaai2022_main_jpv-netjointpoint-voxelrepresentationsforaccurate3dobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "JPV-Net: Joint Point-Voxel Representations for Accurate 3D Object Detection",
    "authors": [
      "Nan Song",
      "Tianyuan Jiang",
      "Jian Yao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20125",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20125/19884",
    "published": "2022-02",
    "summary": "Voxel and point representations are widely applied in recent 3D object detection tasks from LiDAR point clouds. Voxel representations contribute to efficiently and rapidly locating objects, whereas point representations are capable of describing intra-object spatial relationship for detection refinement. In this work, we aim to exploit the strengths of both two representations, and present a novel two-stage detector, named Joint Point-Voxel Network (JPV-Net). Specifically, our framework is equipped with a Dual Encoders-Fusion Decoder, which consists of the dual encoders to extract voxel features of sketchy 3D scenes and point features rich in geometric context, respectively, and the Feature Propagation Fusion (FP-Fusion) decoder to attentively fuse them from coarse to fine. By making use of the advantages of these features, the refinement network can effectively eliminate false detection and provide better accuracy. Besides, to further develop the perception characteristics of voxel CNN and point backbone, we design two novel intersection-over-union (IoU) estimation modules for proposal generation and refinement, both of which can alleviate the misalignment between the localization and the classification confidence. Extensive experiments on the KITTI dataset and ONCE dataset demonstrate that our proposed JPV-Net outperforms other state-of-the-art methods with remarkable margins.",
    "code_link": ""
  },
  "aaai2022_main_fullyattentionalnetworkforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fully Attentional Network for Semantic Segmentation",
    "authors": [
      "Qi Song",
      "Jie Li",
      "Chenghong Li",
      "Hao Guo",
      "Rui Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20126",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20126/19885",
    "published": "2022-02",
    "summary": "Recent non-local self-attention methods have proven to be effective in capturing long-range dependencies for semantic segmentation. These methods usually form a similarity map of R^(CxC) (by compressing spatial dimensions) or R^(HWxHW) (by compressing channels) to describe the feature relations along either channel or spatial dimensions, where C is the number of channels, H and W are the spatial dimensions of the input feature map. However, such practices tend to condense feature dependencies along the other dimensions, hence causing attention missing, which might lead to inferior results for small/thin categories or inconsistent segmentation inside large objects. To address this problem, we propose a new approach, namely Fully Attentional Network (FLANet), to encode both spatial and channel attentions in a single similarity map while maintaining high computational efficiency. Specifically, for each channel map, our FLANet can harvest feature responses from all other channel maps, and the associated spatial positions as well, through a novel fully attentional module. Our new method has achieved state-of-the-art performance on three challenging semantic segmentation datasets, i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes test set, the ADE20K validation set, and the PASCAL VOC test set, respectively.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedobjectlocalizationwithjointgraphpartition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Object Localization with Joint Graph Partition",
    "authors": [
      "Yukun Su",
      "Guosheng Lin",
      "Yun Hao",
      "Yiwen Cao",
      "Wenjun Wang",
      "Qingyao Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20127",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20127/19886",
    "published": "2022-02",
    "summary": "Object localization aims to generate a tight bounding box for the target object, which is a challenging problem that has been deeply studied in recent years. Since collecting bounding-box labels is time-consuming and laborious, many researchers focus on weakly supervised object localization (WSOL). As the recent appealing self-supervised learning technique shows its powerful function in visual tasks, in this paper, we take the early attempt to explore unsupervised object localization by self-supervision. Specifically, we adopt different geometric transformations to image and utilize their parameters as pseudo labels for self-supervised learning. Then, the class-agnostic activation map (CAAM) is used to highlight the target object potential regions. However, such attention maps merely focus on the most discriminative part of the objects, which will affect the quality of the predicted bounding box. Based on the motivation that the activation maps of different transformations of the same image should be equivariant, we further design a siamese network that encodes the paired images and propose a joint graph cluster partition mechanism in an unsupervised manner to enhance the object co-occurrent regions. To validate the effectiveness of the proposed method, extensive experiments are conducted on CUB-200-2011, Stanford Cars and FGVC-Aircraft datasets. Experimental results show that our method outperforms state-of-the-art methods using the same level of supervision, even outperforms some weakly-supervised methods.",
    "code_link": ""
  },
  "aaai2022_main_correlationfieldforboosting3dobjectdetectioninstructuredscenes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Correlation Field for Boosting 3D Object Detection in Structured Scenes",
    "authors": [
      "Jianhua Sun",
      "Hao-Shu Fang",
      "Xianghui Zhu",
      "Jiefeng Li",
      "Cewu Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20128",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20128/19887",
    "published": "2022-02",
    "summary": "Data augmentation is an efficient way to elevate 3D object detection performance. In this paper, we propose a simple but effective online crop-and-paste data augmentation pipeline for structured 3D point cloud scenes, named CorrelaBoost. Observing that 3D objects should have reasonable relative positions in a structured scene because of the objects' functionalities and natural relationships, we express this correlation as a kind of interactive force. An energy field called Correlation Field can be calculated correspondingly across the whole 3D space. According to the Correlation Field, we propose two data augmentation strategies to explore highly congruent positions that a designated object may be pasted to: 1) Category Consistent Exchanging and 2) Energy Optimized Transformation. We conduct exhaustive experiments on various popular benchmarks with different detection frameworks and the results illustrate that our method brings huge free-lunch improvement and significantly outperforms state-of-the-art approaches in terms of data augmentation. It is worth noting that the performance of VoteNet with mAP@0.5 is improved by 7.7 on ScanNetV2 dataset and 5.0 on SUN RGB-D dataset. Our method is simple to implement and increases few computational overhead.",
    "code_link": ""
  },
  "aaai2022_main_boostsupervisedpretrainingforvisualtransferlearningimplicationsofself-supervisedcontrastiverepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Boost Supervised Pretraining for Visual Transfer Learning: Implications of Self-Supervised Contrastive Representation Learning",
    "authors": [
      "Jinghan Sun",
      "Dong Wei",
      "Kai Ma",
      "Liansheng Wang",
      "Yefeng Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20129",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20129/19888",
    "published": "2022-02",
    "summary": "Unsupervised pretraining based on contrastive learning has made significant progress recently and showed comparable or even superior transfer learning performance to traditional supervised pretraining on various tasks. In this work, we first empirically investigate when and why unsupervised pretraining surpasses supervised counterparts for image classification tasks with a series of control experiments. Besides the commonly used accuracy, we further analyze the results qualitatively with the class activation maps and assess the learned representations quantitatively with the representation entropy and uniformity. Our core finding is that it is the amount of information effectively perceived by the learning model that is crucial to transfer learning, instead of absolute size of the dataset. Based on this finding, we propose Classification Activation Map guided contrastive (CAMtrast) learning which better utilizes the label supervsion to strengthen supervised pretraining, by making the networks perceive more information from the training images. CAMtrast is evaluated with three fundamental visual learning tasks: image recognition, object detection, and semantic segmentation, on various public datasets. Experimental results show that our CAMtrast effectively improves the performance of supervised pretraining, and that its performance is superior to both unsupervised counterparts and a recent related work which similarly attempted improving supervised pretraining.",
    "code_link": "https://github.com/jinghanSunn/CAMtrast"
  },
  "aaai2022_main_dualcontrastivelearningforgeneralfaceforgerydetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dual Contrastive Learning for General Face Forgery Detection",
    "authors": [
      "Ke Sun",
      "Taiping Yao",
      "Shen Chen",
      "Shouhong Ding",
      "Jilin Li",
      "Rongrong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20130",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20130/19889",
    "published": "2022-02",
    "summary": "With various facial manipulation techniques arising, face forgery detection has drawn growing attention due to security concerns. Previous works always formulate face forgery detection as a classi\ufb01cation problem based on cross-entropy loss, which emphasizes category-level differences rather than the essential discrepancies between real and fake faces, limiting model generalization in unseen domains. To address this issue, we propose a novel face forgery detection framework, named Dual Contrastive Learning (DCL), which specially constructs positive and negative paired data and performs designed contrastive learning at different granularities to learn generalized feature representation. Concretely, combined with the hard sample selection strategy, Inter-Instance Contrastive Learning (Inter-ICL) is \ufb01rst proposed to promote task-related discriminative features learning by especially constructing instance pairs. Moreover, to further explore the essential discrepancies, Intra-Instance Contrastive Learning (Intra-ICL) is introduced to focus on the local content inconsistencies prevalent in the forged faces by constructing local region pairs inside instances. Extensive experiments and visualizations on several datasets demonstrate the generalization of our method against the state-of-the-art competitors. Our Code is available at https://github.com/Tencent/TFace.git.",
    "code_link": "https://github.com/Tencent/TFace.git"
  },
  "aaai2022_main_ssatasymmetricsemantic-awaretransformernetworkformakeuptransferandremoval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer and Removal",
    "authors": [
      "Zhaoyang Sun",
      "Yaxiong Chen",
      "Shengwu Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20131",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20131/19890",
    "published": "2022-02",
    "summary": "Makeup transfer is not only to extract the makeup style of the reference image, but also to render the makeup style to the semantic corresponding position of the target image. However, most existing methods focus on the former and ignore the latter, resulting in a failure to achieve desired results. To solve the above problems, we propose a unified Symmetric Semantic-Aware Transformer (SSAT) network, which incorporates semantic correspondence learning to realize makeup transfer and removal simultaneously. In SSAT, a novel Symmetric Semantic Corresponding Feature Transfer (SSCFT) module and a weakly supervised semantic loss are proposed to model and facilitate the establishment of accurate semantic correspondence. In the generation process, the extracted makeup features are spatially distorted by SSCFT to achieve semantic alignment with the target image, then the distorted makeup features are combined with unmodified makeup irrelevant features to produce the final result. Experiments show that our method obtains more visually accurate makeup transfer results, and user study in comparison with other state-of-the-art makeup transfer methods reflects the superiority of our method. Besides, we verify the robustness of the proposed method in the difference of expression and pose, object occlusion scenes, and extend it to video makeup transfer.",
    "code_link": ""
  },
  "aaai2022_main_adversarialbonelengthattackonactionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Bone Length Attack on Action Recognition",
    "authors": [
      "Nariki Tanaka",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20132",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20132/19891",
    "published": "2022-02",
    "summary": "Skeleton-based action recognition models have recently been shown to be vulnerable to adversarial attacks. Compared to adversarial attacks on images, perturbations to skeletons are typically bounded to a lower dimension of approximately 100 per frame. This lower-dimensional setting makes it more difficult to generate imperceptible perturbations. Existing attacks resolve this by exploiting the temporal structure of the skeleton motion so that the perturbation dimension increases to thousands. In this paper, we show that adversarial attacks can be performed on skeleton-based action recognition models, even in a significantly low-dimensional setting without any temporal manipulation. Specifically, we restrict the perturbations to the lengths of the skeleton's bones, which allows an adversary to manipulate only approximately 30 effective dimensions. We conducted experiments on the NTU RGB+D and HDM05 datasets and demonstrate that the proposed attack successfully deceived models with sometimes greater than 90% success rate by small perturbations. Furthermore, we discovered an interesting phenomenon: in our low-dimensional setting, the adversarial training with the bone length attack shares a similar property with data augmentation, and it not only improves the adversarial robustness but also improves the classification accuracy on the original data. This is an interesting counterexample of the trade-off between adversarial robustness and clean accuracy, which has been widely observed in studies on adversarial training in the high-dimensional regime.",
    "code_link": ""
  },
  "aaai2022_main_sparsemlpforimagerecognitionisself-attentionreallynecessary?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
    "authors": [
      "Chuanxin Tang",
      "Yucheng Zhao",
      "Guangting Wang",
      "Chong Luo",
      "Wenxuan Xie",
      "Wenjun Zeng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20133",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20133/19892",
    "published": "2022-02",
    "summary": "Transformers have sprung up in the field of computer vision. In this work, we explore whether the core self-attention module in Transformer is the key to achieving excellent performance in image recognition. To this end, we build an attention-free network called sMLPNet based on the existing MLP-based vision models. Specifically, we replace the MLP module in the token-mixing step with a novel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along the axial directions and the parameters are shared among rows or columns. By sparse connection and weight sharing, sMLP module significantly reduces the number of model parameters and computational complexity, avoiding the common over-fitting problem that plagues the performance of MLP-like models. When only trained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1 accuracy with only 24M parameters, which is much better than most CNNs and vision Transformers under the same model size constraint. When scaling up to 66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the state-of-the-art Swin Transformer. The success of sMLPNet suggests that the self-attention mechanism is not necessarily a silver bullet in computer vision. The code and models are publicly available at https://github.com/microsoft/SPACH.",
    "code_link": "https://github.com/microsoft/SPACH"
  },
  "aaai2022_main_notallvoxelsareequalsemanticscenecompletionfromthepoint-voxelperspective": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective",
    "authors": [
      "Jiaxiang Tang",
      "Xiaokang Chen",
      "Jingbo Wang",
      "Gang Zeng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20134",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20134/19893",
    "published": "2022-02",
    "summary": "We revisit Semantic Scene Completion (SSC), a useful task to predict the semantic and occupancy representation of 3D scenes, in this paper. A number of methods for this task are always based on voxelized scene representations. Although voxel representations keep local structures of the scene, these methods suffer from heavy computation redundancy due to the existence of visible empty voxels when the network goes deeper. To address this dilemma, we propose our novel point-voxel aggregation network for this task. We first transfer the voxelized scenes to point clouds by removing these visible empty voxels and adopt a deep point stream to capture semantic information from the scene efficiently. Meanwhile, a light-weight voxel stream containing only two 3D convolution layers preserves local structures of the voxelized scenes. Furthermore, we design an anisotropic voxel aggregation operator to fuse the structure details from the voxel stream into the point stream, and a semantic-aware propagation module to enhance the up-sampling process in the point stream by semantic labels. We demonstrate that our model surpasses state-of-the-arts on two benchmarks by a large margin, with only the depth images as input.",
    "code_link": ""
  },
  "aaai2022_main_transferlearningforcolorconstancyviastatisticperspective": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transfer Learning for Color Constancy via Statistic Perspective",
    "authors": [
      "Yuxiang Tang",
      "Xuejing Kang",
      "Chunxiao Li",
      "Zhaowen Lin",
      "Anlong Ming"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20135",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20135/19894",
    "published": "2022-02",
    "summary": "Color Constancy aims to correct image color casts caused by scene illumination. Recently, although the deep learning approaches have remarkably improved on single-camera data, these models still suffer from the seriously insufficient data problem, resulting in shallow model capacity and degradation in multi-camera settings. In this paper, to alleviate this problem, we present a Transfer Learning Color Constancy (TLCC) method that leverages cross-camera RAW data and massive unlabeled sRGB data to support training. Specifically, TLCC consists of the Statistic Estimation Scheme (SE-Scheme) and Color-Guided Adaption Branch (CGA-Branch). SE-Scheme builds a statistic perspective to map the camera-related illumination labels into camera-agnostic form and produce pseudo labels for sRGB data, which greatly expands data for joint training. Then, CGA-Branch further promotes efficient transfer learning from sRGB to RAW data by extracting color information to regularize the backbone's features adaptively. Experimental results show the TLCC has overcome the data limitation and model degradation, outperforming the state-of-the-art performance on popular benchmarks. Moreover, the experiments also prove the TLCC is capable of learning new scenes information from sRGB data to improve accuracy on the RAW images with similar scenes.",
    "code_link": ""
  },
  "aaai2022_main_tvtthree-wayvisiontransformerthroughmulti-modalhyperspherelearningforzero-shotsketch-basedimageretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval",
    "authors": [
      "Jialin Tian",
      "Xing Xu",
      "Fumin Shen",
      "Yang Yang",
      "Heng Tao Shen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20136",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20136/19895",
    "published": "2022-02",
    "summary": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
    "code_link": ""
  },
  "aaai2022_main_guidedmix-netsemi-supervisedsemanticsegmentationbyusinglabeledimagesasreference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GuidedMix-Net: Semi-supervised Semantic Segmentation by Using Labeled Images as Reference",
    "authors": [
      "Peng Tu",
      "Yawen Huang",
      "Feng Zheng",
      "Zhenyu He",
      "Liujuan Cao",
      "Ling Shao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20137",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20137/19896",
    "published": "2022-02",
    "summary": "Semi-supervised learning is a challenging problem which aims to construct a model by learning from limited labeled examples. Numerous methods for this task focus on utilizing the predictions of unlabeled instances consistency alone to regularize networks. However, treating labeled and unlabeled data separately often leads to the discarding of mass prior knowledge learned from the labeled examples.In this paper, we propose a novel method for semi-supervised semantic segmentation named GuidedMix-Net, by leveraging labeled information to guide the learning of unlabeled instances. Specifically, GuidedMix-Net employs three operations: 1) interpolation of similar labeled-unlabeled image pairs; 2) transfer of mutual information; 3) generalization of pseudo masks. It enables segmentation models can learning the higher-quality pseudo masks of unlabeled data by transfer the knowledge from labeled samples to unlabeled data. Along with supervised learning for labeled data, the prediction of unlabeled data is jointly learned with the generated pseudo masks from the mixed data.Extensive experiments on PASCAL VOC 2012, and Cityscapes demonstrate the effectiveness of our GuidedMix-Net, which achieves competitive segmentation accuracy and significantly improves the mIoU over 7$\\%$ compared to previous approaches.",
    "code_link": ""
  },
  "aaai2022_main_mtldesclookingwidertodescribebetter": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MTLDesc: Looking Wider to Describe Better",
    "authors": [
      "Changwei Wang",
      "Rongtao Xu",
      "Yuyang Zhang",
      "Shibiao Xu",
      "Weiliang Meng",
      "Bin\n      Fan",
      "Xiaopeng Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20138",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20138/19897",
    "published": "2022-02",
    "summary": "Limited by the locality of convolutional neural networks, most existing local features description methods only learn local descriptors with local information and lack awareness of global and surrounding spatial context. In this work, we focus on making local descriptors ``look wider to describe better'' by learning local Descriptors with More Than Local information (MTLDesc). Specifically, we resort to context augmentation and spatial attention mechanism to make the descriptors obtain non-local awareness. First, Adaptive Global Context Augmented Module and Diverse Local Context Augmented Module are proposed to construct robust local descriptors with context information from global to local. Second, we propose the Consistent Attention Weighted Triplet Loss to leverage spatial attention awareness in both optimization and matching of local descriptors. Third, Local Features Detection with Feature Pyramid is proposed to obtain more stable and accurate keypoints localization. With the above innovations, the performance of the proposed MTLDesc significantly surpasses the current state-of-the-art local descriptors on HPatches, Aachen Day-Night localization and InLoc indoor localization benchmarks. Our code is available at https://github.com/vignywang/MTLDesc.",
    "code_link": "https://github.com/vignywang/MTLDesc"
  },
  "aaai2022_main_activeboundarylossforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Active Boundary Loss for Semantic Segmentation",
    "authors": [
      "Chi Wang",
      "Yunke Zhang",
      "Miaomiao Cui",
      "Peiran Ren",
      "Yin Yang",
      "Xuansong Xie",
      "Xian-Sheng Hua",
      "Hujun Bao",
      "Weiwei Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20139",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20139/19898",
    "published": "2022-02",
    "summary": "This paper proposes a novel active boundary loss for semantic segmentation. It can progressively encourage the alignment between predicted boundaries and ground-truth boundaries during end-to-end training, which is not explicitly enforced in commonly used cross-entropy loss. Based on the predicted boundaries detected from the segmentation results using current network parameters, we formulate the boundary alignment problem as a differentiable direction vector prediction problem to guide the movement of predicted boundaries in each iteration. Our loss is model-agnostic and can be plugged in to the training of segmentation networks to improve the boundary details. Experimental results show that training with the active boundary loss can effectively improve the boundary F-score and mean Intersection-over-Union on challenging image and video object segmentation datasets.",
    "code_link": ""
  },
  "aaai2022_main_online-updatedhigh-ordercollaborativenetworksforsingleimagederaining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online-Updated High-Order Collaborative Networks for Single Image Deraining",
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Xiao-Ming Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20140",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20140/19899",
    "published": "2022-02",
    "summary": "Single image deraining is an important and challenging task for some downstream artificial intelligence applications such as video surveillance and self-driving systems. Most of the existing deep-learning-based methods constrain the network to generate derained images but few of them explore features from intermediate layers, different levels, and different modules which are beneficial for rain streaks removal. In this paper, we propose a high-order collaborative network with multi-scale compact constraints and a bidirectional scale-content similarity mining module to exploit features from deep networks externally and internally for rain streaks removal. Externally, we design a deraining framework with three sub-networks trained in a collaborative manner, where the bottom network transmits intermediate features to the middle network which also receives shallower rainy features from the top network and sends back features to the bottom network. Internally, we enforce multi-scale compact constraints on the intermediate layers of deep networks to learn useful features via a Laplacian pyramid. Further, we develop a bidirectional scale-content similarity mining module to explore features at different scales in a down-to-up and up-to-down manner. To improve the model performance on real-world images, we propose an online-update learning approach, which uses real-world rainy images to fine-tune the network and update the deraining results in a self-supervised manner. Extensive experiments demonstrate that our proposed method performs favorably against eleven state-of-the-art methods on five public synthetic datasets and one real-world dataset.",
    "code_link": ""
  },
  "aaai2022_main_fcalearninga3dfull-coveragevehiclecamouflageformulti-viewphysicaladversarialattack": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FCA: Learning a 3D Full-Coverage Vehicle Camouflage for Multi-View Physical Adversarial Attack",
    "authors": [
      "Donghua Wang",
      "Tingsong Jiang",
      "Jialiang Sun",
      "Weien Zhou",
      "Zhiqiang Gong",
      "Xiaoya Zhang",
      "Wen Yao",
      "Xiaoqian Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20141",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20141/19900",
    "published": "2022-02",
    "summary": "Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle\u2019s surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors.",
    "code_link": "https://github.com/lufficc/SSD"
  },
  "aaai2022_main_whenshiftoperationmeetsvisiontransformeranextremelysimplealternativetoattentionmechanism": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20142",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20142/19901",
    "published": "2022-02",
    "summary": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
    "code_link": "https://github.com/microsoft/SPACH"
  },
  "aaai2022_main_self-supervisedrepresentationlearningframeworkforremotephysiologicalmeasurementusingspatiotemporalaugmentationloss": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Representation Learning Framework for Remote Physiological Measurement Using Spatiotemporal Augmentation Loss",
    "authors": [
      "Hao Wang",
      "Euijoon Ahn",
      "Jinman Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20143",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20143/19902",
    "published": "2022-02",
    "summary": "Recent advances in supervised deep learning methods are enabling remote measurements of photoplethysmography-based physiological signals using facial videos. The performance of these supervised methods, however, are dependent on the availability of large labelled data. Contrastive learning as a self-supervised method has recently achieved state-of-the-art performances in learning representative data features by maximising mutual information between different augmented views. However, existing data augmentation techniques for contrastive learning are not designed to learn physiological signals from videos and often fail when there are complicated noise and subtle and periodic colour/shape variations between video frames. To address these problems, we present a novel self-supervised spatiotemporal learning framework for remote physiological signal representation learning, where there is a lack of labelled training data. Firstly, we propose a landmark-based spatial augmentation that splits the face into several informative parts based on the Shafer\u2019s dichromatic re\ufb02ection model to characterise subtle skin colour fluctuations. We also formulate a sparsity-based temporal augmentation exploiting Nyquist\u2013Shannon sampling theorem to effectively capture periodic temporal changes by modelling physiological signal features. Furthermore, we introduce a constrained spatiotemporal loss which generates pseudo-labels for augmented video clips. It is used to regulate the training process and handle complicated noise. We evaluated our framework on 3 public datasets and demonstrated superior performances than other self-supervised methods and achieved competitive accuracy compared to the state-of-the-art supervised methods. Code is available at https://github.com/Dylan-H-Wang/SLF-RPM.",
    "code_link": "https://github.com/Dylan-H-Wang/SLF-RPM"
  },
  "aaai2022_main_uctransnetrethinkingtheskipconnectionsinu-netfromachannel-wiseperspectivewithtransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer",
    "authors": [
      "Haonan Wang",
      "Peng Cao",
      "Jiaqi Wang",
      "Osmar R. Zaiane"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20144",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20144/19903",
    "published": "2022-02",
    "summary": "Most recent semantic segmentation methods adopt a U-Net framework with an encoder-decoder architecture. It is still challenging for U-Net with a simple skip connection scheme to model the global multi-scale context: 1) Not each skip connection setting is effective due to the issue of incompatible feature sets of encoder and decoder stage, even some skip connection negatively influence the segmentation performance; 2) The original U-Net is worse than the one without any skip connection on some datasets. Based on our findings, we propose a new segmentation framework, named UCTransNet (with a proposed CTrans module in U-Net), from the channel perspective with attention mechanism. Specifically, the CTrans (Channel Transformer) module is an alternate of the U-Net skip connections, which consists of a sub-module to conduct the multi-scale Channel Cross fusion with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention (named CCA) to guide the fused multi-scale channel-wise information to effectively connect to the decoder features for eliminating the ambiguity. Hence, the proposed connection consisting of the CCT and CCA is able to replace the original skip connection to solve the semantic gaps for an accurate automatic medical image segmentation. The experimental results suggest that our UCTransNet produces more precise segmentation performance and achieves consistent improvements over the state-of-the-art for semantic segmentation across different datasets and conventional architectures involving transformer or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.",
    "code_link": "https://github.com/McGregorWwww/UCTransNet"
  },
  "aaai2022_main_renovateyourselfcalibratingfeaturerepresentationofmisclassifiedpixelsforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Renovate Yourself: Calibrating Feature Representation of Misclassified Pixels for Semantic Segmentation",
    "authors": [
      "Hualiang Wang",
      "Huanpeng Chu",
      "Siming FU",
      "Zuozhu Liu",
      "Haoji Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20145",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20145/19904",
    "published": "2022-02",
    "summary": "Existing image semantic segmentation methods favor learning consistent representations by extracting long-range contextual features with the attention, multi-scale, or graph aggregation strategies. These methods usually treat the misclassified and correctly classified pixels equally, hence misleading the optimization process and causing inconsistent intra-class pixel feature representations in the embedding space during learning. In this paper, we propose the auxiliary representation calibration head (RCH), which consists of the image decoupling, prototype clustering, error calibration modules and a metric loss function, to calibrate these error-prone feature representations for better intra-class consistency and segmentation performance. RCH could be incorporated into the hidden layers, trained together with the segmentation networks, and decoupled in the inference stage without additional parameters. Experimental results show that our method could significantly boost the performance of current segmentation methods on multiple datasets (e.g., we outperform the original HRNet and OCRNet by 1.1% and 0.9% mIoU on the Cityscapes test set). Codes are available at https://github.com/VipaiLab/RCH.",
    "code_link": ""
  },
  "aaai2022_main_separatedcontrastivelearningfororgan-at-riskandgross-tumor-volumesegmentationwithlimitedannotation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Separated Contrastive Learning for Organ-at-Risk and Gross-Tumor-Volume Segmentation with Limited Annotation",
    "authors": [
      "Jiacheng Wang",
      "Xiaomeng Li",
      "Yiming Han",
      "Jing Qin",
      "Liansheng Wang",
      "Zhou\n      Qichao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20146",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20146/19905",
    "published": "2022-02",
    "summary": "Automatic delineation of organ-at-risk (OAR) and gross-tumor-volume (GTV) is of great significance for radiotherapy planning. However, it is a challenging task to learn powerful representations for accurate delineation under limited pixel (voxel)-wise annotations. Contrastive learning at pixel-level can alleviate the dependency on annotations by learning dense representations from unlabeled data. Recent studies in this direction design various contrastive losses on the feature maps, to yield discriminative features for each pixel in the map. However, pixels in the same map inevitably share semantics to be closer than they actually are, which may affect the discrimination of pixels in the same map and lead to the unfair comparison to pixels in other maps. To address these issues, we propose a separated region-level contrastive learning scheme, namely SepaReg, the core of which is to separate each image into regions and encode each region separately. Specifically, SepaReg comprises two components: a structure-aware image separation (SIS) module and an intra- and inter-organ distillation (IID) module. The SIS is proposed to operate on the image set to rebuild a region set under the guidance of structural information. The inter-organ representation will be learned from this set via typical contrastive losses cross regions. On the other hand, the IID is proposed to tackle the quantity imbalance in the region set as tiny organs may produce fewer regions, by exploiting intra-organ representations. We conducted extensive experiments to evaluate the proposed model on a public dataset and two private datasets. The experimental results demonstrate the effectiveness of the proposed model, consistently achieving better performance than state-of-the-art approaches. Code is available at https://github.com/jcwang123/Separate_CL.",
    "code_link": "https://github.com/jcwang123/Separate"
  },
  "aaai2022_main_contrastivequantizationwithcodememoryforunsupervisedimageretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval",
    "authors": [
      "Jinpeng Wang",
      "Ziyun Zeng",
      "Bin Chen",
      "Tao Dai",
      "Shu-Tao Xia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20147",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20147/19906",
    "published": "2022-02",
    "summary": "The high efficiency in computation and storage makes hashing (including binary hashing and quantization) a common strategy in large-scale retrieval systems. To alleviate the reliance on expensive annotations, unsupervised deep hashing becomes an important research problem. This paper provides a novel solution to unsupervised deep quantization, namely Contrastive Quantization with Code Memory (MeCoQ). Different from existing reconstruction-based strategies, we learn unsupervised binary descriptors by contrastive learning, which can better capture discriminative visual semantics. Besides, we uncover that codeword diversity regularization is critical to prevent contrastive learning-based quantization from model degeneration. Moreover, we introduce a novel quantization code memory module that boosts contrastive learning with lower feature drift than conventional feature memories. Extensive experiments on benchmark datasets show that MeCoQ outperforms state-of-the-art methods. Code and configurations are publicly released.",
    "code_link": ""
  },
  "aaai2022_main_learningtemporallyandsemanticallyconsistentunpairedvideo-to-videotranslationthroughpseudo-supervisionfromsyntheticopticalflow": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Temporally and Semantically Consistent Unpaired Video-to-Video Translation through Pseudo-Supervision from Synthetic Optical Flow",
    "authors": [
      "Kaihong Wang",
      "Kumar Akash",
      "Teruhisa Misu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20148",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20148/19907",
    "published": "2022-02",
    "summary": "Unpaired video-to-video translation aims to translate videos between a source and a target domain without the need of paired training data, making it more feasible for real applications. Unfortunately, the translated videos generally suffer from temporal and semantic inconsistency. To address this, many existing works adopt spatiotemporal consistency constraints incorporating temporal information based on motion estimation. However, the inaccuracies in the estimation of motion deteriorate the quality of the guidance towards spatiotemporal consistency, which leads to unstable translation. In this work, we propose a novel paradigm that regularizes the spatiotemporal consistency by synthesizing motions in input videos with the generated optical flow instead of estimating them. Therefore, the synthetic motion can be applied in the regularization paradigm to keep motions consistent across domains without the risk of errors in motion estimation. Thereafter, we utilize our unsupervised recycle and unsupervised spatial loss, guided by the pseudo-supervision provided by the synthetic optical flow, to accurately enforce spatiotemporal consistency in both domains. Experiments show that our method is versatile in various scenarios and achieves state-of-the-art performance in generating temporally and semantically consistent videos. Code is available at: https://github.com/wangkaihong/Unsup_Recycle_GAN/.",
    "code_link": "https://github.com/wangkaihong/Unsup"
  },
  "aaai2022_main_cross-datasetcollaborativelearningforsemanticsegmentationinautonomousdriving": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Dataset Collaborative Learning for Semantic Segmentation in Autonomous Driving",
    "authors": [
      "Li Wang",
      "Dong Li",
      "Han Liu",
      "JinZhang Peng",
      "Lu Tian",
      "Yi Shan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20149",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20149/19908",
    "published": "2022-02",
    "summary": "Semantic segmentation is an important task for scene understanding in self-driving cars and robotics, which aims to assign dense labels for all pixels in the image. Existing work typically improves semantic segmentation performance by exploring different network architectures on a target dataset. Little attention has been paid to build a unified system by simultaneously learning from multiple datasets due to the inherent distribution shift across different datasets. In this paper, we propose a simple, flexible, and general method for semantic segmentation, termed Cross-Dataset Collaborative Learning (CDCL). Our goal is to train a unified model for improving the performance in each dataset by leveraging information from all the datasets. Specifically, we first introduce a family of Dataset-Aware Blocks (DAB) as the fundamental computing units of the network, which help capture homogeneous convolutional representations and heterogeneous statistics across different datasets. Second, we present a Dataset Alternation Training (DAT) mechanism to facilitate the collaborative optimization procedure. We conduct extensive evaluations on diverse semantic segmentation datasets for autonomous driving. Experiments demonstrate that our method consistently achieves notable improvements over prior single-dataset and cross-dataset training methods without introducing extra FLOPs. Particularly, with the same architecture of PSPNet (ResNet-18), our method outperforms the single-dataset baseline by 5.65\\%, 6.57\\%, and 5.79\\% mIoU on the validation sets of Cityscapes, BDD100K, CamVid, respectively. We also apply CDCL for point cloud 3D semantic segmentation and achieve improved performance, which further validates the superiority and generality of our method. Code and models will be released.",
    "code_link": ""
  },
  "aaai2022_main_scaledrelumattersfortrainingvisiontransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Scaled ReLU Matters for Training Vision Transformers",
    "authors": [
      "Pichao Wang",
      "Xue Wang",
      "Hao Luo",
      "Jingkai Zhou",
      "Zhipeng Zhou",
      "Fan Wang",
      "Hao\n      Li",
      "Rong Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20150",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20150/19909",
    "published": "2022-02",
    "summary": "Vision transformers (ViTs) have been an alternative design paradigm to convolutional neural networks (CNNs). However, the training of ViTs is much harder than CNNs, as it is sensitive to the training parameters, such as learning rate, optimizer and warmup epoch. The reasons for training difficulty are empirically analysed in the paper Early Convolutions Help Transformers See Better, and the authors conjecture that the issue lies with the patchify-stem of ViT models. In this paper, we further investigate this problem and extend the above conclusion: only early convolutions do not help for stable training, but the scaled ReLU operation in the convolutional stem (conv-stem) matters. We verify, both theoretically and empirically, that scaled ReLU in conv-stem not only improves training stabilization, but also increases the diversity of patch tokens, thus boosting peak performance with a large margin via adding few parameters and flops. In addition, extensive experiments are conducted to demonstrate that previous ViTs are far from being well trained, further showing that ViTs have great potential to be a better substitute of CNNs.",
    "code_link": ""
  },
  "aaai2022_main_cqa-facecontrastivequality-awareattentionsforfacerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CQA-Face: Contrastive Quality-Aware Attentions for Face Recognition",
    "authors": [
      "Qiangchang Wang",
      "Guodong Guo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20151",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20151/19910",
    "published": "2022-02",
    "summary": "Few existing face recognition (FR) models take local representations into account. Although some works achieved this by extracting features on cropped parts around face landmarks, landmark detection may be inaccurate or even fail in some extreme cases. Recently, without relying on landmarks, attention-based networks can focus on useful parts automatically. However, there are two issues: 1) It is noticed that these approaches focus on few facial parts, while missing other potentially discriminative regions. This can cause performance drops when emphasized facial parts are invisible under heavy occlusions (e.g. face masks) or large pose variations; 2) Different facial parts may appear at various quality caused by occlusion, blur, or illumination changes. In this paper, we propose contrastive quality-aware attentions, called CQA-Face, to address these two issues. First, a Contrastive Attention Learning (CAL) module is proposed, pushing models to explore comprehensive facial parts. Consequently, more useful parts can help identification if some facial parts are invisible. Second, a Quality-Aware Network (QAN) is developed to emphasize important regions and suppress noisy parts in a global scope. Thus, our CQA-Face model is developed by integrating the CAL with QAN, which extracts diverse quality-aware local representations. It outperforms the state-of-the-art methods on several benchmarks, demonstrating its effectiveness and usefulness.",
    "code_link": ""
  },
  "aaai2022_main_category-specificnuanceexplorationnetworkforfine-grainedobjectretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Category-Specific Nuance Exploration Network for Fine-Grained Object Retrieval",
    "authors": [
      "Shijie Wang",
      "Zhihui Wang",
      "Haojie Li",
      "Wanli Ouyang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20152",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20152/19911",
    "published": "2022-02",
    "summary": "Employing additional prior knowledge to model local features as a final fine-grained object representation has become a trend for fine-grained object retrieval (FGOR). A potential limitation of these methods is that they only focus on common parts across the dataset (e.g. head, body or even leg) by introducing additional prior knowledge, but the retrieval of a fine-grained object may rely on category-specific nuances that contribute to category prediction. To handle this limitation, we propose an end-to-end Category-specific Nuance Exploration Network (CNENet) that elaborately discovers category-specific nuances that contribute to category prediction, and semantically aligns these nuances grouped by subcategory without any additional prior knowledge, to directly emphasize the discrepancy among subcategories. Specifically, we design a Nuance Modelling Module that adaptively predicts a group of category-specific response (CARE) maps via implicitly digging into category-specific nuances, specifying the locations and scales for category-specific nuances. Upon this, two nuance regularizations are proposed: 1) semantic discrete loss that forces each CARE map to attend to different spatial regions to capture diverse nuances; 2) semantic alignment loss that constructs a consistent semantic correspondence for each CARE map of the same order with the same subcategory via guaranteeing each instance and its transformed counterpart to be spatially aligned. Moreover, we propose a Nuance Expansion Module, which exploits context appearance information of discovered nuances and refines the prediction of current nuance by its similar neighbors, leading to further improvement on nuance consistency and completeness. Extensive experiments validate that our CNENet consistently yields the best performance under the same settings against most competitive approaches on CUB Birds, Stanford Cars, and FGVC Aircraft datasets.",
    "code_link": ""
  },
  "aaai2022_main_detail-preservingtransformerforlightfieldimagesuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Detail-Preserving Transformer for Light Field Image Super-resolution",
    "authors": [
      "Shunzhou Wang",
      "Tianfei Zhou",
      "Yao Lu",
      "Huijun Di"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20153",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20153/19912",
    "published": "2022-02",
    "summary": "Recently, numerous algorithms have been developed to tackle the problem of light field super-resolution (LFSR), i.e., super-resolving low-resolution light fields to gain high-resolution views. Despite delivering encouraging results, these approaches are all convolution-based, and are naturally weak in global relation modeling of sub-aperture images necessarily to characterize the inherent structure of light fields. In this paper, we put forth a novel formulation built upon Transformers, by treating LFSR as a sequence-to-sequence reconstruction task. In particular, our model regards sub-aperture images of each vertical or horizontal angular view as a sequence, and establishes long-range geometric dependencies within each sequence via a spatial-angular locally-enhanced self-attention layer, which maintains the locality of each sub-aperture image as well. Additionally, to better recover image details, we propose a detail-preserving Transformer (termed as DPT), by leveraging gradient maps of light field to guide the sequence learning. DPT consists of two branches, with each associated with a Transformer for learning from an original or gradient image sequence. The two branches are finally fused to obtain comprehensive feature representations for reconstruction. Evaluations are conducted on a number of light field datasets, including real-world scenes and synthetic data. The proposed method achieves superior performance comparing with other state-of-the-art schemes. Our code is publicly available at: https://github.com/BITszwang/DPT.",
    "code_link": "https://github.com/BITszwang/DPT"
  },
  "aaai2022_main_one-shottalkingfacegenerationfromsingle-speakeraudio-visualcorrelationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation Learning",
    "authors": [
      "Suzhen Wang",
      "Lincheng Li",
      "Yu Ding",
      "Xin Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20154",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20154/19913",
    "published": "2022-02",
    "summary": "Audio-driven one-shot talking face generation methods are usually trained on video resources of various persons. However, their created videos often suffer unnatural mouth shapes and asynchronous lips because those methods struggle to learn a consistent speech style from different speakers. We observe that it would be much easier to learn a consistent speech style from a specific speaker, which leads to authentic mouth movements. Hence, we propose a novel one-shot talking face generation framework by exploring consistent correlations between audio and visual motions from a specific speaker and then transferring audio-driven motion fields to a reference image. Specifically, we develop an Audio-Visual Correlation Transformer (AVCT) that aims to infer talking motions represented by keypoint based dense motion fields from an input audio. In particular, considering audio may come from different identities in deployment, we incorporate phonemes to represent audio signals. In this manner, our AVCT can inherently generalize to audio spoken by other identities. Moreover, as face keypoints are used to represent speakers, AVCT is agnostic against appearances of the training speaker, and thus allows us to manipulate face images of different identities readily. Considering different face shapes lead to different motions, a motion field transfer module is exploited to reduce the audio-driven dense motion field gap between the training identity and the one-shot reference. Once we obtained the dense motion field of the reference image, we employ an image renderer to generate its talking face videos from an audio clip. Thanks to our learned consistent speaking style, our method generates authentic mouth shapes and vivid movements. Extensive experiments demonstrate that our synthesized videos outperform the state-of-the-art in terms of visual quality and lip-sync.",
    "code_link": "https://github.com/FuxiVirtualHuman/AAAI22-one-shottalking-face"
  },
  "aaai2022_main_pose-guidedfeaturedisentanglingforoccludedpersonre-identificationbasedontransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pose-Guided Feature Disentangling for Occluded Person Re-identification Based on Transformer",
    "authors": [
      "Tao Wang",
      "Hong Liu",
      "Pinhao Song",
      "Tianyu Guo",
      "Wei Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20155",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20155/19914",
    "published": "2022-02",
    "summary": "Occluded person re-identification is a challenging task as human body parts could be occluded by some obstacles (e.g. trees, cars, and pedestrians) in certain scenes. Some existing pose-guided methods solve this problem by aligning body parts according to graph matching, but these graph-based methods are not intuitive and complicated. Therefore, we propose a transformer-based Pose-guided Feature Disentangling (PFD) method by utilizing pose information to clearly disentangle semantic components (e.g. human body or joint parts) and selectively match non-occluded parts correspondingly. First, Vision Transformer (ViT) is used to extract the patch features with its strong capability. Second, to preliminarily disentangle the pose information from patch information, the matching and distributing mechanism is leveraged in Pose-guided Feature Aggregation (PFA) module. Third, a set of learnable semantic views are introduced in transformer decoder to implicitly enhance the disentangled body part features. However, those semantic views are not guaranteed to be related to the body without additional supervision. Therefore, Pose-View Matching (PVM) module is proposed to explicitly match visible body parts and automatically separate occlusion features. Fourth, to better prevent the interference of occlusions, we design a Pose-guided Push Loss to emphasize the features of visible body parts. Extensive experiments over five challenging datasets for two tasks (occluded and holistic Re-ID) demonstrate that our proposed PFD is superior promising, which performs favorably against state-of-the-art methods. Code is available at https://github.com/WangTaoAs/PFD_Net",
    "code_link": "https://github.com/WangTaoAs/PFD"
  },
  "aaai2022_main_ffnetfrequencyfusionnetworkforsemanticscenecompletion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FFNet: Frequency Fusion Network for Semantic Scene Completion",
    "authors": [
      "Xuzhi Wang",
      "Di Lin",
      "Liang Wan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20156",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20156/19915",
    "published": "2022-02",
    "summary": "Semantic scene completion (SSC) requires the estimation of the 3D geometric occupancies of objects in the scene, along with the object categories. Currently, many methods employ RGB-D images to capture the geometric and semantic information of objects. These methods use simple but popular spatial- and channel-wise operations, which fuse the information of RGB and depth data. Yet, they ignore the large discrepancy of RGB-D data and the uncertainty measurements of depth data. To solve this problem, we propose the Frequency Fusion Network (FFNet), a novel method for boosting semantic scene completion by better utilizing RGB-D data. FFNet explicitly correlates the RGB-D data in the frequency domain, different from the features directly extracted by the convolution operation. Then, the network uses the correlated information to guide the feature learning from the RG- B and depth images, respectively. Moreover, FFNet accounts for the properties of different frequency components of RGB- D features. It has a learnable elliptical mask to decompose the features learned from the RGB and depth images, attending to various frequencies to facilitate the correlation process of RGB-D data. We evaluate FFNet intensively on the public SSC benchmarks, where FFNet surpasses the state-of- the-art methods. The code package of FFNet is available at https://github.com/alanWXZ/FFNet.",
    "code_link": "https://github.com/alanWXZ/FFNet"
  },
  "aaai2022_main_privacy-preservingfacerecognitioninthefrequencydomain": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Privacy-Preserving Face Recognition in the Frequency Domain",
    "authors": [
      "Yinggui Wang",
      "Jian Liu",
      "Man Luo",
      "Le Yang",
      "Li Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20157",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20157/19916",
    "published": "2022-02",
    "summary": "Some applications may require performing face recognition (FR) on third-party servers, which could be accessed by attackers with malicious intents to compromise the privacy of users\u2019 face information. This paper advocates a practical privacy-preserving FR scheme without key management realized in the frequency domain. The new scheme first collects the components of the same frequency from different blocks of a face image to form component channels. Only part of the channels are retained and fed into the analysis network that performs an interpretable privacy-accuracy trade-off analysis to identify channels important for face image visualization but not crucial for maintaining high FR accuracy. For this purpose, the loss function of the analysis network consists of the empirical FR error loss and a face visualization penalty term, and the network is trained in an end-to-end manner. We find that with the developed analysis network, more than 94% of the image energy can be dropped while the face recognition accuracy stays almost undegraded. In order to further protect the remaining frequency components, we propose a fast masking method. Effectiveness of the new scheme in removing the visual information of face images while maintaining their distinguishability is validated over several large face datasets. Results show that the proposed scheme achieves a recognition performance and inference time comparable to ArcFace operating on original face images directly.",
    "code_link": ""
  },
  "aaai2022_main_anchordetrquerydesignfortransformer-baseddetector": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Anchor DETR: Query Design for Transformer-Based Detector",
    "authors": [
      "Yingming Wang",
      "Xiangyu Zhang",
      "Tong Yang",
      "Jian Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20158",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20158/19917",
    "published": "2022-02",
    "summary": "In this paper, we propose a novel query design for the transformer-based object detection.In previous transformer-based detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we cannot explain where it will focus on. It is difficult to optimize as the prediction slot of each object query does not have a specific mode.In other words, each object query will not focus on a specific region. To solve these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors.So each object query focuses on the objects near the anchor point.Moreover, our query design can predict multiple objects at one position to solve the difficulty: ``one region, multiple objects''. In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR. Thanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10x fewer training epochs. For example, it achieves 44.2 AP with 19 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods. Code is available at https://github.com/megvii-research/AnchorDETR.",
    "code_link": "https://github.com/megvii-research/AnchorDETR"
  },
  "aaai2022_main_panini-netganpriorbaseddegradation-awarefeatureinterpolationforfacerestoration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration",
    "authors": [
      "Yinhuai Wang",
      "Yujie Hu",
      "Jian Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20159",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20159/19918",
    "published": "2022-02",
    "summary": "Emerging high-quality face restoration (FR) methods often utilize pre-trained GAN models (i.e., StyleGAN2) as GAN Prior. However, these methods usually struggle to balance realness and fidelity when facing various degradation levels. Besides, there is still a noticeable visual quality gap compared with pre-trained GAN models. In this paper, we propose a novel GAN Prior based degradation-aware feature interpolation network, dubbed Panini-Net, for FR tasks by explicitly learning the abstract representations to distinguish various degradations. Specifically, an unsupervised degradation representation learning (UDRL) strategy is first developed to extract degradation representations (DR) of the input degraded images. Then, a degradation-aware feature interpolation (DAFI) module is proposed to dynamically fuse the two types of informative features (i.e., features from input images and features from GAN Prior) with flexible adaption to various degradations based on DR. Ablation studies reveal the working mechanism of DAFI and its potential for editable FR. Extensive experiments demonstrate that our Panini-Net achieves state-of-the-art performance for multi-degradation face restoration and face super-resolution. The source code is available at https://github.com/jianzhangcs/panini.",
    "code_link": "https://github.com/jianzhangcs/panini"
  },
  "aaai2022_main_end-to-endtransformerbasedmodelforimagecaptioning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "End-to-End Transformer Based Model for Image Captioning",
    "authors": [
      "Yiyu Wang",
      "Jungang Xu",
      "Yingfei Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20160",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20160/19919",
    "published": "2022-02",
    "summary": "CNN-LSTM based architectures have played an important role in image captioning, but limited by the training efficiency and expression ability, researchers began to explore the CNN-Transformer based models and achieved great success. Meanwhile, almost all recent works adopt Faster R-CNN as the backbone encoder to extract region-level features from given images. However, Faster R-CNN needs a pre-training on an additional dataset, which divides the image captioning task into two stages and limits its potential applications. In this paper, we build a pure Transformer-based model, which integrates image captioning into one stage and realizes end-to-end training. Firstly, we adopt SwinTransformer to replace Faster R-CNN as the backbone encoder to extract grid-level features from given images; Then, referring to Transformer, we build a refining encoder and a decoder. The refining encoder refines the grid features by capturing the intra-relationship between them, and the decoder decodes the refined features into captions word by word. Furthermore, in order to increase the interaction between multi-modal (vision and language) features to enhance the modeling capability, we calculate the mean pooling of grid features as the global feature, then introduce it into refining encoder to refine with grid features together, and add a pre-fusion process of refined global feature and generated words in decoder. To validate the effectiveness of our proposed model, we conduct experiments on MSCOCO dataset. The experimental results compared to existing published works demonstrate that our model achieves new state-of-the-art performances of 138.2% (single model) and 141.0% (ensemble of 4 models) CIDEr scores on 'Karpathy' offline test split and 136.0% (c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained models and source code will be released.",
    "code_link": ""
  },
  "aaai2022_main_learningtodetect3dfaciallandmarksviaheatmapregressionwithgraphconvolutionalnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Detect 3D Facial Landmarks via Heatmap Regression with Graph Convolutional Network",
    "authors": [
      "Yuan Wang",
      "Min Cao",
      "Zhenfeng Fan",
      "Silong Peng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20161",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20161/19920",
    "published": "2022-02",
    "summary": "3D facial landmark detection is extensively used in many research fields such as face registration, facial shape analysis, and face recognition. Most existing methods involve traditional features and 3D face models for the detection of landmarks, and their performances are limited by the hand-crafted intermediate process. In this paper, we propose a novel 3D facial landmark detection method, which directly locates the coordinates of landmarks from 3D point cloud with a well-customized graph convolutional network. The graph convolutional network learns geometric features adaptively for 3D facial landmark detection with the assistance of constructed 3D heatmaps, which are Gaussian functions of distances to each landmark on a 3D face. On this basis, we further develop a local surface unfolding and registration module to predict 3D landmarks from the heatmaps. The proposed method forms the first baseline of deep point cloud learning method for 3D facial landmark detection. We demonstrate experimentally that the proposed method exceeds the existing approaches by a clear margin on BU-3DFE and FRGC datasets for landmark localization accuracy and stability, and also achieves high-precision results on a recent large-scale dataset.",
    "code_link": "https://github.com/wangyuan123ac/3DFA-GCN"
  },
  "aaai2022_main_low-lightimageenhancementwithnormalizingflow": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Low-Light Image Enhancement with Normalizing Flow",
    "authors": [
      "Yufei Wang",
      "Renjie Wan",
      "Wenhan Yang",
      "Haoliang Li",
      "Lap-Pui Chau",
      "Alex Kot"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20162",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20162/19921",
    "published": "2022-02",
    "summary": "To enhance low-light images to normally-exposed ones is highly ill-posed, namely that the mapping relationship between them is one-to-many. Previous works based on the pixel-wise reconstruction losses and deterministic processes fail to capture the complex conditional distribution of normally exposed images, which results in improper brightness, residual noise, and artifacts. In this paper, we investigate to model this one-to-many relationship via a proposed normalizing flow model. An invertible network that takes the low-light images/features as the condition and learns to map the distribution of normally exposed images into a Gaussian distribution. In this way, the conditional distribution of the normally exposed images can be well modeled, and the enhancement process, i.e., the other inference direction of the invertible network, is equivalent to being constrained by a loss function that better describes the manifold structure of natural images during the training. The experimental results on the existing benchmark datasets show our method achieves better quantitative and qualitative results, obtaining better-exposed illumination, less noise and artifact, and richer colors.",
    "code_link": "https://github.com/wyf0912/LLFlow"
  },
  "aaai2022_main_negativesamplemattersarenaissanceofmetriclearningfortemporalgrounding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding",
    "authors": [
      "Zhenzhi Wang",
      "Limin Wang",
      "Tao Wu",
      "Tianhao Li",
      "Gangshan Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20163",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20163/19922",
    "published": "2022-02",
    "summary": "Temporal grounding aims to localize a video moment which is semantically aligned with a given natural language query. Existing methods typically apply a detection or regression pipeline on the fused representation with the research focus on designing complicated prediction heads or fusion strategies. Instead, from a perspective on temporal grounding as a metric-learning problem, we present a Mutual Matching Network (MMN), to directly model the similarity between language queries and video moments in a joint embedding space. This new metric-learning framework enables fully exploiting negative samples from two new aspects: constructing negative cross-modal pairs in a mutual matching scheme and mining negative pairs across different videos. These new negative samples could enhance the joint representation learning of two modalities via cross-modal mutual matching to maximize their mutual information. Experiments show that our MMN achieves highly competitive performance compared with the state-of-the-art methods on four video grounding benchmarks. Based on MMN, we present a winner solution for the HC-STVG challenge of the 3rd PIC workshop. This suggests that metric learning is still a promising method for temporal grounding via capturing the essential cross-modal correlation in a joint embedding space. Code is available at https://github.com/MCG-NJU/MMN.",
    "code_link": "https://github.com/MCG-NJU/MMN"
  },
  "aaai2022_main_texturereformertowardsfastanduniversalinteractivetexturetransfer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Texture Reformer: Towards Fast and Universal Interactive Texture Transfer",
    "authors": [
      "Zhizhong Wang",
      "Lei Zhao",
      "Haibo Chen",
      "Ailin Li",
      "Zhiwen Zuo",
      "Wei Xing",
      "Dongming Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20164",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20164/19923",
    "published": "2022-02",
    "summary": "In this paper, we present the texture reformer, a fast and universal neural-based framework for interactive texture transfer with user-specified guidance. The challenges lie in three aspects: 1) the diversity of tasks, 2) the simplicity of guidance maps, and 3) the execution efficiency. To address these challenges, our key idea is to use a novel feed-forward multi-view and multi-stage synthesis procedure consisting of I) a global view structure alignment stage, II) a local view texture refinement stage, and III) a holistic effect enhancement stage to synthesize high-quality results with coherent structures and fine texture details in a coarse-to-fine fashion. In addition, we also introduce a novel learning-free view-specific texture reformation (VSTR) operation with a new semantic map guidance strategy to achieve more accurate semantic-guided and structure-preserved texture transfer. The experimental results on a variety of application scenarios demonstrate the effectiveness and superiority of our framework. And compared with the state-of-the-art interactive texture transfer algorithms, it not only achieves higher quality results but, more remarkably, also is 2-5 orders of magnitude faster.",
    "code_link": "https://github.com/EndyWon/Texture-Reformer"
  },
  "aaai2022_main_interact,embed,andenlargeboostingmodality-specificrepresentationsformulti-modalpersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification",
    "authors": [
      "Zi Wang",
      "Chenglong Li",
      "Aihua Zheng",
      "Ran He",
      "Jin Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20165",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20165/19924",
    "published": "2022-02",
    "summary": "Multi-modal person Re-ID introduces more complementary information to assist the traditional Re-ID task. Existing multi-modal methods ignore the importance of modality-specific information in the feature fusion stage. To this end, we propose a novel method to boost modality-specific representations for multi-modal person Re-ID: Interact, Embed, and EnlargE (IEEE). First, we propose a cross-modal interacting module to exchange useful information between different modalities in the feature extraction phase. Second, we propose a relation-based embedding module to enhance the richness of feature descriptors by embedding the global feature into the fine-grained local information. Finally, we propose multi-modal margin loss to force the network to learn modality-specific information for each modality by enlarging the intra-class discrepancy. Superior performance on multi-modal Re-ID dataset RGBNT201 and three constructed Re-ID datasets validate the effectiveness of the proposed method compared with the state-of-the-art approaches.",
    "code_link": ""
  },
  "aaai2022_main_cansemanticlabelsassistself-supervisedvisualrepresentationlearning?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Can Semantic Labels Assist Self-Supervised Visual Representation Learning?",
    "authors": [
      "Longhui Wei",
      "Lingxi Xie",
      "Jianzhong He",
      "Xiaopeng Zhang",
      "Qi Tian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20166",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20166/19925",
    "published": "2022-02",
    "summary": "Recently, contrastive learning has largely advanced the progress of unsupervised visual representation learning. Pre-trained on ImageNet, some self-supervised algorithms reported higher transfer learning performance compared to fully-supervised methods, seeming to deliver the message that human labels hardly contribute to learning transferrable visual features. In this paper, we defend the usefulness of semantic labels but point out that fully-supervised and self-supervised methods are pursuing different kinds of features. To alleviate this issue, we present a new algorithm named Supervised Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the semantic guidance from damaging the appearance feature embedding. In a series of downstream tasks, SCAN achieves superior performance compared to previous fully-supervised and self-supervised methods, and sometimes the gain is significant. More importantly, our study reveals that semantic labels are useful in assisting self-supervised methods, opening a new direction for the community.",
    "code_link": ""
  },
  "aaai2022_main_rethinkingthetwo-stageframeworkforgroundedsituationrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Rethinking the Two-Stage Framework for Grounded Situation Recognition",
    "authors": [
      "Meng Wei",
      "Long Chen",
      "Wei Ji",
      "Xiaoyu Yue",
      "Tat-Seng Chua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20167",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20167/19926",
    "published": "2022-02",
    "summary": "Grounded Situation Recognition (GSR), i.e., recognizing the salient activity (or verb) category in an image (e.g.,buying) and detecting all corresponding semantic roles (e.g.,agent and goods), is an essential step towards \u201chuman-like\u201d event understanding. Since each verb is associated with a specific set of semantic roles, all existing GSR methods resort to a two-stage framework: predicting the verb in the first stage and detecting the semantic roles in the second stage. However, there are obvious drawbacks in both stages: 1) The widely-used cross-entropy (XE) loss for object recognition is insufficient in verb classification due to the large intra-class variation and high inter-class similarity among daily activities. 2) All semantic roles are detected in an autoregressive manner, which fails to model the complex semantic relations between different roles. To this end, we propose a novel SituFormerfor GSR which consists of a Coarse-to-Fine Verb Model (CFVM) and a Transformer-based Noun Model (TNM). CFVM is a two-step verb prediction model: a coarse-grained model trained with XE loss first proposes a set of verb candidates, and then a fine-grained model trained with triplet loss re-ranks these candidates with enhanced verb features (not only separable but also discriminative). TNM is a transformer-based semantic role detection model, which detects all roles parallelly. Owing to the global relation modeling ability and flexibility of the transformer decoder, TNM can fully explore the statistical dependency of the roles. Extensive validations on the challenging SWiG benchmark show that SituFormer achieves a new state-of-the-art performance with significant gains under various metrics. Code is available at https://github.com/kellyiss/SituFormer.",
    "code_link": "https://github.com/kellyiss/SituFormer"
  },
  "aaai2022_main_boostingthetransferabilityofvideoadversarialexamplesviatemporaltranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Boosting the Transferability of Video Adversarial Examples via Temporal Translation",
    "authors": [
      "Zhipeng Wei",
      "Jingjing Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20168",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20168/19927",
    "published": "2022-02",
    "summary": "Although deep-learning based video recognition models have achieved remarkable success, they are vulnerable to adversarial examples that are generated by adding human-imperceptible perturbations on clean video samples. As indicated in recent studies, adversarial examples are transferable, which makes it feasible for black-box attacks in real-world applications. Nevertheless, most existing adversarial attack methods have poor transferability when attacking other video models and transfer-based attacks on video models are still unexplored. To this end, we propose to boost the transferability of video adversarial examples for black-box attacks on video recognition models. Through extensive analysis, we discover that different video recognition models rely on different discriminative temporal patterns, leading to the poor transferability of video adversarial examples. This motivates us to introduce a temporal translation attack method, which optimizes the adversarial perturbations over a set of temporal translated video clips. By generating adversarial examples over translated videos, the resulting adversarial examples are less sensitive to temporal patterns existed in the white-box model being attacked and thus can be better transferred. Extensive experiments on the Kinetics-400 dataset and the UCF-101 dataset demonstrate that our method can significantly boost the transferability of video adversarial examples. For transfer-based attack against video recognition models, it achieves a 61.56% average attack success rate on the Kinetics-400 and 48.60% on the UCF-101.",
    "code_link": "https://github.com/zhipeng-wei/TT"
  },
  "aaai2022_main_towardstransferableadversarialattacksonvisiontransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Transferable Adversarial Attacks on Vision Transformers",
    "authors": [
      "Zhipeng Wei",
      "Jingjing Chen",
      "Micah Goldblum",
      "Zuxuan Wu",
      "Tom Goldstein",
      "Yu-Gang Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20169",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20169/19928",
    "published": "2022-02",
    "summary": "Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance.",
    "code_link": "https://github.com/zhipeng-wei/PNA-PatchOut"
  },
  "aaai2022_main_l-codelanguage-basedcolorizationusingcolor-objectdecoupledconditions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "L-CoDe:Language-Based Colorization Using Color-Object Decoupled Conditions",
    "authors": [
      "Shuchen Weng",
      "Hao Wu",
      "Zheng Chang",
      "Jiajun Tang",
      "Si Li",
      "Boxin Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20170",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20170/19929",
    "published": "2022-02",
    "summary": "Colorizing a grayscale image is inherently an ill-posed problem with multi-modal uncertainty. Language-based colorization offers a natural way of interaction to reduce such uncertainty via a user-provided caption. However, the color-object coupling and mismatch issues make the mapping from word to color difficult. In this paper, we propose L-CoDe, a Language-based Colorization network using color-object Decoupled conditions. A predictor for object-color corresponding matrix (OCCM) and a novel attention transfer module (ATM) are introduced to solve the color-object coupling problem. To deal with color-object mismatch that results in incorrect color-object correspondence, we adopt a soft-gated injection module (SIM). We further present a new dataset containing annotated color-object pairs to provide supervisory signals for resolving the coupling problem. Experimental results show that our approach outperforms state-of-the-art methods conditioned on captions.",
    "code_link": ""
  },
  "aaai2022_main_neuralinterferometryimagereconstructionfromastronomicalinterferometersusingtransformer-conditionedneuralfields": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neural Interferometry: Image Reconstruction from Astronomical Interferometers Using Transformer-Conditioned Neural Fields",
    "authors": [
      "Benjamin Wu",
      "Chao Liu",
      "Benjamin Eckart",
      "Jan Kautz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20171",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20171/19930",
    "published": "2022-02",
    "summary": "Astronomical interferometry enables a collection of telescopes to achieve angular resolutions comparable to that of a single, much larger telescope. This is achieved by combining simultaneous observations from pairs of telescopes such that the signal is mathematically equivalent to sampling the Fourier domain of the object. However, reconstructing images from such sparse sampling is a challenging and ill-posed problem, with current methods requiring precise tuning of parameters and manual, iterative cleaning by experts. We present a novel deep learning approach in which the representation in the Fourier domain of an astronomical source is learned implicitly using a neural field representation. Data-driven priors can be added through a transformer encoder. Results on synthetically observed galaxies show that transformer-conditioned neural fields can successfully reconstruct astronomical observations even when the number of visibilities is very sparse.",
    "code_link": "https://github.com/wubenjamin/neuralinterferometry"
  },
  "aaai2022_main_tdv2anoveltree-structureddecoderforofflinemathematicalexpressionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TDv2: A Novel Tree-Structured Decoder for Offline Mathematical Expression Recognition",
    "authors": [
      "Changjie Wu",
      "Jun Du",
      "Yunqing Li",
      "Jianshu Zhang",
      "Chen Yang",
      "Bo Ren",
      "Yiqing\n      Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20172",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20172/19931",
    "published": "2022-02",
    "summary": "In recent years, tree decoders become more popular than LaTeX string decoders in the field of handwritten mathematical expression recognition (HMER) as they can capture the hierarchical tree structure of mathematical expressions. However previous tree decoders converted the tree structure labels into a fixed and ordered sequence, which could not make full use of the diversified expression of tree labels. In this study, we propose a novel tree decoder (TDv2) to fully utilize the tree structure labels. Compared with previous tree decoders, this new model does not require a fixed priority for different branches of a node during training and inference, which can effectively improve the model generalization capability. The input and output of the model make full use of the tree structure label, so that there is no need to find the parent node in the decoding process, which simplifies the decoding process and adds a prior information to help predict the node. We verified the effectiveness of each part of the model through comprehensive ablation experiments and attention visualization analysis. On the authoritative CROHME 14/16/19 datasets, our method achieves the state-of-the-art results.",
    "code_link": "https://github.com/yqingli123/TDv2.git"
  },
  "aaai2022_main_learningtoken-basedrepresentationforimageretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Token-Based Representation for Image Retrieval",
    "authors": [
      "Hui Wu",
      "Min Wang",
      "Wengang Zhou",
      "Yang Hu",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20173",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20173/19932",
    "published": "2022-02",
    "summary": "In image retrieval, deep local features learned in a data-driven manner have been demonstrated effective to improve retrieval performance. To realize efficient retrieval on large image database, some approaches quantize deep local features with a large codebook and match images with aggregated match kernel. However, the complexity of these approaches is non-trivial with large memory footprint, which limits their capability to jointly perform feature learning and aggregation. To generate compact global representations while maintaining regional matching capability, we propose a unified framework to jointly learn local feature representation and aggregation. In our framework, we first extract local features using CNNs. Then, we design a tokenizer module to aggregate them into a few visual tokens, each corresponding to a specific visual pattern. This helps to remove background noise, and capture more discriminative regions in the image. Next, a refinement block is introduced to enhance the visual tokens with self-attention and cross-attention. Finally, different visual tokens are concatenated to generate a compact global representation. The whole framework is trained end-to-end with image-level labels. Extensive experiments are conducted to evaluate our approach, which outperforms the state-of-the-art methods on the Revisited Oxford and Paris datasets.",
    "code_link": ""
  },
  "aaai2022_main_multi-modalanswervalidationforknowledge-basedvqa": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Modal Answer Validation for Knowledge-Based VQA",
    "authors": [
      "Jialin Wu",
      "Jiasen Lu",
      "Ashish Sabharwal",
      "Roozbeh Mottaghi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20174",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20174/19933",
    "published": "2022-02",
    "summary": "The problem of knowledge-based visual question answering involves answering questions that require external knowledge in addition to the content of the image. Such knowledge typically comes in various forms, including visual, textual, and commonsense knowledge. Using more knowledge sources increases the chance of retrieving more irrelevant or noisy facts, making it challenging to comprehend the facts and find the answer. To address this challenge, we propose Multi-modal Answer Validation using External knowledge (MAVEx), where the idea is to validate a set of promising answer candidates based on answer-specific knowledge retrieval. Instead of searching for the answer in a vast collection of often irrelevant facts as most existing approaches do, MAVEx aims to learn how to extract relevant knowledge from noisy sources, which knowledge source to trust for each answer candidate, and how to validate the candidate using that source. Our multi-modal setting is the first to leverage external visual knowledge (images searched using Google), in addition to textual knowledge in the form of Wikipedia sentences and ConceptNet concepts. Our experiments with OK-VQA, a challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new state-of-the-art results. Our code is available at https://github.com/jialinwu17/MAVEX",
    "code_link": "https://github.com/jialinwu17/MAVEX"
  },
  "aaai2022_main_neighborhoodconsensuscontrastivelearningforbackward-compatiblerepresentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation",
    "authors": [
      "Shengsen Wu",
      "Liang Chen",
      "Yihang Lou",
      "Yan Bai",
      "Tao Bai",
      "Minghua Deng",
      "Ling-Yu Duan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20175",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20175/19934",
    "published": "2022-02",
    "summary": "In object re-identification (ReID), the development of deep learning techniques often involves model updates and deployment. It is unbearable to re-embedding and re-index with the system suspended when deploying new models. Therefore, backward-compatible representation is proposed to enable ``new'' features to be compared with ``old'' features directly, which means that the database is active when there are both ``new'' and ``old'' features in it. Thus we can scroll-refresh the database or even do nothing on the database to update. The existing backward-compatible methods either require a strong overlap between old and new training data or simply conduct constraints at the instance level. Thus they are difficult in handling complicated cluster structures and are limited in eliminating the impact of outliers in old embeddings, resulting in a risk of damaging the discriminative capability of new features.In this work, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method. With no assumptions about the new training data, we estimate the sub-cluster structures of old embeddings. A new embedding is constrained with multiple old embeddings in both embedding space and discrimination space at the sub-class level. The effect of outliers diminished, as the multiple samples serve as ``mean teachers''. Besides, we propose a scheme to filter the old embeddings with low credibility, further improving the compatibility robustness. Our method ensures the compatibility without impairing the accuracy of the new model. It can even improve the new model's accuracy in most scenarios.",
    "code_link": ""
  },
  "aaai2022_main_paletransformerageneralvisiontransformerbackbonewithpale-shapedattention": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20176",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20176/19935",
    "published": "2022-02",
    "summary": "Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224x224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection & instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT.",
    "code_link": "https://github.com/BRIDL/PaddleViT"
  },
  "aaai2022_main_stylemixingandpatchwiseprototypicalmatchingforone-shotunsuperviseddomainadaptivesemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Style Mixing and Patchwise Prototypical Matching for One-Shot Unsupervised Domain Adaptive Semantic Segmentation",
    "authors": [
      "Xinyi Wu",
      "Zhenyao Wu",
      "Yuhang Lu",
      "Lili Ju",
      "Song Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20177",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20177/19936",
    "published": "2022-02",
    "summary": "In this paper, we tackle the problem of one-shot unsupervised domain adaptation (OSUDA) for semantic segmentation where the segmentors only see one unlabeled target image during training. In this case, traditional unsupervised domain adaptation models usually fail since they cannot adapt to the target domain with over-fitting to one (or few) target samples. To address this problem, existing OSUDA methods usually integrate a style-transfer module to perform domain randomization based on the unlabeled target sample, with which multiple domains around the target sample can be explored during training. However, such a style-transfer module relies on an additional set of images as style reference for pre-training and also increases the memory demand for domain adaptation. Here we propose a new OSUDA method that can effectively relieve such computational burden. Specifically, we integrate several style-mixing layers into the segmentor which play the role of style-transfer module to stylize the source images without introducing any learned parameters. Moreover, we propose a patchwise prototypical matching (PPM) method to weighted consider the importance of source pixels during the supervised training to relieve the negative adaptation. Experimental results show that our method achieves new state-of-the-art performance on two commonly used benchmarks for domain adaptive semantic segmentation under the one-shot setting and is more efficient than all comparison approaches.",
    "code_link": "https://github.com/W-zx-Y/SM-PPM"
  },
  "aaai2022_main_multi-centroidrepresentationnetworkfordomainadaptivepersonre-id": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Centroid Representation Network for Domain Adaptive Person Re-ID",
    "authors": [
      "Yuhang Wu",
      "Tengteng Huang",
      "Haotian Yao",
      "Chi Zhang",
      "Yuanjie Shao",
      "Chuchu\n      Han",
      "Changxin Gao",
      "Nong Sang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20178",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20178/19937",
    "published": "2022-02",
    "summary": "Recently, many approaches tackle the Unsupervised Domain Adaptive person re-identification (UDA re-ID) problem through pseudo-label-based contrastive learning. During training, a uni-centroid representation is obtained by simply averaging all the instance features from a cluster with the same pseudo label. However, a cluster may contain images with different identities (label noises) due to the imperfect clustering results, which makes the uni-centroid representation inappropriate. In this paper, we present a novel Multi-Centroid Memory (MCM) to adaptively capture different identity information within the cluster. MCM can effectively alleviate the issue of label noises by selecting proper positive/negative centroids for the query image. Moreover, we further propose two strategies to improve the contrastive learning process. First, we present a Domain-Specific Contrastive Learning (DSCL) mechanism to fully explore intra-domain information by comparing samples only from the same domain. Second, we propose Second-Order Nearest Interpolation (SONI) to obtain abundant and informative negative samples. We integrate MCM, DSCL, and SONI into a unified framework named Multi-Centroid Representation Network (MCRN). Extensive experiments demonstrate the superiority of MCRN over state-of-the-art approaches on multiple UDA re-ID tasks and fully unsupervised re-ID tasks.",
    "code_link": ""
  },
  "aaai2022_main_efficientnon-localcontrastiveattentionforimagesuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Non-local Contrastive Attention for Image Super-resolution",
    "authors": [
      "Bin Xia",
      "Yucheng Hang",
      "Yapeng Tian",
      "Wenming Yang",
      "Qingmin Liao",
      "Jie Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20179",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20179/19938",
    "published": "2022-02",
    "summary": "Non-Local Attention (NLA) brings significant improvement for Single Image Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural images. However, NLA gives noisy information large weights and consumes quadratic computation resources with respect to the input size, limiting its performance and application. In this paper, we propose a novel Efficient Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling and leverage more relevant non-local features. Specifically, ENLCA consists of two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA adopts the kernel method to approximate exponential function and obtains linear computation complexity. For Sparse Aggregation, we multiply inputs by an amplification factor to focus on informative features, yet the variance of approximation increases exponentially. Therefore, contrastive learning is applied to further separate relevant and irrelevant features. To demonstrate the effectiveness of ENLCA, we build an architecture called Efficient Non-Local Contrastive Network (ENLCN) by adding a few of our modules in a simple backbone. Extensive experimental results show that ENLCN reaches superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations.",
    "code_link": ""
  },
  "aaai2022_main_coarse-to-fineembeddedpatchmatchandmulti-scaledynamicaggregationforreference-basedsuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-Based Super-resolution",
    "authors": [
      "Bin Xia",
      "Yapeng Tian",
      "Yucheng Hang",
      "Wenming Yang",
      "Qingmin Liao",
      "Jie Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20180",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20180/19939",
    "published": "2022-02",
    "summary": "Reference-based super-resolution (RefSR) has made significant progress in producing realistic textures using an external reference (Ref) image. However, existing RefSR methods obtain high-quality correspondence matchings consuming quadratic computation resources with respect to the input size, limiting its application. Moreover, these approaches usually suffer from scale misalignments between the low-resolution (LR) image and Ref image. In this paper, we propose an Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based Super-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch) and Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching efficiency, we design a novel Embedded PatchMacth scheme with random samples propagation, which involves end-to-end training with asymptotic linear computational cost to the input size. To further reduce computational cost and speed up convergence, we apply the coarse-to-fine strategy on Embedded PatchMacth constituting CFE-PatchMatch. To fully leverage reference information across multiple scales and enhance robustness to scale misalignment, we develop the MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation. The Dynamic Aggregation corrects minor scale misalignment by dynamically aggregating features, and the Multi-Scale Aggregation brings robustness to large scale misalignment by fusing multi-scale information. Experimental results show that the proposed AMSA achieves superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations.",
    "code_link": ""
  },
  "aaai2022_main_cross-domaincollaborativenormalizationviastructuralknowledge": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Domain Collaborative Normalization via Structural Knowledge",
    "authors": [
      "Haifeng Xia",
      "Zhengming Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20181",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20181/19940",
    "published": "2022-02",
    "summary": "Batch Normalization (BN) as an important component assists Deep Neural Networks in achieving promising performance for extensive learning tasks by scaling distribution of feature representations within mini-batches. However, the application of BN suffers from performance degradation under the scenario of Unsupervised Domain Adaptation (UDA), since the estimated statistics fail to concurrently describe two different domains. In this paper, we develop a novel normalization technique, named Collaborative Normalization (CoN), for eliminating domain discrepancy and accelerating the model training of neural networks for UDA. Unlike typical strategies only exploiting domain-specific statistics during normalization, our CoN excavates cross-domain knowledge and simultaneously scales features from various domains by mimicking the merits of collaborative representation. Our CoN can be easily plugged into popular neural network backbones for cross-domain learning. On the one hand, theoretical analysis guarantees that models with CoN promote discriminability of feature representations and accelerate convergence rate; on the other hand, empirical study verifies that replacing BN with CoN in popular network backbones effectively improves classification accuracy in most learning tasks across three cross-domain visual benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_remonetrecurrentmulti-outputnetworkforefficientvideodenoising": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ReMoNet: Recurrent Multi-Output Network for Efficient Video Denoising",
    "authors": [
      "Liuyu Xiang",
      "Jundong Zhou",
      "Jirui Liu",
      "Zerun Wang",
      "Haidong Huang",
      "Jie Hu",
      "Jungong Han",
      "Yuchen Guo",
      "Guiguang Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20182",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20182/19941",
    "published": "2022-02",
    "summary": "While deep neural network-based video denoising methods have achieved promising results, it is still hard to deploy them on mobile devices due to their high computational cost and memory demands. This paper aims to develop a lightweight deep video denoising method that is friendly to resource-constrained mobile devices. Inspired by the facts that 1) consecutive video frames usually contain redundant temporal coherency, and 2) neural networks are usually over-parameterized, we propose a multi-input multi-output (MIMO) paradigm to process consecutive video frames within one-forward-pass. The basic idea is concretized to a novel architecture termed Recurrent Multi-output Network (ReMoNet), which consists of recurrent temporal fusion and temporal aggregation blocks and is further reinforced by similarity-based mutual distillation. We conduct extensive experiments on NVIDIA GPU and Qualcomm Snapdragon 888 mobile platform with Gaussian noise and simulated Image-Signal-Processor (ISP) noise. The experimental results show that ReMoNet is both effective and efficient on video denoising. Moreover, we show that ReMoNet is more robust under higher noise level scenarios.",
    "code_link": ""
  },
  "aaai2022_main_transferlearningfromsynthetictoreallidarpointcloudforsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation",
    "authors": [
      "Aoran Xiao",
      "Jiaxing Huang",
      "Dayan Guan",
      "Fangneng Zhan",
      "Shijian Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20183",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20183/19942",
    "published": "2022-02",
    "summary": "Knowledge transfer from synthetic to real data has been widely studied to mitigate data annotation constraints in various computer vision tasks such as semantic segmentation. However, the study focused on 2D images and its counterpart in 3D point clouds segmentation lags far behind due to the lack of large-scale synthetic datasets and effective transfer methods. We address this issue by collecting SynLiDAR, a large-scale synthetic LiDAR dataset that contains point-wise annotated point clouds with accurate geometric shapes and comprehensive semantic classes. SynLiDAR was collected from multiple virtual environments with rich scenes and layouts which consists of over 19 billion points of 32 semantic classes. In addition, we design PCT, a novel point cloud translator that effectively mitigates the gap between synthetic and real point clouds. Specifically, we decompose the synthetic-to-real gap into an appearance component and a sparsity component and handle them separately which improves the point cloud translation greatly. We conducted extensive experiments over three transfer learning setups including data augmentation, semi-supervised domain adaptation and unsupervised domain adaptation. Extensive experiments show that SynLiDAR provides a high-quality data source for studying 3D transfer and the proposed PCT achieves superior point cloud translation consistently across the three setups.The dataset is available at https://github.com/xiaoaoran/SynLiDAR.",
    "code_link": "https://github.com/xiaoaoran/SynLiDAR"
  },
  "aaai2022_main_videoasconditionalgraphhierarchyformulti-granularquestionanswering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Video as Conditional Graph Hierarchy for Multi-Granular Question Answering",
    "authors": [
      "Junbin Xiao",
      "Angela Yao",
      "Zhiyuan Liu",
      "Yicong Li",
      "Wei Ji",
      "Tat-Seng Chua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20184",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20184/19943",
    "published": "2022-02",
    "summary": "Video question answering requires the models to understand and reason about both the complex video and language data to correctly derive the answers. Existing efforts have been focused on designing sophisticated cross-modal interactions to fuse the information from two modalities, while encoding the video and question holistically as frame and word sequences. Despite their success, these methods are essentially revolving around the sequential nature of video- and question-contents, providing little insight to the problem of question-answering and lacking interpretability as well. In this work, we argue that while video is presented in frame sequence, the visual elements (e.g., objects, actions, activities and events) are not sequential but rather hierarchical in semantic space. To align with the multi-granular essence of linguistic concepts in language queries, we propose to model video as a conditional graph hierarchy which weaves together visual facts of different granularity in a level-wise manner, with the guidance of corresponding textual cues. Despite the simplicity, our extensive experiments demonstrate the superiority of such conditional hierarchical graph architecture, with clear performance improvements over prior methods and also better generalization across different type of questions. Further analyses also demonstrate the model's reliability as it shows meaningful visual-textual evidences for the predicted answers.",
    "code_link": ""
  },
  "aaai2022_main_adaptiveposehumanpartsasadaptivepoints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AdaptivePose: Human Parts as Adaptive Points",
    "authors": [
      "Yabo Xiao",
      "Xiao Juan Wang",
      "Dongdong Yu",
      "Guoli Wang",
      "Qian Zhang",
      "Mingshu HE"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20185",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20185/19944",
    "published": "2022-02",
    "summary": "Multi-person pose estimation methods generally follow top-down and bottom-up paradigms, both of which can be considered as two-stage approaches thus leading to the high computation cost and low efficiency. Towards a compact and efficient pipeline for multi-person pose estimation task, in this paper, we propose to represent the human parts as points and present a novel body representation, which leverages an adaptive point set including the human center and seven human-part related points to represent the human instance in a more fine-grained manner. The novel representation is more capable of capturing the various pose deformation and adaptively factorizes the long-range center-to-joint displacement thus delivers a single-stage differentiable network to more precisely regress multi-person pose, termed as AdaptivePose. For inference, our proposed network eliminates the grouping as well as refinements and only needs a single-step disentangling process to form multi-person pose. Without any bells and whistles, we achieve the best speed-accuracy trade-offs of 67.4% AP / 29.4 fps with DLA-34 and 71.3% AP / 9.1 fps with HRNet-W48 on COCO test-dev dataset.",
    "code_link": ""
  },
  "aaai2022_main_learningquality-awarerepresentationformulti-personposeregression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Quality-Aware Representation for Multi-Person Pose Regression",
    "authors": [
      "Yabo Xiao",
      "Dongdong Yu",
      "Xiao Juan Wang",
      "Lei Jin",
      "Guoli Wang",
      "Qian Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20186",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20186/19945",
    "published": "2022-02",
    "summary": "Off-the-shelf single-stage multi-person pose regression methods generally leverage the instance score (i.e., confidence of the instance localization) to indicate the pose quality for selecting the pose candidates. We consider that there are two gaps involved in existing paradigm: 1) The instance score is not well interrelated with the pose regression quality. 2) The instance feature representation, which is used for predicting the instance score, does not explicitly encode the structural pose information to predict the reasonable score that represents pose regression quality. To address the aforementioned issues, we propose to learn the pose regression quality-aware representation. Concretely, for the first gap, instead of using the previous instance confidence label (e.g., discrete {1,0} or Gaussian representation) to denote the position and confidence for person instance, we firstly introduce the Consistent Instance Representation (CIR) that unifies the pose regression quality score of instance and the confidence of background into a pixel-wise score map to calibrates the inconsistency between instance score and pose regression quality. To fill the second gap, we further present the Query Encoding Module (QEM) including the Keypoint Query Encoding (KQE) to encode the positional and semantic information for each keypoint and the Pose Query Encoding (PQE) which explicitly encodes the predicted structural pose information to better fit the Consistent Instance Representation (CIR). By using the proposed components, we significantly alleviate the above gaps. Our method outperforms previous single-stage regression-based even bottom-up methods and achieves the state-of-the-art result of 71.7 AP on MS COCO test-dev set.",
    "code_link": ""
  },
  "aaai2022_main_attribute-basedprogressivefusionnetworkforrgbttracking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Attribute-Based Progressive Fusion Network for RGBT Tracking",
    "authors": [
      "Yun Xiao",
      "MengMeng Yang",
      "Chenglong Li",
      "Lei Liu",
      "Jin Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20187",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20187/19946",
    "published": "2022-02",
    "summary": "RGBT tracking usually suffers from various challenge factors, such as fast motion, scale variation, illumination variation, thermal crossover and occlusion, to name a few. Existing works often study fusion models to solve all challenges simultaneously, and it requires fusion models complex enough and training data large enough, which are usually difficult to be constructed in real-world scenarios. In this work, we disentangle the fusion process via the challenge attributes, and thus propose a novel Attribute-based Progressive Fusion Network (APFNet) to increase the fusion capacity with a small number of parameters while reducing the dependence on large-scale training data. In particular, we design five attribute-specific fusion branches to integrate RGB and thermal features under the challenges of thermal crossover, illumination variation, scale variation, occlusion and fast motion respectively. By disentangling the fusion process, we can use a small number of parameters for each branch to achieve robust fusion of different modalities and train each branch using the small training subset with the corresponding attribute annotation. Then, to adaptive fuse features of all branches, we design an aggregation fusion module based on SKNet. Finally, we also design an enhancement fusion transformer to strengthen the aggregated feature and modality-specific features. Experimental results on benchmark datasets demonstrate the effectiveness of our APFNet against other state-of-the-art methods.",
    "code_link": "https://github.com/mmiclcl/source-code"
  },
  "aaai2022_main_detailedfacialgeometryrecoveryfrommulti-viewimagesbylearninganimplicitfunction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function",
    "authors": [
      "Yunze Xiao",
      "Hao Zhu",
      "Haotian Yang",
      "Zhengyu Diao",
      "Xiangju Lu",
      "Xun Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20188",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20188/19947",
    "published": "2022-02",
    "summary": "Recovering detailed facial geometry from a set of calibrated multi-view images is valuable for its wide range of applications. Traditional multi-view stereo (MVS) methods adopt an optimization-based scheme to regularize the matching cost. Recently, learning-based methods integrate all these into an end-to-end neural network and show superiority of efficiency. In this paper, we propose a novel architecture to recover extremely detailed 3D faces within dozens of seconds.Unlike previous learning-based methods that regularize the cost volume via 3D CNN, we propose to learn an implicit function for regressing the matching cost.By fitting a 3D morphable model from multi-view images, the features of multiple images are extracted and aggregated in the mesh-attached UV space, which makes the implicit function more effective in recovering detailed facial shape. Our method outperforms SOTA learning-based MVS in accuracy by a large margin on the FaceScape dataset. The code and data are released in https://github.com/zhuhao-nju/mvfr.",
    "code_link": "https://github.com/zhuhao-nju/mvfr"
  },
  "aaai2022_main_finetdualbranchesfeatureinteractionforpartial-to-partialpointcloudregistration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration",
    "authors": [
      "Hao Xu",
      "Nianjin Ye",
      "Guanghui Liu",
      "Bing Zeng",
      "Shuaicheng Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20189",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20189/19948",
    "published": "2022-02",
    "summary": "Data association is important in the point cloud registration. In this work, we propose to solve the partial-to-partial registration from a new perspective, by introducing multi-level feature interactions between the source and the reference clouds at the feature extraction stage, such that the registration can be realized without the attentions or explicit mask estimation for the overlapping detection as adopted previously. Specifically, we present FINet, a feature interactionbased structure with the capability to enable and strengthen the information associating between the inputs at multiple stages. To achieve this, we first split the features into two components, one for rotation and one for translation, based on the fact that they belong to different solution spaces, yielding a dual branches structure. Second, we insert several interaction modules at the feature extractor for the data association. Third, we propose a transformation sensitivity loss to obtain rotation-attentive and translation-attentive features. Experiments demonstrate that our method performs higher precision and robustness compared to the state-of-the-art traditional and learning-based methods. Code is available at https://github.com/megvii-research/FINet.",
    "code_link": "https://github.com/megvii-research/FINet"
  },
  "aaai2022_main_rendering-awarehdrenvironmentmappredictionfromasingleimage": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Rendering-Aware HDR Environment Map Prediction from a Single Image",
    "authors": [
      "Jun-Peng Xu",
      "Chenyu Zuo",
      "Fang-Lue Zhang",
      "Miao Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20190",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20190/19949",
    "published": "2022-02",
    "summary": "High dynamic range (HDR) illumination estimation from a single low dynamic range (LDR) image is a significant task in computer vision, graphics, and augmented reality. We present a two-stage deep learning-based method to predict an HDR environment map from a single narrow field-of-view LDR image. We first learn a hybrid parametric representation that sufficiently covers high- and low-frequency illumination components in the environment. Taking the estimated illuminations as guidance, we build a generative adversarial network to synthesize an HDR environment map that enables realistic rendering effects. We specifically consider the rendering effect by supervising the networks using rendering losses in both stages, on the predicted environment map as well as the hybrid illumination representation. Quantitative and qualitative experiments demonstrate that our approach achieves lower relighting errors for virtual object insertion and is preferred by users compared to state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_topology-awareconvolutionalneuralnetworkforefficientskeleton-basedactionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Topology-Aware Convolutional Neural Network for Efficient Skeleton-Based Action Recognition",
    "authors": [
      "Kailin Xu",
      "Fanfan Ye",
      "Qiaoyong Zhong",
      "Di Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20191",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20191/19950",
    "published": "2022-02",
    "summary": "In the context of skeleton-based action recognition, graph convolutional networks (GCNs) have been rapidly developed, whereas convolutional neural networks (CNNs) have received less attention. One reason is that CNNs are considered poor in modeling the irregular skeleton topology. To alleviate this limitation, we propose a pure CNN architecture named Topology-aware CNN (Ta-CNN) in this paper. In particular, we develop a novel cross-channel feature augmentation module, which is a combo of map-attend-group-map operations. By applying the module to the coordinate level and the joint level subsequently, the topology feature is effectively enhanced. Notably, we theoretically prove that graph convolution is a special case of normal convolution when the joint dimension is treated as channels. This confirms that the topology modeling power of GCNs can also be implemented by using a CNN. Moreover, we creatively design a SkeletonMix strategy which mixes two persons in a unique manner and further boosts the performance. Extensive experiments are conducted on four widely used datasets, i.e. N-UCLA, SBU, NTU RGB+D and NTU RGB+D 120 to verify the effectiveness of Ta-CNN. We surpass existing CNN-based methods significantly. Compared with leading GCN-based methods, we achieve comparable performance with much less complexity in terms of the required GFLOPs and parameters.",
    "code_link": ""
  },
  "aaai2022_main_transcodedvideorestorationbytemporalspatialauxiliarynetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transcoded Video Restoration by Temporal Spatial Auxiliary Network",
    "authors": [
      "Li Xu",
      "Gang He",
      "Jinjia Zhou",
      "Jie Lei",
      "Weiying Xie",
      "Yunsong Li",
      "Yu-Wing Tai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20192",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20192/19951",
    "published": "2022-02",
    "summary": "In most video platforms, such as Youtube, Kwai, and TikTok, the played videos usually have undergone multiple video encodings such as hardware encoding by recording devices, software encoding by video editing apps, and single/multiple video transcoding by video application servers. Previous works in compressed video restoration typically assume the compression artifacts are caused by one-time encoding. Thus, the derived solution usually does not work very well in practice. In this paper, we propose a new method, temporal spatial auxiliary network (TSAN), for transcoded video restoration. Our method considers the unique traits between video encoding and transcoding, and we consider the initial shallow encoded videos as the intermediate labels to assist the network to conduct self-supervised attention training. In addition, we employ adjacent multi-frame information and propose the temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration. The experimental results demonstrate that the performance of the proposed method is superior to that of the previous techniques. The code is available at https://github.com/icecherylXuli/TSAN.",
    "code_link": "https://github.com/icecherylXuli/TSAN"
  },
  "aaai2022_main_dirldomain-invariantrepresentationlearningforgeneralizablesemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DIRL: Domain-Invariant Representation Learning for Generalizable Semantic Segmentation",
    "authors": [
      "Qi Xu",
      "Liang Yao",
      "Zhengkai Jiang",
      "Guannan Jiang",
      "Wenqing Chu",
      "Wenhui Han",
      "Wei Zhang",
      "Chengjie Wang",
      "Ying Tai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20193",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20193/19952",
    "published": "2022-02",
    "summary": "Model generalization to the unseen scenes is crucial to real-world applications, such as autonomous driving, which requires robust vision systems. To enhance the model generalization, domain generalization through learning the domain-invariant representation has been widely studied. However, most existing works learn the shared feature space within multi-source domains but ignore the characteristic of the feature itself (e.g., the feature sensitivity to the domain-specific style). Therefore, we propose the Domain-invariant Representation Learning (DIRL) for domain generalization which utilizes the feature sensitivity as the feature prior to guide the enhancement of the model generalization capability. The guidance reflects in two folds: 1) Feature re-calibration that introduces the Prior Guided Attention Module (PGAM) to emphasize the insensitive features and suppress the sensitive features. 2): Feature whiting that proposes the Guided Feature Whiting (GFW) to remove the feature correlations which are sensitive to the domain-specific style. We construct the domain-invariant representation which suppresses the effect of the domain-specific style on the quality and correlation of the features. As a result, our method is simple yet effective, and can enhance the robustness of various backbone networks with little computational cost. Extensive experiments over multiple domains generalizable segmentation tasks show the superiority of our approach to other methods.",
    "code_link": ""
  },
  "aaai2022_main_behindthecurtainlearningoccludedshapesfor3dobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Behind the Curtain: Learning Occluded Shapes for 3D Object Detection",
    "authors": [
      "Qiangeng Xu",
      "Yiqi Zhong",
      "Ulrich Neumann"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20194",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20194/19953",
    "published": "2022-02",
    "summary": "Advances in LiDAR sensors provide rich 3D data that supports 3D scene understanding. However, due to occlusion and signal miss, LiDAR point clouds are in practice 2.5D as they cover only partial underlying shapes, which poses a fundamental challenge to 3D perception. To tackle the challenge, we present a novel LiDAR-based 3D object detection model, dubbed Behind the Curtain Detector (BtcDet), which learns the object shape priors and estimates the complete object shapes that are partially occluded (curtained) in point clouds. BtcDet first identifies the regions that are affected by occlusion and signal miss. In these regions, our model predicts the probability of occupancy that indicates if a region contains object shapes and integrates this probability map with detection features and generates high-quality 3D proposals. Finally, the occupancy estimation is integrated into the proposal refinement module to generate accurate bounding boxes. Extensive experiments on the KITTI Dataset and the Waymo Open Dataset demonstrate the effectiveness of BtcDet. Particularly for the 3D detection of both cars and cyclists on the KITTI benchmark, BtcDet surpasses all of the published state-of-the-art methods by remarkable margins. Code is released.",
    "code_link": ""
  },
  "aaai2022_main_domaindisentangledgenerativeadversarialnetworkforzero-shotsketch-based3dshaperetrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval",
    "authors": [
      "Rui Xu",
      "Zongyan Han",
      "Le Hui",
      "Jianjun Qian",
      "Jin Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20195",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20195/19954",
    "published": "2022-02",
    "summary": "Sketch-based 3D shape retrieval is a challenging task due to the large domain discrepancy between sketches and 3D shapes. Since existing methods are trained and evaluated on the same categories, they cannot effectively recognize the categories that have not been used during training. In this paper, we propose a novel domain disentangled generative adversarial network (DD-GAN) for zero-shot sketch-based 3D retrieval, which can retrieve the unseen categories that are not accessed during training. Specifically, we first generate domain-invariant features and domain-specific features by disentangling the learned features of sketches and 3D shapes, where the domain-invariant features are used to align with the corresponding word embeddings. Then, we develop a generative adversarial network that combines the domain-specific features of the seen categories with the aligned domain-invariant features to synthesize samples, where the synthesized samples of the unseen categories are generated by using the corresponding word embeddings. Finally, we use the synthesized samples of the unseen categories combined with the real samples of the seen categories to train the network for retrieval, so that the unseen categories can be recognized. In order to reduce the domain shift problem, we utilize unlabeled unseen samples to enhance the discrimination ability of the discriminator. With the discriminator distinguishing the generated samples from the unlabeled unseen samples, the generator can generate more realistic unseen samples. Extensive experiments on the SHREC'13 and SHREC'14 datasets show that our method significantly improves the retrieval performance of the unseen categories.",
    "code_link": ""
  },
  "aaai2022_main_dualattentionnetworksforfew-shotfine-grainedrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dual Attention Networks for Few-Shot Fine-Grained Recognition",
    "authors": [
      "Shu-Lin Xu",
      "Faen Zhang",
      "Xiu-Shen Wei",
      "Jianhua Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20196",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20196/19955",
    "published": "2022-02",
    "summary": "The task of few-shot fine-grained recognition is to classify images belonging to subordinate categories merely depending on few examples. Due to the fine-grained nature, it is desirable to capture subtle but discriminative part-level patterns from limited training data, which makes it a challenging problem. In this paper, to generate fine-grained tailored representations for few-shot recognition, we propose a Dual Attention Network (Dual Att-Net) consisting of two dual branches of both hard- and soft-attentions. Specifically, by producing attention guidance from deep activations of input images, our hard-attention is realized by keeping a few useful deep descriptors and forming them as a bag of multi-instance learning. Since these deep descriptors could correspond to objects' parts, the advantage of modeling as a multi-instance bag is able to exploit inherent correlation of these fine-grained parts. On the other side, a soft attended activation representation can be obtained by applying attention guidance upon original activations, which brings comprehensive attention information as the counterpart of hard-attention. After that, both outputs of dual branches are aggregated as a holistic image embedding w.r.t. input images. By performing meta-learning, we can learn a powerful image embedding in such a metric space to generalize to novel classes. Experiments on three popular fine-grained benchmark datasets show that our Dual Att-Net obviously outperforms other existing state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_sparsecross-scaleattentionnetworkforefficientlidarpanopticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sparse Cross-Scale Attention Network for Efficient LiDAR Panoptic Segmentation",
    "authors": [
      "Shuangjie Xu",
      "Rui Wan",
      "Maosheng Ye",
      "Xiaoyi Zou",
      "Tongyi Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20197",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20197/19956",
    "published": "2022-02",
    "summary": "Two major challenges of 3D LiDAR Panoptic Segmentation (PS) are that point clouds of an object are surface-aggregated and thus hard to model the long-range dependency especially for large instances, and that objects are too close to separate each other. Recent literature addresses these problems by time-consuming grouping processes such as dual-clustering, mean-shift offsets and etc., or by bird-eye-view (BEV) dense centroid representation that downplays geometry. However, the long-range geometry relationship has not been sufficiently modeled by local feature learning from the above methods. To this end, we present SCAN, a novel sparse cross-scale attention network to first align multi-scale sparse features with global voxel-encoded attention to capture the long-range relationship of instance context, which is able to boost the regression accuracy of the over-segmented large objects. For the surface-aggregated points, SCAN adopts a novel sparse class-agnostic representation of instance centroids, which can not only maintain the sparsity of aligned features to solve the under-segmentation on small objects, but also reduce the computation amount of the network through sparse convolution. Our method outperforms previous methods by a large margin in the SemanticKITTI dataset for the challenging 3D PS task, achieving 1st place with a real-time inference speed.",
    "code_link": ""
  },
  "aaai2022_main_towardsfullysparsetraininginformationrestorationwithspatialsimilarity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Fully Sparse Training: Information Restoration with Spatial Similarity",
    "authors": [
      "Weixiang Xu",
      "Xiangyu He",
      "Ke Cheng",
      "Peisong Wang",
      "Jian Cheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20198",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20198/19957",
    "published": "2022-02",
    "summary": "The 2:4 structured sparsity pattern released by NVIDIA Ampere architecture, requiring four consecutive values containing at least two zeros, enables doubling math throughput for matrix multiplications. Recent works mainly focus on inference speedup via 2:4 sparsity while training acceleration has been largely overwhelmed where backpropagation consumes around 70% of the training time. However, unlike inference, training speedup with structured pruning is nontrivial due to the need to maintain the fidelity of gradients and reduce the additional overhead of performing 2:4 sparsity online. For the first time, this article proposes fully sparse training (FST) where `fully' indicates that ALL matrix multiplications in forward/backward propagation are structurally pruned while maintaining accuracy. To this end, we begin with saliency analysis, investigating the sensitivity of different sparse objects to structured pruning. Based on the observation of spatial similarity among activations, we propose pruning activations with fixed 2:4 masks. Moreover, an Information Restoration block is proposed to retrieve the lost information, which can be implemented by efficient gradient-shift operation. Evaluation of accuracy and efficiency shows that we can achieve 2\u00d7 training acceleration with negligible accuracy degradation on challenging large-scale classification and detection tasks.",
    "code_link": ""
  },
  "aaai2022_main_hierarchicalimagegenerationviatransformer-basedsequentialpatchselection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hierarchical Image Generation via Transformer-Based Sequential Patch Selection",
    "authors": [
      "Xiaogang Xu",
      "Ning Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20199",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20199/19958",
    "published": "2022-02",
    "summary": "To synthesize images with preferred objects and interactions, a controllable way is to generate the image from a scene graph and a large pool of object crops, where the spatial arrangements of the objects in the image are defined by the scene graph while their appearances are determined by the retrieved crops from the pool. In this paper, we propose a novel framework with such a semi-parametric generation strategy. First, to encourage the retrieval of mutually compatible crops, we design a sequential selection strategy where the crop selection for each object is determined by the contents and locations of all object crops that have been chosen previously. Such process is implemented via a transformer trained with contrastive losses. Second, to generate the final image, our hierarchical generation strategy leverages hierarchical gated convolutions which are employed to synthesize areas not covered by any image crops, and a patch guided spatially adaptive normalization module which is proposed to guarantee the final generated images complying with the crop appearance and the scene graph. Evaluated on the challenging Visual Genome and COCO-Stuff dataset, our experimental results demonstrate the superiority of our proposed method over existing state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_reliablepropagation-correctionmodulationforvideoobjectsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reliable Propagation-Correction Modulation for Video Object Segmentation",
    "authors": [
      "Xiaohao Xu",
      "Jinglu Wang",
      "Xiao Li",
      "Yan Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20200",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20200/19959",
    "published": "2022-02",
    "summary": "Error propagation is a general but crucial problem in online semi-supervised video object segmentation.We aim to suppress error propagation through a correction mechanism with high reliability.The key insight is to disentangle the correction from the conventional mask propagation process with reliable cues.We introduce two modulators, propagation and correction modulators, to separately perform channel-wise recalibration on the target frame embeddings according to local temporal correlations and reliable references respectively.Specifically, we assemble the modulators with a cascaded propagation-correction scheme. This avoids overriding the effects of the reliable correction modulator by the propagation modulator. Although the reference frame with the ground truth label provides reliable cues, it could be very different from the target frame and introduce uncertain or incomplete correlations. We augment the reference cues by supplementing reliable feature patches to a maintained pool, thus offering more comprehensive and expressive object representations to the modulators. In addition, a reliability filter is designed to retrieve reliable patches and pass them in subsequent frames.Our model achieves state-of-the-art performance on YouTube-VOS18, YouTube-VOS19 and DAVIS17-Val/Test benchmarks.Extensive experiments demonstrate that the correction mechanism provides considerable performance gain by fully utilizing reliable guidance.",
    "code_link": ""
  },
  "aaai2022_main_adaptivehypergraphneuralnetworkformulti-personposeestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Hypergraph Neural Network for Multi-Person Pose Estimation",
    "authors": [
      "Xixia Xu",
      "Qi Zou",
      "Xue Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20201",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20201/19960",
    "published": "2022-02",
    "summary": "This paper proposes a novel two-stage hypergraph-based framework, dubbed ADaptive Hypergraph Neural Network (AD-HNN) to estimate multiple human poses from a single image, with a keypoint localization network and an Adaptive-Pose Hypergraph Neural Network (AP-HNN) added onto the former network. For providing better guided representations of AP-HNN, we employ a Semantic Interaction Convolution (SIC) module within the initial localization network to acquire more explicit predictions. Build upon this, we design a novel adaptive hypergraph to represent a human body for capturing high-order semantic relations among different joints. Notably, it can adaptively adjust the relations between joints and seek the most reasonable structure for the variable poses to benefit the keypoint localization. These two stages are combined to be trained in an end-to-end fashion. Unlike traditional Graph Convolutional Networks (GCNs) that are based on a fixed tree structure, AP-HNN can deal with ambiguity in human pose estimation. Experimental results demonstrate that the AD-HNN achieves state-of-the-art performance both on the MS-COCO, MPII and CrowdPose datasets.",
    "code_link": ""
  },
  "aaai2022_main_evo-vitslow-fasttokenevolutionfordynamicvisiontransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
    "authors": [
      "Yifan Xu",
      "Zhijie Zhang",
      "Mengdan Zhang",
      "Kekai Sheng",
      "Ke Li",
      "Weiming Dong",
      "Liqing Zhang",
      "Changsheng Xu",
      "Xing Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20202",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20202/19961",
    "published": "2022-02",
    "summary": "Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4% top-1 accuracy on ImageNet-1K, outperforming current token pruning methods on both accuracy and efficiency.",
    "code_link": "https://github.com/YifanXu74/Evo-ViT"
  },
  "aaai2022_main_mobilefaceswapalightweightframeworkforvideofaceswapping": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MobileFaceSwap: A Lightweight Framework for Video Face Swapping",
    "authors": [
      "Zhiliang Xu",
      "Zhibin Hong",
      "Changxing Ding",
      "Zhen Zhu",
      "Junyu Han",
      "Jingtuo\n      Liu",
      "Errui Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20203",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20203/19962",
    "published": "2022-02",
    "summary": "Advanced face swapping methods have achieved appealing results. However, most of these methods have many parameters and computations, which makes it challenging to apply them in real-time applications or deploy them on edge devices like mobile phones. In this work, we propose a lightweight Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by dynamically adjusting the model parameters according to the identity information. In particular, we design an efficient Identity Injection Module (IIM) by introducing two dynamic neural network techniques, including the weights prediction and weights modulation. Once the IDN is updated, it can be applied to swap faces given any target image or video. The presented IDN contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it capable for real-time video face swapping on mobile phones. In addition, we introduce a knowledge distillation-based method for stable training, and a loss reweighting module is employed to obtain better synthesized results. Finally, our method achieves comparable results with the teacher models and other state-of-the-art methods.",
    "code_link": "https://github.com/deepfakes/faceswap"
  },
  "aaai2022_main_clinical-bertvision-languagepre-trainingforradiographdiagnosisandreportsgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Clinical-BERT: Vision-Language Pre-training for Radiograph Diagnosis and Reports Generation",
    "authors": [
      "Bin Yan",
      "Mingtao Pei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20204",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20204/19963",
    "published": "2022-02",
    "summary": "In this paper, we propose a vision-language pre-training model, Clinical-BERT, for the medical domain, and devise three domain-specific tasks: Clinical Diagnosis (CD), Masked MeSH Modeling (MMM), Image-MeSH Matching (IMM), together with one general pre-training task: Masked Language Modeling (MLM), to pre-train the model. The CD task helps the model to learn medical domain knowledge by predicting disease from radiographs. Medical Subject Headings (MeSH) words are important semantic components in radiograph reports, and the MMM task helps the model focus on the prediction of MeSH words. The IMM task helps the model learn the alignment of MeSH words with radiographs by matching scores obtained by a two-level sparse attention: region sparse attention and word sparse attention. Region sparse attention generates corresponding visual features for each word, and word sparse attention enhances the contribution of images-MeSH matching to the matching scores. To the best of our knowledge, this is the first attempt to learn domain knowledge during pre-training for the medical domain. We evaluate the pre-training model on Radiograph Diagnosis and Reports Generation tasks across four challenging datasets: MIMIC-CXR, IU X-Ray, COV-CTR, and NIH, and achieve state-of-the-art results for all the tasks, which demonstrates the effectiveness of our pre-training model.",
    "code_link": "https://github.com/fxsjy/jieba"
  },
  "aaai2022_main_inferringprototypesformulti-labelfew-shotimageclassificationwithwordvectorguidedattention": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Inferring Prototypes for Multi-Label Few-Shot Image Classification with Word Vector Guided Attention",
    "authors": [
      "Kun Yan",
      "Chenbin Zhang",
      "Jun Hou",
      "Ping Wang",
      "Zied Bouraoui",
      "Shoaib Jameel",
      "Steven Schockaert"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20205",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20205/19964",
    "published": "2022-02",
    "summary": "Multi-label few-shot image classification (ML-FSIC) is the task of assigning descriptive labels to previously unseen images, based on a small number of training examples. A key feature of the multi-label setting is that images often have multiple labels, which typically refer to different regions of the image. When estimating prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data makes this highly challenging. As a solution, in this paper we propose to use word embeddings as a form of prior knowledge about the meaning of the labels. In particular, visual prototypes are obtained by aggregating the local feature maps of the support images, using an attention mechanism that relies on the label embeddings. As an important advantage, our model can infer prototypes for unseen labels without the need for fine-tuning any model parameters, which demonstrates its strong generalization abilities. Experiments on COCO and PASCAL VOC furthermore show that our model substantially improves the current state-of-the-art.",
    "code_link": ""
  },
  "aaai2022_main_unsuperviseddomainadaptivesalientobjectdetectionthroughuncertainty-awarepseudo-labellearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Domain Adaptive Salient Object Detection through Uncertainty-Aware Pseudo-Label Learning",
    "authors": [
      "Pengxiang Yan",
      "Ziyi Wu",
      "Mengmeng Liu",
      "Kun Zeng",
      "Liang Lin",
      "Guanbin Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20206",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20206/19965",
    "published": "2022-02",
    "summary": "Recent advances in deep learning significantly boost the performance of salient object detection (SOD) at the expense of labeling larger-scale per-pixel annotations. To relieve the burden of labor-intensive labeling, deep unsupervised SOD methods have been proposed to exploit noisy labels generated by handcrafted saliency methods. However, it is still difficult to learn accurate saliency details from rough noisy labels. In this paper, we propose to learn saliency from synthetic but clean labels, which naturally has higher pixel-labeling quality without the effort of manual annotations. Specifically, we first construct a novel synthetic SOD dataset by a simple copy-paste strategy. Considering the large appearance differences between the synthetic and real-world scenarios, directly training with synthetic data will lead to performance degradation on real-world scenarios. To mitigate this problem, we propose a novel unsupervised domain adaptive SOD method to adapt between these two domains by uncertainty-aware self-training. Experimental results show that our proposed method outperforms the existing state-of-the-art deep unsupervised SOD methods on several benchmark datasets, and is even comparable to fully-supervised ones.",
    "code_link": ""
  },
  "aaai2022_main_transmission-guidedbayesiangenerativemodelforsmokesegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transmission-Guided Bayesian Generative Model for Smoke Segmentation",
    "authors": [
      "Siyuan Yan",
      "Jing Zhang",
      "Nick Barnes"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20207",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20207/19966",
    "published": "2022-02",
    "summary": "Smoke segmentation is essential to precisely localize wild\ufb01re so that it can be extinguished in an early phase. Although deep neural networks have achieved promising results on image segmentation tasks, they are prone to be overcon\ufb01dent for smoke segmentation due to its non-rigid shape and transparent appearance. This is caused by both knowledge level uncertainty due to limited training data for accurate smoke segmentation and labeling level uncertainty representing the dif\ufb01culty in labeling ground-truth. To effectively model the two types of uncertainty, we introduce a Bayesian generative model to simultaneously estimate the posterior distribution of model parameters and its predictions. Further, smoke images suffer from low contrast and ambiguity, inspired by physics-based image dehazing methods, we design a transmission-guided local coherence loss to guide the network to learn pair-wise relationships based on pixel distance and the transmission feature. To promote the development of this \ufb01eld, we also contribute a high-quality smoke segmentation dataset, SMOKE5K, consisting of 1,400 real and 4,000 synthetic images with pixel-wise annotation. Experimental results on benchmark testing datasets illustrate that our model achieves both accurate predictions and reliable uncertainty maps representing model ignorance about its prediction. Our code and dataset are publicly available at: https://github.com/redlessme/Transmission-BVM.",
    "code_link": ""
  },
  "aaai2022_main_cross-species3dfacemorphingviaalignment-awarecontroller": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Species 3D Face Morphing via Alignment-Aware Controller",
    "authors": [
      "Xirui Yan",
      "Zhenbo Yu",
      "Bingbing Ni",
      "Hang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20208",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20208/19967",
    "published": "2022-02",
    "summary": "We address cross-species 3D face morphing (i.e., 3D face morphing from human to animal), a novel problem with promising applications in social media and movie industry. It remains challenging how to preserve target structural information and source \ufb01ne-grained facial details simultaneously. To this end, we propose an Alignment-aware 3D Face Morphing (AFM) framework, which builds semantic-adaptive correspondence between source and target faces across species, via an alignment-aware controller mesh (Explicit Controller, EC) with explicit source/target mesh binding. Based on EC, we introduce Controller-Based Mapping (CBM), which builds semantic consistency between source and target faces according to the semantic importance of different face regions. Additionally, an inference-stage coarse-to-\ufb01ne strategy is exploited to produce \ufb01ne-grained meshes with rich facial details from rough meshes. Extensive experimental results in multiple people and animals demonstrate that our method produces high-quality deformation results.",
    "code_link": ""
  },
  "aaai2022_main_exploringvisualcontextforweaklysupervisedpersonsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploring Visual Context for Weakly Supervised Person Search",
    "authors": [
      "Yichao Yan",
      "Jinpeng Li",
      "Shengcai Liao",
      "Jie Qin",
      "Bingbing Ni",
      "Ke Lu",
      "Xiaokang Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20209",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20209/19968",
    "published": "2022-02",
    "summary": "Person search has recently emerged as a challenging task that jointly addresses pedestrian detection and person re-identification. Existing approaches follow a fully supervised setting where both bounding box and identity annotations are available. However, annotating identities is labor-intensive, limiting the practicability and scalability of current frameworks. This paper inventively considers weakly supervised person search with only bounding box annotations. We propose to address this novel task by investigating three levels of context clues (i.e., detection, memory and scene) in unconstrained natural images. The first two are employed to promote local and global discriminative capabilities, while the latter enhances clustering accuracy. Despite its simple design, our CGPS boosts the baseline model by 8.8% in mAP on CUHK-SYSU. Surprisingly, it even achieves comparable performance with several supervised person search models. Our code is available at https://github. com/ljpadam/CGPS.",
    "code_link": ""
  },
  "aaai2022_main_cross-modalmutuallearningforaudio-visualspeechrecognitionandmanipulation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation",
    "authors": [
      "Chih-Chun Yang",
      "Wan-Cyuan Fan",
      "Cheng-Fu Yang",
      "Yu-Chiang Frank Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20210",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20210/19969",
    "published": "2022-02",
    "summary": "As a key characteristic in audio-visual speech recognition (AVSR), relating linguistic information observed across visual and audio data has been a challenge, benefiting not only audio/visual speech recognition (ASR/VSR) but also for manipulating data within/across modalities. In this paper, we present a feature disentanglement-based framework for jointly addressing the above tasks. By advancing cross-modal mutual learning strategies, our model is able to convert visual or audio-based linguistic features into modality-agnostic representations. Such derived linguistic representations not only allow one to perform ASR, VSR, and AVSR, but also to manipulate audio and visual data output based on the desirable subject identity and linguistic content information. We perform extensive experiments on different recognition and synthesis tasks to show that our model performs favorably against state-of-the-art approaches on each individual task, while ours is a unified solution that is able to jointly tackle the aforementioned audio-visual learning tasks.",
    "code_link": ""
  },
  "aaai2022_main_mutualcontrastivelearningforvisualrepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Mutual Contrastive Learning for Visual Representation Learning",
    "authors": [
      "Chuanguang Yang",
      "Zhulin An",
      "Linhang Cai",
      "Yongjun Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20211",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20211/19970",
    "published": "2022-02",
    "summary": "We present a collaborative learning method called Mutual Contrastive Learning (MCL) for general visual representation learning. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks. A crucial component of MCL is Interactive Contrastive Learning (ICL). Compared with vanilla contrastive learning, ICL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations for visual recognition tasks. We emphasize that the resulting MCL is conceptually simple yet empirically powerful. It is a generic framework that can be applied to both supervised and self-supervised representation learning. Experimental results on image classification and transfer learning to object detection show that MCL can lead to consistent performance gains, demonstrating that MCL can guide the network to generate better feature representations. Code is available at https://github.com/winycg/MCL.",
    "code_link": "https://github.com/winycg/MCL"
  },
  "aaai2022_main_temporalactionproposalgenerationwithbackgroundconstraint": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Temporal Action Proposal Generation with Background Constraint",
    "authors": [
      "Haosen Yang",
      "Wenhao Wu",
      "Lining Wang",
      "Sheng Jin",
      "Boyang Xia",
      "Hongxun Yao",
      "Hujie Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20212",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20212/19971",
    "published": "2022-02",
    "summary": "Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries.To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth.In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels.Extensive experiments are conducted on two popular benchmarks, ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task.",
    "code_link": ""
  },
  "aaai2022_main_cross-modalfederatedhumanactivityrecognitionviamodality-agnosticandmodality-specificrepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning",
    "authors": [
      "Xiaoshan Yang",
      "Baochen Xiong",
      "Yi Huang",
      "Changsheng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20213",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20213/19972",
    "published": "2022-02",
    "summary": "In this paper, we propose a new task of cross-modal federated human activity recognition (CMF-HAR), which is conducive to promote the large-scale use of the HAR model on more local devices. To address the new task, we propose a feature-disentangled activity recognition network (FDARN), which has five important modules of altruistic encoder, egocentric encoder, shared activity classifier, private activity classifier and modality discriminator. The altruistic encoder aims to collaboratively embed local instances on different clients into a modality-agnostic feature subspace. The egocentric encoder aims to produce modality-specific features that cannot be shared across clients with different modalities. The modality discriminator is used to adversarially guide the parameter learning of the altruistic and egocentric encoders. Through decentralized optimization with a spherical modality discriminative loss, our model can not only generalize well across different clients by leveraging the modality-agnostic features but also capture the modality-specific discriminative characteristics of each client. Extensive experiment results on four datasets demonstrate the effectiveness of our method.",
    "code_link": ""
  },
  "aaai2022_main_polygon-to-polygondistancelossforrotatedobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Polygon-to-Polygon Distance Loss for Rotated Object Detection",
    "authors": [
      "Yang Yang",
      "Jifeng Chen",
      "Xiaopin Zhong",
      "Yuanlong Deng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20214",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20214/19973",
    "published": "2022-02",
    "summary": "There are two key issues that limit further improvements in the performance of existing rotational detectors: 1) Periodic sudden change of the parameters in the rotating bounding box (RBBox) definition causes a numerical discontinuity in the loss (such as smoothL1 loss). 2) There is a gap of optimization asynchrony between the loss in the RBBox regression and evaluation metrics. In this paper, we define a new distance formulation between two convex polygons describing the overlapping degree and non-overlapping degree. Based on this smooth distance, we propose a loss called Polygon-to-Polygon distance loss (P2P Loss). The distance is derived from the area sum of triangles specified by the vertexes of one polygon and the edges of the other. Therefore, the P2P Loss is continuous, differentiable, and inherently free from any RBBox definition. Our P2P Loss is not only consistent with the detection metrics but also able to measure how far, as well as how similar, a RBBox is from another one even when they are completely non-overlapping. These features allow the RetinaNet using the P2P Loss to achieve 79.15% mAP on the DOTA dataset, which is quite competitive compared with many state-of-the-art rotated object detectors.",
    "code_link": ""
  },
  "aaai2022_main_anempiricalstudyofgpt-3forfew-shotknowledge-basedvqa": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
    "authors": [
      "Zhengyuan Yang",
      "Zhe Gan",
      "Jianfeng Wang",
      "Xiaowei Hu",
      "Yumao Lu",
      "Zicheng Liu",
      "Lijuan Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20215",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20215/19974",
    "published": "2022-02",
    "summary": "Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3\u2019s power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.",
    "code_link": ""
  },
  "aaai2022_main_acgnetactioncomplementgraphnetworkforweakly-supervisedtemporalactionlocalization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ACGNet: Action Complement Graph Network for Weakly-Supervised Temporal Action Localization",
    "authors": [
      "Zichen Yang",
      "Jie Qin",
      "Di Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20216",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20216/19975",
    "published": "2022-02",
    "summary": "Weakly-supervised temporal action localization (WTAL) in untrimmed videos has emerged as a practical but challenging task since only video-level labels are available. Existing approaches typically leverage off-the-shelf segment-level features, which suffer from spatial incompleteness and temporal incoherence, thus limiting their performance. In this paper, we tackle this problem from a new perspective by enhancing segment-level representations with a simple yet effective graph convolutional network, namely action complement graph network (ACGNet). It facilitates the current video segment to perceive spatial-temporal dependencies from others that potentially convey complementary clues, implicitly mitigating the negative effects caused by the two issues above. By this means, the segment-level features are more discriminative and robust to spatial-temporal variations, contributing to higher localization accuracies. More importantly, the proposed ACGNet works as a universal module that can be flexibly plugged into different WTAL frameworks, while maintaining the end-to-end training fashion. Extensive experiments are conducted on the THUMOS'14 and ActivityNet1.2 benchmarks, where the state-of-the-art results clearly demonstrate the superiority of the proposed approach.",
    "code_link": ""
  },
  "aaai2022_main_enhancingpseudolabelqualityforsemi-superviseddomain-generalizedmedicalimagesegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Enhancing Pseudo Label Quality for Semi-supervised Domain-Generalized Medical Image Segmentation",
    "authors": [
      "Huifeng Yao",
      "Xiaowei Hu",
      "Xiaomeng Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20217",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20217/19976",
    "published": "2022-02",
    "summary": "Generalizing the medical image segmentation algorithms to unseen domains is an important research topic for computer-aided diagnosis and surgery. Most existing methods require a fully labeled dataset in each source domain. Although some researchers developed a semi-supervised domain generalized method, it still requires the domain labels. This paper presents a novel confidence-aware cross pseudo supervision algorithm for semi-supervised domain generalized medical image segmentation. The main goal is to enhance the pseudo label quality for unlabeled images from unknown distributions. To achieve it, we perform the Fourier transformation to learn low-level statistic information across domains and augment the images to incorporate cross-domain information. With these augmentations as perturbations, we feed the input to a confidence-aware cross pseudo supervision network to measure the variance of pseudo labels and regularize the network to learn with more confident pseudo labels. Our method sets new records on public datasets, i.e., M&Ms and SCGM. Notably, without using domain labels, our method surpasses the prior art that even uses domain labels by 11.67% on Dice on M&Ms dataset with 2% labeled data. Code is available at https://github.com/XMed-Lab/EPL SemiDG.",
    "code_link": "https://github.com/XMed-Lab/EPL"
  },
  "aaai2022_main_imagedifferencecaptioningwithpre-trainingandcontrastivelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Image Difference Captioning with Pre-training and Contrastive Learning",
    "authors": [
      "Linli Yao",
      "Weiying Wang",
      "Qin Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20218",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20218/19977",
    "published": "2022-02",
    "summary": "The Image Difference Captioning (IDC) task aims to describe the visual differences between two similar images with natural language. The major challenges of this task lie in two aspects: 1) fine-grained visual differences that require learning stronger vision and language association and 2) high-cost of manual annotations that leads to limited supervised data. To address these challenges, we propose a new modeling framework following the pre-training-finetuning paradigm. Specifically, we design three self-supervised tasks and contrastive learning strategies to align visual differences and text descriptions at a fine-grained level. Moreover, we propose a data expansion strategy to utilize extra cross-task supervision information, such as data for fine-grained image classification, to alleviate the limitation of available supervised IDC data. Extensive experiments on two IDC benchmark datasets, CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed modeling framework. The codes and models will be released at https://github.com/yaolinli/IDC.",
    "code_link": "https://github.com/yaolinli/IDC"
  },
  "aaai2022_main_safedistillationbox": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Safe Distillation Box",
    "authors": [
      "Jingwen Ye",
      "Yining Mao",
      "Jie Song",
      "Xinchao Wang",
      "Cheng Jin",
      "Mingli Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20219",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20219/19978",
    "published": "2022-02",
    "summary": "Knowledge distillation (KD) has recently emerged as a powerful strategy to transfer knowledge from a pre-trained teacher model to a lightweight student, and has demonstrated its unprecedented success over a wide spectrum of applications. In spite of the encouraging results, the KD process \\emph{per se} poses a potential threat to network ownership protection, since the knowledge contained in network can be effortlessly distilled and hence exposed to a malicious user.In this paper, we propose a novel framework, termed as Safe Distillation Box~(SDB), that allows us to wrap a pre-trained model in a virtual box for intellectual property protection. Specifically, SDB preserves the inference capability of the wrapped model to all users, but precludes KD from unauthorized users. For authorized users, on the other hand, SDB carries out a knowledge augmentation scheme to strengthen the KD performances and the results of the student model. In other words, all users may employ a model in SDB for inference, but only authorized users get access to KD from the model. The proposed SDB imposes no constraints over the model architecture, and may readily serve as a plug-and-play solution to protect the ownership of a pre-trained network. Experiments across various datasets and architectures demonstrate that, with SDB, the performance of an unauthorized KD drops significantly while that of an authorized gets enhanced, demonstrating the effectiveness of SDB.",
    "code_link": ""
  },
  "aaai2022_main_jointdeepmulti-graphmatchingand3dgeometrylearningfrominhomogeneous2dimagecollections": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections",
    "authors": [
      "Zhenzhang Ye",
      "Tarun Yenamandra",
      "Florian Bernard",
      "Daniel Cremers"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20220",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20220/19979",
    "published": "2022-02",
    "summary": "Graph matching aims to establish correspondences between vertices of graphs such that both the node and edge attributes agree. Various learning-based methods were recently proposed for finding correspondences between image key points based on deep graph matching formulations. While these approaches mainly focus on learning node and edge attributes, they completely ignore the 3D geometry of the underlying 3D objects depicted in the 2D images. We fill this gap by proposing a trainable framework that takes advantage of graph neural networks for learning a deformable 3D geometry model from inhomogeneous image collections, i.e. a set of images that depict different instances of objects from the same category. Experimentally we demonstrate that our method outperforms recent learning-based approaches for graph matching considering both accuracy and cycle-consistency error, while we in addition obtain the underlying 3D geometry of the objects depicted in the 2D images.",
    "code_link": ""
  },
  "aaai2022_main_content-variantreferenceimagequalityassessmentviaknowledgedistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Content-Variant Reference Image Quality Assessment via Knowledge Distillation",
    "authors": [
      "Guanghao Yin",
      "Wei Wang",
      "Zehuan Yuan",
      "Chuchu Han",
      "Wei Ji",
      "Shouqian Sun",
      "Changhu Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20221",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20221/19980",
    "published": "2022-02",
    "summary": "Generally, humans are more skilled at perceiving differences between high-quality (HQ) and low-quality (LQ) images than directly judging the quality of a single LQ image. This situation also applies to image quality assessment (IQA). Although recent no-reference (NR-IQA) methods have made great progress to predict image quality free from the reference image, they still have the potential to achieve better performance since HQ image information is not fully exploited. In contrast, full-reference (FR-IQA) methods tend to provide more reliable quality evaluation, but its practicability is affected by the requirement for pixel-level aligned reference images. To address this, we firstly propose the content-variant reference method via knowledge distillation (CVRKD-IQA). Specifically, we use non-aligned reference (NAR) images to introduce various prior distributions of high-quality images. The comparisons of distribution differences between HQ and LQ images can help our model better assess the image quality. Further, the knowledge distillation transfers more HQ-LQ distribution difference information from the FR-teacher to the NAR-student and stabilizing CVRKD-IQA performance. Moreover, to fully mine the local-global combined information, while achieving faster inference speed, our model directly processes multiple image patches from the input with the MLP-mixer. Cross-dataset experiments verify that our model can outperform all NAR/NR-IQA SOTAs, even reach comparable performance than FR-IQA methods on some occasions. Since the content-variant and non-aligned reference HQ images are easy to obtain, our model can support more IQA applications with its robustness to content variations. Our code is available: https://github.com/guanghaoyin/CVRKD-IQA.",
    "code_link": "https://github.com/guanghaoyin/CVRKD-IQA"
  },
  "aaai2022_main_width&depthpruningforvisiontransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Width & Depth Pruning for Vision Transformers",
    "authors": [
      "Fang Yu",
      "Kun Huang",
      "Meng Wang",
      "Yuan Cheng",
      "Wei Chu",
      "Li Cui"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20222",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20222/19981",
    "published": "2022-02",
    "summary": "Transformer models have demonstrated their promising potential and achieved excellent performance on a series of computer vision tasks. However, the huge computational cost of vision transformers hinders their deployment and application to edge devices. Recent works have proposed to \ufb01nd and remove the unimportant units of vision transformers. Despite achieving remarkable results, these methods take one dimension of network width into consideration and ignore network depth, which is another important dimension for pruning vision transformers. Therefore, we propose a Width & Depth Pruning (WDPruning) framework that reduces both width and depth dimensions simultaneously. Speci\ufb01cally, for width pruning, a set of learnable pruning-related parameters is used to adaptively adjust the width of transformer. For depth pruning, we introduce several shallow classi\ufb01ers by using the intermediate information of the transformer blocks, which allows images to be classi\ufb01ed by shallow classi\ufb01ers instead of the deeper classi\ufb01ers. In the inference period, all of the blocks after shallow classi\ufb01ers can be dropped so they don\u2019t bring additional parameters and computation. Experimental results on benchmark datasets demonstrate that the proposed method can signi\ufb01cantly reduce the computational costs of mainstream vision transformers such as DeiT and Swin Transformer with a minor accuracy drop. In particular, on ILSVRC-12, we achieve over 22% pruning ratio of FLOPs by compressing DeiT-Base, even with an increase of 0.14% Top-1 accuracy.",
    "code_link": ""
  },
  "aaai2022_main_anisotropicfourierfeaturesforneuralimage-basedrenderingandrelighting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Anisotropic Fourier Features for Neural Image-Based Rendering and Relighting",
    "authors": [
      "Huangjie Yu",
      "Anpei Chen",
      "Xin Chen",
      "Lan Xu",
      "Ziyu Shao",
      "Jingyi Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20223",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20223/19982",
    "published": "2022-02",
    "summary": "Recent neural rendering techniques have greatly benefited image-based modeling and relighting tasks. They provide a continuous, compact, and parallelable representation by modeling the plenoptic function as multilayer perceptrons (MLPs). However, vanilla MLPs suffer from spectral biases on multidimensional datasets. Recent rescues based on isotropic Fourier features mapping mitigate the problem but still fall short of handling heterogeneity across different dimensions, causing imbalanced regression and visual artifacts such as excessive blurs. We present an anisotropic random Fourier features (RFF) mapping scheme to tackle spectral biases. We first analyze the influence of bandwidth from a different perspective: we show that the optimal bandwidth exhibits strong correlations with the frequency spectrum of the training data across various dimensions. We then introduce an anisotropic feature mapping scheme with multiple bandwidths to model the multidimensional signal characteristics. We further propose an efficient bandwidth searching scheme through iterative golden-section search that can significantly reduce the training overload from polynomial time to logarithm. Our anisotropic scheme directly applies to neural surface light-field rendering and image-based relighting. Comprehensive experiments show that our scheme can more faithfully model lighting conditions and object features as well as preserve fine texture details and smooth view transitions even when angular and spatial samples are highly imbalanced.",
    "code_link": ""
  },
  "aaai2022_main_self-labelingframeworkfornovelcategorydiscoveryoverdomains": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Labeling Framework for Novel Category Discovery over Domains",
    "authors": [
      "Qing Yu",
      "Daiki Ikami",
      "Go Irie",
      "Kiyoharu Aizawa"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20224",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20224/19983",
    "published": "2022-02",
    "summary": "Unsupervised domain adaptation (UDA) has been highly successful in transferring knowledge acquired from a label-rich source domain to a label-scarce target domain. Open-set domain adaptation (open-set DA) and universal domain adaptation (UniDA) have been proposed as solutions to the problem concerning the presence of additional novel categories in the target domain. Existing open-set DA and UniDA approaches treat all novel categories as one unified unknown class and attempt to detect this unknown class during the training process. However, the features of the novel categories learned by these methods are not discriminative. This limits the applicability of UDA in the further classification of these novel categories into their original categories, rather than assigning them to a single unified class. In this paper, we propose a self-labeling framework to cluster all target samples, including those in the ''unknown'' categories. We train the network to learn the representations of target samples via self-supervised learning (SSL) and to identify the seen and unseen (novel) target-sample categories simultaneously by maximizing the mutual information between labels and input data. We evaluated our approach under different DA settings and concluded that our method generally outperformed existing ones by a wide margin.",
    "code_link": ""
  },
  "aaai2022_main_efficientcompactbilinearpoolingviakroneckerproduct": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Compact Bilinear Pooling via Kronecker Product",
    "authors": [
      "Tan Yu",
      "Yunfeng Cai",
      "Ping Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20225",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20225/19984",
    "published": "2022-02",
    "summary": "Bilinear pooling has achieved excellent performance in fine-grained recognition tasks. Nevertheless, high-dimensional bilinear features suffer from over-fitting and inefficiency. To alleviate these issues, compact bilinear pooling (CBP) methods were developed to generate low-dimensional features. Although the low-dimensional features from existing CBP methods enable high efficiency in subsequent classification, CBP methods themselves are inefficient. Thus, the inefficiency issue of the bilinear pooling is still unsolved. In this work, we propose an efficient compact bilinear pooling method to solve the inefficiency problem inherited in bilinear pooling thoroughly. It decomposes the huge-scale projection matrix into a two-level Kronecker product of several small-scale matrices. By exploiting the ``vec trick'' and the tensor modal product, we can obtain the compact bilinear feature through the decomposed projection matrices in a speedy manner. Systematic experiments on four public benchmarks using two backbones demonstrate the efficiency and effectiveness of the proposed method in fine-grained recognition.",
    "code_link": ""
  },
  "aaai2022_main_hybridgraphneuralnetworksforfew-shotlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hybrid Graph Neural Networks for Few-Shot Learning",
    "authors": [
      "Tianyuan Yu",
      "Sen He",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20226",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20226/19985",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs) have been used to tackle the few-shot learning (FSL) problem and shown great potentials under the transductive setting. However under the inductive setting, existing GNN based methods are less competitive. This is because they use an instance GNN as a label propagation/classification module, which is jointly meta-learned with a feature embedding network. This design is problematic because the classifier needs to adapt quickly to new tasks while the embedding does not. To overcome this problem, in this paper we propose a novel hybrid GNN (HGNN) model consisting of two GNNs, an instance GNN and a prototype GNN. Instead of label propagation, they act as feature embedding adaptation modules for quick adaptation of the meta-learned feature embedding to new tasks. Importantly they are designed to deal with a fundamental yet often neglected challenge in FSL, that is, with only a handful of shots per class, any few-shot classifier would be sensitive to badly sampled shots which are either outliers or can cause inter-class distribution overlapping. Extensive experiments show that our HGNN obtains new state-of-the-art on three FSL benchmarks. The code and models are available at https://github.com/TianyuanYu/HGNN.",
    "code_link": "https://github.com/TianyuanYu/HGNN"
  },
  "aaai2022_main_soitsegmentingobjectswithinstance-awaretransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SOIT: Segmenting Objects with Instance-Aware Transformers",
    "authors": [
      "Xiaodong Yu",
      "Dahu Shi",
      "Xing Wei",
      "Ye Ren",
      "Tingqun Ye",
      "Wenming Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20227",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20227/19986",
    "published": "2022-02",
    "summary": "This paper presents an end-to-end instance segmentation framework, termed SOIT, that Segments Objects with Instance-aware Transformers. Inspired by DETR, our method views instance segmentation as a direct set prediction problem and effectively removes the need for many hand-crafted components like RoI cropping, one-to-many label assignment, and non-maximum suppression (NMS). In SOIT, multiple queries are learned to directly reason a set of object embeddings of semantic category, bounding-box location, and pixel-wise mask in parallel under the global image context. The class and bounding-box can be easily embedded by a fixed-length vector. The pixel-wise mask, especially, is embedded by a group of parameters to construct a lightweight instance-aware transformer. Afterward, a full-resolution mask is produced by the instance-aware transformer without involving any RoI-based operation. Overall, SOIT introduces a simple single-stage instance segmentation framework that is both RoI- and NMS-free. Experimental results on the MS COCO dataset demonstrate that SOIT outperforms state-of-the-art instance segmentation approaches significantly. Moreover, the joint learning of multiple tasks in a unified query embedding can also substantially improve the detection performance. Code is available at https://github.com/yuxiaodongHRI/SOIT.",
    "code_link": "https://github.com/yuxiaodongHRI/SOIT"
  },
  "aaai2022_main_msmlenhancingocclusion-robustnessbymulti-scalesegmentation-basedmasklearningforfacerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MSML: Enhancing Occlusion-Robustness by Multi-Scale Segmentation-Based Mask Learning for Face Recognition",
    "authors": [
      "Ge Yuan",
      "Huicheng Zheng",
      "Jiayu Dong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20228",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20228/19987",
    "published": "2022-02",
    "summary": "In unconstrained scenarios, face recognition remains challenging, particularly when faces are occluded. Existing methods generalize poorly due to the distribution distortion induced by unpredictable occlusions. To tackle this problem, we propose a hierarchical segmentation-based mask learning strategy for face recognition, enhancing occlusion-robustness by integrating segmentation representations of occlusion into face recognition in the latent space. We present a novel multi-scale segmentation-based mask learning (MSML) network, which consists of a face recognition branch (FRB), an occlusion segmentation branch (OSB), and hierarchical elaborate feature masking (FM) operators. With the guidance of hierarchical segmentation representations of occlusion learned by the OSB, the FM operators can generate multi-scale latent masks to eliminate mistaken responses introduced by occlusions and purify the contaminated facial features at multiple layers. In this way, the proposed MSML network can effectively identify and remove the occlusions from feature representations at multiple levels and aggregate features from visible facial areas. Experiments on face verification and recognition under synthetic or realistic occlusions demonstrate the effectiveness of our method compared to state-of-the-art methods.",
    "code_link": "https://github.com/ygtxr1997/MSML"
  },
  "aaai2022_main_detectinghuman-objectinteractionswithobject-guidedcross-modalcalibratedsemantics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics",
    "authors": [
      "Hangjie Yuan",
      "Mang Wang",
      "Dong Ni",
      "Liangpeng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20229",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20229/19988",
    "published": "2022-02",
    "summary": "Human-Object Interaction (HOI) detection is an essential task to understand human-centric images from a fine-grained perspective. Although end-to-end HOI detection models thrive, their paradigm of parallel human/object detection and verb class prediction loses two-stage methods' merit: object-guided hierarchy. The object in one HOI triplet gives direct clues to the verb to be predicted. In this paper, we aim to boost end-to-end models with object-guided statistical priors. Specifically, We propose to utilize a Verb Semantic Model (VSM) and use semantic aggregation to profit from this object-guided hierarchy. Similarity KL (SKL) loss is proposed to optimize VSM to align with the HOI dataset's priors. To overcome the static semantic embedding problem, we propose to generate cross-modality-aware visual and semantic features by Cross-Modal Calibration (CMC). The above modules combined composes Object-guided Cross-modal Calibration Network (OCN). Experiments conducted on two popular HOI detection benchmarks demonstrate the significance of incorporating the statistical prior knowledge and produce state-of-the-art performances. More detailed analysis indicates proposed modules serve as a stronger verb predictor and a more superior method of utilizing prior knowledge. The codes are available at https://github.com/JacobYuan7/OCN-HOI-Benchmark.",
    "code_link": "https://github.com/JacobYuan7/OCNHOI-Benchmark"
  },
  "aaai2022_main_task-levelself-supervisionforcross-domainfew-shotlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Task-Level Self-Supervision for Cross-Domain Few-Shot Learning",
    "authors": [
      "Wang Yuan",
      "Zhizhong Zhang",
      "Cong Wang",
      "Haichuan Song",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20230",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20230/19989",
    "published": "2022-02",
    "summary": "Learning with limited labeled data is a long-standing problem. Among various solutions, episodic training progres-sively classifies a series of few-shot tasks and thereby is as-sumed to be beneficial for improving the model\u2019s generalization ability. However, recent studies show that it is eveninferior to the baseline model when facing domain shift between base and novel classes. To tackle this problem, we pro-pose a domain-independent task-level self-supervised (TL-SS) method for cross-domain few-shot learning.TL-SS strategy promotes the general idea of label-based instance-levelsupervision to task-level self-supervision by augmenting mul-tiple views of tasks. Two regularizations on task consistencyand correlation metric are introduced to remarkably stabi-lize the training process and endow the generalization ability into the prediction model. We also propose a high-order associated encoder (HAE) being adaptive to various tasks.By utilizing 3D convolution module, HAE is able to generate proper parameters and enables the encoder to flexibly toany unseen tasks. Two modules complement each other andshow great promotion against state-of-the-art methods experimentally. Finally, we design a generalized task-agnostic test,where our intriguing findings highlight the need to re-think the generalization ability of existing few-shot approaches.",
    "code_link": ""
  },
  "aaai2022_main_improving360monoculardepthestimationvianon-localdensepredictiontransformerandjointsupervisedandself-supervisedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-Supervised Learning",
    "authors": [
      "Ilwi Yun",
      "Hyuk-Jae Lee",
      "Chae Eun Rhee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20231",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20231/19990",
    "published": "2022-02",
    "summary": "Due to difficulties in acquiring ground truth depth of equirectangular (360) images, the quality and quantity of equirectangular depth data today is insufficient to represent the various scenes in the world. Therefore, 360 depth estimation studies, which relied solely on supervised learning, are destined to produce unsatisfactory results. Although self-supervised learning methods focusing on equirectangular images (EIs) are introduced, they often have incorrect or non-unique solutions, causing unstable performance. In this paper, we propose 360 monocular depth estimation methods which improve on the areas that limited previous studies. First, we introduce a self-supervised 360 depth learning method that only utilizes gravity-aligned videos, which has the potential to eliminate the needs for depth data during the training procedure. Second, we propose a joint learning scheme realized by combining supervised and self-supervised learning. The weakness of each learning is compensated, thus leading to more accurate depth estimation. Third, we propose a non-local fusion block, which can further retain the global information encoded by vision transformer when reconstructing the depths. With the proposed methods, we successfully apply the transformer to 360 depth estimations, to the best of our knowledge, which has not been tried before. On several benchmarks, our approach achieves significant improvements over previous works and establishes a state of the art.",
    "code_link": ""
  },
  "aaai2022_main_homographydecompositionnetworksforplanarobjecttracking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Homography Decomposition Networks for Planar Object Tracking",
    "authors": [
      "Xinrui Zhan",
      "Yueran Liu",
      "Jianke Zhu",
      "Yang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20232",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20232/19991",
    "published": "2022-02",
    "summary": "Planar object tracking plays an important role in AI applications, such as robotics, visual servoing, and visual SLAM. Although the previous planar trackers work well in most scenarios, it is still a challenging task due to the rapid motion and large transformation between two consecutive frames. The essential reason behind this problem is that the condition number of such a non-linear system changes unstably when the searching range of the homography parameter space becomes larger. To this end, we propose a novel Homography Decomposition Networks~(HDN) approach that drastically reduces and stabilizes the condition number by decomposing the homography transformation into two groups. Specifically, a similarity transformation estimator is designed to predict the first group robustly by a deep convolution equivariant network. By taking advantage of the scale and rotation estimation with high confidence, a residual transformation is estimated by a simple regression model. Furthermore, the proposed end-to-end network is trained in a semi-supervised fashion. Extensive experiments show that our proposed approach outperforms the state-of-the-art planar tracking methods at a large margin on the challenging POT, UCSB and POIC datasets. Codes and models are available at https://github.com/zhanxinrui/HDN.",
    "code_link": "https://github.com/zhanxinrui/HDN"
  },
  "aaai2022_main_patchdiffusionageneralmoduleforfacemanipulationdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Patch Diffusion: A General Module for Face Manipulation Detection",
    "authors": [
      "Baogen Zhang",
      "Sheng Li",
      "Guorui Feng",
      "Zhenxing Qian",
      "Xinpeng Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20233",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20233/19992",
    "published": "2022-02",
    "summary": "Detection of manipulated face images has attracted a lot of interest recently. Various schemes have been proposed to tackle this challenging problem, where the patch-based approaches are shown to be promising. However, the existing patch-based approaches tend to treat different patches equally, which do not fully exploit the patch discrepancy for effective feature learning. In this paper, we propose a Patch Diffusion (PD) module which can be integrated into the existing face manipulation detection networks to boost the performance. The PD consists of Discrepancy Patch Feature Learning (DPFL) and Attention-Aware Message Passing (AMP). The DPFL effectively learns the patch features by a newly designed Pairwise Patch Loss (PPLoss), which takes both the patch importance and correlations into consideration. The AMP diffuses the patches through attention-aware message passing in a graph network, where the attentions are explicitly computed based on the patch features learnt in DPFL. We integrate our PD module into four recent face manipulation detection networks, and carry out the experiments on four popular datasets. The results demonstrate that our PD module is able to boost the performance of the existing networks for face manipulation detection.",
    "code_link": ""
  },
  "aaai2022_main_semi-supervisedobjectdetectionwithadaptiveclass-rebalancingself-training": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semi-supervised Object Detection with Adaptive Class-Rebalancing Self-Training",
    "authors": [
      "Fangyuan Zhang",
      "Tianxiang Pan",
      "Bin Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20234",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20234/19993",
    "published": "2022-02",
    "summary": "While self-training achieves state-of-the-art results in semi-supervised object detection (SSOD), it severely suffers from foreground-background and foreground-foreground imbalances in SSOD. In this paper, we propose an Adaptive Class-Rebalancing Self-Training (ACRST) with a novel memory module called CropBank to alleviate these imbalances and generate unbiased pseudo-labels. Besides, we observe that both self-training and data-rebalancing procedures suffer from noisy pseudo-labels in SSOD. Therefore, we contribute a simple yet effective two-stage pseudo-label filtering scheme to obtain accurate supervision. Our method achieves competitive performance on MS-COCO and VOC benchmarks. When using only 1% labeled data of MS-COCO, our method achieves 17.02 mAP improvement over the supervised method and 5.32 mAP gains compared with state-of-the-arts.",
    "code_link": ""
  },
  "aaai2022_main_showyourfaithcross-modalconfidence-awarenetworkforimage-textmatching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching",
    "authors": [
      "Huatian Zhang",
      "Zhendong Mao",
      "Kun Zhang",
      "Yongdong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20235",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20235/19994",
    "published": "2022-02",
    "summary": "Image-text matching bridges vision and language, which is a crucial task in the field of multi-modal intelligence. The key challenge lies in how to measure image-text relevance accurately as matching evidence. Most existing works aggregate the local semantic similarities of matched region-word pairs as the overall relevance, and they typically assume that the matched pairs are equally reliable. However, although a region-word pair is locally matched across modalities, it may be inconsistent/unreliable from the global perspective of image-text, resulting in inaccurate relevance measurement. In this paper, we propose a novel Cross-Modal Confidence-Aware Network to infer the matching confidence that indicates the reliability of matched region-word pairs, which is combined with the local semantic similarities to refine the relevance measurement. Specifically, we first calculate the matching confidence via the relevance between the semantic of image regions and the complete described semantic in the image, with the text as a bridge. Further, to richly express the region semantics, we extend the region to its visual context in the image. Then, local semantic similarities are weighted with the inferred confidence to filter out unreliable matched pairs in aggregating. Comprehensive experiments show that our method achieves state-of-the-art performance on benchmarks Flickr30K and MSCOCO.",
    "code_link": "https://github.com/CrossmodalGroup/CMCAN"
  },
  "aaai2022_main_scsnetanefficientparadigmforlearningsimultaneouslyimagecolorizationandsuper-resolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-resolution",
    "authors": [
      "Jiangning Zhang",
      "Chao Xu",
      "Jian Li",
      "Yue Han",
      "Yabiao Wang",
      "Ying Tai",
      "Yong\n      Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20236",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20236/19995",
    "published": "2022-02",
    "summary": "In the practical application of restoring low-resolution gray-scale images, we generally need to run three separate processes of image colorization, super-resolution, and dows-sampling operation for the target device. However, this pipeline is redundant and inefficient for the independent processes, and some inner features could have been shared. Therefore, we present an efficient paradigm to perform Simultaneously Image Colorization and Super-resolution (SCS) and propose an end-to-end SCSNet to achieve this goal. The proposed method consists of two parts: colorization branch for learning color information that employs the proposed plug-and-play Pyramid Valve Cross Attention (PVCAttn) module to aggregate feature maps between source and reference images; and super-resolution branch for integrating color and texture information to predict target images, which uses the designed Continuous Pixel Mapping (CPM) module to predict high-resolution images at continuous magnification. Furthermore, our SCSNet supports both automatic and referential modes that is more flexible for practical application. Abundant experiments demonstrate the superiority of our method for generating authentic images over state-of-the-art methods, e.g., averagely decreasing FID by 1.8 and 5.1 compared with current best scores for automatic and referential modes, respectively, while owning fewer parameters (more than x2) and faster running speed (more than x3).",
    "code_link": ""
  },
  "aaai2022_main_energy-basedgenerativecooperativesaliencyprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Energy-Based Generative Cooperative Saliency Prediction",
    "authors": [
      "Jing Zhang",
      "Jianwen Xie",
      "Zilong Zheng",
      "Nick Barnes"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20237",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20237/19996",
    "published": "2022-02",
    "summary": "Conventional saliency prediction models typically learn a deterministic mapping from an image to its saliency map, and thus fail to explain the subjective nature of human attention.In this paper, to model the uncertainty of visual saliency, we study the saliency prediction problem from the perspective of generative models by learning a conditional probability distribution over the saliency map given an input image, and treating the saliency prediction as a sampling process from the learned distribution. Specifically, we propose a generative cooperative saliency prediction framework, where a conditional latent variable model~(LVM) and a conditional energy-based model~(EBM) are jointly trained to predict salient objects in a cooperative manner. The LVM serves as a fast but coarse predictor to efficiently produce an initial saliency map, which is then refined by the iterative Langevin revision of the EBM that serves as a slow but fine predictor. Such a coarse-to-fine cooperative saliency prediction strategy offers the best of both worlds. Moreover, we propose a ``cooperative learning while recovering\" strategy and apply it to weakly supervised saliency prediction, where saliency annotations of training images are partially observed. Lastly, we find that the learned energy function in the EBM can serve as a refinement module that can refine the results of other pre-trained saliency prediction models. Experimental results show that our model can produce a set of diverse and plausible saliency maps of an image, and obtain state-of-the-art performance in both fully supervised and weakly supervised saliency prediction tasks.",
    "code_link": ""
  },
  "aaai2022_main_attention-basedtransformationfromlatentfeaturestopointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Attention-Based Transformation from Latent Features to Point Clouds",
    "authors": [
      "Kaiyi Zhang",
      "Ximing Yang",
      "Yuan Wu",
      "Cheng Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20238",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20238/19997",
    "published": "2022-02",
    "summary": "In point cloud generation and completion, previous methods for transforming latent features to point clouds are generally based on fully connected layers (FC-based) or folding operations (Folding-based). However, point clouds generated by FC-based methods are usually troubled by outliers and rough surfaces. For folding-based methods, their data flow is large, convergence speed is slow, and they are also hard to handle the generation of non-smooth surfaces. In this work, we propose AXform, an attention-based method to transform latent features to point clouds. AXform first generates points in an interim space, using a fully connected layer. These interim points are then aggregated to generate the target point cloud. AXform takes both parameter sharing and data flow into account, which makes it has fewer outliers, fewer network parameters, and a faster convergence speed. The points generated by AXform do not have the strong 2-manifold constraint, which improves the generation of non-smooth surfaces. When AXform is expanded to multiple branches for local generations, the centripetal constraint makes it has properties of self-clustering and space consistency, which further enables unsupervised semantic segmentation. We also adopt this scheme and design AXformNet for point cloud completion. Considerable experiments on different datasets show that our methods achieve state-of-the-art results.",
    "code_link": ""
  },
  "aaai2022_main_suppressingstaticvisualcuesvianormalizingflowsforself-supervisedvideorepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Suppressing Static Visual Cues via Normalizing Flows for Self-Supervised Video Representation Learning",
    "authors": [
      "Manlin Zhang",
      "Jinpeng Wang",
      "Andy J. Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20239",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20239/19998",
    "published": "2022-02",
    "summary": "Despite the great progress in video understanding made by deep convolutional neural networks, feature representation learned by existing methods may be biased to static visual cues. To address this issue, we propose a novel method to suppress static visual cues (SSVC) based on probabilistic analysis for self-supervised video representation learning. In our method, video frames are first encoded to obtain latent variables under standard normal distribution via normalizing flows. By modelling static factors in a video as a random variable, the conditional distribution of each latent variable becomes shifted and scaled normal. Then, the less-varying latent variables along time are selected as static cues and suppressed to generate motion-preserved videos. Finally, positive pairs are constructed by motion-preserved videos for contrastive learning to alleviate the problem of representation bias to static cues. The less-biased video representation can be better generalized to various downstream tasks. Extensive experiments on publicly available benchmarks demonstrate that the proposed method outperforms the state of the art when only single RGB modality is used for pre-training.",
    "code_link": ""
  },
  "aaai2022_main_lgdlabel-guidedself-distillationforobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LGD: Label-Guided Self-Distillation for Object Detection",
    "authors": [
      "Peizhen Zhang",
      "Zijian Kang",
      "Tong Yang",
      "Xiangyu Zhang",
      "Nanning Zheng",
      "Jian\n      Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20240",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20240/19999",
    "published": "2022-02",
    "summary": "In this paper, we propose the first self-distillation framework for general object detection, termed LGD (Label-Guided self-Distillation). Previous studies rely on a strong pretrained teacher to provide instructive knowledge that could be unavailable in real-world scenarios. Instead, we generate an instructive knowledge by inter-and-intra relation modeling among objects, requiring only student representations and regular labels. Concretely, our framework involves sparse label-appearance encoding, inter-object relation adaptation and intra-object knowledge mapping to obtain the instructive knowledge. They jointly form an implicit teacher at training phase, dynamically dependent on labels and evolving student representations. Modules in LGD are trained end-to-end with student detector and are discarded in inference. Experimentally, LGD obtains decent results on various detectors, datasets, and extensive tasks like instance segmentation. For example in MS-COCO dataset, LGD improves RetinaNet with ResNet-50 under 2x single-scale training from 36.2% to 39.0% mAP (+ 2.8%). It boosts much stronger detectors like FCOS with ResNeXt-101 DCN v2 under 2x multi-scale training from 46.1% to 47.9% (+ 1.8%). Compared with a classical teacher-based method FGFI, LGD not only performs better without requiring pretrained teacher but also reduces 51% training cost beyond inherent student learning.",
    "code_link": "https://github.com/megvii-research/LGD"
  },
  "aaai2022_main_uncertaintymodelingwithsecond-ordertransformerforgroupre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Uncertainty Modeling with Second-Order Transformer for Group Re-identification",
    "authors": [
      "Quan Zhang",
      "Jian-Huang Lai",
      "Zhanxiang Feng",
      "Xiaohua Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20241",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20241/20000",
    "published": "2022-02",
    "summary": "Group re-identification (G-ReID) focuses on associating the group images containing the same persons under different cameras. The key challenge of G-ReID is that all the cases of the intra-group member and layout variations are hard to exhaust. To this end, we propose a novel uncertainty modeling, which treats each image as a distribution depending on the current member and layout, then digs out potential group features by random samplings. Based on potential and original group features, uncertainty modeling can learn better decision boundaries, which is implemented by two modules, member variation module (MVM) and layout variation module (LVM). Furthermore, we propose a novel second-order transformer framework (SOT), which is inspired by the fact that the position modeling in the transformer is coped with the G-ReID task. SOT is composed of the intra-member module and inter-member module. Specifically, the intra-member module extracts the first-order token for each member, and then the inter-member module learns a second-order token as a group feature by the above first-order tokens, which can be regarded as the token of tokens. A large number of experiments have been conducted on three available datasets, including CSG, DukeGroup and RoadGroup. The experimental results show that the proposed SOT outperforms all previous state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_deepspatialadaptivenetworkforrealimagedemosaicing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Spatial Adaptive Network for Real Image Demosaicing",
    "authors": [
      "Tao Zhang",
      "Ying Fu",
      "Cheng Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20242",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20242/20001",
    "published": "2022-02",
    "summary": "Demosaicing is the crucial step in the image processing pipeline and is a highly ill-posed inverse problem. Recently, various deep learning based demosaicing methods have achieved promising performance, but they often design the same nonlinear mapping function for different spatial location and are not well consider the difference of mosaic pattern for each color. In this paper, we propose a deep spatial adaptive network (SANet) for real image demosaicing, which can adaptively learn the nonlinear mapping function for different locations. The weights of spatial adaptive convolution layer are generated by the pattern information in the receptive filed. Besides, we collect a paired real demosaicing dataset to train and evaluate the deep network, which can make the learned demosaicing network more practical in the real world. The experimental results show that our SANet outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality in both noiseless and noisy cases.",
    "code_link": ""
  },
  "aaai2022_main_magicmultimodalrelationalgraphadversarialinferencefordiverseandunpairedtext-basedimagecaptioning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning",
    "authors": [
      "Wenqiao Zhang",
      "Haochen Shi",
      "Jiannan Guo",
      "Shengyu Zhang",
      "Qingpeng Cai",
      "Juncheng Li",
      "Sihui Luo",
      "Yueting Zhuang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20243",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20243/20002",
    "published": "2022-02",
    "summary": "Text-based image captioning (TextCap) requires simultaneous comprehension of visual content and reading the text of images to generate a natural language description. Although a task can teach machines to understand the complex human environment further given that text is omnipresent in our daily surroundings, it poses additional challenges in normal captioning. A text-based image intuitively contains abundant and complex multimodal relational content, that is, image details can be described diversely from multiview rather than a single caption. Certainly, we can introduce additional paired training data to show the diversity of images' descriptions, this process is labor-intensive and time-consuming for TextCap pair annotations with extra texts. Based on the insight mentioned above, we investigate how to generate diverse captions that focus on different image parts using an unpaired training paradigm. We propose the Multimodal relAtional Graph adversarIal InferenCe (MAGIC) framework for diverse and unpaired TextCap. This framework can adaptively construct multiple multimodal relational graphs of images and model complex relationships among graphs to represent descriptive diversity. Moreover, a cascaded generative adversarial network is developed from modeled graphs to infer the unpaired caption generation in image\u2013sentence feature alignment and linguistic coherence levels. We validate the effectiveness of MAGIC in generating diverse captions from different relational information items of an image. Experimental results show that MAGIC can generate very promising outcomes without using any image\u2013caption training pairs.",
    "code_link": "https://github.com/tylin/coco-caption"
  },
  "aaai2022_main_classguidedchannelweightingnetworkforfine-grainedsemanticsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Class Guided Channel Weighting Network for Fine-Grained Semantic Segmentation",
    "authors": [
      "Xiang Zhang",
      "Wanqing Zhao",
      "Hangzai Luo",
      "Jinye Peng",
      "Jianping Fan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20244",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20244/20003",
    "published": "2022-02",
    "summary": "Deep learning has achieved promising performance on semantic segmentation, but few works focus on semantic segmentation at the fine-grained level. Fine-grained semantic segmentation requires recognizing and distinguishing hundreds of sub-categories. Due to the high similarity of different sub-categories and large variations in poses, scales, rotations, and color of the same sub-category in the fine-grained image set, the performance of traditional semantic segmentation methods will decline sharply. To alleviate these dilemmas, a new approach, named Class Guided Channel Weighting Network (CGCWNet), is developed in this paper to enable fine-grained semantic segmentation. For the large intra-class variations, we propose a Class Guided Weighting (CGW) module, which learns the image-level fine-grained category probabilities by exploiting second-order feature statistics, and use them as global information to guide semantic segmentation. For the high similarity between different sub-categories, we specially build a Channel Relationship Attention (CRA) module to amplify the distinction of features. Furthermore, a Detail Enhanced Guided Filter (DEGF) module is proposed to refine the boundaries of object masks by using an edge contour cue extracted from the enhanced original image. Experimental results on PASCAL VOC 2012 and six fine-grained image sets show that our proposed CGCWNet has achieved state-of-the-art results.",
    "code_link": ""
  },
  "aaai2022_main_context-basedcontrastivelearningforscenetextrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Context-Based Contrastive Learning for Scene Text Recognition",
    "authors": [
      "Xinyun Zhang",
      "Binwu Zhu",
      "Xufeng Yao",
      "Qi Sun",
      "Ruiyu Li",
      "Bei Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20245",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20245/20004",
    "published": "2022-02",
    "summary": "Pursuing accurate and robust recognizers has been a long-lasting goal for scene text recognition (STR) researchers. Recently, attention-based methods have demonstrated their effectiveness and achieved impressive results on public benchmarks. The attention mechanism enables models to recognize scene text with severe visual distortions by leveraging contextual information. However, recent studies revealed that the implicit over-reliance of context leads to catastrophic out-of-vocabulary performance. On the contrary to the superior accuracy of the seen text, models are prone to misrecognize unseen text even with good image quality. We propose a novel framework, Context-based contrastive learning (ConCLR), to alleviate this issue. Our proposed method first generates characters with different contexts via simple image concatenation operations and then optimizes contrastive loss on their embeddings. By pulling together clusters of identical characters within various contexts and pushing apart clusters of different characters in embedding space, ConCLR suppresses the side-effect of overfitting to specific contexts and learns a more robust representation. Experiments show that ConCLR significantly improves out-of-vocabulary generalization and achieves state-of-the-art performance on public benchmarks together with attention-based recognizers.",
    "code_link": ""
  },
  "aaai2022_main_learningnetworkarchitectureforopen-setrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Network Architecture for Open-Set Recognition",
    "authors": [
      "Xuelin Zhang",
      "Xuelian Cheng",
      "Donghao Zhang",
      "Paul Bonnington",
      "Zongyuan Ge"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20246",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20246/20005",
    "published": "2022-02",
    "summary": "Given the incomplete knowledge of classes that exist in the world, Open-set Recognition (OSR) enables networks to identify and reject the unseen classes after training. This problem of breaking the common closed-set assumption is far from being solved. Recent studies focus on designing new losses, neural network encoding structures, and calibration methods to optimize a feature space for OSR relevant tasks. In this work, we make the first attempt to tackle OSR by searching the architecture of a Neural Network (NN) under the open-set assumption. In contrast to the prior arts, we develop a mechanism to both search the architecture of the network and train a network suitable for tackling OSR. Inspired by the compact abating probability (CAP) model, which is theoretically proven to reduce the open space risk, we regularize the searching space by VAE contrastive learning. To discover a more robust structure for OSR, we propose Pseudo Auxiliary Searching (PAS), in which we split a pretended set of know-unknown classes from the original training set in the searching phase, hence enabling the super-net to explore an effective architecture that can handle unseen classes in advance. We demonstrate the benefits of this learning pipeline on 5 OSR datasets, including MNIST, SVHN, CIFAR10, CIFARAdd10, and CIFARAdd50, where our approach outperforms prior state-of-the-art networks designed by humans. To spark research in this field, our code is available at https://github.com/zxl101/NAS OSR.",
    "code_link": "https://github.com/zxl101/NAS"
  },
  "aaai2022_main_anadversarialframeworkforgeneratingunseenimagesbyactivationmaximization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Adversarial Framework for Generating Unseen Images by Activation Maximization",
    "authors": [
      "Yang Zhang",
      "Wang Zhou",
      "Gaoyuan Zhang",
      "David Cox",
      "Shiyu Chang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20247",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20247/20006",
    "published": "2022-02",
    "summary": "Activation maximization (AM) refers to the task of generating input examples that maximize the activation of a target class of a classifier, which can be used for class-conditional image generation and model interpretation. A popular class of AM method, GAN-based AM, introduces a GAN pre-trained on a large image set, and performs AM over its input random seed or style embeddings, so that the generated images are natural and adversarial attacks are prevented. Most of these methods would require the image set to contain some images of the target class to be visualized. Otherwise they tend to generate other seen class images that most maximizes the target class activation. In this paper, we aim to tackle the case where information about the target class is completely removed from the image set. This would ensure that the generated images truly reflect the target class information residing in the classifier, not the target class information in the image set, which contributes to a more faithful interpretation technique. To this end, we propose PROBEGAN, a GAN-based AM algorithm capable of generating image classes unseen in the image set. Rather than using a pre-trained GAN, PROBEGAN trains a new GAN with AM explicitly included in its training objective. PROBEGAN consists of a class-conditional generator, a seen-class discriminator, and an all-class unconditional discriminator. It can be shown that such a framework can generate images with the features of the unseen target class, while retaining the naturalness as depicted in the image set. Experiments have shown that PROBEGAN can generate unseen-class images with much higher quality than the baselines. We also explore using PROBEGAN as a model interpretation tool. Our code is at https://github.com/csmiler/ProbeGAN/.",
    "code_link": ""
  },
  "aaai2022_main_contrastivespatio-temporalpretextlearningforself-supervisedvideorepresentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contrastive Spatio-Temporal Pretext Learning for Self-Supervised Video Representation",
    "authors": [
      "Yujia Zhang",
      "Lai-Man Po",
      "Xuyuan Xu",
      "Mengyang Liu",
      "Yexin Wang",
      "Weifeng Ou",
      "Yuzhi Zhao",
      "Wing-Yin Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20248",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20248/20007",
    "published": "2022-02",
    "summary": "Spatio-temporal representation learning is critical for video self-supervised representation. Recent approaches mainly use contrastive learning and pretext tasks. However, these approaches learn representation by discriminating sampled instances via feature similarity in the latent space while ignoring the intermediate state of the learned representations, which limits the overall performance. In this work, taking into account the degree of similarity of sampled instances as the intermediate state, we propose a novel pretext task - spatio-temporal overlap rate (STOR) prediction. It stems from the observation that humans are capable of discriminating the overlap rates of videos in space and time. This task encourages the model to discriminate the STOR of two generated samples to learn the representations. Moreover, we employ a joint optimization combining pretext tasks with contrastive learning to further enhance the spatio-temporal representation learning. We also study the mutual influence of each component in the proposed scheme. Extensive experiments demonstrate that our proposed STOR task can favor both contrastive learning and pretext tasks and the joint optimization scheme can significantly improve the spatio-temporal representation in video understanding. The code is available at https://github.com/Katou2/CSTP.",
    "code_link": "https://github.com/Katou2/CSTP"
  },
  "aaai2022_main_pose-invariantfacerecognitionviaadaptiveangulardistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pose-Invariant Face Recognition via Adaptive Angular Distillation",
    "authors": [
      "Zhenduo Zhang",
      "Yongru Chen",
      "Wenming Yang",
      "Guijin Wang",
      "Qingmin Liao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20249",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20249/20008",
    "published": "2022-02",
    "summary": "Pose-invariant face recognition is a practically useful but challenging task. This paper introduces a novel method to learn pose-invariant feature representation without normalizing profile faces to frontal ones or learning disentangled features. We first design a novel strategy to learn pose-invariant feature embeddings by distilling the angular knowledge of frontal faces extracted by teacher network to student network, which enables the handling of faces with large pose variations. In this way, the features of faces across variant poses can cluster compactly for the same person to create a pose-invariant face representation. Secondly, we propose a Pose-Adaptive Angular Distillation loss to mitigate the negative effect of uneven distribution of face poses in the training dataset to pay more attention to the samples with large pose variations. Extensive experiments on two challenging benchmarks (IJB-A and CFP-FP) show that our approach consistently outperforms the existing methods.",
    "code_link": ""
  },
  "aaai2022_main_end-to-endlearningthepartialpermutationmatrixforrobust3dpointcloudregistration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "End-to-End Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration",
    "authors": [
      "Zhiyuan Zhang",
      "Jiadai Sun",
      "Yuchao Dai",
      "Dingfu Zhou",
      "Xibin Song",
      "Mingyi He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20250",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20250/20009",
    "published": "2022-02",
    "summary": "Even though considerable progress has been made in deep learning-based 3D point cloud processing, how to obtain accurate correspondences for robust registration remains a major challenge because existing hard assignment methods cannot deal with outliers naturally. Alternatively, the soft matching-based methods have been proposed to learn the matching probability rather than hard assignment. However, in this paper, we prove that these methods have an inherent ambiguity causing many deceptive correspondences. To address the above challenges, we propose to learn a partial permutation matching matrix, which does not assign corresponding points to outliers, and implements hard assignment to prevent ambiguity. However, this proposal poses two new problems, i.e. existing hard assignment algorithms can only solve a full rank permutation matrix rather than a partial permutation matrix, and this desired matrix is defined in the discrete space, which is non-differentiable. In response, we design a dedicated soft-to-hard (S2H) matching procedure within the registration pipeline consisting of two steps: solving the soft matching matrix (S-step) and projecting this soft matrix to the partial permutation matrix (H-step). Specifically, we augment the profit matrix before the hard assignment to solve an augmented permutation matrix, which is cropped to achieve the final partial permutation matrix. Moreover, to guarantee end-to-end learning, we supervise the learned partial permutation matrix but propagate the gradient to the soft matrix instead. Our S2H matching procedure can be easily integrated with existing registration frameworks, which has been verified in representative frameworks including DCP, RPMNet, and DGR. Extensive experiments have validated our method, which creates a new state-of-the-art performance.",
    "code_link": ""
  },
  "aaai2022_main_petsganrethinkingpriorsforsingleimagegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PetsGAN: Rethinking Priors for Single Image Generation",
    "authors": [
      "Zicheng Zhang",
      "Yinglu Liu",
      "Congying Han",
      "Hailin Shi",
      "Tiande Guo",
      "Bowen\n      Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20251",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20251/20010",
    "published": "2022-02",
    "summary": "Single image generation (SIG), described as generating diverse samples that have the same visual content as the given natural image, is first introduced by SinGAN, which builds a pyramid of GANs to progressively learn the internal patch distribution of the single image. It shows excellent performance in a wide range of image manipulation tasks. However, SinGAN has some limitations. Firstly, due to lack of semantic information, SinGAN cannot handle the object images well as it does on the scene and texture images. Secondly, the independent progressive training scheme is time-consuming and easy to cause artifacts accumulation. To tackle these problems, in this paper, we dig into the single image generation problem and improve SinGAN by fully-utilization of internal and external priors. The main contributions of this paper include: 1) We interpret single image generation from the perspective of the general generative task, that is, to learn a diverse distribution from the Dirac distribution composed of a single image. In order to solve this non-trivial problem, we construct a regularized latent variable model to formulate SIG. To the best of our knowledge, it is the first time to give a clear formulation and optimization goal of SIG, and all the existing methods for SIG can be regarded as special cases of this model. 2) We design a novel Prior-based end-to-end training GAN (PetsGAN), which is infused with internal prior and external prior to overcome the problems of SinGAN. For one thing, we employ the pre-trained GAN model to inject external prior for image generation, which can alleviate the problem of lack of semantic information and generate natural, reasonable and diverse samples, even for the object image. For another, we fully-utilize the internal prior by a differential Patch Matching module and an effective reconstruction network to generate consistent and realistic texture. 3) We construct abundant of qualitative and quantitative experiments on three datasets. The experimental results show our method surpasses other methods on both generated image quality, diversity, and training speed. Moreover, we apply our method to other image manipulation tasks (e.g., style transfer, harmonization) and the results further prove the effectiveness and efficiency of our method.",
    "code_link": ""
  },
  "aaai2022_main_nestedhierarchicaltransformertowardsaccurate,data-efficientandinterpretablevisualunderstanding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
    "authors": [
      "Zizhao Zhang",
      "Han Zhang",
      "Long Zhao",
      "Ting Chen",
      "Sercan \u00d6. Arik",
      "Tomas\n      Pfister"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20252",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20252/20011",
    "published": "2022-02",
    "summary": "Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold:(1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8 times faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.",
    "code_link": "https://github.com/jacobgil/vit-explain"
  },
  "aaai2022_main_oa-fsui2itanovelfew-shotcrossdomainobjectdetectionframeworkwithobject-awarefew-shotunsupervisedimage-to-imagetranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "OA-FSUI2IT: A Novel Few-Shot Cross Domain Object Detection Framework with Object-Aware Few-Shot Unsupervised Image-to-Image Translation",
    "authors": [
      "Lifan Zhao",
      "Yunlong Meng",
      "Lin Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20253",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20253/20012",
    "published": "2022-02",
    "summary": "Unsupervised image-to-image (UI2I) translation methods aim to learn a mapping between different visual domains with well-preserved content and consistent structure. It has been proven that the generated images are quite useful for enhancing the performance of computer vision tasks like object detection in a different domain with distribution discrepancies. Current methods require large amounts of images in both source and target domains for successful translation. However, data collection and annotations in many scenarios are infeasible or even impossible. In this paper, we propose an Object-Aware Few-Shot UI2I Translation (OA-FSUI2IT) framework to address the few-shot cross domain (FSCD) object detection task with limited unlabeled images in the target domain. To this end, we first introduce a discriminator augmentation (DA) module into the OA-FSUI2IT framework for successful few-shot UI2I translation. Then, we present a patch pyramid contrastive learning (PPCL) strategy to further improve the quality of the generated images. Last, we propose a self-supervised content-consistency (SSCC) loss to enforce the content-consistency in the translation. We implement extensive experiments to demonstrate the effectiveness of our OA-FSUI2IT framework for FSCD object detection and achieve state-of-the-art performance on the benchmarks of Normal-to-Foggy, Day-to-Night, and Cross-scene adaptation. The source code of our proposed method is also available at https://github.com/emdata-ailab/FSCD-Det.",
    "code_link": "https://github.com/emdata-ailab/FSCD-Det"
  },
  "aaai2022_main_static-dynamicco-teachingforclass-incremental3dobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Static-Dynamic Co-teaching for Class-Incremental 3D Object Detection",
    "authors": [
      "Na Zhao",
      "Gim Hee Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20254",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20254/20013",
    "published": "2022-02",
    "summary": "Deep learning-based approaches have shown remarkable performance in the 3D object detection task. However, they suffer from a catastrophic performance drop on the originally trained classes when incrementally learning new classes without revisiting the old data. This \"catastrophic forgetting\" phenomenon impedes the deployment of 3D object detection approaches in real-world scenarios, where continuous learning systems are needed. In this paper, we study the unexplored yet important class-incremental 3D object detection problem and present the first solution - SDCoT, a novel static-dynamic co-teaching method. Our SDCoT alleviates the catastrophic forgetting of old classes via a static teacher, which provides pseudo annotations for old classes in the new samples and regularizes the current model by extracting previous knowledge with a distillation loss. At the same time, SDCoT consistently learns the underlying knowledge from new data via a dynamic teacher. We conduct extensive experiments on two benchmark datasets and demonstrate the superior performance of our SDCoT over baseline approaches in several incremental learning scenarios. Our code is available at https://github.com/Na-Z/SDCoT.",
    "code_link": "https://github.com/Na-Z/SDCoT"
  },
  "aaai2022_main_localsurfacedescriptorforgeometryandfeaturepreservedmeshdenoising": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Local Surface Descriptor for Geometry and Feature Preserved Mesh Denoising",
    "authors": [
      "Wenbo Zhao",
      "Xianming Liu",
      "Junjun Jiang",
      "Debin Zhao",
      "Ge Li",
      "Xiangyang Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20255",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20255/20014",
    "published": "2022-02",
    "summary": "3D meshes are widely employed to represent geometry structure of 3D shapes. Due to limitation of scanning sensor precision and other issues, meshes are inevitably affected by noise, which hampers the subsequent applications. Convolultional neural networks (CNNs) achieve great success in image processing tasks, including 2D image denoising, and have been proven to own the capacity of modeling complex features at different scales, which is also particularly useful for mesh denoising. However, due to the nature of irregular structure, CNNs-based denosing strategies cannot be trivially applied for meshes. To circumvent this limitation, in the paper, we propose the local surface descriptor (LSD), which is able to transform the local deformable surface around a face into 2D grid representation and thus facilitates the deployment of CNNs to generate denoised face normals. To verify the superiority of LSD, we directly feed LSD into the classical Resnet without any complicated network design. The extensive experimental results show that, compared to the state-of-the-arts, our method achieves encouraging performance with respect to both objective and subjective evaluations.",
    "code_link": ""
  },
  "aaai2022_main_boostinggenerativezero-shotlearningbysynthesizingdiversefeatureswithattributeaugmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Boosting Generative Zero-Shot Learning by Synthesizing Diverse Features with Attribute Augmentation",
    "authors": [
      "Xiaojie Zhao",
      "Yuming Shen",
      "Shidong Wang",
      "Haofeng Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20256",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20256/20015",
    "published": "2022-02",
    "summary": "The recent advance in deep generative models outlines a promising perspective in the realm of Zero-Shot Learning (ZSL).Most generative ZSL methods use category semantic attributes plus a Gaussian noise to generate visual features. After generating unseen samples, this family of approaches effectively transforms the ZSL problem into a supervised classification scheme. However, the existing models use a single semantic attribute, which contains the complete attribute information of the category. The generated data also carry the complete attribute information, but in reality, visual samples usually have limited attributes. Therefore, the generated data from attribute could have incomplete semantics. Based on this fact, we propose a novel framework to boost ZSL by synthesizing diverse features. This method uses augmented semantic attributes to train the generative model, so as to simulate the real distribution of visual features. We evaluate the proposed model on four benchmark datasets, observing significant performance improvement against the state-of-the-art.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedpretrainingforrgb-dsalientobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Pretraining for RGB-D Salient Object Detection",
    "authors": [
      "Xiaoqi Zhao",
      "Youwei Pang",
      "Lihe Zhang",
      "Huchuan Lu",
      "Xiang Ruan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20257",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20257/20016",
    "published": "2022-02",
    "summary": "Existing CNNs-Based RGB-D salient object detection (SOD) networks are all required to be pretrained on the ImageNet to learn the hierarchy features which helpsprovide a good initialization. However, the collection and annotation of large-scale datasets are time-consuming and expensive. In this paper, we utilize self-supervised representation learning (SSL) to design two pretext tasks: the cross-modal auto-encoder and the depth-contour estimation. Our pretext tasks require only a few and unlabeled RGB-D datasets to perform pretraining, which makes the network capture rich semantic contexts and reduce the gap between two modalities, thereby providing an effective initialization for the downstream task. In addition, for the inherent problem of cross-modal fusion in RGB-D SOD, we propose a consistency-difference aggregation (CDA) module that splits a single feature fusion into multi-path fusion to achieve an adequate perception of consistent and differential information. The CDA module is general and suitable for cross-modal and cross-level feature fusion.Extensive experiments on six benchmark datasets show that our self-supervised pretrained model performs favorably against most state-of-the-art methods pretrained on ImageNet.The source code will be publicly available athttps://github.com/Xiaoqi-Zhao-DLUT/SSLSOD.",
    "code_link": "https://github.com/Xiaoqi-Zhao-DLUT/SSLSOD"
  },
  "aaai2022_main_adaptivelogitadjustmentlossforlong-tailedvisualrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Logit Adjustment Loss for Long-Tailed Visual Recognition",
    "authors": [
      "Yan Zhao",
      "Weicong Chen",
      "Xu Tan",
      "Kai Huang",
      "Jihong Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20258",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20258/20017",
    "published": "2022-02",
    "summary": "Data in the real world tends to exhibit a long-tailed label distribution, which poses great challenges for the training of neural networks in visual recognition. Existing methods tackle this problem mainly from the perspective of data quantity, i.e., the number of samples in each class. To be specific, they pay more attention to tail classes, like applying larger adjustments to the logit. However, in the training process, the quantity and difficulty of data are two intertwined and equally crucial problems. For some tail classes, the features of their instances are distinct and discriminative, which can also bring satisfactory accuracy; for some head classes, although with sufficient samples, the high semantic similarity with other classes and lack of discriminative features will bring bad accuracy. Based on these observations, we propose Adaptive Logit Adjustment Loss (ALA Loss) to apply an adaptive adjusting term to the logit. The adaptive adjusting term is composed of two complementary factors: 1) quantity factor, which pays more attention to tail classes, and 2) difficulty factor, which adaptively pays more attention to hard instances in the training process. The difficulty factor can alleviate the over-optimization on tail yet easy instances and under-optimization on head yet hard instances. The synergy of the two factors can not only advance the performance on tail classes even further, but also promote the accuracy on head classes. Unlike previous logit adjusting methods that only concerned about data quantity, ALA Loss tackles the long-tailed problem from a more comprehensive, fine-grained and adaptive perspective. Extensive experimental results show that our method achieves the state-of-the-art performance on challenging recognition benchmarks, including ImageNet-LT, iNaturalist 2018, and Places-LT.",
    "code_link": ""
  },
  "aaai2022_main_cadreacascadedeepreinforcementlearningframeworkforvision-basedautonomousurbandriving": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-Based Autonomous Urban Driving",
    "authors": [
      "Yinuo Zhao",
      "Kun Wu",
      "Zhiyuan Xu",
      "Zhengping Che",
      "Qi Lu",
      "Jian Tang",
      "Chi\n      Harold Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20259",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20259/20018",
    "published": "2022-02",
    "summary": "Vision-based autonomous urban driving in dense traffic is quite challenging due to the complicated urban environment and the dynamics of the driving behaviors. Widely-applied methods either heavily rely on hand-crafted rules or learn from limited human experience, which makes them hard to generalize to rare but critical scenarios. In this paper, we present a novel CAscade Deep REinforcement learning framework, CADRE, to achieve model-free vision-based autonomous urban driving. In CADRE, to derive representative latent features from raw observations, we first offline train a Co-attention Perception Module (CoPM) that leverages the co-attention mechanism to learn the inter-relationships between the visual and control information from a pre-collected driving dataset. Cascaded by the frozen CoPM, we then present an efficient distributed proximal policy optimization framework to online learn the driving policy under the guidance of particularly designed reward functions. We perform a comprehensive empirical study with the CARLA NoCrash benchmark as well as specific obstacle avoidance scenarios in autonomous urban driving tasks. The experimental results well justify the effectiveness of CADRE and its superiority over the state-of-the-art by a wide margin.",
    "code_link": ""
  },
  "aaai2022_main_learningfromthetangramtosolveminivisualtasks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning from the Tangram to Solve Mini Visual Tasks",
    "authors": [
      "Yizhou Zhao",
      "Liang Qiu",
      "Pan Lu",
      "Feng Shi",
      "Tian Han",
      "Song-Chun Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20260",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20260/20019",
    "published": "2022-02",
    "summary": "Current pre-training methods in computer vision focus on natural images in the daily-life context. However, abstract diagrams such as icons and symbols are common and important in the real world. We are inspired by Tangram, a game that requires replicating an abstract pattern from seven dissected shapes. By recording human experience in solving tangram puzzles, we present the Tangram dataset and show that a pre-trained neural model on the Tangram helps solve some mini visual tasks based on low-resolution vision. Extensive experiments demonstrate that our proposed method generates intelligent solutions for aesthetic tasks such as folding clothes and evaluating room layouts. The pre-trained feature extractor can facilitate the convergence of few-shot learning tasks on human handwriting and improve the accuracy in identifying icons by their contours. The Tangram dataset is available at https://github.com/yizhouzhao/Tangram.",
    "code_link": "https://github.com/yizhouzhao/Tangram"
  },
  "aaai2022_main_handlingslicepermutationsvariabilityintensorrecovery": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Handling Slice Permutations Variability in Tensor Recovery",
    "authors": [
      "Jingjing Zheng",
      "Xiaoqin Zhang",
      "Wenzhe Wang",
      "Xianta Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20261",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20261/20020",
    "published": "2022-02",
    "summary": "This work studies the influence of slice permutations on tensor recovery, which is derived from a reasonable assumption about algorithm, i.e. changing data order should not affect the effectiveness of the algorithm. However, as we will discussed in this paper, this assumption is not satisfied by tensor recovery under some cases. We call this interesting problem as Slice Permutations Variability (SPV) in tensor recovery. In this paper,we discuss SPV of several key tensor recovery problemstheoretically and experimentally. The obtained results show that there is a huge gap between results by tensor recovery using tensor with different slices sequences. To overcome SPVin tensor recovery, we develop a novel tensor recovery algorithmby Minimum Hamiltonian Circle for SPV (TRSPV)whichexploits a low dimensional subspace structures within data tensormore exactly. To the best of our knowledge, this is the first work to discussandeffectively solve the SPV problem in tensor recovery. The experimental results demonstrate the effectiveness of the proposed algorithm in eliminating SPV in tensor recovery.",
    "code_link": ""
  },
  "aaai2022_main_boostingcontrastivelearningwithrelationknowledgedistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Boosting Contrastive Learning with Relation Knowledge Distillation",
    "authors": [
      "Kai Zheng",
      "Yuanjiang Wang",
      "Ye Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20262",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20262/20021",
    "published": "2022-02",
    "summary": "While self-supervised representation learning (SSL) has proved to be effective in the large model, there is still a huge gap between the SSL and supervised method in the lightweight model when following the same solution. We delve into this problem and find that the lightweight model is prone to collapse in semantic space when simply performing instance-wise contrast. To address this issue, we propose a relation-wise contrastive paradigm with Relation Knowledge Distillation (ReKD). We introduce a heterogeneous teacher to explicitly mine the semantic information and transferring a novel relation knowledge to the student (lightweight model). The theoretical analysis supports our main concern about instance-wise contrast and verify the effectiveness of our relation-wise contrastive learning. Extensive experimental results also demonstrate that our method achieves significant improvements on multiple lightweight models. Particularly, the linear evaluation on AlexNet obviously improves the current state-of-art from 44.7% to 50.1% , which is the first work to get close to the supervised (50.5%). Code will be made available.",
    "code_link": ""
  },
  "aaai2022_main_weaklysupervisedvideomomentlocalizationwithcontrastivenegativesamplemining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining",
    "authors": [
      "Minghang Zheng",
      "Yanjie Huang",
      "Qingchao Chen",
      "Yang Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20263",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20263/20022",
    "published": "2022-02",
    "summary": "Video moment localization aims at localizing the video segments which are most related to the given free-form natural language query. The weakly supervised setting, where only video level description is available during training, is getting more and more attention due to its lower annotation cost. Prior weakly supervised methods mainly use sliding windows to generate temporal proposals, which are independent of video content and low quality, and train the model to distinguish matched video-query pairs and unmatched ones collected from different videos, while neglecting what the model needs is to distinguish the unaligned segments within the video. In this work, we propose a novel weakly supervised solution by introducing Contrastive Negative sample Mining (CNM). Specifically, we use a learnable Gaussian mask to generate positive samples, highlighting the video frames most related to the query, and consider other frames of the video and the whole video as easy and hard negative samples respectively. We then train our network with the Intra-Video Contrastive loss to make our positive and negative samples more discriminative. Our method has two advantages: (1) Our proposal generation process with a learnable Gaussian mask is more efficient and makes our positive sample higher quality. (2) The more difficult intra-video negative samples enable our model to distinguish highly confusing scenes. Experiments on two datasets show the effectiveness of our method. Code can be found at https://github.com/minghangz/cnm.",
    "code_link": "https://github.com/minghangz/cnm"
  },
  "aaai2022_main_dualdecouplingtrainingforsemi-supervisedobjectdetectionwithnoise-bypasshead": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dual Decoupling Training for Semi-supervised Object Detection with Noise-Bypass Head",
    "authors": [
      "Shida Zheng",
      "Chenshu Chen",
      "Xiaowei Cai",
      "Tingqun Ye",
      "Wenming Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20264",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20264/20023",
    "published": "2022-02",
    "summary": "Pseudo bounding boxes from the self-training paradigm are inevitably noisy for semi-supervised object detection. To cope with that, a dual decoupling training framework is proposed in the present study, i.e. clean and noisy data decoupling, and classification and localization task decoupling. In the first decoupling, two-level thresholds are used to categorize pseudo boxes into three groups, i.e. clean backgrounds, noisy foregrounds and clean foregrounds. With a specially designed noise-bypass head focusing on noisy data, backbone networks can extract coarse but diverse information; and meanwhile, an original head learns from clean samples for more precise predictions. In the second decoupling, we take advantage of the two-head structure for better evaluation of localization quality, thus the category label and location of a pseudo box can remain independent of each other during training. The approach of two-level thresholds is also applied to group pseudo boxes into three sections of different location accuracy. We outperform existing works by a large margin on VOC datasets, reaching 54.8 mAP(+1.8), and even up to 55.9 mAP(+1.5) by leveraging MS-COCO train2017 as extra unlabeled data. On MS-COCO benchmark, our method also achieves about 1.0 mAP improvements averaging across protocols compared with the prior state-of-the-art.",
    "code_link": ""
  },
  "aaai2022_main_scalosssideandcorneralignedlossforboundingboxregression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SCALoss: Side and Corner Aligned Loss for Bounding Box Regression",
    "authors": [
      "Tu Zheng",
      "Shuai Zhao",
      "Yang Liu",
      "Zili Liu",
      "Deng Cai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20265",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20265/20024",
    "published": "2022-02",
    "summary": "Bounding box regression is an important component in object detection. Recent work achieves promising performance by optimizing the Intersection over Union (IoU). However, IoU-based loss has the gradient vanish problem in the case of low overlapping bounding boxes, and the model could easily ignore these simple cases. In this paper, we propose Side Overlap (SO) loss by maximizing the side overlap of two bounding boxes, which puts more penalty for low overlapping bounding box cases. Besides, to speed up the convergence, the Corner Distance (CD) is added into the objective function. Combining the Side Overlap and Corner Distance, we get a new regression objective function, Side and Corner Align Loss (SCALoss). The SCALoss is well-correlated with IoU loss, which also benefits the evaluation metric but produces more penalty for low-overlapping cases. It can serve as a comprehensive similarity measure, leading to better localization performance and faster convergence speed. Experiments on COCO, PASCAL VOC, and LVIS benchmarks show that SCALoss can bring consistent improvement and outperform ln loss and IoU based loss with popular object detectors such as YOLOV3, SSD, Faster-RCNN. Code is available at: https://github.com/Turoad/SCALoss.",
    "code_link": ""
  },
  "aaai2022_main_sepfusionfindingoptimalfusionstructuresforvisualsoundseparation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SepFusion: Finding Optimal Fusion Structures for Visual Sound Separation",
    "authors": [
      "Dongzhan Zhou",
      "Xinchi Zhou",
      "Di Hu",
      "Hang Zhou",
      "Lei Bai",
      "Ziwei Liu",
      "Wanli\n      Ouyang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20266",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20266/20025",
    "published": "2022-02",
    "summary": "Multiple modalities can provide rich semantic information; and exploiting such information will normally lead to better performance compared with the single-modality counterpart.However, it is not easy to devise an effective cross-modal fusion structure due to the variations of feature dimensions and semantics, especially when the inputs even come from different sensors, as in the field of audio-visual learning. In this work, we propose SepFusion, a novel framework that can smoothly produce optimal fusion structures for visual-sound separation. The framework is composed of two components, namely the model generator and the evaluator. To construct the generator, we devise a lightweight architecture space that can adapt to different input modalities. In this way, we can easily obtain audio-visual fusion structures according to our demands. For the evaluator, we adopt the idea of neural architecture search to select superior networks effectively. This automatic process can significantly save human efforts while achieving competitive performances. Moreover, since our SepFusion provides a series of strong models, we can utilize the model family for broader applications, such as further promoting performance via model assembly, or providing suitable architectures for the separation of certain instrument classes. These potential applications further enhance the competitiveness of our approach.",
    "code_link": ""
  },
  "aaai2022_main_pan-sharpeningwithcustomizedtransformerandinvertibleneuralnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pan-Sharpening with Customized Transformer and Invertible Neural Network",
    "authors": [
      "Man Zhou",
      "Jie Huang",
      "Yanchi Fang",
      "Xueyang Fu",
      "Aiping Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20267",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20267/20026",
    "published": "2022-02",
    "summary": "In remote sensing imaging systems, pan-sharpening is an important technique to obtain high-resolution multispectral images from a high-resolution panchromatic image and its corresponding low-resolution multispectral image. Owing to the powerful learning capability of convolution neural network (CNN), CNN-based methods have dominated this field. However, due to the limitation of the convolution operator, long-range spatial features are often not accurately obtained, thus limiting the overall performance. To this end, we propose a novel and effective method by exploiting a customized transformer architecture and information-lossless invertible neural module for long-range dependencies modeling and effective feature fusion in this paper. Specifically, the customized transformer formulates the PAN and MS features as queries and keys to encourage joint feature learning across two modalities while the designed invertible neural module enables effective feature fusion to generate the expected pan-sharpened results. To the best of our knowledge, this is the first attempt to introduce transformer and invertible neural network into pan-sharpening field. Extensive experiments over different kinds of satellite datasets demonstrate that our method outperforms state-of-the-art algorithms both visually and quantitatively with fewer parameters and flops. Further, the ablation experiments also prove the effectiveness of the proposed customized long-range transformer and effective invertible neural feature fusion module for pan-sharpening.",
    "code_link": ""
  },
  "aaai2022_main_promotingsingle-modalopticalflownetworkfordiversecross-modalflowestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Promoting Single-Modal Optical Flow Network for Diverse Cross-Modal Flow Estimation",
    "authors": [
      "Shili Zhou",
      "Weimin Tan",
      "Bo Yan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20268",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20268/20027",
    "published": "2022-02",
    "summary": "In recent years, optical flow methods develop rapidly, achieving unprecedented high performance. Most of the methods only consider single-modal optical flow under the well-known brightness-constancy assumption. However, in many application systems, images of different modalities need to be aligned, which demands to estimate cross-modal flow between the cross-modal image pairs. A lot of cross-modal matching methods are designed for some specific cross-modal scenarios. We argue that the prior knowledge of the advanced optical flow models can be transferred to the cross-modal flow estimation, which may be a simple but unified solution for diverse cross-modal matching tasks. To verify our hypothesis, we design a self-supervised framework to promote the single-modal optical flow networks for diverse corss-modal flow estimation. Moreover, we add a Cross-Modal-Adapter block as a plugin tothe state-of-the-art optical flow model RAFT for better performance in cross-modal scenarios. Our proposed Modality Promotion Framework and Cross-Modal Adapter have multiple advantages compared to the existing methods. The experiments demonstrate that our method is effective on multiple datasets of different cross-modal scenarios.",
    "code_link": ""
  },
  "aaai2022_main_edge-awareguidancefusionnetworkforrgb\u2013thermalsceneparsing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Edge-Aware Guidance Fusion Network for RGB\u2013Thermal Scene Parsing",
    "authors": [
      "Wujie Zhou",
      "Shaohua Dong",
      "Caie Xu",
      "Yaguan Qian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20269",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20269/20028",
    "published": "2022-02",
    "summary": "RGB\u2013thermal scene parsing has recently attracted increasing research interest in the field of computer vision. However, most existing methods fail to perform good boundary extraction for prediction maps and cannot fully use high-level features. In addition, these methods simply fuse the features from RGB and thermal modalities but are unable to obtain comprehensive fused features. To address these problems, we propose an edge-aware guidance fusion network (EGFNet) for RGB\u2013thermal scene parsing. First, we introduce a prior edge map generated using the RGB and thermal images to capture detailed information in the prediction map and then embed the prior edge information in the feature maps. To effectively fuse the RGB and thermal information, we propose a multimodal fusion module that guarantees adequate cross-modal fusion. Considering the importance of high-level semantic information, we propose a global information module and a semantic information module to extract rich semantic information from the high-level features. For decoding, we use simple elementwise addition for cascaded feature fusion. Finally, to improve the parsing accuracy, we apply multitask deep supervision to the semantic and boundary maps. Extensive experiments were performed on benchmark datasets to demonstrate the effectiveness of the proposed EGFNet and its superior performance compared with state-of-the-art methods. The code and results can be found at https://github.com/ShaohuaDong2021/EGFNet.",
    "code_link": "https://github.com/ShaohuaDong2021/EGFNet"
  },
  "aaai2022_main_tigantext-basedinteractiveimagegenerationandmanipulation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TiGAN: Text-Based Interactive Image Generation and Manipulation",
    "authors": [
      "Yufan Zhou",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Chris Tensmeyer",
      "Tong Yu",
      "Changyou\n      Chen",
      "Jinhui Xu",
      "Tong Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20270",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20270/20029",
    "published": "2022-02",
    "summary": "Using natural-language feedback to guide image generation and manipulation can greatly lower the required efforts and skills. This topic has received increased attention in recent years through refinement of Generative Adversarial Networks (GANs); however, most existing works are limited to single-round interaction, which is not reflective of real world interactive image editing workflows. Furthermore, previous works dealing with multi-round scenarios are limited to predefined feedback sequences, which is also impractical. In this paper, we propose a novel framework for Text-based Interactive image generation and manipulation (TiGAN) that responds to users' natural-language feedback. TiGAN utilizes the powerful pre-trained CLIP model to understand users' natural-language feedback and exploits contrastive learning for a better text-to-image mapping. To maintain the image consistency during interactions, TiGAN generates intermediate feature vectors aligned with the feedback and selectively feeds these vectors to our proposed generative model. Empirical results on several datasets show that TiGAN improves both interaction efficiency and image quality while better avoids undesirable image manipulation during interactions.",
    "code_link": ""
  },
  "aaai2022_main_cross-domainempiricalriskminimizationforunbiasedlong-tailedclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Domain Empirical Risk Minimization for Unbiased Long-Tailed Classification",
    "authors": [
      "Beier Zhu",
      "Yulei Niu",
      "Xian-Sheng Hua",
      "Hanwang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20271",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20271/20030",
    "published": "2022-02",
    "summary": "We address the overlooked unbiasedness in existing long-tailed classification methods: we find that their overall improvement is mostly attributed to the biased preference of \"tail\" over \"head\", as the test distribution is assumed to be balanced; however, when the test is as imbalanced as the long-tailed training data---let the test respect Zipf's law of nature---the \"tail\" bias is no longer beneficial overall because it hurts the \"head\" majorities. In this paper, we propose Cross-Domain Empirical Risk Minimization (xERM) for training an unbiased test-agnostic model to achieve strong performances on both test distributions, which empirically demonstrates that xERM fundamentally improves the classification by learning better feature representation rather than the \"head vs. tail\" game. Based on causality, we further theoretically explain why xERM achieves unbiasedness: the bias caused by the domain selection is removed by adjusting the empirical risks on the imbalanced domain and the balanced but unseen domain.",
    "code_link": ""
  },
  "aaai2022_main_deeprecurrentneuralnetworkwithmulti-scalebi-directionalpropagationforvideodeblurring": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Recurrent Neural Network with Multi-Scale Bi-directional Propagation for Video Deblurring",
    "authors": [
      "Chao Zhu",
      "Hang Dong",
      "Jinshan Pan",
      "Boyang Liang",
      "Yuhao Huang",
      "Lean Fu",
      "Fei\n      Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20272",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20272/20031",
    "published": "2022-02",
    "summary": "The success of the state-of-the-art video deblurring methods stems mainly from implicit or explicit estimation of alignment among the adjacent frames for latent video restoration. However, due to the influence of the blur effect, estimating the alignment information from the blurry adjacent frames is not a trivial task. Inaccurate estimations will interfere the following frame restoration. Instead of estimating alignment information, we propose a simple and effective deep Recurrent Neural Network with Multi-scale Bi-directional Propagation (RNN-MBP) to effectively propagate and gather the information from unaligned neighboring frames for better video deblurring. Specifically, we build a Multi-scale Bi-directional Propagation (MBP) module with two U-Net RNN cells which can directly exploit the inter-frame information from unaligned neighboring hidden states by integrating them in different scales. Moreover, to better evaluate the proposed algorithm and existing state-of-the-art methods on real-world blurry scenes, we also create a Real-World Blurry Video Dataset (RBVD) by a well-designed Digital Video Acquisition System (DVAS) and use it as the training and evaluation dataset. Extensive experimental results demonstrate that the proposed RBVD dataset effectively improve the performance of existing algorithms on real-world blurry videos, and the proposed algorithm performs favorably against the state-of-the-art methods on three typical benchmarks. The code is available at https://github.com/XJTU-CVLAB-LOWLEVEL/RNN-MBP.",
    "code_link": "https://github.com/XJTUCVLAB-LOWLEVEL/RNN-MBP"
  },
  "aaai2022_main_icanfindyou!boundary-guidedseparatedattentionnetworkforcamouflagedobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "I Can Find You! Boundary-Guided Separated Attention Network for Camouflaged Object Detection",
    "authors": [
      "Hongwei Zhu",
      "Peng Li",
      "Haoran Xie",
      "Xuefeng Yan",
      "Dong Liang",
      "Dapeng Chen",
      "Mingqiang Wei",
      "Jing Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20273",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20273/20032",
    "published": "2022-02",
    "summary": "Can you find me? By simulating how humans to discover the so-called 'perfectly'-camouflaged object, we present a novel boundary-guided separated attention network (call BSA-Net). Beyond the existing camouflaged object detection (COD) wisdom, BSA-Net utilizes two-stream separated attention modules to highlight the separator (or say the camouflaged object's boundary) between an image's background and foreground: the reverse attention stream helps erase the camouflaged object's interior to focus on the background, while the normal attention stream recovers the interior and thus pay more attention to the foreground; and both streams are followed by a boundary guider module and combined to strengthen the understanding of boundary. The core design of such separated attention is motivated by the COD procedure of humans: find the subtle difference between the foreground and background to delineate the boundary of a camouflaged object, then the boundary can help further enhance the COD accuracy. We validate on three benchmark datasets that the proposed BSA-Net is very beneficial to detect camouflaged objects with the blurred boundaries and similar colors/patterns with their backgrounds. Extensive results exhibit very clear COD improvements on our BSA-Net over sixteen SOTAs.",
    "code_link": "https://github.com/WolfberryCoke/BSA-Net"
  },
  "aaai2022_main_mocanetmotionretargetingin-the-wildviacanonicalizationnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MoCaNet: Motion Retargeting In-the-Wild via Canonicalization Networks",
    "authors": [
      "Wentao Zhu",
      "Zhuoqian Yang",
      "Ziang Di",
      "Wayne Wu",
      "Yizhou Wang",
      "Chen Change\n      Loy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20274",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20274/20033",
    "published": "2022-02",
    "summary": "We present a novel framework that brings the 3D motion retargeting task from controlled environments to in-the-wild scenarios. In particular, our method is capable of retargeting body motion from a character in a 2D monocular video to a 3D character without using any motion capture system or 3D reconstruction procedure. It is designed to leverage massive online videos for unsupervised training, needless of 3D annotations or motion-body pairing information. The proposed method is built upon two novel canonicalization operations, structure canonicalization and view canonicalization. Trained with the canonicalization operations and the derived regularizations, our method learns to factorize a skeleton sequence into three independent semantic subspaces, i.e., motion, structure, and view angle. The disentangled representation enables motion retargeting from 2D to 3D with high precision. Our method achieves superior performance on motion transfer benchmarks with large body variations and challenging actions. Notably, the canonicalized skeleton sequence could serve as a disentangled and interpretable representation of human motion that benefits action analysis and motion retrieval.",
    "code_link": ""
  },
  "aaai2022_main_robustdepthcompletionwithuncertainty-drivenlossfunctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Depth Completion with Uncertainty-Driven Loss Functions",
    "authors": [
      "Yufan Zhu",
      "Weisheng Dong",
      "Leida Li",
      "Jinjian Wu",
      "Xin Li",
      "Guangming Shi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20275",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20275/20034",
    "published": "2022-02",
    "summary": "Recovering a dense depth image from sparse LiDAR scans is a challenging task. Despite the popularity of color-guided methods for sparse-to-dense depth completion, they treated pixels equally during optimization, ignoring the uneven distribution characteristics in the sparse depth map and the accumulated outliers in the synthesized ground truth. In this work, we introduce uncertainty-driven loss functions to improve the robustness of depth completion and handle the uncertainty in depth completion. Specifically, we propose an explicit uncertainty formulation for robust depth completion with Jeffrey's prior. A parametric uncertain-driven loss is introduced and translated to new loss functions that are robust to noisy or missing data. Meanwhile, we propose a multiscale joint prediction model that can simultaneously predict depth and uncertainty maps. The estimated uncertainty map is also used to perform adaptive prediction on the pixels with high uncertainty, leading to a residual map for refining the completion results. Our method has been tested on KITTI Depth Completion Benchmark and achieved the state-of-the-art robustness performance in terms of MAE, IMAE, and IRMSE metrics.",
    "code_link": ""
  },
  "aaai2022_main_efficientmodel-drivennetworkforshadowremoval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Model-Driven Network for Shadow Removal",
    "authors": [
      "Yurui Zhu",
      "Zeyu Xiao",
      "Yanchi Fang",
      "Xueyang Fu",
      "Zhiwei Xiong",
      "Zheng-Jun Zha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20276",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20276/20035",
    "published": "2022-02",
    "summary": "Deep Convolutional Neural Networks (CNNs) based methods have achieved significant breakthroughs in the task of single image shadow removal. However, the performance of these methods remains limited for several reasons. First, the existing shadow illumination model ignores the spatially variant property of the shadow images, hindering their further performance. Second, most deep CNNs based methods directly estimate the shadow free results from the input shadow images like a black box, thus losing the desired interpretability. To address these issues, we first propose a new shadow illumination model for the shadow removal task. This new shadow illumination model ensures the identity mapping among unshaded regions, and adaptively performs fine grained spatial mapping between shadow regions and their references. Then, based on the shadow illumination model, we reformulate the shadow removal task as a variational optimization problem. To effectively solve the variational problem, we design an iterative algorithm and unfold it into a deep network, naturally increasing the interpretability of the deep model. Experiments show that our method could achieve SOTA performance with less than half parameters, one-fifth of floating-point of operations (FLOPs), and over seventeen times faster than SOTA method (DHAN).",
    "code_link": ""
  },
  "aaai2022_main_learningdisentangledclassificationandlocalizationrepresentationsfortemporalactionlocalization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Disentangled Classification and Localization Representations for Temporal Action Localization",
    "authors": [
      "Zixin Zhu",
      "Le Wang",
      "Wei Tang",
      "Ziyi Liu",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20277",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20277/20036",
    "published": "2022-02",
    "summary": "A common approach to Temporal Action Localization (TAL) is to generate action proposals and then perform action classification and localization on them. For each proposal, existing methods universally use a shared proposal-level representation for both tasks. However, our analysis indicates that this shared representation focuses on the most discriminative frames for classification, e.g., ``take-offs\" rather than ``run-ups\" in distinguishing ``high jump\" and ``long jump\", while frames most relevant to localization, such as the start and end frames of an action, are largely ignored. In other words, such a shared representation can not simultaneously handle both classification and localization tasks well, and it makes precise TAL difficult. To address this challenge, this paper disentangles the shared representation into classification and localization representations. The disentangled classification representation focuses on the most discriminative frames, and the disentangled localization representation focuses on the action phase as well as the action start and end. Our model could be divided into two sub-networks, i.e., the disentanglement network and the context-based aggregation network. The disentanglement network is an autoencoder to learn orthogonal hidden variables of classification and localization. The context-based aggregation network aggregates the classification and localization representations by modeling local and global contexts. We evaluate our proposed method on two popular benchmarks for TAL, which outperforms all state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_acdnetadaptivelycombineddilatedconvolutionformonocularpanoramadepthestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation",
    "authors": [
      "Chuanqing Zhuang",
      "Zhengda Lu",
      "Yiqun Wang",
      "Jun Xiao",
      "Ying Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20278",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20278/20037",
    "published": "2022-02",
    "summary": "Depth estimation is a crucial step for 3D reconstruction with panorama images in recent years. Panorama images maintain the complete spatial information but introduce distortion with equirectangular projection. In this paper, we propose an ACDNet based on the adaptively combined dilated convolution to predict the dense depth map for a monocular panoramic image. Specifically, we combine the convolution kernels with different dilations to extend the receptive field in the equirectangular projection. Meanwhile, we introduce an adaptive channel-wise fusion module to summarize the feature maps and get diverse attention areas in the receptive field along the channels. Due to the utilization of channel-wise attention in constructing the adaptive channel-wise fusion module, the network can capture and leverage the cross-channel contextual information efficiently. Finally, we conduct depth estimation experiments on three datasets (both virtual and real-world) and the experimental results demonstrate that our proposed ACDNet substantially outperforms the current state-of-the-art (SOTA) methods. Our codes and model parameters are accessed in https://github.com/zcq15/ACDNet.",
    "code_link": ""
  },
  "aaai2022_main_makingadversarialexamplesmoretransferableandindistinguishable": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Making Adversarial Examples More Transferable and Indistinguishable",
    "authors": [
      "Junhua Zou",
      "Yexin Duan",
      "Boyu Li",
      "Wu Zhang",
      "Yu Pan",
      "Zhisong Pan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20279",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20279/20038",
    "published": "2022-02",
    "summary": "Fast gradient sign attack series are popular methods that are used to generate adversarial examples. However, most of the approaches based on fast gradient sign attack series cannot balance the indistinguishability and transferability due to the limitations of the basic sign structure. To address this problem, we propose a method, called Adam Iterative Fast Gradient Tanh Method (AI-FGTM), to generate indistinguishable adversarial examples with high transferability. Besides, smaller kernels and dynamic step size are also applied to generate adversarial examples for further increasing the attack success rates. Extensive experiments on an ImageNet-compatible dataset show that our method generates more indistinguishable adversarial examples and achieves higher attack success rates without extra running time and resource. Our best transfer-based attack NI-TI-DI-AITM can fool six classic defense models with an average success rate of 89.3% and three advanced defense models with an average success rate of 82.7%, which are higher than the state-of-the-art gradient-based attacks. Additionally, our method can also reduce nearly 20% mean perturbation. We expect that our method will serve as a new baseline for generating adversarial examples with better transferability and indistinguishability.",
    "code_link": ""
  },
  "aaai2022_main_undercoverbooleanmatrixfactorizationwithmaxsat": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Undercover Boolean Matrix Factorization with MaxSAT",
    "authors": [
      "Florent Avellaneda",
      "Roger Villemaire"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20280",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20280/20039",
    "published": "2022-02",
    "summary": "The k-undercover Boolean matrix factorization problem aims to approximate a m\u00d7n Boolean matrix X as the Boolean product of an m\u00d7k and a k\u00d7n matrices A\u25e6B such that X is a cover of A\u25e6B, i.e., no representation error is allowed on the 0\u2019s entries of the matrix X. To infer an optimal and \u201cblock-optimal\u201d k-undercover, we propose two exact methods based on MaxSAT encodings. From a theoretical standpoint, we prove that our method of inferring \u201cblock-optimal\u201d k-undercover is a (1 - 1/e) \u2248 0.632 approximation for the optimal k-undercover problem. From a practical standpoint, experimental results indicate that our \u201cblock-optimal\u201d k-undercover algorithm outperforms the state-of-the-art even when compared with algorithms for the more general k-undercover Boolean Matrix Factorization problem for which only minimizing reconstruction error is required.",
    "code_link": ""
  },
  "aaai2022_main_achievingzeroconstraintviolationforconstrainedreinforcementlearningviaprimal-dualapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach",
    "authors": [
      "Qinbo Bai",
      "Amrit Singh Bedi",
      "Mridul Agarwal",
      "Alec Koppel",
      "Vaneet Aggarwal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20281",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20281/20040",
    "published": "2022-02",
    "summary": "Reinforcement learning is widely used in applications where one needs to perform sequential decisions while interacting with the environment. The problem becomes more challenging when the decision requirement includes satisfying some safety constraints. The problem is mathematically formulated as constrained Markov decision process (CMDP). In the literature, various algorithms are available to solve CMDP problems in a model-free manner to achieve epsilon-optimal cumulative reward with epsilon feasible policies. An epsilon-feasible policy implies that it suffers from constraint violation. An important question here is whether we can achieve epsilon-optimal cumulative reward with zero constraint violations or not. To achieve that, we advocate the use of a randomized primal-dual approach to solve the CMDP problems and propose a conservative stochastic primal-dual algorithm (CSPDA) which is shown to exhibit O(1/epsilon^2) sample complexity to achieve epsilon-optimal cumulative reward with zero constraint violations. In the prior works, the best available sample complexity for the epsilon-optimal policy with zero constraint violation is O(1/epsilon^5). Hence, the proposed algorithm provides a significant improvement compared to the state of the art.",
    "code_link": ""
  },
  "aaai2022_main_geqcagenericqualitativeconstraintacquisition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GEQCA: Generic Qualitative Constraint Acquisition",
    "authors": [
      "Mohamed-Bachir Belaid",
      "Nassim Belmecheri",
      "Arnaud Gotlieb",
      "Nadjib Lazaar",
      "Helge Spieker"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20282",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20282/20041",
    "published": "2022-02",
    "summary": "Many planning, scheduling or multi-dimensional packing problems involve the design of subtle logical combinations of temporal or spatial constraints. On the one hand, the precise modelling of these constraints, which are formulated in various relation algebras, entails a number of possible logical combinations and requires expertise in constraint-based modelling.On the other hand, active constraint acquisition (CA) has been used successfully to support non-experienced users in learning conjunctive constraint networks through the generation of a sequence of queries. In this paper, we propose GEACQ, which stands for Generic Qualitative Constraint Acquisition, an active CA method that learns qualitative constraints via the concept of qualitative queries.GEACQ combines qualitative queries with time-bounded path consistency (PC) and background knowledge propagation to acquire the qualitative constraints of any scheduling or packing problem.We prove soundness, completeness and termination of GEACQ by exploiting the jointly exhaustive and pairwise disjoint property of qualitative calculus and we give an experimental evaluation that shows (i) the efficiency of our approach in learning temporal constraints and, (ii) the use of GEACQ on real scheduling instances.",
    "code_link": ""
  },
  "aaai2022_main_certifiedsymmetryanddominancebreakingforcombinatorialoptimisation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Certified Symmetry and Dominance Breaking for Combinatorial Optimisation",
    "authors": [
      "Bart Bogaerts",
      "Stephan Gocht",
      "Ciaran McCreesh",
      "Jakob Nordstr\u00f6m"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20283",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20283/20042",
    "published": "2022-02",
    "summary": "Symmetry and dominance breaking can be crucial for solving hard combinatorial search and optimisation problems, but the correctness of these techniques sometimes relies on subtle arguments. For this reason, it is desirable to produce efficient, machine-verifiable certificates that solutions have been computed correctly. Building on the cutting planes proof system, we develop a certification methodfor optimisation problems in which symmetry and dominance breaking are easily expressible. Our experimental evaluation demonstrates that we can efficiently verify fully general symmetry breaking in Boolean satisfiability (SAT) solving, thus providing, for the first time, a unified method to certify a range of advanced SAT techniques that also includes XOR and cardinality reasoning. In addition, we apply our method to maximum clique solving and constraint programming as a proof of concept that the approach applies to a wider range of combinatorial problems.",
    "code_link": ""
  },
  "aaai2022_main_theperilsoflearningbeforeoptimizing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Perils of Learning Before Optimizing",
    "authors": [
      "Chris Cameron",
      "Jason Hartford",
      "Taylor Lundy",
      "Kevin Leyton-Brown"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20284",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20284/20043",
    "published": "2022-02",
    "summary": "Formulating real-world optimization problems often begins with making predictions from historical data (e.g., an optimizer that aims to recommend fast routes relies upon travel-time predictions). Typically, learning the prediction model used to generate the optimization problem and solving that problem are performed in two separate stages. Recent work has showed how such prediction models can be learned end-to-end by differentiating through the optimization task. Such methods often yield empirical improvements, which are typically attributed to end-to-end making better error tradeoffs than the standard loss function used in a two-stage solution. We refine this explanation and more precisely characterize when end-to-end can improve performance. When prediction targets are stochastic, a two-stage solution must make an a priori choice about which statistics of the target distribution to model---we consider expectations over prediction targets---while an end-to-end solution can make this choice adaptively. We show that the performance gap between a two-stage and end-to-end approach is closely related to the \\emph{price of correlation} concept in stochastic optimization and show the implications of some existing POC results for the predict-then-optimize problem. We then consider a novel and particularly practical setting, where multiple prediction targets are combined to obtain each of the objective function\u2019s coefficients. We give explicit constructions where (1) two-stage performs unboundedly worse than end-to-end; and (2) two-stage is optimal. We use simulations to experimentally quantify performance gaps and identify a wide range of real-world applications from the literature whose objective functions rely on multiple prediction targets, suggesting that end-to-end learning could yield significant improvements.",
    "code_link": ""
  },
  "aaai2022_main_alyapunov-basedmethodologyforconstrainedoptimizationwithbanditfeedback": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback",
    "authors": [
      "Semih Cayci",
      "Yilin Zheng",
      "Atilla Eryilmaz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20285",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20285/20044",
    "published": "2022-02",
    "summary": "In a wide variety of applications including online advertising, contractual hiring, and wireless scheduling, the controller is constrained by a stringent budget constraint on the available resources, which are consumed in a random amount by each action, and a stochastic feasibility constraint that may impose important operational limitations on decision-making. In this work, we consider a general model to address such problems, where each action returns a random reward, cost, and penalty from an unknown joint distribution, and the decision-maker aims to maximize the total reward under a budget constraint B on the total cost and a stochastic constraint on the time-average penalty. We propose a novel low-complexity algorithm based on Lyapunov optimization methodology, named LyOn, and prove that for K arms it achieves square root of KBlog(B) regret and zero constraint-violation when B is sufficiently large. The low computational cost and sharp performance bounds of LyOn suggest that Lyapunov-based algorithm design methodology can be effective in solving constrained bandit optimization problems.",
    "code_link": ""
  },
  "aaai2022_main_resolvinginconsistenciesinsimpletemporalproblemsaparameterizedapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Resolving Inconsistencies in Simple Temporal Problems: A Parameterized Approach",
    "authors": [
      "Konrad K. Dabrowski",
      "Peter Jonsson",
      "Sebastian Ordyniak",
      "George Osipov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20286",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20286/20045",
    "published": "2022-02",
    "summary": "The simple temporal problem (STP) is one of the most influential reasoning formalisms for representing temporal information in AI. We study the problem of resolving inconsistency of data encoded in the STP. We prove that the problem of identifying a maximally large consistent subset of data is NP-hard. In practical instances, it is reasonable to assume that the amount of erroneous data is small. We therefore parameterize by the number of constraints that need to be removed to achieve consistency. Using tools from parameterized complexity we design fixed-parameter tractable algorithms for two large fragments of the STP. Our main algorithmic results employ reductions to the Directed Subset Feedback Arc Set problem anditerative compression combined with an efficient algorithm for the Edge Multicut problem. We complement our algorithmic results with hardness results that rule out fixed-parameter tractable algorithms for all remaining non-trivial fragments of the STP (under standard complexity-theoretic assumptions). Together, our results give a full classification of the classical and parameterized complexity of the problem.",
    "code_link": ""
  },
  "aaai2022_main_efficientriemannianmeta-optimizationbyimplicitdifferentiation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Riemannian Meta-Optimization by Implicit Differentiation",
    "authors": [
      "Xiaomeng Fan",
      "Yuwei Wu",
      "Zhi Gao",
      "Yunde Jia",
      "Mehrtash Harandi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20287",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20287/20046",
    "published": "2022-02",
    "summary": "To solve optimization problems with nonlinear constrains, the recently developed Riemannian meta-optimization methods show promise, which train neural networks as an optimizer to perform optimization on Riemannian manifolds.A key challenge is the heavy computational and memory burdens, because computing the meta-gradient with respect to the optimizer involves a series of time-consuming derivatives, and stores large computation graphs in memory.In this paper, we propose an efficient Riemannian meta-optimization method that decouples the complex computation scheme from the meta-gradient.We derive Riemannian implicit differentiation to compute the meta-gradient by establishing a link between Riemannian optimization and the implicit function theorem. As a result, the updating our optimizer is only related to the final two iterations, which in turn speeds up our method and reduces the memory footprint significantly. We theoretically study the computational load and memory footprint of our method for long optimization trajectories, and conduct an empirical study to demonstrate the benefits of the proposed method. Evaluations of three optimization problems on different Riemannian manifolds show that our method achieves state-of-the-art performance in terms of the convergence speed and the quality of optima.",
    "code_link": "https://github.com/XiaomengFanmcislab/I-RMM"
  },
  "aaai2022_main_fasteralgorithmsforweakbackdoors": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Faster Algorithms for Weak Backdoors",
    "authors": [
      "Serge Gaspers",
      "Andrew Kaploun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20288",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20288/20047",
    "published": "2022-02",
    "summary": "A weak backdoor, or simply a backdoor, for a Boolean SAT formula F into a class of SAT formulae C is a partial truth assignment T such that F[T] is in C and satisfiability is preserved. The problem of finding a backdoor from class C1 into class C2, or WB(C1,C2), can be stated as follows: Given a formula F in C1, and a natural number k, determine whether there exists a backdoor for F into C2 assigning at most k variables. The class 0-Val contains all Boolean formulae with at least one negative literal in each clause. We design a new algorithm for WB(3CNF, 0-Val) by reducing it to a local search variant of 3-SAT. We show that our algorithm runs in time O*(2.562^k), improving on the previous state-of-the-art of O*(2.85^k). Here, the O* notation is a variant of the big-O notation that allows to omit polynomial factors in the input size. Next, we look at WB(3CNF, Null), where Null is the class consisting of the empty formula. This problem was known to have a trivial running time upper bound of O*(6^k) and can easily be solved in O*(3^k) time. We use a reduction to Conflict-Free-d-Hitting-Set to prove an upper bound of O*(2.2738^k), and also prove a lower bound of 2^o(k) assuming the Exponential Time Hypothesis. Finally, Horn is the class of formulae with at most one positive literal per clause. We improve the previous O*(4.54^k) running time for WB(3CNF, Horn) problem to O*(4.17^k), by exploiting the structure of the SAT instance to give a novel proof of the non-existence of the slowest cases after a slight restructuring of the branching priorities.",
    "code_link": ""
  },
  "aaai2022_main_adivideandconqueralgorithmforpredict+optimizewithnon-convexproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Divide and Conquer Algorithm for Predict+Optimize with Non-convex Problems",
    "authors": [
      "Ali Ugur Guler",
      "Emir Demirovi\u0107",
      "Jeffrey Chan",
      "James Bailey",
      "Christopher\n      Leckie",
      "Peter J. Stuckey"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20289",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20289/20048",
    "published": "2022-02",
    "summary": "The predict+optimize problem combines machine learning andcombinatorial optimization by predicting the problem coefficients first and then using these coefficients to solve the optimization problem. While this problem can be solved in two separate stages, recent research shows end to end models can achieve better results. This requires differentiating through a discrete combinatorial function.Models that use differentiable surrogates are prone to approximation errors, while existing exact models are limited to dynamic programming, or they do not generalize well with scarce data. In this work we propose a noveldivide and conquer algorithm based on transition points to reason over exact optimization problems and predictthe coefficients using the optimization loss. Moreover, our model is not limited to dynamic programming problems.We also introduce a greedy version, which achieves similarresults with less computation. In comparison with other predict+optimize frameworks, we show our method outperforms existing exact frameworks and can reason over hard combinatorial problems better than surrogate methods.",
    "code_link": "https://github.com/Patyrn/Divide-and-Learn"
  },
  "aaai2022_main_computingdiverseshortestpathsefficientlyatheoreticalandexperimentalstudy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Computing Diverse Shortest Paths Efficiently: A Theoretical and Experimental Study",
    "authors": [
      "Tesshu Hanaka",
      "Yasuaki Kobayashi",
      "Kazuhiro Kurita",
      "See Woo Lee",
      "Yota\n      Otachi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20290",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20290/20049",
    "published": "2022-02",
    "summary": "Finding diverse solutions in combinatorial problems recently has received considerable attention (Baste et al. 2020; Fomin et al. 2020; Hanaka et al. 2021). In this paper we study the following type of problems: given an integer k, the problem asks for k solutions such that the sum of pairwise (weighted) Hamming distances between these solutions is maximized. Such solutions are called diverse solutions. We present a polynomial-time algorithm for finding diverse shortest st-paths in weighted directed graphs. Moreover, we study the diverse version of other classical combinatorial problems such as diverse weighted matroid bases, diverse weighted arborescences, and diverse bipartite matchings. We show that these problems can be solved in polynomial time as well. To evaluate the practical performance of our algorithm for finding diverse shortest st-paths, we conduct a computational experiment with synthetic and real-world instances. The experiment shows that our algorithm successfully computes diverse solutions within reasonable computational time.",
    "code_link": "https://github.com/Dotolation/diversegraph-algo"
  },
  "aaai2022_main_optimizingbinarydecisiondiagramswithmaxsatforclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimizing Binary Decision Diagrams with MaxSAT for Classification",
    "authors": [
      "Hao Hu",
      "Marie-Jos\u00e9 Huguet",
      "Mohamed Siala"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20291",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20291/20050",
    "published": "2022-02",
    "summary": "The growing interest in explainable artificial intelligence(XAI) for critical decision making motivates the need for interpretable machine learning (ML) models. In fact, due to their structure (especially with small sizes), these models are inherently understandable by humans. Recently, several exact methods for computing such models are proposed to overcome weaknesses of traditional heuristic methods by providing more compact models or better prediction quality. Despite their compressed representation of Boolean functions, Binary decision diagrams (BDDs) did not gain enough interest as other interpretable ML models. In this paper, we first propose SAT-based models for learning optimal BDDs (in terms of the number of features) that classify all input examples. Then, we lift the encoding to a MaxSAT model to learn optimal BDDs in limited depths, that maximize the number of examples correctly classified. Finally, we tackle the fragmentation problem by introducing a method to merge compatible subtrees for the BDDs found via the MaxSAT model. Our empirical study shows clear benefits of the proposed approach in terms of prediction quality and interpretability (i.e., lighter size) compared to the state-of-the-art approaches.",
    "code_link": ""
  },
  "aaai2022_main_usingmaxsatforefficientexplanationsoftreeensembles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Using MaxSAT for Efficient Explanations of Tree Ensembles",
    "authors": [
      "Alexey Ignatiev",
      "Yacine Izza",
      "Peter J. Stuckey",
      "Joao Marques-Silva"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20292",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20292/20051",
    "published": "2022-02",
    "summary": "Tree ensembles (TEs) denote a prevalent machine learning model that do not offer guarantees of interpretability, that represent a challenge from the perspective of explainable artificial intelligence. Besides model agnostic approaches, recent work proposed to explain TEs with formally-defined explanations, which are computed with oracles for propositional satisfiability (SAT) and satisfiability modulo theories. The computation of explanations for TEs involves linear constraints to express the prediction. In practice, this deteriorates scalability of the underlying reasoners. Motivated by the inherent propositional nature of TEs, this paper proposes to circumvent the need for linear constraints and instead employ an optimization engine for pure propositional logic to efficiently handle the prediction. Concretely, the paper proposes to use a MaxSAT solver and exploit the objective function to determine a winning class. This is achieved by devising a propositional encoding for computing explanations of TEs. Furthermore, the paper proposes additional heuristics to improve the underlying MaxSAT solving procedure. Experimental results obtained on a wide range of publicly available datasets demonstrate that the proposed MaxSAT-based approach is either on par or outperforms the existing reasoning-based explainers, thus representing a robust and efficient alternative for computing formal explanations for TEs.",
    "code_link": ""
  },
  "aaai2022_main_findingbackdoorstointegerprogramsamontecarlotreesearchframework": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework",
    "authors": [
      "Elias B. Khalil",
      "Pashootan Vaezipoor",
      "Bistra Dilkina"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20293",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20293/20052",
    "published": "2022-02",
    "summary": "In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a ``small\" subset of an instance's integer variables with the following property: in a branch-and-bound procedure, the instance can be solved to global optimality by branching only on the variables in the backdoor. Constructing datasets of pre-computed backdoors for widely used MIP benchmark sets or particular problem families can enable new questions around novel structural properties of a MIP, or explain why a problem that is hard in theory can be solved efficiently in practice. Existing algorithms for finding backdoors rely on sampling candidate variable subsets in various ways, an approach which has demonstrated the existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010. However, these algorithms fall short of consistently succeeding at the task due to an imbalance between exploration and exploitation. We propose BaMCTS, a Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive algorithmic engineering, hybridization with traditional MIP concepts, and close integration with the CPLEX solver have enabled our method to outperform baselines on MIPLIB2017 instances, finding backdoors more frequently and more efficiently.",
    "code_link": "https://github.com/lyeskhalil/backdoorsearch"
  },
  "aaai2022_main_learningtosearchinlocalbranching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Search in Local Branching",
    "authors": [
      "Defeng Liu",
      "Matteo Fischetti",
      "Andrea Lodi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20294",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20294/20053",
    "published": "2022-02",
    "summary": "Finding high-quality solutions to mixed-integer linear programming problems (MILPs) is of great importance for many practical applications. In this respect, the refinement heuristic local branching (LB) has been proposed to produce improving solutions and has been highly influential for the development of local search methods in MILP. The algorithm iteratively explores a sequence of solution neighborhoods defined by the so-called local branching constraint, namely, a linear inequality limiting the distance from a reference solution. For a LB algorithm, the choice of the neighborhood size is critical to performance. Although it was initialized by a conservative value in the original LB scheme, our new observation is that the \"best\" size is strongly dependent on the particular MILP instance. In this work, we investigate the relation between the size of the search neighborhood and the behavior of the underlying LB algorithm, and we devise a leaning-based framework for guiding the neighborhood search of the LB heuristic. The framework consists of a two-phase strategy. For the first phase, a scaled regression model is trained to predict the size of the LB neighborhood at the first iteration through a regression task. In the second phase, we leverage reinforcement learning and devise a reinforced neighborhood search strategy to dynamically adapt the size at the subsequent iterations. We computationally show that the neighborhood size can indeed be learned, leading to improved performances and that the overall algorithm generalizes well both with respect to the instance size and, remarkably, across instances.",
    "code_link": "https://github.com/pandat8/ML4LB"
  },
  "aaai2022_main_analysisofpureliteraleliminationrulefornon-uniformrandom(max)k-satproblemwithanarbitrarydegreedistribution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Analysis of Pure Literal Elimination Rule for Non-uniform Random (MAX) k-SAT Problem with an Arbitrary Degree Distribution",
    "authors": [
      "Oleksii Omelchenko",
      "Andrei A. Bulatov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20295",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20295/20054",
    "published": "2022-02",
    "summary": "MAX k-SAT is one of the archetypal NP-hard problems. Its variation called random MAX k-SAT problem was introduced in order to understand how hard it is to solve instances of the problem on average. The most common model to sample random instances is the uniform model, which has received a large amount of attention. However, the uniform model often fails to capture important structural properties we observe in the real-world instances. \t To address these limitations, a more general (in a certain sense) model has been proposed, the configuration model, which is able to produce instances with an arbitrary distribution ofvariables' degrees, and so can simulate biases in instances appearing in various applications. Our overall goal is to expand the theory built around the uniform model to the more general configuration model for a wide range of degree distributions. This includes locating satisfiability thresholds and analysing the performance of the standard heuristics applied to instances sampled from the configuration model. \t In this paper we analyse the performance of the pure literal elimination rule. We provide an equation that given an underlying degree distribution gives the number of clauses the pure literal elimination rule satisfies w.h.p. We also show how the distribution of variable degrees changes over time as the algorithm is being executed.",
    "code_link": ""
  },
  "aaai2022_main_thesoftcumulativeconstraintwithquadraticpenalty": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The SoftCumulative Constraint with Quadratic Penalty",
    "authors": [
      "Yanick Ouellet",
      "Claude-Guy Quimper"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20296",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20296/20055",
    "published": "2022-02",
    "summary": "The Cumulative constraint greatly contributes to the success of constraint programming at solving scheduling problems. The SoftCumulative, a version of the Cumulative where overloading the resource incurs a penalty is, however, less studied. We introduce a checker and a filtering algorithm for the SoftCumulative, which are inspired by the powerful energetic reasoning rule for the Cumulative. Both algorithms can be used with classic linear penalty function, but also with a quadratic penalty function, where the penalty of overloading the resource increases quadratically with the amount of the overload. We show that these algorithms are more general than existing algorithms and vastly outperform a decomposition of the SoftCumulative in practice.",
    "code_link": ""
  },
  "aaai2022_main_efficientvertex-orientedpolytopicprojectionforweb-scaleapplications": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Vertex-Oriented Polytopic Projection for Web-Scale Applications",
    "authors": [
      "Rohan Ramanath",
      "S. Sathiya Keerthi",
      "Yao Pan",
      "Konstantin Salomatin",
      "Kinjal\n      Basu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20297",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20297/20056",
    "published": "2022-02",
    "summary": "We consider applications involving a large set of instances of projecting points to polytopes. We develop an intuition guided by theoretical and empirical analysis to show that when these instances follow certain structures, a large majority of the projections lie on vertices of the polytopes. To do these projections efficiently we derive a vertex-oriented incremental algorithm to project a point onto any arbitrary polytope, as well as give specific algorithms to cater to simplex projection and polytopes where the unit box is cut by planes. Such settings are especially useful in web-scale applications such as optimal matching or allocation problems. Several such problems in internet marketplaces (e-commerce, ride-sharing, food delivery, professional services, advertising, etc.), can be formulated as Linear Programs (LP) with such polytope constraints that require a projection step in the overall optimization process. We show that in some of the very recent works, the polytopic projection is the most expensive step and our efficient projection algorithms help in gaining massive improvements in performance.",
    "code_link": ""
  },
  "aaai2022_main_avariantofconcurrentconstraintprogrammingongpu": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Variant of Concurrent Constraint Programming on GPU",
    "authors": [
      "Pierre Talbot",
      "Fr\u00e9d\u00e9ric G Pinel",
      "Pascal Bouvry"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20298",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20298/20057",
    "published": "2022-02",
    "summary": "The number of cores on graphical computing units (GPUs) is reaching thousands nowadays, whereas the clock speed of processors stagnates. Unfortunately, constraint programming solvers do not take advantage yet of GPU parallelism. One reason is that constraint solvers were primarily designed within the mental frame of sequential computation. To solve this issue, we take a step back and contribute to a simple, intrinsically parallel, lock-free and formally correct programming language based on concurrent constraint programming. We then re-examine parallel constraint solving on GPUs within this formalism, and develop Turbo, a simple constraint solver entirely programmed on GPUs. Turbo validates the correctness of our approach and compares positively to a parallel CPU-based solver.",
    "code_link": ""
  },
  "aaai2022_main_real-timedriver-requestassignmentinridesourcing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Real-Time Driver-Request Assignment in Ridesourcing",
    "authors": [
      "Hao Wang",
      "Xiaohui Bei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20299",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20299/20058",
    "published": "2022-02",
    "summary": "Online on-demand ridesourcing service has played a huge role in transforming urban transportation. A central function in most on-demand ridesourcing platforms is to dynamically assign drivers to rider requests that could balance the request waiting times and the driver pick-up distances. To deal with the online nature of this problem, existing literature either divides the time horizon into short windows and applies a static offline assignment algorithm within each window or assumes a fully online setting that makes decisions for each request immediately upon its arrival. In this paper, we propose a more realistic model for the driver-request assignment that bridges the above two settings together. Our model allows the requests to wait after their arrival but assumes that they may leave at any time following a quitting function. Under this model, we design an efficient algorithm for assigning available drivers to requests in real-time. Our algorithm is able to incorporate future estimated driver arrivals into consideration and make strategic waiting and matching decisions that could balance the waiting time and pick-up distance of the assignment. We prove that our algorithm is optimal ex-ante in the single-request setting, and demonstrate its effectiveness in the general multi-request setting through experiments on both synthetic and real-world datasets.",
    "code_link": ""
  },
  "aaai2022_main_encodingmulti-valueddecisiondiagramconstraintsasbinaryconstrainttrees": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Encoding Multi-Valued Decision Diagram Constraints as Binary Constraint Trees",
    "authors": [
      "Ruiwei Wang",
      "Roland H.C. Yap"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20300",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20300/20059",
    "published": "2022-02",
    "summary": "Ordered Multi-valued Decision Diagram (MDD) is a compact representation used to model various constraints, such as regular constraints and table constraints. It can be particularly useful for representing ad-hoc problem specific constraints. Many algorithms have been proposed to enforce Generalized Arc Consistency (GAC) on MDD constraints. In this paper, we introduce a new compact representation called Binary Constraint Tree (BCT). We propose tree binary encodings to transform any MDD constraint into a BCT constraint. We also present a specialized algorithm enforcing GAC on the BCT constraint resulting from a MDD constraint. Experimental results on a large set of benchmarks show that the BCT GAC algorithm can significantly outperform state-of-the-art MDD as well as table GAC algorithms.",
    "code_link": "https://github.com/zayenz/minizinc-pentominoes-generator"
  },
  "aaai2022_main_sampleaverageapproximationforstochasticoptimizationwithdependentdataperformanceguaranteesandtractability": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sample Average Approximation for Stochastic Optimization with Dependent Data: Performance Guarantees and Tractability",
    "authors": [
      "Yafei Wang",
      "Bo Pan",
      "Wei Tu",
      "Peng Liu",
      "Bei Jiang",
      "Chao Gao",
      "Wei Lu",
      "Shangling Jui",
      "Linglong Kong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20301",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20301/20060",
    "published": "2022-02",
    "summary": "Sample average approximation (SAA), a popular method for tractably solving stochastic optimization problems, enjoys strong asymptotic performance guarantees in settings with independent training samples. However, these guarantees are not known to hold generally with dependent samples, such as in online learning with time series data or distributed computing with Markovian training samples. In this paper, we show that SAA remains tractable when the distribution of unknown parameters is only observable through dependent instances and still enjoys asymptotic consistency and finite sample guarantees. Specifically, we provide a rigorous probability error analysis to derive 1 - beta confidence bounds for the out-of-sample performance of SAA estimators and show that these estimators are asymptotically consistent. We then, using monotone operator theory, study the performance of a class of stochastic first-order algorithms trained on a dependent source of data. We show that approximation error for these algorithms is bounded and concentrates around zero, and establish deviation bounds for iterates when the underlying stochastic process is phi-mixing. The algorithms presented can be used to handle numerically inconvenient loss functions such as the sum of a smooth and non-smooth function or of non-smooth functions with constraints. To illustrate the usefulness of our results, we present several stochastic versions of popular algorithms such as stochastic proximal gradient descent (S-PGD), stochastic relaxed Peaceman-Rachford splitting algorithms (S-rPRS), and numerical experiment.",
    "code_link": ""
  },
  "aaai2022_main_aprovably-efficientmodel-freealgorithmforinfinite-horizonaverage-rewardconstrainedmarkovdecisionprocesses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Provably-Efficient Model-Free Algorithm for Infinite-Horizon Average-Reward Constrained Markov Decision Processes",
    "authors": [
      "Honghao Wei",
      "Xin Liu",
      "Lei Ying"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20302",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20302/20061",
    "published": "2022-02",
    "summary": "This paper presents a model-free reinforcement learning (RL) algorithm for infinite-horizon average-reward Constrained Markov Decision Processes (CMDPs). Considering a learning horizon K, which is sufficiently large, the proposed algorithm achieves sublinear regret and zero constraint violation. The bounds depend on the number of states S, the number of actions A, and two constants which are independent of the learning horizon K.",
    "code_link": ""
  },
  "aaai2022_main_texthoaxerbudgetedhard-labeladversarialattacksontext": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TextHoaxer: Budgeted Hard-Label Adversarial Attacks on Text",
    "authors": [
      "Muchao Ye",
      "Chenglin Miao",
      "Ting Wang",
      "Fenglong Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20303",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20303/20062",
    "published": "2022-02",
    "summary": "This paper focuses on a newly challenging setting in hard-label adversarial attacks on text data by taking the budget information into account. Although existing approaches can successfully generate adversarial examples in the hard-label setting, they follow an ideal assumption that the victim model does not restrict the number of queries. However, in real-world applications the query budget is usually tight or limited. Moreover, existing hard-label adversarial attack techniques use the genetic algorithm to optimize discrete text data by maintaining a number of adversarial candidates during optimization, which can lead to the problem of generating low-quality adversarial examples in the tight-budget setting. To solve this problem, in this paper, we propose a new method named TextHoaxer by formulating the budgeted hard-label adversarial attack task on text data as a gradient-based optimization problem of perturbation matrix in the continuous word embedding space. Compared with the genetic algorithm-based optimization, our solution only uses a single initialized adversarial example as the adversarial candidate for optimization, which significantly reduces the number of queries. The optimization is guided by a new objective function consisting of three terms, i.e., semantic similarity term, pair-wise perturbation constraint, and sparsity constraint. Semantic similarity term and pair-wise perturbation constraint can ensure the high semantic similarity of adversarial examples from both comprehensive text-level and individual word-level, while the sparsity constraint explicitly restricts the number of perturbed words, which is also helpful for enhancing the quality of generated text. We conduct extensive experiments on eight text datasets against three representative natural language models, and experimental results show that TextHoaxer can generate high-quality adversarial examples with higher semantic similarity and lower perturbation rate under the tight-budget setting.",
    "code_link": "https://github.com/machinelearning4health/TextHoaxer"
  },
  "aaai2022_main_twocompactedmodelsforefficientmodel-baseddiagnosis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Two Compacted Models for Efficient Model-Based Diagnosis",
    "authors": [
      "Huisi Zhou",
      "Dantong Ouyang",
      "Xiangfu Zhao",
      "Liming Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20304",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20304/20063",
    "published": "2022-02",
    "summary": "Model-based diagnosis (MBD) with multiple observations is complicated and difficult to manage over. In this paper, we proposed two new diagnosis models, namely, the Compacted Model with Multiple Observations (CMMO) and theDominated-based Compacted Model with Multiple Observations (D-CMMO), to solve the problem in which a considerable amount of time is needed when multiple observations are given and more than one fault is injected. Three ideas are presented in this paper. First, we propose to encode MBD with each observation as a subsystem and share as many system variables as possible to compress the size of encoded clauses. Second, we utilize the notion of gate dominance in the CMMO approach to compute Top-Level Diagnosis with Compacted Model (CM-TLD) to reduce the solution space. Finally, we explore the performance of our model using three fault models. Experimental results on the ISCAS-85 benchmarks show that CMMO and D-CMMO perform better thanthe state-of-the-art algorithms.",
    "code_link": ""
  },
  "aaai2022_main_parameterizedapproximationalgorithmsfork-centerclusteringandvariants": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Parameterized Approximation Algorithms for K-center Clustering and Variants",
    "authors": [
      "Sayan Bandyapadhyay",
      "Zachary Friggstad",
      "Ramin Mousavi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20305",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20305/20064",
    "published": "2022-02",
    "summary": "k-center is one of the most popular clustering models. While it admits a simple 2-approximation in polynomial time in general metrics, the Euclidean version is NP-hard to approximate within a factor of 1.93, even in the plane, if one insists the dependence on k in the running time be polynomial. Without this restriction, a classic algorithm yields a 2^{O((klog k)/{epsilon})}dn-time (1+epsilon)-approximation for Euclidean k-center, where d is the dimension.In this work, we give a faster algorithm for small dimensions: roughly speaking an O^*(2^{O((1/epsilon)^{O(d)} k^{1-1/d} log k)})-time (1+epsilon)-approximation. In particular, the running time is roughly O^*(2^{O((1/epsilon)^{O(1)}sqrt{k}log k)}) in the plane. We complement our algorithmic result with a matching hardness lower bound. We also consider a well-studied generalization of k-center, called Non-uniform k-center (NUkC), where we allow different radii clusters. NUkC is NP-hard to approximate within any factor, even in the Euclidean case. We design a 2^{O(klog k)}n^2 time 3-approximation for NUkC, and a 2^{O((klog k)/epsilon)}dn time (1+\\epsilon)-approximation for Euclidean NUkC. The latter time bound matches the bound for k-center.",
    "code_link": ""
  },
  "aaai2022_main_howtofindagoodexplanationforclustering?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How to Find a Good Explanation for Clustering?",
    "authors": [
      "Sayan Bandyapadhyay",
      "Fedor Fomin",
      "Petr A Golovach",
      "William Lochet",
      "Nidhi\n      Purohit",
      "Kirill Simonov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20306",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20306/20065",
    "published": "2022-02",
    "summary": "k-means and k-median clustering are powerful unsupervised machine learning techniques. However, due to complicated dependences on all the features, it is challenging to interpret the resulting cluster assignments. Moshkovitz, Dasgupta, Rashtchian, andFrostproposed an elegant model of explainable k-means and k-median clusteringin ICML 2020. In this model, adecision tree with k leaves provides a straightforward characterization of the data set into clusters. We study two natural algorithmic questions about explainable clustering.(1) For a given clustering, how to find the ``best explanation'' by using a decision tree with k leaves?(2) For a given set of points, how to find a decision tree with k leaves minimizing the k-means/median objective of the resulting explainable clustering? To address the first question, we introduce a new model of explainable clustering. Our model, inspired by the notion of outliers in robust statistics, is the following. We are seeking a small number of points (outliers) whose removal makes the existing clustering well-explainable. For addressing the second question, we initiate the study of the model of Moshkovitz et al. from the perspective of multivariate complexity. Our rigorous algorithmic analysis sheds some light on the influence of parameters like the input size, dimension of the data, the number of outliers, the number of clusters, and the approximation ratio,on the computational complexity of explainable clustering.",
    "code_link": ""
  },
  "aaai2022_main_regularizinggraphneuralnetworksviaconsistency-diversitygraphaugmentations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Regularizing Graph Neural Networks via Consistency-Diversity Graph Augmentations",
    "authors": [
      "Deyu Bo",
      "Binbin Hu",
      "Xiao Wang",
      "Zhiqiang Zhang",
      "Chuan Shi",
      "Jun Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20307",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20307/20066",
    "published": "2022-02",
    "summary": "Despite the remarkable performance of graph neural networks (GNNs) in semi-supervised learning, it is criticized for not making full use of unlabeled data and suffering from over-fitting. Recently, graph data augmentation, used to improve both accuracy and generalization of GNNs, has received considerable attentions. However, one fundamental question is how to evaluate the quality of graph augmentations in principle? In this paper, we propose two metrics, Consistency and Diversity, from the aspects of augmentation correctness and generalization. Moreover, we discover that existing augmentations fall into a dilemma between these two metrics. Can we find a graph augmentation satisfying both consistency and diversity? A well-informed answer can help us understand the mechanism behind graph augmentation and improve the performance of GNNs. To tackle this challenge, we analyze two representative semi-supervised learning algorithms: label propagation (LP) and consistency regularization (CR). We find that LP utilizes the prior knowledge of graphs to improve consistency and CR adopts variable augmentations to promote diversity. Based on this discovery, we treat neighbors as augmentations to capture the prior knowledge embodying homophily assumption, which promises a high consistency of augmentations. To further promote diversity, we randomly replace the immediate neighbors of each node with its remote neighbors. After that, a neighbor-constrained regularization is proposed to enforce the predictions of the augmented neighbors to be consistent with each other. Extensive experiments on five real-world graphs validate the superiority of our method in improving the accuracy and generalization of GNNs.",
    "code_link": ""
  },
  "aaai2022_main_two-stageoctaveresidualnetworkforend-to-endimagecompression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Two-Stage Octave Residual Network for End-to-End Image Compression",
    "authors": [
      "Fangdong Chen",
      "Yumeng Xu",
      "Li Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20308",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20308/20067",
    "published": "2022-02",
    "summary": "Octave Convolution (OctConv) is a generic convolutional unit that has already achieved good performances in many computer vision tasks. Recent studies also have shown the potential of applying the OctConv in end-to-end image compression. However, considering the characteristic of image compression task, current works of OctConv may limit the performance of the image compression network due to the loss of spatial information caused by the sampling operations of inter-frequency communication. Besides, the correlation between multi-frequency latents produced by OctConv is not utilized in current architectures. In this paper, to address these problems, we propose a novel Two-stage Octave Residual (ToRes) block which strips the sampling operation from OctConv to strengthen the capability of preserving useful information. Moreover, to capture the redundancy between the multi-frequency latents, a context transfer module is designed. The results show that both ToRes block and the incorporation of context transfer module help to improve the Rate-Distortion performance, and the combination of these two strategies makes our model achieve the state-of-the-art performance and outperform the latest compression standard Versatile Video Coding (VVC) in terms of both PSNR and MS-SSIM.",
    "code_link": ""
  },
  "aaai2022_main_danetsdeepabstractnetworksfortabulardataclassificationandregression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DANets: Deep Abstract Networks for Tabular Data Classification and Regression",
    "authors": [
      "Jintai Chen",
      "Kuanlun Liao",
      "Yao Wan",
      "Danny Z. Chen",
      "Jian Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20309",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20309/20068",
    "published": "2022-02",
    "summary": "Tabular data are ubiquitous in real world applications. Although many commonly-used neural components (e.g., convolution) and extensible neural networks (e.g., ResNet) have been developed by the machine learning community, few of them were effective for tabular data and few designs were adequately tailored for tabular data structures. In this paper, we propose a novel and flexible neural component for tabular data, called Abstract Layer (AbstLay), which learns to explicitly group correlative input features and generate higher-level features for semantics abstraction. Also, we design a structure re-parameterization method to compress the trained AbstLay, thus reducing the computational complexity by a clear margin in the reference phase. A special basic block is built using AbstLays, and we construct a family of Deep Abstract Networks (DANets) for tabular data classification and regression by stacking such blocks. In DANets, a special shortcut path is introduced to fetch information from raw tabular features, assisting feature interactions across different levels. Comprehensive experiments on seven real-world tabular datasets show that our AbstLay and DANets are effective for tabular data classification and regression, and the computational complexity is superior to competitive methods. Besides, we evaluate the performance gains of DANet as it goes deep, verifying the extendibility of our method. Our code is available at https://github.com/WhatAShot/DANet.",
    "code_link": ""
  },
  "aaai2022_main_fuzzylogicbasedlogicalqueryansweringonknowledgegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fuzzy Logic Based Logical Query Answering on Knowledge Graphs",
    "authors": [
      "Xuelu Chen",
      "Ziniu Hu",
      "Yizhou Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20310",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20310/20069",
    "published": "2022-02",
    "summary": "Answering complex First-Order Logical (FOL) queries on large-scale incomplete knowledge graphs (KGs) is an important yet challenging task. Recent advances embed logical queries and KG entities in the same space and conduct query answering via dense similarity search. However, most logical operators designed in previous studies do not satisfy the axiomatic system of classical logic, limiting their performance. Moreover, these logical operators are parameterized and thus require many complex FOL queries as training data, which are often arduous to collect or even inaccessible in most real-world KGs. We thus present FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a principled and learning-free manner, where only entity and relation embeddings require learning. FuzzQE can further benefit from labeled complex logical queries for training. Extensive experiments on two benchmark datasets demonstrate that FuzzQE provides significantly better performance in answering FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained with only KG link prediction can achieve comparable performance to those trained with extra complex query data.",
    "code_link": "https://github.com/snap-stanford/KGReasoning"
  },
  "aaai2022_main_taglearningtimedautomatafromlogs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TAG: Learning Timed Automata from Logs",
    "authors": [
      "L\u00e9na\u00efg Cornanguer",
      "Christine Largou\u00ebt",
      "Laurence Roz\u00e9",
      "Alexandre Termier"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20311",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20311/20070",
    "published": "2022-02",
    "summary": "Event logs are often one of the main sources of information to understand the behavior of a system. While numerous approaches have extracted partial information from event logs, in this work, we aim at inferring a global model of a system from its event logs.We consider real-time systems, which can be modeled with Timed Automata: our approach is thus a Timed Automata learner. There is a handful of related work, however, they might require a lot of parameters or produce Timed Automata that either are undeterministic or lack precision. In contrast, our proposed approach, called TAG, requires only one parameter and learns a deterministic Timed Automaton having a good tradeoff between accuracy and complexity of the automata. This allows getting an interpretable and accurate global model of the real-time system considered. Our experiments compare our approach to the related work and demonstrate its merits.",
    "code_link": ""
  },
  "aaai2022_main_differentiallydescribinggroupsofgraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Differentially Describing Groups of Graphs",
    "authors": [
      "Corinna Coupette",
      "Sebastian Dalleiger",
      "Jilles Vreeken"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20312",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20312/20071",
    "published": "2022-02",
    "summary": "How does neural connectivity in autistic children differ from neural connectivity in healthy children or autistic youths? What patterns in global trade networks are shared across classes of goods, and how do these patterns change over time? Answering questions like these requires us to differentially describe groups of graphs: Given a set of graphs and a partition of these graphs into groups, discover what graphs in one group have in common, how they systematically differ from graphs in other groups, and how multiple groups of graphs are related. We refer to this task as graph group analysis, which seeks to describe similarities and differences between graph groups by means of statistically significant subgraphs. To perform graph group analysis, we introduce Gragra, which uses maximum entropy modeling to identify a non-redundant set of subgraphs with statistically significant associations to one or more graph groups. Through an extensive set of experiments on a wide range of synthetic and real-world graph groups, we confirm that Gragra works well in practice.",
    "code_link": ""
  },
  "aaai2022_main_molecularcontrastivelearningwithchemicalelementknowledgegraph": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Molecular Contrastive Learning with Chemical Element Knowledge Graph",
    "authors": [
      "Yin Fang",
      "Qiang Zhang",
      "Haihong Yang",
      "Xiang Zhuang",
      "Shumin Deng",
      "Wen Zhang",
      "Ming Qin",
      "Zhuo Chen",
      "Xiaohui Fan",
      "Huajun Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20313",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20313/20072",
    "published": "2022-02",
    "summary": "Molecular representation learning contributes to multiple downstream tasks such as molecular property prediction and drug design. To properly represent molecules, graph contrastive learning is a promising paradigm as it utilizes self-supervision signals and has no requirements for human annotations. However, prior works fail to incorporate fundamental domain knowledge into graph semantics and thus ignore the correlations between atoms that have common attributes but are not directly connected by bonds. To address these issues, we construct a Chemical Element Knowledge Graph (KG) to summarize microscopic associations between elements and propose a novel Knowledge-enhanced Contrastive Learning (KCL) framework for molecular representation learning. KCL framework consists of three modules. The first module, knowledge-guided graph augmentation, augments the original molecular graph based on the Chemical Element KG. The second module, knowledge-aware graph representation, extracts molecular representations with a common graph encoder for the original molecular graph and aKnowledge-aware Message Passing Neural Network (KMPNN) to encode complex information in the augmented molecular graph. The final module is a contrastive objective, where we maximize agreement between these two views of molecular graphs. Extensive experiments demonstrated that KCL obtained superior performances against state-of-the-art baselines on eight molecular datasets. Visualization experiments properly interpret what KCL has learned from atoms and attributes in the augmented molecular graphs.",
    "code_link": "https://github.com/ZJU-Fangyin/KCL"
  },
  "aaai2022_main_heterogeneity-awaretwitterbotdetectionwithrelationalgraphtransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Heterogeneity-Aware Twitter Bot Detection with Relational Graph Transformers",
    "authors": [
      "Shangbin Feng",
      "Zhaoxuan Tan",
      "Rui Li",
      "Minnan Luo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20314",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20314/20073",
    "published": "2022-02",
    "summary": "Twitter bot detection has become an important and challenging task to combat misinformation and protect the integrity of the online discourse. State-of-the-art approaches generally leverage the topological structure of the Twittersphere, while they neglect the heterogeneity of relations and influence among users. In this paper, we propose a novel bot detection framework to alleviate this problem, which leverages the topological structure of user-formed heterogeneous graphs and models varying influence intensity between users. Specifically, we construct a heterogeneous information network with users as nodes and diversified relations as edges. We then propose relational graph transformers to model heterogeneous influence between users and learn node representations. Finally, we use semantic attention networks to aggregate messages across users and relations and conduct heterogeneity-aware Twitter bot detection. Extensive experiments demonstrate that our proposal outperforms state-of-the-art methods on a comprehensive Twitter bot detection benchmark. Additional studies also bear out the effectiveness of our proposed relational graph transformers, semantic attention networks and the graph-based approach in general.",
    "code_link": ""
  },
  "aaai2022_main_subspacedifferentialprivacy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Subspace Differential Privacy",
    "authors": [
      "Jie Gao",
      "Ruobin Gong",
      "Fang-Yi Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20315",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20315/20074",
    "published": "2022-02",
    "summary": "Many data applications have certain invariant constraints due to practical needs. Data curators who employ differential privacy need to respect such constraints on the sanitized data product as a primary utility requirement. Invariants challenge the formulation, implementation, and interpretation of privacy guarantees. We propose subspace differential privacy, to honestly characterize the dependence of the sanitized output on confidential aspects of the data. We discuss two design frameworks that convert well-known differentially private mechanisms, such as the Gaussian and the Laplace mechanisms, to subspace differentially private ones that respect the invariants specified by the curator. For linear queries, we discuss the design of near-optimal mechanisms that minimize the mean squared error. Subspace differentially private mechanisms rid the need for post-processing due to invariants, preserve transparency and statistical intelligibility of the output, and can be suitable for distributed implementation. We showcase the proposed mechanisms on the 2020 Census Disclosure Avoidance demonstration data, and a spatio-temporal dataset of mobile access point connections on a large university campus.",
    "code_link": ""
  },
  "aaai2022_main_orthogonalgraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Orthogonal Graph Neural Networks",
    "authors": [
      "Kai Guo",
      "Kaixiong Zhou",
      "Xia Hu",
      "Yu Li",
      "Yi Chang",
      "Xin Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20316",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20316/20075",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs) have received tremendous attention due to their superiority in learning node representations. These models rely on message passing and feature transformation functions to encode the structural and feature information from neighbors. However, stacking more convolutional layers significantly decreases the performance of GNNs. Most recent studies attribute this limitation to the over-smoothing issue, where node embeddings converge to indistinguishable vectors. Through a number of experimental observations, we argue that the main factor degrading the performance is the unstable forward normalization and backward gradient resulted from the improper design of the feature transformation, especially for shallow GNNs where the over-smoothing has not happened. Therefore, we propose a novel orthogonal feature transformation, named Ortho-GConv, which could generally augment the existing GNN backbones to stabilize the model training and improve the model's generalization performance. Specifically, we maintain the orthogonality of the feature transformation comprehensively from three perspectives, namely hybrid weight initialization, orthogonal transformation, and orthogonal regularization. By equipping the existing GNNs (e.g. GCN, JKNet, GCNII) with Ortho-GConv, we demonstrate the generality of the orthogonal feature transformation to enable stable training, and show its effectiveness for node and graph classification tasks.",
    "code_link": "https://github.com/KaiGuo20/Ortho-GConv"
  },
  "aaai2022_main_learningtemporalpointprocessesforefficientretrievalofcontinuoustimeeventsequences": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences",
    "authors": [
      "Vinayak Gupta",
      "Srikanta Bedathur",
      "Abir De"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20317",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20317/20076",
    "published": "2022-02",
    "summary": "Recent developments in predictive modeling using marked temporal point processes (MTPPs) have enabled an accurate characterization of several real-world applications involving continuous-time event sequences (CTESs). However, the retrieval problem of such sequences remains largely unaddressed in literature. To tackle this, we propose NEUROSEQRET which learns to retrieve and rank a relevant set of continuous-time event sequences for a given query sequence, from a large corpus of sequences. More specifically, NEUROSEQRET first applies a trainable unwarping function on the query sequence, which makes it comparable with corpus sequences, especially when a relevant query-corpus pair has individually different attributes. Next, it feeds the unwarped query sequence and the corpus sequence into MTPP guided neural relevance models. We develop two variants of the relevance model which offer a tradeoff between accuracy and efficiency. We also propose an optimization framework to learn binary sequence embeddings from the relevance scores, suitable for the locality-sensitive hashing leading to a significant speedup in returning top-K results for a given query sequence. Our experiments with several datasets show the significant accuracy boost of NEUROSEQRET beyond several baselines, as well as the efficacy of our hashing mechanism.",
    "code_link": ""
  },
  "aaai2022_main_gnn-retroretrosyntheticplanningwithgraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GNN-Retro: Retrosynthetic Planning with Graph Neural Networks",
    "authors": [
      "Peng Han",
      "Peilin Zhao",
      "Chan Lu",
      "Junzhou Huang",
      "Jiaxiang Wu",
      "Shuo Shang",
      "Bin Yao",
      "Xiangliang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20318",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20318/20077",
    "published": "2022-02",
    "summary": "Retrosynthetic planning plays an important role in the field of organic chemistry, which could generate a synthetic route for the target product. The synthetic route is a series of reactions which are started from the available molecules. The most challenging problem in the generation of the synthetic route is the large search space of the candidate reactions. Estimating the cost of candidate reactions has been proved effectively to prune the search space, which could achieve a higher accuracy with the same search iteration. And the estimation of one reaction is comprised of the estimations of all its reactants. So, how to estimate the cost of these reactants will directly influence the quality of results. To get a better performance, we propose a new framework, named GNN-Retro, for retrosynthetic planning problem by combining graph neural networks(GNN) and the latest search algorithm. The structure of GNN in our framework could incorporate the information of neighboring molecules, which will improve the estimation accuracy of our framework. The experiments on the USPTO dataset show that our framework could outperform the state-of-the-art methods with a large margin under the same settings.",
    "code_link": ""
  },
  "aaai2022_main_blockmodeling-guidedgraphconvolutionalneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Block Modeling-Guided Graph Convolutional Neural Networks",
    "authors": [
      "Dongxiao He",
      "Chundong Liang",
      "Huixin Liu",
      "Mingxiang Wen",
      "Pengfei Jiao",
      "Zhiyong Feng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20319",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20319/20078",
    "published": "2022-02",
    "summary": "Graph Convolutional Network (GCN) has shown remarkable potential of exploring graph representation. However, the GCN aggregating mechanism fails to generalize to networks with heterophily where most nodes have neighbors from different classes, which commonly exists in real-world networks. In order to make the propagation and aggregation mechanism of GCN suitable for both homophily and heterophily (or even their mixture), we introduce block modelling into the framework of GCN so that it can realize \u201cblock-guided classified aggregation\u201d, and automatically learn the corresponding aggregation rules for neighbors of different classes. By incorporating block modelling into the aggregation process, GCN is able to automatically aggregate information from homophilic and heterophilic neighbors discriminately according to their homophily degree. We compared our algorithm with state-of-art methods which deal with the heterophily problem. Empirical results demonstrate the superiority of our new approach over existing methods in heterophilic datasets while maintaining a competitive performance in homophilic datasets.",
    "code_link": ""
  },
  "aaai2022_main_catncrossattentivetree-awarenetworkformultivariatetimeseriesforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CATN: Cross Attentive Tree-Aware Network for Multivariate Time Series Forecasting",
    "authors": [
      "Hui He",
      "Qi Zhang",
      "Simeng Bai",
      "Kun Yi",
      "Zhendong Niu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20320",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20320/20079",
    "published": "2022-02",
    "summary": "Modeling complex hierarchical and grouped feature interaction in the multivariate time series data is indispensable to comprehend the data dynamics and predicting the future condition. The implicit feature interaction and high-dimensional data make multivariate forecasting very challenging. Many existing works did not put more emphasis on exploring explicit correlation among multiple time series data, and complicated models are designed to capture long- and short-range pattern with the aid of attention mechanism. In this work, we think that pre-defined graph or general learning method is difficult due to their irregular structure. Hence, we present CATN, an end-to-end model of Cross Attentive Tree-aware Network to jointly capture the inter-series correlation and intra-series temporal pattern. We first construct a tree structure to learn hierarchical and grouped correlation and design an embedding approach that can pass dynamic message to generalize implicit but interpretable cross features among multiple time series. Next in temporal aspect, we propose a multi-level dependency learning mechanism including global&local learning and cross attention mechanism, which can combine long-range dependencies, short-range dependencies as well as cross dependencies at different time steps. The extensive experiments on different datasets from real world show the effectiveness and robustness of the method we proposed when compared with existing state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_fpadametricfalse-positive-awareadaptivemetriclearningforsession-basedrecommendation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FPAdaMetric: False-Positive-Aware Adaptive Metric Learning for Session-Based Recommendation",
    "authors": [
      "Jongwon Jeong",
      "Jeong Choi",
      "Hyunsouk Cho",
      "Sehee Chung"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20321",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20321/20080",
    "published": "2022-02",
    "summary": "Modern recommendation systems are mostly based on implicit feedback data which can be quite noisy due to false positives (FPs) caused by many reasons, such as misclicks or quick curiosity. Numerous recommendation algorithms based on collaborative filtering have leveraged post-click user behavior (e.g., skip) to identify false positives. They effectively involved these false positives in the model supervision as negative-like signals. Yet, false positives had not been considered in existing session-based recommendation systems (SBRs) although they provide just as deleterious effects. To resolve false positives in SBRs, we first introduce FP-Metric model which reformulates the objective of the session-based recommendation with FP constraints into metric learning regularization. In addition, we propose FP-AdaMetric that enhances the metric-learning regularization terms with an adaptive module that elaborately calculates the impact of FPs inside sequential patterns. We verify that FP-AdaMetric improves several session-based recommendation models' performances in terms of Hit Rate (HR), MRR, and NDCG on datasets from different domains including music, movie, and game. Furthermore, we show that the adaptive module plays a much more crucial role in FP-AdaMetric model than in other baselines.",
    "code_link": "https://github.com/jongwonJeong/FPAdaMetric"
  },
  "aaai2022_main_stdentowardsphysics-guidedneuralnetworksfortrafficflowprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "STDEN: Towards Physics-Guided Neural Networks for Traffic Flow Prediction",
    "authors": [
      "Jiahao Ji",
      "Jingyuan Wang",
      "Zhe Jiang",
      "Jiawei Jiang",
      "Hu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20322",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20322/20081",
    "published": "2022-02",
    "summary": "High-performance traffic flow prediction model designing, a core technology of Intelligent Transportation System, is a long-standing but still challenging task for industrial and academic communities. The lack of integration between physical principles and data-driven models is an important reason for limiting the development of this field. In the literature, physics-based methods can usually provide a clear interpretation of the dynamic process of traffic flow systems but are with limited accuracy, while data-driven methods, especially deep learning with black-box structures, can achieve improved performance but can not be fully trusted due to lack of a reasonable physical basis. To bridge the gap between purely data-driven and physics-driven approaches, we propose a physics-guided deep learning model named Spatio-Temporal Differential Equation Network (STDEN), which casts the physical mechanism of traffic flow dynamics into a deep neural network framework. Specifically, we assume the traffic flow on road networks is driven by a latent potential energy field (like water flows are driven by the gravity field), and model the spatio-temporal dynamic process of the potential energy field as a differential equation network. STDEN absorbs both the performance advantage of data-driven models and the interpretability of physics-based models, so is named a physics-guided prediction model. Experiments on three real-world traffic datasets in Beijing show that our model outperforms state-of-the-art baselines by a significant margin. A case study further verifies that STDEN can capture the mechanism of urban traffic and generate accurate predictions with physical meaning. The proposed framework of differential equation network modeling may also cast light on other similar applications.",
    "code_link": "https://github.com/Echo-Ji/STDEN"
  },
  "aaai2022_main_namingthemostanomalousclusterinhilbertspaceforstructureswithattributeinformation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Naming the Most Anomalous Cluster in Hilbert Space for Structures with Attribute Information",
    "authors": [
      "Janis Kalofolias",
      "Jilles Vreeken"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20323",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20323/20082",
    "published": "2022-02",
    "summary": "We consider datasets consisting of arbitrarily structured entities (e.g., molecules,sequences, graphs, etc) whose similarity can be assessed with a reproducing ker-nel (or a family thereof). These entities are assumed to additionally have aset of named attributes (e.g.: number_of_atoms, stock_price, etc). Theseattributes can be used to classify the structured entities in discrete sets (e.g.,\u2018number_of_atoms < 3\u2019, \u2018stock_price \u2264 100\u2019, etc) and can effectively serveas Boolean predicates. Our goal is to use this side-information to provide explain-able kernel-based clustering. To this end, we propose a method which is ableto find among all possible entity subsets that can be described as a conjunctionof the available predicates either a) the optimal cluster within the ReproducingKernel Hilbert Space, or b) the most anomalous subset within the same space.Our method works employs combinatorial optimisation via an adaptation of theMaximum-Mean-Discrepancy measure that captures the above intuition. Finally,we propose a criterion to select the optimal one out of a family of kernels in away that preserves the available side-information. We provide several real worlddatasets that demonstrate the usefulness of our proposed method.",
    "code_link": ""
  },
  "aaai2022_main_meta-learningforonlineupdateofrecommendersystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Meta-Learning for Online Update of Recommender Systems",
    "authors": [
      "Minseok Kim",
      "Hwanjun Song",
      "Yooju Shin",
      "Dongmin Park",
      "Kijung Shin",
      "Jae-Gil\n      Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20324",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20324/20083",
    "published": "2022-02",
    "summary": "Online recommender systems should be always aligned with users' current interest to accurately suggest items that each user would like. Since user interest usually evolves over time, the update strategy should be flexible to quickly catch users' current interest from continuously generated new user-item interactions. Existing update strategies focus either on the importance of each user-item interaction or the learning rate for each recommender parameter, but such one-directional flexibility is insufficient to adapt to varying relationships between interactions and parameters. In this paper, we propose MeLON, a meta-learning based novel online recommender update strategy that supports two-directional flexibility. It is featured with an adaptive learning rate for each parameter-interaction pair for inducing a recommender to quickly learn users' up-to-date interest. The procedure of MeLON is optimized following a meta-learning approach: it learns how a recommender learns to generate the optimal learning rates for future updates. Specifically, MeLON first enriches the meaning of each interaction based on previous interactions and identifies the role of each parameter for the interaction; and then combines these two pieces of information to generate an adaptive learning rate. Theoretical analysis and extensive evaluation on three real-world online recommender datasets validate the effectiveness of MeLON.",
    "code_link": "https://github.com/kaist-dmlab/MeLON"
  },
  "aaai2022_main_thetriangle-densest-k-subgraphproblemhardness,lov\u00e1szextension,andapplicationtodocumentsummarization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Triangle-Densest-K-Subgraph Problem: Hardness, Lov\u00e1sz Extension, and Application to Document Summarization",
    "authors": [
      "Aritra Konar",
      "Nicholas D. Sidiropoulos"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20325",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20325/20084",
    "published": "2022-02",
    "summary": "We introduce the triangle-densest-K-subgraph problem (TDKS) for undirected graphs: given a size parameter K, compute a subset of K vertices that maximizes the number of induced triangles. The problem corresponds to the simplest generalization of the edge based densest-K-subgraph problem (DKS) to the case of higher-order network motifs. We prove that TDKS is NP-hard and is not amenable to efficient approximation, in the worst-case. By judiciously exploiting the structure of the problem, we propose a relaxation algorithm for the purpose of obtaining high-quality, sub-optimal solutions. Our approach utilizes the fact that the cost function of TDKS is submodular to construct a convex relaxation for the problem based on the Lov\u00e1sz extension for submodular functions. We demonstrate that our approaches attain state-of-the-art performance on real-world graphs and can offer substantially improved exploration of the optimal density-size curve compared to sophisticated approximation baselines for DKS. We use document summarization to showcase why TDKS is a useful generalization of DKS.",
    "code_link": ""
  },
  "aaai2022_main_obtainingcalibratedprobabilitieswithpersonalizedrankingmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Obtaining Calibrated Probabilities with Personalized Ranking Models",
    "authors": [
      "Wonbin Kweon",
      "SeongKu Kang",
      "Hwanjo Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20326",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20326/20085",
    "published": "2022-02",
    "summary": "For personalized ranking models, the well-calibrated probability of an item being preferred by a user has great practical value.While existing work shows promising results in image classification, probability calibration has not been much explored for personalized ranking.In this paper, we aim to estimate the calibrated probability of how likely a user will prefer an item.We investigate various parametric distributions and propose two parametric calibration methods, namely Gaussian calibration and Gamma calibration.Each proposed method can be seen as a post-processing function that maps the ranking scores of pre-trained models to well-calibrated preference probabilities, without affecting the recommendation performance.We also design the unbiased empirical risk minimization framework that guides the calibration methods to learning of true preference probability from the biased user-item interaction dataset.Extensive evaluations with various personalized ranking models on real-world datasets show that both the proposed calibration methods and the unbiased empirical risk minimization significantly improve the calibration performance.",
    "code_link": "https://github.com/WonbinKweon/CalibratedRankingModels"
  },
  "aaai2022_main_ddg-dadatadistributiongenerationforpredictableconceptdriftadaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation",
    "authors": [
      "Wendi Li",
      "Xiao Yang",
      "Weiqing Liu",
      "Yingce Xia",
      "Jiang Bian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20327",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20327/20086",
    "published": "2022-02",
    "summary": "In many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as the concept drift in the literature. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work. In this paper, we propose a novel method DDG-DA, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data. We conduct experiments on three real-world tasks (forecasting on stock price trend, electricity load and solar irradiance) and obtained significant improvement on multiple widely-used models.",
    "code_link": "https://github.com/Microsoft/qlib"
  },
  "aaai2022_main_unsupervisedanomalydetectionbyrobustdensityestimation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Anomaly Detection by Robust Density Estimation",
    "authors": [
      "Boyang Liu",
      "Pang-Ning Tan",
      "Jiayu Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20328",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20328/20087",
    "published": "2022-02",
    "summary": "Density estimation is a widely used method to perform unsupervised anomaly detection. By learning the density function, data points with relatively low densities are classified as anomalies. Unfortunately, the presence of anomalies in training data may significantly impact the density estimation process, thereby imposing significant challenges to the use of more sophisticated density estimation methods such as those based on deep neural networks. In this work, we propose RobustRealNVP, a deep density estimation framework that enhances the robustness of flow-based density estimation methods, enabling their application to unsupervised anomaly detection. RobustRealNVP differs from existing flow-based models from two perspectives. First, RobustRealNVP discards data points with low estimated densities during optimization to prevent them from corrupting the density estimation process. Furthermore, it imposes Lipschitz regularization to the flow-based model to enforce smoothness in the estimated density function. We demonstrate the robustness of our algorithm against anomalies in training data from both theoretical and empirical perspectives. The results show that our algorithm achieves competitive results as compared to state-of-the-art unsupervised anomaly detection methods.",
    "code_link": ""
  },
  "aaai2022_main_fromonetoalllearningtomatchheterogeneousandpartiallyoverlappedgraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "From One to All: Learning to Match Heterogeneous and Partially Overlapped Graphs",
    "authors": [
      "Weijie Liu",
      "Hui Qian",
      "Chao Zhang",
      "Jiahao Xie",
      "Zebang Shen",
      "Nenggan Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20329",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20329/20088",
    "published": "2022-02",
    "summary": "Recent years have witnessed a flurry of research activity in graph matching, which aims at finding the correspondence of nodes across two graphs and lies at the heart of many artificial intelligence applications. However, matching heterogeneous graphs with partial overlap remains a challenging problem in real-world applications. This paper proposes the first practical learning-to-match method to meet this challenge. The proposed unsupervised method adopts a novel partial optimal transport paradigm to learn a transport plan and node embeddings simultaneously. In a from-one-to-all manner, the entire learning procedure is decomposed into a series of easy-to-solve sub-procedures, each of which only handles the alignment of a single type of nodes. A mechanism for searching the transport mass is also proposed. Experimental results demonstrate that the proposed method outperforms state-of-the-art graph matching methods.",
    "code_link": ""
  },
  "aaai2022_main_tlogictemporallogicalrulesforexplainablelinkforecastingontemporalknowledgegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs",
    "authors": [
      "Yushan Liu",
      "Yunpu Ma",
      "Marcel Hildebrandt",
      "Mitchell Joblin",
      "Volker Tresp"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20330",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20330/20089",
    "published": "2022-02",
    "summary": "Conventional static knowledge graphs model entities in relational data as nodes, connected by edges of specific relation types. However, information and knowledge evolve continuously, and temporal dynamics emerge, which are expected to influence future situations. In temporal knowledge graphs, time information is integrated into the graph by equipping each edge with a timestamp or a time range. Embedding-based methods have been introduced for link prediction on temporal knowledge graphs, but they mostly lack explainability and comprehensible reasoning chains. Particularly, they are usually not designed to deal with link forecasting -- event prediction involving future timestamps. We address the task of link forecasting on temporal knowledge graphs and introduce TLogic, an explainable framework that is based on temporal logical rules extracted via temporal random walks. We compare TLogic with state-of-the-art baselines on three benchmark datasets and show better overall performance while our method also provides explanations that preserve time consistency. Furthermore, in contrast to most state-of-the-art embedding-based methods, TLogic works well in the inductive setting where already learned rules are transferred to related datasets with a common vocabulary.",
    "code_link": ""
  },
  "aaai2022_main_transferringthecontaminationfactorbetweenanomalydetectiondomainsbyshapesimilarity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity",
    "authors": [
      "Lorenzo Perini",
      "Vincent Vercruyssen",
      "Jesse Davis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20331",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20331/20090",
    "published": "2022-02",
    "summary": "Anomaly detection attempts to find examples in a dataset that do not conform to the expected behavior. Algorithms for this task assign an anomaly score to each example representing its degree of anomalousness. Setting a threshold on the anomaly scores enables converting these scores into a discrete prediction for each example. Setting an appropriate threshold is challenging in practice since anomaly detection is often treated as an unsupervised problem. A common approach is to set the threshold based on the dataset's contamination factor, i.e., the proportion of anomalous examples in the data. While the contamination factor may be known based on domain knowledge, it is often necessary to estimate it by labeling data. However, many anomaly detection problems involve monitoring multiple related, yet slightly different entities (e.g., a fleet of machines). Then, estimating the contamination factor for each dataset separately by labeling data would be extremely time-consuming. Therefore, this paper introduces a method for transferring the known contamination factor from one dataset (the source domain) to a related dataset where it is unknown (the target domain). Our approach does not require labeled target data and is based on modeling the shape of the distribution of the anomaly scores in both domains. We theoretically analyze how our method behaves when the (biased) target domain anomaly score distribution converges to its true one. Empirically, our method outperforms several baselines on real-world datasets.",
    "code_link": ""
  },
  "aaai2022_main_unifyingknowledgebasecompletionwithpulearningtomitigatetheobservationbias": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unifying Knowledge Base Completion with PU Learning to Mitigate the Observation Bias",
    "authors": [
      "Jonas Schouterden",
      "Jessa Bekker",
      "Jesse Davis",
      "Hendrik Blockeel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20332",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20332/20091",
    "published": "2022-02",
    "summary": "Methods for Knowledge Base Completion (KBC) reason about a knowledge base (KB) in order to derive new facts that should be included in the KB. This is challenging for two reasons. First, KBs only contain positive examples. This complicates model evaluation which needs both positive and negative examples. Second, those facts that were selected to be included in the knowledge base, are most likely not an i.i.d. sample of the true facts, due to the way knowledge bases are constructed. In this paper, we focus on rule-based approaches, which traditionally address the first challenge by making assumptions that enable identifying negative examples, which in turn makes it possible to compute a rule's confidence or precision. However, they largely ignore the second challenge, which means that their estimates of a rule's confidence can be biased. This paper approaches rule-based KBC through the lens of PU-learning, which can cope with both challenges. We make three contributions.: (1) We provide a unifying view that formalizes the relationship between multiple existing confidences measures based on (i) what assumption they make about and (ii) how their accuracy depends on the selection mechanism. (2) We introduce two new confidence measures that can mitigate known biases by using propensity scores that quantify how likely a fact is to be included the KB. (3) We show through theoretical and empirical analysis that taking the bias into account improves the confidence estimates, even when the propensity scores are not known exactly.",
    "code_link": ""
  },
  "aaai2022_main_aself-supervisedmixed-curvaturegraphneuralnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Self-Supervised Mixed-Curvature Graph Neural Network",
    "authors": [
      "Li Sun",
      "Zhongbao Zhang",
      "Junda Ye",
      "Hao Peng",
      "Jiawei Zhang",
      "Sen Su",
      "Philip S\n      Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20333",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20333/20092",
    "published": "2022-02",
    "summary": "Graph representation learning received increasing attentions in recent years. Most of the existing methods ignore the complexity of the graph structures and restrict graphs in a single constant-curvature representation space, which is only suitable to particular kinds of graph structure indeed. Additionally, these methods follow the supervised or semi-supervised learning paradigm, and thereby notably limit their deployment on the unlabeled graphs in real applications. To address these aforementioned limitations, we take the first attempt to study the self-supervised graph representation learning in the mixed-curvature spaces. In this paper, we present a novel Self-Supervised Mixed-Curvature Graph Neural Network (SelfMGNN). To capture the complex graph structures, we construct a mixed-curvature space via the Cartesian product of multiple Riemannian component spaces, and design hierarchical attention mechanisms for learning and fusing graph representations across these component spaces. To enable the self-supervised learning, we propose a novel dual contrastive approach. The constructed mixed-curvature space actually provides multiple Riemannian views for the contrastive learning. We introduce a Riemannian projector to reveal these views, and utilize a well-designed Riemannian discriminator for the single-view and cross-view contrastive learning within and across the Riemannian views. Finally, extensive experiments show that SelfMGNN captures the complex graph structures and outperforms state-of-the-art baselines.",
    "code_link": ""
  },
  "aaai2022_main_ms-hgatmemory-enhancedsequentialhypergraphattentionnetworkforinformationdiffusionprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MS-HGAT: Memory-Enhanced Sequential Hypergraph Attention Network for Information Diffusion Prediction",
    "authors": [
      "Ling Sun",
      "Yuan Rao",
      "Xiangbo Zhang",
      "Yuqian Lan",
      "Shuanghe Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20334",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20334/20093",
    "published": "2022-02",
    "summary": "Predicting the diffusion cascades is a critical task to understand information spread on social networks. Previous methods usually focus on the order or structure of the infected users in a single cascade, thus ignoring the global dependencies of users and cascades, limiting the performance of prediction. Current strategies to introduce social networks only learn the social homogeneity among users, which is not enough to describe their interaction preferences, let alone the dynamic changes. To address the above issues, we propose a novel information diffusion prediction model named Memory-enhanced Sequential Hypergraph Attention Networks (MS-HGAT). Specifically, to introduce the global dependencies of users, we not only take advantages of their friendships, but also consider their interactions at the cascade level. Furthermore, to dynamically capture user' preferences, we divide the diffusion hypergraph into several sub graphs based on timestamps, develop Hypergraph Attention Networks to learn the sequential hypergraphs, and connect them with gated fusion strategy. In addition, a memory-enhanced embedding lookup module is proposed to capture the learned user representations into the cascade-specific embedding space, thus highlighting the feature interaction within the cascade. The experimental results over four realistic datasets demonstrate that MS-HGAT significantly outperforms the state-of-the-art diffusion prediction models in both Hits@K and MAP@k metrics.",
    "code_link": ""
  },
  "aaai2022_main_graphstructurelearningwithvariationalinformationbottleneck": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph Structure Learning with Variational Information Bottleneck",
    "authors": [
      "Qingyun Sun",
      "Jianxin Li",
      "Hao Peng",
      "Jia Wu",
      "Xingcheng Fu",
      "Cheng Ji",
      "Philip\n      S Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20335",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20335/20094",
    "published": "2022-02",
    "summary": "Graph Neural Networks (GNNs) have shown promising results on a broad spectrum of applications. Most empirical studies of GNNs directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, graphs in the real-world are inevitably noisy or incomplete, which could even exacerbate the quality of graph representations. In this work, we propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the perspective of information theory. VIB-GSL is the first attempt to advance the Information Bottleneck (IB) principle for graph structure learning, providing a more elegant and universal framework for mining underlying task-relevant relations. VIB-GSL learns an informative and compressive graph structure to distill the actionable information for specific downstream tasks. VIB-GSL deduces a variational approximation for irregular graph data to form a tractable IB objective function, which facilitates training stability. Extensive experimental results demonstrate that the superior effectiveness and robustness of the proposed VIB-GSL.",
    "code_link": ""
  },
  "aaai2022_main_heterogeneouspeereffectsinthelinearthresholdmodel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Heterogeneous Peer Effects in the Linear Threshold Model",
    "authors": [
      "Christopher Tran",
      "Elena Zheleva"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20336",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20336/20095",
    "published": "2022-02",
    "summary": "The Linear Threshold Model is a widely used model that describes how information diffuses through a social network. According to this model, an individual adopts an idea or product after the proportion of their neighbors who have adopted it reaches a certain threshold. Typical applications of the Linear Threshold Model assume that thresholds are either the same for all network nodes or randomly distributed, even though some people may be more susceptible to peer pressure than others. To address individual-level differences, we propose causal inference methods for estimating individual thresholds that can more accurately predict whether and when individuals will be affected by their peers. We introduce the concept of heterogeneous peer effects and develop a Structural Causal Model which corresponds to the Linear Threshold Model and supports heterogeneous peer effect identification and estimation. We develop two algorithms for individual threshold estimation, one based on causal trees and one based on causal meta-learners. Our experimental results on synthetic and real- world datasets show that our proposed models can better predict individual-level thresholds in the Linear Threshold Model and thus more precisely predict which nodes will get activated over time.",
    "code_link": "https://github.com/edgeslab/hpe-ltm"
  },
  "aaai2022_main_exploringrelationalsemanticsforinductiveknowledgegraphcompletion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploring Relational Semantics for Inductive Knowledge Graph Completion",
    "authors": [
      "Changjian Wang",
      "Xiaofei Zhou",
      "Shirui Pan",
      "Linhua Dong",
      "Zeliang Song",
      "Ying\n      Sha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20337",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20337/20096",
    "published": "2022-02",
    "summary": "Knowledge graph completion (KGC) aims to infer missing information in incomplete knowledge graphs (KGs). Most previous works only consider the transductive scenario where entities are existing in KGs, which cannot work effectively for the inductive scenario containing emerging entities. Recently some graph neural network-based methods have been proposed for inductive KGC by aggregating neighborhood information to capture some uncertainty semantics from the neighboring auxiliary triples. But these methods ignore the more general relational semantics underlying all the known triples that can provide richer information to represent emerging entities so as to satisfy the inductive scenario. In this paper, we propose a novel model called CFAG, which utilizes two granularity levels of relational semantics in a coarse-grained aggregator (CG-AGG) and a fine-grained generative adversarial net (FG-GAN), for inductive KGC. The CG-AGG firstly generates entity representations with multiple semantics through a hypergraph neural network-based global aggregator and a graph neural network-based local aggregator, and the FG-GAN further enhances entity representations with specific semantics through conditional generative adversarial nets. Experimental results on benchmark datasets show that our model outperforms state-of-the-art models for inductive KGC.",
    "code_link": "https://github.com/changjianw/CFAG"
  },
  "aaai2022_main_hagenhomophily-awaregraphconvolutionalrecurrentnetworkforcrimeforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "HAGEN: Homophily-Aware Graph Convolutional Recurrent Network for Crime Forecasting",
    "authors": [
      "Chenyu Wang",
      "Zongyu Lin",
      "Xiaochen Yang",
      "Jiao Sun",
      "Mingxuan Yue",
      "Cyrus\n      Shahabi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20338",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20338/20097",
    "published": "2022-02",
    "summary": "The goal of the crime forecasting problem is to predict different types of crimes for each geographical region (like a neighborhood or censor tract) in the near future. Since nearby regions usually have similar socioeconomic characteristics which indicate similar crime patterns, recent state-of-the-art solutions constructed a distance-based region graph and utilized Graph Neural Network (GNN) techniques for crime forecasting, because the GNN techniques could effectively exploit the latent relationships between neighboring region nodes in the graph if the edges reveal high dependency or correlation. However, this distance-based pre-defined graph can not fully capture crime correlation between regions that are far from each other but share similar crime patterns. Hence, to make a more accurate crime prediction, the main challenge is to learn a better graph that reveals the dependencies between regions in crime occurrences and meanwhile captures the temporal patterns from historical crime records. To address these challenges, we propose an end-to-end graph convolutional recurrent network called HAGEN with several novel designs for crime prediction. Specifically, our framework could jointly capture the crime correlation between regions and the temporal crime dynamics by combining an adaptive region graph learning module with the Diffusion Convolution Gated Recurrent Unit (DCGRU). Based on the homophily assumption of GNN (i.e., graph convolution works better where neighboring nodes share the same label), we propose a homophily-aware constraint to regularize the optimization of the region graph so that neighboring region nodes on the learned graph share similar crime patterns, thus fitting the mechanism of diffusion convolution. Empirical experiments and comprehensive analysis on two real-world datasets showcase the effectiveness of HAGEN.",
    "code_link": "https://github.com/Rafa-zy/HAGEN"
  },
  "aaai2022_main_calibratednonparametricscanstatisticsforanomalouspatterndetectioningraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Calibrated Nonparametric Scan Statistics for Anomalous Pattern Detection in Graphs",
    "authors": [
      "Chunpai Wang",
      "Daniel B. Neill",
      "Feng Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20339",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20339/20098",
    "published": "2022-02",
    "summary": "We propose a new approach, the calibrated nonparametric scan statistic (CNSS), for more accurate detection of anomalous patterns in large-scale, real-world graphs. Scan statistics identify connected subgraphs that are interesting or unexpected through maximization of a likelihood ratio statistic; in particular, nonparametric scan statistics (NPSSs) identify subgraphs with a higher than expected proportion of individually significant nodes. However, we show that recently proposed NPSS methods are miscalibrated, failing to account for the maximization of the statistic over the multiplicity of subgraphs. This results in both reduced detection power for subtle signals, and low precision of the detected subgraph even for stronger signals. Thus we develop a new statistical approach to recalibrate NPSSs, correctly adjusting for multiple hypothesis testing and taking the underlying graph structure into account. While the recalibration, based on randomization testing, is computationally expensive, we propose both an efficient (approximate) algorithm and new, closed-form lower bounds (on the expected maximum proportion of significant nodes for subgraphs of a given size, under the null hypothesis of no anomalous patterns). These advances, along with the integration of recent core-tree decomposition methods, enable CNSS to scale to large real-world graphs, with substantial improvement in the accuracy of detected subgraphs. Extensive experiments on both semi-synthetic and real-world datasets are demonstrated to validate the effectiveness of our proposed methods, in comparison with state-of-the-art counterparts.",
    "code_link": ""
  },
  "aaai2022_main_powerfulgraphconvolutionalnetworkswithadaptivepropagationmechanismforhomophilyandheterophily": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Powerful Graph Convolutional Networks with Adaptive Propagation Mechanism for Homophily and Heterophily",
    "authors": [
      "Tao Wang",
      "Di Jin",
      "Rui Wang",
      "Dongxiao He",
      "Yuxiao Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20340",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20340/20099",
    "published": "2022-02",
    "summary": "Graph Convolutional Networks (GCNs) have been widely applied in various fields due to their significant power on processing graph-structured data. Typical GCN and its variants work under a homophily assumption (i.e., nodes with same class are prone to connect to each other), while ignoring the heterophily which exists in many real-world networks (i.e., nodes with different classes tend to form edges). Existing methods deal with heterophily by mainly aggregating higher-order neighborhoods or combing the immediate representations, which leads to noise and irrelevant information in the result. But these methods did not change the propagation mechanism which works under homophily assumption (that is a fundamental part of GCNs). This makes it difficult to distinguish the representation of nodes from different classes. To address this problem, in this paper we design a novel propagation mechanism, which can automatically change the propagation and aggregation process according to homophily or heterophily between node pairs. To adaptively learn the propagation process, we introduce two measurements of homophily degree between node pairs, which is learned based on topological and attribute information, respectively. Then we incorporate the learnable homophily degree into the graph convolution framework, which is trained in an end-to-end schema, enabling it to go beyond the assumption of homophily. More importantly, we theoretically prove that our model can constrain the similarity of representations between nodes according to their homophily degree. Experiments on seven real-world datasets demonstrate that this new approach outperforms the state-of-the-art methods under heterophily or low homophily, and gains competitive performance under homophily.",
    "code_link": ""
  },
  "aaai2022_main_shuttlenetposition-awarefusionofrallyprogressandplayerstylesforstrokeforecastinginbadminton": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ShuttleNet: Position-Aware Fusion of Rally Progress and Player Styles for Stroke Forecasting in Badminton",
    "authors": [
      "Wei-Yao Wang",
      "Hong-Han Shuai",
      "Kai-Shiang Chang",
      "Wen-Chih Peng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20341",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20341/20100",
    "published": "2022-02",
    "summary": "The increasing demand for analyzing the insights in sports has stimulated a line of productive studies from a variety of perspectives, e.g., health state monitoring, outcome prediction. In this paper, we focus on objectively judging what and where to return strokes, which is still unexplored in turn-based sports. By formulating stroke forecasting as a sequence prediction task, existing works can tackle the problem but fail to model information based on the characteristics of badminton. To address these limitations, we propose a novel Position-aware Fusion of Rally Progress and Player Styles framework (ShuttleNet) that incorporates rally progress and information of the players by two modified encoder-decoder extractors. Moreover, we design a fusion network to integrate rally contexts and contexts of the players by conditioning on information dependency and different positions. Extensive experiments on the badminton dataset demonstrate that ShuttleNet significantly outperforms the state-of-the-art methods and also empirically validates the feasibility of each component in ShuttleNet. On top of that, we provide an analysis scenario for the stroke forecasting problem.",
    "code_link": "https://github.com/wywyWang/ShuttleNet"
  },
  "aaai2022_main_event-awaremultimodalmobilitynowcasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Event-Aware Multimodal Mobility Nowcasting",
    "authors": [
      "Zhaonan Wang",
      "Renhe Jiang",
      "Hao Xue",
      "Flora D. Salim",
      "Xuan Song",
      "Ryosuke\n      Shibasaki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20342",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20342/20101",
    "published": "2022-02",
    "summary": "As a decisive part in the success of Mobility-as-a-Service (MaaS), spatio-temporal predictive modeling for crowd movements is a challenging task particularly considering scenarios where societal events drive mobility behavior deviated from the normality. While tremendous progress has been made to model high-level spatio-temporal regularities with deep learning, most, if not all of the existing methods are neither aware of the dynamic interactions among multiple transport modes nor adaptive to unprecedented volatility brought by potential societal events. In this paper, we are therefore motivated to improve the canonical spatio-temporal network (ST-Net) from two perspectives: (1) design a heterogeneous mobility information network (HMIN) to explicitly represent intermodality in multimodal mobility; (2) propose a memory-augmented dynamic filter generator (MDFG) to generate sequence-specific parameters in an on-the-fly fashion for various scenarios. The enhanced event-aware spatio-temporal network, namely EAST-Net, is evaluated on several real-world datasets with a wide variety and coverage of societal events. Both quantitative and qualitative experimental results verify the superiority of our approach compared with the state-of-the-art baselines. Code and data are published on https://github.com/underdoc-wang/EAST-Net.",
    "code_link": "https://github.com/underdoc-wang/EAST-Net"
  },
  "aaai2022_main_discoveringinterpretabledata-to-sequencegenerators": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Discovering Interpretable Data-to-Sequence Generators",
    "authors": [
      "Boris Wiegand",
      "Dietrich Klakow",
      "Jilles Vreeken"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20343",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20343/20102",
    "published": "2022-02",
    "summary": "We study the problem of predicting an event sequence given some meta data. In particular, we are interested in learning easily interpretable models that can accurately generate a sequence based on an attribute vector. To this end, we propose to learn a sparse event-flow graph over the training sequences, and statistically robust rules that use meta data to determine which paths to follow. We formalize the problem in terms of the Minimum Description Length (MDL) principle, by which we identify the best model as the one that compresses the data best. As the resulting optimization problem is NP-hard, we propose the efficient ConSequence algorithm to discover good event-flow graphs from data. Through an extensive set of experiments including a case study, we show that it ably discovers compact, interpretable and accurate models for the generation and prediction of event sequences from data, has a low sample complexity, and is particularly robust against noise.",
    "code_link": ""
  },
  "aaai2022_main_deepgpdadeeplearningapproachformodelinggeospatio-temporalextremeevents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepGPD: A Deep Learning Approach for Modeling Geospatio-Temporal Extreme Events",
    "authors": [
      "Tyler Wilson",
      "Pang-Ning Tan",
      "Lifeng Luo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20344",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20344/20103",
    "published": "2022-02",
    "summary": "Geospatio-temporal data are pervasive across numerous application domains.These rich datasets can be harnessed to predict extreme events such as disease outbreaks, flooding, crime spikes, etc. However, since the extreme events are rare, predicting them is a hard problem. Statistical methods based on extreme value theory provide a systematic way for modeling the distribution of extreme values. In particular, the generalized Pareto distribution (GPD) is useful for modeling the distribution of excess values above a certain threshold. However, applying such methods to large-scale geospatio-temporal data is a challenge due to the difficulty in capturing the complex spatial relationships between extreme events at multiple locations. This paper presents a deep learning framework for long-term prediction of the distribution of extreme values at different locations. We highlight its computational challenges and present a novel framework that combines convolutional neural networks with deep set and GPD. We demonstrate the effectiveness of our approach on a real-world dataset for modeling extreme climate events.",
    "code_link": "https://github.com/TylerPWilson/deepGPD"
  },
  "aaai2022_main_smartidxreducingcommunicationcostinfederatedlearningbyexploitingthecnnsstructures": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SmartIdx: Reducing Communication Cost in Federated Learning by Exploiting the CNNs Structures",
    "authors": [
      "Donglei Wu",
      "Xiangyu Zou",
      "Shuyu Zhang",
      "Haoyu Jin",
      "Wen Xia",
      "Binxing Fang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20345",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20345/20104",
    "published": "2022-02",
    "summary": "Top-k sparsification method is popular and powerful forreducing the communication cost in Federated Learning(FL). However, according to our experimental observation, it spends most of the total communication cost on the index of the selected parameters (i.e., their position informa-tion), which is inefficient for FL training. To solve this problem, we propose a FL compression algorithm for convolution neural networks (CNNs), called SmartIdx, by extending the traditional Top-k largest variation selection strategy intothe convolution-kernel-based selection, to reduce the proportion of the index in the overall communication cost and thusachieve a high compression ratio. The basic idea of SmartIdx is to improve the 1:1 proportion relationship betweenthe value and index of the parameters to n:1, by regarding the convolution kernel as the basic selecting unit in parameter selection, which can potentially deliver more informationto the parameter server under the limited network traffic. Tothis end, a set of rules are designed for judging which kernel should be selected and the corresponding packaging strategies are also proposed for further improving the compressionratio. Experiments on mainstream CNNs and datasets show that our proposed SmartIdx performs 2.5\u00d7\u221269.2\u00d7 higher compression ratio than the state-of-the-art FL compression algorithms without degrading model performance.",
    "code_link": "https://github.com/wudonglei99/smartidx"
  },
  "aaai2022_main_onlineenhancedsemantichashingtowardseffectiveandefficientretrievalforstreamingmulti-modaldata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data",
    "authors": [
      "Xiao-Ming Wu",
      "Xin Luo",
      "Yu-Wei Zhan",
      "Chen-Lu Ding",
      "Zhen-Duo Chen",
      "Xin-Shun\n      Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20346",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20346/20105",
    "published": "2022-02",
    "summary": "With the vigorous development of multimedia equipments and applications, efficient retrieval of large-scale multi-modal data has become a trendy research topic.Thereinto, hashing has become a prevalent choice due to its retrieval efficiency and low storage cost. Although multi-modal hashing has drawn lots of attention in recent years, there still remain some problems. The first point is that existing methods are mainly designed in batch mode and not able to efficiently handle streaming multi-modal data. The second point is that all existing online multi-modal hashing methods fail to effectively handle unseen new classes which come continuously with streaming data chunks. In this paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS). We design novel semantic-enhanced representation for data, which could help handle the new coming classes, and thereby construct the enhanced semantic objective function. An efficient and effective discrete online optimization algorithm is further proposed for OASIS. Extensive experiments show that our method can exceed the state-of-the-art models. For good reproducibility and benefiting the community, our code and data are already publicly available.",
    "code_link": ""
  },
  "aaai2022_main_cocosenhancingsemi-supervisedlearningongraphswithunlabeleddataviacontrastivecontextsharing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CoCoS: Enhancing Semi-supervised Learning on Graphs with Unlabeled Data via Contrastive Context Sharing",
    "authors": [
      "Siyue Xie",
      "Da Sun Handason Tam",
      "Wing Cheong Lau"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20347",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20347/20106",
    "published": "2022-02",
    "summary": "Graph Neural Networks (GNNs) have recently become a popular framework for semi-supervised learning on graph-structured data. However, typical GNN models heavily rely on labeled data in the learning process, while ignoring or paying little attention to the data that are unlabeled but available. To make full use of available data, we propose a generic framework, Contrastive Context Sharing (CoCoS), to enhance the learning capacity of GNNs for semi-supervised tasks. By sharing the contextual information among nodes estimated to be in the same class, different nodes can be correlated even if they are unlabeled and remote from each other in the graph. Models can therefore learn different combinations of contextual patterns, which improves the robustness of node representations. Additionally, motivated by recent advances in self-supervised learning, we augment the context sharing strategy by integrating with contrastive learning, which naturally correlates intra-class and inter-class data. Such operations utilize all available data for training and effectively improve a model's learning capacity. CoCoS can be easily extended to a wide range of GNN-based models with little computational overheads. Extensive experiments show that CoCoS considerably enhances typical GNN models, especially when labeled data are sparse in a graph, and achieves state-of-the-art or competitive results in real-world public datasets. The code of CoCoS is available online.",
    "code_link": "https://github.com/XsLangley/CoCoS"
  },
  "aaai2022_main_ensemblesemi-supervisedentityalignmentviacycle-teaching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Ensemble Semi-supervised Entity Alignment via Cycle-Teaching",
    "authors": [
      "Kexuan Xin",
      "Zequn Sun",
      "Wen Hua",
      "Bing Liu",
      "Wei Hu",
      "Jianfeng Qu",
      "Xiaofang\n      Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20348",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20348/20107",
    "published": "2022-02",
    "summary": "Entity alignment is to find identical entities in different knowledge graphs. Although embedding-based entity alignment has recently achieved remarkable progress, training data insufficiency remains a critical challenge. Conventional semi-supervised methods also suffer from the incorrect entity alignment in newly proposed training data. To resolve these issues, we design an iterative cycle-teaching framework for semi-supervised entity alignment. The key idea is to train multiple entity alignment models (called aligners) simultaneously and let each aligner iteratively teach its successor the proposed new entity alignment. We propose a diversity-aware alignment selection method to choose reliable entity alignment for each aligner. We also design a conflict resolution mechanism to resolve the alignment conflict when combining the new alignment of an aligner and that from its teacher. Besides, considering the influence of cycle-teaching order, we elaborately design a strategy to arrange the optimal order that can maximize the overall performance of multiple aligners. The cycle-teaching process can break the limitations of each model's learning capability and reduce the noise in new training data, leading to improved performance. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed cycle-teaching framework, which significantly outperforms the state-of-the-art models when the training data is insufficient and the new entity alignment has much noise.",
    "code_link": "https://github.com/JadeXIN/CycTEA"
  },
  "aaai2022_main_unsupervisedadversariallyrobustrepresentationlearningongraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Adversarially Robust Representation Learning on Graphs",
    "authors": [
      "Jiarong Xu",
      "Yang Yang",
      "Junru Chen",
      "Xin Jiang",
      "Chunping Wang",
      "Jiangang Lu",
      "Yizhou Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20349",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20349/20108",
    "published": "2022-02",
    "summary": "Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.",
    "code_link": "https://github.com/galina0217/robustgraph"
  },
  "aaai2022_main_blindfoldedattackersstillthreateningstrictblack-boxadversarialattacksongraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Blindfolded Attackers Still Threatening: Strict Black-Box Adversarial Attacks on Graphs",
    "authors": [
      "Jiarong Xu",
      "Yizhou Sun",
      "Xin Jiang",
      "Yanhao Wang",
      "Chunping Wang",
      "Jiangang\n      Lu",
      "Yang Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20350",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20350/20109",
    "published": "2022-02",
    "summary": "Adversarial attacks on graphs have attracted considerable research interests. Existing works assume the attacker is either (partly) aware of the victim model, or able to send queries to it. These assumptions are, however, unrealistic. To bridge the gap between theoretical graph attacks and real-world scenarios, in this work, we propose a novel and more realistic setting: strict black-box graph attack, in which the attacker has no knowledge about the victim model at all and is not allowed to send any queries. To design such an attack strategy, we first propose a generic graph filter to unify different families of graph-based models. The strength of attacks can then be quantified by the change in the graph filter before and after attack. By maximizing this change, we are able to find an effective attack strategy, regardless of the underlying model. To solve this optimization problem, we also propose a relaxation technique and approximation theories to reduce the difficulty as well as the computational expense. Experiments demonstrate that, even with no exposure to the model, the Macro-F1 drops 6.4% in node classification and 29.5% in graph classification, which is a significant result compared with existent works.",
    "code_link": "https://github.com/galina0217/stack"
  },
  "aaai2022_main_polygonemodelingn-aryrelationaldataasgyro-polygonsinhyperbolicspace": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PolygonE: Modeling N-ary Relational Data as Gyro-Polygons in Hyperbolic Space",
    "authors": [
      "Shiyao Yan",
      "Zequn Zhang",
      "Xian Sun",
      "Guangluan Xu",
      "Shuchao Li",
      "Qing Liu",
      "Nayu Liu",
      "Shensi Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20351",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20351/20110",
    "published": "2022-02",
    "summary": "N-ary relational knowledge base (KBs) embedding aims to map binary and beyond-binary facts into low-dimensional vector space simultaneously. Existing approaches typically decompose n-ary relational facts into subtuples (entity pairs, triples or quintuples, etc.), and they generally model n-ary relational KBs in Euclidean space. However, n-ary relational facts are semantically and structurally intact, decomposition leads to the loss of global information and undermines the semantical and structural integrity. Moreover, compared to the binary relational KBs, n-ary ones are characterized by more abundant and complicated hierarchy structures, which could not be well expressed in Euclidean space. To address the issues, we propose a gyro-polygon embedding approach to realize n-ary fact integrity keeping and hierarchy capturing, termed as PolygonE. Specifically, n-ary relational facts are modeled as gyro-polygons in the hyperbolic space, where we denote entities in facts as vertexes of gyro-polygons and relations as entity translocation operations. Importantly, we design a fact plausibility measuring strategy based on the vertex-gyrocentroid geodesic to optimize the relation-adjusted gyro-polygon. Extensive experiments demonstrate that PolygonE shows SOTA performance on all benchmark datasets, generalizability to binary data, and applicability to arbitrary arity fact. Finally, we also visualize the embedding to help comprehend PolygonE's awareness of hierarchies.",
    "code_link": ""
  },
  "aaai2022_main_cross-taskknowledgedistillationinmulti-taskrecommendation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Task Knowledge Distillation in Multi-Task Recommendation",
    "authors": [
      "Chenxiao Yang",
      "Junwei Pan",
      "Xiaofeng Gao",
      "Tingyu Jiang",
      "Dapeng Liu",
      "Guihai\n      Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20352",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20352/20111",
    "published": "2022-02",
    "summary": "Multi-task learning (MTL) has been widely used in recommender systems, wherein predicting each type of user feedback on items (e.g, click, purchase) are treated as individual tasks and jointly trained with a unified model. Our key observation is that the prediction results of each task may contain task-specific knowledge about user\u2019s fine-grained preference towards items. While such knowledge could be transferred to benefit other tasks, it is being overlooked under the current MTL paradigm. This paper, instead, proposes a Cross-Task Knowledge Distillation framework that attempts to leverage prediction results of one task as supervised signals to teach another task. However, integrating MTL and KD in a proper manner is non-trivial due to several challenges including task conflicts, inconsistent magnitude and requirement of synchronous optimization. As countermeasures, we 1) introduce auxiliary tasks with quadruplet loss functions to capture cross-task fine-grained ranking information and avoid task conflicts, 2) design a calibrated distillation approach to align and distill knowledge from auxiliary tasks, and 3) propose a novel error correction mechanism to enable and facilitate synchronous training of teacher and student models. Comprehensive experiments are conducted to verify the effectiveness of our framework in real-world datasets.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedgraphneuralnetworksviadiverseandinteractivemessagepassing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Graph Neural Networks via Diverse and Interactive Message Passing",
    "authors": [
      "Liang Yang",
      "Cheng Chen",
      "Weixun Li",
      "Bingxin Niu",
      "Junhua Gu",
      "Chuan Wang",
      "Dongxiao He",
      "Yuanfang Guo",
      "Xiaochun Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20353",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20353/20112",
    "published": "2022-02",
    "summary": "By interpreting Graph Neural Networks (GNNs) as the message passing from the spatial perspective, their success is attributed to Laplacian smoothing. However, it also leads to serious over-smoothing issue by stacking many layers. Recently, many efforts have been paid to overcome this issue in semi-supervised learning. Unfortunately, it is more serious in unsupervised node representation learning task due to the lack of supervision information. Thus, most of the unsupervised or self-supervised GNNs often employ \\textit{one-layer GCN} as the encoder. Essentially, the over-smoothing issue is caused by the over-simplification of the existing message passing, which possesses two intrinsic limits: blind message and uniform passing. In this paper, a novel Diverse and Interactive Message Passing (DIMP) is proposed for self-supervised learning by overcoming these limits. Firstly, to prevent the message from blindness and make it interactive between two connected nodes, the message is determined by both the two connected nodes instead of the attributes of one node. Secondly, to prevent the passing from uniformness and make it diverse over different attribute channels, different propagation weights are assigned to different elements in the message. To this end, a natural implementation of the message in DIMP is the element-wise product of the representations of two connected nodes. From the perspective of numerical optimization, the proposed DIMP is equivalent to performing an overlapping community detection via expectation-maximization (EM). Both the objective function of the community detection and the convergence of EM algorithm guarantee that DMIP can prevent from over-smoothing issue. Extensive evaluations on node-level and graph-level tasks demonstrate the superiority of DIMP on improving performance and overcoming over-smoothing issue.",
    "code_link": ""
  },
  "aaai2022_main_multi-scaledistillationfrommultiplegraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Scale Distillation from Multiple Graph Neural Networks",
    "authors": [
      "Chunhai Zhang",
      "Jie Liu",
      "Kai Dang",
      "Wenzheng Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20354",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20354/20113",
    "published": "2022-02",
    "summary": "Knowledge Distillation (KD), which is an effective model compression and acceleration technique, has been successfully applied to graph neural networks (GNNs) recently. Existing approaches utilize a single GNN model as the teacher to distill knowledge. However, we notice that GNN models with different number of layers demonstrate different classification abilities on nodes with different degrees. On the one hand, for nodes with high degrees, their local structures are dense and complex, hence more message passing is needed. Therefore, GNN models with more layers perform better. On the other hand, for nodes with low degrees, whose local structures are relatively sparse and simple, the repeated message passing can easily lead to over-smoothing. Thus, GNN models with less layers are more suitable. However, existing single-teacher GNN knowledge distillation approaches which are based on a single GNN model, are sub-optimal. To this end, we propose a novel approach to distill multi-scale knowledge, which learns from multiple GNN teacher models with different number of layers to capture the topological semantic at different scales. Instead of learning from the teacher models equally, the proposed method automatically assigns proper weights for each teacher model via an attention mechanism which enables the student to select teachers for different local structures. Extensive experiments are conducted to evaluate the proposed method on four public datasets. The experimental results demonstrate the superiority of our proposed method over state-of-the-art methods. Our code is publicly available at https://github.com/NKU-IIPLab/MSKD.",
    "code_link": "https://github.com/NKU-IIPLab/MSKD"
  },
  "aaai2022_main_mindthegapcross-lingualinformationretrievalwithhierarchicalknowledgeenhancement": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement",
    "authors": [
      "Fuwei Zhang",
      "Zhao Zhang",
      "Xiang Ao",
      "Dehong Gao",
      "Fuzhen Zhuang",
      "Yi Wei",
      "Qing\n      He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20355",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20355/20114",
    "published": "2022-02",
    "summary": "Cross-Lingual Information Retrieval (CLIR) aims to rank the documents written in a language different from the user\u2019s query. The intrinsic gap between different languages is an essential challenge for CLIR. In this paper, we introduce the multilingual knowledge graph (KG) to the CLIR task due to the sufficient information of entities in multiple languages. It is regarded as a \u201csilver bullet\u201d to simultaneously perform explicit alignment between queries and documents and also broaden the representations of queries. And we propose a model named CLIR with HIerarchical Knowledge Enhancement (HIKE) for our task. The proposed model encodes the textual information in queries, documents and the KG with multilingual BERT, and incorporates the KG information in the query-document matching process with a hierarchical information fusion mechanism. Particularly, HIKE first integrates the entities and their neighborhood in KG into query representations with a knowledge-level fusion, then combines the knowledge from both source and target languages to further mitigate the linguistic gap with a language-level fusion. Finally, experimental results demonstrate that HIKE achieves substantial improvements over state-of-the-art competitors.",
    "code_link": ""
  },
  "aaai2022_main_anisotropicadditivequantizationforfastinnerproductsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Anisotropic Additive Quantization for Fast Inner Product Search",
    "authors": [
      "Jin Zhang",
      "Qi Liu",
      "Defu Lian",
      "Zheng Liu",
      "Le Wu",
      "Enhong Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20356",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20356/20115",
    "published": "2022-02",
    "summary": "Maximum Inner Product Search (MIPS) plays an important role in many applications ranging from information retrieval, recommender systems to natural language processing and machine learning. However, exhaustive MIPS is often expensive and impractical when there are a large number of candidate items. The state-of-the-art approximated MIPS is product quantization with a score-aware loss, which weighs more heavily on items with larger inner product scores. However, it is challenging to extend the score-aware loss for additive quantization due to parallel-orthogonal decomposition of residual error. Learning additive quantization with respect to this loss is important since additive quantization can achieve a lower approximation error than product quantization. To this end, we propose a quantization method called Anisotropic Additive Quantization to combine the score-aware anisotropic loss and additive quantization. To efficiently update the codebooks in this algorithm, we develop a new alternating optimization algorithm. The proposed algorithm is extensively evaluated on three real-world datasets. The experimental results show that it outperforms the state-of-the-art baselines with respect to approximate search accuracy while guaranteeing a similar retrieval efficiency.",
    "code_link": ""
  },
  "aaai2022_main_robustheterogeneousgraphneuralnetworksagainstadversarialattacks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Heterogeneous Graph Neural Networks against Adversarial Attacks",
    "authors": [
      "Mengmei Zhang",
      "Xiao Wang",
      "Meiqi Zhu",
      "Chuan Shi",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20357",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20357/20116",
    "published": "2022-02",
    "summary": "Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. However, despite their wide use, there is currently no understanding of their robustness to adversarial attacks. In this work, we first systematically study the robustness of HGNNs and show that they can be easily fooled by adding the adversarial edge between the target node and large-degree node (i.e., hub). Furthermore, we show two key reasons for such vulnerability of HGNNs: one is perturbation enlargement effect, i.e., HGNNs, failing to encode transiting probability, will enlarge the effect of the adversarial hub in comparison of GCNs, and the other is soft attention mechanism, i.e., such mechanism assigns positive attention values to obviously unreliable neighbors. Based on the two facts, we propose a novel robust HGNN framework RoHe against topology adversarial attacks by equipping an attention purifier, which can prune malicious neighbors based on topology and feature. Specifically, to eliminate the perturbation enlargement, we introduce the metapath-based transiting probability as the prior criterion of the purifier, restraining the confidence of malicious neighbors from the adversarial hub. Then the purifier learns to mask out neighbors with low confidence, thus can effectively alleviate the negative effect of malicious neighbors in the soft attention mechanism. Extensive experiments on different benchmark datasets for multiple HGNNs are conducted, where the considerable improvement of HGNNs under adversarial attacks will demonstrate the effectiveness and generalization ability of our defense framework.",
    "code_link": ""
  },
  "aaai2022_main_multi-dimensionalpredictionofguildhealthinonlinegamesastability-awaremulti-tasklearningapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Dimensional Prediction of Guild Health in Online Games: A Stability-Aware Multi-Task Learning Approach",
    "authors": [
      "Chuang Zhao",
      "Hongke Zhao",
      "Runze Wu",
      "Qilin Deng",
      "Yu Ding",
      "Jianrong Tao",
      "Changjie Fan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20358",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20358/20117",
    "published": "2022-02",
    "summary": "Guild is the most important long-term virtual community and emotional bond in massively multiplayer online role-playing games (MMORPGs). It matters a lot to the player retention and game ecology how the guilds are going, e.g., healthy or not. The main challenge now is to characterize and predict the guild health in a quantitative, dynamic, and multi-dimensional manner based on complicated multi-media data streams. To this end, we propose a novel framework, namely Stability-Aware Multi-task Learning Approach(SAMLA) to address these challenges. Specifically, different media-specific modules are designed to extract information from multiple media types of tabular data, time seriescharacteristics, and heterogeneous graphs. To capture the dynamics of guild health, we introduce a representation encoder to provide a time series view of multi-media data that is used for task prediction. Inspiredby well-received theories on organization management, we delicately define five specific and quantitative dimensions of guild health and make parallel predictions based on a multi-task approach. Besides, we devise a novel auxiliary task, i.e.,the guild stability, to boost the performance of the guild health prediction task. Extensive experiments on a real-world large-scale MMORPG dataset verify that our proposed method outperforms the state-of-the-art methods in the task of organizational health characterization and prediction. Moreover, our work has been practically deployed in online MMORPG, and case studies clearly illustrate the significant value.",
    "code_link": "https://github.com/DataDesigner/AAAI2022-GHP.git"
  },
  "aaai2022_main_multi-viewintentdisentanglegraphnetworksforbundlerecommendation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-View Intent Disentangle Graph Networks for Bundle Recommendation",
    "authors": [
      "Sen Zhao",
      "Wei Wei",
      "Ding Zou",
      "Xianling Mao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20359",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20359/20118",
    "published": "2022-02",
    "summary": "Bundle recommendation aims to recommend the user a bundle of items as a whole. Previous models capture user\u2019s preferences on both items and the association of items. Nevertheless, they usually neglect the diversity of user\u2019s intents on adopting items and fail to disentangle user\u2019s intents in representations. In the real scenario of bundle recommendation, a user\u2019s intent may be naturally distributed in the different bundles of that user (Global view). And a bundle may contain multiple intents of a user (Local view). Each view has its advantages for intent disentangling: 1) In the global view, more items are involved to present each intent, which can demonstrate the user\u2019s preference under each intent more clearly. 2) The local view can reveal the association between items undereach intent since the items within the same bundle are highly correlated to each other. To this end, in this paper we propose a novel model named Multi-view Intent Disentangle Graph Networks (MIDGN), which is capable of precisely and comprehensively capturing the diversity of user intent and items\u2019 associations at the finer granularity. Specifically, MIDGN disentangles user\u2019s intents from two different perspectives, respectively: 1) taking the Global view, MIDGN disentanglesthe user\u2019s intent coupled with inter-bundle items; 2) taking the Local view, MIDGN disentangles the user\u2019s intent coupled with items within each bundle. Meanwhile, we compare user\u2019s intents disentangled from different views by a contrast method to improve the learned intents. Extensive experiments are conducted on two benchmark datasets and MIDGN outperforms the state-of-the-art methods by over 10.7% and 26.8%, respectively.",
    "code_link": ""
  },
  "aaai2022_main_multi-typeurbancrimeprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Type Urban Crime Prediction",
    "authors": [
      "Xiangyu Zhao",
      "Wenqi Fan",
      "Hui Liu",
      "Jiliang Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20360",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20360/20119",
    "published": "2022-02",
    "summary": "Crime prediction plays an impactful role in enhancing public security and sustainable development of urban. With recent advances in data collection and integration technologies, a large amount of urban data with rich crime-related information and fine-grained spatio-temporal logs have been recorded. Such helpful information can boost our understandings of the temporal evolution and spatial factors of urban crimes and can enhance accurate crime prediction. However, the vast majority of existing crime prediction algorithms either do not distinguish different types of crime or treat each crime type separately, which fails to capture the intrinsic correlations among different types of crime. In this paper, we perform crime prediction exploiting the cross-type and spatio-temporal correlations of urban crimes. In particular, we verify the existence of correlations among different types of crime from temporal and spatial perspectives, and propose a coherent framework to mathematically model these correlations for crime prediction. Extensive experiments on real-world datasets validate the effectiveness of our framework.",
    "code_link": ""
  },
  "aaai2022_main_forecastingassetdependenciestoreduceportfoliorisk": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Forecasting Asset Dependencies to Reduce Portfolio Risk",
    "authors": [
      "Haoren Zhu",
      "Shih-Yang Liu",
      "Pengfei Zhao",
      "Yingying Chen",
      "Dik Lun Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20361",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20361/20120",
    "published": "2022-02",
    "summary": "Financial assets exhibit dependence structures, i.e., movements of their prices or returns show various correlations. Knowledge of assets\u2019 price dependencies can help investors to create a diversified portfolio, aiming to reduce portfolio risk due to the high volatility of the financial market. Since asset dependency changes with time in complex patterns, asset dependency forecast is an essential problem in finance. In this paper, we organize pairwise assets dependencies in an Asset Dependency Matrix (ADM) and formulate the problem of assets dependencies forecast to predict the future ADM given a sequence of past ADMs. We propose a novel idea viewing a sequence of ADMs as a sequence of images to capture the spatial and temporal dependencies among the assets. Inspired by video prediction tasks, we develop a novel Asset Dependency Neural Network (ADNN) to tackle the ADM prediction problem. Experiments show that our proposed framework consistently outperforms baselines on both future ADM prediction and portfolio risk reduction tasks.",
    "code_link": ""
  },
  "aaai2022_main_defendinggraphconvolutionalnetworksagainstdynamicgraphperturbationsviabayesianself-supervision": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Defending Graph Convolutional Networks against Dynamic Graph Perturbations via Bayesian Self-Supervision",
    "authors": [
      "Jun Zhuang",
      "Mohammad Al Hasan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20362",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20362/20121",
    "published": "2022-02",
    "summary": "In recent years, plentiful evidence illustrates that Graph Convolutional Networks (GCNs) achieve extraordinary accomplishments on the node classification task. However, GCNs may be vulnerable to adversarial attacks on label-scarce dynamic graphs. Many existing works aim to strengthen the robustness of GCNs; for instance, adversarial training is used to shield GCNs against malicious perturbations. However, these works fail on dynamic graphs for which label scarcity is a pressing issue. To overcome label scarcity, self-training attempts to iteratively assign pseudo-labels to highly confident unlabeled nodes but such attempts may suffer serious degradation under dynamic graph perturbations. In this paper, we generalize noisy supervision as a kind of self-supervised learning method and then propose a novel Bayesian self-supervision model, namely GraphSS, to address the issue. Extensive experiments demonstrate that GraphSS can not only affirmatively alert the perturbations on dynamic graphs but also effectively recover the prediction of a node classifier when the graph is under such perturbations. These two advantages prove to be generalized over three classic GCNs across five public graph datasets.",
    "code_link": "https://github.com/junzhuang-code/GraphSS"
  },
  "aaai2022_main_canmachinesreadcodingmanualsyet?\u2013abenchmarkforbuildingbetterlanguagemodelsforcodeunderstanding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Can Machines Read Coding Manuals Yet? \u2013 A Benchmark for Building Better Language Models for Code Understanding",
    "authors": [
      "Ibrahim Abdelaziz",
      "Julian Dolby",
      "Jamie McCusker",
      "Kavitha Srinivas"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20363",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20363/20122",
    "published": "2022-02",
    "summary": "Code understanding is an increasingly important application of Artificial Intelligence. A fundamental aspect of understanding code is understanding text about code, e.g., documentationand forum discussions.Pre-trained language models (e.g., BERT) are a popular approach for various NLP tasks, and there are now a variety of benchmarks, such as GLUE, to help improve the development of such models for natural language understanding. However, little is known about how well such models work on textual artifacts about code, and we are unaware of any systematic set of downstream tasks for such an evaluation.In this paper, we derive a set of benchmarks (BLANCA - Benchmarks for LANguage models on Coding Artifacts) that assess code understanding based on tasks such as predicting the best answer to a question in a forum post, finding related forum posts, or predicting classes related in a hierarchy from class documentation.We evaluate performance of current state-of-the-art language models on these tasks and show that there is significant improvement on each task from fine tuning.We also show that multi-task training over BLANCA tasks help build better language models for code understanding.",
    "code_link": ""
  },
  "aaai2022_main_notaskleftbehindmulti-tasklearningofknowledgetracingandoptiontracingforbetterstudentassessment": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "No Task Left Behind: Multi-Task Learning of Knowledge Tracing and Option Tracing for Better Student Assessment",
    "authors": [
      "Suyeong An",
      "Junghoon Kim",
      "Minsam Kim",
      "Juneyoung Park"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20364",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20364/20123",
    "published": "2022-02",
    "summary": "Student assessment is one of the most fundamental tasks in the field of AI Education (AIEd). One of the most common approach to student assessment is Knowledge Tracing (KT), which evaluates a student's knowledge state by predicting whether the student will answer a given question correctly or not. However, in the context of multiple choice (polytomous) questions, conventional KT approaches are limited in that they only consider the binary (dichotomous) correctness label (i.e., correct or incorrect), and disregard the specific option chosen by the student. Meanwhile, Option Tracing (OT) attempts to model a student by predicting which option they will choose for a given question, but overlooks the correctness information. In this paper, we propose Dichotomous-Polytomous Multi-Task Learning (DP-MTL), a multi-task learning framework that combines KT and OT for more precise student assessment. In particular, we show that the KT objective acts as a regularization term for OT in the DP-MTL framework, and propose an appropriate architecture for applying our method on top of existing deep learning-based KT models. We experimentally confirm that DP-MTL significantly improves both KT and OT performances, and also benefits downstream tasks such as Score Prediction (SP).",
    "code_link": "https://github.com/godtn0/DP-MTL"
  },
  "aaai2022_main_diaformerautomaticdiagnosisviasymptomssequencegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Diaformer: Automatic Diagnosis via Symptoms Sequence Generation",
    "authors": [
      "Junying Chen",
      "Dongfang Li",
      "Qingcai Chen",
      "Wenxiu Zhou",
      "Xin Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20365",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20365/20124",
    "published": "2022-02",
    "summary": "Automatic diagnosis has attracted increasing attention but remains challenging due to multi-step reasoning. Recent works usually address it by reinforcement learning methods. However, these methods show low efficiency and require task-specific reward functions. Considering the conversation between doctor and patient allows doctors to probe for symptoms and make diagnoses, the diagnosis process can be naturally seen as the generation of a sequence including symptoms and diagnoses. Inspired by this, we reformulate automatic diagnosis as a symptoms Sequence Generation (SG) task and propose a simple but effective automatic Diagnosis model based on Transformer (Diaformer). We firstly design the symptom attention framework to learn the generation of symptom inquiry and the disease diagnosis. To alleviate the discrepancy between sequential generation and disorder of implicit symptoms, we further design three orderless training mechanisms. Experiments on three public datasets show that our model outperforms baselines on disease diagnosis by 1%, 6% and 11.5% with the highest training efficiency. Detailed analysis on symptom inquiry prediction demonstrates that the potential of applying symptoms sequence generation for automatic diagnosis.",
    "code_link": "https://github.com/jymChen/Diaformer"
  },
  "aaai2022_main_zero-shotaudiosourceseparationthroughquery-basedlearningfromweakly-labeleddata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Zero-Shot Audio Source Separation through Query-Based Learning from Weakly-Labeled Data",
    "authors": [
      "Ke Chen",
      "Xingjian Du",
      "Bilei Zhu",
      "Zejun Ma",
      "Taylor Berg-Kirkpatrick",
      "Shlomo\n      Dubnov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20366",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20366/20125",
    "published": "2022-02",
    "summary": "Deep learning techniques for separating audio into different sound sources face several challenges. Standard architectures require training separate models for different types of audio sources. Although some universal separators employ a single model to target multiple sources, they have difficulty generalizing to unseen sources. In this paper, we propose a three-component pipeline to train a universal audio source separator from a large, but weakly-labeled dataset: AudioSet. First, we propose a transformer-based sound event detection system for processing weakly-labeled training data. Second, we devise a query-based audio separation model that leverages this data for model training. Third, we design a latent embedding processor to encode queries that specify audio targets for separation, allowing for zero-shot generalization. Our approach uses a single model for source separation of multiple sound types, and relies solely on weakly-labeled data for training. In addition, the proposed audio separator can be used in a zero-shot setting, learning to separate types of audio sources that were never seen in training. To evaluate the separation performance, we test our model on MUSDB18, while training on the disjoint AudioSet. We further verify the zero-shot performance by conducting another experiment on audio source types that are held-out from training. The model achieves comparable Source-to-Distortion Ratio (SDR) performance to current supervised models in both cases.",
    "code_link": ""
  },
  "aaai2022_main_deephardmarktowardswatermarkingneuralnetworkhardware": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepHardMark: Towards Watermarking Neural Network Hardware",
    "authors": [
      "Joseph Clements",
      "Yingjie Lao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20367",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20367/20126",
    "published": "2022-02",
    "summary": "This paper presents a framework for embedding watermarks into DNN hardware accelerators. Unlike previous works that have looked at protecting the algorithmic intellectual properties of deep learning systems, this work proposes a methodology for defending deep learning hardware. Our methodology embeds modifications into the hardware accelerator's functional blocks that can be revealed with the rightful owner's key DNN and corresponding key sample, verifying the legitimate owner. We propose an Lp-box ADMM based algorithm to co-optimize watermark's hardware overhead and impact on the design's algorithmic functionality. We evaluate the performance of the hardware watermarking scheme on popular image classifier models using various accelerator designs. Our results demonstrate that the proposed methodology effectively embeds watermarks while preserving the original functionality of the hardware architecture. Specifically, we can successfully embed watermarks into the deep learning hardware and reliably execute a ResNet ImageNet classifiers with an accuracy degradation of only 0.009%",
    "code_link": ""
  },
  "aaai2022_main_aunifiedframeworkforrealtimemotioncompletion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Unified Framework for Real Time Motion Completion",
    "authors": [
      "Yinglin Duan",
      "Yue Lin",
      "Zhengxia Zou",
      "Yi Yuan",
      "Zhehui Qian",
      "Bohan Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20368",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20368/20127",
    "published": "2022-02",
    "summary": "Motion completion, as a challenging and fundamental problem, is of great significance in film and game applications. For different motion completion application scenarios (in-betweening, in-filling, and blending), most previous methods deal with the completion problems with case-by-case methodology designs. In this work, we propose a simple but effective method to solve multiple motion completion problems under a unified framework and achieves a new state-of-the-art accuracy on LaFAN1 (+17% better than previous sota) under multiple evaluation settings. Inspired by the recent great success of self-attention-based transformer models, we consider the completion as a sequence-to-sequence prediction problem. Our method consists of three modules - a standard transformer encoder with self-attention that learns long-range dependencies of input motions, a trainable mixture embedding module that models temporal information and encodes different key-frame combinations in a unified form, and a new motion perceptual loss for better capturing high-frequency movements. Our method can predict multiple missing frames within a single forward propagation in real-time and get rid of the post-processing requirement. We also introduce a novel large-scale dance movement dataset for exploring the scaling capability of our method and its effectiveness in complex motion applications.",
    "code_link": ""
  },
  "aaai2022_main_factorvaeaprobabilisticdynamicfactormodelbasedonvariationalautoencoderforpredictingcross-sectionalstockreturns": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FactorVAE: A Probabilistic Dynamic Factor Model Based on Variational Autoencoder for Predicting Cross-Sectional Stock Returns",
    "authors": [
      "Yitong Duan",
      "Lei Wang",
      "Qizhong Zhang",
      "Jian Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20369",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20369/20128",
    "published": "2022-02",
    "summary": "As an asset pricing model in economics and finance, factor model has been widely used in quantitative investment. Towards building more effective factor models, recent years have witnessed the paradigm shift from linear models to more flexible nonlinear data-driven machine learning models. However, due to low signal-to-noise ratio of the financial data, it is quite challenging to learn effective factor models. In this paper, we propose a novel factor model, FactorVAE, as a probabilistic model with inherent randomness for noise modeling. Essentially, our model integrates the dynamic factor model (DFM) with the variational autoencoder (VAE) in machine learning, and we propose a prior-posterior learning method based on VAE, which can effectively guide the learning of model by approximating an optimal posterior factor model with future information. Particularly, considering that risk modeling is important for the noisy stock data, FactorVAE can estimate the variances from the distribution over the latent space of VAE, in addition to predicting returns. The experiments on the real stock market data demonstrate the effectiveness of FactorVAE, which outperforms various baseline methods.",
    "code_link": "https://github.com/microsoft/qlib"
  },
  "aaai2022_main_axm-netimplicitcross-modalfeaturealignmentforpersonre-identification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification",
    "authors": [
      "Ammarah Farooq",
      "Muhammad Awais",
      "Josef Kittler",
      "Syed Safwan Khalid"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20370",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20370/20129",
    "published": "2022-02",
    "summary": "Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align cross-modality representations conforming to semantic information present for a person and ignore background information. This work presents a novel convolutional neural network (CNN) based architecture designed to learn semantically aligned cross-modal visual and textual representations. The underlying building block, named AXM-Block, is a unified multi-layer network that dynamically exploits the multi-scale knowledge from both modalities and re-calibrates each modality according to shared semantics. To complement the convolutional design, contextual attention is applied in the text branch to manipulate long-term dependencies. Moreover, we propose a unique design to enhance visual part-based feature coherence and locality information. Our framework is novel in its ability to implicitly learn aligned semantics between modalities during the feature learning stage. The unified feature learning effectively utilizes textual data as a super-annotation signal for visual representation learning and automatically rejects irrelevant information. The entire AXM-Net is trained end-to-end on CUHK-PEDES data. We report results on two tasks, person search and cross-modal Re-ID. The AXM-Net outperforms the current state-of-the-art (SOTA) methods and achieves 64.44% Rank@1 on the CUHK-PEDES test set. It also outperforms by >10% for cross-viewpoint text-to-image Re-ID scenarios on CrossRe-ID and CUHK-SYSU datasets.",
    "code_link": ""
  },
  "aaai2022_main_scir-netstructuredcolorimagerepresentationbased3dobjectdetectionnetworkfrompointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SCIR-Net: Structured Color Image Representation Based 3D Object Detection Network from Point Clouds",
    "authors": [
      "Qingdong He",
      "Hao Zeng",
      "Yi Zeng",
      "Yijun Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20371",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20371/20130",
    "published": "2022-02",
    "summary": "3D object detection from point clouds data has become an indispensable part in autonomous driving. Previous works for processing point clouds lie in either projection or voxelization. However, projection-based methods suffer from information loss while voxelization-based methods bring huge computation. In this paper, we propose to encode point clouds into structured color image representation (SCIR) and utilize 2D CNN to fulfill the 3D detection task. Specifically, we use the structured color image encoding module to convert the irregular 3D point clouds into a squared 2D tensor image, where each point corresponds to a spatial point in the 3D space. Furthermore, in order to fit for the Euclidean structure, we apply feature normalization to parameterize the 2D tensor image onto a regular dense color image. Then, we conduct repeated multi-scale fusion with different levels so as to augment the initial features and learn scale-aware feature representations for box prediction. Extensive experiments on KITTI benchmark, Waymo Open Dataset and more challenging nuScenes dataset show that our proposed method yields decent results and demonstrate the effectiveness of such representations for point clouds.",
    "code_link": ""
  },
  "aaai2022_main_learninganddynamicalmodelsforsub-seasonalclimateforecastingcomparisonandcollaboration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning and Dynamical Models for Sub-seasonal Climate Forecasting: Comparison and Collaboration",
    "authors": [
      "Sijie He",
      "Xinyan Li",
      "Laurie Trenary",
      "Benjamin A Cash",
      "Timothy DelSole",
      "Arindam Banerjee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20372",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20372/20131",
    "published": "2022-02",
    "summary": "Sub-seasonal forecasting (SSF) is the prediction of key climate variables such as temperature and precipitation on the 2-week to 2-month time horizon. Skillful SSF would have substantial societal value in areas such as agricultural productivity, hydrology and water resource management, and emergency planning for extreme events such as droughts and wildfires. Despite its societal importance, SSF has stayed a challenging problem compared to both short-term weather forecasting and long-term seasonal forecasting. Recent studies have shown the potential of machine learning (ML) models to advance SSF. In this paper, for the first time, we perform a fine-grained comparison of a suite of modern ML models with start-of-the-art physics-based dynamical models from the Subseasonal Experiment (SubX) project for SSF in the western contiguous United States. Additionally, we explore mechanisms to enhance the ML models by using forecasts from dynamical models. Empirical results illustrate that, on average, ML models outperform dynamical models while the ML models tend to generate forecasts with conservative magnitude compared to the SubX models. Further, we illustrate that ML models make forecasting errors under extreme weather conditions, e.g., cold waves due to the polar vortex, highlighting the need for separate models for extreme events. Finally, we show that suitably incorporating dynamical model forecasts as inputs to ML models can substantially improve the forecasting performance of the ML models. The SSF dataset constructed for the work and code for the ML models are released along with the paper for the benefit of the artificial intelligence community.",
    "code_link": "https://github.com/Sijie-umn/SSF-MIP"
  },
  "aaai2022_main_solvingpde-constrainedcontrolproblemsusingoperatorlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Solving PDE-Constrained Control Problems Using Operator Learning",
    "authors": [
      "Rakhoon Hwang",
      "Jae Yong Lee",
      "Jin Young Shin",
      "Hyung Ju Hwang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20373",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20373/20132",
    "published": "2022-02",
    "summary": "The modeling and control of complex physical systems are essential in real-world problems. We propose a novel framework that is generally applicable to solving PDE-constrained optimal control problems by introducing surrogate models for PDE solution operators with special regularizers. The procedure of the proposed framework is divided into two phases: solution operator learning for PDE constraints (Phase 1) and searching for optimal control (Phase 2). Once the surrogate model is trained in Phase 1, the optimal control can be inferred in Phase 2 without intensive computations. Our framework can be applied to both data-driven and data-free cases. We demonstrate the successful application of our method to various optimal control problems for different control variables with diverse PDE constraints from the Poisson equation to Burgers' equation.",
    "code_link": ""
  },
  "aaai2022_main_proxylearningofvisualconceptsoffineartpaintingsfromstylesthroughlanguagemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Proxy Learning of Visual Concepts of Fine Art Paintings from Styles through Language Models",
    "authors": [
      "Diana Kim",
      "Ahmed Elgammal",
      "Marian Mazzone"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20374",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20374/20133",
    "published": "2022-02",
    "summary": "We present a machine learning system that can quantify fine art paintings with a set of visual elements and principles of art. The formal analysis is fundamental for understanding art, but developing such a system is challenging. Paintings have high visual complexities, but it is also difficult to collect enough training data with direct labels. To resolve these practical limitations, we introduce a novel mechanism, called proxy learning, which learns visual concepts in paintings through their general relation to styles. This framework does not require any visual annotation, but only uses style labels and a general relationship between visual concepts and style. In this paper, we propose a novel proxy model and reformulate four pre-existing methods in the context of proxy learning.Through quantitative and qualitative comparison, we evaluate these methods and compare their effectiveness in quantifying the artistic visual concepts, where the general relationship is estimated by language models; GloVe or BERT. The language modeling is a practical and scalable solution requiring no labeling, but it is inevitably imperfect. We demonstrate how the new proxy model is robust to the imperfection, while the other methods are sensitively affected by it.",
    "code_link": ""
  },
  "aaai2022_main_spate-ganimprovedgenerativemodelingofdynamicspatio-temporalpatternswithanautoregressiveembeddingloss": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss",
    "authors": [
      "Konstantin Klemmer",
      "Tianlin Xu",
      "Beatrice Acciaio",
      "Daniel B. Neill"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20375",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20375/20134",
    "published": "2022-02",
    "summary": "From ecology to atmospheric sciences, many academic disciplines deal with data characterized by intricate spatio-temporal complexities, the modeling of which often requires specialized approaches. Generative models of these data are of particular interest, as they enable a range of impactful downstream applications like simulation or creating synthetic training data. Recently, COT-GAN, a new GAN algorithm inspired by the theory of causal optimal transport (COT), was proposed in an attempt to improve generation of sequential data.However, the task of learning complex patterns over time and space requires additional knowledge of the specific data structures. In this study, we propose a novel loss objective combined with COT-GAN based on an autoregressive embedding to reinforce the learning of spatio-temporal dynamics. We devise SPATE (spatio-temporal association), a new metric measuring spatio-temporal autocorrelation. We compute SPATE for real and synthetic data samples and use it to compute an embedding loss that considers space-time interactions, nudging the GAN to learn outputs that are faithful to the observed dynamics. We test our new SPATE-GAN on a diverse set of spatio-temporal patterns: turbulent flows, log-Gaussian Cox processes and global weather data. We show that our novel embedding loss improves performance without any changes to the architecture of the GAN backbone, highlighting our model's increased capacity for capturing autoregressive structures.",
    "code_link": ""
  },
  "aaai2022_main_intra-intersubjectself-supervisedlearningformultivariatecardiacsignals": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Intra-Inter Subject Self-Supervised Learning for Multivariate Cardiac Signals",
    "authors": [
      "Xiang Lan",
      "Dianwen Ng",
      "Shenda Hong",
      "Mengling Feng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20376",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20376/20135",
    "published": "2022-02",
    "summary": "Learning information-rich and generalizable representations effectively from unlabeled multivariate cardiac signals to identify abnormal heart rhythms (cardiac arrhythmias) is valuable in real-world clinical settings but often challenging due to its complex temporal dynamics. Cardiac arrhythmias can vary significantly in temporal patterns even for the same patient (i.e., intra subject difference). Meanwhile, the same type of cardiac arrhythmia can show different temporal patterns among different patients due to different cardiac structures (i.e., inter subject difference). In this paper, we address the challenges by proposing an Intra-Inter Subject Self-Supervised Learning (ISL) model that is customized for multivariate cardiac signals. Our proposed ISL model integrates medical knowledge into self-supervision to effectively learn from intra-inter subject differences. In intra subject self-supervision, ISL model first extracts heartbeat-level features from each subject using a channel-wise attentional CNN-RNN encoder. Then a stationarity test module is employed to capture the temporal dependencies between heartbeats. In inter subject self-supervision, we design a set of data augmentations according to the clinical characteristics of cardiac signals and perform contrastive learning among subjects to learn distinctive representations for various types of patients. Extensive experiments on three real-world datasets were conducted. In a semi-supervised transfer learning scenario, our pre-trained ISL model leads about 10% improvement over supervised training when only 1% labeled data is available, suggesting strong generalizability and robustness of the model.",
    "code_link": ""
  },
  "aaai2022_main_geomgclgeometricgraphcontrastivelearningformolecularpropertyprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GeomGCL: Geometric Graph Contrastive Learning for Molecular Property Prediction",
    "authors": [
      "Shuangli Li",
      "Jingbo Zhou",
      "Tong Xu",
      "Dejing Dou",
      "Hui Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20377",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20377/20136",
    "published": "2022-02",
    "summary": "Recently many efforts have been devoted to applying graph neural networks (GNNs) to molecular property prediction which is a fundamental task for computational drug and material discovery. One of major obstacles to hinder the successful prediction of molecular property by GNNs is the scarcity of labeled data. Though graph contrastive learning (GCL) methods have achieved extraordinary performance with insufficient labeled data, most focused on designing data augmentation schemes for general graphs. However, the fundamental property of a molecule could be altered with the augmentation method (like random perturbation) on molecular graphs. Whereas, the critical geometric information of molecules remains rarely explored under the current GNN and GCL architectures. To this end, we propose a novel graph contrastive learning method utilizing the geometry of the molecule across 2D and 3D views, which is named GeomGCL. Specifically, we first devise a dual-view geometric message passing network (GeomMPNN) to adaptively leverage the rich information of both 2D and 3D graphs of a molecule. The incorporation of geometric properties at different levels can greatly facilitate the molecular representation learning. Then a novel geometric graph contrastive scheme is designed to make both geometric views collaboratively supervise each other to improve the generalization ability of GeomMPNN. We evaluate GeomGCL on various downstream property prediction tasks via a finetune process. Experimental results on seven real-life molecular datasets demonstrate the effectiveness of our proposed GeomGCL against state-of-the-art baselines.",
    "code_link": ""
  },
  "aaai2022_main_oamanoption-actionreinforcementlearningframeworkforuniversalmulti-intersectioncontrol": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "OAM: An Option-Action Reinforcement Learning Framework for Universal Multi-Intersection Control",
    "authors": [
      "Enming Liang",
      "Zicheng Su",
      "Chilin Fang",
      "Renxin Zhong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20378",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20378/20137",
    "published": "2022-02",
    "summary": "Efficient traffic signal control is an important means to alleviate urban traffic congestion. Reinforcement learning (RL) has shown great potentials in devising optimal signal plans that can adapt to dynamic traffic congestion. However, several challenges still need to be overcome. Firstly, a paradigm of state, action, and reward design is needed, especially for an optimality-guaranteed reward function. Secondly, the generalization of the RL algorithms is hindered by the varied topologies and physical properties of intersections. Lastly, enhancing the cooperation between intersections is needed for large network applications. To address these issues, the Option-Action RL framework for universal Multi-intersection control (OAM) is proposed. Based on the well-known cell transmission model, we first define a lane-cell-level state to better model the traffic flow propagation. Based on this physical queuing dynamics, we propose a regularized delay as the reward to facilitate temporal credit assignment while maintaining the equivalence with minimizing the average travel time. We then recapitulate the phase actions as the constrained combinations of lane options and design a universal neural network structure to realize model generalization to any intersection with any phase definition. The multiple-intersection cooperation is then rigorously discussed using the potential game theory.We test the OAM algorithm under four networks with different settings, including a city-level scenario with 2,048 intersections using synthetic and real-world datasets. The results show that the OAM can outperform the state-of-the-art controllers in reducing the average travel time.",
    "code_link": ""
  },
  "aaai2022_main_end-to-endlinedrawingvectorization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "End-to-End Line Drawing Vectorization",
    "authors": [
      "Hanyuan Liu",
      "Chengze Li",
      "Xueting Liu",
      "Tien-Tsin Wong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20379",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20379/20138",
    "published": "2022-02",
    "summary": "Vector graphics is broadly used in a variety of forms, such as illustrations, logos, posters, billboards, and printed ads. Despite its broad use, many artists still prefer to draw with pen and paper, which leads to a high demand of converting raster designs into the vector form. In particular, line drawing is a primary art and attracts many research efforts in automatically converting raster line drawings to vector form. However, the existing methods generally adopt a two-step approach, stroke segmentation and vectorization. Without vector guidance, the raster-based stroke segmentation frequently obtains unsatisfying segmentation results, such as over-grouped strokes and broken strokes. In this paper, we make an attempt in proposing an end-to-end vectorization method which directly generates vectorized stroke primitives from raster line drawing in one step. We propose a Transformer-based framework to perform stroke tracing like human does in an automatic stroke-by-stroke way with a novel stroke feature representation and multi-modal supervision to achieve vectorization with high quality and fidelity. Qualitative and quantitative evaluations show that our method achieves state of the art performance.",
    "code_link": ""
  },
  "aaai2022_main_context-awarehealtheventpredictionviatransitionfunctionsondynamicdiseasegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Context-Aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs",
    "authors": [
      "Chang Lu",
      "Tian Han",
      "Yue Ning"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20380",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20380/20139",
    "published": "2022-02",
    "summary": "With the wide application of electronic health records (EHR) in healthcare facilities, health event prediction with deep learning has gained more and more attention. A common feature of EHR data used for deep-learning-based predictions is historical diagnoses. Existing work mainly regards a diagnosis as an independent disease and does not consider clinical relations among diseases in a visit. Many machine learning approaches assume disease representations are static in different visits of a patient. However, in real practice, multiple diseases that are frequently diagnosed at the same time reflect hidden patterns that are conducive to prognosis. Moreover, the development of a disease is not static since some diseases can emerge or disappear and show various symptoms in different visits of a patient. To effectively utilize this combinational disease information and explore the dynamics of diseases, we propose a novel context-aware learning framework using transition functions on dynamic disease graphs. Specifically, we construct a global disease co-occurrence graph with multiple node properties for disease combinations. We design dynamic subgraphs for each patient's visit to leverage global and local contexts. We further define three diagnosis roles in each visit based on the variation of node properties to model disease transition processes. Experimental results on two real-world EHR datasets show that the proposed model outperforms state of the art in predicting health events.",
    "code_link": "https://github.com/LuChang-CS/Chet"
  },
  "aaai2022_main_hyperverletasymplectichypersolverforhamiltoniansystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hyperverlet: A Symplectic Hypersolver for Hamiltonian Systems",
    "authors": [
      "Frederik Baymler Mathiesen",
      "Bin Yang",
      "Jilin Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20381",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20381/20140",
    "published": "2022-02",
    "summary": "Hamiltonian systems represent an important class of dynamical systems such as pendulums, molecular dynamics, and cosmic systems. The choice of solvers is significant to the accuracy when simulating Hamiltonian systems, where symplectic solvers show great significance. Recent advances in neural network-based hypersolvers, though achieve competitive results, still lack the symplecity necessary for reliable simulations, especially over long time horizons. To alleviate this, we introduce Hyperverlet, a new hypersolver composing the traditional, symplectic velocity Verlet and symplectic neural network-based solvers. More specifically, we propose a parameterization of symplectic neural networks and prove that hyperbolic tangent is r-finite expanding the set of allowable activation functions for symplectic neural networks, improving the accuracy. Extensive experiments on a spring-mass and a pendulum system justify the design choices and suggest that Hyperverlet outperforms both traditional solvers and hypersolvers.",
    "code_link": "https://github.com/Zinoex/hyperverlet"
  },
  "aaai2022_main_learninghumandrivingbehaviorswithsequentialcausalimitationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Human Driving Behaviors with Sequential Causal Imitation Learning",
    "authors": [
      "Kangrui Ruan",
      "Xuan Di"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20382",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20382/20141",
    "published": "2022-02",
    "summary": "Learning human driving behaviors is an efficient approach for self-driving vehicles. Traditional Imitation Learning (IL) methods assume that the expert demonstrations follow Markov Decision Processes (MDPs). However, in reality, this assumption does not always hold true. Spurious correlation may exist through the paths of historical variables because of the existence of unobserved confounders. Accounting for the latent causal relationships from unobserved variables to outcomes, this paper proposes Sequential Causal Imitation Learning (SeqCIL) for imitating driver behaviors. We develop a sequential causal template that generalizes the default MDP settings to one with Unobserved Confounders (MDPUC-HD). Then we develop a sufficient graphical criterion to determine when ignoring causality leads to poor performances in MDPUC-HD. Through the framework of Adversarial Imitation Learning, we develop a procedure to imitate the expert policy by blocking \u03c0-backdoor paths at each time step. Our methods are evaluated on a synthetic dataset and a real-world highway driving dataset, both demonstrating that the proposed procedure significantly outperforms non-causal imitation learning methods.",
    "code_link": ""
  },
  "aaai2022_main_emvlightadecentralizedreinforcementlearningframeworkforefficientpassageofemergencyvehicles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "EMVLight: A Decentralized Reinforcement Learning Framework for Efficient Passage of Emergency Vehicles",
    "authors": [
      "Haoran Su",
      "Yaofeng Desmond Zhong",
      "Biswadip Dey",
      "Amit Chakraborty"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20383",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20383/20142",
    "published": "2022-02",
    "summary": "Emergency vehicles (EMVs) play a crucial role in responding to time-critical events such as medical emergencies and fire outbreaks in an urban area. The less time EMVs spend traveling through the traffic, the more likely it would help save people's lives and reduce property loss. To reduce the travel time of EMVs, prior work has used route optimization based on historical traffic-flow data and traffic signal pre-emption based on the optimal route. However, traffic signal pre-emption dynamically changes the traffic flow which, in turn, modifies the optimal route of an EMV. In addition, traffic signal pre-emption practices usually lead to significant disturbances in traffic flow and subsequently increase the travel time for non-EMVs. In this paper, we propose EMVLight, a decentralized reinforcement learning (RL) framework for simultaneous dynamic routing and traffic signal control. EMVLight extends Dijkstra's algorithm to efficiently update the optimal route for the EMVs in real-time as it travels through the traffic network. The decentralized RL agents learn network-level cooperative traffic signal phase strategies that not only reduce EMV travel time but also reduce the average travel time of non-EMVs in the network. This benefit has been demonstrated through comprehensive experiments with synthetic and real-world maps. These experiments show that EMVLight outperforms benchmark transportation engineering techniques and existing RL-based signal control methods.",
    "code_link": ""
  },
  "aaai2022_main_constrainedprescriptivetreesviacolumngeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Constrained Prescriptive Trees via Column Generation",
    "authors": [
      "Shivaram Subramanian",
      "Wei Sun",
      "Youssef Drissi",
      "Markus Ettl"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20384",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20384/20143",
    "published": "2022-02",
    "summary": "With the abundance of available data, many enterprises seek to implement data-driven prescriptive analytics to help them make informed decisions. These prescriptive policies need to satisfy operational constraints, and proactively eliminate rule conflicts, both of which are ubiquitous in practice. It is also desirable for them to be simple and interpretable, so they can be easily verified and implemented. Existing approaches from the literature center around constructing variants of prescriptive decision trees to generate interpretable policies. However, none of the existing methods is able to handle constraints. In this paper, we propose a scalable method that solves the constrained prescriptive policy generation problem. We introduce a novel path-based mixed-integer program (MIP) formulation which identifies a (near) optimal policy efficiently via column generation. The policy generated can be represented as a multiway-split tree which is more interpretable and informative than binary-split trees due to its shorter rules. We demonstrate the efficacy of our method with extensive computational experiments on both synthetic and real datasets.",
    "code_link": ""
  },
  "aaai2022_main_ddgcndualdynamicgraphconvolutionalnetworksforrumordetectiononsocialmedia": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DDGCN: Dual Dynamic Graph Convolutional Networks for Rumor Detection on Social Media",
    "authors": [
      "Mengzhu Sun",
      "Xi Zhang",
      "Jiaqi Zheng",
      "Guixiang Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20385",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20385/20144",
    "published": "2022-02",
    "summary": "Detecting rumors on social media has become particular important due to the rapid dissemination and adverse impacts on our lives. Though a set of rumor detection models have exploited the message propagation structural or temporal information, they seldom model them altogether to enjoy the best of both worlds. Moreover, the dynamics of knowledge information associated with the comments are not involved, either. To this end, we propose a novel Dual-Dynamic Graph Convolutional Networks, termed as DDGCN, which can model the dynamics of messages in propagation as well as the dynamics of the background knowledge from Knowledge graphs in one unified framework. Specifically, two Graph Convolutional Networks are adopted to capture the above two types of structure information at different time stages, which are then combined with a temporal fusing unit. This allows for learning the dynamic event representations in a more fine-grained manner, and incrementally aggregating them to capture the cascading effect for better rumor detection. Extensive experiments on two public real-world datasets demonstrate that our proposal yields significant improvements compared to strong baselines and can detect rumors at early stages.",
    "code_link": ""
  },
  "aaai2022_main_contact-distilboostinglowhomologousproteincontactmappredictionbyself-superviseddistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contact-Distil: Boosting Low Homologous Protein Contact Map Prediction by Self-Supervised Distillation",
    "authors": [
      "Qin Wang",
      "Jiayang Chen",
      "Yuzhe Zhou",
      "Yu Li",
      "Liangzhen Zheng",
      "Sheng Wang",
      "Zhen Li",
      "Shuguang Cui"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20386",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20386/20145",
    "published": "2022-02",
    "summary": "Accurate protein contact map prediction (PCMP) is essential for precise protein structure estimation and further biological studies. Recent works achieve significant performance on this task with high quality multiple sequence alignment (MSA). However, the PCMP accuracy drops dramatically while only poor MSA (e.g., absolute MSA count less than 10) is available. Therefore, in this paper, we propose the Contact-Distil to improve the low homologous PCMP accuracy through knowledge distillation on a self-supervised model. Particularly, two pre-trained transformers are exploited to learn the high quality and low quality MSA representation in parallel for the teacher and student model correspondingly. Besides, the co-evolution information is further extracted from pure sequence through a pretrained ESM-1b model, which provides auxiliary knowledge to improve student performance. Extensive experiments show Contact-Distil outperforms previous state-of-the-arts by large margins on CAMEO-L dataset for low homologous PCMP, i.e., around 13.3% and 9.5% improvements against Alphafold2 and MSA Transformer respectively when MSA count less than 10.",
    "code_link": ""
  },
  "aaai2022_main_etinynetextremelytinynetworkfortinyml": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "EtinyNet: Extremely Tiny Network for TinyML",
    "authors": [
      "Kunran Xu",
      "Yishi Li",
      "Huawei Zhang",
      "Rui Lai",
      "Lin Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20387",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20387/20146",
    "published": "2022-02",
    "summary": "There are many AI applications in high-income countries because their implementation depends on expensive GPU cards (~2000$) and reliable power supply (~200W). To deploy AI in resource-poor settings on cheaper (~20$) and low-power devices (<1W), key modifications are required to adapt neural networks for Tiny machine learning (TinyML). In this paper, for putting CNNs into storage limited devices, we developed efficient tiny models with only hundreds of KB parameters. Toward this end, we firstly design a parameter-efficient tiny architecture by introducing dense linear depthwise block. Then, a novel adaptive scale quantization (ASQ) method is proposed for further quantizing tiny models in aggressive low-bit while retaining the accuracy. With the optimized architecture and 4-bit ASQ, we present a family of ultralightweight networks, named EtinyNet, that achieves 57.0% ImageNet top-1 accuracy with an extremely tiny model size of 340KB. When deployed on an off-the-shelf commercial microcontroller for object detection tasks, EtinyNet achieves state-of-the-art 56.4% mAP on Pascal VOC. Furthermore, the experimental results on Xilinx compact FPGA indicate that EtinyNet achieves prominent low power of 620mW, about 5.6x lower than existing FPGA designs. The code and demo are in https://github.com/aztc/EtinyNet",
    "code_link": "https://github.com/aztc/EtinyNet"
  },
  "aaai2022_main_repbinconstraint-basedgraphrepresentationlearningformetagenomicbinning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "RepBin: Constraint-Based Graph Representation Learning for Metagenomic Binning",
    "authors": [
      "Hansheng Xue",
      "Vijini Mallawaarachchi",
      "Yujia Zhang",
      "Vaibhav Rajan",
      "Yu Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20388",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20388/20147",
    "published": "2022-02",
    "summary": "Mixed communities of organisms are found in many environments -- from the human gut to marine ecosystems -- and can have profound impact on human health and the environment. Metagenomics studies the genomic material of such communities through high-throughput sequencing that yields DNA subsequences for subsequent analysis. A fundamental problem in the standard workflow, called binning, is to discover clusters, of genomic subsequences, associated with the constituent organisms. Inherent noise in the subsequences, various biological constraints that need to be imposed on them and the skewed cluster size distribution exacerbate the difficulty of this unsupervised learning problem. In this paper, we present a new formulation using a graph where the nodes are subsequences and edges represent homophily information. In addition, we model biological constraints providing heterophilous signal about nodes that cannot be clustered together. We solve the binning problem by developing new algorithms for (i) graph representation learning that preserves both homophily relations and heterophily constraints (ii) constraint-based graph clustering method that addresses the problems of skewed cluster size distribution. Extensive experiments, on real and synthetic datasets, demonstrate that our approach, called RepBin, outperforms a wide variety of competing methods. Our constraint-based graph representation learning and clustering methods, that may be useful in other domains as well, advance the state-of-the-art in both metagenomics binning and graph representation learning.",
    "code_link": "https://github.com/xuehansheng/RepBin"
  },
  "aaai2022_main_nsgzeroefficientlylearningnon-exploitablepolicyinlarge-scalenetworksecuritygameswithneuralmontecarlotreesearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NSGZero: Efficiently Learning Non-exploitable Policy in Large-Scale Network Security Games with Neural Monte Carlo Tree Search",
    "authors": [
      "Wanqi Xue",
      "Bo An",
      "Chai Kiat Yeo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20389",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20389/20148",
    "published": "2022-02",
    "summary": "How resources are deployed to secure critical targets in networks can be modelled by Network Security Games (NSGs). While recent advances in deep learning (DL) provide a powerful approach to dealing with large-scale NSGs, DL methods such as NSG-NFSP suffer from the problem of data inefficiency. Furthermore, due to centralized control, they cannot scale to scenarios with a large number of resources. In this paper, we propose a novel DL-based method, NSGZero, to learn a non-exploitable policy in NSGs. NSGZero improves data efficiency by performing planning with neural Monte Carlo Tree Search (MCTS). Our main contributions are threefold. First, we design deep neural networks (DNNs) to perform neural MCTS in NSGs. Second, we enable neural MCTS with decentralized control, making NSGZero applicable to NSGs with many resources. Third, we provide an efficient learning paradigm, to achieve joint training of the DNNs in NSGZero. Compared to state-of-the-art algorithms, our method achieves significantly better data efficiency and scalability.",
    "code_link": ""
  },
  "aaai2022_main_rid-noisetowardsrobustinversedesignundernoisyenvironments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "RID-Noise: Towards Robust Inverse Design under Noisy Environments",
    "authors": [
      "Jia-Qi Yang",
      "Ke-Bin Fan",
      "Hao Ma",
      "De-Chuan Zhan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20390",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20390/20149",
    "published": "2022-02",
    "summary": "From an engineering perspective, a design should not only perform well in an ideal condition, but should also resist noises. Such a design methodology, namely robust design, has been widely implemented in the industry for product quality control. However, classic robust design requires a lot of evaluations for a single design target, while the results of these evaluations could not be reused for a new target. To achieve data-efficient robust design, we propose Robust Inverse Design under Noise (RID-Noise), which can utilize existing data to train a conditional invertible neural network. Specifically, we estimate the robustness of a design parameter by its predictability, measured by the prediction error of a forward neural network. We also define a sample-wise weight, which can be used in the maximum weighted likelihood estimation of an inverse model based on a conditional invertible neural network. With the visual results from experiments, we clearly justify how RID-Noise works by learning the distribution and robustness from data. Further experiments on several real-world benchmark tasks with noises confirm that our method is more effective than other state-of-the-art inverse design methods. Code and supplementary is publicly available at https://github.com/ThyrixYang/rid-noise-aaai22",
    "code_link": "https://github.com/ThyrixYang/rid-noise-aaai22"
  },
  "aaai2022_main_deepfakenetworkarchitectureattribution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deepfake Network Architecture Attribution",
    "authors": [
      "Tianyun Yang",
      "Ziyao Huang",
      "Juan Cao",
      "Lei Li",
      "Xirong Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20391",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20391/20150",
    "published": "2022-02",
    "summary": "With the rapid progress of generation technology, it has become necessary to attribute the origin of fake images. Existing works on fake image attribution perform multi-class classification on several Generative Adversarial Network (GAN) models and obtain high accuracies. While encouraging, these works are restricted to model-level attribution, only capable of handling images generated by seen models with a specific seed, loss and dataset, which is limited in real-world scenarios when fake images may be generated by privately trained models. This motivates us to ask whether it is possible to attribute fake images to the source models' architectures even if they are finetuned or retrained under different configurations. In this work, we present the first study on Deepfake Network Architecture Attribution to attribute fake images on architecture-level. Based on an observation that GAN architecture is likely to leave globally consistent fingerprints while traces left by model weights vary in different regions, we provide a simple yet effective solution named by DNA-Det for this problem. Extensive experiments on multiple cross-test setups and a large-scale dataset demonstrate the effectiveness of DNA-Det.",
    "code_link": ""
  },
  "aaai2022_main_zinb-basedgraphembeddingautoencoderforsingle-cellrna-seqinterpretations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ZINB-Based Graph Embedding Autoencoder for Single-Cell RNA-Seq Interpretations",
    "authors": [
      "Zhuohan Yu",
      "Yifu Lu",
      "Yunhe Wang",
      "Fan Tang",
      "Ka-Chun Wong",
      "Xiangtao Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20392",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20392/20151",
    "published": "2022-02",
    "summary": "Single-cell RNA sequencing (scRNA-seq) provides high-throughput information about the genome-wide gene expression levels at the single-cell resolution, bringing a precise understanding on the transcriptome of individual cells. Unfortunately, the rapidly growing scRNA-seq data and the prevalence of dropout events pose substantial challenges for cell type annotation. Here, we propose a single-cell model-based deep graph embedding clustering (scTAG) method, which simultaneously learns cell\u2013cell topology representations and identifies cell clusters based on deep graph convolutional network. scTAG integrates the zero-inflated negative binomial (ZINB) model into a topology adaptive graph convolutional autoencoder to learn the low-dimensional latent representation and adopts Kullback\u2013Leibler (KL) divergence for the clustering tasks. By simultaneously optimizing the clustering loss, ZINB loss, and the cell graph reconstruction loss, scTAG jointly optimizes cluster label assignment and feature learning with the topological structures preserved in an end-to-end manner. Extensive experiments on 16 single-cell RNA-seq datasets from diverse yet representative single-cell sequencing platforms demonstrate the superiority of scTAG over various state-of-the-art clustering methods.",
    "code_link": ""
  },
  "aaai2022_main_deepthermalcombustionoptimizationforthermalpowergeneratingunitsusingofflinereinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning",
    "authors": [
      "Xianyuan Zhan",
      "Haoran Xu",
      "Yue Zhang",
      "Xiangyu Zhu",
      "Honglei Yin",
      "Yu Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20393",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20393/20152",
    "published": "2022-02",
    "summary": "Optimizing the combustion efficiency of a thermal power generating unit (TPGU) is a highly challenging and critical task in the energy industry. We develop a new data-driven AI system, namely DeepThermal, to optimize the combustion control strategy for TPGUs. At its core, is a new model-based offline reinforcement learning (RL) framework, called MORE, which leverages historical operational data of a TGPU to solve a highly complex constrained Markov decision process problem via purely offline training. In DeepThermal, we first learn a data-driven combustion process simulator from the offline dataset. The RL agent of MORE is then trained by combining real historical data as well as carefully filtered and processed simulation data through a novel restrictive exploration scheme. DeepThermal has been successfully deployed in four large coal-fired thermal power plants in China. Real-world experiments show that DeepThermal effectively improves the combustion efficiency of TPGUs. We also report the superior performance of MORE by comparing with the state-of-the-art algorithms on the standard offline RL benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_alphaholdemhigh-performanceartificialintelligenceforheads-upno-limitpokerviaend-to-endreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AlphaHoldem: High-Performance Artificial Intelligence for Heads-Up No-Limit Poker via End-to-End Reinforcement Learning",
    "authors": [
      "Enmin Zhao",
      "Renye Yan",
      "Jinqiu Li",
      "Kai Li",
      "Junliang Xing"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20394",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20394/20153",
    "published": "2022-02",
    "summary": "Heads-up no-limit Texas hold\u2019em (HUNL) is the quintessential game with imperfect information. Representative priorworks like DeepStack and Libratus heavily rely on counter-factual regret minimization (CFR) and its variants to tackleHUNL. However, the prohibitive computation cost of CFRiteration makes it difficult for subsequent researchers to learnthe CFR model in HUNL and apply it in other practical applications. In this work, we present AlphaHoldem, a high-performance and lightweight HUNL AI obtained with an end-to-end self-play reinforcement learning framework. The proposed framework adopts a pseudo-siamese architecture to directly learn from the input state information to the output actions by competing the learned model with its different historical versions. The main technical contributions include anovel state representation of card and betting information, amultitask self-play training loss function, and a new modelevaluation and selection metric to generate the final model.In a study involving 100,000 hands of poker, AlphaHoldemdefeats Slumbot and DeepStack using only one PC with threedays training. At the same time, AlphaHoldem only takes 2.9milliseconds for each decision-making using only a singleGPU, more than 1,000 times faster than DeepStack. We release the history data among among AlphaHoldem, Slumbot,and top human professionals in the author\u2019s GitHub repository to facilitate further studies in this direction.",
    "code_link": "https://github.com/ZhaoEnMin/AlphaHoldem_Data"
  },
  "aaai2022_main_hierarchicalmulti-supervisionmulti-interactiongraphattentionnetworkformulti-camerapedestriantrajectoryprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hierarchical Multi-Supervision Multi-Interaction Graph Attention Network for Multi-Camera Pedestrian Trajectory Prediction",
    "authors": [
      "Guoliang Zhao",
      "Yuxun Zhou",
      "Zhanbo Xu",
      "Yadong Zhou",
      "Jiang Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20395",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20395/20154",
    "published": "2022-02",
    "summary": "Pedestrian trajectory prediction has become an essential underpinning in various human-centric applications including but not limited to autonomous vehicles, intelligent surveillance system and social robotics. Previous research endeavors mainly focus on single camera trajectory prediction (SCTP), while the problem of multi-camera trajectory prediction (MCTP) is often overly simplified into predicting presence in the next camera. This paper addresses MCTP from a more realistic yet challenging perspective, by redefining the task as a joint estimation of both future destination and possible trajectory. As such, two major efforts are devoted to facilitating related research and advancing modeling techniques. Firstly, we establish a comprehensive multi-camera Scenes Pedestrian Trajectory Dataset (mcScenes), which is collected from a real-world multi-camera space combined with thorough human interaction annotations and carefully designed evaluation metrics. Secondly, we propose a novel joint prediction framework, namely HM3GAT, for the MCTP task by building a tailored network architecture. The core idea behind HM3GAT is a fusion of topological and trajectory information that are mutually beneficial to the prediction of each task, achieved by deeply customized networks. The proposed framework is comprehensively evaluated on the mcScenes dataset with multiple ablation experiments. Status-of-the-art SCTP models are adopted as baselines to further validate the advantages of our method in terms of both information fusion and technical improvement. The mcScenes dataset, the HM3GAT, and alternative models are made publicly available for interested readers.",
    "code_link": ""
  },
  "aaai2022_main_6dcnnwithroto-translationalconvolutionfiltersforvolumetricdataprocessing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "6DCNN with Roto-Translational Convolution Filters for Volumetric Data Processing",
    "authors": [
      "Dmitrii Zhemchuzhnikov",
      "Ilia Igashov",
      "Sergei Grudinin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20396",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20396/20155",
    "published": "2022-02",
    "summary": "In this work, we introduce 6D Convolutional Neural Network (6DCNN) designed to tackle the problem of detecting relative positions and orientations of local patterns when processing three-dimensional volumetric data. 6DCNN also includes SE(3)-equivariant message-passing and nonlinear activation operations constructed in the Fourier space. Working in the Fourier space allows significantly reducing the computational complexity of our operations. We demonstrate the properties of the 6D convolution and its efficiency in the recognition of spatial patterns. We also assess the 6DCNN model on several datasets from the recent CASP protein structure prediction challenges. Here, 6DCNN improves over the baseline architecture and also outperforms the state of the art.",
    "code_link": ""
  },
  "aaai2022_main_deeplytensorcompressedtransformersforend-to-endobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deeply Tensor Compressed Transformers for End-to-End Object Detection",
    "authors": [
      "Peining Zhen",
      "Ziyang Gao",
      "Tianshu Hou",
      "Yuan Cheng",
      "Hai-Bao Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20397",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20397/20156",
    "published": "2022-02",
    "summary": "DEtection TRansformer (DETR) is a recently proposed method that streamlines the detection pipeline and achieves competitive results against two-stage detectors such as Faster-RCNN. The DETR models get rid of complex anchor generation and post-processing procedures thereby making the detection pipeline more intuitive. However, the numerous redundant parameters in transformers make the DETR models computation and storage intensive, which seriously hinder them to be deployed on the resources-constrained devices. In this paper, to obtain a compact end-to-end detection framework, we propose to deeply compress the transformers with low-rank tensor decomposition. The basic idea of the tensor-based compression is to represent the large-scale weight matrix in one network layer with a chain of low-order matrices. Furthermore, we propose a gated multi-head attention (GMHA) module to mitigate the accuracy drop of the tensor-compressed DETR models. In GMHA, each attention head has an independent gate to determine the passed attention value. The redundant attention information can be suppressed by adopting the normalized gates. Lastly, to obtain fully compressed DETR models, a low-bitwidth quantization technique is introduced for further reducing the model storage size. Based on the proposed methods, we can achieve significant parameter and model size reduction while maintaining high detection performance. We conduct extensive experiments on the COCO dataset to validate the effectiveness of our tensor-compressed (tensorized) DETR models. The experimental results show that we can attain 3.7 times full model compression with 482 times feed forward network (FFN) parameter reduction and only 0.6 points accuracy drop.",
    "code_link": ""
  },
  "aaai2022_main_dynamicmanifoldlearningforlanddeformationforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dynamic Manifold Learning for Land Deformation Forecasting",
    "authors": [
      "Fan Zhou",
      "Rongfan Li",
      "Qiang Gao",
      "Goce Trajcevski",
      "Kunpeng Zhang",
      "Ting\n      Zhong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20398",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20398/20157",
    "published": "2022-02",
    "summary": "Landslides refer to occurrences of massive ground movements due to geological (and meteorological) factors, and can have disastrous impact on property, economy, and even lead to loss of life. The advances of remote sensing provide accurate and continuous terrain monitoring, enabling the study and analysis of land deformation which, in turn, can be used for possible landslides forecast. Prior studies either rely on independent observations for displacement prediction or model static land characteristics without considering the subtle interactions between different locations and the dynamic changes of the surface conditions. We present DyLand -- Dynamic Manifold Learning with Normalizing Flows for Land deformation prediction -- a novel framework for learning dynamic structures of terrain surface and improving the performance of land deformation prediction. DyLand models the spatial connections of InSAR measurements and estimates conditional distributions of deformations on the terrain manifold with a novel normalizing flow-based method. Instead of modeling the stable terrains, it incorporates surface permutations and captures the innate dynamics of the land surface while allowing for tractable likelihood estimates on the manifold. Our extensive evaluations on curated InSAR datasets from continuous monitoring of slopes prone to landslides show that DyLand outperforms existing bechmarking models.",
    "code_link": ""
  },
  "aaai2022_main_fullyadaptiveframeworkneuralcomputerizedadaptivetestingforonlineeducation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fully Adaptive Framework: Neural Computerized Adaptive Testing for Online Education",
    "authors": [
      "Yan Zhuang",
      "Qi Liu",
      "Zhenya Huang",
      "Zhi Li",
      "Shuanghong Shen",
      "Haiping Ma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20399",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20399/20158",
    "published": "2022-02",
    "summary": "Computerized Adaptive Testing (CAT) refers to an efficient and personalized test mode in online education, aiming to accurately measure student proficiency level on the required subject/domain. The key component of CAT is the \"adaptive\" question selection algorithm, which automatically selects the best suited question for student based on his/her current estimated proficiency, reducing test length. Existing algorithms rely on some manually designed and pre-fixed informativeness/uncertainty metrics of question for selections, which is labor-intensive and not sufficient for capturing complex relations between students and questions. In this paper, we propose a fully adaptive framework named Neural Computerized Adaptive Testing (NCAT), which formally redefines CAT as a reinforcement learning problem and directly learns selection algorithm from real-world data. Specifically, a bilevel optimization is defined and simplified under CAT's application scenarios to make the algorithm learnable. Furthermore, to address the CAT task effectively, we tackle it as an equivalent reinforcement learning problem and propose an attentive neural policy to model complex non-linear interactions. Extensive experiments on real-world datasets demonstrate the effectiveness and robustness of NCAT compared with several state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_analgorithmicintroductiontosavingscircles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Algorithmic Introduction to Savings Circles",
    "authors": [
      "Rediet Abebe",
      "Adam Eck",
      "Christian Ikeokwu",
      "Sam Taggart"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20400",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20400/20159",
    "published": "2022-02",
    "summary": "Rotating savings and credit associations (roscas) are informal financial organizations common in settings where communities have reduced access to formal financial institutions. In a rosca, a fixed group of participants regularly contribute sums of money to a pot. This pot is then allocated periodically using lottery, aftermarket, or auction mechanisms. Roscas are empirically well-studied in economics. They are, however, challenging to study theoretically due to their dynamic nature. Typical economic analyses of roscas stop at coarse ordinal welfare comparisons to other credit allocation mechanisms, leaving much of roscas' ubiquity unexplained. In this work, we take an algorithmic perspective on the study of roscas. Building on techniques from the price of anarchy literature, we present worst-case welfare approximation guarantees. We further experimentally compare the welfare of outcomes as key features of the environment vary. These cardinal welfare analyses further rationalize the prevalence of roscas. We conclude by discussing several other promising avenues.",
    "code_link": ""
  },
  "aaai2022_main_locallyfairpartitioning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Locally Fair Partitioning",
    "authors": [
      "Pankaj K. Agarwal",
      "Shao-Heng Ko",
      "Kamesh Munagala",
      "Erin Taylor"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20401",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20401/20160",
    "published": "2022-02",
    "summary": "We model the societal task of redistricting political districts as a partitioning problem: Given a set of n points in the plane, each belonging to one of two parties, and a parameter k, our goal is to compute a partition P of the plane into regions so that each region contains roughlys = n/k points. P should satisfy a notion of\"local\" fairness, which is related to the notion of core, a well-studied concept in cooperative game theory. A region is associated with the majority party in that region, and a point is unhappy in P if it belongs to the minority party. A group D of roughly s contiguous points is called a deviating group with respect to P if majority of points in D are unhappy in P. The partition P is locally fair if there is no deviating group with respect to P.This paper focuses on a restricted case when points lie in 1D. The problem is non-trivial even in this case. We consider both adversarial and \"beyond worst-case\" settings for this problem. For the former, we characterize the input parameters for which a locally fair partition always exists; we also show that a locally fair partition may not exist for certain parameters. We then consider input models where there are \"runs\" of red and blue points. For such clustered inputs, we show that a locally fair partition may not exist for certain values of s, but an approximate locally fair partition exists if we allow some regions to have smaller sizes. We finally present a polynomial-time algorithm for computing a locally fair partition if one exists.",
    "code_link": ""
  },
  "aaai2022_main_maximizingnashsocialwelfarein2-valueinstances": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Maximizing Nash Social Welfare in 2-Value Instances",
    "authors": [
      "Hannaneh Akrami",
      "Bhaskar Ray Chaudhury",
      "Martin Hoefer",
      "Kurt Mehlhorn",
      "Marco Schmalhofer",
      "Golnoosh Shahkarami",
      "Giovanna Varricchio",
      "Quentin\n      Vermande",
      "Ernest van Wijland"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20402",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20402/20161",
    "published": "2022-02",
    "summary": "We consider the problem of maximizing the Nash social welfare when allocating a set G of indivisible goods to a set N of agents. We study instances, in which all agents have 2-value additive valuations: The value of every agent for every good is either p or q, where p and q are integers and p2.In terms of approximation, we present positive and negative results for general p and q. We show that our algorithm obtains an approximation ratio of at most 1.0345. Moreover, we prove that the problem is APX-hard, with a lower bound of 1.000015 achieved at p/q = 4/5.",
    "code_link": ""
  },
  "aaai2022_main_truth-trackingviaapprovalvotingsizematters": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Truth-Tracking via Approval Voting: Size Matters",
    "authors": [
      "Tahar Allouche",
      "J\u00e9r\u00f4me Lang",
      "Florian Yger"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20403",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20403/20162",
    "published": "2022-02",
    "summary": "Epistemic social choice aims at unveiling a hidden ground truth given votes, which are interpreted as noisy signals about it. We consider here a simple setting where votes consist of approval ballots: each voter approves a set of alternatives which they believe can possibly be the ground truth. Based on the intuitive idea that more reliable votes contain fewer alternatives, we define several noise models that are approval voting variants of the Mallows model. The likelihood-maximizing alternative is then characterized as the winner of a weighted approval rule, where the weight of a ballot decreases with its cardinality. We have conducted an experiment on three image annotation datasets; they conclude that rules based on our noise model outperform standard approval voting; the best performance is obtained by a variant of the Condorcet noise model.",
    "code_link": ""
  },
  "aaai2022_main_dimensionalityandcoordinationinvotingthedistortionofstv": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dimensionality and Coordination in Voting: The Distortion of STV",
    "authors": [
      "Ioannis Anagnostides",
      "Dimitris Fotakis",
      "Panagiotis Patsilinakos"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20404",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20404/20163",
    "published": "2022-02",
    "summary": "We study the performance of voting mechanisms from a utilitarian standpoint, under the recently introduced framework of metric-distortion, offering new insights along two main lines. First, if d represents the doubling dimension of the metric space, we show that the distortion of STV is O(d log log m), where m represents the number of candidates. For doubling metrics this implies an exponential improvement over the lower bound for general metrics, and as a special case it effectively answers a question left open by Skowron and Elkind (AAAI '17) regarding the distortion of STV under low-dimensional Euclidean spaces. More broadly, this constitutes the first nexus between the performance of any voting rule and the ``intrinsic dimensionality'' of the underlying metric space. We also establish a nearly-matching lower bound, refining the construction of Skowron and Elkind. Moreover, motivated by the efficiency of STV, we investigate whether natural learning rules can lead to low-distortion outcomes. Specifically, we introduce simple, deterministic and decentralized exploration/exploitation dynamics, and we show that they converge to a candidate with O(1) distortion.",
    "code_link": ""
  },
  "aaai2022_main_fairandtruthfulgiveawaylotteries": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fair and Truthful Giveaway Lotteries",
    "authors": [
      "Tal Arbiv",
      "Yonatan Aumann"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20405",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20405/20164",
    "published": "2022-02",
    "summary": "We consider a setting where a large number of agents are all interested in attending some public resource of limited capacity. Attendance is thus allotted by lottery. If agents arrive individually, then randomly choosing the agents \u2013 one by one - is a natural, fair and efficient solution. We consider the case where agents are organized in groups (e.g. families, friends), the members of each of which must all be admitted together. We study the question of how best to design such lotteries. We first establish the desired properties of such lotteries, in terms of fairness and efficiency, and define the appropriate notions of strategy proofness (providing that agents cannot gain by misrepresenting the true groups, e.g. joining or splitting groups). We establish inter-relationships between the different properties, proving properties that cannot be fulfilled simultaneously (e.g. leximin optimality and strong group stratagy proofness). Our main contribution is a polynomial mechanism for the problem, which guarantees many of the desired properties, including: leximin optimality, Pareto-optimality, anonymity, group strategy proofness, and adjunctive strategy proofness (which provides that no benefit can be obtained by registering additional - uninterested or bogus - individuals). The mechanism approximates the utilitarian optimum to within a factor of 2, which, we prove, is optimal for any mechanism that guarantees any one of the following properties: egalitarian welfare optimality, leximin optimality, envyfreeness, and adjunctive strategy proofness.",
    "code_link": ""
  },
  "aaai2022_main_universalandtightonlinealgorithmsforgeneralized-meanwelfare": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Universal and Tight Online Algorithms for Generalized-Mean Welfare",
    "authors": [
      "Siddharth Barman",
      "Arindam Khan",
      "Arnab Maiti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20406",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20406/20165",
    "published": "2022-02",
    "summary": "We study fair and efficient allocation of divisible goods, in an online manner, among n agents. The goods arrive online in a sequence of T time periods. The agents' values for a good are revealed only after its arrival, and the online algorithm needs to fractionally allocate the good, immediately and irrevocably, among the agents. Towards a unifying treatment of fairness and economic efficiency objectives, we develop an algorithmic framework for finding online allocations to maximize the generalized mean of the values received by the agents. In particular, working with the assumption that each agent's value for the grand bundle of goods is appropriately scaled, we address online maximization of p-mean welfare. Parameterized by an exponent term p in (-infty, 1], these means encapsulate a range of welfare functions, including social welfare (p=1), egalitarian welfare (p to -infty), and Nash social welfare (p to 0).We present a simple algorithmic template that takes a threshold as input and, with judicious choices for this threshold, leads to both universal and tailored competitive guarantees. First, we show that one can compute online a single allocation that O (sqrt(n) log n)-approximates the optimal p-mean welfare for all p <= 1. The existence of such a universal allocation is interesting in and of itself. Moreover, this universal guarantee achieves essentially tight competitive ratios for specific values of p. Next, we obtain improved competitive ratios for different ranges of p by executing our algorithm with p-specific thresholds, e.g., we provide O(log^3 n)-competitive ratio for all p in (-1/(log 2n),1). We complement our positive results by establishing lower bounds to show that our guarantees are essentially tight for a wide range of the exponent parameter.",
    "code_link": ""
  },
  "aaai2022_main_truthfulandfairmechanismsformatroid-rankvaluations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Truthful and Fair Mechanisms for Matroid-Rank Valuations",
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20407",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20407/20166",
    "published": "2022-02",
    "summary": "We study the problem of allocating indivisible goods among strategic agents. We focus on settings wherein monetary transfers are not available and each agent's private valuation is a submodular function with binary marginals, i.e., the agents' valuations are matroid-rank functions. In this setup, we establish a notable dichotomy between two of the most well-studied fairness notions in discrete fair division; specifically, between envy-freeness up to one good (EF1) and maximin shares (MMS). First, we show that a known Pareto-efficient mechanism is group strategy-proof for finding EF1 allocations, under matroid-rank valuations. The group strategy-proofness guarantee strengthens an existing result that establishes truthfulness (individually for each agent) in the same context. Our result also generalizes prior work from binary additive valuations to the matroid-rank case.Next, we establish that an analogous positive result cannot be achieved for MMS, even when considering truthfulness on an individual level. Specifically, we prove that, for matroid-rank valuations, there does not exist a truthful mechanism that is index oblivious, Pareto efficient, and maximin fair. For establishing our results, we develop a characterization of truthful mechanisms for matroid-rank functions. This characterization in fact holds for a broader class of valuations (specifically, holds for binary XOS functions) and might be of independent interest.",
    "code_link": ""
  },
  "aaai2022_main_truthfulcakesharing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Truthful Cake Sharing",
    "authors": [
      "Xiaohui Bei",
      "Xinhang Lu",
      "Warut Suksompong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20408",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20408/20167",
    "published": "2022-02",
    "summary": "The classic cake cutting problem concerns the fair allocation of a heterogeneous resource among interested agents. In this paper, we study a public goods variant of the problem, where instead of competing with one another for the cake, the agents all share the same subset of the cake which must be chosen subject to a length constraint. We focus on the design of truthful and fair mechanisms in the presence of strategic agents who have piecewise uniform utilities over the cake. On the one hand, we show that the leximin solution is truthful and moreover maximizes an egalitarian welfare measure among all truthful and position oblivious mechanisms. On the other hand, we demonstrate that the maximum Nash welfare solution is truthful for two agents but not in general. Our results assume that mechanisms can block each agent from accessing parts that the agent does not claim to desire; we provide an impossibility result when blocking is not allowed.",
    "code_link": ""
  },
  "aaai2022_main_thesecretaryproblemwithcompetingemployersonrandomedgearrivals": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Secretary Problem with Competing Employers on Random Edge Arrivals",
    "authors": [
      "Xiaohui Bei",
      "Shengyu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20409",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20409/20168",
    "published": "2022-02",
    "summary": "The classic secretary problem concerns the problem of an employer facing a random sequence of candidates and making online hiring decisions to try to hire the best candidate. In this paper, we study a game-theoretic generalization of the secretary problem where a set of employers compete with each other to hire the best candidate. Different from previous secretary market models, our model assumes that the sequence of candidates arriving at each employer is uniformly random but independent from other sequences. We consider two versions of this secretary game where employers can have adaptive or non-adaptive strategies, and provide characterizations of the best response and Nash equilibrium of each game.",
    "code_link": ""
  },
  "aaai2022_main_almostfullefxexistsforfouragents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Almost Full EFX Exists for Four Agents",
    "authors": [
      "Ben Berger",
      "Avi Cohen",
      "Michal Feldman",
      "Amos Fiat"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20410",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20410/20169",
    "published": "2022-02",
    "summary": "The existence of EFX allocations of goods is a major open problem in fair division, even for additive valuations. The current state of the art is that no setting where EFX allocations are impossible is known, and yet, existence results are known only for very restricted settings, such as: (i) agents with identical valuations, (ii) 2 agents, and (iii) 3 agents with additive valuations. It is also known that EFX exists if one can leave n-1 items unallocated, where n is the number of agents.We develop new techniques that allow us to push the boundaries of the enigmatic EFX problem beyond these known results, and (arguably) to simplify proofs of earlier results. Our main result is that every setting with 4 additive agents admits an EFX allocation that leaves at most a single item unallocated. Beyond our main result, we introduce a new class of valuations, termed nice cancelable, which includes additive, unit-demand, budget-additive and multiplicative valuations, among others. Using our new techniques, we show that both our results and previous results for additive valuations extend to nice cancelable valuations.",
    "code_link": ""
  },
  "aaai2022_main_sequentialblockedmatching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sequential Blocked Matching",
    "authors": [
      "Nicholas Bishop",
      "Hau Chan",
      "Debmalya Mandal",
      "Long Tran-Thanh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20411",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20411/20170",
    "published": "2022-02",
    "summary": "We consider a sequential blocked matching (SBM) model where strategic agents repeatedly report ordinal preferences over a set of services to a central planner. The planner's goal is to elicit agents' true preferences and design a policy that matches services to agents in order to maximize the expected social welfare with the added constraint thateach matched service can be blocked or unavailable for a number of time periods. Naturally, SBM models the repeated allocation of reusable services to a set of agents where each allocated service becomes unavailable for a fixed duration. We first consider the offline SBM setting, where the strategic agents are aware of their true preferences. We measure the performance of any policy by distortion, the worst-case multiplicative approximation guaranteed by any policy. For the setting with s services, we establish lower bounds of \u03a9(s) and \u03a9(\u221as) on the distortions of any deterministic and randomised mechanisms, respectively. We complement these results by providing approximately truthful, measured by incentive ratio, deterministic and randomised policies based on random serial dictatorship which match our lower bounds. Our results show that there is a significant improvement if one considers the class of randomised policies.Finally, we consider the online SBM setting with bandit feedback where each agent is initially unaware of her true preferences, and the planner must facilitate each agent in the learning of their preferences through the matching of services over time. We design an approximately truthful mechanism based on the explore-then-commit paradigm, which achieves logarithmic dynamic approximate regret.",
    "code_link": ""
  },
  "aaai2022_main_combatingcollusionringsishardbutpossible": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Combating Collusion Rings Is Hard but Possible",
    "authors": [
      "Niclas Boehmer",
      "Robert Bredereck",
      "Andr\u00e9 Nichterlein"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20412",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20412/20171",
    "published": "2022-02",
    "summary": "A recent report of Littmann published in the Communications of the ACM outlines the existence and the fatal impact of collusion rings in academic peer reviewing. We introduce and analyze the problem Cycle-Free Reviewing that aims at finding a review assignment without the following kind of collusion ring: A sequence of reviewers each reviewing a paper authored by the next reviewer in the sequence (with the last reviewer reviewing a paper of the first), thus creating a review cycle where each reviewer gives favorable reviews. As a result, all papers in that cycle have a high chance of acceptance independent of their respective scientific merit.We observe that review assignments computed using a standard Linear Programming approach typically admit many short review cycles. On the negative side, we show that Cycle-Free Reviewing is NP-hard in various restricted cases (i.e., when every author is qualified to review all papers and one wants to prevent that authors review each other's or their own papers or when every author has only one paper and is only qualified to review few papers). On the positive side, among others, we show that, in some realistic settings, an assignment without any review cycles of small length always exists. This result also gives rise to an efficient heuristic for computing (weighted) cycle-free review assignments, which we show to be of excellent quality in practice.",
    "code_link": "https://github.com/nboehmer/Combating-Collusion-Rings-is-Hard-but-Possible"
  },
  "aaai2022_main_theoryofandexperimentsonminimallyinvasivestabilitypreservationinchangingtwo-sidedmatchingmarkets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Theory of and Experiments on Minimally Invasive Stability Preservation in Changing Two-Sided Matching Markets",
    "authors": [
      "Niclas Boehmer",
      "Klaus Heeger",
      "Rolf Niedermeier"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20413",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20413/20172",
    "published": "2022-02",
    "summary": "Following up on purely theoretical work, we contribute further theoretical insights into adapting stable two-sided matchings to change. Moreover, we perform extensive empirical studies hinting at numerous practically useful properties. Our theoretical extensions include the study of new problems (that is, incremental variants of Almost Stable Marriage and Hospital Residents), focusing on their (parameterized) computational complexity and the equivalence of various change types (thus simplifying algorithmic and complexity-theoretic studies for various natural change scenarios). Our experimental findings reveal, for instance, that allowing the new matching to be blocked by a few pairs significantly decreases the difference between the old and the new matching.",
    "code_link": ""
  },
  "aaai2022_main_acalculusforcomputingstructuredjustificationsforelectionoutcomes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Calculus for Computing Structured Justifications for Election Outcomes",
    "authors": [
      "Arthur Boixel",
      "Ulle Endriss",
      "Ronald de Haan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20414",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20414/20173",
    "published": "2022-02",
    "summary": "In the context of social choice theory, we develop a tableau-based calculus for reasoning about voting rules. This calculus can be used to obtain structured explanations for why a given set of axioms justifies a given election outcome for a given profile of voter preferences. We then show how to operationalise this calculus, using a combination of SAT solving and answer set programming, to arrive at a flexible framework for presenting human-readable justifications to users.",
    "code_link": ""
  },
  "aaai2022_main_single-agentdynamicsinadditivelyseparablehedonicgames": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Single-Agent Dynamics in Additively Separable Hedonic Games",
    "authors": [
      "Felix Brandt",
      "Martin Bullinger",
      "Leo Tappe"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20415",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20415/20174",
    "published": "2022-02",
    "summary": "The formation of stable coalitions is a central concern in multiagent systems. A considerable stream of research defines stability via the absence of beneficial deviations by single agents. Such deviations require an agent to improve her utility by joining another coalition while possibly imposing further restrictions on the consent of the agents in the welcoming as well as the abandoned coalition. While most of the literature focuses on unanimous consent, we also study consent decided by majority vote, and introduce two new stability notions that can be seen as local variants of popularity. We investigate these notions in additively separable hedonic games by pinpointing boundaries to computational complexity depending on the type of consent and restrictions on the utility functions. The latter restrictions shed new light on well-studied classes of games based on the appreciation of friends or the aversion to enemies. Many of our positive results follow from the Deviation Lemma, a general combinatorial observation, which can be leveraged to prove the convergence of simple and natural single-agent dynamics under fairly general conditions.",
    "code_link": ""
  },
  "aaai2022_main_onimprovingresourceallocationsbysharing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On Improving Resource Allocations by Sharing",
    "authors": [
      "Robert Bredereck",
      "Andrzej Kaczmarczyk",
      "Junjie Luo",
      "Rolf Niedermeier",
      "Florian Sachse"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20416",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20416/20175",
    "published": "2022-02",
    "summary": "Given an initial resource allocation, where some agents may envy others or where a different distribution of resources might lead to higher social welfare, our goal is to improve the allocation without reassigning resources. We consider a sharing concept allowing resources being shared with social network neighbors of the resource owners. To this end, we introduce a formal model that allows a central authority to compute an optimal sharing between neighbors based on an initial allocation. Advocating this point of view, we focus on the most basic scenario where a resource may be shared by two neighbors in a social network and each agent can participate in a bounded number of sharings. We present algorithms for optimizing utilitarian and egalitarian social welfare of allocations and for reducing the number of envious agents. In particular, we examine the computational complexity with respect to several natural parameters. Furthermore, we study cases with restricted social network structures and, among others, devise polynomial-time algorithms in path- and tree-like (hierarchical) social networks.",
    "code_link": ""
  },
  "aaai2022_main_liquiddemocracywithrankeddelegations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Liquid Democracy with Ranked Delegations",
    "authors": [
      "Markus Brill",
      "Th\u00e9o Delemazure",
      "Anne-Marie George",
      "Martin Lackner",
      "Ulrike\n      Schmidt-Kraepelin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20417",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20417/20176",
    "published": "2022-02",
    "summary": "Liquid democracy is a novel paradigm for collective decision-making that gives agents the choice between casting a direct vote or delegating their vote to another agent. We consider a generalization of the standard liquid democracy setting by allowing agents to specify multiple potential delegates, together with a preference ranking among them. This generalization increases the number of possible delegation paths and enables higher participation rates because fewer votes are lost due to delegation cycles or abstaining agents. In order to implement this generalization of liquid democracy, we need to find a principled way of choosing between multiple delegation paths. In this paper, we provide a thorough axiomatic analysis of the space of delegation rules, i.e., functions assigning a feasible delegation path to each delegating agent. In particular, we prove axiomatic characterizations as well as an impossibility result for delegation rules. We also analyze requirements on delegation rules that have been suggested by practitioners, and introduce novel rules with attractive properties. By performing an extensive experimental analysis on synthetic as well as real-world data, we compare delegation rules with respect to several quantitative criteria relating to the chosen paths and the resulting distribution of voting power. Our experiments reveal that delegation rules can be aligned on a spectrum reflecting an inherent trade-off between competing objectives.",
    "code_link": "https://github.com/TheoDlmz/rankeddelegation"
  },
  "aaai2022_main_individualrepresentationinapproval-basedcommitteevoting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Individual Representation in Approval-Based Committee Voting",
    "authors": [
      "Markus Brill",
      "Jonas Israel",
      "Evi Micha",
      "Jannik Peters"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20418",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20418/20177",
    "published": "2022-02",
    "summary": "When selecting multiple candidates based on approval preferences of agents, the proportional representation of agents' opinions is an important and well-studied desideratum. Existing criteria for evaluating the representativeness of outcomes focus on groups of agents and demand that sufficiently large and cohesive groups are \"represented\" in the sense that candidates approved by some group members are selected. Crucially, these criteria say nothing about the representation of individual agents, even if these agents are members of groups that deserve representation. In this paper, we formalize the concept of individual representation (IR) and explore to which extent, and under which circumstances, it can be achieved. We show that checking whether an IR outcome exists is computationally intractable, and we verify that all common approval-based voting rules may fail to provide IR even in cases where this is possible. We then focus on domain restrictions and establish an interesting contrast between \"voter interval\" and \"candidate interval\" preferences. This contrast can also be observed in our experimental results, where we analyze the attainability of IR for realistic preference profiles.",
    "code_link": ""
  },
  "aaai2022_main_themetricdistortionofmultiwinnervoting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Metric Distortion of Multiwinner Voting",
    "authors": [
      "Ioannis Caragiannis",
      "Nisarg Shah",
      "Alexandros A. Voudouris"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20419",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20419/20178",
    "published": "2022-02",
    "summary": "We extend the recently introduced framework of metric distortion to multiwinner voting. In this framework, n agents and m alternatives are located in an underlying metric space. The exact distances between agents and alternatives are unknown. Instead, each agent provides a ranking of the alternatives, ordered from the closest to the farthest. Typically, the goal is to select a single alternative that approximately minimizes the total distance from the agents, and the worst-case approximation ratio is termed distortion. In the case of multiwinner voting, the goal is to select a committee of k alternatives that (approximately) minimizes the total cost to all agents. We consider the scenario where the cost of an agent for a committee is her distance from the q-th closest alternative in the committee. We reveal a surprising trichotomy on the distortion of multiwinner voting rules in terms of k and q: The distortion is unbounded when q <= k/3, asymptotically linear in the number of agents when k/3 < q <= k/2, and constant when q > k/2.",
    "code_link": ""
  },
  "aaai2022_main_alittlecharityguaranteesfairconnectedgraphpartitioning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Little Charity Guarantees Fair Connected Graph Partitioning",
    "authors": [
      "Ioannis Caragiannis",
      "Evi Micha",
      "Nisarg Shah"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20420",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20420/20179",
    "published": "2022-02",
    "summary": "Motivated by fair division applications, we study a fair connected graph partitioning problem, in which an undirected graph with m nodes must be divided between n agents such that each agent receives a connected subgraph and the partition is fair. We study approximate versions of two fairness criteria: \\alpha-proportionality requires that each agent receive a subgraph with at least (1/\\alpha)*m/n nodes, and \\alpha-balancedness requires that the ratio between the sizes of the largest and smallest subgraphs be at most \\alpha. Unfortunately, there exist simple examples in which no partition is reasonably proportional or balanced. To circumvent this, we introduce the idea of charity. We show that by \"donating\" just n-1 nodes, we can guarantee the existence of 2-proportional and almost 2-balanced partitions (and find them in polynomial time), and that this result is almost tight. More generally, we chart the tradeoff between the size of charity and the approximation of proportionality or balancedness we can guarantee.",
    "code_link": ""
  },
  "aaai2022_main_truthfulaggregationofbudgetproposalswithproportionalityguarantees": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Truthful Aggregation of Budget Proposals with Proportionality Guarantees",
    "authors": [
      "Ioannis Caragiannis",
      "George Christodoulou",
      "Nicos Protopapas"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20421",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20421/20180",
    "published": "2022-02",
    "summary": "We study a participatory budgeting problem, where a set of strategic agents wish to split a divisible budget among different projects by aggregating their proposals on a single division. Unfortunately, the straightforward rule that divides the budget proportionally is susceptible to manipulation. Recently, a class of truthful mechanisms has been proposed, namely the moving phantom mechanisms. One such mechanism satisfies the proportionality property, in the sense that in the extreme case where all agents prefer a single project to receive the whole amount, the budget is assigned proportionally. While proportionality is a naturally desired property, it is defined over a limited type of preference profiles. To address this, we expand the notion of proportionality, by proposing a quantitative framework that evaluates a budget aggregation mechanism according to its worst-case distance from the proportional allocation. Crucially, this is defined for every preference profile. We study this measure on the class of moving phantom mechanisms, and we provide approximation guarantees. For two projects, we show that the Uniform Phantom mechanism is optimal among all truthful mechanisms. For three projects, we propose a new, proportional mechanism that is optimal among all moving phantom mechanisms. Finally, we provide impossibility results regarding the approximability of moving phantom mechanisms.",
    "code_link": ""
  },
  "aaai2022_main_thecomplexityoflearningapproval-basedmultiwinnervotingrules": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Complexity of Learning Approval-Based Multiwinner Voting Rules",
    "authors": [
      "Ioannis Caragiannis",
      "Karl Fehrs"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20422",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20422/20181",
    "published": "2022-02",
    "summary": "We study the PAC learnability of multiwinner voting, focusing on the class of approval-based committee scoring (ABCS) rules. These are voting rules applied on profiles with approval ballots, where each voter approves some of the candidates. According to ABCS rules, each committee of k candidates collects from each voter a score, that depends on the size of the voter's ballot and on the size of its intersection with the committee. Then, committees of maximum score are the winning ones. Our goal is to learn a target rule (i.e., to learn the corresponding scoring function) using information about the winning committees of a small number of sampled profiles. Despite the existence of exponentially many outcomes compared to single-winner elections, we show that the sample complexity is still low: a polynomial number of samples carries enough information for learning the target rule with high confidence and accuracy. Unfortunately, even simple tasks that need to be solved for learning from these samples are intractable. We prove that deciding whether there exists some ABCS rule that makes a given committee winning in a given profile is a computationally hard problem. Our results extend to the class of sequential Thiele rules, which have received attention due to their simplicity.",
    "code_link": ""
  },
  "aaai2022_main_efficiencyofadauctionswithpricedisplaying": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficiency of Ad Auctions with Price Displaying",
    "authors": [
      "Matteo Castiglioni",
      "Diodato Ferraioli",
      "Nicola Gatti",
      "Alberto Marchesi",
      "Giulia Romano"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20423",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20423/20182",
    "published": "2022-02",
    "summary": "Most economic reports suggest that almost half of the market value unlocked by artificial intelligence (AI) by the next decade (about 9 trillion USD per year) will be in marketing&sales. In particular, AI will allow the optimization of more and more intricate economic settings in which multiple different activities can be automated jointly. A relatively recent example is that one of ad auctions in which similar products or services are displayed together with their price, thus merging advertising and pricing in a unique website. This is the case, e.g., of Google Hotel Ads and TripAdvisor. More precisely, as in a classical ad auction, the ranking of the ads depends on the advertisers' bids, while, differently from classical ad auctions, the price is displayed together with the ad, so as to provide a direct comparison among the prices and thus dramatically affect the behavior of the users. This paper investigates how displaying prices and ads together conditions the properties of the main economic mechanisms such as VCG and GSP. Initially, we focus on the direct-revelation mechanism, showing that prices are chosen by the mechanisms once given the advertisers' reports. We also provide an efficient algorithm to compute the optimal allocation given the private information reported by the advertisers. Then, with both VCG and GSP payments, we show the inefficiency in terms of Price of Anarchy (PoA) and Stability (PoS) over the social welfare and mechanism's revenue when the advertisers choose the prices. The main results show that, with both VCG and GSP, PoS over the revenue may be unbounded even with two slots, while PoA over the social welfare may be as large as the number of slots. Finally, we show that, under some assumptions, simple modifications to VCG and GSP allow us to obtain a better PoS over the revenue.",
    "code_link": ""
  },
  "aaai2022_main_signalinginpostedpriceauctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Signaling in Posted Price Auctions",
    "authors": [
      "Matteo Castiglioni",
      "Giulia Romano",
      "Alberto Marchesi",
      "Nicola Gatti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20424",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20424/20183",
    "published": "2022-02",
    "summary": "We study single-item single-unit Bayesian posted price auctions, where buyers arrive sequentially and their valuations for the item being sold depend on a random, unknown state of nature. The seller has complete knowledge of the actual state and can send signals to the buyers so as to disclose information about it. For instance, the state of nature may reflect the condition and/or some particular features of the item, which are known to the seller only. The problem faced by the seller is about how to partially disclose information about the state so as to maximize revenue. Unlike classical signaling problems, in this setting, the seller must also correlate the signals being sent to the buyers with some price proposals for them. This introduces additional challenges compared to standard settings. We consider two cases: the one where the seller can only send signals publicly visible to all buyers, and the case in which the seller can privately send a different signal to each buyer. As a first step, we prove that, in both settings, the problem of maximizing the seller's revenue does not admit an FPTAS unless P=NP, even for basic instances with a single buyer. As a result, in the rest of the paper, we focus on designing PTASs. In order to do so, we first introduce a unifying framework encompassing both public and private signaling, whose core result is a decomposition lemma that allows focusing on a finite set of possible buyers' posteriors. This forms the basis on which our PTASs are developed. In particular, in the public signaling setting, our PTAS employs some ad hoc techniques based on linear programming, while our PTAS for the private setting relies on the ellipsoid method to solve an exponentially-sized LP in polynomial time. In the latter case, we need a custom approximate separation oracle, which we implement with a dynamic programming approach.",
    "code_link": ""
  },
  "aaai2022_main_weightedfairnessnotionsforindivisibleitemsrevisited": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Weighted Fairness Notions for Indivisible Items Revisited",
    "authors": [
      "Mithun Chakraborty",
      "Erel Segal-Halevi",
      "Warut Suksompong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20425",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20425/20184",
    "published": "2022-02",
    "summary": "We revisit the setting of fairly allocating indivisible items when agents have different weights representing their entitlements. First, we propose a parameterized family of relaxations for weighted envy-freeness and the same for weighted proportionality; the parameters indicate whether smaller-weight or larger-weight agents should be given a higher priority. We show that each notion in these families can always be satisfied, but any two cannot necessarily be fulfilled simultaneously. We then introduce an intuitive weighted generalization of maximin share fairness and establish the optimal approximation of it that can be guaranteed. Furthermore, we characterize the implication relations between the various weighted fairness notions introduced in this and prior work, and relate them to the lower and upper quota axioms from apportionment.",
    "code_link": ""
  },
  "aaai2022_main_pizzasharingisppa-hard": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pizza Sharing Is PPA-Hard",
    "authors": [
      "Argyrios Deligkas",
      "John Fearnley",
      "Themistoklis Melissourgos"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20426",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20426/20185",
    "published": "2022-02",
    "summary": "We study the computational complexity of computing solutions for thestraight-cut and square-cut pizza sharing problems. We show that finding anapproximate solution is PPA-hard for the straight-cut problem, andPPA-complete for the square-cut problem, while finding an exact solution forthe square-cut problem is FIXP-hard and in BU. Our PPA-hardness results applyeven when all mass distributions are unions of non-overlapping squares, and ourFIXP-hardness result applies even when all mass distributions are unions ofweighted squares and right-angled triangles. We also prove that decision variantsof the square-cut problem are hard: the approximate problem isNP-complete, and the exact problem is ETR-complete.",
    "code_link": ""
  },
  "aaai2022_main_heterogeneousfacilitylocationwithlimitedresources": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Heterogeneous Facility Location with Limited Resources",
    "authors": [
      "Argyrios Deligkas",
      "Aris Filos-Ratsikas",
      "Alexandros A. Voudouris"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20427",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20427/20186",
    "published": "2022-02",
    "summary": "We initiate the study of the heterogeneous facility location problem with limited resources. We mainly focus on the fundamental case where a set of agents are positioned in the line segment [0,1] and have approval preferences over two available facilities. A mechanism takes as input the positions and the preferences of the agents, and chooses to locate a single facility based on this information. We study mechanisms that aim to maximize the social welfare (the total utility the agents derive from facilities they approve), under the constraint of incentivizing the agents to truthfully report their positions and preferences. We consider three different settings depending on the level of agent-related information that is public or private. For each setting, we design deterministic and randomized strategyproof mechanisms that achieve a good approximation of the optimal social welfare, and complement these with nearly-tight impossibility results.",
    "code_link": ""
  },
  "aaai2022_main_complexityofdeliberativecoalitionformation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Complexity of Deliberative Coalition Formation",
    "authors": [
      "Edith Elkind",
      "Abheek Ghosh",
      "Paul Goldberg"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20428",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20428/20187",
    "published": "2022-02",
    "summary": "Elkind et al. (AAAI'21) introduced a model for deliberative coalition formation, where a community wishes to identify a strongly supported proposal from a space of alternatives, in order to change the status quo. In their model, agents and proposals are points in a metric space, agents' preferences are determined by distances, and agents deliberate by dynamically forming coalitions around proposals that they prefer over the status quo. The deliberation process operates via k-compromise transitions, where agents from k (current) coalitions come together to form a larger coalition in order to support a (perhaps new) proposal, possibly leaving behind some of the dissenting agents from their old coalitions. A deliberation succeeds if it terminates by identifying a proposal with the largest possible support. For deliberation in d dimensions, Elkind et al. consider two variants of their model: in the Euclidean model, proposals and agent locations are points in R^d and the distance is measured according to ||...||_2; and in the hypercube model, proposals and agent locations are vertices of the d-dimensional hypercube and the metric is the Hamming distance. They show that in the Euclidean model 2-compromises are guaranteed to succeed, but in the hypercube model for deliberation to succeed it may be necessary to use k-compromises with k >= d.We complement their analysis by (1) proving that in both models it is hard to find a proposal with a high degree of support, and even a 2-compromise transition may be hard to compute;(2) showing that a sequence of 2-compromise transitions may be exponentially long;(3) strengthening the lower bound on the size of the compromise for the d-hypercube model from d to 2^\u2126(d).",
    "code_link": ""
  },
  "aaai2022_main_thepriceofjustifiedrepresentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Price of Justified Representation",
    "authors": [
      "Edith Elkind",
      "Piotr Faliszewski",
      "Ayumi Igarashi",
      "Pasin Manurangsi",
      "Ulrike\n      Schmidt-Kraepelin",
      "Warut Suksompong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20429",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20429/20188",
    "published": "2022-02",
    "summary": "In multiwinner approval voting, the goal is to select k-member committees based on voters' approval ballots. A well-studied concept of proportionality in this context is the justified representation (JR) axiom, which demands that no large cohesive group of voters remains unrepresented. However, the JR axiom may conflict with other desiderata, such as coverage (maximizing the number of voters who approve at least one committee member) or social welfare (maximizing the number of approvals obtained by committee members). In this work, we investigate the impact of imposing the JR axiom (as well as the more demanding EJR axiom) on social welfare and coverage. Our approach is threefold: we derive worst-case bounds on the loss of welfare/coverage that is caused by imposing JR, study the computational complexity of finding 'good' committees that provide JR (obtaining a hardness result, an approximation algorithm, and an exact algorithm for one-dimensional preferences), and examine this setting empirically on several synthetic datasets.",
    "code_link": "https://github.com/ProjectPRAGMA/PriceOfJR-AAAI-2022"
  },
  "aaai2022_main_thecomplexityofsubelectionisomorphismproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Complexity of Subelection Isomorphism Problems",
    "authors": [
      "Piotr Faliszewski",
      "Krzysztof Sornat",
      "Stanis\u0142aw Szufa"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20430",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20430/20189",
    "published": "2022-02",
    "summary": "We study extensions of the Election Isomorphism problem, focused on the existence of isomorphic subelections. Specifically, we propose the Subelection Isomorphism and the Maximum Common Subelection problems and study their computational complexity and approximability. Using our problems in experiments, we provide some insights into the nature of several statistical models of elections.",
    "code_link": "https://github.com/Project-PRAGMA/Subelections-AAAI2022"
  },
  "aaai2022_main_fastpayoffmatrixsparsificationtechniquesforstructuredextensive-formgames": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast Payoff Matrix Sparsification Techniques for Structured Extensive-Form Games",
    "authors": [
      "Gabriele Farina",
      "Tuomas Sandholm"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20431",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20431/20190",
    "published": "2022-02",
    "summary": "The practical scalability of many optimization algorithms for large extensive-form games is often limited by the games' huge payoff matrices. To ameliorate the issue, Zhang and Sandholm recently proposed a sparsification technique that factorizes the payoff matrix A into a sparser object A = \u00c2 + UV\u1d40, where the total combined number of nonzeros of \u00c2, U, and V, is significantly smaller. Such a factorization can be used in place of the original payoff matrix in many optimization algorithm, such as interior-point and second-order methods, thus increasing the size of games that can be handled. Their technique significantly sparsifies poker (end)games, standard benchmarks used in computational game theory, AI, and more broadly. We show that the existence of extremely sparse factorizations in poker games can be tied to their particular Kronecker-product structure. We clarify how such structure arises and introduce the connection between that structure and sparsification. By leveraging such structure, we give two ways of computing strong sparsifications of poker games (as well as any other game with a similar structure) that are i) orders of magnitude faster to compute, ii) more numerically stable, and iii) produce a dramatically smaller number of nonzeros than the prior technique. Our techniques enable\u2014for the first time\u2014effective computation of high-precision Nash equilibria and strategies subject to constraints on the amount of allowed randomization. Furthermore, they significantly speed up parallel first-order game-solving algorithms; we show state-of-the-art speed on a GPU.",
    "code_link": ""
  },
  "aaai2022_main_two-priceequilibrium": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Two-Price Equilibrium",
    "authors": [
      "Michal Feldman",
      "Galia Shabtai",
      "Aner Wolfenfeld"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20432",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20432/20191",
    "published": "2022-02",
    "summary": "Walrasian equilibrium is a prominent market equilibrium notion, but rarely exists in markets with indivisible items.We introduce a new market equilibrium notion, called two-price equilibrium (2PE). A 2PE is a relaxation of Walrasian equilibrium, where instead of a single price per item, every item has two prices: one for the item's owner and a (possibly) higher one for all other buyers. Thus, a 2PE is given by a tuple (S,p_high,p_low) of an allocation S and two price vectors p_high,p_low, where every buyer i is maximally happy with her bundle S_i, given prices p_low for items in S_i and prices p_high for all other items. 2PE generalizes previous market equilibrium notions, such as conditional equilibrium, and is related to relaxed equilibrium notions like endowment equilibrium. We define the discrepancy of a 2PE --- a measure of distance from Walrasian equilibrium --- as the sum of differences p_high_j-p_low_j over all items (normalized by social welfare).We show that the social welfare degrades gracefully with the discrepancy; namely, the social welfare of a 2PE with discrepancy d is at least a fraction 1/d+1 of the optimal welfare.We use this to establish welfare guarantees for markets with subadditive valuations over identical items.In particular, we show that every such market admits a 2PE with at least 1/7 of the optimal welfare.This is in contrast to Walrasian equilibrium or conditional equilibrium which may not even exist.Our techniques provide new insights regarding valuation functions over identical items, which we also use to characterize instances that admit a WE.",
    "code_link": ""
  },
  "aaai2022_main_algorithmicbayesianpersuasionwithcombinatorialactions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Algorithmic Bayesian Persuasion with Combinatorial Actions",
    "authors": [
      "Kaito Fujii",
      "Shinsaku Sakaue"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20433",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20433/20192",
    "published": "2022-02",
    "summary": "Bayesian persuasion is a model for understanding strategic information revelation: an agent with an informational advantage, called a sender, strategically discloses information by sending signals to another agent, called a receiver. In algorithmic Bayesian persuasion, we are interested in efficiently designing the sender's signaling schemes that lead the receiver to take action in favor of the sender. This paper studies algorithmic Bayesian-persuasion settings where the receiver's feasible actions are specified by combinatorial constraints, e.g., matroids or paths in graphs. We first show that constant-factor approximation is NP-hard even in some special cases of matroids or paths. We then propose a polynomial-time algorithm for general matroids by assuming the number of states of nature to be a constant. We finally consider a relaxed notion of persuasiveness, called CCE-persuasiveness, and present a sufficient condition for polynomial-time approximability.",
    "code_link": ""
  },
  "aaai2022_main_bayesianpersuasioninsequentialdecision-making": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bayesian Persuasion in Sequential Decision-Making",
    "authors": [
      "Jiarui Gan",
      "Rupak Majumdar",
      "Goran Radanovic",
      "Adish Singla"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20434",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20434/20193",
    "published": "2022-02",
    "summary": "We study a dynamic model of Bayesian persuasion in sequential decision-making settings. An informed principal observes an external parameter of the world and advises an uninformed agent about actions to take over time. The agent takes actions in each time step based on the current state, the principal's advice/signal, and beliefs about the external parameter. The action of the agent updates the state according to a stochastic process. The model arises naturally in many applications, e.g., an app (the principal) can advice the user (the agent) on possible choices between actions based on additional real-time information the app has. We study the problem of designing a signaling strategy from the principal's point of view. We show that the principal has an optimal strategy against a myopic agent, who only optimizes their rewards locally, and the optimal strategy can be computed in polynomial time. In contrast, it is NP-hard to approximate an optimal policy against a far-sighted agent. Further, we show that if the principal has the power to threaten the agent by not providing future signals, then we can efficiently design a threat-based strategy. This strategy guarantees the principal's payoff as if playing against an agent who is far-sighted but myopic to future signals.",
    "code_link": ""
  },
  "aaai2022_main_hedonicdiversitygamesacomplexitypicturewithmorethantwocolors": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hedonic Diversity Games: A Complexity Picture with More than Two Colors",
    "authors": [
      "Robert Ganian",
      "Thekla Hamm",
      "Du\u0161an Knop",
      "\u0160imon Schierreich",
      "Ond\u0159ej Such\u00fd"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20435",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20435/20194",
    "published": "2022-02",
    "summary": "Hedonic diversity games are a variant of the classical Hedonic games designed to better model a variety of questions concerning diversity and fairness. Previous works mainly targeted the case with two diversity classes (represented as colors in the model) and provided a set of initial complexity-theoretic and existential results concerning Nash and Individually stable outcomes. Here, we design new algorithms accompanied with lower bounds which provide a full parameterized-complexity picture for computing Nash and Individually stable outcomes with respect to the most natural parameterizations of the problem. Crucially, our results hold for general Hedonic diversity games where the number of colors is not necessarily restricted to two, and show that---apart from two trivial cases---a necessary condition for tractability in this setting is that the number of colors is bounded by the parameter. Moreover, for the special case of two colors we resolve an open question asked in previous work~(Boehmer and Elkind, AAAI 2020).",
    "code_link": ""
  },
  "aaai2022_main_fairandefficientallocationsofchoresunderbivaluedpreferences": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fair and Efficient Allocations of Chores under Bivalued Preferences",
    "authors": [
      "Jugal Garg",
      "Aniket Murhekar",
      "John Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20436",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20436/20195",
    "published": "2022-02",
    "summary": "We study the problem of fair and efficient allocation of a set of indivisible chores to agents with additive cost functions. We consider the popular fairness notion of envy-freeness up to one good (EF1) with the efficiency notion of Pareto-optimality (PO). While it is known that EF1+PO allocations exists and can be computed in pseudo-polynomial time in the case of goods, the same problem is open for chores.Our first result is a strongly polynomial-time algorithm for computing an EF1+PO allocation for bivalued instances, where agents have (at most) two disutility values for the chores. To the best of our knowledge, this is the first non-trivial class of chores to admit an EF1+PO allocation and an efficient algorithm for its computation. We also study the problem of computing an envy-free (EF) and PO allocation for the case of divisible chores. While the existence of EF+PO allocation is known via competitive equilibrium with equal incomes, its efficient computation is open. Our second result shows that for bivalued instances, an EF+PO allocation can be computed in strongly polynomial-time.",
    "code_link": ""
  },
  "aaai2022_main_secretarymatchingwithvertexarrivalsandnorejections": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Secretary Matching with Vertex Arrivals and No Rejections",
    "authors": [
      "Mohak Goyal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20437",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20437/20196",
    "published": "2022-02",
    "summary": "Most prior work on online matching problems has been with the flexibility of keeping some vertices unmatched. We study three related online matching problems with the constraint of matching every vertex, i.e., with no rejections. We adopt a model in which vertices arrive in a uniformly random order and the edge-weights are arbitrary positive numbers. For the capacitated online bipartite matching problem in which the vertices of one side of the graph are offline and those of the other side arrive online, we give a 4.62-competitive algorithm when the capacity of each offline vertex is 2. For the online general (non-bipartite) matching problem, where all vertices arrive online, we give a 3.34-competitive algorithm. We also study the online roommate matching problem, in which each room (offline vertex) holds 2 persons (online vertices). Persons derive non-negative additive utilities from their room as well as roommate. In this model, with the goal of maximizing the sum of utilities of all persons, we give a 7.96-competitive algorithm. This is an improvement over the 24.72 approximation factor in prior work.",
    "code_link": ""
  },
  "aaai2022_main_machine-learnedpredictionequilibriumfordynamictrafficassignment": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Machine-Learned Prediction Equilibrium for Dynamic Traffic Assignment",
    "authors": [
      "Lukas Graf",
      "Tobias Harks",
      "Kostas Kollias",
      "Michael Markl"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20438",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20438/20197",
    "published": "2022-02",
    "summary": "We study a dynamic traffic assignment model, where agents base their instantaneous routing decisions on real-time delay predictions. We formulate a mathematically concise model and derive properties of the predictors that ensure a dynamic prediction equilibrium exists. We demonstrate the versatility of our framework by showing that it subsumes the well-known full information and instantaneous information models, in addition to admitting further realistic predictors as special cases. We complement our theoretical analysis by an experimental study, in which we systematically compare the induced average travel times of different predictors, including a machine-learning model trained on data gained from previously computed equilibrium flows, both on a synthetic and a real road network.",
    "code_link": ""
  },
  "aaai2022_main_multi-leadercongestiongameswithanadversary": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Leader Congestion Games with an Adversary",
    "authors": [
      "Tobias Harks",
      "Mona Henle",
      "Max Klimm",
      "Jannik Matuschke",
      "Anja Schedel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20439",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20439/20198",
    "published": "2022-02",
    "summary": "We study a multi-leader single-follower congestion game where multiple users (leaders) choose one resource out of a set of resources and, after observing the realized loads, an adversary (single-follower) attacks the resources with maximum loads causing additional costs for the leaders. For the resulting strategic game among the leaders, we show that pure Nash equilibria fail to exist and therefore, we consider approximate equilibria instead.As our first main result, we show that the existence of a K-approximate equilibrium can always be guaranteed, where K (approximately equal to 1.1974) is the unique solution of a cubic polynomial equation. To this end, we give a polynomial time combinatorial algorithm which computes a K-approximate equilibrium. The factor K is tight, meaning that there is an instance that does not admit an A-approximate equilibrium for any A < K. Thus A = K is the smallest possible value of A such that the existence of an A-approximate equilibrium can be guaranteed for any instance of the considered game. Secondly, we focus on approximate equilibria of a given fixed instance. We show how to compute efficiently a best approximate equilibrium, that is, with smallest possible A among all A-approximate equilibria of the given instance.",
    "code_link": ""
  },
  "aaai2022_main_approval-basedcommitteevotingunderincompleteinformation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Approval-Based Committee Voting under Incomplete Information",
    "authors": [
      "Aviram Imber",
      "Jonas Israel",
      "Markus Brill",
      "Benny Kimelfeld"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20440",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20440/20199",
    "published": "2022-02",
    "summary": "We investigate approval-based committee voting with incomplete information about the approval preferences of voters. We consider several models of incompleteness where each voter partitions the set of candidates into approved, disapproved, and unknown candidates, possibly with ordinal preference constraints among candidates in the latter category. This captures scenarios where voters have not evaluated all candidates and/or it is unknown where voters draw the threshold between approved and disapproved candidates. We study the complexity of some fundamental computational problems for a number of classic approval-based committee voting rules including Proportional Approval Voting and Chamberlin-Courant. These problems include that of determining whether a given set of candidates is a possible or necessary winning committee and whether it forms a committee that possibly or necessarily satisfies representation axioms. We also consider the problem whether a given candidate is possibly or necessarily a member of the winning committee.",
    "code_link": ""
  },
  "aaai2022_main_reforminganenvy-freematching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reforming an Envy-Free Matching",
    "authors": [
      "Takehiro Ito",
      "Yuni Iwamasa",
      "Naonori Kakimura",
      "Naoyuki Kamiyama",
      "Yusuke\n      Kobayashi",
      "Yuta Nozaki",
      "Yoshio Okamoto",
      "Kenta Ozeki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20441",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20441/20200",
    "published": "2022-02",
    "summary": "We consider the problem of reforming an envy-free matching when each agent is assigned a single item. Given an envy-free matching, we consider an operation to exchange the item of an agent with an unassigned item preferred by the agent that results in another envy-free matching. We repeat this operation as long as we can. We prove that the resulting envy-free matching is uniquely determined up to the choice of an initial envy-free matching, and can be found in polynomial time. We call the resulting matching a reformist envy-free matching, and then we study a shortest sequence to obtain the reformist envy-free matching from an initial envy-free matching. We prove that a shortest sequence is computationally hard to obtain even when each agent accepts at most four items and each item is accepted by at most three agents. On the other hand, we give polynomial-time algorithms when each agent accepts at most three items or each item is accepted by at most two agents. Inapproximability and fixed-parameter (in)tractability are also discussed.",
    "code_link": ""
  },
  "aaai2022_main_thecomplexityofproportionalitydegreeincommitteeelections": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Complexity of Proportionality Degree in Committee Elections",
    "authors": [
      "\u0141ukasz Janeczko",
      "Piotr Faliszewski"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20442",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20442/20201",
    "published": "2022-02",
    "summary": "Over the last few years, researchers have put significant effort into understanding of the notion of proportional representation in committee election. In particular, recently they have proposed the notion of proportionality degree. We study the complexity of computing committees with a given proportionality degree and of testing if a given committee provides a particular one. This way, we complement recent studies that mostly focused on the notion of (extended) justified representation.We also study the problems of testing if a cohesive group of a given size exists and of counting such groups.",
    "code_link": ""
  },
  "aaai2022_main_worst-casevotingwhenthestakesarehigh": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Worst-Case Voting When the Stakes Are High",
    "authors": [
      "Anson Kahng",
      "Gregory Kehne"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20443",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20443/20202",
    "published": "2022-02",
    "summary": "We study the additive distortion of social choice functions in the implicit utilitarian model, and argue that it is a more appropriate metric than multiplicative distortion when an alternative that confers significant social welfare may exist (i.e., when the stakes are high). We define a randomized analog of positional scoring rules, and present a rule which is asymptotically optimal within this class as the number of alternatives increases. We then show that the instance-optimal social choice function can be efficiently computed. Next, we take a beyond-worst-case view, bounding the additive distortion of prominent voting rules as a function of the best welfare attainable in an instance. Lastly, we evaluate the additive distortion of a range of rules on real-world election data.",
    "code_link": ""
  },
  "aaai2022_main_pagerankforedgesaxiomaticcharacterization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PageRank for Edges: Axiomatic Characterization",
    "authors": [
      "Natalia Kucharczuk",
      "Tomasz W\u0105s",
      "Oskar Skibski"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20444",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20444/20203",
    "published": "2022-02",
    "summary": "Edge centrality measures are functions that evaluate the importance of edges in a network. They can be used to assess the role of a backlink for the popularity of a website as well as the importance of a flight in virus spreading. Various node centralities have been translated to apply for edges, including Edge Betweenness, Eigenedge (edge version of eigenvector centrality), and Edge PageRank. With this paper, we initiate the discussion on the axiomatic properties of edge centrality measures. We do it by proposing an axiomatic characterization of Edge PageRank. Our characterization is the first characterization of any edge centrality measures in the literature.",
    "code_link": ""
  },
  "aaai2022_main_safesubgameresolvingforextensiveformcorrelatedequilibrium": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Safe Subgame Resolving for Extensive Form Correlated Equilibrium",
    "authors": [
      "Chun Kai Ling",
      "Fei Fang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20445",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20445/20204",
    "published": "2022-02",
    "summary": "Correlated Equilibrium is a solution concept that is more general than Nash Equilibrium (NE) and can lead to outcomes with better social welfare. However, its natural extension to the sequential setting, the Extensive Form Correlated Equilibrium (EFCE), requires a quadratic amount of space to solve, even in restricted settings without randomness in nature. To alleviate these concerns, we apply subgame resolving, a technique extremely successful in finding NE in zero-sum games to solving general-sum EFCEs. Subgame resolving refines a correlation plan in an online manner: instead of solving for the full game upfront, it only solves for strategies in subgames that are reached in actual play, resulting in significant computational gains. In this paper, we (i) lay out the foundations to quantify the quality of a refined strategy, in terms of the social welfare and exploitability of correlation plans, (ii) show that EFCEs possess a sufficient amount of independence between subgames to perform resolving efficiently, and (iii) provide two algorithms for resolving, one using linear programming and the other based on regret minimization. Both methods guarantee safety, i.e., they will never be counterproductive. Our methods are the first time an online method has been applied to the correlated, general-sum setting.",
    "code_link": ""
  },
  "aaai2022_main_thesemi-randomlikelihoodofdoctrinalparadoxes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Semi-random Likelihood of Doctrinal Paradoxes",
    "authors": [
      "Ao Liu",
      "Lirong Xia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20446",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20446/20205",
    "published": "2022-02",
    "summary": "When aggregating logically interconnected judgements from n agents, the result might be logically inconsistent. This phenomenon is known as the doctrinal paradox, which plays a central role in the field of judgement aggregation. Previous work has mostly focused on the worst-case analysis of the doctrinal paradox, leading to many impossibility results. Little is known about its likelihood of occurrence in practical settings,except for the study under certain distributions by List in 2005. In this paper, we characterize the likelihood of the doctrinal paradox under ageneral and realistic model called semi-random social choice framework (proposed by Xia in 2020). In the framework, agents' ground truth judgements can be arbitrarily correlated, while the noises are independent.Our main theorem states that under mild conditions, the semi-random likelihood of the doctrinal paradox is either 0, exp(-\u0398(n)), \u0398(n\\^~(-0.5)) or \u0398(1). This not only answers open questions by List in 2005, but also draws clear lines between situations with frequent paradoxes and with vanishing paradoxes.",
    "code_link": ""
  },
  "aaai2022_main_isthereastrongestdieinasetofdicewiththesamemeanpips?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Is There a Strongest Die in a Set of Dice with the Same Mean Pips?",
    "authors": [
      "Shang Lu",
      "Shuji Kijima"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20447",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20447/20206",
    "published": "2022-02",
    "summary": "Jan-ken, a.k.a. rock-paper-scissors, is a cerebrated example of a non-transitive game with three (pure) strategies, rock, paper and scissors. Interestingly, any Jan-ken generalized to four strategies contains at least one useless strategy unless it allows a tie between distinct pure strategies. Non-transitive dice could be a stochastic analogue of Jan-ken: the stochastic transitivity does not hold on some sets of dice, e.g., Efron's dice. Including the non-transitive dice, this paper is interested in dice sets which do not contain a useless die. In particular, we are concerned with the existence of a strongest (or weakest, symmetrically) die in a dice set under the two conditions that (1) any number appears on at most one die and at most one side, i.e., no tie break between two distinct dice, and (2) the mean pips of dice are the same. We firstly prove that a strongest die never exist if a set of n dice of m-sided is given as a partition of the set of numbers {1,\u2026,mn}. Next, we show some sufficient conditions that a strongest die exists in a dice set which is not a partition of a set of numbers. We also give some algorithms to find a strongest die in a dice set which includes given dice.",
    "code_link": ""
  },
  "aaai2022_main_choicesarenotindependentstackelbergsecuritygameswithnestedquantalresponsemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Choices Are Not Independent: Stackelberg Security Games with Nested Quantal Response Models",
    "authors": [
      "Tien Mai",
      "Arunesh Sinha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20448",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20448/20207",
    "published": "2022-02",
    "summary": "The quantal response (QR) model is widely used in Stackelberg security games (SSG) to model a bounded rational adversary. The QR model is a model of human response from among a large variety of prominent models known as discrete choice models. QR is the simplest type of discrete choice models and does not capture commonly observed phenomenon such as correlation among choices. We introduce the nested QR adversary model (based on nested logit model in discrete choice theory) in SSG which addresses shortcoming of the QR model. We present tractable approximation of the resulting equilibrium problem with nested QR adversary. We do so by deriving an interesting property of the equilibrium problem, namely a loosely coupled split into nested problems that mirrors the nested decision making by the adversary in the nested QR model. We show that each separate nested problem can be approximated efficiently and that the loosely coupled overall problem can be solved approximately by formulating it as a discretized version of a continuous dynamic program. Finally, we conduct experiments that show the scalability and parallelizability of our approach, as well as advantages of the nested QR model.",
    "code_link": ""
  },
  "aaai2022_main_strictlypropercontractfunctionscanbearbitrage-free": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Strictly Proper Contract Functions Can Be Arbitrage-Free",
    "authors": [
      "Eric Neyman",
      "Tim Roughgarden"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20449",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20449/20208",
    "published": "2022-02",
    "summary": "We consider mechanisms for truthfully eliciting probabilistic predictions from a group of experts. The standard approach --- using a proper scoring rule to separately reward each expert --- is not robust to collusion: experts may collude to misreport their beliefs in a way that guarantees them a larger total reward no matter the eventual outcome. It is a long-standing open question whether there is a truthful elicitation mechanism that makes any such collusion (also called \"arbitrage\") impossible. We resolve this question positively, exhibiting a class of strictly proper arbitrage-free contract functions. These contract functions have two parts: one ensures that the total reward of a coalition of experts depends only on the average of their reports; the other ensures that changing this average report hurts the experts under at least one outcome.",
    "code_link": ""
  },
  "aaai2022_main_characterizationofincentivecompatibilityofanex-anteconstrainedplayer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Characterization of Incentive Compatibility of an Ex-ante Constrained Player",
    "authors": [
      "Bonan Ni",
      "Pingzhong Tang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20450",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20450/20209",
    "published": "2022-02",
    "summary": "We consider a variant of the standard Bayesian mechanism, where players evaluate their outcomes and constraints in an ex-ante manner. Such a model captures a major form of modern online advertising where an advertiser is concerned with her/his expected utility over a time period and her/his type may change over time. We are interested in the incentive compatibility (IC) problem of such Bayesian mechanism. Under very mild conditions on the mechanism environments, we give a full characterization of IC via the taxation principle and show, perhaps surprisingly, that such IC mechanisms are fully characterized by the so-called auto-bidding mechanisms, which are pervasively fielded in the online advertising industry.",
    "code_link": ""
  },
  "aaai2022_main_onlineelicitationofnecessarilyoptimalmatchings": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Elicitation of Necessarily Optimal Matchings",
    "authors": [
      "Jannik Peters"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20451",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20451/20210",
    "published": "2022-02",
    "summary": "In this paper, we study the problem of eliciting preferences of agents in the house allocation model. For this we build on a recently introduced modeland focus on the task of eliciting preferences to find matchings which are necessarily optimal, i.e., optimal under all possible completions of the elicited preferences.In particular, we investigate the elicitation of necessarily Pareto optimal (NPO) and necessarily rank-maximal (NRM) matchings. Most importantly, we answer an open question and give an online algorithm for eliciting an NRM matching in the next-best query model which is3/2-competitive, i.e., it takes at most 3/2 as many queries as an optimal algorithm. Besides this, we extend this field of research by introducing two new natural models of elicitation and by studying both the complexity of determining whether a necessarily optimal matching exists in them, and by giving online algorithms for these models.",
    "code_link": ""
  },
  "aaai2022_main_generalizeddynamiccognitivehierarchymodelsforstrategicdrivingbehavior": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generalized Dynamic Cognitive Hierarchy Models for Strategic Driving Behavior",
    "authors": [
      "Atrisha Sarkar",
      "Kate Larson",
      "Krzysztof Czarnecki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20452",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20452/20211",
    "published": "2022-02",
    "summary": "While there has been an increasing focus on the use of game theoretic models for autonomous driving, empirical evidence shows that there are still open questions around dealing with the challenges of common knowledge assumptions as well as modeling bounded rationality. To address some of these practical challenges, we develop a framework of generalized dynamic cognitive hierarchy for both modelling naturalistic human driving behavior as well as behavior planning for autonomous vehicles (AV). This framework is built upon a rich model of level-0 behavior through the use of automata strategies, an interpretable notion of bounded rationality through safety and maneuver satisficing, and a robust response for planning. Based on evaluation on two large naturalistic datasets as well as simulation of critical traffic scenarios, we show that i) automata strategies are well suited for level-0 behavior in a dynamic level-k framework, and ii) the proposed robust response to a heterogeneous population of strategic and non-strategic reasoners can be an effective approach for game theoretic planning in AV.",
    "code_link": ""
  },
  "aaai2022_main_improvedmaximinguaranteesforsubadditiveandfractionallysubadditivefairallocationproblem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improved Maximin Guarantees for Subadditive and Fractionally Subadditive Fair Allocation Problem",
    "authors": [
      "Masoud Seddighin",
      "Saeed Seddighin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20453",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20453/20212",
    "published": "2022-02",
    "summary": "In this work, we study the maximin share fairness notion for allocation of indivisible goods in the subadditive and fractionally subadditive settings. While previous work refutes the possibility of obtaining an allocation which is better than 1/2-MMS, the only positive result for the subadditive setting states that when the number of items is equal to m, there always exists an \u03a9(1/log m)-\\MMS allocation. Since the number of items may be larger than the number of agents (n), such a bound can only imply a weak bound of \u03a9(1/(n log n))-MMS allocation in general.In this work, we improve this gap exponentially to an \u03a9(1/(log n log log n))-MMS guarantee. In addition to this, we prove that when the valuation functions are fractionally subadditive, a 1/4.6-MMS allocation is guaranteed to exist. This also improves upon the previous bound of 1/5-MMS guarantee for the fractionally subadditive setting.",
    "code_link": ""
  },
  "aaai2022_main_proportionalpublicdecisions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Proportional Public Decisions",
    "authors": [
      "Piotr Skowron",
      "Adrian G\u00f3recki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20454",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20454/20213",
    "published": "2022-02",
    "summary": "We consider a setting where a group of individuals needs to make a number of independent decisions. The decisions should proportionally represent the views of the voters. We formulate new criteria of proportionality and analyse two rules, Proportional Approval Voting and the Method of Equal Shares, that are inspired by the corresponding approval-based committee election rules. We prove that the two rules provide very strong proportionality guarantees when applied to the setting of public decisions.",
    "code_link": ""
  },
  "aaai2022_main_onlinetaskassignmentproblemswithreusableresources": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Task Assignment Problems with Reusable Resources",
    "authors": [
      "Hanna Sumita",
      "Shinji Ito",
      "Kei Takemura",
      "Daisuke Hatano",
      "Takuro Fukunaga",
      "Naonori Kakimura",
      "Ken-ichi Kawarabayashi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20455",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20455/20214",
    "published": "2022-02",
    "summary": "We study online task assignment problem with reusable resources, motivated by practical applications such as ridesharing, crowdsourcing and job hiring. In the problem, we are given a set of offline vertices (agents), and, at each time, an online vertex (task) arrives randomly according to a known time-dependent distribution. Upon arrival, we assign the task to agents immediately and irrevocably. The goal of the problem is to maximize the expected total profit produced by completed tasks. The key features of our problem are (1) an agent is reusable, i.e., an agent comes back to the market after completing the assigned task, (2) an agent may reject the assigned task to stay the market, and (3) a task may accommodate multiple agents. The setting generalizes that of existing work in which an online task is assigned to one agent under (1).In this paper, we propose an online algorithm that is 1/2-competitive for the above setting, which is tight. Moreover, when each agent can reject assigned tasks at most \u0394 times, the algorithm is shown to have the competitive ratio \u0394/(3\u0394-1), which is at least 1/3. We also evaluate our proposed algorithm with numerical experiments.",
    "code_link": ""
  },
  "aaai2022_main_iterativecalculusofvotingunderplurality": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Iterative Calculus of Voting under Plurality",
    "authors": [
      "Fabricio Vasselai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20456",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20456/20215",
    "published": "2022-02",
    "summary": "We formalize a voting model for plurality elections that combines Iterative Voting and Calculus of Voting. Each iteration, autonomous agents simultaneously maximize the utility they expect from candidates. Agents are aware of neither other individuals\u2019 preferences or choices, nor of the distribution of preferences. They know only of candidates\u2019 latest vote shares and with that calculate expected rewards from each candidate, pondering the probability that voting for each would alter the election. We define the general form of those pivotal probabilities, then we derive efficient exact and approximated calculations. Lastly, we prove formally the model converges with asymptotically large electorates and show via simulations that it nearly always converges even with very few agents.",
    "code_link": "https://github.com/vasselai/aaai22-icv-plurality"
  },
  "aaai2022_main_coordinatingfollowerstoreachbetterequilibriaend-to-endgradientdescentforstackelberggames": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Coordinating Followers to Reach Better Equilibria: End-to-End Gradient Descent for Stackelberg Games",
    "authors": [
      "Kai Wang",
      "Lily Xu",
      "Andrew Perrault",
      "Michael K. Reiter",
      "Milind Tambe"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20457",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20457/20216",
    "published": "2022-02",
    "summary": "A growing body of work in game theory extends the traditional Stackelberg game to settings with one leader and multiple followers who play a Nash equilibrium. Standard approaches for computing equilibria in these games reformulate the followers' best response as constraints in the leader's optimization problem. These reformulation approaches can sometimes be effective, but make limiting assumptions on the followers' objectives and the equilibrium reached by followers, e.g., uniqueness, optimism, or pessimism. To overcome these limitations, we run gradient descent to update the leader's strategy by differentiating through the equilibrium reached by followers. Our approach generalizes to any stochastic equilibrium selection procedure that chooses from multiple equilibria, where we compute the stochastic gradient by back-propagating through a sampled Nash equilibrium using the solution to a partial differential equation to establish the unbiasedness of the stochastic gradient. Using the unbiased gradient estimate, we implement the gradient-based approach to solve three Stackelberg problems with multiple followers. Our approach consistently outperforms existing baselines to achieve higher utility for the leader.",
    "code_link": ""
  },
  "aaai2022_main_multi-unitauctioninsocialnetworkswithbudgets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Unit Auction in Social Networks with Budgets",
    "authors": [
      "Mingyu Xiao",
      "Yuchao Song",
      "Bakh Khoussainov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20458",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20458/20217",
    "published": "2022-02",
    "summary": "We study multi-unit auctions in social networks, where each buyer has a fixed budget and can spread the sale information to the network neighbors. We design a mechanism encouraging buyers to report their valuations truthfully and spread the sale information. Our design uses the idea of the clinching mechanism to decide the transaction price and can be viewed as a network version of the mechanism. Most of the previous clinching mechanisms search for the transaction prices by increasing the current price. Our mechanism directly computes the transaction prices in polynomial time. Furthermore, the mechanism applies a technique to iteratively activate new buyers in the network. This ensures utility preservations of the buyers and benefits the seller. We prove key properties of our mechanism, such as no-positive-transfers, individual rationality, incentive compatibility, non-wastefulness and social welfare preservation.",
    "code_link": ""
  },
  "aaai2022_main_thestrangeroleofinformationasymmetryinauctions\u2014doesmoreaccuratevalueestimationbenefitabidder?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Strange Role of Information Asymmetry in Auctions\u2014Does More Accurate Value Estimation Benefit a Bidder?",
    "authors": [
      "Haifeng Xu",
      "Ruggiero Cavallo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20459",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20459/20218",
    "published": "2022-02",
    "summary": "We study the second-price auction in which bidders have asymmetric information regarding the item\u2019s value. Each bidder\u2019s value for the item depends on a private component and a public component. While each bidder observes their own private component, they hold different and asymmetric information about the public component. We characterize the equilibrium of this auction game and study how the asymmetric bidder information affects their equilibrium bidding strategies. We also discover multiple surprisingly counter-intuitive equilibrium phenomena. For instance, a bidder may be better off if she is less informed regarding the public component. Conversely, a bidder may sometimes be worse off if she obtains more accurate estimation about the auctioned item. Our results suggest that efforts devoted by bidders to improve their value estimations, as widely seen in today\u2019s online advertising auctions, may not always be to their benefit.",
    "code_link": ""
  },
  "aaai2022_main_autocfrlearningtodesigncounterfactualregretminimizationalgorithms": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AutoCFR: Learning to Design Counterfactual Regret Minimization Algorithms",
    "authors": [
      "Hang Xu",
      "Kai Li",
      "Haobo Fu",
      "Qiang Fu",
      "Junliang Xing"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20460",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20460/20219",
    "published": "2022-02",
    "summary": "Counterfactual regret minimization (CFR) is the most commonly used algorithm to approximately solving two-player zero-sum imperfect-information games (IIGs). In recent years, a series of novel CFR variants such as CFR+, Linear CFR, DCFR have been proposed and have significantly improved the convergence rate of the vanilla CFR. However, most of these new variants are hand-designed by researchers through trial and error based on different motivations, which generally requires a tremendous amount of efforts and insights. This work proposes to meta-learn novel CFR algorithms through evolution to ease the burden of manual algorithm design. We first design a search language that is rich enough to represent many existing hand-designed CFR variants. We then exploit a scalable regularized evolution algorithm with a bag of acceleration techniques to efficiently search over the combinatorial space of algorithms defined by this language. The learned novel CFR algorithm can generalize to new IIGs not seen during training and performs on par with or better than existing state-of-the-art CFR variants. The code is available at https://github.com/rpSebastian/AutoCFR.",
    "code_link": ""
  },
  "aaai2022_main_teamcorrelatedequilibriainzero-sumextensive-formgamesviatreedecompositions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Team Correlated Equilibria in Zero-Sum Extensive-Form Games via Tree Decompositions",
    "authors": [
      "Brian Hu Zhang",
      "Tuomas Sandholm"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20461",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20461/20220",
    "published": "2022-02",
    "summary": "Despite the many recent practical and theoretical breakthroughs in computational game theory, equilibrium finding in extensive-form team games remains a significant challenge. While NP-hard in the worst case, there are provably efficient algorithms for certain families of team game. In particular, if the game has common external information, also known as A-loss recall---informally, actions played by non-team members (i.e., the opposing team or nature) are either unknown to the entire team, or common knowledge within the team---then polynomial-time algorithms exist. In this paper, we devise a completely new algorithm for solving team games. It uses a tree decomposition of the constraint system representing each team's strategy to reduce the number and degree of constraints required for correctness (tightness of the mathematical program). Our approach has the bags of the tree decomposition correspond to team-public states---that is, minimal sets of nodes (that is, states of the team) such that, upon reaching the set, it is common knowledge among the players on the team that the set has been reached. Our algorithm reduces the problem of solving team games to a linear program with at most O(NW^(w+1)) nonzero entries in the constraint matrix, where N is the size of the game tree, w is a parameter that depends on the amount of uncommon external information, and W is the treewidth of the tree decomposition. In public-action games, our program size is bounded by the tighter 2^(O(nt))N for teams of n players with t types each. Our algorithm is based on a new way to write a custom, concise tree decomposition, and its fast run time does not assume that the decomposition has small treewidth. Since our algorithm describes the polytope of correlated strategies directly, we get equilibrium finding in correlated strategies for free---instead of, say, having to run a double oracle algorithm. We show via experiments on a standard suite of games that our algorithm achieves state-of-the-art performance on all benchmark game classes except one. We also present, to our knowledge, the first experiments for this setting where both teams have more than one member.",
    "code_link": ""
  },
  "aaai2022_main_planningwithparticipationconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Planning with Participation Constraints",
    "authors": [
      "Hanrui Zhang",
      "Yu Cheng",
      "Vincent Conitzer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20462",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20462/20221",
    "published": "2022-02",
    "summary": "We pose and study the problem of planning in Markov decision processes (MDPs), subject to participation constraints as studied in mechanism design. In this problem, a planner must work with a self-interested agent on a given MDP. Each action in the MDP provides an immediate reward to the planner and a (possibly different) reward to the agent. The agent has no control in choosing the actions, but has the option to end the entire process at any time. The goal of the planner is to find a policy that maximizes her cumulative reward, taking into consideration the agent's ability to terminate.We give a fully polynomial-time approximation scheme for this problem.En route, we present polynomial-time algorithms for computing (exact) optimal policies for important special cases of this problem, including when the time horizon is constant, or when the MDP exhibits a \"definitive decisions\" property. We illustrate our algorithms with two different game-theoretic applications: the problem of assigning rides in ride-sharing and the problem of designing screening policies. Our results imply efficient algorithms for computing (approximately) optimal policies in both applications.",
    "code_link": ""
  },
  "aaai2022_main_\u201cidon\u2019tthinkso\u201dsummarizingpolicydisagreementsforagentcomparison": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "\u201cI Don\u2019t Think So\u201d: Summarizing Policy Disagreements for Agent Comparison",
    "authors": [
      "Yotam Amitai",
      "Ofra Amir"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20463",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20463/20222",
    "published": "2022-02",
    "summary": "With Artificial Intelligence on the rise, human interaction with autonomous agents becomes more frequent. Effective human-agent collaboration requires users to understand the agent's behavior, as failing to do so may cause reduced productivity, misuse or frustration. Agent strategy summarization methods are used to describe the strategy of an agent to users through demonstrations. A summary's objective is to maximize the user's understanding of the agent's aptitude by showcasing its behaviour in a selected set of world states. While shown to be useful, we show that current methods are limited when tasked with comparing between agents, as each summary is independently generated for a specific agent. In this paper, we propose a novel method for generating dependent and contrastive summaries that emphasize the differences between agent policies by identifying states in which the agents disagree on the best course of action. We conducted user studies to assess the usefulness of disagreement-based summaries for identifying superior agents and conveying agent differences. Results show disagreement-based summaries lead to improved user performance compared to summaries generated using HIGHLIGHTS, a strategy summarization algorithm which generates summaries for each agent independently.",
    "code_link": ""
  },
  "aaai2022_main_explain,edit,andunderstandrethinkinguserstudydesignforevaluatingmodelexplanations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations",
    "authors": [
      "Siddhant Arora",
      "Danish Pruthi",
      "Norman Sadeh",
      "William W. Cohen",
      "Zachary C.\n      Lipton",
      "Graham Neubig"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20464",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20464/20223",
    "published": "2022-02",
    "summary": "In attempts to \"explain\" predictions of machine learning models, researchers have proposed hundreds of techniques for attributing predictions to features that are deemed important. While these attributions are often claimed to hold the potential to improve human \"understanding\" of the models, surprisingly little work explicitly evaluates progress towards this aspiration. In this paper, we conduct a crowdsourcing study, where participants interact with deception detection models that have been trained to distinguish between genuine and fake hotel reviews. They are challenged both to simulate the model on fresh reviews, and to edit reviews with the goal of lowering the probability of the originally predicted class. Successful manipulations would lead to an adversarial example. During the training (but not the test) phase, input spans are highlighted to communicate salience. Through our evaluation, we observe that for a linear bag-of-words model, participants with access to the feature coefficients during training are able to cause a larger reduction in model confidence in the testing phase when compared to the no-explanation control. For the BERT-based classifier, popular local explanations do not improve their ability to reduce the model confidence over the no-explanation case. Remarkably, when the explanation for the BERT model is given by the (global) attributions of a linear model trained to imitate the BERT model, people can effectively manipulate the model.",
    "code_link": ""
  },
  "aaai2022_main_roleofhuman-aiinteractioninselectiveprediction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Role of Human-AI Interaction in Selective Prediction",
    "authors": [
      "Elizabeth Bondi",
      "Raphael Koster",
      "Hannah Sheahan",
      "Martin Chadwick",
      "Yoram\n      Bachrach",
      "Taylan Cemgil",
      "Ulrich Paquet",
      "Krishnamurthy Dvijotham"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20465",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20465/20224",
    "published": "2022-02",
    "summary": "Recent work has shown the potential benefit ofselective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation. However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves. We show that this is not the case by performing experiments to quantify human-AI interaction in the context of selective prediction. In particular, we study the impact of communicating different types of information to humans about the AI system's decision to defer. Using real-world conservation data and a selective prediction system that improves expected accuracy over that of the human or AI system working individually, we show that this messaging has a significant impact on the accuracy of human judgements. Our results study two components of the messaging strategy: 1) Whether humans are informed about the prediction of the AI system and 2) Whether they are informed about the decision of the selective prediction system to defer. By manipulating these messaging components, we show that it is possible to significantly boost human performance by informing the human of the decision to defer, but not revealing theprediction of the AI. We therefore show that it is vital to consider how the decision to defer is communicated to a human when designing selective prediction systems, and that the composite accuracy of a human-AI team must be carefully evaluated using a human-in-the-loop framework.",
    "code_link": "https://github.com/deepmind/HAI"
  },
  "aaai2022_main_howgeneral-purposeisalanguagemodel?usefulnessandsafetywithhumanpromptersinthewild": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild",
    "authors": [
      "Pablo Antonio Moreno Casares",
      "Bao Sheng Loe",
      "John Burden",
      "Sean\n      hEigeartaigh",
      "Jos\u00e9 Hern\u00e1ndez-Orallo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20466",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20466/20225",
    "published": "2022-02",
    "summary": "The new generation of language models is reported to solve some extraordinary tasks the models were never trained for specifically, in few-shot or zero-shot settings. However, these reports usually cherry-pick the tasks, use the best prompts, and unwrap or extract the solutions leniently even if they are followed by nonsensical text. In sum, they are specialised results for one domain, a particular way of using the models and interpreting the results. In this paper, we present a novel theoretical evaluation framework and a distinctive experimental study assessing language models as general-purpose systems when used directly by human prompters --- in the wild. For a useful and safe interaction in these increasingly more common conditions, we need to understand when the model fails because of a lack of capability or a misunderstanding of the user's intents. Our results indicate that language models such as GPT-3 have limited understanding of the human command; far from becoming general-purpose systems in the wild.",
    "code_link": ""
  },
  "aaai2022_main_adversariallearningfromcrowds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Learning from Crowds",
    "authors": [
      "Pengpeng Chen",
      "Hailong Sun",
      "Yongqiang Yang",
      "Zhijun Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20467",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20467/20226",
    "published": "2022-02",
    "summary": "Learning from Crowds (LFC) seeks to induce a high-quality classifier from training instances, which are linked to a range of possible noisy annotations from crowdsourcing workers under their various levels of skills and their own preconditions. Recent studies on LFC focus on designing new methods to improve the performance of the classifier trained from crowdsourced labeled data.To this day, however, there remain under-explored security aspects of LFC systems. In this work, we seek to bridge this gap. We first show that LFC models are vulnerable to adversarial examples---small changes to input data can cause classifiers to make prediction mistakes. Second, we propose an approach, A-LFC for training a robust classifier from crowdsourced labeled data. Our empirical results on three real-world datasets show that the proposed approach can substantially improve the performance of the trained classifier even with the existence of adversarial examples. On average, A-LFC has 10.05% and 11.34% higher test robustness than the state-of-the-art in the white-box and black-box attack settings, respectively.",
    "code_link": "https://github.com/yongqiangyang/ALFC"
  },
  "aaai2022_main_focusflexibleoptimizablecounterfactualexplanationsfortreeensembles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FOCUS: Flexible Optimizable Counterfactual Explanations for Tree Ensembles",
    "authors": [
      "Ana Lucic",
      "Harrie Oosterhuis",
      "Hinda Haned",
      "Maarten de Rijke"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20468",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20468/20227",
    "published": "2022-02",
    "summary": "Model interpretability has become an important problem in machine learning (ML) due to the increased effect algorithmic decisions have on humans. Counterfactual explanations can help users understand not only why ML models make certain decisions, but also how these decisions can be changed. We frame the problem of finding counterfactual explanations as an optimization task and extend previous work that could only be applied to differentiable models. In order to accommodate non-differentiable models such as tree ensembles, we use probabilistic model approximations in the optimization framework.We introduce an approximation technique that is effective for finding counterfactual explanations for predictions of the original model and show that our counterfactual examples are significantly closer to the original instances than those produced by other methods specifically designed for tree ensembles.",
    "code_link": ""
  },
  "aaai2022_main_teachinghumanswhentodefertoaclassifierviaexemplars": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Teaching Humans When to Defer to a Classifier via Exemplars",
    "authors": [
      "Hussein Mozannar",
      "Arvind Satyanarayan",
      "David Sontag"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20469",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20469/20228",
    "published": "2022-02",
    "summary": "Expert decision makers are starting to rely on data-driven automated agents to assist them with various tasks. For this collaboration to perform properly, the human decision maker must have a mental model of when and when not to rely on the agent. In this work, we aim to ensure that human decision makers learn a valid mental model of the agent's strengths and weaknesses. To accomplish this goal, we propose an exemplar-based teaching strategy where humans solve a set of selected examples and with our help generalize from them to the domain. We present a novel parameterization of the human's mental model of the AI that applies a nearest neighbor rule in local regions surrounding the teaching examples. Using this model, we derive a near-optimal strategy for selecting a representative teaching set. We validate the benefits of our teaching strategy on a multi-hop question answering task with an interpretable AI model using crowd workers. We find that when workers draw the right lessons from the teaching stage, their task performance improves. We furthermore validate our method on a set of synthetic experiments.",
    "code_link": ""
  },
  "aaai2022_main_deceptivedecision-makingunderuncertainty": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deceptive Decision-Making under Uncertainty",
    "authors": [
      "Yagiz Savas",
      "Christos K. Verginis",
      "Ufuk Topcu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20470",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20470/20229",
    "published": "2022-02",
    "summary": "We study the design of autonomous agents that are capable of deceiving outside observers about their intentions while carrying out tasks in stochastic, complex environments. By modeling the agent's behavior as a Markov decision process, we consider a setting where the agent aims to reach one of multiple potential goals while deceiving outside observers about its true goal. We propose a novel approach to model observer predictions based on the principle of maximum entropy and to efficiently generate deceptive strategies via linear programming. The proposed approach enables the agent to exhibit a variety of tunable deceptive behaviors while ensuring the satisfaction of probabilistic constraints on the behavior. We evaluate the performance of the proposed approach via comparative user studies and present a case study on the streets of Manhattan, New York, using real travel time distributions.",
    "code_link": ""
  },
  "aaai2022_main_onoptimizinginterventionsinsharedautonomy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On Optimizing Interventions in Shared Autonomy",
    "authors": [
      "Weihao Tan",
      "David Koleczek",
      "Siddhant Pradhan",
      "Nicholas Perello",
      "Vivek\n      Chettiar",
      "Vishal Rohra",
      "Aaslesha Rajaram",
      "Soundararajan Srinivasan",
      "H M\n      Sajjad Hossain",
      "Yash Chandak"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20471",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20471/20230",
    "published": "2022-02",
    "summary": "Shared autonomy refers to approaches for enabling an autonomous agent to collaborate with a human with the aim of improving human performance. However, besides improving performance, it may often also be beneficial that the agent concurrently accounts for preserving the user\u2019s experience or satisfaction of collaboration. In order to address this additional goal, we examine approaches for improving the user experience by constraining the number of interventions by the autonomous agent. We propose two model-free reinforcement learning methods that can account for both hard and soft constraints on the number of interventions. We show that not only does our method outperform the existing baseline, but also eliminates the need to manually tune a black-box hyperparameter for controlling the level of assistance. We also provide an in-depth analysis of intervention scenarios in order to further illuminate system understanding.",
    "code_link": "https://github.com/DavidKoleczek/human_marl"
  },
  "aaai2022_main_openvocabularyelectroencephalography-to-textdecodingandzero-shotsentimentclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Open Vocabulary Electroencephalography-to-Text Decoding and Zero-Shot Sentiment Classification",
    "authors": [
      "Zhenhailong Wang",
      "Heng Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20472",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20472/20231",
    "published": "2022-02",
    "summary": "State-of-the-art brain-to-text systems have achieved great success in decoding language directly from brain signals using neural networks. However, current approaches are limited to small closed vocabularies which are far from enough for natural communication. In addition, most of the high-performing approaches require data from invasive devices (e.g., ECoG). In this paper, we extend the problem to open vocabulary Electroencephalography(EEG)-To-Text Sequence-To-Sequence decoding and zero-shot sentence sentiment classification on natural reading tasks. We hypothesis that the human brain functions as a special text encoder and propose a novel framework leveraging pre-trained language models (e.g., BART). Our model achieves a 40.1% BLEU-1 score on EEG-To-Text decoding and a 55.6% F1 score on zero-shot EEG-based ternary sentiment classification, which significantly outperforms supervised baselines. Furthermore, we show that our proposed model can handle data from various subjects and sources, showing great potential for a high-performance open vocabulary brain-to-text system once sufficient data is available. The code is made publicly available for research purpose at https://github.com/MikeWangWZHL/EEG-To-Text.",
    "code_link": ""
  },
  "aaai2022_main_deepvisualinsighttime-travellingvisualizationforspatio-temporalcausalityofdeepclassificationtraining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training",
    "authors": [
      "Xianglin Yang",
      "Yun Lin",
      "Ruofan Liu",
      "Zhenfeng He",
      "Chao Wang",
      "Jin Song Dong",
      "Hong Mei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20473",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20473/20232",
    "published": "2022-02",
    "summary": "Understanding how the predictions of deep learning models are formed during the training process is crucial to improve model performance and fix model defects, especially when we need to investigate nontrivial training strategies such as active learning, and track the root cause of unexpected training results such as performance degeneration.In this work, we propose a time-travelling visual solution DeepVisualInsight (DVI), aiming to manifest the spatio-temporal causality while training a deep learning image classifier. The spatio-temporal causality demonstrates how the gradient-descent algorithm and various training data sampling techniques can influence and reshape the layout of learnt input representation and the classification boundaries in consecutive epochs. Such causality allows us to observe and analyze the whole learning process in the visible low dimensional space. Technically, we propose four spatial and temporal properties and design our visualization solution to satisfy them. These properties preserve the most important information when projecting and inverse-projecting input samples between the visible low-dimensional and the invisible high-dimensional space, for causal analyses. Our extensive experiments show that, comparing to baseline approaches, we achieve the best visualization performance regarding the spatial/temporal properties and visualization efficiency. Moreover, our case study shows that our visual solution can well reflect the characteristics of various training scenarios, showing good potential of DVI as a debugging tool for analyzing deep learning training processes.",
    "code_link": ""
  },
  "aaai2022_main_whenfacialexpressionrecognitionmeetsfew-shotlearningajointandalternatelearningframework": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "When Facial Expression Recognition Meets Few-Shot Learning: A Joint and Alternate Learning Framework",
    "authors": [
      "Xinyi Zou",
      "Yan Yan",
      "Jing-Hao Xue",
      "Si Chen",
      "Hanzi Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20474",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20474/20233",
    "published": "2022-02",
    "summary": "Human emotions involve basic and compound facial expressions. However, current research on facial expression recognition (FER) mainly focuses on basic expressions, and thus fails to address the diversity of human emotions in practical scenarios. Meanwhile, existing work on compound FER relies heavily on abundant labeled compound expression training data, which are often laboriously collected under the professional instruction of psychology. In this paper, we study compound FER in the cross-domain few-shot learning setting, where only a few images of novel classes from the target domain are required as a reference. In particular, we aim to identify unseen compound expressions with the model trained on easily accessible basic expression datasets. To alleviate the problem of limited base classes in our FER task, we propose a novel Emotion Guided Similarity Network (EGS-Net), consisting of an emotion branch and a similarity branch, based on a two-stage learning framework. Specifically, in the first stage, the similarity branch is jointly trained with the emotion branch in a multi-task fashion. With the regularization of the emotion branch, we prevent the similarity branch from overfitting to sampled base classes that are highly overlapped across different episodes. In the second stage, the emotion branch and the similarity branch play a \u201ctwo-student game\u201d to alternately learn from each other, thereby further improving the inference ability of the similarity branch on unseen compound expressions. Experimental results on both in-the-lab and in-the-wild compound expression datasets demonstrate the superiority of our proposed method against several state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_discoveringstateandactionabstractionsforgeneralizedtaskandmotionplanning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Discovering State and Action Abstractions for Generalized Task and Motion Planning",
    "authors": [
      "Aidan Curtis",
      "Tom Silver",
      "Joshua B. Tenenbaum",
      "Tom\u00e1s Lozano-P\u00e9rez",
      "Leslie\n      Kaelbling"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20475",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20475/20234",
    "published": "2022-02",
    "summary": "Generalized planning accelerates classical planning by finding an algorithm-like policy that solves multiple instances of a task. A generalized plan can be learned from a few training examples and applied to an entire domain of problems. Generalized planning approaches perform well in discrete AI planning problems that involve large numbers of objects and extended action sequences to achieve the goal. In this paper, we propose an algorithm for learning features, abstractions, and generalized plans for continuous robotic task and motion planning (TAMP) and examine the unique difficulties that arise when forced to consider geometric and physical constraints as a part of the generalized plan. Additionally, we show that these simple generalized plans learned from only a handful of examples can be used to improve the search efficiency of TAMP solvers.",
    "code_link": ""
  },
  "aaai2022_main_recurrentneuralnetworkcontrollerssynthesiswithstabilityguaranteesforpartiallyobservedsystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Recurrent Neural Network Controllers Synthesis with Stability Guarantees for Partially Observed Systems",
    "authors": [
      "Fangda Gu",
      "He Yin",
      "Laurent El Ghaoui",
      "Murat Arcak",
      "Peter Seiler",
      "Ming Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20476",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20476/20235",
    "published": "2022-02",
    "summary": "Neural network controllers have become popular in control tasks thanks to their flexibility and expressivity. Stability is a crucial property for safety-critical dynamical systems, while stabilization of partially observed systems, in many cases, requires controllers to retain and process long-term memories of the past. We consider the important class of recurrent neural networks (RNN) as dynamic controllers for nonlinear uncertain partially-observed systems, and derive convex stability conditions based on integral quadratic constraints, S-lemma and sequential convexification. To ensure stability during the learning and control process, we propose a projected policy gradient method that iteratively enforces the stability conditions in the reparametrized space taking advantage of mild additional information on system dynamics. Numerical experiments show that our method learns stabilizing controllers with fewer samples and achieves higher final performance compared with policy gradient.",
    "code_link": "https://github.com/beeperman/IQCRNN"
  },
  "aaai2022_main_randommappingmethodforlarge-scaleterrainmodeling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Random Mapping Method for Large-Scale Terrain Modeling",
    "authors": [
      "Xu Liu",
      "Decai Li",
      "Yuqing He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20477",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20477/20236",
    "published": "2022-02",
    "summary": "The vast amount of data captured by robots in large-scale environments brings the computing and storage bottlenecks to the typical methods of modeling the spaces the robots travel in. In order to efficiently construct a compact terrain model from uncertain, incomplete point cloud data of large-scale environments, in this paper, we first propose a novel feature mapping method, named random mapping, based on the fast random construction of base functions, which can efficiently project the messy points in the low-dimensional space into the high-dimensional space where the points are approximately linearly distributed. Then, in this mapped space, we propose to learn a continuous linear regression model to represent the terrain. We show that this method can model the environments in much less computation time, memory consumption, and access time, with high accuracy. Furthermore, the models possess the generalization capabilities comparable to the performances on the training set, and its inference accuracy gradually increases as the random mapping dimension increases. To better solve the large-scale environmental modeling problem, we adopt the idea of parallel computing to train the models. This strategy greatly reduces the wall-clock time of calculation without losing much accuracy. Experiments show the effectiveness of the random mapping method and the effects of some important parameters on its performance. Moreover, we evaluate the proposed terrain modeling method based on the random mapping method and compare its performances with popular typical methods and state-of-art methods.",
    "code_link": "https://github.com/LiuXuSIA/rmm"
  },
  "aaai2022_main_conservativeandadaptivepenaltyformodel-basedsafereinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning",
    "authors": [
      "Yecheng Jason Ma",
      "Andrew Shen",
      "Osbert Bastani",
      "Jayaraman Dinesh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20478",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20478/20237",
    "published": "2022-02",
    "summary": "Reinforcement Learning (RL) agents in the real world must satisfy safety constraints in addition to maximizing a reward objective. Model-based RL algorithms hold promise for reducing unsafe real-world actions: they may synthesize policies that obey all constraints using simulated samples from a learned model. However, imperfect models can result in real-world constraint violations even for actions that are predicted to satisfy all constraints. We propose Conservative and Adaptive Penalty (CAP), a model-based safe RL framework that accounts for potential modeling errors by capturing model uncertainty and adaptively exploiting it to balance the reward and the cost objectives. First, CAP inflates predicted costs using an uncertainty-based penalty. Theoretically, we show that policies that satisfy this conservative cost constraint are guaranteed to also be feasible in the true environment. We further show that this guarantees the safety of all intermediate solutions during RL training. Further, CAP adaptively tunes this penalty during training using true cost feedback from the environment. We evaluate this conservative and adaptive penalty-based approach for model-based safe RL extensively on state and image-based environments. Our results demonstrate substantial gains in sample-efficiency while incurring fewer violations than prior safe RL algorithms. Code is available at: https://github.com/Redrew/CAP",
    "code_link": ""
  },
  "aaai2022_main_ctinrobustcontextualtransformernetworkforinertialnavigation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CTIN: Robust Contextual Transformer Network for Inertial Navigation",
    "authors": [
      "Bingbing Rao",
      "Ehsan Kazemi",
      "Yifan Ding",
      "Devu M Shila",
      "Frank M Tucker",
      "Liqiang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20479",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20479/20238",
    "published": "2022-02",
    "summary": "Recently, data-driven inertial navigation approaches have demonstrated their capability of using well-trained neural networks to obtain accurate position estimates from inertial measurement units (IMUs) measurements. In this paper, we propose a novel robust Contextual Transformer-based network for Inertial Navigation (CTIN) to accurately predict velocity and trajectory. To this end, we first design a ResNet-based encoder enhanced by local and global multi-head self-attention to capture spatial contextual information from IMU measurements. Then we fuse these spatial representations with temporal knowledge by leveraging multi-head attention in the Transformer decoder. Finally, multi-task learning with uncertainty reduction is leveraged to improve learning efficiency and prediction accuracy of velocity and trajectory. Through extensive experiments over a wide range of inertial datasets (e.g., RIDI, OxIOD, RoNIN, IDOL, and our own), CTIN is very robust and outperforms state-of-the-art models.",
    "code_link": ""
  },
  "aaai2022_main_monocularcamera-basedpoint-goalnavigationbylearningdepthchannelandcross-modalitypyramidfusion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Monocular Camera-Based Point-Goal Navigation by Learning Depth Channel and Cross-Modality Pyramid Fusion",
    "authors": [
      "Tianqi Tang",
      "Heming Du",
      "Xin Yu",
      "Yi Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20480",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20480/20239",
    "published": "2022-02",
    "summary": "For a monocular camera-based navigation system, if we could effectively explore scene geometric cues from RGB images, the geometry information will significantly facilitate the efficiency of the navigation system. Motivated by this, we propose a highly efficient point-goal navigation framework, dubbed Geo-Nav. In a nutshell, our Geo-Nav consists of two parts: a visual perception part and a navigation part. In the visual perception part, we firstly propose a Self-supervised Depth Estimation network (SDE) specially tailored for the monocular camera-based navigation agent. Our SDE learns a mapping from an RGB input image to its corresponding depth image by exploring scene geometric constraints in a self-consistency manner. Then, in order to achieve a representative visual representation from the RGB inputs and learned depth images, we propose a Cross-modality Pyramid Fusion module (CPF). Concretely, our CPF computes a patch-wise cross-modality correlation between different modal features and exploits the correlation to fuse and enhance features at each scale. Thanks to the patch-wise nature of our CPF, we can fuse feature maps at high resolution, allowing our visual network to perceive more image details. In the navigation part, our extracted visual representations are fed to a navigation policy network to learn how to map the visual representations to agent actions effectively. Extensive experiments on a widely-used multiple-room environment Gibson demonstrate that Geo-Nav outperforms the state-of-the-art in terms of efficiency and effectiveness.",
    "code_link": ""
  },
  "aaai2022_main_robustadversarialreinforcementlearningwithdissipationinequationconstraint": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Adversarial Reinforcement Learning with Dissipation Inequation Constraint",
    "authors": [
      "Peng Zhai",
      "Jie Luo",
      "Zhiyan Dong",
      "Lihua Zhang",
      "Shunli Wang",
      "Dingkang Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20481",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20481/20240",
    "published": "2022-02",
    "summary": "Robust adversarial reinforcement learning is an effective method to train agents to manage uncertain disturbance and modeling errors in real environments. However, for systems that are sensitive to disturbances or those that are difficult to stabilize, it is easier to learn a powerful adversary than establish a stable control policy. An improper strong adversary can destabilize the system, introduce biases in the sampling process, make the learning process unstable, and even reduce the robustness of the policy. In this study, we consider the problem of ensuring system stability during training in the adversarial reinforcement learning architecture. The dissipative principle of robust H-in\ufb01nity control is extended to the Markov Decision Process, and robust stability constraints are obtained based on L2 gain performance in the reinforcement learning system. Thus, we propose a dissipation-inequation-constraint-based adversarial reinforcement learning architecture. This architecture ensures the stability of the system during training by imposing constraints on the normal and adversarial agents. Theoretically, this architecture can be applied to a large family of deep reinforcement learning algorithms. Results of experiments in MuJoCo and GymFc environments show that our architecture effectively improves the robustness of the controller against environmental changes and adapts to more powerful adversaries. Results of the flight experiments on a real quadcopter indicate that our method can directly deploy the policy trained in the simulation environment to the real environment, and our controller outperforms the PID controller based on hardware-in-the-loop. Both our theoretical and empirical results provide new and critical outlooks on the adversarial reinforcement learning architecture from a rigorous robust control perspective.",
    "code_link": ""
  },
  "aaai2022_main_sim2realobject-centrickeypointdetectionanddescription": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sim2Real Object-Centric Keypoint Detection and Description",
    "authors": [
      "Chengliang Zhong",
      "Chao Yang",
      "Fuchun Sun",
      "Jinshan Qi",
      "Xiaodong Mu",
      "Huaping\n      Liu",
      "Wenbing Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20482",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20482/20241",
    "published": "2022-02",
    "summary": "Keypoint detection and description play a central role in computer vision. Most existing methods are in the form of scene-level prediction, without returning the object classes of different keypoints. In this paper, we propose the object-centric formulation, which, beyond the conventional setting, requires further identifying which object each interest point belongs to. With such fine-grained information, our framework enables more downstream potentials, such as object-level matching and pose estimation in a clustered environment. To get around the difficulty of label collection in the real world, we develop a sim2real contrastive learning mechanism that can generalize the model trained in simulation to real-world applications. The novelties of our training method are three-fold: (i) we integrate the uncertainty into the learning framework to improve feature description of hard cases, e.g., less-textured or symmetric patches; (ii) we decouple the object descriptor into two independent branches, intra-object salience and inter-object distinctness, resulting in a better pixel-wise description; (iii) we enforce cross-view semantic consistency for enhanced robustness in representation learning. Comprehensive experiments on image matching and 6D pose estimation verify the encouraging generalization ability of our method. Particularly for 6D pose estimation, our method significantly outperforms typical unsupervised/sim2real methods, achieving a closer gap with the fully supervised counterpart.",
    "code_link": ""
  },
  "aaai2022_main_incompleteargumentationframeworkspropertiesandcomplexity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Incomplete Argumentation Frameworks: Properties and Complexity",
    "authors": [
      "Gianvincenzo Alfano",
      "Sergio Greco",
      "Francesco Parisi",
      "Irina Trubitsyna"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20483",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20483/20242",
    "published": "2022-02",
    "summary": "Dung\u2019s Argumentation Framework (AF) has been extended in several directions, including the possibility of representing unquantified uncertainty about the existence of arguments and attacks. The framework resulting from such an extension is called incomplete AF (iAF). In this paper, we first introduce three new satisfaction problems named totality, determinism and functionality, and investigate their computational complexity for both AF and iAF under several semantics. We also investigate the complexity of credulous and skeptical acceptance in iAF under semi-stable semantics\u2014a problem left open in the literature. We then show that any iAF can be rewritten into an equivalent one where either only (unattacked) arguments or only attacks are uncertain.Finally, we relate iAF to probabilistic argumentation framework, where uncertainty is quantified.",
    "code_link": ""
  },
  "aaai2022_main_tradingcomplexityforsparsityinrandomforestexplanations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Trading Complexity for Sparsity in Random Forest Explanations",
    "authors": [
      "Gilles Audemard",
      "Steve Bellart",
      "Lou\u00e8nas Bounia",
      "Fr\u00e9d\u00e9ric Koriche",
      "Jean-Marie Lagniez",
      "Pierre Marquis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20484",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20484/20243",
    "published": "2022-02",
    "summary": "Random forests have long been considered as powerful model ensembles in machine learning. By training multiple decision trees, whose diversity is fostered through data and feature subsampling, the resulting random forest can lead to more stable and reliable predictions than a single decision tree. This however comes at the cost of decreased interpretability: while decision trees are often easily interpretable, the predictions made by random forests are much more difficult to understand, as they involve a majority vote over multiple decision trees. In this paper, we examine different types of reasons that explain \"why\" an input instance is classified as positive or negative by a Boolean random forest. Notably, as an alternative to prime-implicant explanations taking the form of subset-minimal implicants of the random forest, we introduce majoritary reasons which are subset-minimal implicants of a strict majority of decision trees. For these abductive explanations, the tractability of the generation problem (finding one reason) and the optimization problem (finding one minimum-sized reason) are investigated. Unlike prime-implicant explanations, majoritary reasons may contain redundant features. However, in practice, prime-implicant explanations - for which the identification problem is DP-complete - are slightly larger than majoritary reasons that can be generated using a simple linear-time greedy algorithm. They are also significantly larger than minimum-sized majoritary reasons which can be approached using an anytime Partial MaxSAT algorithm.",
    "code_link": ""
  },
  "aaai2022_main_fromactionstoprogramsasabstractactualcauses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "From Actions to Programs as Abstract Actual Causes",
    "authors": [
      "Bita Banihashemi",
      "Shakil M. Khan",
      "Mikhail Soutchanski"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20485",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20485/20244",
    "published": "2022-02",
    "summary": "Causality plays a central role in reasoning about observations. In many cases, it might be useful to define the conditions under which a non-deterministic program can be called an actual cause of an effect in a setting where a sequence of programs are executed one after another. There can be two perspectives, one where at least one execution of the program leads to the effect, and another where all executions do so. The former captures a ''weak'' notion of causation and is more general than the latter stronger notion. In this paper, we give a definition of weak potential causes. Our analysis is performed within the situation calculus basic action theories and we consider programs formulated in the logic programming language ConGolog. Within this setting, we show how one can utilize a recently developed abstraction framework to relate causes at various levels of abstraction, which facilitates reasoning about programs as causes.",
    "code_link": ""
  },
  "aaai2022_main_equivalenceinargumentationframeworkswithaclaim-centricview\u2013classicalresultswithnovelingredients": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Equivalence in Argumentation Frameworks with a Claim-Centric View \u2013 Classical Results with Novel Ingredients",
    "authors": [
      "Ringo Baumann",
      "Anna Rapberger",
      "Markus Ulbricht"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20486",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20486/20245",
    "published": "2022-02",
    "summary": "A common feature of non-monotonic logics is that the classical notion of equivalence does not preserve the intended meaning in light of additional information. Consequently, the term strong equivalence was coined in the literature and thoroughly investigated. In the present paper, the knowledge representation formalism under consideration are claim-augmented argumentation frameworks (CAFs) which provide a formal basis to analyze conclusion-oriented problems in argumentation by adapting a claim-focused perspective. CAFs extend Dung AFs by associating a claim to each argument representing its conclusion. In this paper, we investigate both ordinary and strong equivalence in CAFs. Thereby, we take the fact into account that one might either be interested in the actual arguments or their claims only. The former point of view naturally yields an extension of strong equivalence for AFs to the claim-based setting while the latter gives rise to a novel equivalence notion which is genuine for CAFs. We tailor, examine and compare these notions and obtain a comprehensive study of this matter for CAFs. We conclude by investigating the computational complexity of naturally arising decision problems.",
    "code_link": ""
  },
  "aaai2022_main_finiteentailmentoflocalqueriesinthezfamilyofdescriptionlogics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Finite Entailment of Local Queries in the Z Family of Description Logics",
    "authors": [
      "Bartosz Bednarczyk",
      "Emanuel Kiero\u0144ski"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20487",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20487/20246",
    "published": "2022-02",
    "summary": "In the last few years the field of logic-based knowledge representation took a lot of inspiration from database theory. A vital example is that the finite model semantics in description logics (DLs) is reconsidered as a desirable alternative to the classical one and that query entailment has replaced knowledge-base satisfiability (KBSat) checking as the key inference problem. However, despite the considerable effort, the overall picture concerning finite query answering in DLs is still incomplete. In this work we study the complexity of finite entailment of local queries (conjunctive queries and positive boolean combinations thereof) in the Z family of DLs, one of the most powerful KR formalisms, lying on the verge of decidability. Our main result is that the DLs ZOQ and ZOI are finitely controllable, i.e. that their finite and unrestricted entailment problems for local queries coincide. This allows us to reuse recently established upper bounds on querying these logics under the classical semantics. While we will not solve finite query entailment for the third main logic in the Z family, ZIQ, we provide a generic reduction from the finite entail- ment problem to the finite KBSat problem, working for ZIQ and some of its sublogics. Our proofs unify and solidify previously established results on finite satisfiability and finite query entailment for many known DLs.",
    "code_link": ""
  },
  "aaai2022_main_thepriceofselfishnessconjunctivequeryentailmentforalcselfis2exptime-hard": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Price of Selfishness: Conjunctive Query Entailment for ALCSelf Is 2EXPTIME-Hard",
    "authors": [
      "Bartosz Bednarczyk",
      "Sebastian Rudolph"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20488",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20488/20247",
    "published": "2022-02",
    "summary": "In logic-based knowledge representation, query answering has essentially replaced mere satisfiability checking as the inferencing problem of primary interest. For knowledge bases in the basic description logic ALC, the computational complexity of conjunctive query (CQ) answering is well known to be EXPTIME-complete and hence not harder than satisfiability. This does not change when the logic is extended by certain features (such as counting or role hierarchies), whereas adding others (inverses, nominals or transitivity together with role-hierarchies) turns CQ answering exponentially harder.We contribute to this line of results by showing the surprising fact that even extending ALC by just the Self operator \u2013 which proved innocuous in many other contexts \u2013 increases the complexity of CQ entailment to 2EXPTIME. As common for this type of problem, our proof establishes a reduction from alternating Turing machines running in exponential space, but several novel ideas and encoding tricks are required to make the approach work in that specific, restricted setting.",
    "code_link": ""
  },
  "aaai2022_main_expressivityofplanningwithhorndescriptionlogicontologies": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Expressivity of Planning with Horn Description Logic Ontologies",
    "authors": [
      "Stefan Borgwardt",
      "J\u00f6rg Hoffmann",
      "Alisa Kovtunova",
      "Markus Kr\u00f6tzsch",
      "Bernhard Nebel",
      "Marcel Steinmetz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20489",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20489/20248",
    "published": "2022-02",
    "summary": "State constraints in AI Planning globally restrict the legal environment states. Standard planning languages make closed-domain and closed-world assumptions. Here we address open-world state constraints formalized by planning over a description logic (DL) ontology. Previously, this combination of DL and planning has been investigated for the light-weight DL DL-Lite. Here we propose a novel compilation scheme into standard PDDL with derived predicates, which applies to more expressive DLs and is based on the rewritability of DL queries into Datalog with stratified negation. We also provide a new rewritability result for the DL Horn-ALCHOIQ, which allows us to apply our compilation scheme to quite expressive ontologies. In contrast, we show that in the slight extension Horn-SROIQ no such compilation is possible unless the weak exponential hierarchy collapses. Finally, we show that our approach can outperform previous work on existing benchmarks for planning with DL ontologies, and is feasible on new benchmarks taking advantage of more expressive ontologies.",
    "code_link": "https://github.com/ghxiao/clipper"
  },
  "aaai2022_main_erequivarianceregularizerforknowledgegraphcompletion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ER: Equivariance Regularizer for Knowledge Graph Completion",
    "authors": [
      "Zongsheng Cao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Qingming Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20490",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20490/20249",
    "published": "2022-02",
    "summary": "Tensor factorization and distanced based models play important roles in knowledge graph completion (KGC). However, the relational matrices in KGC methods often induce a high model complexity, bearing a high risk of overfitting. As a remedy, researchers propose a variety of different regularizers such as the tensor nuclear norm regularizer. Our motivation is based on the observation that the previous work only focuses on the \u201csize\u201d of the parametric space, while leaving the implicit semantic information widely untouched. To address this issue, we propose a new regularizer, namely, Equivariance Regularizer (ER), which can suppress overfitting by leveraging the implicit semantic information. Specifically, ER can enhance the generalization ability of the model by employing the semantic equivariance between the head and tail entities. Moreover, it is a generic solution for both distance based models and tensor factorization based models. Our experimental results indicate a clear and substantial improvement over the state-of-the-art relation prediction methods.",
    "code_link": "https://github.com/Lion-ZS/ER"
  },
  "aaai2022_main_geometryinteractionknowledgegraphembeddings": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Geometry Interaction Knowledge Graph Embeddings",
    "authors": [
      "Zongsheng Cao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20491",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20491/20250",
    "published": "2022-02",
    "summary": "Knowledge graph (KG) embeddings have shown great power in learning representations of entities and relations for link prediction tasks. Previous work usually embeds KGs into a single geometric space such as Euclidean space (zero curved), hyperbolic space (negatively curved) or hyperspherical space (positively curved) to maintain their specific geometric structures (e.g., chain, hierarchy and ring structures). However, the topological structure of KGs appears to be complicated, since it may contain multiple types of geometric structures simultaneously. Therefore, embedding KGs in a single space, no matter the Euclidean space, hyperbolic space or hyperspheric space, cannot capture the complex structures of KGs accurately. To overcome this challenge, we propose Geometry Interaction knowledge graph Embeddings (GIE), which learns spatial structures interactively between the Euclidean, hyperbolic and hyperspherical spaces. Theoretically, our proposed GIE can capture a richer set of relational information, model key inference patterns, and enable expressive semantic matching across entities. Experimental results on three well-established knowledge graph completion benchmarks show that our GIE achieves the state-of-the-art performance with fewer parameters.",
    "code_link": "https://github.com/Lion-ZS/GIE"
  },
  "aaai2022_main_multi-relationalgraphrepresentationlearningwithbayesiangaussianprocessnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Relational Graph Representation Learning with Bayesian Gaussian Process Network",
    "authors": [
      "Guanzheng Chen",
      "Jinyuan Fang",
      "Zaiqiao Meng",
      "Qiang Zhang",
      "Shangsong Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20492",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20492/20251",
    "published": "2022-02",
    "summary": "Learning effective representations of entities and relations for knowledge graphs (KGs) is critical to the success of many multi-relational learning tasks. Existing methods based on graph neural networks learn a deterministic embedding function, which lacks sufficient flexibility to explore better choices when dealing with the imperfect and noisy KGs such as the scarce labeled nodes and noisy graph structure. To this end, we propose a novel multi-relational graph Gaussian Process network (GGPN), which aims to improve the flexibility of deterministic methods by simultaneously learning a family of embedding functions, i.e., a stochastic embedding function. Specifically, a Bayesian Gaussian Process (GP) is proposed to model the distribution of this stochastic function and the resulting representations are obtained by aggregating stochastic function values, i.e., messages, from neighboring entities. The two problems incurred when leveraging GP in GGPN are the proper choice of kernel function and the cubic computational complexity. To address the first problem, we further propose a novel kernel function that can explicitly take the diverse relations between each pair of entities into account and be adaptively learned in a data-driven way. We address the second problem by reformulating GP as a Bayesian linear model, resulting in a linear computational complexity. With these two solutions, our GGPN can be efficiently trained in an end-to-end manner. We evaluate our GGPN in link prediction and entity classification tasks, and the experimental results demonstrate the superiority of our method. Our code is available at https://github.com/sysu-gzchen/GGPN.",
    "code_link": "https://github.com/sysu-gzchen/GGPN"
  },
  "aaai2022_main_asp-baseddeclarativeprocessmining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ASP-Based Declarative Process Mining",
    "authors": [
      "Francesco Chiariello",
      "Fabrizio Maria Maggi",
      "Fabio Patrizi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20493",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20493/20252",
    "published": "2022-02",
    "summary": "We put forward Answer Set Programming (ASP) as a solution approach for three classical problems in Declarative Process Mining: Log Generation, Query Checking, and Conformance Checking. These problems correspond to different ways of analyzing business processes under execution, starting from sequences of recorded events, a.k.a. event logs. We tackle them in their data-aware variant, i.e., by considering events that carry a payload (set of attribute-value pairs), in addition to the performed activity, specifying processes declaratively with an extension of linear-time temporal logic over finite traces (LTLf). The data-aware setting is significantly more challenging than the control-flow one: Query Checking is still open, while the existing approaches for the other two problems do not scale well. The contributions of the work include an ASP encoding schema for the three problems, their solution, and experiments showing the feasibility of the approach.",
    "code_link": "https://github.com/fracchiariello/process-mining-ASP"
  },
  "aaai2022_main_ontestingfordiscriminationusingcausalmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On Testing for Discrimination Using Causal Models",
    "authors": [
      "Hana Chockler",
      "Joseph Y. Halpern"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20494",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20494/20253",
    "published": "2022-02",
    "summary": "Consider a bank that uses an AI system to decide which loan applications to approve. We want to ensure that the system is fair, that is, it does not discriminate against applicants based on a predefined list of sensitive attributes, such as gender and ethnicity. We expect there to be a regulator whose job it is to certify the bank\u2019s system as fair or unfair. We consider issues that the regulator will have to confront when making such a decision, including the precise definition of fairness, dealing with proxy variables, and dealing with what we call allowed variables, that is, variables such as salary on which the decision is allowed to depend, despite being correlated with sensitive variables. We show (among other things) that the problem of deciding fairness as we have defined it is co-NP-complete, but then argue that, despite that, in practice the problem should be manageable.",
    "code_link": ""
  },
  "aaai2022_main_monotoneabstractionsinontology-baseddatamanagement": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Monotone Abstractions in Ontology-Based Data Management",
    "authors": [
      "Gianluca Cima",
      "Marco Console",
      "Maurizio Lenzerini",
      "Antonella Poggi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20495",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20495/20254",
    "published": "2022-02",
    "summary": "In Ontology-Based Data Management (OBDM), an abstraction of a source query q is a query over the ontology capturing the semantics of q in terms of the concepts and the relations available in the ontology. Since a perfect characterization of a source query may not exist, the notions of best sound and complete approximations of an abstraction have been introduced and studied in the typical OBDM context, i.e., in the case where the ontology is expressed in DL-Lite, and source queries are expressed as unions of conjunctive queries (UCQs). Interestingly, if we restrict our attention to abstractions expressed as UCQs, even best approximations of abstractions are not guaranteed to exist. Thus, a natural question to ask is whether such limitations affect even larger classes of queries. In this paper, we answer this fundamental question for an essential class of queries, namely the class of monotone queries. We define a monotone query language based on disjunctive Datalog enriched with an epistemic operator, and show that its expressive power suffices for expressing the best approximations of monotone abstractions of UCQs.",
    "code_link": ""
  },
  "aaai2022_main_lowerboundsonintermediateresultsinbottom-upknowledgecompilation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Lower Bounds on Intermediate Results in Bottom-Up Knowledge Compilation",
    "authors": [
      "Alexis de Colnet",
      "Stefan Mengel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20496",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20496/20255",
    "published": "2022-02",
    "summary": "Bottom-up knowledge compilation is a paradigm for generating representations of functions by iteratively conjoining constraints using a so-called apply function. When the input is not efficiently compilable into a language - generally a class of circuits - because optimal compiled representations are provably large, the problem is not the compilation algorithm as much as the choice of a language too restrictive for the input. In contrast, in this paper, we look at CNF formulas for which very small circuits exists and look at the efficiency of their bottom-up compilation in one of the most general languages, namely that of structured decomposable negation normal forms (str-DNNF). We prove that, while the inputs have constant size representations as str-DNNF, any bottom-up compilation in the general setting where conjunction and structure modification are allowed takes exponential time and space, since large intermediate results have to be produced. This unconditionally proves that the inefficiency of bottom-up compilation resides in the bottom-up paradigm itself.",
    "code_link": ""
  },
  "aaai2022_main_enforcementheuristicsforargumentationwithdeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Enforcement Heuristics for Argumentation with Deep Reinforcement Learning",
    "authors": [
      "Dennis Craandijk",
      "Floris Bex"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20497",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20497/20256",
    "published": "2022-02",
    "summary": "In this paper, we present a learning-based approach to the symbolic reasoning problem of dynamic argumentation, where the knowledge about attacks between arguments is incomplete or evolving. Specifically, we employ deep reinforcement learning to learn which attack relations between arguments should be added or deleted in order to enforce the acceptability of (a set of) arguments. We show that our Graph Neural Network (GNN) architecture EGNN can learn a near optimal enforcement heuristic for all common argument-fixed enforcement problems, including problems for which no other (symbolic) solvers exist. We demonstrate that EGNN outperforms other GNN baselines and on enforcement problems with high computational complexity performs better than state-of-the-art symbolic solvers with respect to efficiency. Thus, we show our neuro-symbolic approach is able to learn heuristics without the expert knowledge of a human designer and offers a valid alternative to symbolic solvers. We publish our code at https://github.com/DennisCraandijk/DL-Abstract-Argumentation.",
    "code_link": "https://github.com/DennisCraandijk/DLAbstract-Argumentation"
  },
  "aaai2022_main_onthecomputationofnecessaryandsufficientexplanations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Computation of Necessary and Sufficient Explanations",
    "authors": [
      "Adnan Darwiche",
      "Chunxi Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20498",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20498/20257",
    "published": "2022-02",
    "summary": "The complete reason behind a decision is a Boolean formula that characterizes why the decision was made. This recently introduced notion has a number of applications, which include generating explanations, detecting decision bias and evaluating counterfactual queries. Prime implicants of the complete reason are known as sufficient reasons for the decision and they correspond to what is known as PI explanations and abductive explanations. In this paper, we refer to the prime implicates of a complete reason as necessary reasons for the decision. We justify this terminology semantically and show that necessary reasons correspond to what is known as contrastive explanations. We also study the computation of complete reasons for multi-class decision trees and graphs with nominal and numeric features for which we derive efficient, closed-form complete reasons. We further investigate the computation of shortest necessary and sufficient reasons for a broad class of complete reasons, which include the derived closed forms and the complete reasons for Sentential Decision Diagrams (SDDs). We provide an algorithm which can enumerate their shortest necessary reasons in output polynomial time. Enumerating shortest sufficient reasons for this class of complete reasons is hard even for a single reason. For this problem, we provide an algorithm that appears to be quite efficient as we show empirically.",
    "code_link": ""
  },
  "aaai2022_main_machinelearningforutilitypredictioninargument-basedcomputationalpersuasion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Machine Learning for Utility Prediction in Argument-Based Computational Persuasion",
    "authors": [
      "Ivan Donadello",
      "Anthony Hunter",
      "Stefano Teso",
      "Mauro Dragoni"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20499",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20499/20258",
    "published": "2022-02",
    "summary": "Automated persuasion systems (APS) aim to persuade a user to believe something by entering into a dialogue in which arguments and counterarguments are exchanged. To maximize the probability that an APS is successful in persuading a user, it can identify a global policy that will allow it to select the best arguments it presents at each stage of the dialogue whatever arguments the user presents. However, in real applications, such as for healthcare, it is unlikely the utility of the outcome of the dialogue will be the same, or the exact opposite, for the APS and user. In order to deal with this situation, games in extended form have been harnessed for argumentation in Bi-party Decision Theory. This opens new problems that we address in this paper: (1) How can we use Machine Learning (ML) methods to predict utility functions for different subpopulations of users? and (2) How can we identify for a new user the best utility function from amongst those that we have learned. To this extent, we develop two ML methods, EAI and EDS, that leverage information coming from the users to predict their utilities. EAI is restricted to a fixed amount of information, whereas EDS can choose the information that best detects the subpopulations of a user. We evaluate EAI and EDS in a simulation setting and in a realistic case study concerning healthy eating habits. Results are promising in both cases, but EDS is more effective at predicting useful utility functions.",
    "code_link": ""
  },
  "aaai2022_main_onthecomplexityofinductivelylearningguardedclauses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Complexity of Inductively Learning Guarded Clauses",
    "authors": [
      "Andrei Draghici",
      "Georg Gottlob",
      "Matthias Lanzinger"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20500",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20500/20259",
    "published": "2022-02",
    "summary": "We investigate the computational complexity of mining guarded clauses from clausal datasets through the framework of inductive logic programming (ILP). We show that learning guarded clauses is NP-complete and thus one step below the Sigma2-complete task of learning Horn clauses on the polynomial hierarchy. Motivated by practical applications on large datasets we identify a natural tractable fragment of the problem. Finally, we also generalise all of our results to k-guarded clauses for constant k.",
    "code_link": ""
  },
  "aaai2022_main_tractableabstractargumentationviabackdoor-treewidth": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Tractable Abstract Argumentation via Backdoor-Treewidth",
    "authors": [
      "Wolfgang Dvo\u0159\u00e1k",
      "Markus Hecher",
      "Matthias K\u00f6nig",
      "Andr\u00e9 Schidler",
      "Stefan\n      Szeider",
      "Stefan Woltran"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20501",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20501/20260",
    "published": "2022-02",
    "summary": "Argumentation frameworks (AFs) are a core formalism in the field of formal argumentation. As most standard computational tasks regarding AFs are hard for the first or second level of the Polynomial Hierarchy, a variety of algorithmic approaches to achieve manageable runtimes have been considered in the past. Among them, the backdoor-approach and the treewidth-approach turned out to yield fixed-parameter tractable fragments. However, many applications yield high parameter values for these methods, often rendering them infeasible in practice. We introduce the backdoor-treewidth approach for abstract argumentation, combining the best of both worlds with a guaranteed parameter value that does not exceed the minimum of the backdoor- and treewidth-parameter. In particular, we formally define backdoor-treewidth and establish fixed-parameter tractability for standard reasoning tasks of abstract argumentation. Moreover, we provide systems to find and exploit backdoors of small width, and conduct systematic experiments evaluating the new parameter.",
    "code_link": ""
  },
  "aaai2022_main_large-neighbourhoodsearchforoptimisationinanswer-setsolving": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Large-Neighbourhood Search for Optimisation in Answer-Set Solving",
    "authors": [
      "Thomas Eiter",
      "Tobias Geibinger",
      "Nelson Higuera Ruiz",
      "Nysret Musliu",
      "Johannes Oetsch",
      "Daria Stepanova"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20502",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20502/20261",
    "published": "2022-02",
    "summary": "While Answer-Set Programming (ASP) is a prominent approach to declarative problem solving, optimisation problems can still be a challenge for it. Large-Neighbourhood Search (LNS) is a metaheuristic for optimisation where parts of a solution are alternately destroyed and reconstructed that has high but untapped potential for ASP solving. We present a framework for LNS optimisation in answer-set solving, in which neighbourhoods can be specified either declaratively as part of the ASP encoding, or automatically generated by code. To effectively explore different neighbourhoods, we focus on multi-shot solving as it allows to avoid program regrounding. We illustrate the framework on different optimisation problems, some of which are notoriously difficult, including shift planning and a parallel machine scheduling problem from semi-conductor production which demonstrate the effectiveness of the LNS approach.",
    "code_link": ""
  },
  "aaai2022_main_answeringquerieswithnegationoverexistentialrules": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Answering Queries with Negation over Existential Rules",
    "authors": [
      "Stefan Ellmauthaler",
      "Markus Kr\u00f6tzsch",
      "Stephan Mennicke"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20503",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20503/20262",
    "published": "2022-02",
    "summary": "Ontology-based query answering with existential rules is well understood and implemented for positive queries, in particular conjunctive queries. For queries with negation, however, there is no agreed-upon semantics or standard implementation. This problem is unknown for simpler rule languages, such as Datalog, where it is intuitive and practical to evaluate negative queries over the least model. This fails for existential rules, which instead of a single least model have multiple universal models that may not lead to the same results for negative queries. We therefore propose universal core models as a basis for a meaningful (non-monotonic) semantics for queries with negation. Since cores are hard to compute, we identify syntactic conditions (on rules and queries) under which our core-based semantics can equivalently be obtained for other universal models, such as those produced by practical chase algorithms. Finally, we use our findings to propose a semantics for a broad class of existential rules with negation.",
    "code_link": ""
  },
  "aaai2022_main_axiomatizationofaggregatesinanswersetprogramming": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Axiomatization of Aggregates in Answer Set Programming",
    "authors": [
      "Jorge Fandinno",
      "Zachary Hansen",
      "Yuliya Lierler"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20504",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20504/20263",
    "published": "2022-02",
    "summary": "The paper presents a characterization of logic programs with aggregates based on many-sorted generalization of operator SM that refers neither to grounding nor to fixpoints. This characterization introduces new symbols for aggregate operations and aggregate elements, whose meaning is fixed by adding appropriate axioms to the result of the SM transformation. We prove that for programs without positive recursion through aggregates our semantics coincides with the semantics of the answer set solver Clingo.",
    "code_link": ""
  },
  "aaai2022_main_linear-timeverificationofdata-awaredynamicsystemswitharithmetic": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Linear-Time Verification of Data-Aware Dynamic Systems with Arithmetic",
    "authors": [
      "Paolo Felli",
      "Marco Montali",
      "Sarah Winkler"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20505",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20505/20264",
    "published": "2022-02",
    "summary": "Combined modeling and verification of dynamic systems and the data they operate on has gained momentum in AI and in several application domains. We investigate the expressive yet concise framework of data-aware dynamic systems (DDS), extending it with linear arithmetic, and providing the following contributions. First, we introduce a new, semantic property of \u201cfinite summary\u201d, which guarantees the existence of a faithful finite-state abstraction. We rely on this to show that checking whether a witness exists for a linear-time, finite-trace property is decidable for DDSs with finite summary. Second, we demonstrate that several decidability conditions studied in formal methods and database theory can be seen as concrete, checkable instances of this property. This also gives rise to new decidability results. Third, we show how the abstract, uniform property of finite summary leads to modularity results: a system enjoys finite summary if it can be partitioned appropriately into smaller systems that possess the property. Our results allow us to analyze systems that were out of reach in earlier approaches. Finally, we demonstrate the feasibility of our approach in a prototype implementation.",
    "code_link": ""
  },
  "aaai2022_main_rushingandstrollingamonganswersets\u2013navigationmadeeasy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Rushing and Strolling among Answer Sets \u2013 Navigation Made Easy",
    "authors": [
      "Johannes Klaus Fichte",
      "Sarah Alice Gaggl",
      "Dominik Rusovac"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20506",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20506/20265",
    "published": "2022-02",
    "summary": "Answer set programming (ASP) is a popular declarative programming paradigm with a wide range of applications in artificial intelligence. Oftentimes, when modeling an AI problem with ASP, and in particular when we are interested beyond simple search for optimal solutions, an actual solution, differences between solutions, or number of solutions of the ASP program matter. For example, when a user aims to identify a specific answer set according to her needs, or requires the total number of diverging solutions to comprehend probabilistic applications such as reasoning in medical domains. Then, there are only certain problem specific and handcrafted encoding techniques available to navigate the solution space of ASP programs, which is oftentimes not enough. In this paper, we propose a formal and general framework for interactive navigation toward desired subsets of answer sets analogous to faceted browsing. Our approach enables the user to explore the solution space by consciously zooming in or out of sub-spaces of solutions at a certain configurable pace. We illustrate that weighted faceted navigation is computationally hard. Finally, we provide an implementation of our approach that demonstrates the feasibility of our framework for incomprehensible solution spaces.",
    "code_link": ""
  },
  "aaai2022_main_sufficientreasonsforclassifierdecisionsinthepresenceofdomainconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sufficient Reasons for Classifier Decisions in the Presence of Domain Constraints",
    "authors": [
      "Niku Gorji",
      "Sasha Rubin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20507",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20507/20266",
    "published": "2022-02",
    "summary": "Recent work has unveiled a theory for reasoning about the decisions made by binary classifiers: a classifier describes a Boolean function, and the reasons behind an instance being classified as positive are the prime-implicants of the function that are satisfied by the instance. One drawback of these works is that they do not explicitly treat scenarios where the underlying data is known to be constrained, e.g., certain combinations of features may not exist, may not be observable, or may be required to be disregarded. We propose a more general theory, also based on prime-implicants, tailored to taking constraints into account. The main idea is to view classifiers as describing partial Boolean functions that are undefined on instances that do not satisfy the constraints. We prove that this simple idea results in more parsimonious reasons. That is, not taking constraints into account (e.g., ignoring, or taking them as negative instances) results in reasons that are subsumed by reasons that do take constraints into account. We illustrate this improved succinctness on synthetic classifiers and classifiers learnt from real data.",
    "code_link": ""
  },
  "aaai2022_main_reasoningaboutcausalmodelswithinfinitelymanyvariables": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reasoning about Causal Models with Infinitely Many Variables",
    "authors": [
      "Joseph Y. Halpern",
      "Spencer Peters"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20508",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20508/20267",
    "published": "2022-02",
    "summary": "Generalized structural equations models (GSEMs) (Peters and Halpern 2021), are, as the name suggests, a generalization of structural equations models (SEMs). They can deal with (among other things) infinitely many variables with infinite ranges, which is critical for capturing dynamical systems. We provide a sound and complete axiomatization of causal reasoning in GSEMs that is an extension of the sound and complete axiomatization provided by Halpern (2000) for SEMs. Considering GSEMs helps clarify what properties Halpern's axioms capture.",
    "code_link": ""
  },
  "aaai2022_main_anaxiomaticapproachtorevisingpreferences": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Axiomatic Approach to Revising Preferences",
    "authors": [
      "Adrian Haret",
      "Johannes Peter Wallner"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20509",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20509/20268",
    "published": "2022-02",
    "summary": "We study a model of preference revision in which a prior preference over a set of alternatives is adjusted in order to accommodate input from an authoritative source, while maintaining certain structural constraints (e.g., transitivity, completeness), and without giving up more information than strictly necessary. We analyze this model under two aspects: the first allows us to capture natural distance-based operators, at the cost of a mismatch between the input and output formats of the revision operator. Requiring the input and output to be aligned yields a second type of operator, which we characterize using preferences on the comparisons in the prior preference Prefence revision is set in a logic-based framework and using the formal machinery of belief change, along the lines of the well-known AGM approach: we propose rationality postulates for each of the two versions of our model and derive representation results, thus situating preference revision within the larger family of belief change operators.",
    "code_link": ""
  },
  "aaai2022_main_bertmapabert-basedontologyalignmentsystem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "BERTMap: A BERT-Based Ontology Alignment System",
    "authors": [
      "Yuan He",
      "Jiaoyan Chen",
      "Denvar Antonyrajah",
      "Ian Horrocks"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20510",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20510/20269",
    "published": "2022-02",
    "summary": "Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.",
    "code_link": "https://github.com/KRR-Oxford/BERTMap"
  },
  "aaai2022_main_conditionalabstractdialecticalframeworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Conditional Abstract Dialectical Frameworks",
    "authors": [
      "Jesse Heyninck",
      "Matthias Thimm",
      "Gabriele Kern-Isberner",
      "Tjitze Rienstra",
      "Kenneth Skiba"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20511",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20511/20270",
    "published": "2022-02",
    "summary": "Abstract dialectical frameworks (in short, ADFs) are a unifying model of formal argumentation, where argumentative relations between arguments are represented by assigning acceptance conditions to atomic arguments. This idea is generalized by letting acceptance conditions being assigned to complex formulas, resulting in conditional abstract dialectical frameworks (in short, cADFs). We define the semantics of cADFs in terms of a non-truth-functional four-valued logic, and study the semantics in-depth, by showing existence results and proving that all semantics are generalizations of the corresponding semantics for ADFs.",
    "code_link": ""
  },
  "aaai2022_main_multiplexnettowardsfullysatisfiedlogicalconstraintsinneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MultiplexNet: Towards Fully Satisfied Logical Constraints in Neural Networks",
    "authors": [
      "Nick Hoernle",
      "Rafael Michael Karampatsis",
      "Vaishak Belle",
      "Kobi Gal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20512",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20512/20271",
    "published": "2022-02",
    "summary": "We propose a novel way to incorporate expert knowledge into the training of deep neural networks. Many approaches encode domain constraints directly into the network architecture, requiring non-trivial or domain-specific engineering. In contrast, our approach, called MultiplexNet, represents domain knowledge as a quantifier-free logical formula in disjunctive normal form (DNF) which is easy to encode and to elicit from human experts. It introduces a latent Categorical variable that learns to choose which constraint term optimizes the error function of the network and it compiles the constraints directly into the output of existing learning algorithms. We demonstrate the efficacy of this approach empirically on several classical deep learning tasks, such as density estimation and classification in both supervised and unsupervised settings where prior knowledge about the domains was expressed as logical constraints. Our results show that the MultiplexNet approach learned to approximate unknown distributions well, often requiring fewer data samples than the alternative approaches. In some cases, MultiplexNet finds better solutions than the baselines; or solutions that could not be achieved with the alternative approaches. Our contribution is in encoding domain knowledge in a way that facilitates inference. We specifically focus on quantifier-free logical formulae that are specified over the output domain of a network. We show that this approach is both efficient and general; and critically, our approach guarantees 100% constraint satisfaction in a network's output.",
    "code_link": ""
  },
  "aaai2022_main_towardsexplainableactionrecognitionbysalientqualitativespatialobjectrelationchains": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Explainable Action Recognition by Salient Qualitative Spatial Object Relation Chains",
    "authors": [
      "Hua Hua",
      "Dongxu Li",
      "Ruiqi Li",
      "Peng Zhang",
      "Jochen Renz",
      "Anthony Cohn"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20513",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20513/20272",
    "published": "2022-02",
    "summary": "In order to be trusted by humans, Artificial Intelligence agents should be able to describe rationales behind their decisions. One such application is human action recognition in critical or sensitive scenarios, where trustworthy and explainable action recognizers are expected. For example, reliable pedestrian action recognition is essential for self-driving cars and explanations for real-time decision making are critical for investigations if an accident happens. In this regard, learning-based approaches, despite their popularity and accuracy, are disadvantageous due to their limited interpretability. This paper presents a novel neuro-symbolic approach that recognizes actions from videos with human-understandable explanations. Specifically, we first propose to represent videos symbolically by qualitative spatial relations between objects called qualitative spatial object relation chains. We further develop a neural saliency estimator to capture the correlation between such object relation chains and the occurrence of actions. Given an unseen video, this neural saliency estimator is able to tell which object relation chains are more important for the action recognized. We evaluate our approach on two real-life video datasets, with respect to recognition accuracy and the quality of generated action explanations. Experiments show that our approach achieves superior performance on both aspects to previous symbolic approaches, thus facilitating trustworthy intelligent decision making. Our approach can be used to augment state-of-the-art learning approaches with explainabilities.",
    "code_link": ""
  },
  "aaai2022_main_tractableexplanationsford-dnnfclassifiers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Tractable Explanations for d-DNNF Classifiers",
    "authors": [
      "Xuanxiang Huang",
      "Yacine Izza",
      "Alexey Ignatiev",
      "Martin Cooper",
      "Nicholas\n      Asher",
      "Joao Marques-Silva"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20514",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20514/20273",
    "published": "2022-02",
    "summary": "Compilation into propositional languages finds a growing number of practical uses, including in constraint programming, diagnosis and machine learning (ML), among others. One concrete example is the use of propositional languages as classifiers, and one natural question is how to explain the predictions made. This paper shows that for classifiers represented with some of the best-known propositional languages, different kinds of explanations can be computed in polynomial time. These languages include deterministic decomposable negation normal form (d-DNNF), and so any propositional language that is strictly less succinct than d-DNNF. Furthermore, the paper describes optimizations, specific to Sentential Decision Diagrams (SDDs), which are shown to yield more efficient algorithms in practice.",
    "code_link": ""
  },
  "aaai2022_main_understandingenthymemesindeductiveargumentationusingsemanticdistancemeasures": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Understanding Enthymemes in Deductive Argumentation Using Semantic Distance Measures",
    "authors": [
      "Anthony Hunter"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20515",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20515/20274",
    "published": "2022-02",
    "summary": "An argument can be regarded as some premises and a claim following from those premises. Normally, arguments exchanged by human agents are enthymemes, which generally means that some premises are implicit. So when an enthymeme is presented, the presenter expects that the recipient can identify the missing premises. An important kind of implicitness arises when a presenter assumes that two symbols denote the same, or nearly the same, concept (e.g. dad and father), and uses the symbols interchangeably. To model this process, we propose the use of semantic distance measures (e.g. based on a vector representation of word embeddings or a semantic network representation of words) to determine whether one symbol can be substituted by another. We present a theoretical framework for using substitutions, together with abduction of default knowledge, for understanding enthymemes based on deductive argumentation, and investigate how this could be used in practice.",
    "code_link": ""
  },
  "aaai2022_main_inferringlexicographically-orderedrewardsfrompreferences": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Inferring Lexicographically-Ordered Rewards from Preferences",
    "authors": [
      "Alihan H\u00fcy\u00fck",
      "William R. Zame",
      "Mihaela van der Schaar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20516",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20516/20275",
    "published": "2022-02",
    "summary": "Modeling the preferences of agents over a set of alternatives is a principal concern in many areas. The dominant approach has been to find a single reward/utility function with the property that alternatives yielding higher rewards are preferred over alternatives yielding lower rewards. However, in many settings, preferences are based on multiple\u2014often competing\u2014objectives; a single reward function is not adequate to represent such preferences. This paper proposes a method for inferring multi-objective reward-based representations of an agent's observed preferences. We model the agent's priorities over different objectives as entering lexicographically, so that objectives with lower priorities matter only when the agent is indifferent with respect to objectives with higher priorities. We offer two example applications in healthcare\u2014one inspired by cancer treatment, the other inspired by organ transplantation\u2014to illustrate how the lexicographically-ordered rewards we learn can provide a better understanding of a decision-maker's preferences and help improve policies when used in reinforcement learning.",
    "code_link": ""
  },
  "aaai2022_main_towardsfine-grainedreasoningforfakenewsdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Fine-Grained Reasoning for Fake News Detection",
    "authors": [
      "Yiqiao Jin",
      "Xiting Wang",
      "Ruichao Yang",
      "Yizhou Sun",
      "Wei Wang",
      "Hao Liao",
      "Xing Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20517",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20517/20276",
    "published": "2022-02",
    "summary": "The detection of fake news often requires sophisticated reasoning skills, such as logically combining information by considering word-level subtle clues. In this paper, we move towards fine-grained reasoning for fake news detection by better reflecting the logical processes of human thinking and enabling the modeling of subtle clues. In particular, we propose a fine-grained reasoning framework by following the human\u2019s information-processing model, introduce a mutual-reinforcement-based method for incorporating human knowledge about which evidence is more important, and design a prior-aware bi-channel kernel graph network to model subtle differences between pieces of evidence. Extensive experiments show that our model outperforms the state-of-the-art methods and demonstrate the explainability of our approach.",
    "code_link": ""
  },
  "aaai2022_main_approxasp\u2013ascalableapproximateanswersetcounter": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ApproxASP \u2013 a Scalable Approximate Answer Set Counter",
    "authors": [
      "Mohimenul Kabir",
      "Flavio O Everardo",
      "Ankit K Shukla",
      "Markus Hecher",
      "Johannes Klaus Fichte",
      "Kuldeep S Meel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20518",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20518/20277",
    "published": "2022-02",
    "summary": "Answer Set Programming (ASP) is a framework in artificial intelligence and knowledge representation for declarative modeling and problem solving. Modern ASP solvers focus on the computation or enumeration of answer sets. However, a variety of probabilistic applications in reasoning or logic programming require counting answer sets. While counting can be done by enumeration, simple enumeration becomes immediately infeasible if the number of solutions is high. On the other hand, approaches to exact counting are of high worst-case complexity. In fact, in propositional model counting, exact counting becomes impractical. In this work, we present a scalable approach to approximate counting for answer set programming. Our approach is based on systematically adding XOR constraints to ASP programs, which divide the search space. We prove that adding random XOR constraints partitions the answer sets of an ASP program. In practice, we use a Gaussian elimination-based approach by lifting ideas from SAT to ASP and integrating it into a state of the art ASP solver, which we call ApproxASP. Finally, our experimental evaluation shows the scalability of our approach over the existing ASP systems.",
    "code_link": ""
  },
  "aaai2022_main_unitselectionwithcausaldiagram": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unit Selection with Causal Diagram",
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20519",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20519/20278",
    "published": "2022-02",
    "summary": "The unit selection problem aims to identify a set of individuals who are most likely to exhibit a desired mode of behavior, for example, selecting individuals who would respond one way if encouraged and a different way if not encouraged. Using a combination of experimental and observational data, Li and Pearl derived tight bounds on the \"benefit function\" - the payoff/cost associated with selecting an individual with given characteristics. This paper shows that these bounds can be narrowed significantly (enough to change decisions) when structural information is available in the form of a causal model. We address the problem of estimating the benefit function using observational and experimental data when specific graphical criteria are assumed to hold.",
    "code_link": ""
  },
  "aaai2022_main_boundsoncausaleffectsandapplicationtohighdimensionaldata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bounds on Causal Effects and Application to High Dimensional Data",
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20520",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20520/20279",
    "published": "2022-02",
    "summary": "This paper addresses the problem of estimating causal effects when adjustment variables in the back-door or front-door criterion are partially observed. For such scenarios, we derive bounds on the causal effects by solving two non-linear optimization problems, and demonstrate that the bounds are sufficient. Using this optimization method, we propose a framework for dimensionality reduction that allows one to trade bias for estimation power, and demonstrate its performance using simulation studies.",
    "code_link": ""
  },
  "aaai2022_main_howdoesknowledgegraphembeddingextrapolatetounseendataasemanticevidenceview": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View",
    "authors": [
      "Ren Li",
      "Yanan Cao",
      "Qiannan Zhu",
      "Guanqun Bi",
      "Fang Fang",
      "Yi Liu",
      "Qian Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20521",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20521/20280",
    "published": "2022-02",
    "summary": "Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability?For the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods. For the problem 2, to make better use of the three levels of SE, we propose a novel GNN-based KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation.Finally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN.",
    "code_link": "https://github.com/renli1024/SE-GNN"
  },
  "aaai2022_main_multi-viewgraphrepresentationforprogramminglanguageprocessinganinvestigationintoalgorithmdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-View Graph Representation for Programming Language Processing: An Investigation into Algorithm Detection",
    "authors": [
      "Ting Long",
      "Yutong Xie",
      "Xianyu Chen",
      "Weinan Zhang",
      "Qinxiang Cao",
      "Yong Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20522",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20522/20281",
    "published": "2022-02",
    "summary": "Program representation, which aims at converting program source code into vectors with automatically extracted features, is a fundamental problem in programming language processing (PLP). Recent work tries to represent programs with neural networks based on source code structures. However, such methods often focus on the syntax and consider only one single perspective of programs, limiting the representation power of models. This paper proposes a multi-view graph (MVG) program representation method. MVG pays more attention to code semantics and simultaneously includes both data flow and control flow as multiple views. These views are then combined and processed by a graph neural network (GNN) to obtain a comprehensive program representation that covers various aspects. We thoroughly evaluate our proposed MVG approach in the context of algorithm detection, an important and challenging subfield of PLP. Specifically, we use a public dataset POJ-104 and also construct a new challenging dataset ALG-109 to test our method. In experiments, MVG outperforms previous methods significantly, demonstrating our model's strong capability of representing source code.",
    "code_link": "https://github.com/githubg0/mvg"
  },
  "aaai2022_main_automatedsynthesisofgeneralizedinvariantstrategiesviacounterexample-guidedstrategyrefinement": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Automated Synthesis of Generalized Invariant Strategies via Counterexample-Guided Strategy Refinement",
    "authors": [
      "Kailun Luo",
      "Yongmei Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20523",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20523/20282",
    "published": "2022-02",
    "summary": "Strategy synthesis for multi-agent systems has proved to be a hard task, even when limited to two-player games with safety objectives. Generalized strategy synthesis, an extension of generalized planning which aims to produce a single solution for multiple (possibly infinitely many) planning instances, is a promising direction to deal with the state-space explosion problem. In this paper, we formalize the problem of generalized strategy synthesis in the situation calculus. The synthesis task involves second-order theorem proving generally. Thus we consider strategies aiming to maintain invariants; such strategies can be verified with first-order theorem proving. We propose a sound but incomplete approach to synthesize invariant strategies by adapting the framework of counterexample-guided refinement. The key idea for refinement is to generate a strategy using a model checker for a game constructed from the counterexample, and use it to refine the candidate general strategy. We implemented our method and did experiments with a number of game problems. Our system can successfully synthesize solutions for most of the domains within a reasonable amount of time.",
    "code_link": ""
  },
  "aaai2022_main_usingconditionalindependenceforbeliefrevision": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Using Conditional Independence for Belief Revision",
    "authors": [
      "Matthew James Lynn",
      "James P. Delgrande",
      "Pavlos Peppas"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20524",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20524/20283",
    "published": "2022-02",
    "summary": "We present an approach to incorporating qualitative assertions of conditional irrelevance into belief revision, in order to address the limitations of existing work which considers only unconditional irrelevance. These assertions serve to enforce the requirement of minimal change to existing beliefs, while also suggesting a route to reducing the computational cost of belief revision by excluding irrelevant beliefs from consideration. In our approach, a knowledge engineer specifies a collection of multivalued dependencies that encode domain-dependent assertions of conditional irrelevance in the knowledge base. We consider these as capturing properties of the underlying domain which should be taken into account during belief revision. We introduce two related notions of what it means for a multivalued dependency to be taken into account by a belief revision operator: partial and full compliance. We provide characterisations of partially and fully compliant belief revision operators in terms of semantic conditions on their associated faithful rankings. Using these characterisations, we show that the constraints for partially and fully compliant belief revision operators are compatible with the AGM postulates. Finally, we compare our approach to existing work on unconditional irrelevance in belief revision.",
    "code_link": ""
  },
  "aaai2022_main_weightedmodelcountinginfo2withcardinalityconstraintsandcountingquantifiersaclosedformformula": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Weighted Model Counting in FO2 with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula",
    "authors": [
      "Sagar Malhotra",
      "Luciano Serafini"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20525",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20525/20284",
    "published": "2022-02",
    "summary": "Weighted First-Order Model Counting (WFOMC) computes the weighted sum of the models of a first-order logic theory on a given finite domain. First-Order Logic theories that admit polynomial-time WFOMC w.r.t domain cardinality are called domain liftable. We introduce the concept of lifted interpretations as a tool for formulating closed forms for WFOMC. Using lifted interpretations, we reconstruct the closed-form formula for polynomial-time FOMC in the universally quantified fragment of FO2, earlier proposed by Beame et al. We then expand this closed-form to incorporate cardinality constraints, existential quantifiers, and counting quantifiers (a.k.a C2) without losing domain-liftability. Finally, we show that the obtained closed-form motivates a natural definition of a family of weight functions strictly larger than symmetric weight functions.",
    "code_link": ""
  },
  "aaai2022_main_tempoqrtemporalquestionreasoningoverknowledgegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TempoQR: Temporal Question Reasoning over Knowledge Graphs",
    "authors": [
      "Costas Mavromatis",
      "Prasanna Lakkur Subramanyam",
      "Vassilis N. Ioannidis",
      "Adesoji Adeshina",
      "Phillip R Howard",
      "Tetiana Grinberg",
      "Nagib Hakim",
      "George\n      Karypis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20526",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20526/20285",
    "published": "2022-02",
    "summary": "Knowledge Graph Question Answering (KGQA) involves retrieving facts from a Knowledge Graph (KG) using natural language queries. A KG is a curated set of facts consisting of entities linked by relations. Certain facts include also temporal information forming a Temporal KG (TKG). Although many natural questions involve explicit or implicit time constraints, question answering (QA) over TKGs has been a relatively unexplored area. Existing solutions are mainly designed for simple temporal questions that can be answered directly by a single TKG fact.This paper puts forth a comprehensive embedding-based framework for answering complex questions over TKGs. Our method termed temporal question reasoning (TempoQR) exploits TKG embeddings to ground the question to the specific entities and time scope it refers to. It does so by augmenting the question embeddings with context, entity and time-aware information by employing three specialized modules. The first computes a textual representation of a given question, the second combines it with the entity embeddings for entities involved in the question, and the third generates question-specific time embeddings. Finally, a transformer-based encoder learns to fuse the generated temporal information with the question representation, which is used for answer predictions. Extensive experiments show that TempoQR improves accuracy by 25--45 percentage points on complex temporal questions over state-of-the-art approaches and it generalizes better to unseen question types.",
    "code_link": "https://github.com/cmavro/TempoQR"
  },
  "aaai2022_main_compilationofaggregatesinaspsystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Compilation of Aggregates in ASP Systems",
    "authors": [
      "Giuseppe Mazzotta",
      "Francesco Ricca",
      "Carmine Dodaro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20527",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20527/20286",
    "published": "2022-02",
    "summary": "Answer Set Programming (ASP) is a well-known declarative AI formalism for knowledge representation and reasoning. State-of-the-art ASP implementations employ the ground&solve approach, and they were successfully applied to industrial and academic problems. Nonetheless there are classes of ASP programs whose evaluation is not efficient (sometimes not feasible) due to the combinatorial blow-up of the program produced by the grounding step. Recent researches suggest that compilation-based techniques can mitigate the grounding bottleneck problem. However, no compilation-based technique has been developed for ASP programs that contain aggregates, which are one of the most relevant and commonly-employed constructs of ASP. In this paper, we propose a compilation-based approach for ASP programs with aggregates. We implement it on top of a state-of-the-art ASP system, and evaluate the performance on publicly-available benchmarks. Experiments show our approach is effective on ground-intensive ASP programs.",
    "code_link": ""
  },
  "aaai2022_main_prevailinginthedarkinformationwallsinstrategicgames": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Prevailing in the Dark: Information Walls in Strategic Games",
    "authors": [
      "Pavel Naumov",
      "Wenxuan Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20528",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20528/20287",
    "published": "2022-02",
    "summary": "The paper studies strategic abilities that rise from restrictions on the information sharing in multi-agent systems. The main technical result is a sound and complete logical system that describes the interplay between the knowledge and the strategic ability modalities.",
    "code_link": ""
  },
  "aaai2022_main_knowledgecompilationmeetslogicalseparability": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Knowledge Compilation Meets Logical Separability",
    "authors": [
      "Junming Qiu",
      "Wenqing Li",
      "Zhanhao Xiao",
      "Quanlong Guan",
      "Liangda Fang",
      "Zhao-Rong Lai",
      "Qian Dong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20529",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20529/20288",
    "published": "2022-02",
    "summary": "Knowledge compilation is an alternative solution to address demanding reasoning tasks with high complexity via converting knowledge bases into a suitable target language. Interestingly, the notion of logical separability, proposed by Levesque, offers a general explanation for the tractability of clausal entailment for two remarkable languages: decomposable negation normal form and prime implicates. It is interesting to explore what role logical separability on earth plays in problem tractability. In this paper, we apply the notion of logical separability in three reasoning problems within the context of propositional logic: satisfiability check (CO), clausal entailment check (CE) and model counting (CT), contributing to three corresponding polytime procedures. We provide three logical separability based properties: CO- logical separability, CE-logical separability and CT-logical separability. We then identify three novel normal forms: CO-LSNNF, CE-LSNNF and CT-LSNNF based on the above properties. Besides, we show that every normal form is the necessary and sufficient condition under which the corresponding procedure is correct. We finally integrate the above four normal forms into the knowledge compilation map.",
    "code_link": ""
  },
  "aaai2022_main_propositionalencodingsofacyclicityandreachabilitybyusingvertexelimination": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Propositional Encodings of Acyclicity and Reachability by Using Vertex Elimination",
    "authors": [
      "Masood Feyzbakhsh Rankooh",
      "Jussi Rintanen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20530",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20530/20289",
    "published": "2022-02",
    "summary": "We introduce novel methods for encoding acyclicity and s-t-reachability constraints for propositional formulas with underlying directed graphs, based on vertex elimination graphs, which makes them suitable for cases where the underlying graph has a low directed elimination width. In contrast to solvers with ad hoc constraint propagators for graph constraints such as GraphSAT, our methods encode these constraints as standard propositional clauses, making them directly applicable with any SAT solver. An empirical study demonstrates that our methods do often outperform both earlier encodings of these constraints as well as GraphSAT especially when underlying graphs have a low directed elimination width.",
    "code_link": ""
  },
  "aaai2022_main_randomvs.best-firstimpactofsamplingstrategiesondecisionmakinginmodel-baseddiagnosis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Random vs. Best-First: Impact of Sampling Strategies on Decision Making in Model-Based Diagnosis",
    "authors": [
      "Patrick Rodler"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20531",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20531/20290",
    "published": "2022-02",
    "summary": "Statistical samples, in order to be representative, have to be drawn from a population in a random and unbiased way. Nevertheless, it is common practice in the \ufb01eld of model-based diagnosis to make estimations from (biased) best-\ufb01rst samples. One example is the computation of a few most probable fault explanations for a defective system and the use of these to assess which aspect of the system, if measured, would bring the highest information gain. In this work, we scrutinize whether these statistically not well-founded conventions, that both diagnosis researchers and practitioners have adhered to for decades, are indeed reasonable. To this end, we empirically analyze various sampling methods that generate fault explanations. We study the representativeness of the produced samples in terms of their estimations about fault explanations and how well they guide diagnostic decisions, and we investigate the impact of sample size, the optimal trade-off between sampling ef\ufb01ciency and effectivity, and how approximate sampling techniques compare to exact ones.",
    "code_link": ""
  },
  "aaai2022_main_onparaconsistentbeliefrevisioninlp": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On Paraconsistent Belief Revision in LP",
    "authors": [
      "Nicolas Schwind",
      "S\u00e9bastien Konieczny",
      "Ram\u00f3n Pino P\u00e9rez"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20532",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20532/20291",
    "published": "2022-02",
    "summary": "Belief revision aims at incorporating, in a rational way, a new piece of information into the beliefs of an agent. Most works in belief revision suppose a classical logic setting, where the beliefs of the agent are consistent. Moreover, the consistency postulate states that the result of the revision should be consistent if the new piece of information is consistent. But in real applications it may easily happen that (some parts of) the beliefs of the agent are not consistent. In this case then it seems reasonable to use paraconsistent logics to derive sensible conclusions from these inconsistent beliefs. However, in this context, the standard belief revision postulates trivialize the revision process. In this work we discuss how to adapt these postulates when the underlying logic is Priest's LP logic, in order to model a rational change, while being a conservative extension of AGM/KM belief revision. This implies, in particular, to adequately adapt the notion of expansion. We provide a representation theorem and some examples of belief revision operators in this setting.",
    "code_link": ""
  },
  "aaai2022_main_weaklysupervisedneuralsymboliclearningforcognitivetasks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Weakly Supervised Neural Symbolic Learning for Cognitive Tasks",
    "authors": [
      "Jidong Tian",
      "Yitian Li",
      "Wenqing Chen",
      "Liqiang Xiao",
      "Hao He",
      "Yaohui Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20533",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20533/20292",
    "published": "2022-02",
    "summary": "Despite the recent success of end-to-end deep neural networks, there are growing concerns about their lack of logical reasoning abilities, especially on cognitive tasks with perception and reasoning processes. A solution is the neural symbolic learning (NeSyL) method that can effectively utilize pre-defined logic rules to constrain the neural architecture making it perform better on cognitive tasks. However, it is challenging to apply NeSyL to these cognitive tasks because of the lack of supervision, the non-differentiable manner of the symbolic system, and the difficulty to probabilistically constrain the neural network. In this paper, we propose WS-NeSyL, a weakly supervised neural symbolic learning model for cognitive tasks with logical reasoning. First, WS-NeSyL employs a novel back search algorithm to sample the possible reasoning process through logic rules. This sampled process can supervise the neural network as the pseudo label. Based on this algorithm, we can backpropagate gradients to the neural network of WS-NeSyL in a weakly supervised manner. Second, we introduce a probabilistic logic regularization into WS-NeSyL to help the neural network learn probabilistic logic. To evaluate WS-NeSyL, we have conducted experiments on three cognitive datasets, including temporal reasoning, handwritten formula recognition, and relational reasoning datasets. Experimental results show that WS-NeSyL not only outperforms the end-to-end neural model but also beats the state-of-the-art neural symbolic learning models.",
    "code_link": ""
  },
  "aaai2022_main_firstorderrewritabilityinontology-mediatedqueryinginhorndescriptionlogics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "First Order Rewritability in Ontology-Mediated Querying in Horn Description Logics",
    "authors": [
      "David Toman",
      "Grant Weddell"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20534",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20534/20293",
    "published": "2022-02",
    "summary": "We consider first-order (FO) rewritability for query answering in ontology mediated querying (OMQ) in which ontologies are formulated in Horn fragments of description logics (DLs). In general, OMQ approaches for such logics rely on non-FO rewriting of the query and/or on non-FO completion of the data, called a ABox. Specifically, we consider the problem of FO rewritability in terms of Beth definability, and show how Craig interpolation can then be used to effectively construct the rewritings, when they exist, from the Clark\u2019s completion of Datalog-like programs encoding a given DL TBox and optionally a query. We show how this approach to FO rewritability can also be used to (a) capture integrity constraints commonly available in backend relational data sources, (b) capture constraints inherent in mapping such sources to an ABox , and (c) can be used an alternative to deriving so-called perfect rewritings of queries in the case of DL-Lite ontologies.",
    "code_link": ""
  },
  "aaai2022_main_meteorpracticalreasoningindatalogwithmetrictemporaloperators": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MeTeoR: Practical Reasoning in Datalog with Metric Temporal Operators",
    "authors": [
      "Dingmin Wang",
      "Pan Hu",
      "Przemys\u0142aw Andrzej Wa\u0142\u0119ga",
      "Bernardo Cuenca Grau"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20535",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20535/20294",
    "published": "2022-02",
    "summary": "DatalogMTL is an extension of Datalog with operators from metric temporal logic which has received significant attention in recent years. It is a highly expressive knowledge representation language that is well-suited for applications in temporal ontology-based query answering and stream processing. Reasoning in DatalogMTL is, however, of high computational complexity, making implementation challenging and hindering its adoption in applications. In this paper, we present a novel approach for practical reasoning in DatalogMTL which combines materialisation (a.k.a. forward chaining) with automata-based techniques. We have implemented this approach in a reasoner called MeTeoR and evaluated its performance using a temporal extension of the Lehigh University Benchmark and a benchmark based on real-world meteorological data. Our experiments show that MeTeoR is a scalable system which enables reasoning over complex temporal rules and datasets involving tens of millions of temporal facts.",
    "code_link": ""
  },
  "aaai2022_main_sgeitlscenegraphenhancedimage-textlearningforvisualcommonsensereasoning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning",
    "authors": [
      "Zhecan Wang",
      "Haoxuan You",
      "Liunian Harold Li",
      "Alireza Zareian",
      "Suji Park",
      "Yiqing Liang",
      "Kai-Wei Chang",
      "Shih-Fu Chang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20536",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20536/20295",
    "published": "2022-02",
    "summary": "Answering complex questions about images is an ambitious goal for machine intelligence, which requires a joint understanding of images, text, and commonsense knowledge, as well as a strong reasoning ability. Recently, multimodal Transformers have made a great progress in the task of Visual Commonsense Reasoning (VCR), by jointly understanding visual objects and text tokens through layers of cross-modality attention. However, these approaches do not utilize the rich structure of the scene and the interactions between objects which are essential in answering complex commonsense questions. We propose a Scene Graph EnhancedImage-TextLearning(SGEITL) framework to incorporate visual scene graph in commonsense reasoning. In order to exploit the scene graph structure, at the model structure level, we propose a multihop graph transformer for regularizing attention interaction among hops. As for pre-training, a scene-graph-aware pre-training method is proposed to leverage structure knowledge extracted in visual scene graph. Moreover, we introduce a method to train and generate domain relevant visual scene graph using textual annotations in a weakly-supervised manner. Extensive experiments on VCR and other tasks show significant performance boost compared with the state-of-the-art methods, and prove the efficacy of each proposed component.",
    "code_link": ""
  },
  "aaai2022_main_inductiverelationpredictionbybert": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Inductive Relation Prediction by BERT",
    "authors": [
      "Hanwen Zha",
      "Zhiyu Chen",
      "Xifeng Yan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20537",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20537/20296",
    "published": "2022-02",
    "summary": "Relation prediction in knowledge graphs is dominated by embedding based methods which mainly focus on the transductive setting. Unfortunately, they are not able to handle inductive learning where unseen entities and relations are present and cannot take advantage of prior knowledge. Furthermore, their inference process is not easily explainable. In this work, we propose an all-in-one solution, called BERTRL (BERT-based Relational Learning), which leverages pre-trained language model and fine-tunes it by taking relation instances and their possible reasoning paths as training samples. BERTRL outperforms the SOTAs in 15 out of 18 cases in both inductive and transductive settings. Meanwhile, it demonstrates strong generalization capability in few-shot learning and is explainable. The data and code can be found at https://github.com/zhw12/BERTRL.",
    "code_link": ""
  },
  "aaai2022_main_learningtowalkwithdualagentsforknowledgegraphreasoning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Walk with Dual Agents for Knowledge Graph Reasoning",
    "authors": [
      "Denghui Zhang",
      "Zixuan Yuan",
      "Hao Liu",
      "Xiaodong lin",
      "Hui Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20538",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20538/20297",
    "published": "2022-02",
    "summary": "Graph walking based on reinforcement learning (RL) has shown great success in navigating an agent to automatically complete various reasoning tasks over an incomplete knowledge graph (KG) by exploring multi-hop relational paths. However, existing multi-hop reasoning approaches only work well on short reasoning paths and tend to miss the target entity with the increasing path length. This is undesirable for many reasoning tasks in real-world scenarios, where short paths connecting the source and target entities are not available in incomplete KGs, and thus the reasoning performances drop drastically unless the agent is able to seek out more clues from longer paths. To address the above challenge, in this paper, we propose a dual-agent reinforcement learning framework, which trains two agents (Giant and Dwarf) to walk over a KG jointly and search for the answer collaboratively. Our approach tackles the reasoning challenge in long paths by assigning one of the agents (Giant) searching on cluster-level paths quickly and providing stage-wise hints for another agent (Dwarf). Finally, experimental results on several KG reasoning benchmarks show that our approach can search answers more accurately and efficiently, and outperforms existing RL-based methods for long path queries by a large margin.",
    "code_link": ""
  },
  "aaai2022_main_residualsimilaritybasedconditionalindependencetestanditsapplicationincausaldiscovery": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Residual Similarity Based Conditional Independence Test and Its Application in Causal Discovery",
    "authors": [
      "Hao Zhang",
      "Shuigeng Zhou",
      "Kun Zhang",
      "Jihong Guan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20539",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20539/20298",
    "published": "2022-02",
    "summary": "Recently, many regression based conditional independence (CI) test methods have been proposed to solve the problem of causal discovery. These methods provide alternatives to test CI by first removing the information of the controlling set from the two target variables, and then testing the independence between the corresponding residuals Res1 and Res2. When the residuals are linearly uncorrelated, the independence test between them is nontrivial. With the ability to calculate inner product in high-dimensional space, kernel-based methods are usually used to achieve this goal, but still consume considerable time. In this paper, we investigate the independence between two linear combinations under linear non-Gaussian structural equation model. We show that the dependence between the two residuals can be captured by the difference between the similarity of (Res1, Res2) and that of (Res1, Res3) (Res3 is generated by random permutation) in high-dimensional space. With this result, we design a new method called SCIT for CI test, where permutation test is performed to control Type I error rate. The proposed method is simpler yet more efficient and effective than the existing ones. When applied to causal discovery, the proposed method outperforms the counterparts in terms of both speed and Type II error rate, especially in the case of small sample size, which is validated by our extensive experiments on various datasets.",
    "code_link": "https://github.com/Causality-Inference/SCIT"
  },
  "aaai2022_main_characterizingtheprogramexpressivepowerofexistentialrulelanguages": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Characterizing the Program Expressive Power of Existential Rule Languages",
    "authors": [
      "Heng Zhang",
      "Guifei Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20540",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20540/20299",
    "published": "2022-02",
    "summary": "Existential rule languages are a family of ontology languages that have been widely used in ontology-mediated query answering (OMQA). However, for most of them, the expressive power of representing domain knowledge for OMQA, known as the program expressive power, is not well-understood yet. In this paper, we establish a number of novel characterizations for the program expressive power of several important existential rule languages, including tuple-generating dependencies (TGDs), linear TGDs, as well as disjunctive TGDs. The characterizations employ natural model-theoretic properties, and automata-theoretic properties sometimes, which thus provide powerful tools for identifying the definability of domain knowledge for OMQA in these languages.",
    "code_link": ""
  },
  "aaai2022_main_context-specificrepresentationabstractionfordeepoptionlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Context-Specific Representation Abstraction for Deep Option Learning",
    "authors": [
      "Marwa Abdulhai",
      "Dong-Ki Kim",
      "Matthew Riemer",
      "Miao Liu",
      "Gerald Tesauro",
      "Jonathan P. How"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20541",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20541/20300",
    "published": "2022-02",
    "summary": "Hierarchical reinforcement learning has focused on discovering temporally extended actions, such as options, that can provide benefits in problems requiring extensive exploration. One promising approach that learns these options end-to-end is the option-critic (OC) framework. We examine and show in this paper that OC does not decompose a problem into simpler sub-problems, but instead increases the size of the search over policy space with each option considering the entire state space during learning. This issue can result in practical limitations of this method, including sample inefficient learning. To address this problem, we introduce Context-Specific Representation Abstraction for Deep Option Learning (CRADOL), a new framework that considers both temporal abstraction and context-specific representation abstraction to effectively reduce the size of the search over policy space. Specifically, our method learns a factored belief state representation that enables each option to learn a policy over only a subsection of the state space. We test our method against hierarchical, non-hierarchical, and modular recurrent neural network baselines, demonstrating significant sample efficiency improvements in challenging partially observable environments.",
    "code_link": "https://github.com/maximecb/gym-minigrid"
  },
  "aaai2022_main_fisheyehdkhyperbolicdeformablekernellearningforultra-widefield-of-viewimagerecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition",
    "authors": [
      "Ola Ahmad",
      "Freddy Lecue"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20542",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20542/20301",
    "published": "2022-02",
    "summary": "Conventional convolution neural networks (CNNs) trained on narrow Field-of-View (FoV) images are the state-of-the art approaches for object recognition tasks. Some methods proposed the adaptation of CNNs to ultra-wide FoV images by learning deformable kernels. However, they are limited by the Euclidean geometry and their accuracy degrades under strong distortions caused by fisheye projections. In this work, we demonstrate that learning the shape of convolution kernels in non-Euclidean spaces is better than existing deformable kernel methods. In particular, we propose a new approach that learns deformable kernel parameters (positions) in hyperbolic space. FisheyeHDK is a hybrid CNN architecture combining hyperbolic and Euclidean convolution layers for positions and features learning. First, we provide intuition of hyperbolic space for wide FoV images. Using synthetic distortion profiles, we demonstrate the effectiveness of our approach. We select two datasets - Cityscapes and BDD100K 2020 - of perspective images which we transform to fisheye equivalents at different scaling factors (analogue to focal lengths). Finally, we provide an experiment on data collected by a real fisheye camera. Validations and experiments show that our approach improves existing deformable kernel methods for CNN adaptation on fisheye images.",
    "code_link": ""
  },
  "aaai2022_main_distributedlearningwithstrategicusersarepeatedgameapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Distributed Learning with Strategic Users: A Repeated Game Approach",
    "authors": [
      "Abdullah B Akbay",
      "Junshan Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20543",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20543/20302",
    "published": "2022-02",
    "summary": "We consider a distributed learning setting where strategic users are incentivized by a fusion center, to train a learning model based on local data. The users are not obliged to provide their true gradient updates and the fusion center is not capable of validating the authenticity of reported updates. Thus motivated, we formulate the interactions between the fusion center and the users as repeated games, manifesting an under-explored interplay between machine learning and game theory. We then develop an incentive mechanism for the fusion center based on a joint gradient estimation and user action classification scheme, and study its impact on the convergence performance of distributed learning. Further, we devise adaptive zero-determinant (ZD) strategies, thereby generalizing the classical ZD strategies to the repeated games with time-varying stochastic errors. Theoretical and empirical analysis show that the fusion center can incentivize the strategic users to cooperate and report informative gradient updates, thus ensuring the convergence.",
    "code_link": ""
  },
  "aaai2022_main_privaterankaggregationincentralandlocalmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Private Rank Aggregation in Central and Local Models",
    "authors": [
      "Daniel Alabi",
      "Badih Ghazi",
      "Ravi Kumar",
      "Pasin Manurangsi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20544",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20544/20303",
    "published": "2022-02",
    "summary": "In social choice theory, (Kemeny) rank aggregation is a well-studied problem where the goal is to combine rankings from multiple voters into a single ranking on the same set of items. Since rankings can reveal preferences of voters (which a voter might like to keep private), it is important to aggregate preferences in such a way to preserve privacy. In this work, we present differentially private algorithms for rank aggregation in the pure and approximate settings along with distribution-independent utility upper and lower bounds. In addition to bounds in the central model, we also present utility bounds for the local model of differential privacy.",
    "code_link": ""
  },
  "aaai2022_main_combatingadversarieswithanti-adversaries": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Combating Adversaries with Anti-adversaries",
    "authors": [
      "Motasem Alfarra",
      "Juan C. Perez",
      "Ali Thabet",
      "Adel Bibi",
      "Philip H.S. Torr",
      "Bernard Ghanem"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20545",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20545/20304",
    "published": "2022-02",
    "summary": "Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and ImageNet. Our layer significantly enhances model robustness while coming at no cost on clean accuracy.",
    "code_link": "https://github.com/MotasemAlfarra/CombatingAdversaries-with-Anti-Adversaries"
  },
  "aaai2022_main_deformrscertifyinginputdeformationswithrandomizedsmoothing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing",
    "authors": [
      "Motasem Alfarra",
      "Adel Bibi",
      "Naeemullah Khan",
      "Philip H.S. Torr",
      "Bernard\n      Ghanem"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20546",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20546/20305",
    "published": "2022-02",
    "summary": "Deep neural networks are vulnerable to input deformations in the form of vector fields of pixel displacements and to other parameterized geometric deformations e.g. translations, rotations, etc. Current input deformation certification methods either (i) do not scale to deep networks on large input datasets, or (ii) can only certify a specific class of deformations, e.g. only rotations. We reformulate certification in randomized smoothing setting for both general vector field and parameterized deformations and propose DeformRS-VF and DeformRS-Par, respectively. Our new formulation scales to large networks on large input datasets. For instance, DeformRS-Par certifies rich deformations, covering translations, rotations, scaling, affine deformations, and other visually aligned deformations such as ones parameterized by Discrete-Cosine-Transform basis. Extensive experiments on MNIST, CIFAR10, and ImageNet show competitive performance of DeformRS-Par achieving a certified accuracy of 39\\% against perturbed rotations in the set [-10 degree, 10 degree] on ImageNet.",
    "code_link": "https://github.com/MotasemAlfarra/DeformRS"
  },
  "aaai2022_main_latenttimeneuralordinarydifferentialequations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Latent Time Neural Ordinary Differential Equations",
    "authors": [
      "Srinivas Anumasa",
      "P. K. Srijith"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20547",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20547/20306",
    "published": "2022-02",
    "summary": "Neural ordinary differential equations (NODE) have been proposed as a continuous depth generalization to popular deep learning models such as Residual networks (ResNets). They provide parameter efficiency and automate the model selection process in deep learning models to some extent. However, they lack the much-required uncertainty modelling and robustness capabilities which are crucial for their use in several real-world applications such as autonomous driving and healthcare. We propose a novel and unique approach to model uncertainty in NODE by considering a distribution over the end-time T of the ODE solver. The proposed approach, latent time NODE (LT-NODE), treats T as a latent variable and apply Bayesian learning to obtain a posterior distribution over T from the data. In particular, we use variational inference to learn an approximate posterior and the model parameters. Prediction is done by considering the NODE representations from different samples of the posterior and can be done efficiently using a single forward pass. As T implicitly defines the depth of a NODE, posterior distribution over T would also help in model selection in NODE. We also propose, adaptive latent time NODE (ALT-NODE), which allow each data point to have a distinct posterior distribution over end-times. ALT-NODE uses amortized variational inference to learn an approximate posterior using inference networks. We demonstrate the effectiveness of the proposed approaches in modelling uncertainty and robustness through experiments on synthetic and several real-world image classification data.",
    "code_link": "https://github.com/srinivas-quan/LTNODE"
  },
  "aaai2022_main_beyondgnnsanefficientarchitectureforgraphproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Beyond GNNs: An Efficient Architecture for Graph Problems",
    "authors": [
      "Pranjal Awasthi",
      "Abhimanyu Das",
      "Sreenivas Gollapudi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20548",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20548/20307",
    "published": "2022-02",
    "summary": "Despite their popularity for graph structured data, existing Graph Neural Networks (GNNs) have inherent limitations for fundamental graph problems such as shortest paths, k-connectivity, minimum spanning tree and minimum cuts. In these instances, it is known that one needs GNNs of high depth, scaling at a polynomial rate with the number of nodes n, to provably encode the solution space, in turn affecting their statistical efficiency.In this work we propose a new hybrid architecture to overcome this limitation. Our proposed architecture that we call as GNNplus networks involve a combination of multiple parallel low depth GNNs along with simple pooling layers involving low depth fully connected networks. We provably demonstrate that for many graph problems, the solution space can be encoded by GNNplus networks using depth that scales only poly-logarithmically in the number of nodes. This also has statistical advantages that we demonstrate via generalization bounds for GNNplus networks. We empirically show the effectiveness of our proposed architecture for a variety of graph problems and real world classification problems.",
    "code_link": ""
  },
  "aaai2022_main_programmaticmodelingandgenerationofreal-timestrategicsoccerenvironmentsforreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Programmatic Modeling and Generation of Real-Time Strategic Soccer Environments for Reinforcement Learning",
    "authors": [
      "Abdus Salam Azad",
      "Edward Kim",
      "Qiancheng Wu",
      "Kimin Lee",
      "Ion Stoica",
      "Pieter\n      Abbeel",
      "Alberto Sangiovanni-Vincentelli",
      "Sanjit A. Seshia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20549",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20549/20308",
    "published": "2022-02",
    "summary": "The capability of a reinforcement learning (RL) agent heavily depends on the diversity of the learning scenarios generated by the environment. Generation of diverse realistic scenarios is challenging for real-time strategy (RTS) environments. The RTS environments are characterized by intelligent entities/non-RL agents cooperating and competing with the RL agents with large state and action spaces over a long period of time, resulting in an infinite space of feasible, but not necessarily realistic, scenarios involving complex interaction among different RL and non-RL agents. Yet, most of the existing simulators rely on randomly generating the environments based on predefined settings/layouts and offer limited flexibility and control over the environment dynamics for researchers to generate diverse, realistic scenarios as per their demand. To address this issue, for the first time, we formally introduce the benefits of adopting an existing formal scenario specification language, SCENIC, to assist researchers to model and generate diverse scenarios in an RTS environment in a flexible, systematic, and programmatic manner. To showcase the benefits, we interfaced SCENIC to an existing RTS environment Google Research Football (GRF) simulator and introduced a benchmark consisting of 32 realistic scenarios, encoded in SCENIC, to train RL agents and testing their generalization capabilities. We also show how researchers/RL practitioners can incorporate their domain knowledge to expedite the training process by intuitively modeling stochastic programmatic policies with SCENIC.",
    "code_link": ""
  },
  "aaai2022_main_admissiblepolicyteachingthroughrewarddesign": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Admissible Policy Teaching through Reward Design",
    "authors": [
      "Kiarash Banihashem",
      "Adish Singla",
      "Jiarui Gan",
      "Goran Radanovic"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20550",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20550/20309",
    "published": "2022-02",
    "summary": "We study reward design strategies for incentivizing a reinforcement learning agent to adopt a policy from a set of admissible policies. The goal of the reward designer is to modify the underlying reward function cost-efficiently while ensuring that any approximately optimal deterministic policy under the new reward function is admissible and performs well under the original reward function. This problem can be viewed as a dual to the problem of optimal reward poisoning attacks: instead of forcing an agent to adopt a specific policy, the reward designer incentivizes an agent to avoid taking actions that are inadmissible in certain states. Perhaps surprisingly, and in contrast to the problem of optimal reward poisoning attacks, we first show that the reward design problem for admissible policy teaching is computationally challenging, and it is NP-hard to find an approximately optimal reward modification. We then proceed by formulating a surrogate problem whose optimal solution approximates the optimal solution to the reward design problem in our setting, but is more amenable to optimization techniques and analysis. For this surrogate problem, we present characterization results that provide bounds on the value of the optimal solution. Finally, we design a local search algorithm to solve the surrogate problem and showcase its utility using simulation-based experiments.",
    "code_link": ""
  },
  "aaai2022_main_entropy-basedlogicexplanationsofneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Entropy-Based Logic Explanations of Neural Networks",
    "authors": [
      "Pietro Barbiero",
      "Gabriele Ciravegna",
      "Francesco Giannini",
      "Pietro Li\u00f3",
      "Marco\n      Gori",
      "Stefano Melacci"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20551",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20551/20310",
    "published": "2022-02",
    "summary": "Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (i.e. concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In this paper, we propose a novel end-to-end differentiable approach enabling the extraction of logic explanations from neural networks using the formalism of First-Order Logic. The method relies on an entropy-based criterion which automatically identifies the most relevant concepts. We consider four different case studies to demonstrate that: (i) this entropy-based criterion enables the distillation of concise logic explanations in safety-critical domains from clinical data to computer vision; (ii) the proposed approach outperforms state-of-the-art white-box models in terms of classification accuracy.",
    "code_link": ""
  },
  "aaai2022_main_trainingrobustdeepmodelsfortime-seriesdomainnovelalgorithmsandtheoreticalanalysis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis",
    "authors": [
      "Taha Belkhouja",
      "Yan Yan",
      "Janardhan Rao Doppa"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20552",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20552/20311",
    "published": "2022-02",
    "summary": "Despite the success of deep neural networks (DNNs) for real-world applications over time-series data such as mobile health, little is known about how to train robust DNNs for time-series domain due to its unique characteristics compared to images and text data. In this paper, we fill this gap by proposing a novel algorithmic framework referred as RObust Training for Time-Series (RO-TS) to create robust deep models for time-series classification tasks. Specifically, we formulate a min-max optimization problem over the model parameters by explicitly reasoning about the robustness criteria in terms of additive perturbations to time-series inputs measured by the global alignment kernel (GAK) based distance.We also show the generality and advantages of our formulation using the summation structure over time-series alignments by relating both GAK and dynamic time warping (DTW). This problem is an instance of a family of compositional min-max optimization problems, which are challenging and open with unclear theoretical guarantee.We propose a principled stochastic compositional alternating gradient descent ascent (SCAGDA) algorithm for this family of optimization problems. Unlike traditional methods for time-series that require approximate computation of distance measures, SCAGDA approximates the GAK based distance on-the-fly using a moving average approach. We theoretically analyze the convergence rate of SCAGDA and provide strong theoretical support for the estimation of GAK based distance. Our experiments on real-world benchmarks demonstrate that RO-TS creates more robust deep models when compared to adversarial training using prior methods that rely on data augmentation or new definitions of loss functions. We also demonstrate the importance of GAK for time-series data over the Euclidean distance.",
    "code_link": "https://github.com/tahabelkhouja/Robust-Training-forTime-Series"
  },
  "aaai2022_main_afastalgorithmforpaccombinatorialpureexploration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
    "authors": [
      "Noa Ben-David",
      "Sivan Sabato"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20553",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20553/20312",
    "published": "2022-02",
    "summary": "We consider the problem of Combinatorial Pure Exploration (CPE), which deals with finding a combinatorial set of arms with a high reward, when the rewards of individual arms are unknown in advance and must be estimated using arm pulls. Previous algorithms for this problem, while obtaining sample complexity reductions in many cases, are highly computationally intensive, thus making them impractical even for mildly large problems. In this work, we propose a new CPE algorithm in the PAC setting, which is computationally light weight, and so can easily be applied to problems with tens of thousands of arms. This is achieved since the proposed algorithm requires a very small number of combinatorial oracle calls. The algorithm is based on successive acceptance of arms, along with elimination which is based on the combinatorial structure of the problem. We provide sample complexity guarantees for our algorithm, and demonstrate in experiments its usefulness on large problems, whereas previous algorithms are impractical to run on problems of even a few dozen arms. The code is provided at https://github.com/noabdavid/csale. The full version of this paper is available at https://arxiv.org/abs/2112.04197.",
    "code_link": ""
  },
  "aaai2022_main_modelingattritioninrecommendersystemswithdepartingbandits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Modeling Attrition in Recommender Systems with Departing Bandits",
    "authors": [
      "Omer Ben-Porat",
      "Lee Cohen",
      "Liu Leqi",
      "Zachary C. Lipton",
      "Yishay Mansour"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20554",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20554/20313",
    "published": "2022-02",
    "summary": "Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisfied users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a finite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user's type is initially unknownand can only be inferred through their response to recommendations. Moreover, if a user is dissatisfied with their recommendation, they might depart the system. We first address the case where all users share the same type,demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two types. While naive approaches cannot handle this setting,we provide an efficient learning algorithm that achieves O(sqrt(T)ln(T)) regret, where T is the number of users.",
    "code_link": ""
  },
  "aaai2022_main_federateddynamicsparsetrainingcomputingless,communicatingless,yetlearningbetter": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better",
    "authors": [
      "Sameer Bibikar",
      "Haris Vikalo",
      "Zhangyang Wang",
      "Xiaohan Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20555",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20555/20314",
    "published": "2022-02",
    "summary": "Federated learning (FL) enables distribution of machine learning workloads from the cloud to resource-limited edge devices. Unfortunately, current deep networks remain not only too compute-heavy for inference and training on edge devices, but also too large for communicating updates over bandwidth-constrained networks. In this paper, we develop, implement, and experimentally validate a novel FL framework termed Federated Dynamic Sparse Training (FedDST) by which complex neural networks can be deployed and trained with substantially improved efficiency in both on-device computation and in-network communication. At the core of FedDST is a dynamic process that extracts and trains sparse sub-networks from the target full network. With this scheme, \"two birds are killed with one stone:'' instead of full models, each client performs efficient training of its own sparse networks, and only sparse networks are transmitted between devices and the cloud. Furthermore, our results reveal that the dynamic sparsity during FL training more flexibly accommodates local heterogeneity in FL agents than the fixed, shared sparse masks. Moreover, dynamic sparsity naturally introduces an \"in-time self-ensembling effect'' into the training dynamics, and improves the FL performance even over dense training. In a realistic and challenging non i.i.d. FL setting, FedDST consistently outperforms competing algorithms in our experiments: for instance, at any fixed upload data cap on non-iid CIFAR-10, it gains an impressive accuracy advantage of 10% over FedAvgM when given the same upload data cap; the accuracy gap remains 3% even when FedAvgM is given 2 times the upload data cap, further demonstrating efficacy of FedDST. Code is available at: https://github.com/bibikar/feddst.",
    "code_link": "https://github.com/bibikar/feddst"
  },
  "aaai2022_main_robustandresource-efficientdata-freeknowledgedistillationbygenerativepseudoreplay": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay",
    "authors": [
      "Kuluhan Binici",
      "Shivam Aggarwal",
      "Nam Trung Pham",
      "Karianto Leman",
      "Tulika\n      Mitra"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20556",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20556/20315",
    "published": "2022-02",
    "summary": "Data-Free Knowledge Distillation (KD) allows knowledge transfer from a trained neural network (teacher) to a more compact one (student) in the absence of original training data. Existing works use a validation set to monitor the accuracy of the student over real data and report the highest performance throughout the entire process. However, validation data may not be available at distillation time either, making it infeasible to record the student snapshot that achieved the peak accuracy. Therefore, a practical data-free KD method should be robust and ideally provide monotonically increasing student accuracy during distillation. This is challenging because the student experiences knowledge degradation due to the distribution shift of the synthetic data. A straightforward approach to overcome this issue is to store and rehearse the generated samples periodically, which increases the memory footprint and creates privacy concerns. We propose to model the distribution of the previously observed synthetic samples with a generative network. In particular, we design a Variational Autoencoder (VAE) with a training objective that is customized to learn the synthetic data representations optimally. The student is rehearsed by the generative pseudo replay technique, with samples produced by the VAE. Hence knowledge degradation can be prevented without storing any samples. Experiments on image classification benchmarks show that our method optimizes the expected value of the distilled model accuracy while eliminating the large memory overhead incurred by the sample-storing methods.",
    "code_link": ""
  },
  "aaai2022_main_erfactandpserfnon-monotonicsmoothtrainableactivationfunctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions",
    "authors": [
      "Koushik Biswas",
      "Sandeep Kumar",
      "Shilpak Banerjee",
      "Ashish Kumar Pandey"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20557",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20557/20316",
    "published": "2022-02",
    "summary": "An activation function is a crucial component of a neural network that introduces non-linearity in the network. The state-of-the-art performance of a neural network depends also on the perfect choice of an activation function. We propose two novel non-monotonic smooth trainable activation functions, called ErfAct and Pserf. Experiments suggest that the proposed functions improve the network performance significantly compared to the widely used activations like ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and 5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean average precision (mAP) on SSD300 model in Pascal VOC dataset.",
    "code_link": ""
  },
  "aaai2022_main_feedbackgradientdescentefficientandstableoptimizationwithorthogonalityfordnns": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs",
    "authors": [
      "Fanchen Bu",
      "Dong Eui Chang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20558",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20558/20317",
    "published": "2022-02",
    "summary": "The optimization with orthogonality has been shown useful in training deep neural networks (DNNs). To impose orthogonality on DNNs, both computational efficiency and stability are important.However, existing methods utilizing Riemannian optimization or hard constraints can only ensure stability while those using soft constraints can only improve efficiency.In this paper, we propose a novel method, named Feedback Gradient Descent (FGD), to our knowledge, the first work showing high efficiency and stability simultaneously.FGD induces orthogonality based on the simple yet indispensable Euler discretization of a continuous-time dynamical system on the tangent bundle of the Stiefel manifold.In particular, inspired by a numerical integration method on manifolds called Feedback Integrators, we propose to instantiate it on the tangent bundle of the Stiefel manifold for the first time.In our extensive image classification experiments, FGD comprehensively outperforms the existing state-of-the-art methods in terms of accuracy, efficiency, and stability.",
    "code_link": ""
  },
  "aaai2022_main_breakingtheconvergencebarrieroptimizationviafixed-timeconvergentflows": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows",
    "authors": [
      "Param Budhraja",
      "Mayank Baranwal",
      "Kunal Garg",
      "Ashish Hota"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20559",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20559/20318",
    "published": "2022-02",
    "summary": "Accelerated gradient methods are the cornerstones of large-scale, data-driven optimization problems that arise naturally in machine learning and other fields concerning data analysis. We introduce a gradient-based optimization framework for achieving acceleration, based on the recently introduced notion of fixed-time stability of dynamical systems. The method presents itself as a generalization of simple gradient-based methods suitably scaled to achieve convergence to the optimizer in a fixed-time, independent of the initialization. We achieve this by first leveraging a continuous-time framework for designing fixed-time stable dynamical systems, and later providing a consistent discretization strategy, such that the equivalent discrete-time algorithm tracks the optimizer in a practically fixed number of iterations. We also provide a theoretical analysis of the convergence behavior of the proposed gradient flows, and their robustness to additive disturbances for a range of functions obeying strong convexity, strict convexity, and possibly nonconvexity but satisfying the Polyak-\u0141ojasiewicz inequality. We also show that the regret bound on the convergence rate is constant by virtue of the fixed-time convergence. The hyperparameters have intuitive interpretations and can be tuned to fit the requirements on the desired convergence rates. We validate the accelerated convergence properties of the proposed schemes on a range of numerical examples against the state-of-the-art optimization algorithms. Our work provides insights on developing novel optimization algorithms via discretization of continuous-time flows.",
    "code_link": ""
  },
  "aaai2022_main_shrubensemblesforonlineclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shrub Ensembles for Online Classification",
    "authors": [
      "Sebastian Buschj\u00e4ger",
      "Sibylle Hess",
      "Katharina J. Morik"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20560",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20560/20319",
    "published": "2022-02",
    "summary": "Online learning algorithms have become a ubiquitous tool in the machine learning toolbox and are frequently used in small, resource-constraint environments. Among the most successful online learning methods are Decision Tree (DT) ensembles. DT ensembles provide excellent performance while adapting to changes in the data, but they are not resource efficient. Incremental tree learners keep adding new nodes to the tree but never remove old ones increasing the memory consumption over time. Gradient-based tree learning, on the other hand, requires the computation of gradients over the entire tree which is costly for even moderately sized trees.In this paper, we propose a novel memory-efficient online classification ensemble called shrub ensembles for resource-constraint systems. Our algorithm trains small to medium-sized decision trees on small windows and uses stochastic proximal gradient descent to learn the ensemble weights of these `shrubs'. We provide a theoretical analysis of our algorithm and include an extensive discussion on the behavior of our approach in the online setting. In a series of 2~959 experiments on 12 different datasets, we compare our method against 8 state-of-the-art methods. Our Shrub Ensembles retain an excellent performance even when only little memory is available. We show that SE offers a better accuracy-memory trade-off in 7 of 12 cases, while having a statistically significant better performance than most other methods. Our implementation is available under https://github.com/sbuschjaeger/se-online .",
    "code_link": ""
  },
  "aaai2022_main_noisegrad\u2014enhancingexplanationsbyintroducingstochasticitytomodelweights": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NoiseGrad \u2014 Enhancing Explanations by Introducing Stochasticity to Model Weights",
    "authors": [
      "Kirill Bykov",
      "Anna Hedstr\u00f6m",
      "Shinichi Nakajima",
      "Marina M.-C. H\u00f6hne"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20561",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20561/20320",
    "published": "2022-02",
    "summary": "Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad - FusionGrad - qualitatively and quantitatively with several evaluation criteria, and show that our novel approach significantly outperforms the baseline methods. Both NoiseGrad and FusionGrad are method-agnostic and as handy as SmoothGrad using a simple heuristic for the choice of the hyperparameter setting without the need of fine-tuning.",
    "code_link": ""
  },
  "aaai2022_main_leapingthroughtimewithgradient-basedadaptationforrecommendation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Leaping through Time with Gradient-Based Adaptation for Recommendation",
    "authors": [
      "Nuttapong Chairatanakul",
      "Hoang NT",
      "Xin Liu",
      "Tsuyoshi Murata"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20562",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20562/20321",
    "published": "2022-02",
    "summary": "Modern recommender systems are required to adapt to the change in user preferences and item popularity. Such a problem is known as the temporal dynamics problem, and it is one of the main challenges in recommender system modeling. Different from the popular recurrent modeling approach, we propose a new solution named LeapRec to the temporal dynamic problem by using trajectory-based meta-learning to model time dependencies. LeapRec characterizes temporal dynamics by two complement components named global time leap (GTL) and ordered time leap (OTL). By design, GTL learns long-term patterns by finding the shortest learning path across unordered temporal data. Cooperatively, OTL learns short-term patterns by considering the sequential nature of the temporal data. Our experimental results show that LeapRec consistently outperforms the state-of-the-art methods on several datasets and recommendation metrics. Furthermore, we provide an empirical study of the interaction between GTL and OTL, showing the effects of long- and short-term modeling.",
    "code_link": ""
  },
  "aaai2022_main_activesamplingfortextclassificationwithsubinstancelevelqueries": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Active Sampling for Text Classification with Subinstance Level Queries",
    "authors": [
      "Shayok Chakraborty",
      "Ankita Singh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20563",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20563/20322",
    "published": "2022-02",
    "summary": "Active learning algorithms are effective in identifying the salient and exemplar samples from large amounts of unlabeled data. This tremendously reduces the human annotation effort in inducing a machine learning model as only a few samples, which are identified by the algorithm, need to be labeled manually. In problem domains like text mining and video classification, human oracles peruse the data instances incrementally to derive an opinion about their class labels (such as reading a movie review progressively to assess its sentiment). In such applications, it is not necessary for the human oracles to review an unlabeled sample end-to-end in order to provide a label; it may be more efficient to identify an optimal subinstance size (percentage of the sample from the start) for each unlabeled sample, and request the human annotator to label the sample by analyzing only the subinstance, instead of the whole data sample. In this paper, we propose a novel framework to address this challenging problem, in an effort to further reduce the labeling burden on the human oracles and utilize the available labeling budget more efficiently. We pose the sample and subinstance size selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of exemplar samples, together with the optimal subinstance size of each, which can potentially augment maximal information to the underlying classification model. Our extensive empirical studies on six challenging datasets from the text mining domain corroborate the practical usefulness of our framework over competing baselines.",
    "code_link": ""
  },
  "aaai2022_main_aunifyingtheoryofthompsonsamplingforcontinuousrisk-aversebandits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Unifying Theory of Thompson Sampling for Continuous Risk-Averse Bandits",
    "authors": [
      "Joel Q. L. Chang",
      "Vincent Y. F. Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20564",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20564/20323",
    "published": "2022-02",
    "summary": "This paper unifies the design and the analysis of risk-averse Thompson sampling algorithms for the multi-armed bandit problem for a class of risk functionals \u03c1 that are continuous and dominant. We prove generalised concentration bounds for these continuous and dominant risk functionals and show that a wide class of popular risk functionals belong to this class. Using our newly developed analytical toolkits, we analyse the algorithm \u03c1-MTS (for multinomial distributions) and prove that they admit asymptotically optimal regret bounds of risk-averse algorithms under the CVaR, proportional hazard, and other ubiquitous risk measures. More generally, we prove the asymptotic optimality of \u03c1-MTS for Bernoulli distributions for a class of risk measures known as empirical distribution performance measures (EDPMs); this includes the well-known mean-variance. Numerical simulations show that the regret bounds incurred by our algorithms are reasonably tight vis-\u00e0-vis algorithm-independent lower bounds.",
    "code_link": ""
  },
  "aaai2022_main_locallyprivatek-meansclusteringwithconstantmultiplicativeapproximationandnear-optimaladditiveerror": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Locally Private k-Means Clustering with Constant Multiplicative Approximation and Near-Optimal Additive Error",
    "authors": [
      "Anamay Chaturvedi",
      "Matthew Jones",
      "Huy L\u00ea Nguy\u1ec5n"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20565",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20565/20324",
    "published": "2022-02",
    "summary": "Given a data set of size n in d'-dimensional Euclidean space, the k-means problem asks for a set of k points (called centers) such that the sum of the l_2^2-distances between the data points and the set of centers is minimized. Previous work on this problem in the local differential privacy setting shows how to achieve multiplicative approximation factors arbitrarily close to optimal, but suffers high additive error. The additive error has also been seen to be an issue in implementations of differentially private k-means clustering algorithms in both the central and local settings. In this work, we introduce a new locally private k-means clustering algorithm that achieves near-optimal additive error whilst retaining constant multiplicative approximation factors and round complexity. Concretely, given any c>\u221a2, our algorithm achieves O(k^(1 + O(1/(2c^2-1))) \u221a(d' n) log d' poly log n) additive error with an O(c^2) multiplicative approximation factor.",
    "code_link": ""
  },
  "aaai2022_main_safeonlineconvexoptimizationwithunknownlinearsafetyconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Safe Online Convex Optimization with Unknown Linear Safety Constraints",
    "authors": [
      "Sapana Chaudhary",
      "Dileep Kalathil"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20566",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20566/20325",
    "published": "2022-02",
    "summary": "We study the problem of safe online convex optimization, where the action at each time step must satisfy a set of linear safety constraints. The goal is to select a sequence of actions to minimize the regret without violating the safety constraints at any time step (with high probability). The parameters that specify the linear safety constraints are unknown to the algorithm. The algorithm has access to only the noisy observations of constraints for the chosen actions. We propose an algorithm, called the Safe Online Projected Gradient Descent (SO-PGD) algorithm, to address this problem. We show that, under the assumption of availability of a safe baseline action, the SO-PGD algorithm achieves a regret O(T^{2/3}). While there are many algorithms for online convex optimization (OCO) problems with safety constraints available in the literature, they allow constraint violations during learning/optimization, and the focus has been on characterizing the cumulative constraint violations. To the best of our knowledge, ours is the first work that provides an algorithm with provable guarantees on the regret, without violating the linear safety constraints (with high probability) at any time step.",
    "code_link": ""
  },
  "aaai2022_main_deconvolutionaldensitynetworkmodelingfree-formconditionaldistributions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deconvolutional Density Network: Modeling Free-Form Conditional Distributions",
    "authors": [
      "Bing Chen",
      "Mazharul Islam",
      "Jisuo Gao",
      "Lin Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20567",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20567/20326",
    "published": "2022-02",
    "summary": "Conditional density estimation (CDE) is the task of estimating the probability of an event conditioned on some inputs. A neural network (NN) can also be used to compute the output distribution for continuous-domain, which can be viewed as an extension of regression task. Nevertheless, it is difficult to explicitly approximate a distribution without knowing the information of its general form a priori. In order to fit an arbitrary conditional distribution, discretizing the continuous domain into bins is an effective strategy, as long as we have sufficiently narrow bins and very large data. However, collecting enough data is often hard to reach and falls far short of that ideal in many circumstances, especially in multivariate CDE for the curse of dimensionality. In this paper, we demonstrate the benefits of modeling free-form conditional distributions using a deconvolution-based neural net framework, coping with data deficiency problems in discretization. It has the advantage of being flexible but also takes advantage of the hierarchical smoothness offered by the deconvolution layers. We compare our method to a number of other density-estimation approaches and show that our Deconvolutional Density Network (DDN) outperforms the competing methods on many univariate and multivariate tasks. The code of DDN is available at https://github.com/NBICLAB/DDN",
    "code_link": "https://github.com/NBICLAB/DDN"
  },
  "aaai2022_main_multiscalegenerativemodelsimprovingperformanceofagenerativemodelusingfeedbackfromotherdependentgenerativemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models",
    "authors": [
      "Changyu Chen",
      "Avinandan Bose",
      "Shih-Fen Cheng",
      "Arunesh Sinha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20568",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20568/20327",
    "published": "2022-02",
    "summary": "Realistic fine-grained multi-agent simulation of real-world complex systems is crucial for many downstream tasks such as reinforcement learning. Recent work has used generative models (GANs in particular) for providing high-fidelity simulation of real-world systems. However, such generative models are often monolithic and miss out on modeling the interaction in multi-agent systems. In this work, we take a first step towards building multiple interacting generative models (GANs) that reflects the interaction in real world. We build and analyze a hierarchical set-up where a higher-level GAN is conditioned on the output of multiple lower-level GANs. We present a technique of using feedback from the higher-level GAN to improve performance of lower-level GANs. We mathematically characterize the conditions under which our technique is impactful, including understanding the transfer learning nature of our set-up. We present three distinct experiments on synthetic data, time series data, and image domain, revealing the wide applicability of our technique.",
    "code_link": "https://github.com/cameron-chen/mgm"
  },
  "aaai2022_main_simultaneouslylearningstochasticandadversarialbanditsundertheposition-basedmodel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Simultaneously Learning Stochastic and Adversarial Bandits under the Position-Based Model",
    "authors": [
      "Cheng Chen",
      "Canzhe Zhao",
      "Shuai Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20569",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20569/20328",
    "published": "2022-02",
    "summary": "Online learning to rank (OLTR) interactively learns to choose lists of items from a large collection based on certain click models that describe users' click behaviors. Most recent works for this problem focus on the stochastic environment where the item attractiveness is assumed to be invariant during the learning process. In many real-world scenarios, however, the environment could be dynamic or even arbitrarily changing. This work studies the OLTR problem in both stochastic and adversarial environments under the position-based model (PBM). We propose a method based on the follow-the-regularized-leader (FTRL) framework with Tsallis entropy and develop a new self-bounding constraint especially designed for PBM. We prove the proposed algorithm simultaneously achieves O(log T) regret in the stochastic environment and O(m\u221anT) regret in the adversarial environment, where T is the number of rounds, n is the number of items and m is the number of positions. We also provide a lower bound of order \u2126(m\u221anT) for adversarial PBM, which matches our upper bound and improves over the state-of-the-art lower bound. The experiments show that our algorithm could simultaneously learn in both stochastic and adversarial environments and is competitive compared to existing methods that are designed for a single environment.",
    "code_link": ""
  },
  "aaai2022_main_clusteringinterval-censoredtime-seriesfordiseasephenotyping": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Clustering Interval-Censored Time-Series for Disease Phenotyping",
    "authors": [
      "Irene Y. Chen",
      "Rahul G. Krishnan",
      "David Sontag"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20570",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20570/20329",
    "published": "2022-02",
    "summary": "Unsupervised learning is often used to uncover clusters in data. However, different kinds of noise may impede the discovery of useful patterns from real-world time-series data. In this work, we focus on mitigating the interference of interval censoring in the task of clustering for disease phenotyping. We develop a deep generative, continuous-time model of time-series data that clusters time-series while correcting for censorship time. We provide conditions under which clusters and the amount of delayed entry may be identified from data under a noiseless model. On synthetic data, we demonstrate accurate, stable, and interpretable results that outperform several benchmarks. On real-world clinical datasets of heart failure and Parkinson's disease patients, we study how interval censoring can adversely affect the task of disease phenotyping. Our model corrects for this source of error and recovers known clinical subtypes.",
    "code_link": ""
  },
  "aaai2022_main_efficientrobusttrainingviabackwardsmoothing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Robust Training via Backward Smoothing",
    "authors": [
      "Jinghui Chen",
      "Yu Cheng",
      "Zhe Gan",
      "Quanquan Gu",
      "Jingjing Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20571",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20571/20330",
    "published": "2022-02",
    "summary": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational costs due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve fast Adversarial Training by performing a single-step attack with random initialization. However, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. Following this new perspective, we also propose a new initialization strategy, backward smoothing, to further improve the stability and model robustness over single-step robust training methods.Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method while using much less training time (~3x improvement with the same training schedule).",
    "code_link": "https://github.com/fra31/auto-attack"
  },
  "aaai2022_main_anonlinelearningapproachtosequentialuser-centricselectionproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Online Learning Approach to Sequential User-Centric Selection Problems",
    "authors": [
      "Junpu Chen",
      "Hong Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20572",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20572/20331",
    "published": "2022-02",
    "summary": "This paper proposes a new variant of multi-play MAB model, to capture important factors of the sequential user-centric selection problem arising from mobile edge computing, ridesharing applications, etc. In the proposed model, each arm is associated withdiscrete units of resources, each play is associate with movement costs and multiple plays can pull the same arm simultaneously. To learn the optimal action profile (an action profile prescribes the arm that each play pulls), there are two challenges: (1) the number of action profiles is large, i.e., M^K, where K and M denote the number of plays and arms respectively; (2) feedbacks on action profiles are not available, but instead feedbacks on some model parameters can be observed.To address the first challenge, we formulate a completed weighted bipartite graph to capture key factors of the offline decision problem with given model parameters. We identify the correspondence between action profiles and a special class of matchings of the graph. We also identify a dominance structure of this class of matchings. This correspondence and dominance structure enable us to design an algorithm named OffOptActPrf to locate the optimal action efficiently. To address the second challenge, we design an OnLinActPrf algorithm. We design estimators for model parameters and use these estimators to design a Quasi-UCB index for each action profile. The OnLinActPrf uses OffOptActPrf as a subroutine to select the action profile with the largest Quasi-UCB index. We conduct extensive experiments to validate the efficiency of OnLinActPrf.",
    "code_link": ""
  },
  "aaai2022_main_betterparameter-freestochasticoptimizationwithodeupdatesforcoin-betting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Better Parameter-Free Stochastic Optimization with ODE Updates for Coin-Betting",
    "authors": [
      "Keyi Chen",
      "John Langford",
      "Francesco Orabona"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20573",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20573/20332",
    "published": "2022-02",
    "summary": "Parameter-free stochastic gradient descent (PFSGD) algorithms do not require setting learning rates while achieving optimal theoretical performance. In practical applications, however, there remains an empirical gap between tuned stochastic gradient descent (SGD) and PFSGD. In this paper, we close the empirical gap with a new parameter-free algorithm based on continuous-time Coin-Betting on truncated models. The new update is derived through the solution of an Ordinary Differential Equation (ODE) and solved in a closed form. We show empirically that this new parameter-free algorithm outperforms algorithms with the ``best default'' learning rates and almost matches the performance of finely tuned baselines without anything to tune.",
    "code_link": ""
  },
  "aaai2022_main_mutualnearestneighborcontrastandhybridprototypeself-trainingforuniversaldomainadaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Mutual Nearest Neighbor Contrast and Hybrid Prototype Self-Training for Universal Domain Adaptation",
    "authors": [
      "Liang Chen",
      "Qianjin Du",
      "Yihang Lou",
      "Jianzhong He",
      "Tao Bai",
      "Minghua Deng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20574",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20574/20333",
    "published": "2022-02",
    "summary": "Universal domain adaptation (UniDA) aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain under domain shift and category shift. Without prior category overlap information, it is challenging to simultaneously align the common categories between two domains and separate their respective private categories. Additionally, previous studies utilize the source classifier's prediction to obtain various known labels and one generic \"unknown\" label of target samples. However, over-reliance on learned classifier knowledge is inevitably biased to source data, ignoring the intrinsic structure of target domain. Therefore, in this paper, we propose a novel two-stage UniDA framework called MATHS based on the principle of mutual nearest neighbor contrast and hybrid prototype discrimination. In the first stage, we design an efficient mutual nearest neighbor contrastive learning scheme to achieve feature alignment, which exploits the instance-level affinity relationship to uncover the intrinsic structure of two domains. We introduce a bimodality hypothesis for the maximum discriminative probability distribution to detect the possible target private samples, and present a data-based statistical approach to separate the common and private categories. In the second stage, to obtain more reliable label predictions, we propose an incremental pseudo-classifier for target data only, which is driven by the hybrid representative prototypes. A confidence-guided prototype contrastive loss is designed to optimize the category allocation uncertainty via a self-training mechanism. Extensive experiments on three benchmarks demonstrate that MATHS outperforms previous state-of-the-arts on most UniDA settings.",
    "code_link": ""
  },
  "aaai2022_main_evidentialneighborhoodcontrastivelearningforuniversaldomainadaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation",
    "authors": [
      "Liang Chen",
      "Yihang Lou",
      "Jianzhong He",
      "Tao Bai",
      "Minghua Deng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20575",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20575/20334",
    "published": "2022-02",
    "summary": "Universal domain adaptation (UniDA) aims to transfer the knowledge learned from a labeled source domain to an unlabeled target domain without any constraints on the label sets. However, domain shift and category shift make UniDA extremely challenging, mainly attributed to the requirement of identifying both shared \u201cknown\u201d samples and private \u201cunknown\u201d samples. Previous methods barely exploit the intrinsic manifold structure relationship between two domains for feature alignment, and they rely on the softmax-based scores with class competition nature to detect underlying \u201cunknown\u201d samples. Therefore, in this paper, we propose a novel evidential neighborhood contrastive learning framework called TNT to address these issues. Specifically, TNT first proposes a new domain alignment principle: semantically consistent samples should be geometrically adjacent to each other, whether within or across domains. From this criterion, a cross-domain multi-sample contrastive loss based on mutual nearest neighbors is designed to achieve common category matching and private category separation. Second, toward accurate \u201cunknown\u201d sample detection, TNT introduces a class competition-free uncertainty score from the perspective of evidential deep learning. Instead of setting a single threshold, TNT learns a category-aware heterogeneous threshold vector to reject diverse \u201cunknown\u201d samples. Extensive experiments on three benchmarks demonstrate that TNT significantly outperforms previous state-of-the-art UniDA methods.",
    "code_link": ""
  },
  "aaai2022_main_zerostabilitywellpredictsperformanceofconvolutionalneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Zero Stability Well Predicts Performance of Convolutional Neural Networks",
    "authors": [
      "Liangming Chen",
      "Long Jin",
      "Mingsheng Shang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20576",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20576/20335",
    "published": "2022-02",
    "summary": "The question of what kind of convolutional neural network (CNN) structure performs well is fascinating. In this work, we move toward the answer with one more step by connecting zero stability and model performance. Specifically, we found that if a discrete solver of an ordinary differential equation is zero stable, the CNN corresponding to that solver performs well. We first give the interpretation of zero stability in the context of deep learning and then investigate the performance of existing first- and second-order CNNs under different zero-stable circumstances. Based on the preliminary observation, we provide a higher-order discretization to construct CNNs and then propose a zero-stable network (ZeroSNet). To guarantee zero stability of the ZeroSNet, we first deduce a structure that meets consistency conditions and then give a zero stable region of a training-free parameter. By analyzing the roots of a characteristic equation, we theoretically obtain the optimal coefficients of feature maps. Empirically, we present our results from three aspects: We provide extensive empirical evidence of different depth on different datasets to show that the moduli of the characteristic equation's roots are the keys for the performance of CNNs that require historical features; Our experiments show that ZeroSNet outperforms existing CNNs which is based on high-order discretization; ZeroSNets show better robustness against noises on the input. The source code is available at https://github.com/logichen/ZeroSNet.",
    "code_link": "https://github.com/logichen/ZeroSNet"
  },
  "aaai2022_main_semi-supervisedlearningwithmulti-headco-training": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semi-supervised Learning with Multi-Head Co-Training",
    "authors": [
      "Mingcai Chen",
      "Yuntao Du",
      "Yi Zhang",
      "Shuwei Qian",
      "Chongjun Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20577",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20577/20336",
    "published": "2022-02",
    "summary": "Co-training, extended from self-training, is one of the frameworks for semi-supervised learning. Without natural split of features, single-view co-training works at the cost of training extra classifiers, where the algorithm should be delicately designed to prevent individual classifiers from collapsing into each other. To remove these obstacles which deter the adoption of single-view co-training, we present a simple and efficient algorithm Multi-Head Co-Training. By integrating base learners into a multi-head structure, the model is in a minimal amount of extra parameters. Every classification head in the unified model interacts with its peers through a \u201cWeak and Strong Augmentation\u201d strategy, in which the diversity is naturally brought by the strong data augmentation. Therefore, the proposed method facilitates single-view co-training by 1). promoting diversity implicitly and 2). only requiring a small extra computational overhead. The effectiveness of Multi-Head Co-Training is demonstrated in an empirical study on standard semi-supervised learning benchmarks.",
    "code_link": "https://github.com/hollance/reliabilitydiagrams"
  },
  "aaai2022_main_instanceselectionabayesiandecisiontheoryperspective": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Instance Selection: A Bayesian Decision Theory Perspective",
    "authors": [
      "Qingqiang Chen",
      "Fuyuan Cao",
      "Ying Xing",
      "Jiye Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20578",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20578/20337",
    "published": "2022-02",
    "summary": "In this paper, we consider the problem of lacking theoretical foundation and low execution efficiency of the instance selection methods based on the k-nearest neighbour rule when processing large-scale data. We point out that the core idea of these methods can be explained from the perspective of Bayesian decision theory, that is, to find which instances are reducible, irreducible, and deleterious. Then, based on the percolation theory, we establish the relationship between these three types of instances and local homogeneous cluster (i.e., a set of instances with the same labels). Finally, we propose a method based on an accelerated k-means algorithm to construct local homogeneous clusters and remove the superfluous instances. The performance of our method is studied on extensive synthetic and benchmark data sets. Our proposed method can handle large-scale data more effectively than the state-of-the-art instance selection methods. All code and data results are available at https://github.com/CQQXY161120/Instance-Selection.",
    "code_link": ""
  },
  "aaai2022_main_input-specificrobustnesscertificationforrandomizedsmoothing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Input-Specific Robustness Certification for Randomized Smoothing",
    "authors": [
      "Ruoxin Chen",
      "Jie Li",
      "Junchi Yan",
      "Ping Li",
      "Bin Sheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20579",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20579/20338",
    "published": "2022-02",
    "summary": "Although randomized smoothing has demonstrated high certified robustness and superior scalability to other certified defenses, the high computational overhead of the robustness certification bottlenecks the practical applicability, as it depends heavily on the large sample approximation for estimating the confidence interval. In existing works, the sample size for the confidence interval is universally set and agnostic to the input for prediction. This Input-Agnostic Sampling (IAS) scheme may yield a poor Average Certified Radius (ACR)-runtime trade-off which calls for improvement. In this paper, we propose Input-Specific Sampling (ISS) acceleration to achieve the cost-effectiveness for robustness certification, in an adaptive way of reducing the sampling size based on the input characteristic. Furthermore, our method universally controls the certified radius decline from the ISS sample size reduction. The empirical results on CIFAR-10 and ImageNet show that ISS can speed up the certification by more than three times at a limited cost of 0.05 certified radius. Meanwhile, ISS surpasses IAS on the average certified radius across the extensive hyperparameter settings. Specifically, ISS achieves ACR=0.958 on ImageNet in 250 minutes, compared to ACR=0.917 by IAS under the same condition. We release our code in https://github.com/roy-ch/Input-Specific-Certification.",
    "code_link": "https://github.com/roy-ch/Input-Specific-Certification"
  },
  "aaai2022_main_multimodaladversariallylearnedinferencewithfactorizeddiscriminators": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multimodal Adversarially Learned Inference with Factorized Discriminators",
    "authors": [
      "Wenxue Chen",
      "Jianke Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20580",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20580/20339",
    "published": "2022-02",
    "summary": "Learning from multimodal data is an important research topic in machine learning, which has the potential to obtain better representations. In this work, we propose a novel approach to generative modeling of multimodal data based on generative adversarial networks. To learn a coherent multimodal generative model, we show that it is necessary to align different encoder distributions with the joint decoder distribution simultaneously. To this end, we construct a specific form of the discriminator to enable our model to utilize data efficiently, which can be trained constrastively. By taking advantage of contrastive learning through factorizing the discriminator, we train our model on unimodal data. We have conducted experiments on the benchmark datasets, whose promising results show that our proposed approach outperforms the-state-ofthe-art methods on a variety of metrics. The source code is publicly available at https://github.com/6b5d/mmali.",
    "code_link": "https://github.com/6b5d/mmali"
  },
  "aaai2022_main_imbalance-awareupliftmodelingforobservationaldata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Imbalance-Aware Uplift Modeling for Observational Data",
    "authors": [
      "Xuanying Chen",
      "Zhining Liu",
      "Li Yu",
      "Liuyi Yao",
      "Wenpeng Zhang",
      "Yi Dong",
      "Lihong Gu",
      "Xiaodong Zeng",
      "Yize Tan",
      "Jinjie Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20581",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20581/20340",
    "published": "2022-02",
    "summary": "Uplift modeling aims to model the incremental impact of a treatment on an individual outcome, which has attracted great interests of researchers and practitioners from different communities. Existing uplift modeling methods rely on either the data collected from randomized controlled trials (RCTs) or the observational data which is more realistic. However, we notice that on the observational data, it is often the case that only a small number of subjects receive treatment, but finally infer the uplift on a much large group of subjects. Such highly imbalanced data is common in various fields such as marketing and medical treatment but it is rarely handled by existing works. In this paper, we theoretically and quantitatively prove that the existing representative methods, transformed outcome (TOM) and doubly robust (DR), suffer from large bias and deviation on highly imbalanced datasets with skewed propensity scores, mainly because they are proportional to the reciprocal of the propensity score. To reduce the bias and deviation of uplift modeling with an imbalanced dataset, we propose an imbalance-aware uplift modeling (IAUM) method via constructing a robust proxy outcome, which adaptively combines the doubly robust estimator and the imputed treatment effects based on the propensity score. We theoretically prove that IAUM can obtain a better bias-variance trade-off than existing methods on a highly imbalanced dataset. We conduct extensive experiments on a synthetic dataset and two real-world datasets, and the experimental results well demonstrate the superiority of our method over state-of-the-art.",
    "code_link": ""
  },
  "aaai2022_main_kamtheorymeetsstatisticallearningtheoryhamiltonianneuralnetworkswithnon-zerotrainingloss": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-zero Training Loss",
    "authors": [
      "Yuhan Chen",
      "Takashi Matsubara",
      "Takaharu Yaguchi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20582",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20582/20341",
    "published": "2022-02",
    "summary": "Many physical phenomena are described by Hamiltonian mechanics using an energy function (Hamiltonian). Recently, the Hamiltonian neural network, which approximates the Hamiltonian by a neural network, and its extensions have attracted much attention. This is a very powerful method, but theoretical studies are limited. In this study, by combining the statistical learning theory and KAM theory, we provide a theoretical analysis of the behavior of Hamiltonian neural networks when the learning error is not completely zero. A Hamiltonian neural network with non-zero errors can be considered as a perturbation from the true dynamics, and the perturbation theory of the Hamilton equation is widely known as KAM theory. To apply KAM theory, we provide a generalization error bound for Hamiltonian neural networks by deriving an estimate of the covering number of the gradient of the multi-layer perceptron, which is the key ingredient of the model. This error bound gives a sup-norm bound on the Hamiltonian that is required in the application of KAM theory.",
    "code_link": "https://github.com/tksmatsubara/discrete-autograd"
  },
  "aaai2022_main_bscnetsblocksimplicialcomplexneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "BScNets: Block Simplicial Complex Neural Networks",
    "authors": [
      "Yuzhou Chen",
      "Yulia R. Gel",
      "H. Vincent Poor"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20583",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20583/20342",
    "published": "2022-02",
    "summary": "Simplicial neural networks (SNNs) have recently emerged as a new direction in graph learning which expands the idea of convolutional architectures from node space to simplicial complexes on graphs. Instead of predominantly assessing pairwise relations among nodes as in the current practice, simplicial complexes allow us to describe higher-order interactions and multi-node graph structures. By building upon connection between the convolution operation and the new block Hodge-Laplacian, we propose the first SNN for link prediction. Our new Block Simplicial Complex Neural Networks (BScNets) model generalizes existing graph convolutional network (GCN) frameworks by systematically incorporating salient interactions among multiple higher-order graph structures of different dimensions. We discuss theoretical foundations behind BScNets and illustrate its utility for link prediction on eight real-world and synthetic datasets. Our experiments indicate that BScNets outperforms the state-of-the-art models by a significant margin while maintaining low computation costs. Finally, we show utility of BScNets as a new promising alternative for tracking spread of infectious diseases such as COVID-19 and measuring the effectiveness of the healthcare risk mitigation strategies.",
    "code_link": ""
  },
  "aaai2022_main_asm2tvanadaptivesemi-supervisedmulti-taskmulti-viewlearningframeworkforhumanactivityrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ASM2TV: An Adaptive Semi-supervised Multi-Task Multi-View Learning Framework for Human Activity Recognition",
    "authors": [
      "Zekai Chen",
      "Xiao Zhang",
      "Xiuzhen Cheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20584",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20584/20343",
    "published": "2022-02",
    "summary": "Many real-world scenarios, such as human activity recognition (HAR) in IoT, can be formalized as a multi-task multi-view learning problem. Each specific task consists of multiple shared feature views collected from multiple sources, either homogeneous or heterogeneous. Common among recent approaches is to employ a typical hard/soft sharing strategy at the initial phase separately for each view across tasks to uncover common knowledge, underlying the assumption that all views are conditionally independent. On the one hand, multiple views across tasks possibly relate to each other under practical situations. On the other hand, supervised methods might be insufficient when labeled data is scarce. To tackle these challenges, we introduce a novel framework ASM2TV for semi-supervised multi-task multi-view learning. We present a new perspective named gating control policy, a learnable task-view-interacted sharing policy that adaptively selects the most desirable candidate shared block for any view across any task, which uncovers more fine-grained task-view-interacted relatedness and improves inference efficiency. Significantly, our proposed gathering consistency adaption procedure takes full advantage of large amounts of unlabeled fragmented time-series, making it a general framework that accommodates a wide range of applications. Experiments on two diverse real-world HAR benchmark datasets collected from various subjects and sources demonstrate our framework's superiority over other state-of-the-arts. Anonymous codes are available at https://github.com/zachstarkk/ASM2TV.",
    "code_link": "https://github.com/zachstarkk/ASM2TV"
  },
  "aaai2022_main_identificationoflinearlatentvariablemodelwitharbitrarydistribution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Identification of Linear Latent Variable Model with Arbitrary Distribution",
    "authors": [
      "Zhengming Chen",
      "Feng Xie",
      "Jie Qiao",
      "Zhifeng Hao",
      "Kun Zhang",
      "Ruichu Cai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20585",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20585/20344",
    "published": "2022-02",
    "summary": "An important problem across multiple disciplines is to infer and understand meaningful latent variables. One strategy commonly used is to model the measured variables in terms of the latent variables under suitable assumptions on the connectivity from the latents to the measured (known as measurement model). Furthermore, it might be even more interesting to discover the causal relations among the latent variables (known as structural model). Recently, some methods have been proposed to estimate the structural model by assuming that the noise terms in the measured and latent variables are non-Gaussian. However, they are not suitable when some of the noise terms become Gaussian. To bridge this gap, we investigate the problem of identification of the structural model with arbitrary noise distributions. We provide necessary and sufficient condition under which the structural model is identifiable: it is identifiable iff for each pair of adjacent latent variables Lx, Ly, (1) at least one of Lx and Ly has non-Gaussian noise, or (2) at least one of them has a non-Gaussian ancestor and is not d-separated from the non-Gaussian component of this ancestor by the common causes of Lx and Ly. This identifiability result relaxes the non-Gaussianity requirements to only a (hopefully small) subset of variables, and accordingly elegantly extends the application scope of the structural model. Based on the above identifiability result, we further propose a practical algorithm to learn the structural model. We verify the correctness of the identifiability result and the effectiveness of the proposed method through empirical studies.",
    "code_link": ""
  },
  "aaai2022_main_dpnasneuralarchitecturesearchfordeeplearningwithdifferentialprivacy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DPNAS: Neural Architecture Search for Deep Learning with Differential Privacy",
    "authors": [
      "Anda Cheng",
      "Jiaxing Wang",
      "Xi Sheryl Zhang",
      "Qiang Chen",
      "Peisong Wang",
      "Jian\n      Cheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20586",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20586/20345",
    "published": "2022-02",
    "summary": "Training deep neural networks (DNNs) for meaningful differential privacy (DP) guarantees severely degrades model utility. In this paper, we demonstrate that the architecture of DNNs has a significant impact on model utility in the context of private deep learning, whereas its effect is largely unexplored in previous studies. In light of this missing, we propose the very first framework that employs neural architecture search to automatic model design for private deep learning, dubbed as DPNAS. To integrate private learning with architecture search, a DP-aware approach is introduced for training candidate models composed on a delicately defined novel search space. We empirically certify the effectiveness of the proposed framework. The searched model DPNASNet achieves state-of-the-art privacy/utility trade-offs, e.g., for the privacy budget of (epsilon, delta)=(3, 1e-5), our model obtains test accuracy of 98.57% on MNIST, 88.09% on FashionMNIST, and 68.33% on CIFAR-10. Furthermore, by studying the generated architectures, we provide several intriguing findings of designing private-learning-friendly DNNs, which can shed new light on model design for deep learning with differential privacy.",
    "code_link": ""
  },
  "aaai2022_main_graphneuralcontrolleddifferentialequationsfortrafficforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph Neural Controlled Differential Equations for Traffic Forecasting",
    "authors": [
      "Jeongwhan Choi",
      "Hwangyong Choi",
      "Jeehyun Hwang",
      "Noseong Park"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20587",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20587/20346",
    "published": "2022-02",
    "summary": "Traffic forecasting is one of the most popular spatio-temporal tasks in the field of machine learning. A prevalent approach in the field is to combine graph convolutional networks and recurrent neural networks for the spatio-temporal processing. There has been fierce competition and many novel methods have been proposed. In this paper, we present the method of spatio-temporal graph neural controlled differential equation (STG-NCDE). Neural controlled differential equations (NCDEs) are a breakthrough concept for processing sequential data. We extend the concept and design two NCDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework. We conduct experiments with 6 benchmark datasets and 20 baselines. STG-NCDE shows the best accuracy in all cases, outperforming all those 20 baselines by non-trivial margins.",
    "code_link": ""
  },
  "aaai2022_main_differentiallyprivateregretminimizationinepisodicmarkovdecisionprocesses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Differentially Private Regret Minimization in Episodic Markov Decision Processes",
    "authors": [
      "Sayak Ray Chowdhury",
      "Xingyu Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20588",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20588/20347",
    "published": "2022-02",
    "summary": "We study regret minimization in finite horizon tabular Markov decision processes (MDPs) under the constraints of differential privacy (DP). This is motivated by the widespread applications of reinforcement learning (RL) in real-world sequential decision making problems, where protecting users' sensitive and private information is becoming paramount. We consider two variants of DP -- joint DP (JDP), where a centralized agent is responsible for protecting users' sensitive data and local DP (LDP), where information needs to be protected directly on the user side. We first propose two general frameworks -- one for policy optimization and another for value iteration -- for designing private, optimistic RL algorithms. We then instantiate these frameworks with suitable privacy mechanisms to satisfy JDP and LDP requirements, and simultaneously obtain sublinear regret guarantees. The regret bounds show that under JDP, the cost of privacy is only a lower order additive term, while for a stronger privacy protection under LDP, the cost suffered is multiplicative. Finally, the regret bounds are obtained by a unified analysis, which, we believe, can be extended beyond tabular MDPs.",
    "code_link": ""
  },
  "aaai2022_main_learningbycompetitionofself-interestedreinforcementlearningagents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning by Competition of Self-Interested Reinforcement Learning Agents",
    "authors": [
      "Stephen Chung"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20589",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20589/20348",
    "published": "2022-02",
    "summary": "An artificial neural network can be trained by uniformly broadcasting a reward signal to units that implement a REINFORCE learning rule. Though this presents a biologically plausible alternative to backpropagation in training a network, the high variance associated with it renders it impractical to train deep networks. The high variance arises from the inefficient structural credit assignment since a single reward signal is used to evaluate the collective action of all units. To facilitate structural credit assignment, we propose replacing the reward signal to hidden units with the change in the L2 norm of the unit's outgoing weight. As such, each hidden unit in the network is trying to maximize the norm of its outgoing weight instead of the global reward, and thus we call this learning method Weight Maximization. We prove that Weight Maximization is approximately following the gradient of rewards in expectation. In contrast to backpropagation, Weight Maximization can be used to train both continuous-valued and discrete-valued units. Moreover, Weight Maximization solves several major issues of backpropagation relating to biological plausibility. Our experiments show that a network trained with Weight Maximization can learn significantly faster than REINFORCE and slightly slower than backpropagation. Weight Maximization illustrates an example of cooperative behavior automatically arising from a population of self-interested agents in a competitive game without any central coordination.",
    "code_link": ""
  },
  "aaai2022_main_howtodistributedataacrosstasksformeta-learning?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How to Distribute Data across Tasks for Meta-Learning?",
    "authors": [
      "Alexandru Cioba",
      "Michael Bromberg",
      "Qian Wang",
      "Ritwik Niyogi",
      "Georgios\n      Batzolis",
      "Jezabel Garcia",
      "Da-shan Shiu",
      "Alberto Bernacchia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20590",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20590/20349",
    "published": "2022-02",
    "summary": "Meta-learning models transfer the knowledge acquired from previous tasks to quickly learn new ones. They are trained on benchmarks with a fixed number of data points per task. This number is usually arbitrary and it is unknown how it affects performance at testing. Since labelling of data is expensive, finding the optimal allocation of labels across training tasks may reduce costs. Given a fixed budget of labels, should we use a small number of highly labelled tasks, or many tasks with few labels each? Should we allocate more labels to some tasks and less to others? We show that: 1) If tasks are homogeneous, there is a uniform optimal allocation, whereby all tasks get the same amount of data; 2) At fixed budget, there is a trade-off between number of tasks and number of data points per task, with a unique solution for the optimum; 3) When trained separately, harder task should get more data, at the cost of a smaller number of tasks; 4) When training on a mixture of easy and hard tasks, more data should be allocated to easy tasks. Interestingly, Neuroscience experiments have shown that human visual skills also transfer better from easy tasks. We prove these results mathematically on mixed linear regression, and we show empirically that the same results hold for few-shot image classification on CIFAR-FS and mini-ImageNet. Our results provide guidance for allocating labels across tasks when collecting data for meta-learning.",
    "code_link": ""
  },
  "aaai2022_main_similaritysearchforefficientactivelearningandsearchofrareconcepts": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts",
    "authors": [
      "Cody Coleman",
      "Edward Chou",
      "Julian Katz-Samuels",
      "Sean Culatana",
      "Peter\n      Bailis",
      "Alexander C. Berg",
      "Robert Nowak",
      "Roshan Sumbaly",
      "Matei Zaharia",
      "I.\n      Zeki Yalniz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20591",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20591/20350",
    "published": "2022-02",
    "summary": "Many active learning and search approaches are intractable for large-scale industrial settings with billions of unlabeled examples. Existing approaches search globally for the optimal examples to label, scaling linearly or even quadratically with the unlabeled data. In this paper, we improve the computational efficiency of active learning and search methods by restricting the candidate pool for labeling to the nearest neighbors of the currently labeled set instead of scanning over all of the unlabeled data. We evaluate several selection strategies in this setting on three large-scale computer vision datasets: ImageNet, OpenImages, and a de-identified and aggregated dataset of 10 billion publicly shared images provided by a large internet company. Our approach achieved similar mAP and recall as the traditional global approach while reducing the computational cost of selection by up to three orders of magnitude, enabling web-scale active learning.",
    "code_link": ""
  },
  "aaai2022_main_learninginfluenceadoptioninheterogeneousnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Influence Adoption in Heterogeneous Networks",
    "authors": [
      "Vincent Conitzer",
      "Debmalya Panigrahi",
      "Hanrui Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20592",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20592/20351",
    "published": "2022-02",
    "summary": "We study the problem of learning influence adoption in networks. In this problem, a communicable entity (such as an infectious disease, a computer virus, or a social media meme) propagates through a network, and the goal is to learn the state of each individual node by sampling only a small number of nodes and observing/testing their states. We study this problem in heterogeneous networks, in which each individual node has a set of distinct features that determine how it is affected by the propagating entity. We give an efficient algorithm with nearly optimal sample complexity for two variants of this learning problem, corresponding to symptomatic and asymptomatic spread. In each case, the optimal sample complexity naturally generalizes the complexity of learning how nodes are affected in isolation, and the complexity of learning influence adoption in a homogeneous network.",
    "code_link": ""
  },
  "aaai2022_main_graph-wisecommonlatentfactorextractionforunsupervisedgraphrepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph-Wise Common Latent Factor Extraction for Unsupervised Graph Representation Learning",
    "authors": [
      "Thilini Cooray",
      "Ngai-Man Cheung"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20593",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20593/20352",
    "published": "2022-02",
    "summary": "Unsupervised graph-level representation learning plays a crucial role in a variety of tasks such as molecular property prediction and community analysis, especially when data annotation is expensive. Currently, most of the best-performing graph embedding methods are based on Infomax principle. The performance of these methods highly depends on the selection of negative samples and hurt the performance, if the samples were not carefully selected. Inter-graph similarity-based methods also suffer if the selected set of graphs for similarity matching is low in quality. To address this, we focus only on utilizing the current input graph for embedding learning. We are motivated by an observation from real-world graph generation processes where the graphs are formed based on one or more global factors which are common to all elements of the graph (e.g., topic of a discussion thread, solubility level of a molecule). We hypothesize extracting these common factors could be highly beneficial. Hence, this work proposes a new principle for unsupervised graph representation learning: Graph-wise Common latent Factor EXtraction (GCFX). We further propose a deep model for GCFX, deepGCFX, based on the idea of reversing the above-mentioned graph generation process which could explicitly extract common latent factors from an input graph and achieve improved results on downstream tasks to the current state-of-the-art. Through extensive experiments and analysis, we demonstrate that, while extracting common latent factors is beneficial for graph-level tasks to alleviate distractions caused by local variations of individual nodes or local neighbourhoods, it also benefits node-level tasks by enabling long-range node dependencies, especially for disassortative graphs.",
    "code_link": "https://github.com/thilinicooray/deepGCFX"
  },
  "aaai2022_main_reinforcementlearningwithstochasticrewardmachines": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reinforcement Learning with Stochastic Reward Machines",
    "authors": [
      "Jan Corazza",
      "Ivan Gavran",
      "Daniel Neider"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20594",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20594/20353",
    "published": "2022-02",
    "summary": "Reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequences of actions.However, existing algorithms for learning reward machines assume an overly idealized setting where rewards have to be free of noise.To overcome this practical limitation, we introduce a novel type of reward machines, called stochastic reward machines, and an algorithm for learning them.Our algorithm, based on constraint solving, learns minimal stochastic reward machines from the explorations of a reinforcement learning agent.This algorithm can easily be paired with existing reinforcement learning algorithms for reward machines and guarantees to converge to an optimal policy in the limit.We demonstrate the effectiveness of our algorithm in two case studies and show that it outperforms both existing methods and a naive approach for handling noisy reward functions.",
    "code_link": ""
  },
  "aaai2022_main_sparse-rsaversatileframeworkforquery-efficientsparseblack-boxadversarialattacks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sparse-RS: A Versatile Framework for Query-Efficient Sparse Black-Box Adversarial Attacks",
    "authors": [
      "Francesco Croce",
      "Maksym Andriushchenko",
      "Naman D. Singh",
      "Nicolas\n      Flammarion",
      "Matthias Hein"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20595",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20595/20354",
    "published": "2022-02",
    "summary": "We propose a versatile framework based on random search, Sparse-RS, for score-based sparse targeted and untargeted attacks in the black-box setting. Sparse-RS does not rely on substitute models and achieves state-of-the-art success rate and query efficiency for multiple sparse attack models: L0-bounded perturbations, adversarial patches, and adversarial frames. The L0-version of untargeted Sparse-RS outperforms all black-box and even all white-box attacks for different models on MNIST, CIFAR-10, and ImageNet. Moreover, our untargeted Sparse-RS achieves very high success rates even for the challenging settings of 20x20 adversarial patches and 2-pixel wide adversarial frames for 224x224 images. Finally, we show that Sparse-RS can be applied to generate targeted universal adversarial patches where it significantly outperforms the existing approaches. Our code is available at https://github.com/fra31/sparse-rs.",
    "code_link": ""
  },
  "aaai2022_main_learninglogicprogramsthoughdivide,constrain,andconquer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Logic Programs Though Divide, Constrain, and Conquer",
    "authors": [
      "Andrew Cropper"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20596",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20596/20355",
    "published": "2022-02",
    "summary": "We introduce an inductive logic programming approach that combines classical divide-and-conquer search with modern constraint-driven search. Our anytime approach can learn optimal, recursive, and large programs and supports predicate invention. Our experiments on three domains (classification, inductive general game playing, and program synthesis) show that our approach can increase predictive accuracies and reduce learning times.",
    "code_link": ""
  },
  "aaai2022_main_implicitgradientalignmentindistributedandfederatedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Implicit Gradient Alignment in Distributed and Federated Learning",
    "authors": [
      "Yatin Dandi",
      "Luis Barba",
      "Martin Jaggi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20597",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20597/20356",
    "published": "2022-02",
    "summary": "A major obstacle to achieving global convergence in distributed and federated learning is the misalignment of gradients across clients or mini-batches due to heterogeneity and stochasticity of the distributed data. In this work, we show that data heterogeneity can in fact be exploited to improve generalization performance through implicit regularization. One way to alleviate the effects of heterogeneity is to encourage the alignment of gradients across different clients throughout training. Our analysis reveals that this goal can be accomplished by utilizing the right optimization method that replicates the implicit regularization effect of SGD, leading to gradient alignment as well as improvements in test accuracies. Since the existence of this regularization in SGD completely relies on the sequential use of different mini-batches during training, it is inherently absent when training with large mini-batches. To obtain the generalization benefits of this regularization while increasing parallelism, we propose a novel GradAlign algorithm that induces the same implicit regularization while allowing the use of arbitrarily large batches in each update. We experimentally validate the benefits of our algorithm in different distributed and federated learning settings.",
    "code_link": ""
  },
  "aaai2022_main_howgoodarelow-rankapproximationsingaussianprocessregression?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How Good Are Low-Rank Approximations in Gaussian Process Regression?",
    "authors": [
      "Constantinos Daskalakis",
      "Petros Dellaportas",
      "Aristeidis Panos"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20598",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20598/20357",
    "published": "2022-02",
    "summary": "We provide guarantees for approximate Gaussian Process (GP) regression resulting from two common low-rank kernel approximations: based on random Fourier features, and based on truncating the kernel's Mercer expansion. In particular, we bound the Kullback\u2013Leibler divergence between an exact GP and one resulting from one of the afore-described low-rank approximations to its kernel, as well as between their corresponding predictive densities, and we also bound the error between predictive mean vectors and between predictive covariance matrices computed using the exact versus using the approximate GP. We provide experiments on both simulated data and standard benchmarks to evaluate the effectiveness of our theoretical bounds.",
    "code_link": ""
  },
  "aaai2022_main_koalaakalmanoptimizationalgorithmwithlossadaptivity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "KOALA: A Kalman Optimization Algorithm with Loss Adaptivity",
    "authors": [
      "Aram Davtyan",
      "Sepehr Sameni",
      "Llukman Cerkezi",
      "Givi Meishvili",
      "Adam\n      Bielski",
      "Paolo Favaro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20599",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20599/20358",
    "published": "2022-02",
    "summary": "Optimization is often cast as a deterministic problem, where the solution is found through some iterative procedure such as gradient descent. However, when training neural networks the loss function changes over (iteration) time due to the randomized selection of a subset of the samples. This randomization turns the optimization problem into a stochastic one. We propose to consider the loss as a noisy observation with respect to some reference optimum. This interpretation of the loss allows us to adopt Kalman filtering as an optimizer, as its recursive formulation is designed to estimate unknown parameters from noisy measurements. Moreover, we show that the Kalman Filter dynamical model for the evolution of the unknown parameters can be used to capture the gradient dynamics of advanced methods such as Momentum and Adam. We call this stochastic optimization method KOALA, which is short for Kalman Optimization Algorithm with Loss Adaptivity. KOALA is an easy to implement, scalable, and efficient method to train neural networks. We provide convergence analysis and show experimentally that it yields parameter estimates that are on par with or better than existing state of the art optimization algorithms across several neural network architectures and machine learning tasks, such as computer vision and language modeling. The project page with the code and the supplementary materials is available at https://araachie.github.io/koala/.",
    "code_link": ""
  },
  "aaai2022_main_first-orderconvexfittinganditsapplicationtoeconomicsandoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "First-Order Convex Fitting and Its Application to Economics and Optimization",
    "authors": [
      "Quinlan Dawkins",
      "Minbiao Han",
      "Haifeng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20600",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20600/20359",
    "published": "2022-02",
    "summary": "This paper studies a function fitting problem which we coinfirst-order convex fitting (FCF): given any two vector sequences x1, ..., xT and p1, ..., pT, when is it possible toefficiently constructa convex function f(x) that ``fits'' the two sequences in the first-order sense, i.e, the (sub)gradient of f(xi) equals precisely pi, for all i = 1, ..., T? Despite abasic question of convex analysis,FCF has surprisingly been overlooked in the past literature. With an efficient constructive proof, we provide a clean answer to this question:FCF is possibleif and only if the two sequences are permutation stable: p1 * x1 + ... + pT * xT is greater than or equal to p1 * x\u20191 + ... + pT * x\u2019T where x\u20191, ..., x\u2019T is any permutation of x1, ..., xT. Wedemonstrate the usefulness of FCF intwoapplications. First, we study how it can be used as an empirical risk minimization procedure to learn the original convex function. We provideefficient PAC-learnability bounds for special classes of convex functionslearned via FCF, and demonstrate its application to multiple economic problems where only function gradients (as opposed to function values)can be observed. Second,we empirically show how it can be used as a surrogate to significantly accelerate the minimization of the original convex function.",
    "code_link": ""
  },
  "aaai2022_main_gradienttemporaldifferencewithmomentumstabilityandconvergence": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Gradient Temporal Difference with Momentum: Stability and Convergence",
    "authors": [
      "Rohan Deb",
      "Shalabh Bhatnagar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20601",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20601/20360",
    "published": "2022-02",
    "summary": "Gradient temporal difference (Gradient TD) algorithms are a popular class of stochastic approximation (SA) algorithms used for policy evaluation in reinforcement learning. Here, we consider Gradient TD algorithms with an additional heavy ball momentum term and provide choice of step size and momentum parameter that ensures almost sure convergence of these algorithms asymptotically. In doing so, we decompose the heavy ball Gradient TD iterates into three separate iterates with different step sizes. We first analyze these iterates under one-timescale SA setting using results from current literature. However, the one-timescale case is restrictive and a more general analysis can be provided by looking at a three-timescale decomposition of the iterates. In the process we provide the first conditions for stability and convergence of general three-timescale SA. We then prove that the heavy ball Gradient TD algorithm is convergent using our three-timescale SA analysis. Finally, we evaluate these algorithms on standard RL problems and report improvement in performance over the vanilla algorithms.",
    "code_link": ""
  },
  "aaai2022_main_distillationofrlpolicieswithformalguaranteesviavariationalabstractionofmarkovdecisionprocesses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Distillation of RL Policies with Formal Guarantees via Variational Abstraction of Markov Decision Processes",
    "authors": [
      "Florent Delgrange",
      "Ann Now\u00e9",
      "Guillermo A. P\u00e9rez"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20602",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20602/20361",
    "published": "2022-02",
    "summary": "We consider the challenge of policy simplification and verification in the context of policies learned through reinforcement learning (RL) in continuous environments. In well-behaved settings, RL algorithms have convergence guarantees in the limit. While these guarantees are valuable, they are insufficient for safety-critical applications. Furthermore, they are lost when applying advanced techniques such as deep-RL. To recover guarantees when applying advanced RL algorithms tomore complex environments with (i) reachability, (ii) safety-constrained reachability, or (iii) discounted-reward objectives, we build upon the DeepMDP framework to derive new bisimulation bounds between the unknown environment and a learned discrete latent model of it. Our bisimulation bounds enable the application of formal methods for Markov decision processes. Finally, we show how one can use a policy obtained via state-of-the-art RL to efficiently train a variational autoencoder that yields a discrete latent model with provably approximately correct bisimulation guarantees. Additionally, we obtain a distilled version of the policy for the latent model.",
    "code_link": ""
  },
  "aaai2022_main_reducingflippingerrorsindeepneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reducing Flipping Errors in Deep Neural Networks",
    "authors": [
      "Xiang Deng",
      "Yun Xiao",
      "Bo Long",
      "Zhongfei Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20603",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20603/20362",
    "published": "2022-02",
    "summary": "Deep neural networks (DNNs) have been widely applied in various domains in artificial intelligence including computer vision and natural language processing.A DNN is typically trained for many epochs and then a validation dataset is used to select the DNN in an epoch (we simply call this epoch ``the last epoch\") as the final model for making predictions on unseen samples, while it usually cannot achieve a perfect accuracy on unseen samples. An interesting question is ``how many test (unseen) samples that a DNN misclassifies in the last epoch were ever correctly classified by the DNN before the last epoch?\". In this paper, we empirically study this question and find on several benchmark datasets that the vast majority of the misclassified samples in the last epoch were ever classified correctly before the last epoch, which means that the predictions for these samples were flipped from ``correct\" to ``wrong\". Motivated by this observation, we propose to restrict the behavior changes of a DNN on the correctly-classified samples so that the correct local boundaries can be maintained and the flipping error on unseen samples can be largely reduced. Extensive experiments on different benchmark datasets with different modern network architectures demonstrate that the proposed flipping error reduction (FER) approach can substantially improve the generalization, the robustness, and the transferability of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead.",
    "code_link": ""
  },
  "aaai2022_main_bayesianoptimizationoverpermutationspaces": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bayesian Optimization over Permutation Spaces",
    "authors": [
      "Aryan Deshwal",
      "Syrine Belakaria",
      "Janardhan Rao Doppa",
      "Dae Hyun Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20604",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20604/20363",
    "published": "2022-02",
    "summary": "Optimizing expensive to evaluate black-box functions over an input space consisting of all permutations of d objects is an important problem with many real-world applications. For example, placement of functional blocks in hardware design to optimize performance via simulations. The overall goal is to minimize the number of function evaluations to find high-performing permutations. The key challenge in solving this problem using the Bayesian optimization (BO) framework is to trade-off the complexity of statistical model and tractability of acquisition function optimization. In this paper, we propose and evaluate two algorithms for BO over Permutation Spaces (BOPS). First, BOPS-T employs Gaussian process (GP) surrogate model with Kendall kernels and a Tractable acquisition function optimization approach to select the sequence of permutations for evaluation. Second, BOPS-H employs GP surrogate model with Mallow kernels and a Heuristic search approach to optimize the acquisition function. We theoretically analyze the performance of BOPS-T to show that their regret grows sub-linearly. Our experiments on multiple synthetic and real-world benchmarks show that both BOPS-T and BOPS-H perform better than the state-of-the-art BO algorithm for combinatorial spaces. To drive future research on this important problem, we make new resources and real-world benchmarks available to the community.",
    "code_link": ""
  },
  "aaai2022_main_metapropagationnetworksforgraphfew-shotsemi-supervisedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Meta Propagation Networks for Graph Few-shot Semi-supervised Learning",
    "authors": [
      "Kaize Ding",
      "Jianling Wang",
      "James Caverlee",
      "Huan Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20605",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20605/20364",
    "published": "2022-02",
    "summary": "Inspired by the extensive success of deep learning, graph neural networks (GNNs) have been proposed to learn expressive node representations and demonstrated promising performance in various graph learning tasks. However, existing endeavors predominately focus on the conventional semi-supervised setting where relatively abundant gold-labeled nodes are provided. While it is often impractical due to the fact that data labeling is unbearably laborious and requires intensive domain knowledge, especially when considering the heterogeneity of graph-structured data. Under the few-shot semi-supervised setting, the performance of most of the existing GNNs is inevitably undermined by the overfitting and oversmoothing issues, largely owing to the shortage of labeled data. In this paper, we propose a decoupled network architecture equipped with a novel meta-learning algorithm to solve this problem. In essence, our framework Meta-PN infers high-quality pseudo labels on unlabeled nodes via a meta-learned label propagation strategy, which effectively augments the scarce labeled data while enabling large receptive fields during training. Extensive experiments demonstrate that our approach offers easy and substantial performance gains compared to existing techniques on various benchmark datasets. The implementation and extended manuscript of this work are publicly available at https://github.com/kaize0409/Meta-PN.",
    "code_link": "https://github.com/kaize0409/Meta-PN"
  },
  "aaai2022_main_onlinecertificationofpreference-basedfairnessforpersonalizedrecommendersystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Certification of Preference-Based Fairness for Personalized Recommender Systems",
    "authors": [
      "Virginie Do",
      "Sam Corbett-Davies",
      "Jamal Atif",
      "Nicolas Usunier"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20606",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20606/20365",
    "published": "2022-02",
    "summary": "Recommender systems are facing scrutiny because of their growing impact on the opportunities we have access to. Current audits for fairness are limited to coarse-grained parity assessments at the level of sensitive groups. We propose to audit for envy-freeness, a more granular criterion aligned with individual preferences: every user should prefer their recommendations to those of other users. Since auditing for envy requires to estimate the preferences of users beyond their existing recommendations, we cast the audit as a new pure exploration problem in multi-armed bandits. We propose a sample-efficient algorithm with theoretical guarantees that it does not deteriorate user experience. We also study the trade-offs achieved on real-world recommendation datasets.",
    "code_link": ""
  },
  "aaai2022_main_disentangledspatiotemporalgraphgenerativemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Disentangled Spatiotemporal Graph Generative Models",
    "authors": [
      "Yuanqi Du",
      "Xiaojie Guo",
      "Hengning Cao",
      "Yanfang Ye",
      "Liang Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20607",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20607/20366",
    "published": "2022-02",
    "summary": "Spatiotemporal graph represents a crucial data structure where the nodes and edges are embedded in a geometric space and their attribute values can evolve dynamically over time. Nowadays, spatiotemporal graph data is becoming increasingly popular and important, ranging from microscale (e.g. protein folding), to middle-scale (e.g. dynamic functional connectivity), to macro-scale (e.g. human mobility network). Although disentangling and understanding the correlations among spatial, temporal, and graph aspects have been a long-standing key topic in network science, they typically rely on network processes hypothesized by human knowledge. They usually fit well towards the properties that the predefined principles are tailored for, but usually cannot do well for the others, especially for many key domains where the human has yet very limited knowledge such as protein folding and biological neuronal networks. In this paper, we aim at pushing forward the modeling and understanding of spatiotemporal graphs via new disentangled deep generative models. Specifically, a new Bayesian model is proposed that factorizes spatiotemporal graphs into spatial, temporal, and graph factors as well as the factors that explain the interplay among them. A variational objective function and new mutual information thresholding algorithms driven by information bottleneck theory have been proposed to maximize the disentanglement among the factors with theoretical guarantees. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed model over the state-of-the-arts by up to 69.2% for graph generation and 41.5% for interpretability.",
    "code_link": ""
  },
  "aaai2022_main_learningfromthedarkboostinggraphconvolutionalneuralnetworkswithdiversenegativesamples": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning from the Dark: Boosting Graph Convolutional Neural Networks with Diverse Negative Samples",
    "authors": [
      "Wei Duan",
      "Junyu Xuan",
      "Maoying Qiao",
      "Jie Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20608",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20608/20367",
    "published": "2022-02",
    "summary": "Graph Convolutional Neural Networks (GCNs) have been generally accepted to be an effective tool for node representations learning. An interesting way to understand GCNs is to think of them as a message passing mechanism where each node updates its representation by accepting information from its neighbours (also known as positive samples). However, beyond these neighbouring nodes, graphs have a large, dark, all-but forgotten world in which we find the non-neighbouring nodes (negative samples). In this paper, we show that this great dark world holds a substantial amount of information that might be useful for representation learning. Most specifically, it can provide negative information about the node representations. Our overall idea is to select appropriate negative samples for each node and incorporate the negative information contained in these samples into the representation updates. Moreover, we show that the process of selecting the negative samples is not trivial. Our theme therefore begins by describing the criteria for a good negative sample, followed by a determinantal point process algorithm for efficiently obtaining such samples. A GCN, boosted by diverse negative samples, then jointly considers the positive and negative information when passing messages. Experimental evaluations show that this idea not only improves the overall performance of standard representation learning but also significantly alleviates over-smoothing problems.",
    "code_link": "https://github.com/Wei9711/D2GCN"
  },
  "aaai2022_main_adaptiveanduniversalalgorithmsforvariationalinequalitieswithoptimalconvergence": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive and Universal Algorithms for Variational Inequalities with Optimal Convergence",
    "authors": [
      "Alina Ene",
      "Huy L\u00ea Nguy\u1ec5n"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20609",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20609/20368",
    "published": "2022-02",
    "summary": "We develop new adaptive algorithms for variational inequalities with monotone operators, which capture many problems of interest, notably convex optimization and convex-concave saddle point problems. Our algorithms automatically adapt to unknown problem parameters such as the smoothness and the norm of the operator, and the variance of the stochastic evaluation oracle. We show that our algorithms are universal and simultaneously achieve the optimal convergence rates in the non-smooth, smooth, and stochastic settings. The convergence guarantees of our algorithms improve over existing adaptive methods and match the optimal non-adaptive algorithms. Additionally, prior works require that the optimization domain is bounded. In this work, we remove this restriction and give algorithms for unbounded domains that are adaptive and universal. Our general proof techniques can be used for many variants of the algorithm using one or two operator evaluations per iteration. The classical methods based on the ExtraGradient/MirrorProx algorithm require two operator evaluations per iteration, which is the dominant factor in the running time in many settings.",
    "code_link": ""
  },
  "aaai2022_main_zero-shotout-of-distributiondetectionbasedonthepre-trainedmodelclip": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP",
    "authors": [
      "Sepideh Esmaeilpour",
      "Bing Liu",
      "Eric Robertson",
      "Lei Shu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20610",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20610/20369",
    "published": "2022-02",
    "summary": "In an out-of-distribution (OOD) detection problem, samples of known classes (also called in-distribution classes) are used to train a special classifier. In testing, the classifier can (1) classify the test samples of known classes to their respective classes and also (2) detect samples that do not belong to any of the known classes (i.e., they belong to some unknown or OOD classes). This paper studies the problem of zero-shot out-of-distribution (OOD) detection, which still performs the same two tasks in testing but has no training except using the given known class names. This paper proposes a novel and yet simple method (called ZOC) to solve the problem. ZOC builds on top of the recent advances in zero-shot classification through multi-modal representation learning. It first extends the pre-trained language-vision model CLIP by training a text-based image description generator on top of CLIP. In testing, it uses the extended model to generate candidate unknown class names for each test sample and computes a confidence score based on both the known class names and candidate unknown class names for zero-shot OOD detection. Experimental results on 5 benchmark datasets for OOD detection demonstrate that ZOC outperforms the baselines by a large margin.",
    "code_link": ""
  },
  "aaai2022_main_gradientflowinsparseneuralnetworksandhowlotteryticketswin": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
    "authors": [
      "Utku Evci",
      "Yani Ioannou",
      "Cem Keskin",
      "Yann Dauphin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20611",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20611/20370",
    "published": "2022-02",
    "summary": "Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from \u2014 however, this comes at the cost of learning novel solutions.",
    "code_link": ""
  },
  "aaai2022_main_dynamicnonlinearmatrixcompletionfortime-varyingdataimputation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dynamic Nonlinear Matrix Completion for Time-Varying Data Imputation",
    "authors": [
      "Jicong Fan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20612",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20612/20371",
    "published": "2022-02",
    "summary": "Classical matrix completion methods focus on data with stationary latent structure and hence are not effective in missing value imputation when the latent structure changes with time.This paper proposes a dynamic nonlinear matrix completion (D-NLMC) method, which is able to recover the missing values of streaming data when the low-dimensional nonlinear latent structure of the data changes with time.The paper provides an efficient approach to updating the nonlinear model dynamically.D-NLMC incorporates the information of new data and remove the information of earlier data recursively.The paper shows that the missing data can be estimated if the change of latent structure is slow enough.Different from existing online or adaptive low-rank matrix completion methods,D-NLMC does not require the local low-rank assumption and is able to adaptively recover high-rank matrices with low-dimensional latent structures. Note that existing high-rank matrix completion methods have high-computational costs and are not applicable to streaming data with varying latent structures, which fortunately can be handled by D-NLMC efficiently and accurately.Numerical results show that D-NLMC outperforms the baselines in real applications.",
    "code_link": ""
  },
  "aaai2022_main_upto100xfasterdata-freeknowledgedistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Up to 100x Faster Data-Free Knowledge Distillation",
    "authors": [
      "Gongfan Fang",
      "Kanya Mo",
      "Xinchao Wang",
      "Jie Song",
      "Shitao Bei",
      "Haofei Zhang",
      "Mingli Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20613",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20613/20372",
    "published": "2022-02",
    "summary": "Data-free knowledge distillation (DFKD) has recently been attracting increasing attention from research communities, attributed to its capability to compress a model only using synthetic data. Despite the encouraging results achieved, state-of-the-art DFKD methods still suffer from the inefficiency of data synthesis, making the data-free training process extremely time-consuming and thus inapplicable for large-scale tasks. In this work, we introduce an efficacious scheme, termed as FastDFKD, that allows us to accelerate DFKD by a factor of orders of magnitude. At the heart of our approach is a novel strategy to reuse the shared common features in training data so as to synthesize different data instances. Unlike prior methods that optimize a set of data independently, we propose to learn a meta-synthesizer that seeks common features as the initialization for the fast data synthesis. As a result, FastDFKD achieves data synthesis within only a few steps,significantly enhancing the efficiency of data-free training. Experiments over CIFAR, NYUv2, and ImageNet demonstrate that the proposed FastDFKD achieves 10x and even 100x acceleration while preservingperformances on par with state of the art. Code is available at https://github.com/zju-vipa/Fast-Datafree.",
    "code_link": "https://github.com/zju-vipa/Fast-Datafree"
  },
  "aaai2022_main_learningalignedcross-modalrepresentationforgeneralizedzero-shotclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification",
    "authors": [
      "Zhiyu Fang",
      "Xiaobin Zhu",
      "Chun Yang",
      "Zheng Han",
      "Jingyan Qin",
      "Xu-Cheng Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20614",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20614/20373",
    "published": "2022-02",
    "summary": "Learning a common latent embedding by aligning the latent spaces of cross-modal autoencoders is an effective strategy for Generalized Zero-Shot Classification (GZSC). However, due to the lack of fine-grained instance-wise annotations, it still easily suffer from the domain shift problem for the discrepancy between the visual representation of diversified images and the semantic representation of fixed attributes. In this paper, we propose an innovative autoencoder network by learning Aligned Cross-Modal Representations (dubbed ACMR) for GZSC. Specifically, we propose a novel Vision-Semantic Alignment (VSA) method to strengthen the alignment of cross-modal latent features on the latent subspaces guided by a learned classifier. In addition, we propose a novel Information Enhancement Module (IEM) to reduce the possibility of latent variables collapse meanwhile encouraging the discriminative ability of latent variables. Extensive experiments on publicly available datasets demonstrate the state-of-the-art performance of our method.",
    "code_link": ""
  },
  "aaai2022_main_kergnnsinterpretablegraphneuralnetworkswithgraphkernels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels",
    "authors": [
      "Aosong Feng",
      "Chenyu You",
      "Shiqiang Wang",
      "Leandros Tassiulas"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20615",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20615/20374",
    "published": "2022-02",
    "summary": "Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.",
    "code_link": ""
  },
  "aaai2022_main_scalingneuralprogramsynthesiswithdistribution-basedsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Scaling Neural Program Synthesis with Distribution-Based Search",
    "authors": [
      "Nathana\u00ebl Fijalkow",
      "Guillaume Lagarde",
      "Th\u00e9o Matricon",
      "Kevin Ellis",
      "Pierre\n      Ohlmann",
      "Akarsh Nayan Potta"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20616",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20616/20375",
    "published": "2022-02",
    "summary": "We consider the problem of automatically constructing computer programs from input-output examples. We investigate how to augment probabilistic and neural program synthesis methods with new search algorithms, proposing a framework called distribution-based search. Within this framework, we introduce two new search algorithms: Heap Search, an enumerative method, and SQRT Sampling, a probabilistic method. We prove certain optimality guarantees for both methods, show how they integrate with probabilistic and neural techniques, and demonstrate how they can operate at scale across parallel compute environments. Collectively these findings offer theoretical and applied studies of search algorithms for program synthesis that integrate with recent developments in machine-learned program synthesizers.",
    "code_link": ""
  },
  "aaai2022_main_modification-fairclusterediting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Modification-Fair Cluster Editing",
    "authors": [
      "Vincent Froese",
      "Leon Kellerhals",
      "Rolf Niedermeier"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20617",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20617/20376",
    "published": "2022-02",
    "summary": "The classic Cluster Editing problem (also known as Correlation Clustering) asks to transform a given graph into a disjoint union of cliques (clusters) by a small number of edge modifications. When applied to vertex-colored graphs (the colors representing subgroups), standard algorithms for the NP-hard Cluster Editing problem may yieldsolutions that are biased towards subgroups of data (e.g., demographic groups), measured in the number of modifications incident to the members of the subgroups. We propose a modification fairness constraint which ensures that the number of edits incident to each subgroup is proportional to its size. To start with, we study Modification-Fair Cluster Editing for graphs with two vertex colors. We show that the problem is NP-hard even if one may only insert edges within a subgroup; note that in the classic \"non-fair\" setting, this case is trivially polynomial-time solvable. However, in the more general editing form, the modification-fair variant remains fixed-parameter tractable with respect to the number of edge edits. We complement these and further theoretical results with an empirical analysis of our model on real-world social networks where we find that the price of modification-fairness is surprisingly low, that is, the cost of optimal modification-fair differs from the cost of optimal \"non-fair\" solutions only by a small percentage.",
    "code_link": ""
  },
  "aaai2022_main_reinforcementlearningbaseddynamicmodelcombinationfortimeseriesforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reinforcement Learning Based Dynamic Model Combination for Time Series Forecasting",
    "authors": [
      "Yuwei Fu",
      "Di Wu",
      "Benoit Boulet"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20618",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20618/20377",
    "published": "2022-02",
    "summary": "Time series data appears in many real-world fields such as energy, transportation, communication systems. Accurate modelling and forecasting of time series data can be of significant importance to improve the efficiency of these systems. Extensive research efforts have been taken for time series problems. Different types of approaches, including both statistical-based methods and machine learning-based methods, have been investigated. Among these methods, ensemble learning has shown to be effective and robust. However, it is still an open question that how we should determine weights for base models in the ensemble. Sub-optimal weights may prevent the final model from reaching its full potential. To deal with this challenge, we propose a reinforcement learning (RL) based model combination (RLMC) framework for determining model weights in an ensemble for time series forecasting tasks. By formulating model selection as a sequential decision-making problem, RLMC learns a deterministic policy to output dynamic model weights for non-stationary time series data. RLMC further leverages deep learning to learn hidden features from raw time series data to adapt fast to the changing data distribution. Extensive experiments on multiple real-world datasets have been implemented to showcase the effectiveness of the proposed method.",
    "code_link": "https://github.com/TSRLMC/RLMC"
  },
  "aaai2022_main_jfbjacobian-freebackpropagationforimplicitnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "JFB: Jacobian-Free Backpropagation for Implicit Networks",
    "authors": [
      "Samy Wu Fung",
      "Howard Heaton",
      "Qiuwei Li",
      "Daniel Mckenzie",
      "Stanley Osher",
      "Wotao Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20619",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20619/20378",
    "published": "2022-02",
    "summary": "A promising trend in deep learning replaces traditional feedforward networks with implicit networks. Unlike traditional networks, implicit networks solve a fixed point equation to compute inferences. Solving for the fixed point varies in complexity, depending on provided data and an error tolerance. Importantly, implicit networks may be trained with fixed memory costs in stark contrast to feedforward networks, whose memory requirements scale linearly with depth. However, there is no freelunch --- backpropagation through implicit networks often requires solving a costly Jacobian-based equation arising from the implicit function theorem. We propose Jacobian-Free Backpropagation (JFB), a fixed-memory approach that circumvents the need to solve Jacobian-based equations. JFB makes implicit networks faster to train and significantly easier to implement, without sacrificing test accuracy. Our experiments show implicit networks trained with JFB are competitive with feedforward networks and prior implicit networks given the same number of parameters.",
    "code_link": "https://github.com/howardheaton/jacobian"
  },
  "aaai2022_main_smoothingadvantagelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Smoothing Advantage Learning",
    "authors": [
      "Yaozhong Gan",
      "Zhe Zhang",
      "Xiaoyang Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20620",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20620/20379",
    "published": "2022-02",
    "summary": "Advantage learning (AL) aims to improve the robustness of value-based reinforcement learning against estimation errors with action-gap-based regularization. Unfortunately, the method tends to be unstable in the case of function approximation. In this paper, we propose a simple variant of AL, named smoothing advantage learning (SAL), to alleviate this problem. The key to our method is to replace the original Bellman Optimal operator in AL with a smooth one so as to obtain more reliable estimation of the temporal difference target. We give a detailed account of the resulting action gap and the performance bound for approximate SAL. Further theoretical analysis reveals that the proposed value smoothing technique not only helps to stabilize the training procedure of AL by controlling the trade-off between convergence rate and the upper bound of the approximation errors, but is beneficial to increase the action gap between the optimal and sub-optimal action value as well.",
    "code_link": ""
  },
  "aaai2022_main_enhancingcounterfactualclassificationperformanceviaself-training": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Enhancing Counterfactual Classification Performance via Self-Training",
    "authors": [
      "Ruijiang Gao",
      "Max Biggs",
      "Wei Sun",
      "Ligong Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20621",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20621/20380",
    "published": "2022-02",
    "summary": "Unlike traditional supervised learning, in many settings only partial feedback is available. We may only observe outcomes for the chosen actions, but not the counterfactual outcomes associated with other alternatives. Such settings encompass a wide variety of applications including pricing, online marketing and precision medicine. A key challenge is that observational data are influenced by historical policies deployed in the system, yielding a biased data distribution. We approach this task as a domain adaptation problem and propose a self-training algorithm which imputes outcomes with categorical values for finite unseen actions in the observational data to simulate a randomized trial through pseudolabelling, which we refer to as Counterfactual Self-Training (CST). CST iteratively imputes pseudolabels and retrains the model. In addition, we show input consistency loss can further improve CST performance which is shown in recent theoretical analysis of pseudolabelling. We demonstrate the effectiveness of the proposed algorithms on both synthetic and real datasets.",
    "code_link": "https://github.com/ruijiang81/CST"
  },
  "aaai2022_main_learningv1simplecellswithvectorrepresentationoflocalcontentandmatrixrepresentationoflocalmotion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion",
    "authors": [
      "Ruiqi Gao",
      "Jianwen Xie",
      "Siyuan Huang",
      "Yufan Ren",
      "Song-Chun Zhu",
      "Ying Nian\n      Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20622",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20622/20381",
    "published": "2022-02",
    "summary": "This paper proposes a representational model for image pairs such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1). The model couples the following two components: (1) the vector representations of local contents of images and (2) the matrix representations of local pixel displacements caused by the relative motions between the agent and the objects in the 3D scene. When the image frame undergoes changes due to local pixel displacements, the vectors are multiplied by the matrices that represent the local displacements. Thus the vector representation is equivariant as it varies according to the local displacements. Our experiments show that our model can learn Gabor-like filter pairs of quadrature phases. The profiles of the learned filters match those of simple cells in Macaque V1. Moreover, we demonstrate that the model can learn to infer local motions in either a supervised or unsupervised manner. With such a simple model, we achieve competitive results on optical flow estimation.",
    "code_link": ""
  },
  "aaai2022_main_algorithmicconcept-basedexplainablereasoning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Algorithmic Concept-Based Explainable Reasoning",
    "authors": [
      "Dobrik Georgiev",
      "Pietro Barbiero",
      "Dmitry Kazhdan",
      "Petar Veli\u010dkovi\u0107",
      "Pietro\n      Li\u00f3"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20623",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20623/20382",
    "published": "2022-02",
    "summary": "Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can't be generated. Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept-bottleneck GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts.",
    "code_link": "https://github.com/HekpoMaH/algorithmicconcepts-reasoning"
  },
  "aaai2022_main_recoveringthepropensityscorefrombiasedpositiveunlabeleddata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Recovering the Propensity Score from Biased Positive Unlabeled Data",
    "authors": [
      "Walter Gerych",
      "Thomas Hartvigsen",
      "Luke Buquicchio",
      "Emmanuel Agu",
      "Elke\n      Rundensteiner"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20624",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20624/20383",
    "published": "2022-02",
    "summary": "Positive-Unlabeled (PU) learning methods train a classifier to distinguish between the positive and negative classes given only positive and unlabeled data. While traditional PU methods require the labeled positive samples to be an unbiased sample of the positive distribution, in practice the labeled sample is often a biased draw from the true distribution. Prior work shows that if we know the likelihood that each positive instance will be selected for labeling, referred to as the propensity score, then the biased sample can be used for PU learning. Unfortunately, no prior work has been proposed an inference strategy for which the propensity score is identifiable. In this work, we propose two sets of assumptions under which the propensity score can be uniquely determined: one in which no assumption is made on the functional form of the propensity score (requiring assumptions on the data distribution), and the second which loosens the data assumptions while assuming a functional form for the propensity score. We then propose inference strategies for each case. Our empirical study shows that our approach significantly outperforms the state-of-the-art propensity estimation methods on a rich variety of benchmark datasets.",
    "code_link": ""
  },
  "aaai2022_main_dipsdifferentiablepolicyforsketchinginrecommendersystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DiPS: Differentiable Policy for Sketching in Recommender Systems",
    "authors": [
      "Aritra Ghosh",
      "Saayan Mitra",
      "Andrew Lan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20625",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20625/20384",
    "published": "2022-02",
    "summary": "In sequential recommender system applications, it is important to develop models that can capture users' evolving interest over time to successfully recommend future items that they are likely to interact with. For users with long histories, typical models based on recurrent neural networks tend to forget important items in the distant past. Recent works have shown that storing a small sketch of past items can improve sequential recommendation tasks. However, these works all rely on static sketching policies, i.e., heuristics to select items to keep in the sketch, which are not necessarily optimal and cannot improve over time with more training data. In this paper, we propose a differentiable policy for sketching (DiPS), a framework that learns a data-driven sketching policy in an end-to-end manner together with the recommender system model to explicitly maximize recommendation quality in the future. We also propose an approximate estimator of the gradient for optimizing the sketching algorithm parameters that is computationally efficient. We verify the effectiveness of DiPS on real-world datasets under various practical settings and show that it requires up to 50% fewer sketch items to reach the same predictive quality than existing sketching policies.",
    "code_link": "https://github.com/arghosh/DiPS"
  },
  "aaai2022_main_learninglargedagsbycombiningcontinuousoptimizationandfeedbackarcsetheuristics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Large DAGs by Combining Continuous Optimization and Feedback Arc Set Heuristics",
    "authors": [
      "Pierre Gillot",
      "Pekka Parviainen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20626",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20626/20385",
    "published": "2022-02",
    "summary": "Bayesian networks represent relations between variables using a directed acyclic graph (DAG). Learning the DAG is an NP-hard problem and exact learning algorithms are feasible only for small sets of variables. We propose two scalable heuristics for learning DAGs in the linear structural equation case. Our methods learn the DAG by alternating between unconstrained gradient descent-based step to optimize an objective function and solving a maximum acyclic subgraph problem to enforce acyclicity. Thanks to this decoupling, our methods scale up beyond thousands of variables.",
    "code_link": ""
  },
  "aaai2022_main_regularizedmodalregressiononmarkov-dependentobservationsatheoreticalassessment": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Regularized Modal Regression on Markov-Dependent Observations: A Theoretical Assessment",
    "authors": [
      "Tieliang Gong",
      "Yuxin Dong",
      "Hong Chen",
      "Wei Feng",
      "Bo Dong",
      "Chen Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20627",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20627/20386",
    "published": "2022-02",
    "summary": "Modal regression, a widely used regression protocol, has been extensively investigated in statistical and machine learning communities due to its robustness to outlier and heavy-tailed noises. Understanding modal regression's theoretical behavior can be fundamental in learning theory. Despite significant progress in characterizing its statistical property, the majority results are based on the assumption that samples are independent and identical distributed (i.i.d.), which is too restrictive for real-world applications. This paper concerns about the statistical property of regularized modal regression (RMR) within an important dependence structure - Markov dependent. Specifically, we establish the upper bound for RMR estimator under moderate conditions and give an explicit learning rate. Our results show that the Markov dependence impacts on the generalization error in the way that sample size would be discounted by a multiplicative factor depending on the spectral gap of the underlying Markov chain. This result shed a new light on characterizing the theoretical underpinning for robust regression.",
    "code_link": ""
  },
  "aaai2022_main_partialmulti-labellearningvialargemarginnearestneighbourembeddings": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Partial Multi-Label Learning via Large Margin Nearest Neighbour Embeddings",
    "authors": [
      "Xiuwen Gong",
      "Dong Yuan",
      "Wei Bao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20628",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20628/20387",
    "published": "2022-02",
    "summary": "To deal with ambiguities in partial multi-label learning (PML), existing popular PML research attempts to perform disambiguation by direct ground-truth label identification. However, these approaches can be easily misled by noisy false-positive labels in the iteration of updating the model parameter and the latent ground-truth label variables. When labeling information is ambiguous, we should depend more on underlying structure of data, such as label and feature correlations, to perform disambiguation for partially labeled data. Moreover, large margin nearest neighbour (LMNN) is a popular strategy that considers data structure in classification. However, due to the ambiguity of labeling information in PML, traditional LMNN cannot be used to solve the PML problem directly. In addition, embedding is an effective technology to decrease the noise information of data. Inspried by LMNN and embedding technology, we propose a novel PML paradigm called Partial Multi-label Learning via Large Margin Nearest Neighbour Embeddings (PML-LMNNE), which aims to conduct disambiguation by projecting labels and features into a lower-dimension embedding space and reorganize the underlying structure by LMNN in the embedding space simultaneously. An efficient algorithm is designed to implement the proposed method and the convergence rate of the algorithm is analyzed. Moreover, we present a theoretical analysis of the generalization error bound for the proposed PML-LMNNE, which shows that the generalization error converges to the sum of two times the Bayes error over the labels when the number of instances goes to infinity. Comprehensive experiments on artificial and real-world datasets demonstrate the superiorities of the proposed PML-LMNNE.",
    "code_link": ""
  },
  "aaai2022_main_lunarunifyinglocaloutlierdetectionmethodsviagraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks",
    "authors": [
      "Adam Goodge",
      "Bryan Hooi",
      "See-Kiong Ng",
      "Wee Siong Ng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20629",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20629/20388",
    "published": "2022-02",
    "summary": "Many well-established anomaly detection methods use the distance of a sample to those in its local neighbourhood: so-called `local outlier methods', such as LOF and DBSCAN. They are popular for their simple principles and strong performance on unstructured, feature-based data that is commonplace in many practical applications. However, they cannot learn to adapt for a particular set of data due to their lack of trainable parameters. In this paper, we begin by unifying local outlier methods by showing that they are particular cases of the more general message passing framework used in graph neural networks. This allows us to introduce learnability into local outlier methods, in the form of a neural network, for greater flexibility and expressivity: specifically, we propose LUNAR, a novel, graph neural network-based anomaly detection method. LUNAR learns to use information from the nearest neighbours of each node in a trainable way to find anomalies. We show that our method performs significantly better than existing local outlier methods, as well as state-of-the-art deep baselines. We also show that the performance of our method is much more robust to different settings of the local neighbourhood size.",
    "code_link": "https://github.com/agoodge/LUNAR"
  },
  "aaai2022_main_semi-supervisedconditionaldensityestimationwithwassersteinlaplacianregularisation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semi-supervised Conditional Density Estimation with Wasserstein Laplacian Regularisation",
    "authors": [
      "Olivier Graffeuille",
      "Yun Sing Koh",
      "J\u00f6rg Wicker",
      "Moritz K Lehmann"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20630",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20630/20389",
    "published": "2022-02",
    "summary": "Conditional Density Estimation (CDE) has wide-reaching applicability to various real-world problems, such as spatial density estimation and environmental modelling. CDE estimates the probability density of a random variable rather than a single value and can thus model uncertainty and inverse problems. This task is inherently more complex than regression, and many algorithms suffer from overfitting, particularly when modelled with few labelled data points. For applications where unlabelled data is abundant but labelled data is scarce, we propose Wasserstein Laplacian Regularisation, a semi-supervised learning framework that allows CDE algorithms to leverage these unlabelled data. The framework minimises an objective function which ensures that the learned model is smooth along the manifold of the underlying data, as measured by Wasserstein distance. When applying our framework to Mixture Density Networks, the resulting semi-supervised algorithm can achieve similar performance to a supervised model with up to three times as many labelled data points on baseline datasets. We additionally apply our technique to the problem of remote sensing for chlorophyll-a estimation in inland waters.",
    "code_link": "https://github.com/OGraffeuille/Wasserstein-LaplacianRegularisation"
  },
  "aaai2022_main_gotubescalablestatisticalverificationofcontinuous-depthmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GoTube: Scalable Statistical Verification of Continuous-Depth Models",
    "authors": [
      "Sophie A. Gruenbacher",
      "Mathias Lechner",
      "Ramin Hasani",
      "Daniela Rus",
      "Thomas\n      A. Henzinger",
      "Scott A. Smolka",
      "Radu Grosu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20631",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20631/20390",
    "published": "2022-02",
    "summary": "We introduce a new statistical verification algorithm that formally quantifies the behavioral robustness of any time-continuous process formulated as a continuous-depth model. Our algorithm solves a set of global optimization (Go) problems over a given time horizon to construct a tight enclosure (Tube) of the set of all process executions starting from a ball of initial states. We call our algorithm GoTube. Through its construction, GoTube ensures that the bounding tube is conservative up to a desired probability and up to a desired tightness.GoTube is implemented in JAX and optimized to scale to complex continuous-depth neural network models. Compared to advanced reachability analysis tools for time-continuous neural networks, GoTube does not accumulate overapproximation errors between time steps and avoids the infamous wrapping effect inherent in symbolic techniques. We show that GoTube substantially outperforms state-of-the-art verification tools in terms of the size of the initial ball, speed, time-horizon, task completion, and scalability on a large set of experiments.GoTube is stable and sets the state-of-the-art in terms of its ability to scale to time horizons well beyond what has been previously possible.",
    "code_link": "https://github.com/DatenVorsprung/GoTube"
  },
  "aaai2022_main_balancedself-pacedlearningforaucmaximization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Balanced Self-Paced Learning for AUC Maximization",
    "authors": [
      "Bin Gu",
      "Chenkang Zhang",
      "Huan Xiong",
      "Heng Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20632",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20632/20391",
    "published": "2022-02",
    "summary": "Learning to improve AUC performance is an important topic in machine learning. However, AUC maximization algorithms may decrease generalization performance due to the noisy data. Self-paced learning is an effective method for handling noisy data. However, existing self-paced learning methods are limited to pointwise learning, while AUC maximization is a pairwise learning problem. To solve this challenging problem, we innovatively propose a balanced self-paced AUC maximization algorithm (BSPAUC). Specifically, we first provide a statistical objective for self-paced AUC.Based on this, we propose our self-paced AUC maximization formulation, where a novel balanced self-paced regularization term is embedded to ensure that the selected positive and negative samples have proper proportions. Specially, the sub-problem with respect to all weight variables may be non-convex in our formulation, while the one is normally convex in existing self-paced problems. To address this, we propose a doubly cyclic block coordinate descent method.More importantly, we prove that the sub-problem with respect to all weight variables converges to a stationary point on the basis of closed-form solutions, and our BSPAUC converges to a stationary point of our fixed optimization objective under a mild assumption. Considering both the deep learning and kernel-based implementations, experimental results on several large-scale datasets demonstrate that our BSPAUC has a better generalization performance than existing state-of-the-art AUC maximization methods.",
    "code_link": ""
  },
  "aaai2022_main_theoreticalguaranteesoffictitiousdiscountalgorithmsforepisodicreinforcementlearningandglobalconvergenceofpolicygradientmethods": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Theoretical Guarantees of Fictitious Discount Algorithms for Episodic Reinforcement Learning and Global Convergence of Policy Gradient Methods",
    "authors": [
      "Xin Guo",
      "Anran Hu",
      "Junzi Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20633",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20633/20392",
    "published": "2022-02",
    "summary": "When designing algorithms for finite-time-horizon episodic reinforcement learning problems, a common approach is to introduce a fictitious discount factor and use stationary policies for approximations. Empirically, it has been shown that the fictitious discount factor helps reduce variance, and stationary policies serve to save the per-iteration computational cost. Theoretically, however, there is no existing work on convergence analysis for algorithms with this fictitious discount recipe. This paper takes the first step towards analyzing these algorithms. It focuses on two vanilla policy gradient (VPG) variants: the first being a widely used variant with discounted advantage estimations (DAE), the second with an additional fictitious discount factor in the score functions of the policy gradient estimators. Non-asymptotic convergence guarantees are established for both algorithms, and the additional discount factor is shown to reduce the bias introduced in DAE and thus improve the algorithm convergence asymptotically. A key ingredient of our analysis is to connect three settings of Markov decision processes (MDPs): the finite-time-horizon, the average reward and the discounted settings. To our best knowledge, this is the first theoretical guarantee on fictitious discount algorithms for the episodic reinforcement learning of finite-time-horizon MDPs, which also leads to the (first) global convergence of policy gradient methods for finite-time-horizon episodic reinforcement learning.",
    "code_link": ""
  },
  "aaai2022_main_adaptiveorthogonalprojectionforbatchandonlinecontinuallearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Orthogonal Projection for Batch and Online Continual Learning",
    "authors": [
      "Yiduo Guo",
      "Wenpeng Hu",
      "Dongyan Zhao",
      "Bing Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20634",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20634/20393",
    "published": "2022-02",
    "summary": "Catastrophic forgetting is a key obstacle to continual learning. One of the state-of-the-art approaches is orthogonal projection. The idea of this approach is to learn each task by updating the network parameters or weights only in the direction orthogonal to the subspace spanned by all previous task inputs. This ensures no interference with tasks that have been learned. The system OWM that uses the idea performs very well against other state-of-the-art systems. In this paper, we first discuss an issue that we discovered in the mathematical derivation of this approach and then propose a novel method, called AOP (Adaptive Orthogonal Projection), to resolve it, which results in significant accuracy gains in empirical evaluations in both the batch and online continual learning settings without saving any previous training data as in replay-based methods.",
    "code_link": ""
  },
  "aaai2022_main_learningactiontranslatorformetareinforcementlearningonsparse-rewardtasks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
    "authors": [
      "Yijie Guo",
      "Qiucheng Wu",
      "Honglak Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20635",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20635/20394",
    "published": "2022-02",
    "summary": "Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of training tasks simultaneously and quickly adapting to new tasks. It requires massive amounts of data drawn from training tasks to infer the common structure shared among tasks. Without heavy reward engineering, the sparse rewards in long-horizon tasks exacerbate the problem of sample efficiency in meta-RL. Another challenge in meta-RL is the discrepancy of difficulty level among tasks, which might cause one easy task dominating learning of the shared policy and thus preclude policy adaptation to new tasks. This work introduces a novel objective function to learn an action translator among training tasks. We theoretically verify that the value of the transferred policy with the action translator can be close to the value of the source policy and our objective function (approximately) upper bounds the value difference. We propose to combine the action translator with context-based meta-RL algorithms for better data collection and moreefficient exploration during meta-training. Our approach em-pirically improves the sample efficiency and performance ofmeta-RL algorithms on sparse-reward tasks.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedpre-trainingforproteinembeddingsusingtertiarystructures": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures",
    "authors": [
      "Yuzhi Guo",
      "Jiaxiang Wu",
      "Hehuan Ma",
      "Junzhou Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20636",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20636/20395",
    "published": "2022-02",
    "summary": "The protein tertiary structure largely determines its interaction with other molecules. Despite its importance in various structure-related tasks, fully-supervised data are often time-consuming and costly to obtain. Existing pre-training models mostly focus on amino-acid sequences or multiple sequence alignments, while the structural information is not yet exploited. In this paper, we propose a self-supervised pre-training model for learning structure embeddings from protein tertiary structures. Native protein structures are perturbed with random noise, and the pre-training model aims at estimating gradients over perturbed 3D structures. Specifically, we adopt SE(3)-invariant features as model inputs and reconstruct gradients over 3D coordinates with SE(3)-equivariance preserved. Such paradigm avoids the usage of sophisticated SE(3)-equivariant models, and dramatically improves the computational efficiency of pre-training models. We demonstrate the effectiveness of our pre-training model on two downstream tasks, protein structure quality assessment (QA) and protein-protein interaction (PPI) site prediction. Hierarchical structure embeddings are extracted to enhance corresponding prediction models. Extensive experiments indicate that such structure embeddings consistently improve the prediction accuracy for both downstream tasks.",
    "code_link": ""
  },
  "aaai2022_main_improvedgradient-basedadversarialattacksforquantizednetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improved Gradient-Based Adversarial Attacks for Quantized Networks",
    "authors": [
      "Kartik Gupta",
      "Thalaiyasingam Ajanthan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20637",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20637/20396",
    "published": "2022-02",
    "summary": "Neural network quantization has become increasingly popular due to efficient memory consumption and faster computation resulting from bitwise operations on the quantized networks. Even though they exhibit excellent generalization capabilities, their robustness properties are not well-understood. In this work, we systematically study the robustness of quantized networks against gradient based adversarial attacks and demonstrate that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. By attributing gradient vanishing to poor forward-backward signal propagation in the trained network, we introduce a simple temperature scaling approach to mitigate this issue while preserving the decision boundary. Despite being a simple modification to existing gradient based adversarial attacks, experiments on multiple image classification datasets with multiple network architectures demonstrate that our temperature scaled attacks obtain near-perfect success rate on quantized networks while outperforming original attacks on adversarially trained models as well as floating-point networks.",
    "code_link": ""
  },
  "aaai2022_main_tiggerscalablegenerativemodellingfortemporalinteractiongraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TIGGER: Scalable Generative Modelling for Temporal Interaction Graphs",
    "authors": [
      "Shubham Gupta",
      "Sahil Manchanda",
      "Srikanta Bedathur",
      "Sayan Ranu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20638",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20638/20397",
    "published": "2022-02",
    "summary": "There has been a recent surge in learning generative models for graphs. While impressive progress has been made on static graphs, work on generative modeling of temporal graphs is at a nascent stage with significant scope for improvement. First, existing generative models do not scale with either the time horizon or the number of nodes. Second, existing techniques are transductive in nature and thus do not facilitate knowledge transfer. Finally, due to relying on one-to-one node mapping from source to the generated graph, existing models leak node identity information and do not allow up-scaling/down-scaling the source graph size. In this paper, we bridge these gaps with a novel generative model called TIGGER. TIGGER derives its power through a combination of temporal point processes with auto-regressive modeling enabling both transductive and inductive variants. Through extensive experiments on real datasets, we establish TIGGER generates graphs of superior fidelity, while also being up to 3 orders of magnitude faster than the state-of-the-art.",
    "code_link": ""
  },
  "aaai2022_main_ageneralizedbootstraptargetforvalue-learning,efficientlycombiningvalueandfeaturepredictions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Generalized Bootstrap Target for Value-Learning, Efficiently Combining Value and Feature Predictions",
    "authors": [
      "Anthony GX-Chen",
      "Veronica Chelu",
      "Blake A. Richards",
      "Joelle Pineau"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20639",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20639/20398",
    "published": "2022-02",
    "summary": "Estimating value functions is a core component of reinforcement learning algorithms. Temporal difference (TD) learning algorithms use bootstrapping, i.e. they update the value function toward a learning target using value estimates at subsequent time-steps. Alternatively, the value function can be updated toward a learning target constructed by separately predicting successor features (SF)\u2014a policy-dependent model\u2014and linearly combining them with instantaneous rewards.We focus on bootstrapping targets used when estimating value functions, and propose a new backup target, the ?-return mixture, which implicitly combines value-predictive knowledge (used by TD methods) with (successor) feature-predictive knowledge\u2014with a parameter ? capturing how much to rely on each. We illustrate that incorporating predictive knowledge through an ??-discounted SF model makes more efficient use of sampled experience, compared to either extreme, i.e. bootstrapping entirely on the value function estimate, or bootstrapping on the product of separately estimated successor features and instantaneous reward models. We empirically show this approach leads to faster policy evaluation and better control performance, for tabular and nonlinear function approximations, indicating scalability and generality.",
    "code_link": ""
  },
  "aaai2022_main_oscillatoryfourierneuralnetworkacompactandefficientarchitectureforsequentialprocessing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Oscillatory Fourier Neural Network: A Compact and Efficient Architecture for Sequential Processing",
    "authors": [
      "Bing Han",
      "Cheng Wang",
      "Kaushik Roy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20640",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20640/20399",
    "published": "2022-02",
    "summary": "Tremendous progress has been made in sequential processing with the recent advances in recurrent neural networks. However, recurrent architectures face the challenge of exploding/vanishing gradients during training, and require significant computational resources to execute back-propagation through time. Moreover, large models are typically needed for executing complex sequential tasks. To address these challenges, we propose a novel neuron model that has cosine activation with a time varying component for sequential processing. The proposed neuron provides an efficient building block for projecting sequential inputs into spectral domain, which helps to retain long-term dependencies with minimal extra model parameters and computation. A new type of recurrent network architecture, named Oscillatory Fourier Neural Network, based on the proposed neuron is presented and applied to various types of sequential tasks. We demonstrate that recurrent neural network with the proposed neuron model is mathematically equivalent to a simplified form of discrete Fourier transform applied onto periodical activation. In particular, the computationally intensive back-propagation through time in training is eliminated, leading to faster training while achieving the state of the art inference accuracy in a diverse group of sequential tasks. For instance, applying the proposed model to sentiment analysis on IMDB review dataset reaches 89.4% test accuracy within 5 epochs, accompanied by over 35x reduction in the model size compared to LSTM. The proposed novel RNN architecture is well poised for intelligent sequential processing in resource constrained hardware.",
    "code_link": ""
  },
  "aaai2022_main_end-to-endprobabilisticlabel-specificfeaturelearningformulti-labelclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "End-to-End Probabilistic Label-Specific Feature Learning for Multi-Label Classification",
    "authors": [
      "Jun-Yi Hang",
      "Min-Ling Zhang",
      "Yanghe Feng",
      "Xiaocheng Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20641",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20641/20400",
    "published": "2022-02",
    "summary": "Label-specific features serve as an effective strategy to learn from multi-label data with tailored features accounting for the distinct discriminative properties of each class label. Existing prototype-based label-specific feature transformation approaches work in a three-stage framework, where prototype acquisition, label-specific feature generation and classification model induction are performed independently. Intuitively, this separate framework is suboptimal due to its decoupling nature. In this paper, we make a first attempt towards a unified framework for prototype-based label-specific feature transformation, where the prototypes and the label-specific features are directly optimized for classification. To instantiate it, we propose modelling the prototypes probabilistically by the normalizing flows, which possess adaptive prototypical complexity to fully capture the underlying properties of each class label and allow for scalable stochastic optimization. Then, a label correlation regularized probabilistic latent metric space is constructed via jointly learning the prototypes and the metric-based label-specific features for classification. Comprehensive experiments on 14 benchmark data sets show that our approach outperforms the state-of-the-art counterparts.",
    "code_link": ""
  },
  "aaai2022_main_cross-domainfew-shotgraphclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Domain Few-Shot Graph Classification",
    "authors": [
      "Kaveh Hassani"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20642",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20642/20401",
    "published": "2022-02",
    "summary": "We study the problem of few-shot graph classification across domains with nonequivalent feature spaces by introducing three new cross-domain benchmarks constructed from publicly available datasets. We also propose an attention-based graph encoder that uses three congruent views of graphs, one contextual and two topological views, to learn representations of task-specific information for fast adaptation, and task-agnostic information for knowledge transfer. We run exhaustive experiments to evaluate the performance of contrastive and meta-learning strategies. We show that when coupled with metric-based meta-learning frameworks, the proposed encoder achieves the best average meta-test classification accuracy across all benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_spreadgnndecentralizedmulti-taskfederatedlearningforgraphneuralnetworksonmoleculardata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SpreadGNN: Decentralized Multi-Task Federated Learning for Graph Neural Networks on Molecular Data",
    "authors": [
      "Chaoyang He",
      "Emir Ceyani",
      "Keshav Balasubramanian",
      "Murali Annavaram",
      "Salman\n      Avestimehr"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20643",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20643/20402",
    "published": "2022-02",
    "summary": "Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. We provide convergence guarantees and empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. Our results show that SpreadGNN outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies.",
    "code_link": ""
  },
  "aaai2022_main_notallparametersshouldbetreatedequallydeepsafesemi-supervisedlearningunderclassdistributionmismatch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch",
    "authors": [
      "Rundong He",
      "Zhongyi Han",
      "Yang Yang",
      "Yilong Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20644",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20644/20403",
    "published": "2022-02",
    "summary": "Deep semi-supervised learning (SSL) aims to utilize a sizeable unlabeled set to train deep networks, thereby reducing the dependence on labeled instances. However, the unlabeled set often carries unseen classes that cause the deep SSL algorithm to lose generalization. Previous works focus on the data level that they attempt to remove unseen class data or assign lower weight to them but could not eliminate their adverse effects on the SSL algorithm. Rather than focusing on the data level, this paper turns attention to the model parameter level. We find that only partial parameters are essential for seen-class classification, termed safe parameters. In contrast, the other parameters tend to fit irrelevant data, termed harmful parameters. Driven by this insight, we propose Safe Parameter Learning (SPL) to discover safe parameters and make the harmful parameters inactive, such that we can mitigate the adverse effects caused by unseen-class data. Specifically, we firstly design an effective strategy to divide all parameters in the pre-trained SSL model into safe and harmful ones. Then, we introduce a bi-level optimization strategy to update the safe parameters and kill the harmful parameters. Extensive experiments show that SPL outperforms the state-of-the-art SSL methods on all the benchmarks by a large margin. Moreover, experiments demonstrate that SPL can be integrated into the most popular deep SSL networks and be easily extended to handle other cases of class distribution mismatch.",
    "code_link": ""
  },
  "aaai2022_main_wassersteinunsupervisedreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Wasserstein Unsupervised Reinforcement Learning",
    "authors": [
      "Shuncheng He",
      "Yuhang Jiang",
      "Hongchang Zhang",
      "Jianzhun Shao",
      "Xiangyang Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20645",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20645/20404",
    "published": "2022-02",
    "summary": "Unsupervised reinforcement learning aims to train agents to learn a handful of policies or skills in environments without external reward. These pre-trained policies can accelerate learning when endowed with external reward, and can also be used as primitive options in hierarchical reinforcement learning. Conventional approaches of unsupervised skill discovery feed a latent variable to the agent and shed its empowerment on agent\u2019s behavior by mutual information (MI) maximization. However, the policies learned by MI-based methods cannot sufficiently explore the state space, despite they can be successfully identified from each other. Therefore we propose a new framework Wasserstein unsupervised reinforcement learning (WURL) where we directly maximize the distance of state distributions induced by different policies. Additionally, we overcome difficulties in simultaneously training N(N>2) policies, and amortizing the overall reward to each step. Experiments show policies learned by our approach outperform MI-based methods on the metric of Wasserstein distance while keeping high discriminability. Furthermore, the agents trained by WURL can sufficiently explore the state space in mazes and MuJoCo tasks and the pre-trained policies can be applied to downstream tasks by hierarchical learning.",
    "code_link": ""
  },
  "aaai2022_main_multi-modetensorspaceclusteringbasedonlow-tensor-rankrepresentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Mode Tensor Space Clustering Based on Low-Tensor-Rank Representation",
    "authors": [
      "Yicong He",
      "George K. Atia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20646",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20646/20405",
    "published": "2022-02",
    "summary": "Traditional subspace clustering aims to cluster data lying in a union of linear subspaces. The vectorization of high-dimensional data to 1-D vectors to perform clustering ignores much of the structure intrinsic to such data. To preserve said structure, in this work we exploit clustering in a high-order tensor space rather than a vector space. We develop a novel low-tensor-rank representation (LTRR) for unfolded matrices of tensor data lying in a low-rank tensor space. The representation coefficient matrix of an unfolding matrix is tensorized to a 3-order tensor, and the low-tensor-rank constraint is imposed on the transformed coefficient tensor to exploit the self-expressiveness property. Then, inspired by the multi-view clustering framework, we develop a multi-mode tensor space clustering algorithm (MMTSC) that can deal with tensor space clustering with or without missing entries. The tensor is unfolded along each mode, and the coefficient matrices are obtained for each unfolded matrix. The low tensor rank constraint is imposed on a tensor combined from transformed coefficient tensors of each mode, such that the proposed method can simultaneously capture the low rank property for the data within each tensor space and maintain cluster consistency across different modes. Experimental results demonstrate that the proposed MMTSC algorithm can outperform existing clustering algorithms in many cases.",
    "code_link": "https://github.com/he1c/LTRR-TensorSC"
  },
  "aaai2022_main_towardphysicallyrealizablequantumneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Toward Physically Realizable Quantum Neural Networks",
    "authors": [
      "Mohsen Heidari",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20647",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20647/20406",
    "published": "2022-02",
    "summary": "There has been significant recent interest in quantum neural networks (QNNs), along with their applications in diverse domains. Current solutions for QNNs pose significant challenges concerning their scalability, ensuring that the postulates of quantum mechanics are satisfied and that the networks are physically realizable. The exponential state space of QNNs poses challenges for the scalability of training procedures. The no-cloning principle prohibits making multiple copies of training samples, and the measurement postulates lead to non-deterministic loss functions. Consequently, the physical realizability and efficiency of existing approaches that rely on repeated measurement of several copies of each sample for training QNNs are unclear. This paper presents a new model for QNNs that relies on band-limited Fourier expansions of transfer functions of quantum perceptrons (QPs) to design scalable training procedures. This training procedure is augmented with a randomized quantum stochastic gradient descent technique that eliminates the need for sample replication. We show that this training procedure converges to the true minima in expectation, even in the presence of non-determinism due to quantum measurement. Our solution has a number of important benefits: (i) using QPs with concentrated Fourier power spectrum, we show that the training procedure for QNNs can be made scalable; (ii) it eliminates the need for resampling, thus staying consistent with the no-cloning rule; and (iii) enhanced data efficiency for the overall training process since each data sample is processed once per epoch. We present a detailed theoretical foundation for our models and methods' scalability, accuracy, and data efficiency. We also validate the utility of our approach through a series of numerical experiments.",
    "code_link": "https://github.com/mohsenhdkh/RandomizedQSGD"
  },
  "aaai2022_main_reinforcementlearningofcausalvariablesusingmediationanalysis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reinforcement Learning of Causal Variables Using Mediation Analysis",
    "authors": [
      "Tue Herlau",
      "Rasmus Larsen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20648",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20648/20407",
    "published": "2022-02",
    "summary": "We consider the problem of acquiring causal representations and concepts in a reinforcement learning setting. Our approach defines a causal variable as being both manipulable by a policy, and able to predict the outcome. We thereby obtain a parsimonious causal graph in which interventions occur at the level of policies.The approach avoids defining a generative model of the data, prior pre-processing, or learning the transition kernel of the Markov decision process. Instead, causal variables and policies are determined by maximizing a new optimization target inspired by mediation analysis, which differs from the expected return. The maximization is accomplished using a generalization of Bellman's equation which is shown to converge, and the method finds meaningful causal representations in a simulated environment.",
    "code_link": "https://gitlab.compute.dtu.dk/tuhe/causal"
  },
  "aaai2022_main_anytimeguaranteesunderheavy-taileddata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Anytime Guarantees under Heavy-Tailed Data",
    "authors": [
      "Matthew J. Holland"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20649",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20649/20408",
    "published": "2022-02",
    "summary": "Under data distributions which may be heavy-tailed, many stochastic gradient-based learning algorithms are driven by feedback queried at points with almost no performance guarantees on their own. Here we explore a modified \"anytime online-to-batch\" mechanism which for smooth objectives admits high-probability error bounds while requiring only lower-order moment bounds on the stochastic gradients. Using this conversion, we can derive a wide variety of \"anytime robust\" procedures, for which the task of performance analysis can be effectively reduced to regret control, meaning that existing regret bounds (for the bounded gradient case) can be robustified and leveraged in a straightforward manner. As a direct takeaway, we obtain an easily implemented stochastic gradient-based algorithm for which all queried points formally enjoy sub-Gaussian error bounds, and in practice show noteworthy gains on real-world data applications.",
    "code_link": "https://github.com/feedbackward/anytime"
  },
  "aaai2022_main_adversarialexamplescanbeeffectivedataaugmentationforunsupervisedmachinelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Examples Can Be Effective Data Augmentation for Unsupervised Machine Learning",
    "authors": [
      "Chia-Yi Hsu",
      "Pin-Yu Chen",
      "Songtao Lu",
      "Sijia Liu",
      "Chia-Mu Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20650",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20650/20409",
    "published": "2022-02",
    "summary": "Adversarial examples causing evasive predictions are widely used to evaluate and improve the robustness of machine learning models. However, current studies focus on supervised learning tasks, relying on the ground truth data label, a targeted objective, or supervision from a trained classifier. In this paper, we propose a framework of generating adversarial examples for unsupervised models and demonstrate novel applications to data augmentation. Our framework exploits a mutual information neural estimator as an information theoretic similarity measure to generate adversarial examples without supervision. We propose a new MinMax algorithm with provable convergence guarantees for the efficient generation of unsupervised adversarial examples. Our framework can also be extended to supervised adversarial examples. When using unsupervised adversarial examples as a simple plugin data augmentation tool for model retraining, significant improvements are consistently observed across different unsupervised tasks and datasets, including data reconstruction, representation learning, and contrastive learning. Our results show novel methods and considerable advantages in studying and improving unsupervised machine learning via adversarial examples.",
    "code_link": "https://github.com/IBM/UAE"
  },
  "aaai2022_main_towardsautomatingmodelexplanationswithcertifiedrobustnessguarantees": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Automating Model Explanations with Certified Robustness Guarantees",
    "authors": [
      "Mengdi Huai",
      "Jinduo Liu",
      "Chenglin Miao",
      "Liuyi Yao",
      "Aidong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20651",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20651/20410",
    "published": "2022-02",
    "summary": "Providing model explanations has gained significant popularity recently. In contrast with the traditional feature-level model explanations, concept-based explanations can provide explanations in the form of high-level human concepts. However, existing concept-based explanation methods implicitly follow a two-step procedure that involves human intervention. Specifically, they first need the human to be involved to define (or extract) the high-level concepts, and then manually compute the importance scores of these identified concepts in a post-hoc way. This laborious process requires significant human effort and resource expenditure due to manual work, which hinders their large-scale deployability. In practice, it is challenging to automatically generate the concept-based explanations without human intervention due to the subjectivity of defining the units of concept-based interpretability. In addition, due to its data-driven nature, the interpretability itself is also potentially susceptible to malicious manipulations. Hence, our goal in this paper is to free human from this tedious process, while ensuring that the generated explanations are provably robust to adversarial perturbations. We propose a novel concept-based interpretation method, which can not only automatically provide the prototype-based concept explanations but also provide certified robustness guarantees for the generated prototype-based explanations. We also conduct extensive experiments on real-world datasets to verify the desirable properties of the proposed method.",
    "code_link": ""
  },
  "aaai2022_main_multi-viewclusteringontopologicalmanifold": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-View Clustering on Topological Manifold",
    "authors": [
      "Shudong Huang",
      "Ivor Tsang",
      "Zenglin Xu",
      "Jiancheng Lv",
      "Quan-Hui Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20652",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20652/20411",
    "published": "2022-02",
    "summary": "Multi-view clustering has received a lot of attentions in data mining recently. Though plenty of works have been investigated on this topic, it is still a severe challenge due to the complex nature of the multiple heterogeneous features. Particularly, existing multi-view clustering algorithms fail to consider the topological structure in the data, which is essential for clustering data on manifold. In this paper, we propose to exploit the implied data manifold by learning the topological relationship between data points. Our method coalesces multiple view-wise graphs with the topological relevance considered, and learns the weights as well as the consensus graph interactively in a unified framework. Furthermore, we manipulate the consensus graph by a connectivity constraint such that the data points from the same cluster are precisely connected into the same component. Substantial experiments on both toy data and real datasets are conducted to validate the effectiveness of the proposed method, compared to the state-of-the-art algorithms over the clustering performance.",
    "code_link": ""
  },
  "aaai2022_main_achievingcounterfactualfairnessforcausalbandit": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Achieving Counterfactual Fairness for Causal Bandit",
    "authors": [
      "Wen Huang",
      "Lu Zhang",
      "Xintao Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20653",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20653/20412",
    "published": "2022-02",
    "summary": "In online recommendation, customers arrive in a sequential and stochastic manner from an underlying distribution and the online decision model recommends a chosen item for each arriving individual based on some strategy. We study how to recommend an item at each step to maximize the expected reward while achieving user-side fairness for customers, i.e., customers who share similar profiles will receive a similar reward regardless of their sensitive attributes and items being recommended. By incorporating causal inference into bandits and adopting soft intervention to model the arm selection strategy, we first propose the d-separation based UCB algorithm (D-UCB) to explore the utilization of the d-separation set in reducing the amount of exploration needed to achieve low cumulative regret. Based on that, we then propose the fair causal bandit (F-UCB) for achieving the counterfactual individual fairness. Both theoretical analysis and empirical evaluation demonstrate effectiveness of our algorithms.",
    "code_link": ""
  },
  "aaai2022_main_uncertainty-awarelearningagainstlabelnoiseonimbalanceddatasets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Uncertainty-Aware Learning against Label Noise on Imbalanced Datasets",
    "authors": [
      "Yingsong Huang",
      "Bing Bai",
      "Shengwei Zhao",
      "Kun Bai",
      "Fei Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20654",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20654/20413",
    "published": "2022-02",
    "summary": "Learning against label noise is a vital topic to guarantee a reliable performance for deep neural networks.Recent research usually refers to dynamic noise modeling with model output probabilities and loss values, and then separates clean and noisy samples.These methods have gained notable success. However, unlike cherry-picked data, existing approaches often cannot perform well when facing imbalanced datasets, a common scenario in the real world.We thoroughly investigate this phenomenon and point out two major issues that hinder the performance, i.e., inter-class loss distribution discrepancy and misleading predictions due to uncertainty.The first issue is that existing methods often perform class-agnostic noise modeling. However, loss distributions show a significant discrepancy among classes under class imbalance, and class-agnostic noise modeling can easily get confused with noisy samples and samples in minority classes.The second issue refers to that models may output misleading predictions due to epistemic uncertainty and aleatoric uncertainty, thus existing methods that rely solely on the output probabilities may fail to distinguish confident samples. Inspired by our observations, we propose an Uncertainty-aware Label Correction framework(ULC) to handle label noise on imbalanced datasets. First, we perform epistemic uncertainty-aware class-specific noise modeling to identify trustworthy clean samples and refine/discard highly confident true/corrupted labels.Then, we introduce aleatoric uncertainty in the subsequent learning process to prevent noise accumulation in the label noise modeling process. We conduct experiments on several synthetic and real-world datasets. The results demonstrate the effectiveness of the proposed method, especially on imbalanced datasets.",
    "code_link": ""
  },
  "aaai2022_main_globallyoptimalhierarchicalreinforcementlearningforlinearly-solvablemarkovdecisionprocesses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes",
    "authors": [
      "Guillermo Infante",
      "Anders Jonsson",
      "Vicen\u00e7 G\u00f3mez"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20655",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20655/20414",
    "published": "2022-02",
    "summary": "We present a novel approach to hierarchical reinforcement learning for linearly-solvable Markov decision processes. Our approach assumes that the state space is partitioned, and defines subtasks for moving between the partitions. We represent value functions on several levels of abstraction, and use the compositionality of subtasks to estimate the optimal values of the states in each partition. The policy is implicitly defined on these optimal value estimates, rather than being decomposed among the subtasks. As a consequence, our approach can learn the globally optimal policy, and does not suffer from non-stationarities induced by high-level decisions. If several partitions have equivalent dynamics, the subtasks of those partitions can be shared. We show that our approach is significantly more sample efficient than that of a flat learner and similar hierarchical approaches when the set of boundary states is smaller than the entire state space.",
    "code_link": "https://github.com/guillermoim/HRL"
  },
  "aaai2022_main_causaldiscoveryinhawkesprocessesbyminimumdescriptionlength": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Causal Discovery in Hawkes Processes by Minimum Description Length",
    "authors": [
      "Amirkasra Jalaldoust",
      "Kate\u0159ina Hlav\u00e1\u010dkov\u00e1-Schindler",
      "Claudia Plant"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20656",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20656/20415",
    "published": "2022-02",
    "summary": "Hawkes processes are a special class of temporal point processes which exhibit a natural notion of causality, as occurrence of events in the past may increase the probability of events in the future. Discovery of the underlying in\ufb02uence network among the dimensions of multi-dimensional temporal processes is of high importance in disciplines where a high-frequency data is to model, e.g. in \ufb01nancial data or in seismological data. This paper approaches the problem of learning Granger-causal network in multi-dimensional Hawkes processes. We formulate this problem as a model selection task in which we follow the minimum description length (MDL) principle. Moreover, we propose a general algorithm for MDL-based inference using a Monte-Carlo method and we use it for our causal discovery problem. We compare our algorithm with the state-of-the-art baseline methods on synthetic and real-world \ufb01nancial data. The synthetic experiments demonstrate superiority of our method in causal graph discovery compared to the baseline methods with respect to the size of the data. The results of experiments with the G-7 bonds price data are consistent with the experts\u2019 knowledge.",
    "code_link": ""
  },
  "aaai2022_main_group-awarethresholdadaptationforfairclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Group-Aware Threshold Adaptation for Fair Classification",
    "authors": [
      "Taeuk Jang",
      "Pengyi Shi",
      "Xiaoqian Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20657",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20657/20416",
    "published": "2022-02",
    "summary": "The fairness in machine learning is getting increasing attention, as its applications in different fields continue to expand and diversify. To mitigate the discriminated model behaviors between different demographic groups, we introduce a novel post-processing method to optimize over multiple fairness constraints through group-aware threshold adaptation. We propose to learn adaptive classification thresholds for each demographic group by optimizing the confusion matrix estimated from the probability distribution of a classification model output. As we only need an estimated probability distribution of model output instead of the classification model structure, our post-processing model can be applied to a wide range of classification models and improve fairness in a model-agnostic manner and ensure privacy. This even allows us to post-process existing fairness methods to further improve the trade-off between accuracy and fairness. Moreover, our model has low computational cost. We provide rigorous theoretical analysis on the convergence of our optimization algorithm and the trade-off between accuracy and fairness. Our method theoretically enables a better upper bound in near optimality than previous method under the same condition. Experimental results demonstrate that our method outperforms state-of-the-art methods and obtains the result that is closest to the theoretical accuracy-fairness trade-off boundary.",
    "code_link": "https://github.com/propublica/compas-analysis"
  },
  "aaai2022_main_towardsdiscriminantanalysisclassifiersusingonlineactivelearningviamyoelectricinterfaces": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Discriminant Analysis Classifiers Using Online Active Learning via Myoelectric Interfaces",
    "authors": [
      "Andres G Jaramillo-Yanez",
      "Marco E. Benalc\u00e1zar",
      "Sebastian Sardina",
      "Fabio\n      Zambetta"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20658",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20658/20417",
    "published": "2022-02",
    "summary": "We propose a discriminant analysis (DA) classifier that uses online active learning to address the need for the frequent training of myoelectric interfaces due to covariate shift. This online classifier is initially trained using a small set of examples, and then updated over time using streaming data that are interactively labeled by a user or pseudo-labeled by a soft-labeling technique. We prove, theoretically, that this yields the same model as training a DA classifier via full batch learning. We then provide experimental evidence that our approach improves the performance of DA classifiers and is robust to mislabeled data, and that our soft-labeling technique has better performance than existing state-of-the-art methods. We argue that our proposal is suitable for real-time applications, as its time complexity w.r.t. the streaming data remains constant.",
    "code_link": "https://github.com/andresjarami/OnlineDAclassifier"
  },
  "aaai2022_main_labelhallucinationforfew-shotclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Label Hallucination for Few-Shot Classification",
    "authors": [
      "Yiren Jian",
      "Lorenzo Torresani"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20659",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20659/20418",
    "published": "2022-02",
    "summary": "Few-shot classification requires adapting knowledge learned from a large annotated base dataset to recognize novel unseen classes, each represented by few labeled examples. In such a scenario, pretraining a network with high capacity on the large dataset and then finetuning it on the few examples causes severe overfitting. At the same time, training a simple linear classifier on top of ``frozen'' features learned from the large labeled dataset fails to adapt the model to the properties of the novel classes, effectively inducing underfitting. In this paper we propose an alternative approach to both of these two popular strategies. First, our method pseudo-labels the entire large dataset using the linear classifier trained on the novel classes. This effectively ``hallucinates'' the novel classes in the large dataset, despite the novel categories not being present in the base database (novel and base classes are disjoint). Then, it finetunes the entire model with a distillation loss on the pseudo-labeled base examples, in addition to the standard cross-entropy loss on the novel dataset. This step effectively trains the network to recognize contextual and appearance cues that are useful for the novel-category recognition but using the entire large-scale base dataset and thus overcoming the inherent data-scarcity problem of few-shot learning. Despite the simplicity of the approach, we show that that our method outperforms the state-of-the-art on four well-established few-shot classification benchmarks. The code is available at https://github.com/yiren-jian/LabelHalluc.",
    "code_link": "https://github.com/yiren-jian/LabelHalluc"
  },
  "aaai2022_main_learningexpectedemphatictracesfordeeprl": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Expected Emphatic Traces for Deep RL",
    "authors": [
      "Ray Jiang",
      "Shangtong Zhang",
      "Veronica Chelu",
      "Adam White",
      "Hado van Hasselt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20660",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20660/20419",
    "published": "2022-02",
    "summary": "Off-policy sampling and experience replay are key for improving sample efficiency and scaling model-free temporal difference learning methods. When combined with function approximation, such as neural networks, this combination is known as the deadly triad and is potentially unstable. Recently, it has been shown that stability and good performance at scale can be achieved by combining emphatic weightings and multi-step updates. This approach, however, is generally limited to sampling complete trajectories in order, to compute the required emphatic weighting. In this paper we investigate how to combine emphatic weightings with non-sequential, off-line data sampled from a replay buffer. We develop a multi-step emphatic weighting that can be combined with replay, and a time-reversed n-step TD learning algorithm to learn the required emphatic weighting. We show that these state weightings reduce variance compared with prior approaches, while providing convergence guarantees. We tested the approach at scale on Atari 2600 video games, and observed that the new X-ETD(n) agent improved over baseline agents, highlighting both the scalability and broad applicability of our approach.",
    "code_link": ""
  },
  "aaai2022_main_delvingintosamplelosscurvetoembracenoisyandimbalanceddata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
    "authors": [
      "Shenwang Jiang",
      "Jianan Li",
      "Ying Wang",
      "Bo Huang",
      "Zhang Zhang",
      "Tingfa Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20661",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20661/20420",
    "published": "2022-02",
    "summary": "Corrupted labels and class imbalance are commonly encountered in practically collected training data, which easily leads to over-fitting of deep neural networks (DNNs).Existing approaches alleviate these issuesby adopting a sample re-weighting strategy, which is to re-weight sample by designing weighting function. However, it is only applicable for training data containing only either one type of data biases. In practice, however, biased samples with corrupted labels and of tailed classes commonly co-exist in training data. How to handle them simultaneously is a key but under-explored problem. In this paper, we find that these two types of biased samples, though have similar transient loss, have distinguishable trend and characteristics in loss curves, which could provide valuable priors for sample weight assignment. Motivated by this, we delve into the loss curves and propose a novel probe-and-allocate training strategy: In the probing stage, we train the network on the whole biased training data without intervention, and record the loss curve of each sample as an additional attribute; In the allocating stage, we feed the resulting attribute to a newly designed curve-perception network, named CurveNet, to learn to identify the bias type of each sample and assign proper weights through meta-learning adaptively.The training speed of meta learning also blocks its application. To solve it, we propose a method named skip layer meta optimization (SLMO)to accelerate training speed by skipping the bottom layers. Extensive synthetic and real experiments well validate the proposed method, which achieves state-of-the-art performance on multiple challenging benchmarks.",
    "code_link": "https://github.com/jiangwenj02/CurveNet-V1"
  },
  "aaai2022_main_fastgraphneuraltangentkernelviakroneckersketching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast Graph Neural Tangent Kernel via Kronecker Sketching",
    "authors": [
      "Shunhua Jiang",
      "Yunze Man",
      "Zhao Song",
      "Zheng Yu",
      "Danyang Zhuo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20662",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20662/20421",
    "published": "2022-02",
    "summary": "Many deep learning tasks need to deal with graph data (e.g., social networks, protein structures, code ASTs). Due to the importance of these tasks, people turned to Graph Neural Networks (GNNs) as the de facto method for machine learning on graph data. GNNs have become widely applied due to their convincing performance. Unfortunately, one major barrier to using GNNs is that GNNs require substantial time and resources to train. Recently, a new method for learning on graph data is Graph Neural Tangent Kernel (GNTK). GNTK is an application of Neural Tangent Kernel (NTK) (a kernel method) on graph data, and solving NTK regression is equivalent to using gradient descent to train an infinite-wide neural network. The key benefit of using GNTK is that, similar to any kernel method, GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Meanwhile, sketching has become increasingly used in speeding up various optimization problems, including solving kernel regression. Given a kernel matrix of n graphs, using sketching in solving kernel regression can reduce the running time to o(n^3). But unfortunately such methods usually require extensive knowledge about the kernel matrix beforehand, while in the case of GNTK we find that the construction of the kernel matrix is already O(n^2N^4), assuming each graph has N nodes. The kernel matrix construction time can be a major performance bottleneck when the size of graphs N increases. A natural question to ask is thus whether we can speed up the kernel matrix construction to improve GNTK regression's end-to-end running time. This paper provides the first algorithm to construct the kernel matrix in o(n^2N^3) running time.",
    "code_link": ""
  },
  "aaai2022_main_creativityofaiautomaticsymbolicoptiondiscoveryforfacilitatingdeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Creativity of AI: Automatic Symbolic Option Discovery for Facilitating Deep Reinforcement Learning",
    "authors": [
      "Mu Jin",
      "Zhihao Ma",
      "Kebing Jin",
      "Hankz Hankui Zhuo",
      "Chen Chen",
      "Chao Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20663",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20663/20422",
    "published": "2022-02",
    "summary": "Despite of achieving great success in real life, Deep Reinforcement Learning (DRL) is still suffering from three critical issues, which are data efficiency, lack of the interpretability and transferability. Recent research shows that embedding symbolic knowledge into DRL is promising in addressing those challenges. Inspired by this, we introduce a novel deep reinforcement learning framework with symbolic options. This framework features a loop training procedure, which enables guiding the improvement of policy by planning with action models and symbolic options learned from interactive trajectories automatically. The learned symbolic options help doing the dense requirement of expert domain knowledge and provide inherent interpretabiliy of policies. Moreover, the transferability and data efficiency can be further improved by planning with the action models. To validate the effectiveness of this framework, we conduct experiments on two domains, Montezuma's Revenge and Office World respectively, and the results demonstrate the comparable performance, improved data efficiency, interpretability and transferability.",
    "code_link": ""
  },
  "aaai2022_main_adaptivekernelgraphneuralnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Kernel Graph Neural Network",
    "authors": [
      "Mingxuan Ju",
      "Shifu Hou",
      "Yujie Fan",
      "Jianan Zhao",
      "Yanfang Ye",
      "Liang Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20664",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20664/20423",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs) have demonstrated great success in representation learning for graph-structured data. The layer-wise graph convolution in GNNs is shown to be powerful at capturing graph topology. During this process, GNNs are usually guided by pre-defined kernels such as Laplacian matrix, adjacency matrix, or their variants. However, the adoptions of pre-defined kernels may restrain the generalities to different graphs: mismatch between graph and kernel would entail sub-optimal performance. For example, GNNs that focus on low-frequency information may not achieve satisfactory performance when high-frequency information is significant for the graphs, and vice versa. To solve this problem, in this paper, we propose a novel framework - i.e., namely Adaptive Kernel Graph Neural Network (AKGNN) - which learns to adapt to the optimal graph kernel in a unified manner at the first attempt. In the proposed AKGNN, we first design a data-driven graph kernel learning mechanism, which adaptively modulates the balance between all-pass and low-pass filters by modifying the maximal eigenvalue of the graph Laplacian. Through this process, AKGNN learns the optimal threshold between high and low frequency signals to relieve the generality problem. Later, we further reduce the number of parameters by a parameterization trick and enhance the expressive power by a global readout function. Extensive experiments are conducted on acknowledged benchmark datasets and promising results demonstrate the outstanding performance of our proposed AKGNN by comparison with state-of-the-art GNNs. The source code is publicly available at: https://github.com/jumxglhf/AKGNN.",
    "code_link": "https://github.com/jumxglhf/AKGNN"
  },
  "aaai2022_main_fullyspikingvariationalautoencoder": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fully Spiking Variational Autoencoder",
    "authors": [
      "Hiromichi Kamata",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20665",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20665/20424",
    "published": "2022-02",
    "summary": "Spiking neural networks (SNNs) can be run on neuromorphic devices with ultra-high speed and ultra-low energy consumption because of their binary and event-driven nature. Therefore, SNNs are expected to have various applications, including as generative models being running on edge devices to create high-quality images. In this study, we build a variational autoencoder (VAE) with SNN to enable image generation. VAE is known for its stability among generative models; recently, its quality advanced. In vanilla VAE, the latent space is represented as a normal distribution, and floating-point calculations are required in sampling. However, this is not possible in SNNs because all features must be binary time series data. Therefore, we constructed the latent space with an autoregressive SNN model, and randomly selected samples from its output to sample the latent variables. This allows the latent variables to follow the Bernoulli process and allows variational learning. Thus, we build the Fully Spiking Variational Autoencoder where all modules are constructed with SNN. To the best of our knowledge, we are the first to build a VAE only with SNN layers. We experimented with several datasets, and confirmed that it can generate images with the same or better quality compared to conventional ANNs. The code is available at https://github.com/kamata1729/FullySpikingVAE.",
    "code_link": "https://github.com/kamata1729/FullySpikingVAE"
  },
  "aaai2022_main_classifyingemailsintohumanvsmachinecategory": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Classifying Emails into Human vs Machine Category",
    "authors": [
      "Changsung Kang",
      "Hongwei Shang",
      "Jean-Marc Langlois"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20666",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20666/20425",
    "published": "2022-02",
    "summary": "It is an essential product requirement of Yahoo Mail to distinguish between personal and machine-generated emails. The old production classifier in Yahoo Mail was based on a simple logistic regression model. That model was trained by aggregating features at the SMTP address level. We propose building deep learning models at the message level. We train four individual CNN models: (1) a content model with subject and content as input, (2) a sender model with sender email address and name as input, (3) an action model by analyzing email recipients\u2019 action patterns and generating target labels based on senders\u2019 opening/deleting behaviors and (4) a salutation model by utilizing senders\u2019 \"explicit salutation\" signal as positive labels. Next, we train a final full model after exploring different combinations of the above four models. Experimental results on editorial data show that our full model improves the adjusted-recall from 70.5% to 78.8% and the precision from 94.7% to 96.0% compared to the old production model. Also, our full model significantly outperforms a state-of-the-art BERT model at this task. Our new model has been deployed to the current production system (Yahoo Mail 6).",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedenhancementoflatentdiscoveryingans": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Enhancement of Latent Discovery in GANs",
    "authors": [
      "Adarsh Kappiyath",
      "Silpa Vadakkeeveetil Sreelatha",
      "S. Sumitra"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20667",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20667/20426",
    "published": "2022-02",
    "summary": "Several methods for discovering interpretable directions in the latent space of pre-trained GANs have been proposed. Latent semantics discovered by unsupervised methods are less disentangled than supervised methods since they do not use pre-trained attribute classifiers. We propose Scale Ranking Estimator (SRE), which is trained using self-supervision. SRE enhances the disentanglement in directions obtained by existing unsupervised disentanglement techniques. These directions are updated to preserve the ordering of variation within each direction in latent space. Qualitative and quantitative evaluation of the discovered directions demonstrates that our proposed method significantly improves disentanglement in various datasets. We also show that the learned SRE can be used to perform Attribute-based image retrieval task without any training.",
    "code_link": "https://github.com/deepmind/3dshapes-dataset"
  },
  "aaai2022_main_multiple-sourcedomainadaptationviacoordinateddomainencodersandpairedclassifiers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers",
    "authors": [
      "Payam Karisani"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20668",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20668/20427",
    "published": "2022-02",
    "summary": "We present a novel multiple-source unsupervised model for text classification under domain shift. Our model exploits the update rates in document representations to dynamically integrate domain encoders. It also employs a probabilistic heuristic to infer the error rate in the target domain in order to pair source classifiers. Our heuristic exploits data transformation cost and the classifier accuracy in the target feature space. We have used real world scenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We also used pretrained multi-layer transformers as the document encoder in the experiments to demonstrate whether the improvement achieved by domain adaptation models can be delivered by out-of-the-box language model pretraining. The experiments testify that our model is the top performing approach in this setting.",
    "code_link": ""
  },
  "aaai2022_main_instance-sensitivealgorithmsforpureexplorationinmultinomiallogitbandit": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit",
    "authors": [
      "Nikolai Karpov",
      "Qin Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20669",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20669/20428",
    "published": "2022-02",
    "summary": "Motivated by real-world applications such as fast fashion retailing and online advertising, the Multinomial Logit Bandit (MNL-bandit) is a popular model in online learning and operations research, and has attracted much attention in the past decade. In this paper, we give efficient algorithms for pure exploration in MNL-bandit. Our algorithms achieve instance-sensitive pull complexities. We also complement the upper bounds by an almost matching lower bound.",
    "code_link": ""
  },
  "aaai2022_main_idecodein-distributionequivarianceforconformalout-of-distributiondetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "iDECODe: In-Distribution Equivariance for Conformal Out-of-Distribution Detection",
    "authors": [
      "Ramneet Kaur",
      "Susmit Jha",
      "Anirban Roy",
      "Sangdon Park",
      "Edgar Dobriban",
      "Oleg\n      Sokolsky",
      "Insup Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20670",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20670/20429",
    "published": "2022-02",
    "summary": "Machine learning methods such as deep neural networks (DNNs), despite their success across different domains, are known to often generate incorrect predictions with high confidence on inputs outside their training distribution. The deployment of DNNs in safety-critical domains requires detection of out-of-distribution (OOD) data so that DNNs can abstain from making predictions on those. A number of methods have been recently developed for OOD detection, but there is still room for improvement. We propose the new method iDECODe, leveraging in-distribution equivariance for conformal OOD detection. It relies on a novel base non-conformity measure and a new aggregation method, used in the inductive conformal anomaly detection framework, thereby guaranteeing a bounded false detection rate. We demonstrate the efficacy of iDECODe by experiments on image and audio datasets, obtaining state-of-the-art results. We also show that iDECODe can detect adversarial examples. Code, pre-trained models, and data are available athttps://github.com/ramneetk/iDECODe.",
    "code_link": "https://github.com/ramneetk/iDECODe"
  },
  "aaai2022_main_partialwassersteincovering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Partial Wasserstein Covering",
    "authors": [
      "Keisuke Kawano",
      "Satoshi Koide",
      "Keisuke Otaki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20671",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20671/20430",
    "published": "2022-02",
    "summary": "We consider a general task called partial Wasserstein covering with the goal of providing information on what patterns are not being taken into account in a dataset (e.g., dataset used during development) compared to another (e.g., dataset obtained from actual applications). We model this task as a discrete optimization problem with partial Wasserstein divergence as an objective function. Although this problem is NP-hard, we prove that it satisfies the submodular property, allowing us to use a greedy algorithm with a 0.63 approximation. However, the greedy algorithm is still inefficient because it requires solving linear programming for each objective function evaluation. To overcome this inefficiency, we propose quasi-greedy algorithms, which consist of a series of techniques for acceleration such as sensitivity analysis based on strong duality and the so-called C-transform in the optimal transport field. Experimentally, we demonstrate that we can efficiently fill in the gaps between the two datasets, and find missing scene in real driving scene datasets.",
    "code_link": ""
  },
  "aaai2022_main_optimaltensortransport": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimal Tensor Transport",
    "authors": [
      "Tanguy Kerdoncuff",
      "R\u00e9mi Emonet",
      "Michael Perrot",
      "Marc Sebban"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20672",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20672/20431",
    "published": "2022-02",
    "summary": "Optimal Transport (OT) has become a popular tool in machine learning to align finite datasets typically lying in the same vector space. To expand the range of possible applications, Co-Optimal Transport (Co-OT) jointly estimates two distinct transport plans, one for the rows (points) and one for the columns (features), to match two data matrices that might use different features. On the other hand, Gromov Wasserstein (GW) looks for a single transport plan from two pairwise intra-domain distance matrices. Both Co-OT and GW can be seen as specific extensions of OT to more complex data. In this paper, we propose a unified framework, called Optimal Tensor Transport (OTT), which takes the form of a generic formulation that encompasses OT, GW and Co-OT and can handle tensors of any order by learning possibly multiple transport plans. We derive theoretical results for the resulting new distance and present an efficient way for computing it. We further illustrate the interest of such a formulation in Domain Adaptation and Comparison-based Clustering.",
    "code_link": "https://github.com/Hv0nnus/Optimal"
  },
  "aaai2022_main_dist2cycleasimplicialneuralnetworkforhomologylocalization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dist2Cycle: A Simplicial Neural Network for Homology Localization",
    "authors": [
      "Alexandros D Keros",
      "Vidit Nanda",
      "Kartic Subr"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20673",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20673/20432",
    "published": "2022-02",
    "summary": "Simplicial complexes can be viewed as high dimensional generalizations of graphs that explicitly encode multi-way ordered relations between vertices at different resolutions, all at once. This concept is central towards detection of higher dimensional topological features of data, features to which graphs, encoding only pairwise relationships, remain oblivious. While attempts have been made to extend Graph Neural Networks (GNNs) to a simplicial complex setting, the methods do not inherently exploit, or reason about, the underlying topological structure of the network. We propose a graph convolutional model for learning functions parametrized by the k-homological features of simplicial complexes. By spectrally manipulating their combinatorial k-dimensional Hodge Laplacians, the proposed model enables learning topological features of the underlying simplicial complexes, specifically, the distance of each k-simplex from the nearest \"optimal\" k-th homology generator, effectively providing an alternative to homology localization.",
    "code_link": "https://github.com/alexdkeros/Dist2Cycle"
  },
  "aaai2022_main_samestate,differenttaskcontinualreinforcementlearningwithoutinterference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Same State, Different Task: Continual Reinforcement Learning without Interference",
    "authors": [
      "Samuel Kessler",
      "Jack Parker-Holder",
      "Philip Ball",
      "Stefan Zohren",
      "Stephen J.\n      Roberts"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20674",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20674/20433",
    "published": "2022-02",
    "summary": "Continual Learning (CL) considers the problem of training an agent sequentially on a set of tasks while seeking to retain performance on all previous tasks. A key challenge in CL is catastrophic forgetting, which arises when performance on a previously mastered task is reduced when learning a new task. While a variety of methods exist to combat forgetting, in some cases tasks are fundamentally incompatible with each other and thus cannot be learnt by a single policy. This can occur, in reinforcement learning (RL) when an agent may be rewarded for achieving different goals from the same observation. In this paper we formalize this \"interference\" as distinct from the problem of forgetting. We show that existing CL methods based on single neural network predictors with shared replay buffers fail in the presence of interference. Instead, we propose a simple method, OWL, to address this challenge. OWL learns a factorized policy, using shared feature extraction layers, but separate heads, each specializing on a new task. The separate heads in OWL are used to prevent interference. At test time, we formulate policy selection as a multi-armed bandit problem, and show it is possible to select the best policy for an unknown task using feedback from the environment. The use of bandit algorithms allows the OWL agent to constructively re-use different continually learnt policies at different times during an episode. We show in multiple RL environments that existing replay based CL methods fail, while OWL is able to achieve close to optimal performance when training sequentially.",
    "code_link": ""
  },
  "aaai2022_main_spatialfrequencybiasinconvolutionalgenerativeadversarialnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Spatial Frequency Bias in Convolutional Generative Adversarial Networks",
    "authors": [
      "Mahyar Khayatkhoei",
      "Ahmed Elgammal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20675",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20675/20434",
    "published": "2022-02",
    "summary": "Understanding the capability of Generative Adversarial Networks (GANs) in learning the full spectrum of spatial frequencies, that is, beyond the low-frequency dominant spectrum of natural images, is critical for assessing the reliability of GAN-generated data in any detail-sensitive application. In this work, we show that the ability of convolutional GANs to learn an image distribution depends on the spatial frequency of the underlying carrier signal, that is, they have a bias against learning high spatial frequencies. Our findings are consistent with the recent observations of high-frequency artifacts in GAN-generated images, but further suggest that such artifacts are the consequence of an underlying bias. We also provide a theoretical explanation for this bias as the manifestation of linear dependencies present in the spectrum of filters of a typical generative Convolutional Neural Network (CNN). Finally, by proposing a proof-of-concept method that can effectively manipulate this bias towards other spatial frequencies, we show that the bias is not fixed and can be exploited to explicitly direct computational resources towards any specific spatial frequency of interest in a dataset, with minimal computational overhead.",
    "code_link": ""
  },
  "aaai2022_main_theeffectofmanifoldentanglementandintrinsicdimensionalityonlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Effect of Manifold Entanglement and Intrinsic Dimensionality on Learning",
    "authors": [
      "Daniel Kienitz",
      "Ekaterina Komendantskaya",
      "Michael Lones"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20676",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20676/20435",
    "published": "2022-02",
    "summary": "We empirically investigate the effect of class manifold entanglement and the intrinsic and extrinsic dimensionality of the data distribution on the sample complexity of supervised classification with deep ReLU networks. We separate the effect of entanglement and intrinsic dimensionality and show statistically for artificial and real-world image datasets that the intrinsic dimensionality and the entanglement have an interdependent effect on the sample complexity. Low levels of entanglement lead to low increases of the sample complexity when the intrinsic dimensionality is increased, while for high levels of entanglement the impact of the intrinsic dimensionality increases as well. Further, we show that in general the sample complexity is primarily due to the entanglement and only secondarily due to the intrinsic dimensionality of the data distribution.",
    "code_link": ""
  },
  "aaai2022_main_acomputabledefinitionofthespectralbias": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Computable Definition of the Spectral Bias",
    "authors": [
      "Jonas Kiessling",
      "Filip Thor"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20677",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20677/20436",
    "published": "2022-02",
    "summary": "Neural networks have a bias towards low frequency functions. This spectral bias has been the subject of several previous studies, both empirical and theoretical.Here we present a computable definition of the spectral bias based on a decomposition of the reconstruction error into a low and a high frequency component. The distinction between low and high frequencies is made in a way that allows for easy interpretation of the spectral bias. Furthermore, we present two methods for estimating the spectral bias. Method 1 relies on the use of the discrete Fourier transform to explicitly estimate the Fourier spectrum of the prediction residual, and Method 2 uses convolution to extract the low frequency components, where the convolution integral is estimated by Monte Carlo methods.The spectral bias depends on the distribution of the data, which is approximated with kernel density estimation when unknown. We devise a set of numerical experiments that confirm that low frequencies are learned first, a behavior quantified by our definition.",
    "code_link": ""
  },
  "aaai2022_main_anestedbi-leveloptimizationframeworkforrobustfewshotlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Nested Bi-level Optimization Framework for Robust Few Shot Learning",
    "authors": [
      "Krishnateja Killamsetty",
      "Changbin Li",
      "Chen Zhao",
      "Feng Chen",
      "Rishabh Iyer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20678",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20678/20437",
    "published": "2022-02",
    "summary": "Model-Agnostic Meta-Learning (MAML), a popular gradient-based meta-learning framework, assumes that the contribution of each task or instance to the meta-learner is equal.Hence, it fails to address the domain shift between base and novel classes in few-shot learning. In this work, we propose a novel robust meta-learning algorithm, NESTEDMAML, which learns to assign weights to training tasks or instances. We con-sider weights as hyper-parameters and iteratively optimize them using a small set of validation tasks set in a nested bi-level optimization approach (in contrast to the standard bi-level optimization in MAML). We then applyNESTED-MAMLin the meta-training stage, which involves (1) several tasks sampled from a distribution different from the meta-test task distribution, or (2) some data samples with noisy labels.Extensive experiments on synthetic and real-world datasets demonstrate that NESTEDMAML efficiently mitigates the effects of \u201dunwanted\u201d tasks or instances, leading to significant improvement over the state-of-the-art robust meta-learning methods.",
    "code_link": "https://github.com/Hugo101/NestedMAML"
  },
  "aaai2022_main_fastmonte-carloapproximationoftheattentionmechanism": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
    "authors": [
      "Hyunjun Kim",
      "JeongGil Ko"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20679",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20679/20438",
    "published": "2022-02",
    "summary": "We introduce Monte-Carlo Attention (MCA), a randomized approximation method for reducing the computational cost of self-attention mechanisms in Transformer architectures. MCA exploits the fact that the importance of each token in an input sequence vary with respect to their attention scores; thus, some degree of error can be tolerable when encoding tokens with low attention. Using approximate matrix multiplication, MCA applies different error bounds to encode input tokens such that those with low attention scores are computed with relaxed precision, whereas errors of salient elements are minimized. MCA can operate in parallel with other attention optimization schemes and does not require model modification. We study the theoretical error bounds and demonstrate that MCA reduces attention complexity (in FLOPS) for various Transformer models by up to 11 in GLUE benchmarks without compromising model accuracy. Source code and appendix: https://github.com/eis-lab/monte-carlo-attention",
    "code_link": "https://github.com/eis-lab/monte-carlo-attention"
  },
  "aaai2022_main_towardsarigorousevaluationoftime-seriesanomalydetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards a Rigorous Evaluation of Time-Series Anomaly Detection",
    "authors": [
      "Siwon Kim",
      "Kukjin Choi",
      "Hyun-Soo Choi",
      "Byunghan Lee",
      "Sungroh Yoon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20680",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20680/20439",
    "published": "2022-02",
    "summary": "In recent years, proposed studies on time-series anomaly detection (TAD) report high F1 scores on benchmark TAD datasets, giving the impression of clear improvements in TAD. However, most studies apply a peculiar evaluation protocol called point adjustment (PA) before scoring. In this paper, we theoretically and experimentally reveal that the PA protocol has a great possibility of overestimating the detection performance; even a random anomaly score can easily turn into a state-of-the-art TAD method. Therefore, the comparison of TAD methods after applying the PA protocol can lead to misguided rankings. Furthermore, we question the potential of existing TAD methods by showing that an untrained model obtains comparable detection performance to the existing methods even when PA is forbidden. Based on our findings, we propose a new baseline and an evaluation protocol. We expect that our study will help a rigorous evaluation of TAD and lead to further improvement in future researches.",
    "code_link": ""
  },
  "aaai2022_main_introducingsymmetriestoblackboxmetareinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Introducing Symmetries to Black Box Meta Reinforcement Learning",
    "authors": [
      "Louis Kirsch",
      "Sebastian Flennerhag",
      "Hado van Hasselt",
      "Abram Friesen",
      "Junhyuk Oh",
      "Yutian Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20681",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20681/20440",
    "published": "2022-02",
    "summary": "Meta reinforcement learning (RL) attempts to discover new RL algorithms automatically from environment interaction. In so-called black-box approaches, the policy and the learning algorithm are jointly represented by a single neural network. These methods are very flexible, but they tend to underperform compared to human-engineered RL algorithms in terms of generalisation to new, unseen environments. In this paper, we explore the role of symmetries in meta-generalisation. We show that a recent successful meta RL approach that meta-learns an objective for backpropagation-based learning exhibits certain symmetries (specifically the reuse of the learning rule, and invariance to input and output permutations) that are not present in typical black-box meta RL systems. We hypothesise that these symmetries can play an important role in meta-generalisation. Building off recent work in black-box supervised meta learning, we develop a black-box meta RL system that exhibits these same symmetries. We show through careful experimentation that incorporating these symmetries can lead to algorithms with a greater ability to generalise to unseen action & observation spaces, tasks, and environments.",
    "code_link": ""
  },
  "aaai2022_main_directedgraphauto-encoders": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Directed Graph Auto-Encoders",
    "authors": [
      "Georgios Kollias",
      "Vasileios Kalantzis",
      "Tsuyoshi Ide",
      "Aur\u00e9lie Lozano",
      "Naoki\n      Abe"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20682",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20682/20441",
    "published": "2022-02",
    "summary": "We introduce a new class of auto-encoders for directed graphs, motivated by a direct extension of the Weisfeiler-Leman algorithm to pairs of node labels. The proposed model learns pairs of interpretable latent representations for the nodes of directed graphs, and uses parameterized graph convolutional network (GCN) layers for itsencoder and an asymmetric inner product decoder. Parameters in the encoder control the weighting of representations exchanged between neighboring nodes. We demonstrate the ability of the proposed model to learn meaningful latent embeddings and achieve superior performance on the directed link prediction task on several popular network datasets.",
    "code_link": ""
  },
  "aaai2022_main_hnohigh-ordernumericalarchitectureforode-inspireddeepunfoldingnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "HNO: High-Order Numerical Architecture for ODE-Inspired Deep Unfolding Networks",
    "authors": [
      "Lin Kong",
      "Wei Sun",
      "Fanhua Shang",
      "Yuanyuan Liu",
      "Hongying Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20683",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20683/20442",
    "published": "2022-02",
    "summary": "Recently, deep unfolding networks (DUNs) based on optimization algorithms have received increasing attention, and their high efficiency has been confirmed by many experimental and theoretical results.Since this type of networks combines model-based traditional optimization algorithms, they have high interpretability. In addition, ordinary differential equations (ODEs) are often used to explain deep neural networks, and provide some inspiration for designing innovative network models. In this paper, we transform DUNs into first-order ODE forms, and propose a high-order numerical architecture for ODE-inspired deep unfolding networks. To the best of our knowledge, this is the first work to establish the relationship between DUNs and ODEs. Moreover, we take two representative DUNs as examples, apply our architecture to them and design novel DUNs. In theory, we prove the existence, uniqueness of the solution and convergence of the proposed network, and also prove that our network obtains a fast linear convergence rate. Extensive experiments verify the effectiveness and advantages of our architecture.",
    "code_link": ""
  },
  "aaai2022_main_deepreinforcementlearningpolicieslearnsharedadversarialfeaturesacrossmdps": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Reinforcement Learning Policies Learn Shared Adversarial Features across MDPs",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20684",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20684/20443",
    "published": "2022-02",
    "summary": "The use of deep neural networks as function approximators has led to striking progress for reinforcement learning algorithms and applications. Yet the knowledge we have on decision boundary geometry and the loss landscape of neural policies is still quite limited. In this paper, we propose a framework to investigate the decision boundary and loss landscape similarities across states and across MDPs. We conduct experiments in various games from Arcade Learning Environment, and discover that high sensitivity directions for neural policies are correlated across MDPs. We argue that these high sensitivity directions support the hypothesis that non-robust features are shared across training environments of reinforcement learning agents. We believe our results reveal fundamental properties of the environments used in deep reinforcement learning training, and represent a tangible step towards building robust and reliable deep reinforcement learning agents.",
    "code_link": ""
  },
  "aaai2022_main_fastapproximationsforjobshopschedulingalagrangiandualdeeplearningmethod": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast Approximations for Job Shop Scheduling: A Lagrangian Dual Deep Learning Method",
    "authors": [
      "James Kotary",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20685",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20685/20444",
    "published": "2022-02",
    "summary": "The Jobs Shop Scheduling problem (JSP) is a canonical combinatorial optimization problem that is routinely solved for a variety of industrial purposes. It models the optimal scheduling of multiple sequences of tasks, each under a fixed order of operations, in which individual tasks require exclusive access to a predetermined resource for a specified processing time. The problem is NP-hard and computationally challenging even for medium-sized instances. Motivated by the increased stochasticity in production chains, this paper explores a deep learning approach to deliver efficient and accurate approximations to the JSP. In particular, this paper proposes the design of a deep neural network architecture to exploit the problem structure, its integration with Lagrangian duality to capture the problem constraints, and a post-processing optimization, to guarantee solution feasibility. The resulting method, called JSP-DNN, is evaluated on hard JSP instances from the JSPLIB benchmark library and is shown to produce JSP approximations of high quality at negligible computational costs.",
    "code_link": "https://github.com/tamy0612/JSPLIB"
  },
  "aaai2022_main_learningrobustpolicyagainstdisturbanceintransitiondynamicsviastate-conservativepolicyoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization",
    "authors": [
      "Yufei Kuang",
      "Miao Lu",
      "Jie Wang",
      "Qi Zhou",
      "Bin Li",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20686",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20686/20445",
    "published": "2022-02",
    "summary": "Deep reinforcement learning algorithms can perform poorly in real-world tasks due to the discrepancy between source and target environments. This discrepancy is commonly viewed as the disturbance in transition dynamics. Many existing algorithms learn robust policies by modeling the disturbance and applying it to source environments during training, which usually requires prior knowledge about the disturbance and control of simulators. However, these algorithms can fail in scenarios where the disturbance from target environments is unknown or is intractable to model in simulators. To tackle this problem, we propose a novel model-free actor-critic algorithm---namely, state-conservative policy optimization (SCPO)---to learn robust policies without modeling the disturbance in advance. Specifically, SCPO reduces the disturbance in transition dynamics to that in state space and then approximates it by a simple gradient-based regularizer. The appealing features of SCPO include that it is simple to implement and does not require additional knowledge about the disturbance or specially designed simulators. Experiments in several robot control tasks demonstrate that SCPO learns robust policies against the disturbance in transition dynamics.",
    "code_link": ""
  },
  "aaai2022_main_gradientbasedactivationsforaccuratebias-freelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Gradient Based Activations for Accurate Bias-Free Learning",
    "authors": [
      "Vinod K. Kurmi",
      "Rishabh Sharma",
      "Yash Vardhan Sharma",
      "Vinay P Namboodiri"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20687",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20687/20446",
    "published": "2022-02",
    "summary": "Bias mitigation in machine learning models is imperative, yet challenging. While several approaches have been proposed, one view towards mitigating bias is through adversarial learning. A discriminator is used to identify the bias attributes such as gender, age or race in question. This discriminator is used adversarially to ensure that it cannot distinguish the bias attributes. The main drawback in such a model is that it directly introduces a trade-off with accuracy as the features that the discriminator deems to be sensitive for discrimination of bias could be correlated with classification. In this work we solve the problem. We show that a biased discriminator can actually be used to improve this bias-accuracy tradeoff. Specifically, this is achieved by using a feature masking approach using the discriminator's gradients. We ensure that the features favoured for the bias discrimination are de-emphasized and the unbiased features are enhanced during classification. We show that this simple approach works well to reduce bias as well as improve accuracy significantly. We evaluate the proposed model on standard benchmarks. We improve the accuracy of the adversarial methods while maintaining or even improving the unbiasness and also outperform several other recent methods.",
    "code_link": ""
  },
  "aaai2022_main_trustaltrustworthyactivelearningusingknowledgedistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TrustAL: Trustworthy Active Learning Using Knowledge Distillation",
    "authors": [
      "Beong-woo Kwak",
      "Youngwook Kim",
      "Yu Jin Kim",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20688",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20688/20447",
    "published": "2022-02",
    "summary": "Active learning can be defined as iterations of data labeling, model training, and data acquisition, until sufficient labels are acquired. A traditional view of data acquisition is that, through iterations, knowledge from human labels and models is implicitly distilled to monotonically increase the accuracy and label consistency. Under this assumption, the most recently trained model is a good surrogate for the current labeled data, from which data acquisition is requested based on uncertainty/diversity. Our contribution is debunking this myth and proposing a new objective for distillation. First, we found example forgetting, which indicates the loss of knowledge learned across iterations. Second, for this reason, the last model is no longer the best teacher-- For mitigating such forgotten knowledge, we select one of its predecessor models as a teacher, by our proposed notion of \"consistency\". We show that this novel distillation is distinctive in the following three aspects; First, consistency ensures to avoid forgetting labels. Second, consistency improves both uncertainty/diversity of labeled data. Lastly, consistency redeems defective labels produced by human annotators.",
    "code_link": ""
  },
  "aaai2022_main_tightneuralnetworkverificationviasemidefiniterelaxationsandlinearreformulations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Tight Neural Network Verification via Semidefinite Relaxations and Linear Reformulations",
    "authors": [
      "Jianglin Lan",
      "Yang Zheng",
      "Alessio Lomuscio"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20689",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20689/20448",
    "published": "2022-02",
    "summary": "We present a novel semidefinite programming (SDP) relaxation that enables tight and efficient verification of neural networks. The tightness is achieved by combining SDP relaxations with valid linear cuts, constructed by using the reformulation-linearisation technique (RLT). The computational efficiency results from a layerwise SDP formulation and an iterative algorithm for incrementally adding RLT-generated linear cuts to the verification formulation.The layer RLT-SDP relaxation here presented is shown to produce the tightest SDP relaxation for ReLU neural networks available in the literature. We report experimental results based on MNIST neural networks showing that the method outperforms the state-of-the-art methods while maintaining acceptable computational overheads.For networks of approximately 10k nodes (1k, respectively), the proposed method achieved an improvement in the ratio of certified robustness cases from 0% to 82% (from 35% to 70%, respectively).",
    "code_link": ""
  },
  "aaai2022_main_learningadversarialmarkovdecisionprocesseswithdelayedfeedback": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Adversarial Markov Decision Processes with Delayed Feedback",
    "authors": [
      "Tal Lancewicki",
      "Aviv Rosenberg",
      "Yishay Mansour"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20690",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20690/20449",
    "published": "2022-02",
    "summary": "Reinforcement learning typically assumes that agents observe feedback for their actions immediately, but in many real-world applications (like recommendation systems) feedback is observed in delay. This paper studies online learning in episodic Markov decision processes (MDPs) with unknown transitions, adversarially changing costs and unrestricted delayed feedback. That is, the costs and trajectory of episode k are revealed to the learner only in the end of episode k+d\u1d4f, where the delays d\u1d4f are neither identical nor bounded, and are chosen by an oblivious adversary. We present novel algorithms based on policy optimization that achieve near-optimal high-probability regret of (K+D)\u00b9\u141f\u00b2 under full-information feedback, where K is the number of episodes and D=\u2211\u2096 d\u1d4f is the total delay. Under bandit feedback, we prove similar (K+D)\u00b9\u141f\u00b2 regret assuming the costs are stochastic, and (K+D)\u00b2\u141f\u00b3 regret in the general case. We are the first to consider regret minimization in the important setting of MDPs with delayed feedback.",
    "code_link": ""
  },
  "aaai2022_main_learningnottolearnnatureversusnurtureinsilico": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Not to Learn: Nature versus Nurture In Silico",
    "authors": [
      "Robert Tjarko Lange",
      "Henning Sprekeler"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20691",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20691/20450",
    "published": "2022-02",
    "summary": "Animals are equipped with a rich innate repertoire of sensory, behavioral and motor skills, which allows them to interact with the world immediately after birth. At the same time, many behaviors are highly adaptive and can be tailored to specific environments by means of learning. In this work, we use mathematical analysis and the framework of memory-based meta-learning (or \u2019learning to learn\u2019) to answer when it is beneficial to learn such an adaptive strategy and when to hard-code a heuristic behavior. We find that the interplay of ecological uncertainty, task complexity and the agents\u2019 lifetime has crucial effects on the meta-learned amortized Bayesian inference performed by an agent. There exist two regimes: One in which meta-learning yields a learning algorithm that implements task-dependent information-integration and a second regime in which meta-learning imprints a heuristic or \u2019hard-coded\u2019 behavior. Further analysis reveals that non-adaptive behaviors are not only optimal for aspects of the environment that are stable across individuals, but also in situations where an adaptation to the environment would in fact be highly beneficial, but could not be done quickly enough to be exploited within the remaining lifetime. Hard-coded behaviors should hence not only be those that always work, but also those that are too complex to be learned within a reasonable time frame.",
    "code_link": ""
  },
  "aaai2022_main_optimizationforclassicalmachinelearningproblemsonthegpu": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimization for Classical Machine Learning Problems on the GPU",
    "authors": [
      "S\u00f6ren Laue",
      "Mark Blacher",
      "Joachim Giesen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20692",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20692/20451",
    "published": "2022-02",
    "summary": "Constrained optimization problems arise frequently in classical machine learning. There exist frameworks addressing constrained optimization, for instance, CVXPY and GENO. However, in contrast to deep learning frameworks, GPU support is limited. Here, we extend the GENO framework to also solve constrained optimization problems on the GPU. The framework allows the user to specify constrained optimization problems in an easy-to-read modeling language. A solver is then automatically generated from this specification. When run on the GPU, the solver outperforms state-of-the-art approaches like CVXPY combined with a GPU-accelerated solver such as cuOSQP or SCS by a few orders of magnitude.",
    "code_link": "https://github.com/slaue/genosolver"
  },
  "aaai2022_main_interpretableclusteringviamulti-polytopemachines": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Interpretable Clustering via Multi-Polytope Machines",
    "authors": [
      "Connor Lawless",
      "Jayant Kalagnanam",
      "Lam M Nguyen",
      "Dzung Phan",
      "Chandra Reddy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20693",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20693/20452",
    "published": "2022-02",
    "summary": "Clustering is a popular unsupervised learning tool often used to discover groups within a larger population such as customer segments, or patient subtypes. However, despite its use as a tool for subgroup discovery and description few state-of-the-art algorithms provide any rationale or description behind the clusters found. We propose a novel approach for interpretable clustering that both clusters data points and constructs polytopes around the discovered clusters to explain them. Our framework allows for additional constraints on the polytopes including ensuring that the hyperplanes constructing the polytope are axis-parallel or sparse with integer coefficients. We formulate the problem of constructing clusters via polytopes as a Mixed-Integer Non-Linear Program (MINLP). To solve our formulation we propose a two phase approach where we first initialize clusters and polytopes using alternating minimization, and then use coordinate descent to boost clustering performance. We benchmark our approach on a suite of synthetic and real world clustering problems, where our algorithm outperforms state of the art interpretable and non-interpretable clustering algorithms.",
    "code_link": ""
  },
  "aaai2022_main_episodicpolicygradienttraining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Episodic Policy Gradient Training",
    "authors": [
      "Hung Le",
      "Majid Abdolshah",
      "Thommen K. George",
      "Kien Do",
      "Dung Nguyen",
      "Svetha\n      Venkatesh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20694",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20694/20453",
    "published": "2022-02",
    "summary": "We introduce a novel training procedure for policy gradient methods wherein episodic memory is used to optimize the hyperparameters of reinforcement learning algorithms on-the-fly. Unlike other hyperparameter searches, we formulate hyperparameter scheduling as a standard Markov Decision Process and use episodic memory to store the outcome of used hyperparameters and their training contexts. At any policy update step, the policy learner refers to the stored experiences, and adaptively reconfigures its learning algorithm with the new hyperparameters determined by the memory. This mechanism, dubbed as Episodic Policy Gradient Training (EPGT), enables an episodic learning process, and jointly learns the policy and the learning algorithm's hyperparameters within a single run. Experimental results on both continuous and discrete environments demonstrate the advantage of using the proposed method in boosting the performance of various policy gradient algorithms.",
    "code_link": "https://github.com/thaihungle/EPGT"
  },
  "aaai2022_main_stabilityverificationinstochasticcontrolsystemsvianeuralnetworksupermartingales": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stability Verification in Stochastic Control Systems via Neural Network Supermartingales",
    "authors": [
      "Mathias Lechner",
      "\u0110or\u0111e \u017dikeli\u0107",
      "Krishnendu Chatterjee",
      "Thomas A. Henzinger"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20695",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20695/20454",
    "published": "2022-02",
    "summary": "We consider the problem of formally verifying almost-sure (a.s.) asymptotic stability in discrete-time nonlinear stochastic control systems. While verifying stability in deterministic control systems is extensively studied in the literature, verifying stability in stochastic control systems is an open problem. The few existing works on this topic either consider only specialized forms of stochasticity or make restrictive assumptions on the system, rendering them inapplicable to learning algorithms with neural network policies. In this work, we present an approach for general nonlinear stochastic control problems with two novel aspects: (a) instead of classical stochastic extensions of Lyapunov functions, we use ranking supermartingales (RSMs) to certify a.s. asymptotic stability, and (b) we present a method for learning neural network RSMs. We prove that our approach guarantees a.s. asymptotic stability of the system andprovides the first method to obtain bounds on the stabilization time, which stochastic Lyapunov functions do not.Finally, we validate our approach experimentally on a set of nonlinear stochastic reinforcement learning environments with neural network policies.",
    "code_link": ""
  },
  "aaai2022_main_learninglossesforstrategicclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Losses for Strategic Classification",
    "authors": [
      "Tosca Lechner",
      "Ruth Urner"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20696",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20696/20455",
    "published": "2022-02",
    "summary": "Strategic classification, i.e. classification under possible strategic manipulations of features, has received a lot of attention from both the machine learning and the game theory community. Most works focus on analysing properties of the optimal decision rule under such manipulations. In our work we take a learning theoretic perspective, focusing on the sample complexity needed to learn a good decision rule which is robust to strategic manipulation. We perform this analysis by introducing a novel loss function, the strategic manipulation loss, which takes into account both the accuracy of the final decision rule and its vulnerability to manipulation. We analyse the sample complexity for a known graph of possible manipulations in terms of the complexity of the function class and the manipulation graph. Additionally, we initialize the study of learning under unknown manipulation capabilities of the involved agents. Using techniques from transfer learning theory, we define a similarity measure for manipulation graphs and show that learning outcomes are robust with respect to small changes in the manipulation graph. Lastly, we analyse the (sample complexity of) learning of the manipulation capability of agents with respect to this similarity measure, providing novel guarantees for strategic classification with respect to an unknown manipulation graph.",
    "code_link": ""
  },
  "aaai2022_main_differentiallyprivatenormalizingflowsforsynthetictabulardatageneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Differentially Private Normalizing Flows for Synthetic Tabular Data Generation",
    "authors": [
      "Jaewoo Lee",
      "Minjung Kim",
      "Yonghyun Jeong",
      "Youngmin Ro"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20697",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20697/20456",
    "published": "2022-02",
    "summary": "Normalizing flows have shown to be a promising approach to deep generative modeling due to their ability to exactly evaluate density --- other alternatives either implicitly model the density or use approximate surrogate density. In this work, we present a differentially private normalizing flow model for heterogeneous tabular data. Normalizing flows are in general not amenable to differentially private training because they require complex neural networks with larger depth (compared to other generative models) and use specialized architectures for which per-example gradient computation is difficult (or unknown). To reduce the parameter complexity, the proposed model introduces a conditional spline flow which simulates transformations at different stages depending on additional input and is shared among sub-flows. For privacy, we introduce two fine-grained gradient clipping strategies that provide a better signal-to-noise ratio and derive fast gradient clipping methods for layers with custom parameterization. Our empirical evaluations show that the proposed model preserves statistical properties of original dataset better than other baselines.",
    "code_link": ""
  },
  "aaai2022_main_multi-headmodularizationtoleveragegeneralizationcapabilityinmulti-modalnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks",
    "authors": [
      "Jun-Tae Lee",
      "Hyunsin Park",
      "Sungrack Yun",
      "Simyung Chang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20698",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20698/20457",
    "published": "2022-02",
    "summary": "It has been crucial to leverage the rich information of multiple modalities in many tasks. Existing works have tried to design multi-modal networks with descent multi-modal fusion modules. Instead, we focus on improving generalization capability of multi-modal networks, especially the fusion module. Viewing the multi-modal data as different projections of information, we first observe that bad projection can cause poor generalization behaviors of multi-modal networks. Then, motivated by well-generalized network's low sensitivity to perturbation, we propose a novel multi-modal training method, multi-head modularization (MHM). We modularize a multi-modal network as a series of uni-modal embedding, multi-modal embedding, and task-specific head modules. Also, for training, we exploit multiple head modules learned with different datasets, swapping each other. From this, we can make the multi-modal embedding module robust to all the heads with different generalization behaviors. In testing phase, we select one of the head modules not to increase the computational cost. Owing to the perturbation of head modules, though including one selected head, the deployed network is more well-generalized compared to the simply end-to-end learned. We verify the effectiveness of MHM on various multi-modal tasks. We use the state-of-the-art methods as baselines, and show notable performance gain for all the baselines.",
    "code_link": ""
  },
  "aaai2022_main_fastandefficientmmd-basedfairpcaviaoptimizationoverstiefelmanifold": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast and Efficient MMD-Based Fair PCA via Optimization over Stiefel Manifold",
    "authors": [
      "Junghyun Lee",
      "Gwangsu Kim",
      "Mahbod Olfat",
      "Mark Hasegawa-Johnson",
      "Chang D.\n      Yoo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20699",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20699/20458",
    "published": "2022-02",
    "summary": "This paper defines fair principal component analysis (PCA) as minimizing the maximum mean discrepancy (MMD) between the dimensionality-reduced conditional distributions of different protected classes. The incorporation of MMD naturally leads to an exact and tractable mathematical formulation of fairness with good statistical properties. We formulate the problem of fair PCA subject to MMD constraints as a non-convex optimization over the Stiefel manifold and solve it using the Riemannian Exact Penalty Method with Smoothing (REPMS). Importantly, we provide a local optimality guarantee and explicitly show the theoretical effect of each hyperparameter in practical settings, extending previous results. Experimental comparisons based on synthetic and UCI datasets show that our approach outperforms prior work in explained variance, fairness, and runtime.",
    "code_link": "https://github.com/nick-jhlee/fair-manifold-pca"
  },
  "aaai2022_main_augmentation-freeself-supervisedlearningongraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Augmentation-Free Self-Supervised Learning on Graphs",
    "authors": [
      "Namkyeong Lee",
      "Junseok Lee",
      "Chanyoung Park"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20700",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20700/20459",
    "published": "2022-02",
    "summary": "Inspired by the recent success of self-supervised methods applied on images, self-supervised learning on graph structured data has seen rapid growth especially centered on augmentation-based contrastive methods. However, we argue that without carefully designed augmentation techniques, augmentations on graphs may behave arbitrarily in that the underlying semantics of graphs can drastically change. As a consequence, the performance of existing augmentation-based methods is highly dependent on the choice of augmentation scheme, i.e., augmentation hyperparameters and combinations of augmentation. In this paper, we propose a novel augmentation-free self-supervised learning framework for graphs, named AFGRL. Specifically, we generate an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph. Extensive experiments towards various node-level tasks, i.e., node classification, clustering, and similarity search on various real-world datasets demonstrate the superiority of AFGRL. The source code for AFGRL is available at https://github.com/Namkyeong/AFGRL.",
    "code_link": ""
  },
  "aaai2022_main_fastandrobustonlineinferencewithstochasticgradientdescentviarandomscaling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling",
    "authors": [
      "Sokbae Lee",
      "Yuan Liao",
      "Myung Hwan Seo",
      "Youngki Shin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20701",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20701/20460",
    "published": "2022-02",
    "summary": "We develop a new method of online inference for a vector of parameters estimated by the Polyak-Ruppert averaging procedure of stochastic gradient descent (SGD) algorithms. We leverage insights from time series regression in econometrics and construct asymptotically pivotal statistics via random scaling. Our approach is fully operational with online data and is rigorously underpinned by a functional central limit theorem. Our proposed inference method has a couple of key advantages over the existing methods. First, the test statistic is computed in an online fashion with only SGD iterates and the critical values can be obtained without any resampling methods, thereby allowing for efficient implementation suitable for massive online data. Second, there is no need to estimate the asymptotic variance and our inference method is shown to be robust to changes in the tuning parameters for SGD algorithms in simulation experiments with synthetic data.",
    "code_link": ""
  },
  "aaai2022_main_diverse,globalandamortisedcounterfactualexplanationsforuncertaintyestimates": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates",
    "authors": [
      "Dan Ley",
      "Umang Bhatt",
      "Adrian Weller"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20702",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20702/20461",
    "published": "2022-02",
    "summary": "To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating a single Counterfactual Latent Uncertainty Explanation (CLUE) for a given data point where the model is uncertain. We broaden the exploration to examine \u03b4-CLUE, the set of potential CLUEs within a \u03b4 ball of the original input in latent space. We study the diversity of such sets and find that many CLUEs are redundant; as such, we propose DIVerse CLUE (\u2207-CLUE), a set of CLUEs which each propose a distinct explanation as to how one can decrease the uncertainty associated with an input. We then further propose GLobal AMortised CLUE (GLAM-CLUE), a distinct, novel method which learns amortised mappings that apply to specific groups of uncertain inputs, taking them and efficiently transforming them in a single function call into inputs for which a model will be certain. Our experiments show that \u03b4-CLUE, \u2207-CLUE, and GLAM-CLUE all address shortcomings of CLUE and provide beneficial explanations of uncertainty estimates to practitioners.",
    "code_link": ""
  },
  "aaai2022_main_invariantinformationbottleneckfordomaingeneralization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Invariant Information Bottleneck for Domain Generalization",
    "authors": [
      "Bo Li",
      "Yifei Shen",
      "Yezhen Wang",
      "Wenzhen Zhu",
      "Colorado Reed",
      "Dongsheng Li",
      "Kurt Keutzer",
      "Han Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20703",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20703/20462",
    "published": "2022-02",
    "summary": "Invariant risk minimization (IRM) has recently emerged as a promising alternative for domain generalization. Nevertheless, the loss function is difficult to optimize for nonlinear classifiers and the original optimization objective could fail when pseudo-invariant features and geometric skews exist. Inspired by IRM, in this paper we propose a novel formulation for domain generalization, dubbed invariant information bottleneck (IIB). IIB aims at minimizing invariant risks for nonlinear classifiers and simultaneously mitigating the impact of pseudo-invariant features and geometric skews. Specifically, we first present a novel formulation for invariant causal prediction via mutual information. Then we adopt the variational formulation of the mutual information to develop a tractable loss function for nonlinear classifiers. To overcome the failure modes of IRM, we propose to minimize the mutual information between the inputs and the corresponding representations. IIB significantly outperforms IRM on synthetic datasets, where the pseudo-invariant features and geometric skews occur, showing the effectiveness of proposed formulation in overcoming failure modes of IRM. Furthermore, experiments on DomainBed show that IIB outperforms 13 baselines by 0.9% on average across 7 real datasets.",
    "code_link": ""
  },
  "aaai2022_main_chunkdynamicupdatingforgrouplassowithodes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Chunk Dynamic Updating for Group Lasso with ODEs",
    "authors": [
      "Diyang Li",
      "Bin Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20704",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20704/20463",
    "published": "2022-02",
    "summary": "Group Lasso is an important sparse regression method in machine learning which encourages selecting key explanatory factors in a grouped manner because of the use of L-2,1 norm. In real-world learning tasks, some chunks of data would be added into or removed from the training set in sequence due to the existence of new or obsolete historical data, which is normally called dynamic or lifelong learning scenario. However, most of existing algorithms of group Lasso are limited to offline updating, and only one is online algorithm which can only handle newly added samples inexactly. Due to the complexity of L-2,1 norm, how to achieve accurate chunk incremental and decremental learning efficiently for group Lasso is still an open question. To address this challenging problem, in this paper, we propose a novel accurate dynamic updating algorithm for group Lasso by utilizing the technique of Ordinary Differential Equations (ODEs), which can incorporate or eliminate a chunk of samples from original training set without retraining the model from scratch. Specifically, we introduce a new formulation to reparameterize the adjustment procedures of chunk incremental and decremental learning simultaneously. Based on the new formulation, we propose a path following algorithm for group Lasso regarding to the adjustment parameter. Importantly, we prove that our path following algorithm can exactly track the piecewise smooth solutions thanks to the technique of ODEs, so that the accurate chunk incremental and decremental learning can be achieved. Extensive experimental results not only confirm the effectiveness of proposed algorithm for the chunk incremental and decremental learning, but also validate its efficiency compared to the existing offline and online algorithms.",
    "code_link": "https://github.com/yngvem/group-lasso"
  },
  "aaai2022_main_policylearningforrobustmarkovdecisionprocesswithamismatchedgenerativemodel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Policy Learning for Robust Markov Decision Process with a Mismatched Generative Model",
    "authors": [
      "Jialian Li",
      "Tongzheng Ren",
      "Dong Yan",
      "Hang Su",
      "Jun Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20705",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20705/20464",
    "published": "2022-02",
    "summary": "In high-stake scenarios like medical treatment and auto-piloting, it's risky or even infeasible to collect online experimental data to train the agent. Simulation-based training can alleviate this issue, but may suffer from its inherent mismatches from the simulator and real environment. It is therefore imperative to utilize the simulator to learn a robust policy for the real-world deployment. In this work, we consider policy learning for Robust Markov Decision Processes (RMDP), where the agent tries to seek a robust policy with respect to unexpected perturbations on the environments. Specifically, we focus on the setting where the training environment can be characterized as a generative model and a constrained perturbation can be added to the model during testing. Our goal is to identify a near-optimal robust policy for the perturbed testing environment, which introduces additional technical difficulties as we need to simultaneously estimate the training environment uncertainty from samples and find the worst-case perturbation for testing. To solve this issue, we propose a generic method which formalizes the perturbation as an opponent to obtain a two-player zero-sum game, and further show that the Nash Equilibrium corresponds to the robust policy. We prove that, with a polynomial number of samples from the generative model, our algorithm can find a near-optimal robust policy with a high probability. Our method is able to deal with general perturbations under some mild assumptions and can also be extended to more complex problems like robust partial observable Markov decision process, thanks to the game-theoretical formulation.",
    "code_link": ""
  },
  "aaai2022_main_afullysingleloopalgorithmforbileveloptimizationwithouthessianinverse": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Fully Single Loop Algorithm for Bilevel Optimization without Hessian Inverse",
    "authors": [
      "Junyi Li",
      "Bin Gu",
      "Heng Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20706",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20706/20465",
    "published": "2022-02",
    "summary": "In this paper, we propose a novel Hessian inverse free Fully Single Loop Algorithm (FSLA) for bilevel optimization problems. Classic algorithms for bilevel optimization admit a double loop structure which is computationally expensive. Recently, several single loop algorithms have been proposed with optimizing the inner and outer variable alternatively. However, these algorithms not yet achieve fully single loop. As they overlook the loop needed to evaluate the hyper-gradient for a given inner and outer state. In order to develop a fully single loop algorithm, we first study the structure of the hyper-gradient and identify a general approximation formulation of hyper-gradient computation that encompasses several previous common approaches, e.g. back-propagation through time, conjugate gradient, etc. Based on this formulation, we introduce a new state variable to maintain the historical hyper-gradient information. Combining our new formulation with the alternative update of the inner and outer variables, we propose an efficient fully single loop algorithm. We theoretically show that the error generated by the new state can be bounded and our algorithm converges. Finally, we verify the efficacy our algorithm empirically through multiple bilevel optimization based machine learning tasks. A long version of this paper can be found in: https://arxiv.org/abs/2112.04660.",
    "code_link": ""
  },
  "aaai2022_main_ahybridcausalstructurelearningalgorithmformixed-typedata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Hybrid Causal Structure Learning Algorithm for Mixed-Type Data",
    "authors": [
      "Yan Li",
      "Rui Xia",
      "Chunchen Liu",
      "Liang Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20707",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20707/20466",
    "published": "2022-02",
    "summary": "Inferring the causal structure of a set of random variables is a crucial problem in many disciplines of science. Over the past two decades, various approaches have been pro- posed for causal discovery from observational data. How- ever, most of the existing methods are designed for either purely discrete or continuous data, which limit their practical usage. In this paper, we target the problem of causal structure learning from observational mixed-type data. Although there are a few methods that are able to handle mixed-type data, they suffer from restrictions, such as linear assumption and poor scalability. To overcome these weaknesses, we formulate the causal mechanisms via mixed structure equation model and prove its identifiability under mild conditions. A novel locally consistent score, named CVMIC, is proposed for causal directed acyclic graph (DAG) structure learning. Moreover, we propose an efficient conditional independence test, named MRCIT, for mixed-type data, which is used in causal skeleton learning and final pruning to further improve the computational efficiency and precision of our model. Experimental results on both synthetic and real-world data demonstrate that our proposed hybrid model outperforms the other state-of-the-art methods. Our source code is available at https://github.com/DAMO-DI-ML/AAAI2022-HCM.",
    "code_link": "https://github.com/DAMO-DI-ML/AAAI2022-HCM"
  },
  "aaai2022_main_sharpanalysisofrandomfourierfeaturesinclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sharp Analysis of Random Fourier Features in Classification",
    "authors": [
      "Zhu Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20708",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20708/20467",
    "published": "2022-02",
    "summary": "We study the theoretical properties of random Fourier features classification with Lipschitz continuous loss functions such as support vector machine and logistic regression. Utilizing the regularity condition, we show for the first time that random Fourier features classification can achieve O(1/n^0.5) learning rate with only O(n^0.5) features, as opposed to O(n) features suggested by previous results. Our study covers the standard feature sampling method for which we reduce the number of features required, as well as a problem-dependent sampling method which further reduces the number of features while still keeping the optimal generalization property. Moreover, we prove that the random Fourier features classification can obtain a fast O(1/n) learning rate for both sampling schemes under Massart's low noise assumption. Our results demonstrate the potential effectiveness of random Fourier features approximation in reducing the computational complexity (roughly from O(n^3) in time and O(n^2) in space to O(n^2) and O(n^1.5) respectively) without having to trade-off the statistical prediction accuracy. In addition, the achieved trade-off in our analysis is at least the same as the optimal results in the literature under the worst case scenario and significantly improves the optimal results under benign regularity conditions.",
    "code_link": ""
  },
  "aaai2022_main_zeroth-orderoptimizationforcompositeproblemswithfunctionalconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Zeroth-Order Optimization for Composite Problems with Functional Constraints",
    "authors": [
      "Zichong Li",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Songtao Lu",
      "Yangyang Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20709",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20709/20468",
    "published": "2022-02",
    "summary": "In many real-world problems, first-order (FO) derivative evaluations are too expensive or even inaccessible. For solving these problems, zeroth-order (ZO) methods that only need function evaluations are often more efficient than FO methods or sometimes the only options. In this paper, we propose a novel zeroth-order inexact augmented Lagrangian method (ZO-iALM) to solve black-box optimization problems, which involve a composite (i.e., smooth+nonsmooth) objective and functional constraints. This appears to be the first work that develops an iALM-based ZO method for functional constrained optimization and meanwhile achieves query complexity results matching the best-known FO complexity results up to a factor of variable dimension. With an extensive experimental study, we show the effectiveness of our method. The applications of our method span from classical optimization problems to practical machine learning examples such as resource allocation in sensor networks and adversarial example generation.",
    "code_link": ""
  },
  "aaai2022_main_robustgraph-basedmulti-viewclustering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Graph-Based Multi-View Clustering",
    "authors": [
      "Weixuan Liang",
      "Xinwang Liu",
      "Sihang Zhou",
      "Jiyuan Liu",
      "Siwei Wang",
      "En Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20710",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20710/20469",
    "published": "2022-02",
    "summary": "Graph-based multi-view clustering (G-MVC) constructs a graphical representation of each view and then fuses them to a unified graph for clustering. Though demonstrating promising clustering performance in various applications, we observe that their formulations are usually non-convex, leading to a local optimum. In this paper, we propose a novel MVC algorithm termed robust graph-based multi-view clustering (RG-MVC) to address this issue. In particular, we define a min-max formulation for robust learning and then rewrite it as a convex and differentiable objective function whose convexity and differentiability are carefully proved. Thus, we can efficiently solve the resultant problem using a reduced gradient descent algorithm, and the corresponding solution is guaranteed to be globally optimal. As a consequence, although our algorithm is free of hyper-parameters, it has shown good robustness against noisy views. Extensive experiments on benchmark datasets verify the superiority of the proposed method against the compared state-of-the-art algorithms. Our codes and appendix are available at https://github.com/wx-liang/RG-MVC.",
    "code_link": "https://github.com/wxliang/RG-MVC"
  },
  "aaai2022_main_conditionallocalconvolutionforspatio-temporalmeteorologicalforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Conditional Local Convolution for Spatio-Temporal Meteorological Forecasting",
    "authors": [
      "Haitao Lin",
      "Zhangyang Gao",
      "Yongjie Xu",
      "Lirong Wu",
      "Ling Li",
      "Stan Z. Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20711",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20711/20470",
    "published": "2022-02",
    "summary": "Spatio-temporal forecasting is challenging attributing to the high nonlinearity in temporal dynamics as well as complex location-characterized patterns in spatial domains, especially in fields like weather forecasting. Graph convolutions are usually used for modeling the spatial dependency in meteorology to handle the irregular distribution of sensors' spatial location.In this work, a novel graph-based convolution for imitating the meteorological flows is proposed to capture the local spatial patterns. Based on the assumption of smoothness of location-characterized patterns, we propose conditional local convolution whose shared kernel on nodes' local space is approximated by feedforward networks, with local representations of coordinate obtained by horizon maps into cylindrical-tangent space as its input. The established united standard of local coordinate system preserves the orientation on geography. We further propose the distance and orientation scaling terms to reduce the impacts of irregular spatial distribution. The convolution is embedded in a Recurrent Neural Network architecture to model the temporal dynamics, leading to the Conditional Local Convolution Recurrent Network (CLCRN). Our model is evaluated on real-world weather benchmark datasets, achieving state-of-the-art performance with obvious improvements. We conduct further analysis on local pattern visualization, model's framework choice, advantages of horizon maps and etc. The source code is available at https://github.com/BIRD-TAO/CLCRN.",
    "code_link": "https://github.com/BIRD-TAO/CLCRN"
  },
  "aaai2022_main_ontheuseofunrealisticpredictionsinhundredsofpapersevaluatinggraphrepresentations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Use of Unrealistic Predictions in Hundreds of Papers Evaluating Graph Representations",
    "authors": [
      "Li-Chung Lin",
      "Cheng-Hung Liu",
      "Chih-Ming Chen",
      "Kai-Chin Hsu",
      "I-Feng Wu",
      "Ming-Feng Tsai",
      "Chih-Jen Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20712",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20712/20471",
    "published": "2022-02",
    "summary": "Prediction using the ground truth sounds like an oxymoron in machine learning. However, such an unrealistic setting was used in hundreds, if not thousands of papers in the area of finding graph representations. To evaluate the multi-label problem of node classification by using the obtained representations, many works assume that the number of labels of each test instance is known in the prediction stage. In practice such ground truth information is rarely available, but we point out that such an inappropriate setting is now ubiquitous in this research area. We detailedly investigate why the situation occurs. Our analysis indicates that with unrealistic information, the performance is likely over-estimated. To see why suitable predictions were not used, we identify difficulties in applying some multi-label techniques. For the use in future studies, we propose simple and effective settings without using practically unknown information. Finally, we take this chance to compare major graph representation learning methods on multi-label node classification.",
    "code_link": "https://github.com/ASUS-AICS/LibMultiLabel"
  },
  "aaai2022_main_deepunsupervisedhashingwithlatentsemanticcomponents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Unsupervised Hashing with Latent Semantic Components",
    "authors": [
      "Qinghong Lin",
      "Xiaojun Chen",
      "Qin Zhang",
      "Shaotian Cai",
      "Wenzhe Zhao",
      "Hongfa\n      Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20713",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20713/20472",
    "published": "2022-02",
    "summary": "Deep unsupervised hashing has been appreciated in the regime of image retrieval.However, most prior arts failed to detect the semantic components and their relationships behind the images, which makes them lack discriminative power. To make up the defect, we propose a novel Deep Semantic Components Hashing (DSCH), which involves a common sense that an image normally contains a bunch of semantic components with homology and co-occurrence relationships. Based on this prior, DSCH regards the semantic components as latent variables under the Expectation-Maximization framework and designs a two-step iterative algorithm with the objective of maximum likelihood of training data. Firstly, DSCH constructs a semantic component structure by uncovering the fine-grained semantics components of images with a Gaussian Mixture Modal~(GMM), where an image is represented as a mixture of multiple components, and the semantics co-occurrence are exploited. Besides, coarse-grained semantics components, are discovered by considering the homology relationships between fine-grained components, and the hierarchy organization is then constructed. Secondly, DSCH makes the images close to their semantic component centers at both fine-grained and coarse-grained levels, and also makes the images share similar semantic components close to each other. Extensive experiments on three benchmark datasets demonstrate that the proposed hierarchical semantic components indeed facilitate the hashing model to achieve superior performance.",
    "code_link": ""
  },
  "aaai2022_main_scribset-classifierwithclass-specificriskboundsforblackboxmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SCRIB: Set-Classifier with Class-Specific Risk Bounds for Blackbox Models",
    "authors": [
      "Zhen Lin",
      "Lucas Glass",
      "M. Brandon Westover",
      "Cao Xiao",
      "Jimeng Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20714",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20714/20473",
    "published": "2022-02",
    "summary": "Despite deep learning (DL) success in classification problems, DL classifiers do not provide a sound mechanism to decide when to refrain from predicting. Recent works tried to control the overall prediction risk with classification with rejection options. However, existing works overlook the different significance of different classes. We introduce Set-classifier with class-specific RIsk Bounds (SCRIB) to tackle this problem, assigning multiple labels to each example. Given the output of a black-box model on the validation set, SCRIB constructs a set-classifier that controls the class-specific prediction risks. The key idea is to reject when the set classifier returns more than one label. We validated SCRIB on several medical applications, including sleep staging on electroencephalogram(EEG) data, X-ray COVID image classification, and atrial fibrillation detection based on electrocardiogram (ECG) data.SCRIB obtained desirable class-specific risks, which are 35%-88% closer to the target risks than baseline methods.",
    "code_link": "https://github.com/ieee8023/covid-chestxray-dataset"
  },
  "aaai2022_main_raregangeneratingsamplesforrareclasses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "RareGAN: Generating Samples for Rare Classes",
    "authors": [
      "Zinan Lin",
      "Hao Liang",
      "Giulia Fanti",
      "Vyas Sekar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20715",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20715/20474",
    "published": "2022-02",
    "summary": "We study the problem of learning generative adversarial networks (GANs) for a rare class of an unlabeled dataset subject to a labeling budget. This problem is motivated from practical applications in domains including security (e.g., synthesizing packets for DNS amplification attacks), systems and networking (e.g., synthesizing workloads that trigger high resource usage), and machine learning (e.g., generating images from a rare class). Existing approaches are unsuitable, either requiring fully-labeled datasets or sacrificing the fidelity of the rare class for that of the common classes. We propose RareGAN, a novel synthesis of three key ideas: (1) extending conditional GANs to use labelled and unlabelled data for better generalization; (2) an active learning approach that requests the most useful labels; and (3) a weighted loss function to favor learning the rare class. We show that RareGAN achieves a better fidelity-diversity tradeoff on the rare class than prior work across different applications, budgets, rare class fractions, GAN losses, and architectures.",
    "code_link": ""
  },
  "aaai2022_main_conjugateddiscretedistributionsfordistributionalreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Conjugated Discrete Distributions for Distributional Reinforcement Learning",
    "authors": [
      "Bj\u00f6rn Lindenberg",
      "Jonas Nordqvist",
      "Karl-Olof Lindahl"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20716",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20716/20475",
    "published": "2022-02",
    "summary": "In this work we continue to build upon recent advances in reinforcement learning for finite Markov processes. A common approach among previous existing algorithms, both single-actor and distributed, is to either clip rewards or to apply a transformation method on Q-functions to handle a large variety of magnitudes in real discounted returns. We theoretically show that one of the most successful methods may not yield an optimal policy if we have a non-deterministic process. As a solution, we argue that distributional reinforcement learning lends itself to remedy this situation completely. By the introduction of a conjugated distributional operator we may handle a large class of transformations for real returns with guaranteed theoretical convergence. We propose an approximating single-actor algorithm based on this operator that trains agents directly on unaltered rewards using a proper distributional metric given by the Cram\u00e9r distance. To evaluate its performance in a stochastic setting we train agents on a suite of 55 Atari 2600 games using sticky-actions and obtain state-of-the-art performance compared to other well-known algorithms in the Dopamine framework.",
    "code_link": "https://github.com/bjliaa/c2d"
  },
  "aaai2022_main_lifelonghyper-policyoptimizationwithmultipleimportancesamplingregularization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Lifelong Hyper-Policy Optimization with Multiple Importance Sampling Regularization",
    "authors": [
      "Pierre Liotet",
      "Francesco Vidaich",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20717",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20717/20476",
    "published": "2022-02",
    "summary": "Learning in a lifelong setting, where the dynamics continually evolve, is a hard challenge for current reinforcement learning algorithms. Yet this would be a much needed feature for practical applications. In this paper, we propose an approach which learns a hyper-policy, whose input is time, that outputs the parameters of the policy to be queried at that time. This hyper-policy is trained to maximize the estimated future performance, efficiently reusing past data by means of importance sampling, at the cost of introducing a controlled bias. We combine the future performance estimate with the past performance to mitigate catastrophic forgetting.To avoid overfitting the collected data, we derive a differentiable variance bound that we embed as a penalization term. Finally, we empirically validate our approach, in comparison with state-of-the-art algorithms, on realistic environments, including water resource management and trading.",
    "code_link": "https://github.com/pierresdr/polis"
  },
  "aaai2022_main_learningparameterizedtaskstructureforgeneralizationtounseenentities": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Parameterized Task Structure for Generalization to Unseen Entities",
    "authors": [
      "Anthony Liu",
      "Sungryull Sohn",
      "Mahdi Qazwini",
      "Honglak Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20718",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20718/20477",
    "published": "2022-02",
    "summary": "Real world tasks are hierarchical and compositional. Tasks can be composed of multiple subtasks (or sub-goals) that are dependent on each other. These subtasks are defined in terms of entities (e.g., \"apple\", \"pear\") that can be recombined to form new subtasks (e.g., \"pickup apple\", and \"pickup pear\"). To solve these tasks efficiently, an agent must infer subtask dependencies (e.g. an agent must execute \"pickup apple\" before \"place apple in pot\"), and generalize the inferred dependencies to new subtasks (e.g. \"place apple in pot\" is similar to \"place apple in pan\"). Moreover, an agent may also need to solve unseen tasks, which can involve unseen entities. To this end, we formulate parameterized subtask graph inference (PSGI), a method for modeling subtask dependencies using first-order logic with factored entities. To facilitate this, we learn parameter attributes in a zero-shot manner, which are used as quantifiers (e.g. is_pickable(X)) for the factored subtask graph. We show this approach accurately learns the latent structure on hierarchical and compositional tasks more efficiently than prior work, and show PSGI can generalize by modelling structure on subtasks unseen during adaptation.",
    "code_link": "https://github.com/anthliu/PSGI"
  },
  "aaai2022_main_stationarydiffusionstateneuralestimationformultiviewclustering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stationary Diffusion State Neural Estimation for Multiview Clustering",
    "authors": [
      "Chenghua Liu",
      "Zhuolin Liao",
      "Yixuan Ma",
      "Kun Zhan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20719",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20719/20478",
    "published": "2022-02",
    "summary": "Although many graph-based clustering methods attempt to model the stationary diffusion state in their objectives, their performance limits to using a predefined graph. We argue that the estimation of the stationary diffusion state can be achieved by gradient descent over neural networks. We specifically design the Stationary Diffusion State Neural Estimation (SDSNE) to exploit multiview structural graph information for co-supervised learning. We explore how to design a graph neural network specially for unsupervised multiview learning and integrate multiple graphs into a unified consensus graph by a shared self-attentional module. The view-shared self-attentional module utilizes the graph structure to learn a view-consistent global graph. Meanwhile, instead of using auto-encoder in most unsupervised learning graph neural networks, SDSNE uses a co-supervised strategy with structure information to supervise the model learning. The co-supervised strategy as the loss function guides SDSNE in achieving the stationary state. With the help of the loss and the self-attentional module, we learn to obtain a graph in which nodes in each connected component fully connect by the same weight. Experiments on several multiview datasets demonstrate effectiveness of SDSNE in terms of six clustering evaluation metrics.",
    "code_link": ""
  },
  "aaai2022_main_deepamortizedrelationalmodelwithgroup-wisehierarchicalgenerativeprocess": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Amortized Relational Model with Group-Wise Hierarchical Generative Process",
    "authors": [
      "Huafeng Liu",
      "Tong Zhou",
      "Jiaqi Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20720",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20720/20479",
    "published": "2022-02",
    "summary": "In this paper, we propose Deep amortized Relational Model (DaRM) with group-wise hierarchical generative process for community discovery and link prediction on relational data (e.g., graph, network). It provides an efficient neural relational model architecture by grouping nodes in a group-wise view rather than node-wise or edge-wise view. DaRM simultaneously learns what makes a group, how to divide nodes into groups, and how to adaptively control the number of groups. The dedicated group generative process is able to sufficiently exploit pair-wise or higher-order interactions between data points in both inter-group and intra-group, which is useful to sufficiently mine the hidden structure among data. A series of experiments have been conducted on both synthetic and real-world datasets. The experimental results demonstrated that DaRM can obtain high performance on both community detection and link prediction tasks.",
    "code_link": ""
  },
  "aaai2022_main_learngoal-conditionedpolicywithintrinsicmotivationfordeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning",
    "authors": [
      "Jinxin Liu",
      "Donglin Wang",
      "Qiangxing Tian",
      "Zhengyu Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20721",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20721/20480",
    "published": "2022-02",
    "summary": "It is of significance for an agent to autonomously explore the environment and learn a widely applicable and general-purpose goal-conditioned policy that can achieve diverse goals including images and text descriptions. Considering such perceptually-specific goals, one natural approach is to reward the agent with a prior non-parametric distance over the embedding spaces of states and goals. However, this may be infeasible in some situations, either because it is unclear how to choose suitable measurement, or because embedding (heterogeneous) goals and states is non-trivial. The key insight of this work is that we introduce a latent-conditioned policy to provide goals and intrinsic rewards for learning the goal-conditioned policy. As opposed to directly scoring current states with regards to goals, we obtain rewards by scoring current states with associated latent variables. We theoretically characterize the connection between our unsupervised objective and the multi-goal setting, and empirically demonstrate the effectiveness of our proposed method which substantially outperforms prior techniques in a variety of tasks.",
    "code_link": ""
  },
  "aaai2022_main_transformerwithmemoryreplay": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transformer with Memory Replay",
    "authors": [
      "Rui Liu",
      "Barzan Mozafari"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20722",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20722/20481",
    "published": "2022-02",
    "summary": "Transformers achieve state-of-the-art performance for natural language processing tasks by pre-training on large-scale text corpora. They are extremely compute-intensive and have very high sample complexity. Memory replay is a mechanism that remembers and reuses past examples by saving to and replaying from a memory buffer. It has been successfully used in reinforcement learning and GANs due to better sample efficiency. In this paper, we propose Transformer with Memory Replay, which integrates memory replay with transformer, making transformer more sample efficient. Experiments on GLUE and SQuAD benchmark datasets showed that Transformer with Memory Replay can achieve at least 1% point increase compared to the baseline transformer model when pre-trained with the same number of examples. Further, by adopting a careful design that reduces the wall-clock time overhead of memory replay, we also empirically achieve a better runtime efficiency.",
    "code_link": ""
  },
  "aaai2022_main_efficientone-passmulti-viewsubspaceclusteringwithconsensusanchors": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient One-Pass Multi-View Subspace Clustering with Consensus Anchors",
    "authors": [
      "Suyuan Liu",
      "Siwei Wang",
      "Pei Zhang",
      "Kai Xu",
      "Xinwang Liu",
      "Changwang Zhang",
      "Feng Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20723",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20723/20482",
    "published": "2022-02",
    "summary": "Multi-view subspace clustering (MVSC) optimally integrates multiple graph structure information to improve clustering performance. Recently, many anchor-based variants are proposed to reduce the computational complexity of MVSC. Though achieving considerable acceleration, we observe that most of them adopt fixed anchor points separating from the subsequential anchor graph construction, which may adversely affect the clustering performance. In addition, post-processing is required to generate discrete clustering labels with additional time consumption. To address these issues, we propose a scalable and parameter-free MVSC method to directly output the clustering labels with optimal anchor graph, termed as Efficient One-pass Multi-view Subspace Clustering with Consensus Anchors (EOMSC-CA). Specially, we combine anchor learning and graph construction into a uniform framework to boost clustering performance. Meanwhile, by imposing a graph connectivity constraint, our algorithm directly outputs the clustering labels without any post-processing procedures as previous methods do. Our proposed EOMSC-CA is proven to be linear complexity respecting to the data size. The superiority of our EOMSC-CA over the effectiveness and efficiency is demonstrated by extensive experiments. Our code is publicly available at https://github.com/Tracesource/EOMSC-CA.",
    "code_link": "https://github.com/Tracesource/EOMSC-CA"
  },
  "aaai2022_main_trustedmulti-viewdeeplearningwithopinionaggregation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Trusted Multi-View Deep Learning with Opinion Aggregation",
    "authors": [
      "Wei Liu",
      "Xiaodong Yue",
      "Yufei Chen",
      "Thierry Denoeux"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20724",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20724/20483",
    "published": "2022-02",
    "summary": "Multi-view deep learning is performed based on the deep fusion of data from multiple sources, i.e. data with multiple views. However, due to the property differences and inconsistency of data sources, the deep learning results based on the fusion of multi-view data may be uncertain and unreliable. It is required to reduce the uncertainty in data fusion and implement the trusted multi-view deep learning. Aiming at the problem, we revisit the multi-view learning from the perspective of opinion aggregation and thereby devise a trusted multi-view deep learning method. Within this method, we adopt evidence theory to formulate the uncertainty of opinions as learning results from different data sources and measure the uncertainty of opinion aggregation as multi-view learning results through evidence accumulation. We prove that accumulating the evidences from multiple data views will decrease the uncertainty in multi-view deep learning and facilitate to achieve the trusted learning results. Experiments on various kinds of multi-view datasets verify the reliability and robustness of the proposed multi-view deep learning method.",
    "code_link": ""
  },
  "aaai2022_main_graphconvolutionalnetworkswithdualmessagepassingforsubgraphisomorphismcountingandmatching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph Convolutional Networks with Dual Message Passing for Subgraph Isomorphism Counting and Matching",
    "authors": [
      "Xin Liu",
      "Yangqiu Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20725",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20725/20484",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs) and message passing neural networks (MPNNs) have been proven to be expressive for subgraph structures in many applications. Some applications in heterogeneous graphs require explicit edge modeling, such as subgraph isomorphism counting and matching. However, existing message passing mechanisms are not designed well in theory. In this paper, we start from a particular edge-to-vertex transform and exploit the isomorphism property in the edge-to-vertex dual graphs. We prove that searching isomorphisms on the original graph is equivalent to searching on its dual graph. Based on this observation, we propose dual message passing neural networks (DMPNNs) to enhance the substructure representation learning in an asynchronous way for subgraph isomorphism counting and matching as well as unsupervised node classification. Extensive experiments demonstrate the robust performance of DMPNNs by combining both node and edge representation learning in synthetic and real heterogeneous graphs.",
    "code_link": ""
  },
  "aaai2022_main_deepgraphclusteringviadualcorrelationreduction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Graph Clustering via Dual Correlation Reduction",
    "authors": [
      "Yue Liu",
      "Wenxuan Tu",
      "Sihang Zhou",
      "Xinwang Liu",
      "Linxuan Song",
      "Xihong Yang",
      "En Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20726",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20726/20485",
    "published": "2022-02",
    "summary": "Deep graph clustering, which aims to reveal the underlying graph structure and divide the nodes into different groups, has attracted intensive attention in recent years. However, we observe that, in the process of node encoding, existing methods suffer from representation collapse which tends to map all data into the same representation. Consequently, the discriminative capability of the node representation is limited, leading to unsatisfied clustering performance. To address this issue, we propose a novel self-supervised deep graph clustering method termed Dual Correlation Reduction Network (DCRN) by reducing information correlation in a dual manner. Specifically, in our method, we first design a siamese network to encode samples. Then by forcing the cross-view sample correlation matrix and cross-view feature correlation matrix to approximate two identity matrices, respectively, we reduce the information correlation in the dual-level, thus improving the discriminative capability of the resulting features. Moreover, in order to alleviate representation collapse caused by over-smoothing in GCN, we introduce a propagation regularization term to enable the network to gain long-distance information with the shallow network structure. Extensive experimental results on six benchmark datasets demonstrate the effectiveness of the proposed DCRN against the existing state-of-the-art methods. The code of DCRN is available at https://github.com/yueliu1999/DCRN and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.",
    "code_link": ""
  },
  "aaai2022_main_optimisticinitializationforexplorationincontinuouscontrol": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimistic Initialization for Exploration in Continuous Control",
    "authors": [
      "Sam Lobel",
      "Omer Gottesman",
      "Cameron Allen",
      "Akhil Bagaria",
      "George Konidaris"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20727",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20727/20486",
    "published": "2022-02",
    "summary": "Optimistic initialization underpins many theoretically sound exploration schemes in tabular domains; however, in the deep function approximation setting, optimism can quickly disappear if initialized naively. We propose a framework for more effectively incorporating optimistic initialization into reinforcement learning for continuous control. Our approach uses metric information about the state-action space to estimate which transitions are still unexplored, and explicitly maintains the initial Q-value optimism for the corresponding state-action pairs. We also develop methods for efficiently approximating these training objectives, and for incorporating domain knowledge into the optimistic envelope to improve sample efficiency. We empirically evaluate these approaches on a variety of hard exploration problems in continuous control, where our method outperforms existing exploration techniques.",
    "code_link": ""
  },
  "aaai2022_main_fastanddataefficientreinforcementlearningfrompixelsvianon-parametricvalueapproximation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast and Data Efficient Reinforcement Learning from Pixels via Non-parametric Value Approximation",
    "authors": [
      "Alexander Long",
      "Alan Blair",
      "Herke van Hoof"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20728",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20728/20487",
    "published": "2022-02",
    "summary": "We present Nonparametric Approximation of Inter-Trace returns (NAIT), a Reinforcement Learning algorithm for discrete action, pixel-based environments that is both highly sample and computation efficient. NAIT is a lazy-learning approach with an update that is equivalent to episodic Monte-Carlo on episode completion, but that allows the stable incorporation of rewards while an episode is ongoing. We make use of a fixed domain-agnostic representation, simple distance based exploration and a proximity graph-based lookup to facilitate extremely fast execution. We empirically evaluate NAIT on both the 26 and 57 game variants of ATARI100k where, despite its simplicity, it achieves competitive performance in the online setting with greater than 100x speedup in wall-time.",
    "code_link": "https://github.com/nmslib/hnswlib"
  },
  "aaai2022_main_frozenpretrainedtransformersasuniversalcomputationengines": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Frozen Pretrained Transformers as Universal Computation Engines",
    "authors": [
      "Kevin Lu",
      "Aditya Grover",
      "Pieter Abbeel",
      "Igor Mordatch"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20729",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20729/20488",
    "published": "2022-02",
    "summary": "We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",
    "code_link": "https://github.com/kzl/universal-computation"
  },
  "aaai2022_main_adapttoenvironmentsuddenchangesbylearningacontextsensitivepolicy": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adapt to Environment Sudden Changes by Learning a Context Sensitive Policy",
    "authors": [
      "Fan-Ming Luo",
      "Shengyi Jiang",
      "Yang Yu",
      "ZongZhang Zhang",
      "Yi-Feng Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20730",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20730/20489",
    "published": "2022-02",
    "summary": "Dealing with real-world reinforcement learning (RL) tasks, we shall be aware that the environment may have sudden changes. We expect that a robust policy is able to handle such changes and adapt to the new environment rapidly.Context-based meta reinforcement learning aims at learning environment adaptable policies. These methods adopt a context encoder to perceive the environment on-the-fly, following which a contextual policy makes environment adaptive decisions according to the context. However, previous methods show lagged and unstable context extraction, which are hard to handle sudden changes well. This paper proposes an environment sensitive contextual policy learning (ESCP) approach, in order to improve both the sensitivity and the robustness of context encoding. ESCP is composed of three key components: variance minimization that forces a rapid and stable encoding of the environment context, relational matrix determinant maximization that avoids trivial solutions, and a history-truncated recurrent neural network model that avoids old memory interference.We use a grid-world task and 5 locomotion controlling tasks with changing parameters to empirically assess our algorithm. Experiment results show that in environments with both in-distribution and out-of-distribution parameter changes, ESCP can not only better recover the environment encoding, but also adapt more rapidly to the post-change environment (10x faster in the grid-world) while the return performance is kept or improved, compared with state-of-the-art meta RL methods.",
    "code_link": "https://github.com/FanmingL/ESCP"
  },
  "aaai2022_main_beyondsharedsubspaceaview-specificfusionformulti-viewmulti-labellearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Beyond Shared Subspace: A View-Specific Fusion for Multi-View Multi-Label Learning",
    "authors": [
      "Gengyu Lyu",
      "Xiang Deng",
      "Yanan Wu",
      "Songhe Feng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20731",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20731/20490",
    "published": "2022-02",
    "summary": "In multi-view multi-label learning (MVML), each instance is described by several heterogeneous feature representations and associated with multiple valid labels simultaneously. Although diverse MVML methods have been proposed over the last decade, most previous studies focus on leveraging the shared subspace across different views to represent the multi-view consensus information, while it is still an open issue whether such shared subspace representation is necessary when formulating the desired MVML model. In this paper, we propose a DeepGCN based View-Specific MVML method (D-VSM) which can bypass seeking for the shared subspace representation, and instead directly encoding the feature representation of each individual view through the deep GCN to couple with the information derived from the other views. Specifically, we first construct all instances under different feature representations into the corresponding feature graphs respectively, and then integrate them into a unified graph by integrating the different feature representations of each instance. Afterwards, the graph attention mechanism is adopted to aggregate and update all nodes on the unified graph to form structural representation for each instance, where both intra-view correlations and inter-view alignments have been jointly encoded to discover the underlying semantic relations. Finally, we derive a label confidence score for each instance by averaging the label confidence of its different feature representations with the multi-label soft margin loss. Extensive experiments have demonstrated that our proposed method significantly outperforms state-of-the-art methods.",
    "code_link": ""
  },
  "aaai2022_main_efficientcontinuouscontrolwithdoubleactorsandregularizedcritics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Continuous Control with Double Actors and Regularized Critics",
    "authors": [
      "Jiafei Lyu",
      "Xiaoteng Ma",
      "Jiangpeng Yan",
      "Xiu Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20732",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20732/20491",
    "published": "2022-02",
    "summary": "How to obtain good value estimation is a critical problem in Reinforcement Learning (RL). Current value estimation methods in continuous control, such as DDPG and TD3, suffer from unnecessary over- or under- estimation. In this paper, we explore the potential of double actors, which has been neglected for a long time, for better value estimation in the continuous setting. First, we interestingly find that double actors improve the exploration ability of the agent. Next, we uncover the bias alleviation property of double actors in handling overestimation with single critic, and underestimation with double critics respectively. Finally, to mitigate the potentially pessimistic value estimate in double critics, we propose to regularize the critics under double actors architecture. Together, we present Double Actors Regularized Critics (DARC) algorithm. Extensive experiments on challenging continuous control benchmarks, MuJoCo and PyBullet, show that DARC significantly outperforms current baselines with higher average return and better sample efficiency.",
    "code_link": ""
  },
  "aaai2022_main_recursivereasoninggraphformulti-agentreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Recursive Reasoning Graph for Multi-Agent Reinforcement Learning",
    "authors": [
      "Xiaobai Ma",
      "David Isele",
      "Jayesh K. Gupta",
      "Kikuo Fujimura",
      "Mykel J.\n      Kochenderfer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20733",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20733/20492",
    "published": "2022-02",
    "summary": "Multi-agent reinforcement learning (MARL) provides an efficient way for simultaneously learning policies for multiple agents interacting with each other. However, in scenarios requiring complex interactions, existing algorithms can suffer from an inability to accurately anticipate the influence of self-actions on other agents. Incorporating an ability to reason about other agents' potential responses can allow an agent to formulate more effective strategies. This paper adopts a recursive reasoning model in a centralized-training-decentralized-execution framework to help learning agents better cooperate with or compete against others. The proposed algorithm, referred to as the Recursive Reasoning Graph (R2G), shows state-of-the-art performance on multiple multi-agent particle and robotics games.",
    "code_link": ""
  },
  "aaai2022_main_sharprestrictedisometrypropertyboundsforlow-rankmatrixrecoveryproblemswithcorruptedmeasurements": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sharp Restricted Isometry Property Bounds for Low-Rank Matrix Recovery Problems with Corrupted Measurements",
    "authors": [
      "Ziye Ma",
      "Yingjie Bi",
      "Javad Lavaei",
      "Somayeh Sojoudi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20734",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20734/20493",
    "published": "2022-02",
    "summary": "In this paper, we study a general low-rank matrix recovery problem with linear measurements corrupted by some noise. The objective is to understand under what conditions on the restricted isometry property (RIP) of the problem local search methods can find the ground truth with a small error. By analyzing the landscape of the non-convex problem, we first propose a global guarantee on the maximum distance between an arbitrary local minimizer and the ground truth under the assumption that the RIP constant is smaller than 1/2. We show that this distance shrinks to zero as the intensity of the noise reduces. Our new guarantee is sharp in terms of the RIP constant and is much stronger than the existing results. We then present a local guarantee for problems with an arbitrary RIP constant, which states that any local minimizer is either considerably close to the ground truth or far away from it. Next, we prove the strict saddle property, which guarantees the global convergence of the perturbed gradient descent method in polynomial time. The developed results demonstrate how the noise intensity and the RIP constant of the problem affect the landscape of the problem.",
    "code_link": ""
  },
  "aaai2022_main_cross-lingualadversarialdomainadaptationfornoviceprogramming": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Lingual Adversarial Domain Adaptation for Novice Programming",
    "authors": [
      "Ye Mao",
      "Farzaneh Khoshnevisan",
      "Thomas Price",
      "Tiffany Barnes",
      "Min Chi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20735",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20735/20494",
    "published": "2022-02",
    "summary": "Student modeling sits at the epicenter of adaptive learning technology. In contrast to the voluminous work on student modeling for well-defined domains such as algebra, there has been little research on student modeling in programming (SMP) due to data scarcity caused by the unbounded solution spaces of open-ended programming exercises. In this work, we focus on two essential SMP tasks: program classification and early prediction of student success and propose a Cross-Lingual Adversarial Domain Adaptation (CrossLing) framework that can leverage a large programming dataset to learn features that can improve SMP's build using a much smaller dataset in a different programming language. Our framework maintains one globally invariant latent representation across both datasets via an adversarial learning process, as well as allocating domain-specific models for each dataset to extract local latent representations that cannot and should not be united. By separating globally-shared representations from domain-specific representations, our framework outperforms existing state-of-the-art methods for both SMP tasks.",
    "code_link": ""
  },
  "aaai2022_main_hardtoforgetpoisoningattacksoncertifiedmachineunlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hard to Forget: Poisoning Attacks on Certified Machine Unlearning",
    "authors": [
      "Neil G. Marchant",
      "Benjamin I. P. Rubinstein",
      "Scott Alfeld"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20736",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20736/20495",
    "published": "2022-02",
    "summary": "The right to erasure requires removal of a user's information from data held by organizations, with rigorous interpretations extending to downstream products such as learned models. Retraining from scratch with the particular user's data omitted fully removes its influence on the resulting model, but comes with a high computational cost. Machine \"unlearning\" mitigates the cost incurred by full retraining: instead, models are updated incrementally, possibly only requiring retraining when approximation errors accumulate. Rapid progress has been made towards privacy guarantees on the indistinguishability of unlearned and retrained models, but current formalisms do not place practical bounds on computation. In this paper we demonstrate how an attacker can exploit this oversight, highlighting a novel attack surface introduced by machine unlearning. We consider an attacker aiming to increase the computational cost of data removal. We derive and empirically investigate a poisoning attack on certified machine unlearning where strategically designed training data triggers complete retraining when removed.",
    "code_link": "https://github.com/ngmarchant/attackunlearning"
  },
  "aaai2022_main_exploringsaferbehaviorsfordeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploring Safer Behaviors for Deep Reinforcement Learning",
    "authors": [
      "Enrico Marchesini",
      "Davide Corsi",
      "Alessandro Farinelli"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20737",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20737/20496",
    "published": "2022-02",
    "summary": "We consider Reinforcement Learning (RL) problems where an agent attempts to maximize a reward signal while minimizing a cost function that models unsafe behaviors. Such formalization is addressed in the literature using constrained optimization on the cost, limiting the exploration and leading to a significant trade-off between cost and reward. In contrast, we propose a Safety-Oriented Search that complements Deep RL algorithms to bias the policy toward safety within an evolutionary cost optimization. We leverage evolutionary exploration benefits to design a novel concept of safe mutations that use visited unsafe states to explore safer actions. We further characterize the behaviors of the policies over desired specifics with a sample-based bound estimation, which makes prior verification analysis tractable in the training loop. Hence, driving the learning process towards safer regions of the policy space. Empirical evidence on the Safety Gym benchmark shows that we successfully avoid drawbacks on the return while improving the safety of the policy.",
    "code_link": ""
  },
  "aaai2022_main_fgotgraphdistancesbasedonfiltersandoptimaltransport": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "fGOT: Graph Distances Based on Filters and Optimal Transport",
    "authors": [
      "Hermina Petric Maretic",
      "Mireille El Gheche",
      "Giovanni Chierchia",
      "Pascal\n      Frossard"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20738",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20738/20497",
    "published": "2022-02",
    "summary": "Graph comparison deals with identifying similarities and dissimilarities between graphs. A major obstacle is the unknown alignment of graphs, as well as the lack of accurate and inexpensive comparison metrics. In this work we introduce the filter graph distance. It is an optimal transport based distance which drives graph comparison through the probability distribution of filtered graph signals. This creates a highly flexible distance, capable of prioritising different spectral information in observed graphs, offering a wide range of choices for a comparison metric. We tackle the problem of graph alignment by computing graph permutations that minimise our new filter distances, which implicitly solves the graph comparison problem. We then propose a new approximate cost function that circumvents many computational difficulties inherent to graph comparison and permits the exploitation of fast algorithms such as mirror gradient descent, without grossly sacrificing the performance. We finally propose a novel algorithm derived from a stochastic version of mirror gradient descent, which accommodates the non-convexity of the alignment problem, offering a good trade-off between performance accuracy and speed. The experiments on graph alignment and classification show that the flexibility gained through filter graph distances can have a significant impact on performance, while the difference in speed offered by the approximation cost makes the framework applicable in practical settings.",
    "code_link": "https://github.com/Hermina/fGOT"
  },
  "aaai2022_main_whenaidifficultyiseasytheexplanatorypowerofpredictingirtdifficulty": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "When AI Difficulty Is Easy: The Explanatory Power of Predicting IRT Difficulty",
    "authors": [
      "Fernando Mart\u00ednez-Plumed",
      "David Castellano",
      "Carlos Monserrat-Aranda",
      "Jos\u00e9\n      Hern\u00e1ndez-Orallo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20739",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20739/20498",
    "published": "2022-02",
    "summary": "One of challenges of artificial intelligence as a whole is robustness. Many issues such as adversarial examples, out of distribution performance, Clever Hans phenomena, and the wider areas of AI evaluation and explainable AI, have to do with the following question: Did the system fail because it is a hard instance or because something else? In this paper we address this question with a generic method for estimating IRT-based instance difficulty for a wide range of AI domains covering several areas, from supervised feature-based classification to automated reasoning. We show how to estimate difficulty systematically using off-the-shelf machine learning regression models. We illustrate the usefulness of this estimation for a range of applications.",
    "code_link": ""
  },
  "aaai2022_main_beingfriendsinsteadofadversariesdeepnetworkslearnfromdatasimplifiedbyothernetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Being Friends Instead of Adversaries: Deep Networks Learn from Data Simplified by Other Networks",
    "authors": [
      "Simone Marullo",
      "Matteo Tiezzi",
      "Marco Gori",
      "Stefano Melacci"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20740",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20740/20499",
    "published": "2022-02",
    "summary": "Amongst a variety of approaches aimed at making the learning procedure of neural networks more effective, the scientific community developed strategies to order the examples according to their estimated complexity, to distil knowledge from larger networks, or to exploit the principles behind adversarial machine learning. A different idea has been recently proposed, named Friendly Training, which consists in altering the input data by adding an automatically estimated perturbation, with the goal of facilitating the learning process of a neural classifier. The transformation progressively fades-out as long as training proceeds, until it completely vanishes. In this work we revisit and extend this idea, introducing a radically different and novel approach inspired by the effectiveness of neural generators in the context of Adversarial Machine Learning. We propose an auxiliary multi-layer network that is responsible of altering the input data to make them easier to be handled by the classifier at the current stage of the training procedure.The auxiliary network is trained jointly with the neural classifier, thus intrinsically increasing the 'depth' of the classifier, and it is expected to spot general regularities in the data alteration process. The effect of the auxiliary network is progressively reduced up to the end of training, when it is fully dropped and the classifier is deployed for applications. We refer to this approach as Neural Friendly Training. An extended experimental procedure involving several datasets and different neural architectures shows that Neural Friendly Training overcomes the originally proposed Friendly Training technique, improving the generalization of the classifier, especially in the case of noisy data.",
    "code_link": ""
  },
  "aaai2022_main_anexperimentaldesignapproachforregretminimizationinlogisticbandits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Experimental Design Approach for Regret Minimization in Logistic Bandits",
    "authors": [
      "Blake Mason",
      "Kwang-Sung Jun",
      "Lalit Jain"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20741",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20741/20500",
    "published": "2022-02",
    "summary": "In this work we consider the problem of regret minimization for logistic bandits. The main challenge of logistic bandits is reducing the dependence on a potentially large problem dependent constant that can at worst scale exponentially with the norm of the unknown parameter vector. Previous works have applied self-concordance of the logistic function to remove this worst-case dependence providing regret guarantees that move the reduce the dependence on this worst case parameter to lower order terms with only polylogarithmic dependence on the main term and as well as linear dependence on the dimension of the unknown parameter. This work improves upon the prior art by 1) removing all scaling of the worst case term on the main term and 2) reducing the dependence on the dependence to scale with the square root of dimension in the fixed arm setting by employing an experimental design procedure. Our regret bound in fact takes a tighter instance (i.e., gap) dependent regret bound for the first time in logistic bandits. We also propose a new warmup sampling algorithm that can dramatically reduce the lower order term in the regret in general and prove that it can exponentially reduce the lower order term's dependency on the worst case parameter in some instances. Finally, we discuss the impact of the bias of the MLE on the logistic bandit problem in d dimensions, providing an example where d^2 lower order regret (cf., it is d for linear bandits) may not be improved as long as the MLE is used and how bias-corrected estimators may be used to make it closer to d.",
    "code_link": ""
  },
  "aaai2022_main_coordinatedescentontheorthogonalgroupforrecurrentneuralnetworktraining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Coordinate Descent on the Orthogonal Group for Recurrent Neural Network Training",
    "authors": [
      "Estelle Massart",
      "Vinayak Abrol"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20742",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20742/20501",
    "published": "2022-02",
    "summary": "We address the poor scalability of learning algorithms for orthogonal recurrent neural networks via the use of stochastic coordinate descent on the orthogonal group, leading to a cost per iteration that increases linearly with the number of recurrent states. This contrasts with the cubic dependency of typical feasible algorithms such as stochastic Riemannian gradient descent, which prohibits the use of big network architectures. Coordinate descent rotates successively two columns of the recurrent matrix. When the coordinate (i.e., indices of rotated columns) is selected uniformly at random at each iteration, we prove convergence of the algorithm under standard assumptions on the loss function, stepsize and minibatch noise. In addition, we numerically show that the Riemannian gradient has an approximately sparse structure. Leveraging this observation, we propose a variant of our proposed algorithm that relies on the Gauss-Southwell coordinate selection rule. Experiments on a benchmark recurrent neural network training problem show that the proposed approach is a very promising step towards the training of orthogonal recurrent neural networks with big architectures.",
    "code_link": ""
  },
  "aaai2022_main_curiosity-drivenexplorationvialatentbayesiansurprise": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Curiosity-Driven Exploration via Latent Bayesian Surprise",
    "authors": [
      "Pietro Mazzaglia",
      "Ozan Catal",
      "Tim Verbelen",
      "Bart Dhoedt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20743",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20743/20502",
    "published": "2022-02",
    "summary": "The human intrinsic desire to pursue knowledge, also known as curiosity, is considered essential in the process of skill acquisition. With the aid of artificial curiosity, we could equip current techniques for control, such as Reinforcement Learning, with more natural exploration capabilities. A promising approach in this respect has consisted of using Bayesian surprise on model parameters, i.e. a metric for the difference between prior and posterior beliefs, to favour exploration. In this contribution, we propose to apply Bayesian surprise in a latent space representing the agent\u2019s current understanding of the dynamics of the system, drastically reducing the computational costs. We extensively evaluate our method by measuring the agent's performance in terms of environment exploration, for continuous tasks, and looking at the game scores achieved, for video games. Our model is computationally cheap and compares positively with current state-of-the-art methods on several problems. We also investigate the effects caused by stochasticity in the environment, which is often a failure case for curiosity-driven agents. In this regime, the results suggest that our approach is resilient to stochastic transitions.",
    "code_link": ""
  },
  "aaai2022_main_whatcanwelearnevenfromtheweakest?learningsketchesforprogrammaticstrategies": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "What Can We Learn Even from the Weakest? Learning Sketches for Programmatic Strategies",
    "authors": [
      "Leandro C. Medeiros",
      "David S. Aleixo",
      "Levi H. S. Lelis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20744",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20744/20503",
    "published": "2022-02",
    "summary": "In this paper we show that behavioral cloning can be used to learn effective sketches of programmatic strategies. We show that even the sketches learned by cloning the behavior of weak players can help the synthesis of programmatic strategies. This is because even weak players can provide helpful information, e.g., that a player must choose an action in their turn of the game. If behavioral cloning is not employed, the synthesizer needs to learn even the most basic information by playing the game, which can be computationally expensive. We demonstrate empirically the advantages of our sketch-learning approach with simulated annealing and UCT synthesizers. We evaluate our synthesizers in the games of Can't Stop and MicroRTS. The sketch-based synthesizers are able to learn stronger programmatic strategies than their original counterparts. Our synthesizers generate strategies of Can't Stop that defeat a traditional programmatic strategy for the game. They also synthesize strategies that defeat the best performing method from the latest MicroRTS competition.",
    "code_link": ""
  },
  "aaai2022_main_top-downdeepclusteringwithmulti-generatorgans": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Top-Down Deep Clustering with Multi-Generator GANs",
    "authors": [
      "Daniel P. M. de Mello",
      "Renato M. Assun\u00e7\u00e3o",
      "Fabricio Murai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20745",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20745/20504",
    "published": "2022-02",
    "summary": "Deep clustering (DC) leverages the representation power of deep architectures to learn embedding spaces that are optimal for cluster analysis. This approach filters out low-level information irrelevant for clustering and has proven remarkably successful for high dimensional data spaces. Some DC methods employ Generative Adversarial Networks (GANs), motivated by the powerful latent representations these models are able to learn implicitly. In this work, we propose HC-MGAN, a new technique based on GANs with multiple generators (MGANs), which have not been explored for clustering. Our method is inspired by the observation that each generator of a MGAN tends to generate data that correlates with a sub-region of the real data distribution. We use this clustered generation to train a classifier for inferring from which generator a given image came from, thus providing a semantically meaningful clustering for the real distribution. Additionally, we design our method so that it is performed in a top-down hierarchical clustering tree, thus proposing the first hierarchical DC method, to the best of our knowledge. We conduct several experiments to evaluate the proposed method against recent DC methods, obtaining competitive results. Last, we perform an exploratory analysis of the hierarchical clustering tree that highlights how accurately it organizes the data in a hierarchy of semantically coherent patterns.",
    "code_link": "https://github.com/dmdmello/HC-MGAN"
  },
  "aaai2022_main_temporalknowledgegraphcompletionusingboxembeddings": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Temporal Knowledge Graph Completion Using Box Embeddings",
    "authors": [
      "Johannes Messner",
      "Ralph Abboud",
      "Ismail Ilkan Ceylan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20746",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20746/20505",
    "published": "2022-02",
    "summary": "Knowledge graph completion is the task of inferring missing facts based on existing data in a knowledge graph. Temporal knowledge graph completion (TKGC) is an extension of this task to temporal knowledge graphs, where each fact is additionally associated with a time stamp. Current approaches for TKGC primarily build on existing embedding models which are developed for static knowledge graph completion, and extend these models to incorporate time, where the idea is to learn latent representations for entities, relations, and timestamps and then use the learned representations to predict missing facts at various time steps. In this paper, we propose BoxTE, a box embedding model for TKGC, building on the static knowledge graph embedding model BoxE. We show that BoxTE is fully expressive, and possesses strong inductive capacity in the temporal setting. We then empirically evaluate our model and show that it achieves state-of-the-art results on several TKGC benchmarks",
    "code_link": ""
  },
  "aaai2022_main_anevaluativemeasureofclusteringmethodsincorporatinghyperparametersensitivity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Evaluative Measure of Clustering Methods Incorporating Hyperparameter Sensitivity",
    "authors": [
      "Siddhartha Mishra",
      "Nicholas Monath",
      "Michael Boratko",
      "Ariel Kobren",
      "Andrew\n      McCallum"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20747",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20747/20506",
    "published": "2022-02",
    "summary": "Clustering algorithms are often evaluated using metrics which compare with ground-truth cluster assignments, such as Rand index and NMI. Algorithm performance may vary widely for different hyperparameters, however, and thus model selection based on optimal performance for these metrics is discordant with how these algorithms are applied in practice, where labels are unavailable and tuning is often more art than science. It is therefore desirable to compare clustering algorithms not only on their optimally tuned performance, but also some notion of how realistic it would be to obtain this performance in practice. We propose an evaluation of clustering methods capturing this ease-of-tuning by modeling the expected best clustering score under a given computation budget. To encourage the adoption of the proposed metric alongside classic clustering evaluations, we provide an extensible benchmarking framework. We perform an extensive empirical evaluation of our proposed metric on popular clustering algorithms over a large collection of datasets from different domains, and observe that our new metric leads to several noteworthy observations.",
    "code_link": ""
  },
  "aaai2022_main_simpleunsupervisedgraphrepresentationlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Simple Unsupervised Graph Representation Learning",
    "authors": [
      "Yujie Mo",
      "Liang Peng",
      "Jie Xu",
      "Xiaoshuang Shi",
      "Xiaofeng Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20748",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20748/20507",
    "published": "2022-02",
    "summary": "In this paper, we propose a simple unsupervised graph representation learning method to conduct effective and efficient contrastive learning. Specifically, the proposed multiplet loss explores the complementary information between the structural information and neighbor information to enlarge the inter-class variation, as well as adds an upper bound loss to achieve the finite distance between positive embeddings and anchor embeddings for reducing the intra-class variation. As a result, both enlarging inter-class variation and reducing intra-class variation result in small generalization error, thereby obtaining an effective model. Furthermore, our method removes widely used data augmentation and discriminator from previous graph contrastive learning methods, meanwhile available to output low-dimensional embeddings, leading to an efficient model. Experimental results on various real-world datasets demonstrate the effectiveness and efficiency of our method, compared to state-of-the-art methods. The source codes are released at https://github.com/YujieMo/SUGRL.",
    "code_link": "https://github.com/YujieMo/SUGRL"
  },
  "aaai2022_main_theroleofadaptiveoptimizersforhonestprivatehyperparameterselection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Role of Adaptive Optimizers for Honest Private Hyperparameter Selection",
    "authors": [
      "Shubhankar Mohapatra",
      "Sajin Sasy",
      "Xi He",
      "Gautam Kamath",
      "Om Thakkar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20749",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20749/20508",
    "published": "2022-02",
    "summary": "Hyperparameter optimization is a ubiquitous challenge in machine learning, and the performance of a trained model depends crucially upon their effective selection. While a rich set of tools exist for this purpose, there are currently no practical hyperparameter selection methods under the constraint of differential privacy (DP). We study honest hyperparameter selection for differentially private machine learning, in which the process of hyperparameter tuning is accounted for in the overall privacy budget. To this end, we i) show that standard composition tools outperform more advanced techniques in many settings, ii) empirically and theoretically demonstrate an intrinsic connection between the learning rate and clipping norm hyperparameters, iii) show that adaptive optimizers like DPAdam enjoy a significant advantage in the process of honest hyperparameter tuning, and iv) draw upon novel limiting behaviour of Adam in the DP setting to design a new and more efficient optimizer.",
    "code_link": ""
  },
  "aaai2022_main_learningbayesiannetworksinthepresenceofstructuralsideinformation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Bayesian Networks in the Presence of Structural Side Information",
    "authors": [
      "Ehsan Mokhtarian",
      "Sina Akbari",
      "Fateme Jamshidi",
      "Jalal Etesami",
      "Negar\n      Kiyavash"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20750",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20750/20509",
    "published": "2022-02",
    "summary": "We study the problem of learning a Bayesian network (BN) of a set of variables when structural side information about the system is available. It is well known that learning the structure of a general BN is both computationally and statistically challenging. However, often in many applications, side information about the underlying structure can potentially reduce the learning complexity. In this paper, we develop a recursive constraint-based algorithm that efficiently incorporates such knowledge (i.e., side information) into the learning process. In particular, we study two types of structural side information about the underlying BN: (I) an upper bound on its clique number is known, or (II) it is diamond-free. We provide theoretical guarantees for the learning algorithms, including the worst-case number of tests required in each scenario. As a consequence of our work, we show that bounded treewidth BNs can be learned with polynomial complexity. Furthermore, we evaluate the performance and the scalability of our algorithms in both synthetic and real-world structures and show that they outperform the state-of-the-art structure learning algorithms.",
    "code_link": "https://github.com/Ehsan-Mokhtarian/RSL"
  },
  "aaai2022_main_preemptiveimagerobustificationforprotectingusersagainstman-in-the-middleadversarialattacks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks",
    "authors": [
      "Seungyong Moon",
      "Gaon An",
      "Hyun Oh Song"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20751",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20751/20510",
    "published": "2022-02",
    "summary": "Deep neural networks have become the driving force of modern image recognition systems. However, the vulnerability of neural networks against adversarial attacks poses a serious threat to the people affected by these systems. In this paper, we focus on a real-world threat model where a Man-in-the-Middle adversary maliciously intercepts and perturbs images web users upload online. This type of attack can raise severe ethical concerns on top of simple performance degradation. To prevent this attack, we devise a novel bi-level optimization algorithm that finds points in the vicinity of natural images that are robust to adversarial perturbations. Experiments on CIFAR-10 and ImageNet show our method can effectively robustify natural images within the given modification budget. We also show the proposed method can improve robustness when jointly used with randomized smoothing.",
    "code_link": "https://github.com/snu-mllab/preemptive_robustification"
  },
  "aaai2022_main_provableguaranteesforunderstandingout-of-distributiondetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Provable Guarantees for Understanding Out-of-Distribution Detection",
    "authors": [
      "Peyman Morteza",
      "Yixuan Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20752",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20752/20511",
    "published": "2022-02",
    "summary": "Out-of-distribution (OOD) detection is important for deploying machine learning models in the real world, where test data from shifted distributions can naturally arise. While a plethora of algorithmic approaches have recently emerged for OOD detection, a critical gap remains in theoretical understanding. In this work, we develop an analytical framework that characterizes and unifies the theoretical understanding for OOD detection. Our analytical framework motivates a novel OOD detection method for neural networks, GEM, which demonstrates both theoretical and empirical superiority. In particular, on CIFAR-100 as in-distribution data, our method outperforms a competitive baseline by 16.57% (FPR95). Lastly, we formally provide provable guarantees and comprehensive analysis of our method, underpinning how various properties of data distribution affect the performance of OOD detection.",
    "code_link": "https://github.com/PeymanMorteza/GEM"
  },
  "aaai2022_main_constraintsamplingreinforcementlearningincorporatingexpertiseforfasterlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Constraint Sampling Reinforcement Learning: Incorporating Expertise for Faster Learning",
    "authors": [
      "Tong Mu",
      "Georgios Theocharous",
      "David Arbour",
      "Emma Brunskill"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20753",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20753/20512",
    "published": "2022-02",
    "summary": "Online reinforcement learning (RL) algorithms are often difficult to deploy in complex human-facing applications as they may learn slowly and have poor early performance. To address this, we introduce a practical algorithm for incorporating human insight to speed learning. Our algorithm, Constraint Sampling Reinforcement Learning (CSRL), incorporates prior domain knowledge as constraints/restrictions on the RL policy. It takes in multiple potential policy constraints to maintain robustness to misspecification of individual constraints while leveraging helpful ones to learn quickly. Given a base RL learning algorithm (ex. UCRL, DQN, Rainbow) we propose an upper confidence with elimination scheme that leverages the relationship between the constraints, and their observed performance, to adaptively switch among them. We instantiate our algorithm with DQN-type algorithms and UCRL as base algorithms, and evaluate our algorithm in four environments, including three simulators based on real data: recommendations, educational activity sequencing, and HIV treatment sequencing. In all cases, CSRL learns a good policy faster than baselines.",
    "code_link": "https://github.com/StanfordAI4HI/CSRL"
  },
  "aaai2022_main_unsupervisedreinforcementlearninginmultipleenvironments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Reinforcement Learning in Multiple Environments",
    "authors": [
      "Mirco Mutti",
      "Mattia Mancassola",
      "Marcello Restelli"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20754",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20754/20513",
    "published": "2022-02",
    "summary": "Several recent works have been dedicated to unsupervised reinforcement learning in a single environment, in which a policy is first pre-trained with unsupervised interactions, and then fine-tuned towards the optimal policy for several downstream supervised tasks defined over the same environment. Along this line, we address the problem of unsupervised reinforcement learning in a class of multiple environments, in which the policy is pre-trained with interactions from the whole class, and then fine-tuned for several tasks in any environment of the class. Notably, the problem is inherently multi-objective as we can trade off the pre-training objective between environments in many ways. In this work, we foster an exploration strategy that is sensitive to the most adverse cases within the class. Hence, we cast the exploration problem as the maximization of the mean of a critical percentile of the state visitation entropy induced by the exploration strategy over the class of environments. Then, we present a policy gradient algorithm, alphaMEPOL, to optimize the introduced objective through mediated interactions with the class. Finally, we empirically demonstrate the ability of the algorithm in learning to explore challenging classes of continuous environments and we show that reinforcement learning greatly benefits from the pre-trained exploration strategy w.r.t. learning from scratch.",
    "code_link": "https://github.com/muttimirco/alphamepol"
  },
  "aaai2022_main_isyourdatarelevant?dynamicselectionofrelevantdataforfederatedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Is Your Data Relevant?: Dynamic Selection of Relevant Data for Federated Learning",
    "authors": [
      "Lokesh Nagalapatti",
      "Ruhi Sharma Mittal",
      "Ramasuri Narayanam"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20755",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20755/20514",
    "published": "2022-02",
    "summary": "Federated Learning (FL) is a machine learning paradigm in which multiple clients participate to collectively learn a global machine learning model at the central server. It is plausible that not all the data owned by each client is relevant to the server's learning objective. The updates incorporated from irrelevant data could be detrimental to the global model. The task of selecting relevant data is explored in traditional machine learning settings where the assumption is that all the data is available in one place. In FL settings, the data is distributed across multiple clients and the server can't introspect it. This precludes the application of traditional solutions to selecting relevant data here.In this paper, we propose an approach called Federated Learning with Relevant Data (FLRD), that facilitates clients to derive updates using relevant data. Each client learns a model called Relevant Data Selector (RDS) that is private to itself to do the selection. This in turn helps in building an effective global model.We perform experiments with multiple real-world datasets to demonstrate the efficacy of our solution. The results show (a) the capability of FLRD to identify relevant data samples at each client locally and (b) the superiority of the global model learned by FLRD over other baseline algorithms.",
    "code_link": ""
  },
  "aaai2022_main_adynamicmeta-learningmodelfortime-sensitivecold-startrecommendations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations",
    "authors": [
      "Krishna Prasad Neupane",
      "Ervine Zheng",
      "Yu Kong",
      "Qi Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20756",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20756/20515",
    "published": "2022-02",
    "summary": "We present a novel dynamic recommendation model that focuses on users who have interactions in the past but turn relatively inactive recently. Making effective recommendations to these time-sensitive cold-start users is critical to maintain the user base of a recommender system. Due to the sparse recent interactions, it is challenging to capture these users' current preferences precisely. Solely relying on their historical interactions may also lead to outdated recommendations misaligned with their recent interests. The proposed model leverages historical and current user-item interactions and dynamically factorizes a user's (latent) preference into time-specific and time-evolving representations that jointly affect user behaviors. These latent factors further interact with an optimized item embedding to achieve accurate and timely recommendations. Experiments over real-world data help demonstrate the effectiveness of the proposed time-sensitive cold-start recommendation model.",
    "code_link": ""
  },
  "aaai2022_main_outofdistributiondatadetectionusingdropoutbayesianneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Out of Distribution Data Detection Using Dropout Bayesian Neural Networks",
    "authors": [
      "Andre T. Nguyen",
      "Fred Lu",
      "Gary Lopez Munoz",
      "Edward Raff",
      "Charles Nicholas",
      "James Holt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20757",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20757/20516",
    "published": "2022-02",
    "summary": "We explore the utility of information contained within a dropout based Bayesian neural network (BNN) for the task of detecting out of distribution (OOD) data. We first show how previous attempts to leverage the randomized embeddings induced by the intermediate layers of a dropout BNN can fail due to the distance metric used. We introduce an alternative approach to measuring embedding uncertainty, and demonstrate how incorporating embedding uncertainty improves OOD data identification across three tasks: image classification, language classification, and malware detection.",
    "code_link": ""
  },
  "aaai2022_main_control-orientedmodel-basedreinforcementlearningwithimplicitdifferentiation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation",
    "authors": [
      "Evgenii Nikishin",
      "Romina Abachi",
      "Rishabh Agarwal",
      "Pierre-Luc Bacon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20758",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20758/20517",
    "published": "2022-02",
    "summary": "The shortcomings of maximum likelihood estimation in the context of model-based reinforcement learning have been highlighted by an increasing number of papers. When the model class is misspecified or has a limited representational capacity, model parameters with high likelihood might not necessarily result in high performance of the agent on a downstream control task. To alleviate this problem, we propose an end-to-end approach for model learning which directly optimizes the expected returns using implicit differentiation. We treat a value function that satisfies the Bellman optimality operator induced by the model as an implicit function of model parameters and show how to differentiate the function. We provide theoretical and empirical evidence highlighting the benefits of our approach in the model misspecification regime compared to likelihood-based methods.",
    "code_link": "https://github.com/ikostrikov/jaxrl"
  },
  "aaai2022_main_improvingevidentialdeeplearningviamulti-tasklearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Evidential Deep Learning via Multi-Task Learning",
    "authors": [
      "Dongpin Oh",
      "Bonggun Shin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20759",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20759/20518",
    "published": "2022-02",
    "summary": "The Evidential regression network (ENet) estimates a continuous target and its predictive uncertainty without costly Bayesian model averaging. However, it is possible that the target is inaccurately predicted due to the gradient shrinkage problem of the original loss function of the ENet, the negative log marginal likelihood (NLL) loss. In this paper, the objective is to improve the prediction accuracy of the ENet while maintaining its efficient uncertainty estimation by resolving the gradient shrinkage problem. A multi-task learning (MTL) framework, referred to as MT-ENet, is proposed to accomplish this aim. In the MTL, we define the Lipschitz modified mean squared error (MSE) loss function as another loss and add it to the existing NLL loss. The Lipschitz modified MSE loss is designed to mitigate the gradient conflict with the NLL loss by dynamically adjusting its Lipschitz constant. By doing so, the Lipschitz MSE loss does not disturb the uncertainty estimation of the NLL loss. The MT-ENet enhances the predictive accuracy of the ENet without losing uncertainty estimation capability on the synthetic dataset and real-world benchmarks, including drug-target affinity (DTA) regression. Furthermore, the MT-ENet shows remarkable calibration and out-of-distribution detection capability on the DTA benchmarks.",
    "code_link": "https://github.com/deargen/MT-ENet"
  },
  "aaai2022_main_clusteringapproachtosolvehierarchicalclassificationproblemcomplexity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Clustering Approach to Solve Hierarchical Classification Problem Complexity",
    "authors": [
      "Aomar Osmani",
      "Massinissa Hamidi",
      "Pegah Alizadeh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20760",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20760/20519",
    "published": "2022-02",
    "summary": "In a large domain of classification problems for real applications, like human activity recognition, separable spaces between groups of concepts are easier to learn than each concept alone. This is because the search space biases required to separate groups of classes (or concepts) are more relevant than the ones needed to separate classes individually. For example, it is easier to learn the activities related to the body movements group (running, walking) versus \"on-wheels\" activities group (bicycling, driving a car), before learning more specific classes inside each of these groups. Despite the obvious interest of this approach, our theoretical analysis shows a high complexity for finding an exact solution. We propose in this paper an original approach based on the association of clustering and classification approaches to overcome this limitation. We propose a better approach to learn the concepts by grouping classes recursively rather than learning them class by class. We introduce an effective greedy algorithm and two theoretical measures, namely cohesion and dispersion, to evaluate the connection between the clusters and the classes. Extensive experiments on the SHL dataset show that our approach improves classification performances while reducing the number of instances used to learn each concept.",
    "code_link": "https://github.com/sensorrich/clustering-based-HL"
  },
  "aaai2022_main_randomtensortheoryfortensordecomposition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Random Tensor Theory for Tensor Decomposition",
    "authors": [
      "Mohamed Ouerfelli",
      "Mohamed Tamaazousti",
      "Vincent Rivasseau"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20761",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20761/20520",
    "published": "2022-02",
    "summary": "We propose a new framework for tensor decomposition based on trace invariants, which are particular cases of tensor networks. In general, tensor networks are diagrams/graphs that specify a way to \"multiply\" a collection of tensors together to produce another tensor, matrix or scalar. The particularity of trace invariants is that the operation of multiplying copies of a certain input tensor that produces a scalar obeys specific symmetry constraints. In other words, the scalar resulting from this multiplication is invariant under some specific transformations of the involved tensor. We focus our study on the O(N)-invariant graphs, i.e. invariant under orthogonal transformations of the input tensor. The proposed approach is novel and versatile since it allows to address different theoretical and practical aspects of both CANDECOMP/PARAFAC (CP) and Tucker decomposition models. In particular we obtain several results: (i) we generalize the computational limit of Tensor PCA (a rank-one tensor decomposition) to the case of a tensor with axes of different dimensions (ii) we introduce new algorithms for both decomposition models (iii) we obtain theoretical guarantees for these algorithms and (iv) we show improvements with respect to state of the art on synthetic and real data which also highlights a promising potential for practical applications.",
    "code_link": ""
  },
  "aaai2022_main_baggraphmultipleinstancelearningusingbayesiangraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bag Graph: Multiple Instance Learning Using Bayesian Graph Neural Networks",
    "authors": [
      "Soumyasundar Pal",
      "Antonios Valkanas",
      "Florence Regol",
      "Mark Coates"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20762",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20762/20521",
    "published": "2022-02",
    "summary": "Multiple Instance Learning (MIL) is a weakly supervised learning problem where the aim is to assign labels to sets or bags of instances, as opposed to traditional supervised learning where each instance is assumed to be independent and identically distributed (IID) and is to be labeled individually. Recent work has shown promising results for neural network models in the MIL setting. Instead of focusing on each instance, these models are trained in an end-to-end fashion to learn effective bag-level representations by suitably combining permutation invariant pooling techniques with neural architectures. In this paper, we consider modelling the interactions between bags using a graph and employ Graph Neural Networks (GNNs) to facilitate end-to-end learning. Since a meaningful graph representing dependencies between bags is rarely available, we propose to use a Bayesian GNN framework that can generate a likely graph structure for scenarios where there is uncertainty in the graph or when no graph is available. Empirical results demonstrate the efficacy of the proposed technique for several MIL benchmark tasks and a distribution regression task.",
    "code_link": "https://github.com/networkslab/BagGraph"
  },
  "aaai2022_main_competingmutualinformationconstraintswithstochasticcompetition-basedactivationsforlearningdiversifiedrepresentations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Competing Mutual Information Constraints with Stochastic Competition-Based Activations for Learning Diversified Representations",
    "authors": [
      "Konstantinos P. Panousis",
      "Anastasios Antoniadis",
      "Sotirios Chatzis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20763",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20763/20522",
    "published": "2022-02",
    "summary": "This work aims to address the long-established problem of learning diversified representations. To this end, we combine information-theoretic arguments with stochastic competition-based activations, namely Stochastic Local Winner-Takes-All (LWTA) units. In this context, we ditch the conventional deep architectures commonly used in Representation Learning, that rely on non-linear activations; instead, we replace them with sets of locally and stochastically competing linear units. In this setting, each network layer yields sparse outputs, determined by the outcome of the competition between units that are organized into blocks of competitors. We adopt stochastic arguments for the competition mechanism, which perform posterior sampling to determine the winner of each block. We further endow the considered networks with the ability to infer the sub-part of the network that is essential for modeling the data at hand; we impose appropriate stick-breaking priors to this end. To further enrich the information of the emerging representations, we resort to information-theoretic principles, namely the Information Competing Process (ICP). Then, all the components are tied together under the stochastic Variational Bayes framework for inference. We perform a thorough experimental investigation for our approach using benchmark datasets on image classification. As we experimentally show, the resulting networks yield significant discriminative representation learning abilities. In addition, the introduced paradigm allows for a principled investigation mechanism of the emerging intermediate network representations.",
    "code_link": ""
  },
  "aaai2022_main_blockwisesequentialmodellearningforpartiallyobservablereinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Blockwise Sequential Model Learning for Partially Observable Reinforcement Learning",
    "authors": [
      "Giseung Park",
      "Sungho Choi",
      "Youngchul Sung"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20764",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20764/20523",
    "published": "2022-02",
    "summary": "This paper proposes a new sequential model learning architecture to solve partially observable Markov decision problems. Rather than compressing sequential information at every timestep as in conventional recurrent neural network-based methods, the proposed architecture generates a latent variable in each data block with a length of multiple timesteps and passes the most relevant information to the next block for policy optimization. The proposed blockwise sequential model is implemented based on self-attention, making the model capable of detailed sequential learning in partial observable settings. The proposed model builds an additional learning network to efficiently implement gradient estimation by using self-normalized importance sampling, which does not require the complex blockwise input data reconstruction in the model learning. Numerical results show that the proposed method significantly outperforms previous methods in various partially observable environments.",
    "code_link": ""
  },
  "aaai2022_main_deformablegraphconvolutionalnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deformable Graph Convolutional Networks",
    "authors": [
      "Jinyoung Park",
      "Sungdong Yoo",
      "Jihwan Park",
      "Hyunwoo J. Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20765",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20765/20524",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs) have significantly improved the representation power for graph-structured data. Despite of the recent success of GNNs, the graph convolution in most GNNs have two limitations. Since the graph convolution is performed in a small local neighborhood on the input graph, it is inherently incapable to capture long-range dependencies between distance nodes. In addition, when a node has neighbors that belong to different classes, i.e., heterophily, the aggregated messages from them often negatively affect representation learning. To address the two common problems of graph convolution, in this paper, we propose Deformable Graph Convolutional Networks (Deformable GCNs) that adaptively perform convolution in multiple latent spaces and capture short/long-range dependencies between nodes. Separated from node representations (features), our framework simultaneously learns the node positional embeddings (coordinates) to determine the relations between nodes in an end-to-end fashion. Depending on node position, the convolution kernels are deformed by deformation vectors and apply different transformations to its neighbor nodes. Our extensive experiments demonstrate that Deformable GCNs flexibly handles the heterophily and achieve the best performance in node classification tasks on six heterophilic graph datasets. Our code is publicly available at https://github.com/mlvlab/DeformableGCN.",
    "code_link": ""
  },
  "aaai2022_main_saliencygraftinginnocuousattribution-guidedmixupwithcalibratedlabelmixing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing",
    "authors": [
      "Joonhyung Park",
      "June Yong Yang",
      "Jinwoo Shin",
      "Sung Ju Hwang",
      "Eunho Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20766",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20766/20525",
    "published": "2022-02",
    "summary": "The Mixup scheme suggests mixing a pair of samples to create an augmented training sample and has gained considerable attention recently for improving the generalizability of neural networks. A straightforward and widely used extension of Mixup is to combine with regional dropout-like methods: removing random patches from a sample and replacing it with the features from another sample. Albeit their simplicity and effectiveness, these methods are prone to create harmful samples due to their randomness. To address this issue, 'maximum saliency' strategies were recently proposed: they select only the most informative features to prevent such a phenomenon. However, they now suffer from lack of sample diversification as they always deterministically select regions with maximum saliency, injecting bias into the augmented data. In this paper, we present, a novel, yet simple Mixup-variant that captures the best of both worlds. Our idea is two-fold. By stochastically sampling the features and \u2018grafting\u2019 them onto another sample, our method effectively generates diverse yet meaningful samples. Its second ingredient is to produce the label of the grafted sample by mixing the labels in a saliency-calibrated fashion, which rectifies supervision misguidance introduced by the random sampling procedure. Our experiments under CIFAR, Tiny-ImageNet, and ImageNet datasets show that our scheme outperforms the current state-of-the-art augmentation strategies not only in terms of classification accuracy, but is also superior in coping under stress conditions such as data corruption and object occlusion.",
    "code_link": ""
  },
  "aaai2022_main_graphtransplantnodesaliency-guidedgraphmixupwithlocalstructurepreservation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph Transplant: Node Saliency-Guided Graph Mixup with Local Structure Preservation",
    "authors": [
      "Joonhyung Park",
      "Hajin Shim",
      "Eunho Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20767",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20767/20526",
    "published": "2022-02",
    "summary": "Graph-structured datasets usually have irregular graph sizes and connectivities, rendering the use of recent data augmentation techniques, such as Mixup, difficult. To tackle this challenge, we present the first Mixup-like graph augmentation method called Graph Transplant, which mixes irregular graphs in data space. To be well defined on various scales of the graph, our method identifies the sub-structure as a mix unit that can preserve the local information. Since the mixup-based methods without special consideration of the context are prone to generate noisy samples, our method explicitly employs the node saliency information to select meaningful subgraphs and adaptively determine the labels. We extensively validate our method with diverse GNN architectures on multiple graph classification benchmark datasets from a wide range of graph domains of different sizes. Experimental results show the consistent superiority of our method over other basic data augmentation baselines. We also demonstrate that Graph Transplant enhances the performance in terms of robustness and model calibration.",
    "code_link": ""
  },
  "aaai2022_main_cc-certaprobabilisticapproachtocertifygeneralrobustnessofneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CC-CERT: A Probabilistic Approach to Certify General Robustness of Neural Networks",
    "authors": [
      "Mikhail Pautov",
      "Nurislam Tursynbek",
      "Marina Munkhoeva",
      "Nikita Muravev",
      "Aleksandr Petiushko",
      "Ivan Oseledets"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20768",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20768/20527",
    "published": "2022-02",
    "summary": "In safety-critical machine learning applications, it is crucial to defend models against adversarial attacks --- small modifications of the input that change the predictions. Besides rigorously studied $\\ell_p$-bounded additive perturbations, semantic perturbations (e.g. rotation, translation) raise a serious concern on deploying ML systems in real-world. Therefore, it is important to provide provable guarantees for deep learning models against semantically meaningful input transformations. In this paper, we propose a new universal probabilistic certification approach based on Chernoff-Cramer bounds that can be used in general attack settings. We estimate the probability of a model to fail if the attack is sampled from a certain distribution. Our theoretical findings are supported by experimental results on different datasets.",
    "code_link": ""
  },
  "aaai2022_main_coveredinformationdisentanglementmodeltransparencyviaunbiasedpermutationimportance": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Covered Information Disentanglement: Model Transparency via Unbiased Permutation Importance",
    "authors": [
      "Jo\u00e3o P. B. Pereira",
      "Erik S. G. Stroes",
      "Aeilko H. Zwinderman",
      "Evgeni Levin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20769",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20769/20528",
    "published": "2022-02",
    "summary": "Model transparency is a prerequisite in many domains and an increasingly popular area in machine learning research.In the medical domain, for instance, unveiling the mechanisms behind a disease often has higher priority than the diagnostic itself since it might dictate or guide potential treatments and research directions. One of the most popular approaches to explain model global predictions is the permutation importance where the performance on permuted data is benchmarked against the baseline. However, this method and other related approaches will undervalue the importance of a feature in the presence of covariates since these cover part of its provided information. To address this issue, we propose Covered Information Disentanglement CID, a framework that considers all feature information overlap to correct the values provided by permutation importance. We further show how to compute CID efficiently when coupled with Markov random fields. We demonstrate its efficacy in adjusting permutation importance first on a controlled toy dataset and discuss its effect on real-world medical data.",
    "code_link": "https://github.com/JBPereira/CID"
  },
  "aaai2022_main_ontheimpossibilityofnon-trivialaccuracyinpresenceoffairnessconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Impossibility of Non-trivial Accuracy in Presence of Fairness Constraints",
    "authors": [
      "Carlos Pinz\u00f3n",
      "Catuscia Palamidessi",
      "Pablo Piantanida",
      "Frank Valencia"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20770",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20770/20529",
    "published": "2022-02",
    "summary": "One of the main concerns about fairness in machine learning (ML) is that, in order to achieve it, one may have to trade off some accuracy. To overcome this issue, Hardt et al. proposed the notion of equality of opportunity (EO), which is compatible with maximal accuracy when the target label is deterministic with respect to the input features.In the probabilistic case, however, the issue is more complicated: It has been shown that under differential privacy constraints, there are data sources for which EO can only be achieved at the total detriment of accuracy, in the sense that a classifier that satisfies EO cannot be more accurate than a trivial (random guessing) classifier. In our paper we strengthen this result by removing the privacy constraint. Namely, we show that for certain data sources, the most accurate classifier that satisfies EO is a trivial classifier. Furthermore, we study the trade-off between accuracy and EO loss (opportunity difference), and provide a sufficient condition on the data source under which EO and non-trivial accuracy are compatible.",
    "code_link": ""
  },
  "aaai2022_main_spikingneuralnetworkswithimprovedinherentrecurrencedynamicsforsequentiallearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Spiking Neural Networks with Improved Inherent Recurrence Dynamics for Sequential Learning",
    "authors": [
      "Wachirawit Ponghiran",
      "Kaushik Roy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20771",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20771/20530",
    "published": "2022-02",
    "summary": "Spiking neural networks (SNNs) with leaky integrate and fire (LIF) neurons, can be operated in an event-driven manner and have internal states to retain information over time, providing opportunities for energy-efficient neuromorphic computing, especially on edge devices. Note, however, many representative works on SNNs do not fully demonstrate the usefulness of their inherent recurrence (membrane potential retaining information about the past) for sequential learning. Most of the works train SNNs to recognize static images by artificially expanded input representation in time through rate coding. We show that SNNs can be trained for practical sequential tasks by proposing modifications to a network of LIF neurons that enable internal states to learn long sequences and make their inherent recurrence resilient to the vanishing gradient problem. We then develop a training scheme to train the proposed SNNs with improved inherent recurrence dynamics. Our training scheme allows spiking neurons to produce multi-bit outputs (as opposed to binary spikes) which help mitigate the mismatch between a derivative of spiking neurons' activation function and a surrogate derivative used to overcome spiking neurons' non-differentiability. Our experimental results indicate that the proposed SNN architecture on TIMIT and LibriSpeech 100h speech recognition dataset yields accuracy comparable to that of LSTMs (within 1.10% and 0.36%, respectively), but with 2x fewer parameters than LSTMs. The sparse SNN outputs also lead to 10.13x and 11.14x savings in multiplication operations compared to GRUs, which are generally considered as a lightweight alternative to LSTMs, on TIMIT and LibriSpeech 100h datasets, respectively.",
    "code_link": ""
  },
  "aaai2022_main_howprivateisyourrlpolicy?aninverserlbasedanalysisframework": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How Private Is Your RL Policy? An Inverse RL Based Analysis Framework",
    "authors": [
      "Kritika Prakash",
      "Fiza Husain",
      "Praveen Paruchuri",
      "Sujit Gujar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20772",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20772/20531",
    "published": "2022-02",
    "summary": "Reinforcement Learning (RL) enables agents to learn how to perform various tasks from scratch. In domains like autonomous driving, recommendation systems, and more, optimal RL policies learned could cause a privacy breach if the policies memorize any part of the private reward. We study the set of existing differentially-private RL policies derived from various RL algorithms such as Value Iteration, Deep-Q Networks, and Vanilla Proximal Policy Optimization. We propose a new Privacy-Aware Inverse RL analysis framework (PRIL) that involves performing reward reconstruction as an adversarial attack on private policies that the agents may deploy. For this, we introduce the reward reconstruction attack, wherein we seek to reconstruct the original reward from a privacy-preserving policy using the Inverse RL algorithm. An adversary must do poorly at reconstructing the original reward function if the agent uses a tightly private policy. Using this framework, we empirically test the effectiveness of the privacy guarantee offered by the private algorithms on instances of the FrozenLake domain of varying complexities. Based on the analysis performed, we infer a gap between the current standard of privacy offered and the standard of privacy needed to protect reward functions in RL. We do so by quantifying the extent to which each private policy protects the reward function by measuring distances between the original and reconstructed rewards.",
    "code_link": "https://github.com/magnetar-iiith/PRIL"
  },
  "aaai2022_main_detectingmisclassificationerrorsinneuralnetworkswithagaussianprocessmodel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model",
    "authors": [
      "Xin Qiu",
      "Risto Miikkulainen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20773",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20773/20532",
    "published": "2022-02",
    "summary": "As neural network classifiers are deployed in real-world applications, it is crucial that their failures can be detected reliably. One practical solution is to assign confidence scores to each prediction, then use these scores to filter out possible misclassifications. However, existing confidence metrics are not yet sufficiently reliable for this role. This paper presents a new framework that produces a quantitative metric for detecting misclassification errors. This framework, RED, builds an error detector on top of the base classifier and estimates uncertainty of the detection scores using Gaussian Processes. Experimental comparisons with other error detection methods on 125 UCI datasets demonstrate that this approach is effective. Further implementations on two probabilistic base classifiers and two large deep learning architecture in vision tasks further confirm that the method is robust and scalable. Third, an empirical analysis of RED with out-of-distribution and adversarial samples shows that the method can be used not only to detect errors but also to understand where they come from. RED can thereby be used to improve trustworthiness of neural network classifiers more broadly in the future.",
    "code_link": "https://github.com/cognizant-ai-labs/red-paper"
  },
  "aaai2022_main_deeptype2superhumanentitylinking,allyouneedistypeinteractions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepType 2: Superhuman Entity Linking, All You Need Is Type Interactions",
    "authors": [
      "Jonathan Raiman"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20774",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20774/20533",
    "published": "2022-02",
    "summary": "Across multiple domains from computer vision to speech recognition, machine learning models have been shown to match or outperform human experts at recognition tasks. We lack such a comparison point for Entity Linking. We construct a human benchmark on two standard datasets (TAC KBP 2010 and AIDA (YAGO)) to measure human accuracy. We find that current systems still fall short of human performance.We present DeepType 2, a novel entity linking system that closes the gap. Our proposed approach overcomes shortcomings of previous type-based entity linking systems, and does not use pre-trained language models to reach this level. Three key innovations are responsible for DeepType 2's performance: 1) an abstracted representation of entities that favors shared learning and greater sample efficiency, 2) autoregressive entity features indicating type interactions (e.g. list type homogeneity, shared employers, geographical co-occurrence) with previous predictions that enable globally coherent document-wide predictions, 3) the entire model is trained end to end using a single entity-level maximum likelihood objective function. This is made possible by associating a context-specific score to each of the entity's abstract representation's sub-components (types), and summing these scores to form a candidate entity logit. In this paper, we explain how this factorization focuses the learning on the salient types of the candidate entities. Furthermore, we show how the scores can serve as a rationale for predictions. The key contributions of this work are twofold: 1) we create the first human performance benchmark on standard benchmarks in entity linking (TAC KBP 2010 and AIDA (YAGO)) which will be made publicly available to support further analyses, 2) we obtain a new state of the art on these datasets and are the first to outperform humans on our benchmark. We perform model ablations to measure the contribution of the different facets of our system. We also include an analysis of human and algorithmic errors to provide insights into the causes, notably originating from journalistic style and historical context.",
    "code_link": "https://github.com/deep-type/deeptype2"
  },
  "aaai2022_main_federatednearestneighborclassificationwithacolonyoffruit-flies": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies",
    "authors": [
      "Parikshit Ram",
      "Kaushik Sinha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20775",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20775/20534",
    "published": "2022-02",
    "summary": "The mathematical formalization of a neurological mechanism in the fruit-fly olfactory circuit as a locality sensitive hash (Flyhash) and bloom filter (FBF) has been recently proposed and \"reprogrammed\" for various learning tasks such as similarity search, outlier detection and text embeddings. We propose a novel reprogramming of this hash and bloom filter to emulate the canonical nearest neighbor classifier (NNC) in the challenging Federated Learning (FL) setup where training and test data are spread across parties and no data can leave their respective parties. Specifically, we utilize Flyhash and FBF to create the FlyNN classifier, and theoretically establish conditions where FlyNN matches NNC. We show how FlyNN is trained exactly in a FL setup with low communication overhead to produce FlyNNFL, and how it can be differentially private. Empirically, we demonstrate that (i) FlyNN matches NNC accuracy across 70 OpenML datasets, (ii) FlyNNFL training is highly scalable with low communication overhead, providing up to 8x speedup with 16 parties.",
    "code_link": "https://github.com/rithram/flynn"
  },
  "aaai2022_main_i-seaimportancesamplingandexpectedalignment-baseddeepdistancemetriclearningfortimeseriesanalysisandembedding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "I-SEA: Importance Sampling and Expected Alignment-Based Deep Distance Metric Learning for Time Series Analysis and Embedding",
    "authors": [
      "Sirisha Rambhatla",
      "Zhengping Che",
      "Yan Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20776",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20776/20535",
    "published": "2022-02",
    "summary": "Learning effective embeddings for potentially irregularly sampled time-series, evolving at different time scales, is fundamental for machine learning tasks such as classification and clustering. Task-dependent embeddings rely on similarities between data samples to learn effective geometries. However, many popular time-series similarity measures are not valid distance metrics, and as a result they do not reliably capture the intricate relationships between the multi-variate time-series data samples for learning effective embeddings. One of the primary ways to formulate an accurate distance metric is by forming distance estimates via Monte-Carlo-based expectation evaluations. However, the high-dimensionality of the underlying distribution, and the inability to sample from it, pose significant challenges. To this end, we develop an Importance Sampling based distance metric -- I-SEA -- which enjoys the properties of a metric while consistently achieving superior performance for machine learning tasks such as classification and representation learning. I-SEA leverages Importance Sampling and Non-parametric Density Estimation to adaptively estimate distances, enabling implicit estimation from the underlying high-dimensional distribution, resulting in improved accuracy and reduced variance. We theoretically establish the properties of I-SEA and demonstrate its capabilities via experimental evaluations on real-world healthcare datasets.",
    "code_link": "https://github.com/srambhatla/ISEA"
  },
  "aaai2022_main_savingstochasticbanditsfrompoisoningattacksvialimiteddataverification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification",
    "authors": [
      "Anshuka Rangi",
      "Long Tran-Thanh",
      "Haifeng Xu",
      "Massimo Franceschetti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20777",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20777/20536",
    "published": "2022-02",
    "summary": "This paper studies bandit algorithms under data poisoning attacks in a bounded reward setting. We consider a strong attacker model in which the attacker can observe both the selected actions and their corresponding rewards, and can contaminate the rewards with additive noise. We show that any bandit algorithm with regret O(log T) can be forced to suffer a regret O(T) with an expected amount of contamination O(log T). This amount of contamination is also necessary, as we prove that there exists an O(log T) regret bandit algorithm, specifically the classical UCB, that requires Omega(log T) amount of contamination to suffer regret Omega(T). To combat such poisoning attacks, our second main contribution is to propose verification based mechanisms, which use limited verification to access a limited number of uncontaminated rewards. In particular, for the case of unlimited verifications, we show that with O(log T) expected number of verifications, a simple modified version of the Explore-then-Commit type bandit algorithm can restore the order optimal O(log T) regret irrespective of the amount of contamination used by the attacker.We also provide a UCB-like verification scheme, called Secure-UCB, that also enjoys full recovery from any attacks, also with O(log T) expected number of verifications. To derive a matching lower bound on the number of verifications, we also prove that for any order-optimal bandit algorithm, this number of verifications O(log T) is necessary to recover the order-optimal regret. On the other hand, when the number of verifications is bounded above by a budget B, we propose a novel algorithm, Secure-BARBAR, which provably achieves O(min(C,T/sqrt(B))) regret with high probability against weak attackers (i.e., attackers who have to place the contamination before seeing the actual pulls of the bandit algorithm), where C is the total amount of contamination by the attacker, which breaks the known Omega(C) lower bound of the non-verified setting if C is large.",
    "code_link": ""
  },
  "aaai2022_main_distrealdistributedresource-awarelearninginheterogeneoussystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DISTREAL: Distributed Resource-Aware Learning in Heterogeneous Systems",
    "authors": [
      "Martin Rapp",
      "Ramin Khalili",
      "Kilian Pfeiffer",
      "J\u00f6rg Henkel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20778",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20778/20537",
    "published": "2022-02",
    "summary": "We study the problem of distributed training of neural networks (NNs) on devices with heterogeneous, limited, and time-varying availability of computational resources. We present an adaptive, resource-aware, on-device learning mechanism, DISTREAL, which is able to fully and efficiently utilize the available resources on devices in a distributed manner, increasing the convergence speed. This is achieved with a dropout mechanism that dynamically adjusts the computational complexity of training an NN by randomly dropping filters of convolutional layers of the model. Our main contribution is the introduction of a design space exploration (DSE) technique, which finds Pareto-optimal per-layer dropout vectors with respect to resource requirements and convergence speed of the training. Applying this technique, each device is able to dynamically select the dropout vector that fits its available resource without requiring any assistance from the server. We implement our solution in a federated learning (FL) system, where the availability of computational resources varies both between devices and over time, and show through extensive evaluation that we are able to significantly increase the convergence speed over the state of the art without compromising on the final accuracy.",
    "code_link": ""
  },
  "aaai2022_main_sublineartimeapproximationoftextsimilaritymatrices": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sublinear Time Approximation of Text Similarity Matrices",
    "authors": [
      "Archan Ray",
      "Nicholas Monath",
      "Andrew McCallum",
      "Cameron Musco"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20779",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20779/20538",
    "published": "2022-02",
    "summary": "We study algorithms for approximating pairwise similarity matrices that arise in natural language processing. Generally, computing a similarity matrix for n data points requires Omega(n^2) similarity computations. This quadratic scaling is a significant bottleneck, especially when similarities are computed via expensive functions, e.g., via transformer models.Approximation methods reduce this quadratic complexity, often by using a small subset of exactly computed similarities to approximate the remainder of the complete pairwise similarity matrix.Significantwork focuses on the efficient approximation of positive semidefinite (PSD) similarity matrices, which arise e.g., in kernel methods. However, much less is understood about indefinite (non-PSD) similarity matrices, which oftenarise inNLP. Motivated by the observation that many of these matrices are still somewhat close to PSD, we introduce a generalization of the popular Nystrom method to the indefinite setting. Our algorithm can be applied to any similarity matrix and runs in sublinear time in the size of the matrix, producing a rank-s approximation with just O(ns) similarity computations.We show that our method, along with a simple variant of CUR decomposition, performs very well in approximating a variety of similarity matrices arising in NLP tasks. We demonstrate high accuracy of the approximated similarity matrices in tasks of document classification, sentence similarity, and cross-document coreference.",
    "code_link": ""
  },
  "aaai2022_main_decision-dependentriskminimizationingeometricallydecayingdynamicenvironments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Decision-Dependent Risk Minimization in Geometrically Decaying Dynamic Environments",
    "authors": [
      "Mitas Ray",
      "Lillian J. Ratliff",
      "Dmitriy Drusvyatskiy",
      "Maryam Fazel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20780",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20780/20539",
    "published": "2022-02",
    "summary": "This paper studies the problem of expected loss minimization given a data distribution that is dependent on the decision-maker's action and evolves dynamically in time according to a geometric decay process.Novel algorithms for both the information setting in which the decision-maker has a first order gradient oracle and the setting in which they have simply a loss function oracle are introduced. The algorithms operate on the same underlying principle: the decision-maker deploys a fixed decision repeatedly over the length of an epoch,thereby allowing the dynamically changing environment to sufficiently mix before updating the decision.The iteration complexity in each of the settings is shown to match existing rates for first and zero order stochastic gradient methods up to logarithmic factors. The algorithms are evaluated on a ``semi-synthetic\" example using real world data from the SFpark dynamic pricing pilot study; it is shown that the announced prices result in an improvement for the institution's objective (target occupancy), while achieving an overall reduction in parking rates.",
    "code_link": "https://github.com/ratlifflj/D3simulator.git"
  },
  "aaai2022_main_oncausallydisentangledrepresentations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On Causally Disentangled Representations",
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Benin Godfrey L",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20781",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20781/20540",
    "published": "2022-02",
    "summary": "Representation learners that disentangle factors of variation have already proven to be important in addressing various real world concerns such as fairness and interpretability. Initially consisting of unsupervised models with independence assumptions, more recently, weak supervision and correlated features have been explored, but without a causal view of the generative process. In contrast, we work under the regime of a causal generative process where generative factors are either independent or can be potentially confounded by a set of observed or unobserved confounders. We present an analysis of disentangled representations through the notion of disentangled causal process. We motivate the need for new metrics and datasets to study causal disentanglement and propose two evaluation metrics and a dataset. We show that our metrics capture the desiderata of disentangled causal process. Finally we perform an empirical study on state of the art disentangled representation learners using our metrics and dataset to evaluate them from causal perspective.",
    "code_link": ""
  },
  "aaai2022_main_conditionallossanddeepeulerschemefortimeseriesgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Conditional Loss and Deep Euler Scheme for Time Series Generation",
    "authors": [
      "Carl Remlinger",
      "Joseph Mikael",
      "Romuald Elie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20782",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20782/20541",
    "published": "2022-02",
    "summary": "We introduce three new generative models for time series that are based on Euler discretization of Stochastic Differential Equations (SDEs) and Wasserstein metrics. Two of these methods rely on the adaptation of generative adversarial networks (GANs) to time series. The third algorithm, called Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the transition probability distributions over all time steps. In the context of It\u00f4 processes, we provide theoretical guarantees that minimizing this criterion implies accurate estimations of the drift and volatility parameters. Empirically, CEGEN outperforms state-of-the-art and GANs on both marginal and temporal dynamic metrics. Besides, correlation structures are accurately identified in high dimension. When few real data points are available, we verify the effectiveness of CEGEN when combined with transfer learning methods on model-based simulations. Finally, we illustrate the robustness of our methods on various real-world data sets.",
    "code_link": ""
  },
  "aaai2022_main_offlinereinforcementlearningasanti-exploration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Offline Reinforcement Learning as Anti-exploration",
    "authors": [
      "Shideh Rezaeifar",
      "Robert Dadashi",
      "Nino Vieillard",
      "L\u00e9onard Hussenot",
      "Olivier Bachem",
      "Olivier Pietquin",
      "Matthieu Geist"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20783",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20783/20542",
    "published": "2022-02",
    "summary": "Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset and practically extends some previous pessimism-based offline RL methods to a deep learning setting with arbitrary bonuses. We also connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our simple agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",
    "code_link": ""
  },
  "aaai2022_main_interpretableneuralsubgraphmatchingforgraphretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Interpretable Neural Subgraph Matching for Graph Retrieval",
    "authors": [
      "Indradyumna Roy",
      "Venkata Sai Baba Reddy Velugoti",
      "Soumen Chakrabarti",
      "Abir\n      De"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20784",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20784/20543",
    "published": "2022-02",
    "summary": "Given a query graph and a database of corpus graphs, a graph retrieval system aims to deliver the most relevant corpus graphs. Graph retrieval based on subgraph matching has a wide variety of applications, e.g., molecular fingerprint detection, circuit design, software analysis, and question answering. In such applications, a corpus graph is relevant to a query graph, if the query graph is (perfectly or approximately) a subgraph of the corpus graph. Existing neural graph retrieval models compare the node or graph embeddings of the query-corpus pairs, to compute the relevance scores between them. However, such models may not provide edge consistency between the query and corpus graphs. Moreover, they predominantly use symmetric relevance scores, which are not appropriate in the context of subgraph matching, since the underlying relevance score in subgraph search should be measured using the partial order induced by subgraph-supergraph relationship. Consequently, they show poor retrieval performance in the context of subgraph matching. In response, we propose ISONET, a novel interpretable neural edge alignment formulation, which is better able to learn the edge-consistent mapping necessary for subgraph matching. ISONET incorporates a new scoring mechanism which enforces an asymmetric relevance score, specifically tailored to subgraph matching. ISONET\u2019s design enables it to directly identify the underlying subgraph in a corpus graph, which is relevant to the given query graph. Our experiments on diverse datasets show that ISONET outperforms recent graph retrieval formulations and systems. Additionally, ISONET can provide interpretable alignments between query-corpus graph pairs during inference, despite being trained only using binary relevance labels of whole graphs during training, without any fine-grained ground truth information about node or edge alignments.",
    "code_link": "https://github.com/Indradyumna/ISONET"
  },
  "aaai2022_main_fedsoftsoftclusteredfederatedlearningwithproximallocalupdating": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FedSoft: Soft Clustered Federated Learning with Proximal Local Updating",
    "authors": [
      "Yichen Ruan",
      "Carlee Joe-Wong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20785",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20785/20544",
    "published": "2022-02",
    "summary": "Traditionally, clustered federated learning groups clients with the same data distribution into a cluster, so that every client is uniquely associated with one data distribution and helps train a model for this distribution. We relax this hard association assumption to soft clustered federated learning, which allows every local dataset to follow a mixture of multiple source distributions. We propose FedSoft, which trains both locally personalized models and high-quality cluster models in this setting. FedSoft limits client workload by using proximal updates to require the completion of only one optimization task from a subset of clients in every communication round. We show, analytically and empirically, that FedSoft effectively exploits similarities between the source distributions to learn personalized and cluster models that perform well.",
    "code_link": ""
  },
  "aaai2022_main_knowledgedistillationviaconstrainedvariationalinference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Knowledge Distillation via Constrained Variational Inference",
    "authors": [
      "Ardavan Saeedi",
      "Yuria Utsumi",
      "Li Sun",
      "Kayhan Batmanghelich",
      "Li-wei Lehman"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20786",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20786/20545",
    "published": "2022-02",
    "summary": "Knowledge distillation has been used to capture the knowledge of a teacher model and distill it into a student model with some desirable characteristics such as being smaller, more efficient, or more generalizable. In this paper, we propose a framework for distilling the knowledge of a powerful discriminative model such as a neural network into commonly used graphical models known to be more interpretable (e.g., topic models, autoregressive Hidden Markov Models). Posterior of latent variables in these graphical models (e.g., topic proportions in topic models) is often used as feature representation for predictive tasks. However, these posterior-derived features are known to have poor predictive performance compared to the features learned via purely discriminative approaches. Our framework constrains variational inference for posterior variables in graphical models with a similarity preserving constraint. This constraint distills the knowledge of the discriminative model into the graphical model by ensuring that input pairs with (dis)similar representation in the teacher model also have (dis)similar representation in the student model. By adding this constraint to the variational inference scheme, we guide the graphical model to be a reasonable density model for the data while having predictive features which are as close as possible to those of a discriminative model. To make our framework applicable to a wide range of graphical models, we build upon the Automatic Differentiation Variational Inference (ADVI), a black-box inference framework for graphical models. We demonstrate the effectiveness of our framework on two real-world tasks of disease subtyping and disease trajectory modeling.",
    "code_link": ""
  },
  "aaai2022_main_hypergraphmodelingviaspectralembeddingconnectionhypergraphcut,weightedkernelk-means,andheatkernel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hypergraph Modeling via Spectral Embedding Connection: Hypergraph Cut, Weighted Kernel k-Means, and Heat Kernel",
    "authors": [
      "Shota Saito"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20787",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20787/20546",
    "published": "2022-02",
    "summary": "We propose a theoretical framework of multi-way similarity to model real-valued data into hypergraphs for clustering via spectral embedding.For graph cut based spectral clustering, it is common to model real-valued data into graph by modeling pairwise similarities using kernel function.This is because the kernel function has a theoretical connection to the graph cut.For problems where using multi-way similarities are more suitable than pairwise ones, it is natural to model as a hypergraph, which is generalization of a graph.However, although the hypergraph cut is well-studied, there is not yet established a hypergraph cut based framework to model multi-way similarity.In this paper, we formulate multi-way similarities by exploiting the theoretical foundation of kernel function.We show a theoretical connection between our formulation and hypergraph cut in two ways, generalizing both weighted kernel k-means and the heat kernel, by which we justify our formulation.We also provide a fast algorithm for spectral clustering.Our algorithm empirically shows better performance than existing graph and other heuristic modeling methods.",
    "code_link": "https://github.com/ShotaSAITO/HypergraphModeling"
  },
  "aaai2022_main_reversedifferentiationviapredictivecoding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reverse Differentiation via Predictive Coding",
    "authors": [
      "Tommaso Salvatori",
      "Yuhang Song",
      "Zhenghua Xu",
      "Thomas Lukasiewicz",
      "Rafal\n      Bogacz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20788",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20788/20547",
    "published": "2022-02",
    "summary": "Deep learning has redefined AI thanks to the rise of artificial neural networks, which are inspired by neurological networks in the brain. Through the years, this dualism between AI and neuroscience has brought immense benefits to both fields, allowing neural networks to be used in a plethora of applications. Neural networks use an efficient implementation of reverse differentiation, called backpropagation (BP). This algorithm, however, is often criticized for its biological implausibility (e.g., lack of local update rules for the parameters). Therefore, biologically plausible learning methods that rely on predictive coding (PC), a framework for describing information processing in the brain, are increasingly studied. Recent works prove that these methods can approximate BP up to a certain margin on multilayer perceptrons (MLPs), and asymptotically on any other complex model, and that zero-divergence inference learning (Z-IL), a variant of PC, is able to exactly implement BP on MLPs. However, the recent literature shows also that there is no biologically plausible method yet that can exactly replicate the weight update of BP on complex models. To fill this gap, in this paper, we generalize (PC and) Z-IL by directly defining it on computational graphs, and show that it can perform exact reverse differentiation. What results is the first PC (and so biologically plausible) algorithm that is equivalent to BP in the way of updating parameters on any neural network, providing a bridge between the interdisciplinary research of neuroscience and deep learning. Furthermore, the above results in particular also immediately provide a novel local and parallel implementation of BP.",
    "code_link": ""
  },
  "aaai2022_main_vacadesigningvariationalgraphautoencodersforcausalqueries": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "VACA: Designing Variational Graph Autoencoders for Causal Queries",
    "authors": [
      "Pablo S\u00e1nchez-Martin",
      "Miriam Rateike",
      "Isabel Valera"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20789",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20789/20548",
    "published": "2022-02",
    "summary": "In this paper, we introduce VACA, a novel class of variational graph autoencoders for causal inference in the absence of hidden confounders, when only observational data and the causal graph are available. Without making any parametric assumptions, VACA mimics the necessary properties of a Structural Causal Model (SCM) to provide a flexible and practical framework for approximating interventions (do-operator) and abduction-action-prediction steps. As a result, and as shown by our empirical results, VACA accurately approximates the interventional and counterfactual distributions on diverse SCMs. Finally, we apply VACA to evaluate counterfactual fairness in fair classification problems, as well as to learn fair classifiers without compromising performance.",
    "code_link": "https://github.com/psanch21/VACA"
  },
  "aaai2022_main_verificationofneural-networkcontrolsystemsbyintegratingtaylormodelsandzonotopes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Verification of Neural-Network Control Systems by Integrating Taylor Models and Zonotopes",
    "authors": [
      "Christian Schilling",
      "Marcelo Forets",
      "Sebasti\u00e1n Guadalupe"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20790",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20790/20549",
    "published": "2022-02",
    "summary": "We study the verification problem for closed-loop dynamical systems with neural-network controllers (NNCS). This problem is commonly reduced to computing the set of reachable states. When considering dynamical systems and neural networks in isolation, there exist precise approaches for that task based on set representations respectively called Taylor models and zonotopes. However, the combination of these approaches to NNCS is non-trivial because, when converting between the set representations, dependency information gets lost in each control cycle and the accumulated approximation error quickly renders the result useless. We present an algorithm to chain approaches based on Taylor models and zonotopes, yielding a precise reachability algorithm for NNCS. Because the algorithm only acts at the interface of the isolated approaches, it is applicable to general dynamical systems and neural networks and can benefit from future advances in these areas. Our implementation delivers state-of-the-art performance and is the first to successfully analyze all benchmark problems of an annual reachability competition for NNCS.",
    "code_link": "https://github.com/JuliaReach/AAAI22"
  },
  "aaai2022_main_scalingupinfluencefunctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Scaling Up Influence Functions",
    "authors": [
      "Andrea Schioppa",
      "Polina Zablotskaia",
      "David Vilar",
      "Artem Sokolov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20791",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20791/20550",
    "published": "2022-02",
    "summary": "We address efficient calculation of influence functions for tracking predictions back to the training data. We propose and analyze a new approach to speeding up the inverse Hessian calculation based on Arnoldi iteration. With this improvement, we achieve, to the best of our knowledge, the first successful implementation of influence functions that scales to full-size (language and vision) Transformer models with several hundreds of millions of parameters. We evaluate our approach in image classification and sequence-to-sequence tasks with tens to a hundred of millions of training examples. Our code is available at https://github.com/google-research/jax-influence.",
    "code_link": ""
  },
  "aaai2022_main_chainingvaluefunctionsforoff-policylearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Chaining Value Functions for Off-Policy Learning",
    "authors": [
      "Simon Schmitt",
      "John Shawe-Taylor",
      "Hado van Hasselt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20792",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20792/20551",
    "published": "2022-02",
    "summary": "To accumulate knowledge and improve its policy of behaviour, a reinforcement learning agent can learn `off-policy' about policies that differ from the policy used to generate its experience. This is important to learn counterfactuals, or because the experience was generated out of its own control. However, off-policy learning is non-trivial, and standard reinforcement-learning algorithms can be unstable and divergent.In this paper we discuss a novel family of off-policy prediction algorithms which are convergent by construction. The idea is to first learn on-policy about the data-generating behaviour, and then bootstrap an off-policy value estimate on this on-policy estimate, thereby constructing a value estimate that is partially off-policy. This process can be repeated to build a chain of value functions, each time bootstrapping a new estimate on the previous estimate in the chain. Each step in the chain is stable and hence the complete algorithm is guaranteed to be stable. Under mild conditions this comes arbitrarily close to the off-policy TD solution when we increase the length of the chain. Hence it can compute the solution even in cases where off-policy TD diverges. We prove that the proposed scheme is convergent and corresponds to an iterative decomposition of the inverse key matrix. Furthermore it can be interpreted as estimating a novel objective -- that we call a `k-step expedition' -- of following the target policy for finitely many steps before continuing indefinitely with the behaviour policy. Empirically we evaluate the idea on challenging MDPs such as Baird's counter example and observe favourable results.",
    "code_link": ""
  },
  "aaai2022_main_graphfiltrationkernels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph Filtration Kernels",
    "authors": [
      "Till Schulz",
      "Pascal Welke",
      "Stefan Wrobel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20793",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20793/20552",
    "published": "2022-02",
    "summary": "The majority of popular graph kernels is based on the concept of Haussler's R-convolution kernel and defines graph similarities in terms of mutual substructures.In this work, we enrich these similarity measures by considering graph filtrations: Using meaningful orders on the set of edges, which allow to construct a sequence of nested graphs, we can consider a graph at multiple granularities.A key concept of our approach is to track graph features over the course of such graph resolutions.Rather than to simply compare frequencies of features in graphs, this allows for their comparison in terms of when and for how long they exist in the sequences.In this work, we propose a family of graph kernels that incorporate these existence intervals of features.While our approach can be applied to arbitrary graph features, we particularly highlight Weisfeiler-Lehman vertex labels, leading to efficient kernels.We show that using Weisfeiler-Lehman labels over certain filtrations strictly increases the expressive power over the ordinary Weisfeiler-Lehman procedure in terms of deciding graph isomorphism. In fact, this result directly yields more powerful graph kernels based on such features and has implications to graph neural networks due to their close relationship to the Weisfeiler-Lehman method. We empirically validate the expressive power of our graph kernels and show significant improvements over state-of-the-art graph kernels in terms of predictive performance on various real-world benchmark datasets.",
    "code_link": "https://github.com/mlai-bonn/wl-filtration-kernel"
  },
  "aaai2022_main_neuralnetworksclassifythroughtheclass-wisemeansoftheirrepresentations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neural Networks Classify through the Class-Wise Means of Their Representations",
    "authors": [
      "Mohamed El Amine Seddik",
      "Mohamed Tamaazousti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20794",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20794/20553",
    "published": "2022-02",
    "summary": "In this paper, based on an asymptotic analysis of the Softmax layer, we show that when training neural networks for classification tasks, the weight vectors corre sponding to each class of the Softmax layer tend to converge to the class-wise means computed at the representation layer (for specific choices of the representation activation). We further show some consequences of our findings to the context of transfer learning, essentially by proposing a simple yet effective initialization procedure that significantly accelerates the learning of the Softmax layer weights as the target domain gets closer to the source one. Experiments are notably performed on the datasets: MNIST, Fashion MNIST, Cifar10, and Cifar100 and using a standard CNN architecture.",
    "code_link": ""
  },
  "aaai2022_main_neuro-symbolicinductivelogicprogrammingwithlogicalneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks",
    "authors": [
      "Prithviraj Sen",
      "Breno W. S. R. de Carvalho",
      "Ryan Riegel",
      "Alexander Gray"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20795",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20795/20554",
    "published": "2022-02",
    "summary": "Recent work on neuro-symbolic inductive logic programming has led to promising approaches that can learn explanatory rules from noisy, real-world data. While some proposals approximate logical operators with differentiable operators from fuzzy or real-valued logic that are parameter-free thus diminishing their capacity to fit the data, other approaches are only loosely based on logic making it difficult to interpret the learned ``rules\". In this paper, we propose learning rules with the recently proposed logical neural networks (LNN). Compared to others, LNNs offer a strong connection to classical Boolean logic thus allowing for precise interpretation of learned rules while harboring parameters that can be trained with gradient-based optimization to effectively fit the data. We extend LNNs to induce rules in first-order logic. Our experiments on standard benchmarking tasks confirm that LNN rules are highly interpretable and can achieve comparable or higher accuracy due to their flexible parameterization.",
    "code_link": "https://github.com/shehzaadzd/MINERVA"
  },
  "aaai2022_main_max-margincontrastivelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Max-Margin Contrastive Learning",
    "authors": [
      "Anshul Shah",
      "Suvrit Sra",
      "Rama Chellappa",
      "Anoop Cherian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20796",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20796/20555",
    "published": "2022-02",
    "summary": "Standard contrastive learning approaches usually require a large number of negatives for effective unsupervised learning and often exhibit slow convergence. We suspect this behavior is due to the suboptimal selection of negatives used for offering contrast to the positives. We counter this difficulty by taking inspiration from support vector machines (SVMs) to present max-margin contrastive learning (MMCL). Our approach selects negatives as the sparse support vectors obtained via a quadratic optimization problem, and contrastiveness is enforced by maximizing the decision margin. As SVM optimization can be computationally demanding, especially in an end-to-end setting, we present simplifications that alleviate the computational burden. We validate our approach on standard vision benchmark datasets, demonstrating better performance in unsupervised representation learning over state-of-the-art, while having better empirical convergence properties.",
    "code_link": "https://github.com/anshulbshah/MMCL"
  },
  "aaai2022_main_learningtotransferwithvonneumannconditionaldivergence": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Transfer with von Neumann Conditional Divergence",
    "authors": [
      "Ammar Shaker",
      "Shujian Yu",
      "Daniel O\u00f1oro-Rubio"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20797",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20797/20556",
    "published": "2022-02",
    "summary": "The similarity of feature representations plays a pivotal role in the success of problems related to domain adaptation. Feature similarity includes both the invariance of marginal distributions and the closeness of conditional distributions given the desired response y (e.g., class labels). Unfortunately, traditional methods always learn such features without fully taking into consideration the information in y, which in turn may lead to a mismatch of the conditional distributions or the mixup of discriminative structures underlying data distributions. In this work, we introduce the recently proposed von Neumann conditional divergence to improve the transferability across multiple domains. We show that this new divergence is differentiable and eligible to easily quantify the functional dependence between features and y. Given multiple source tasks, we integrate this divergence to capture discriminative information in y and design novel learning objectives assuming those source tasks are observed either simultaneously or sequentially. In both scenarios, we obtain favorable performance against state-of-the-art methods in terms of smaller generalization error on new tasks and less catastrophic forgetting on source tasks (in the sequential setup).",
    "code_link": ""
  },
  "aaai2022_main_onlineapprenticeshiplearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Apprenticeship Learning",
    "authors": [
      "Lior Shani",
      "Tom Zahavy",
      "Shie Mannor"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20798",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20798/20557",
    "published": "2022-02",
    "summary": "In Apprenticeship Learning (AL), we are given a Markov Decision Process (MDP) without access to the cost function. Instead, we observe trajectories sampled by an expert that acts according to some policy. The goal is to find a policy that matches the expert's performance on some predefined set of cost functions.We introduce an online variant of AL (Online Apprenticeship Learning; OAL), where the agent is expected to perform comparably to the expert while interacting with the environment. We show that the OAL problem can be effectively solved by combining two mirror descent based no-regret algorithms: one for policy optimization and another for learning the worst case cost. By employing optimistic exploration, we derive a convergent algorithm with O(sqrt(K)) regret, where K is the number of interactions with the MDP, and an additional linear error term that depends on the amount of expert trajectories available. Importantly, our algorithm avoids the need to solve an MDP at each iteration, making it more practical compared to prior AL methods. Finally, we implement a deep variant of our algorithm which shares some similarities to GAIL, but where the discriminator is replaced with the costs learned by OAL. Our simulations suggest that OAL performs well in high dimensional control problems.",
    "code_link": "https://github.com/hill-a/stablebaselines"
  },
  "aaai2022_main_hod-nethigh-orderdifferentiabledeepneuralnetworksandapplications": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "HoD-Net: High-Order Differentiable Deep Neural Networks and Applications",
    "authors": [
      "Siyuan Shen",
      "Tianjia Shao",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Feng Luo",
      "Yin Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20799",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20799/20558",
    "published": "2022-02",
    "summary": "We introduce a deep architecture named HoD-Net to enable high-order differentiability for deep learning. HoD-Net is based on and generalizes the complex-step finite difference (CSFD) method. While similar to classic finite difference, CSFD approaches the derivative of a function from a higher-dimension complex domain, leading to highly accurate and robust differentiation computation without numerical stability issues. This method can be coupled with backpropagation and adjoint perturbation methods for an efficient calculation of high-order derivatives. We show how this numerical scheme can be leveraged in challenging deep learning problems, such as high-order network training, deep learning-based physics simulation, and neural differential equations.",
    "code_link": ""
  },
  "aaai2022_main_conditionalgenerativemodelbasedpredicate-awarequeryapproximation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Conditional Generative Model Based Predicate-Aware Query Approximation",
    "authors": [
      "Nikhil Sheoran",
      "Subrata Mitra",
      "Vibhor Porwal",
      "Siddharth Ghetia",
      "Jatin\n      Varshney",
      "Tung Mai",
      "Anup Rao",
      "Vikas Maddukuri"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20800",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20800/20559",
    "published": "2022-02",
    "summary": "The goal of Approximate Query Processing (AQP) is to provide very fast but \"accurate enough\" results for costly aggregate queries thereby improving user experience in interactive exploration of large datasets. Recently proposed Machine-Learning-based AQP techniques can provide very low latency as query execution only involves model inference as compared to traditional query processing on database clusters. However, with increase in the number of filtering predicates (WHERE clauses), the approximation error significantly increases for these methods. Analysts often use queries with a large number of predicates for insights discovery. Thus, maintaining low approximation error is important to prevent analysts from drawing misleading conclusions. In this paper, we propose ELECTRA, a predicate-aware AQP system that can answer analytics-style queries with a large number of predicates with much smaller approximation errors. ELECTRA uses a conditional generative model that learns the conditional distribution of the data and at run-time generates a small (\u2248 1000 rows) but representative sample, on which the query is executed to compute the approximate result. Our evaluations with four different baselines on three real-world datasets show that ELECTRA provides lower AQP error for large number of predicates compared to baselines.",
    "code_link": ""
  },
  "aaai2022_main_learningboundedcontext-free-grammarvialstmandthetransformerdifferenceandtheexplanations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and the Explanations",
    "authors": [
      "Hui Shi",
      "Sicun Gao",
      "Yuandong Tian",
      "Xinyun Chen",
      "Jishen Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20801",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20801/20560",
    "published": "2022-02",
    "summary": "Long Short-Term Memory (LSTM) and Transformers are two popular neural architectures used for natural language processing tasks. Theoretical results show that both are Turing-complete and can represent any context-free language (CFL).In practice, it is often observed that Transformer models have better representation power than LSTM. But the reason is barely understood. We study such practical differences between LSTM and Transformer and propose an explanation based on their latent space decomposition patterns. To achieve this goal, we introduce an oracle training paradigm, which forces the decomposition of the latent representation of LSTMand the Transformer and supervises with the transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With the forced decomposition, we show that the performance upper bounds of LSTM and Transformer in learning CFL are close: both of them can simulate a stack and perform stack operation along with state transitions. However, the absence of forced decomposition leads to the failure of LSTM models to capture the stack and stack operations, while having a marginal impact on the Transformer model. Lastly, we connect the experiment on the prototypical PDA to a real-world parsing task to re-verify the conclusions",
    "code_link": "https://github.com/shihui2010/learn"
  },
  "aaai2022_main_shapepriorguidedattacksparserperturbationson3dpointclouds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shape Prior Guided Attack: Sparser Perturbations on 3D Point Clouds",
    "authors": [
      "Zhenbo Shi",
      "Zhi Chen",
      "Zhenbo Xu",
      "Wei Yang",
      "Zhidong Yu",
      "Liusheng Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20802",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20802/20561",
    "published": "2022-02",
    "summary": "Deep neural networks are extremely vulnerable to malicious input data. As 3D data is increasingly used in vision tasks such as robots, autonomous driving and drones, the internal robustness of the classification models for 3D point cloud has received widespread attention. In this paper, we propose a novel method named SPGA (Shape Prior Guided Attack) to generate adversarial point cloud examples. We use shape prior information to make perturbations sparser and thus achieve imperceptible attacks. In particular, we propose a Spatially Logical Block (SLB) to apply adversarial points through sliding in the oriented bounding box. Moreover, we design an algorithm called FOFA for this type of task, which further refines the adversarial attack in the process of breaking down complicated problems into sub-problems. Compared with the methods of global perturbation, our attack method consumes significantly fewer computations, making it more efficient. Most importantly of all, SPGA can generate examples with a higher attack success rate (even in a defensive situation), less perturbation budget and stronger transferability.",
    "code_link": ""
  },
  "aaai2022_main_trflearningkernelswithtunedrandomfeatures": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TRF: Learning Kernels with Tuned Random Features",
    "authors": [
      "Alistair Shilton",
      "Sunil Gupta",
      "Santu Rana",
      "Arun Kumar Venkatesh",
      "Svetha\n      Venkatesh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20803",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20803/20562",
    "published": "2022-02",
    "summary": "Random Fourier features (RFF) are a popular set of tools for constructing low-dimensional approximations of translation-invariant kernels, allowing kernel methods to be scaled to big data.Apart from their computational advantages, by working in the spectral domain random Fourier features expose the translation invariant kernel as a density function that may, in principle, be manipulated directly to tune the kernel.In this paper we propose selecting the density function from a reproducing kernel Hilbert space to allow us to search the space of all translation-invariant kernels.Our approach, which we call tuned random features (TRF), achieves this by approximating the density function as the RKHS-norm regularised least-squares best fit to an unknown ``true'' optimal density function, resulting in a RFF formulation where kernel selection is reduced to regularised risk minimisation with a novel regulariser.We derive bounds on the Rademacher complexity for our method showing that our random features approximation method converges to optimal kernel selection in the large N,D limit.Finally, we prove experimental results for a variety of real-world learning problems, demonstrating the performance of our approach compared to comparable methods.",
    "code_link": ""
  },
  "aaai2022_main_estimationoflocalaveragetreatmenteffectbydatacombination": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Estimation of Local Average Treatment Effect by Data Combination",
    "authors": [
      "Kazuhiko Shinoda",
      "Takahiro Hoshino"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20804",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20804/20563",
    "published": "2022-02",
    "summary": "It is important to estimate the local average treatment effect (LATE) when compliance with a treatment assignment is incomplete. The previously proposed methods for LATE estimation required all relevant variables to be jointly observed in a single dataset; however, it is sometimes difficult or even impossible to collect such data in many real-world problems for technical or privacy reasons. We consider a novel problem setting in which LATE, as a function of covariates, is nonparametrically identified from the combination of separately observed datasets. For estimation, we show that the direct least squares method, which was originally developed for estimating the average treatment effect under complete compliance, is applicable to our setting. However, model selection and hyperparameter tuning for the direct least squares estimator can be unstable in practice since it is defined as a solution to the minimax problem. We then propose a weighted least squares estimator that enables simpler model selection by avoiding the minimax objective formulation. Unlike the inverse probability weighted (IPW) estimator, the proposed estimator directly uses the pre-estimated weight without inversion, avoiding the problems caused by the IPW methods. We demonstrate the effectiveness of our method through experiments using synthetic and real-world datasets.",
    "code_link": "https://github.com/kazushino/AAAI22"
  },
  "aaai2022_main_constraint-drivenexplanationsforblack-boxmlmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Constraint-Driven Explanations for Black-Box ML Models",
    "authors": [
      "Aditya A. Shrotri",
      "Nina Narodytska",
      "Alexey Ignatiev",
      "Kuldeep S Meel",
      "Joao\n      Marques-Silva",
      "Moshe Y. Vardi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20805",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20805/20564",
    "published": "2022-02",
    "summary": "The need to understand the inner workings of opaque Machine Learning models has prompted researchers to devise various types of post-hoc explanations. A large class of such explainers proceed in two phases: first perturb an input instance whose explanation is sought, and then generate an interpretable artifact to explain the prediction of the opaque model on that instance. Recently, Deutch and Frost proposed to use an additional input from the user: a set of constraints over the input space to guide the perturbation phase. While this approach affords the user the ability to tailor the explanation to their needs, striking a balance between flexibility, theoretical rigor and computational cost has remained an open challenge.We propose a novel constraint-driven explanation generation approach which simultaneously addresses these issues in a modular fashion. Our framework supports the use of expressive Boolean constraints giving the user more flexibility to specify the subspace to generate perturbations from. Leveraging advances in Formal Methods, we can theoretically guarantee strict adherence of the samples to the desired distribution. This also allows us to compute fidelity in a rigorous way, while scaling much better in practice. Our empirical study demonstrates concrete uses of our tool CLIME in obtaining more meaningful explanations with high fidelity.",
    "code_link": "https://gitlab.com/Shrotri/clime"
  },
  "aaai2022_main_noise-robustlearningfrommultipleunsupervisedsourcesofinferredlabels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Noise-Robust Learning from Multiple Unsupervised Sources of Inferred Labels",
    "authors": [
      "Amila Silva",
      "Ling Luo",
      "Shanika Karunasekera",
      "Christopher Leckie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20806",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20806/20565",
    "published": "2022-02",
    "summary": "Deep Neural Networks (DNNs) generally require large-scale datasets for training. Since manually obtaining clean labels for large datasets is extremely expensive, unsupervised models based on domain-specific heuristics can be used to efficiently infer the labels for such datasets. However, the labels from such inferred sources are typically noisy, which could easily mislead and lessen the generalizability of DNNs. Most approaches proposed in the literature to address this problem assume the label noise depends only on the true class of an instance (i.e., class-conditional noise). However, this assumption is not realistic for the inferred labels as they are typically inferred based on the features of the instances. The few recent attempts to model such instance-dependent (i.e., feature-dependent) noise require auxiliary information about the label noise (e.g., noise rates or clean samples). This work proposes a theoretically motivated framework to correct label noise in the presence of multiple labels inferred from unsupervised models. The framework consists of two modules: (1) MULTI-IDNC, a novel approach to correct label noise that is instance-dependent yet not class-conditional; (2) MULTI-CCNC, which extends an existing class-conditional noise-robust approach to yield improved class-conditional noise correction using multiple noisy label sources. We conduct experiments using nine real-world datasets for three different classification tasks (images, text and graph nodes). Our results show that our approach achieves notable improvements (e.g., 6.4% in accuracy) against state-of-the-art baselines while dealing with both instance-dependent and class-conditional noise in inferred label sources.",
    "code_link": ""
  },
  "aaai2022_main_quilteffectivemulti-classclassificationonquantumcomputersusinganensembleofdiversequantumclassifiers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers",
    "authors": [
      "Daniel Silver",
      "Tirthak Patel",
      "Devesh Tiwari"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20807",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20807/20566",
    "published": "2022-02",
    "summary": "Quantum computers can theoretically have significant acceleration over classical computers; but, the near-future era of quantum computing is limited due to small number of qubits that are also error prone. QUILT is a framework for performing multi-class classification task designed to work effectively on current error-prone quantum computers. QUILT is evaluated with real quantum machines as well as with projected noise levels as quantum machines become more noise free. QUILT demonstrates up to 85% multi-class classification accuracy with the MNIST dataset on a five-qubit system.",
    "code_link": ""
  },
  "aaai2022_main_eqgnnequalizednodeopportunityingraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "EqGNN: Equalized Node Opportunity in Graphs",
    "authors": [
      "Uriel Singer",
      "Kira Radinsky"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20808",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20808/20567",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs), has been widely used for supervised learning tasks in graphs reaching state-of-the-art results. However, little work was dedicated to creating unbiased GNNs, i.e., where the classification is uncorrelated with sensitive attributes, such as race or gender. Some ignore the sensitive attributes or optimize for the criteria of statistical parity for fairness. However, it has been shown that neither approaches ensure fairness, but rather cripple the utility of the prediction task. In this work, we present a GNN framework that allows optimizing representations for the notion of Equalized Odds fairness criteria. The architecture is composed of three components: (1) a GNN classifier predicting the utility class, (2) a sampler learning the distribution of the sensitive attributes of the nodes given their labels. It generates samples fed into a (3) discriminator that discriminates between true and sampled sensitive attributes using a novel ``permutation loss'' function. Using these components, we train a model to neglect information regarding the sensitive attribute only with respect to its label. To the best of our knowledge, we are the first to optimize GNNs for the equalized odds criteria. We evaluate our classifier over several graph datasets and sensitive attributes and show our algorithm reaches state-of-the-art results.",
    "code_link": "https://github.com/urielsinger/EqGNN"
  },
  "aaai2022_main_approxiferamodel-agnosticapproachtoresilientandrobustpredictionservingsystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems",
    "authors": [
      "Mahdi Soleymani",
      "Ramy E. Ali",
      "Hessam Mahdavifar",
      "A. Salman Avestimehr"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20809",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20809/20568",
    "published": "2022-02",
    "summary": "Due to the surge of cloud-assisted AI services, the problem of designing resilient prediction serving systems that can effectively cope with stragglers and minimize response delays has attracted much interest. The common approach for tackling this problem is replication which assigns the same prediction task to multiple workers. This approach, however, isinefficient and incurs significant resource overheads. Hence, a learning-based approach known as parity model (ParM) has been recently proposed which learns models that can generate ``parities\u2019\u2019 for a group of predictionsto reconstruct the predictions of the slow/failed workers. While this learning-based approach is more resource-efficient than replication, it is tailored to the specific model hosted by the cloud and is particularly suitable for a small number of queries (typically less than four) and tolerating very few stragglers (mostly one). Moreover, ParM does not handle Byzantine adversarial workers. We propose a different approach, named Approximate Coded Inference (ApproxIFER), that does not require training any parity models, hence it is agnostic to the model hosted by the cloud and can be readily applied to different data domains and model architectures. Compared with earlier works, ApproxIFER can handle a general number of stragglers and scales significantly better with thenumber of queries. Furthermore, ApproxIFER is robust against Byzantine workers. Our extensive experiments on a large number of datasets and model architectures show significant degraded mode accuracy improvement by up to 58% over ParM.",
    "code_link": ""
  },
  "aaai2022_main_featureimportanceexplanationsfortemporalblack-boxmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Feature Importance Explanations for Temporal Black-Box Models",
    "authors": [
      "Akshay Sood",
      "Mark Craven"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20810",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20810/20569",
    "published": "2022-02",
    "summary": "Models in the supervised learning framework may capture rich and complex representations over the features that are hard for humans to interpret. Existing methods to explain such models are often specific to architectures and data where the features do not have a time-varying component. In this work, we propose TIME, a method to explain models that are inherently temporal in nature. Our approach (i) uses a model-agnostic permutation-based approach to analyze global feature importance, (ii) identifies the importance of salient features with respect to their temporal ordering as well as localized windows of influence, and (iii) uses hypothesis testing to provide statistical rigor.",
    "code_link": ""
  },
  "aaai2022_main_reward-weightedregressionconvergestoaglobaloptimum": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reward-Weighted Regression Converges to a Global Optimum",
    "authors": [
      "Miroslav \u0160trupl",
      "Francesco Faccio",
      "Dylan R. Ashley",
      "Rupesh Kumar\n      Srivastava",
      "J\u00fcrgen Schmidhuber"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20811",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20811/20570",
    "published": "2022-02",
    "summary": "Reward-Weighted Regression (RWR) belongs to a family of widely known iterative Reinforcement Learning algorithms based on the Expectation-Maximization framework. In this family, learning at each iteration consists of sampling a batch of trajectories using the current policy and fitting a new policy to maximize a return-weighted log-likelihood of actions. Although RWR is known to yield monotonic improvement of the policy under certain circumstances, whether and under which conditions RWR converges to the optimal policy have remained open questions. In this paper, we provide for the first time a proof that RWR converges to a global optimum when no function approximation is used, in a general compact setting. Furthermore, for the simpler case with finite state and action spaces we prove R-linear convergence of the state-value function to the optimum.",
    "code_link": "https://github.com/dylanashley/rewardweighted-regression"
  },
  "aaai2022_main_gradient-basednoveltydetectionboostedbyself-supervisedbinaryclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Gradient-Based Novelty Detection Boosted by Self-Supervised Binary Classification",
    "authors": [
      "Jingbo Sun",
      "Li Yang",
      "Jiaxin Zhang",
      "Frank Liu",
      "Mahantesh Halappanavar",
      "Deliang Fan",
      "Yu Cao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20812",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20812/20571",
    "published": "2022-02",
    "summary": "Novelty detection aims to automatically identify out-of-distribution (OOD) data, without any prior knowledge of them. It is a critical step in data monitoring, behavior analysis and other applications, helping enable continual learning in the field. Conventional methods of OOD detection perform multi-variate analysis on an ensemble of data or features, and usually resort to the supervision with OOD data to improve the accuracy. In reality, such supervision is impractical as one cannot anticipate the anomalous data. In this paper, we propose a novel, self-supervised approach that does not rely on any pre-defined OOD data: (1) The new method evaluates the Mahalanobis distance of the gradients between the in-distribution and OOD data. (2) It is assisted by a self-supervised binary classifier to guide the label selection to generate the gradients, and maximize the Mahalanobis distance. In the evaluation with multiple datasets, such as CIFAR-10, CIFAR-100, SVHN and TinyImageNet, the proposed approach consistently outperforms state-of-the-art supervised and unsupervised methods in the area under the receiver operating characteristic (AUROC) and area under the precision-recall curve (AUPR) metrics. We further demonstrate that this detector is able to accurately learn one OOD class in continual learning.",
    "code_link": ""
  },
  "aaai2022_main_deterministicanddiscriminativeimitation(d2-imitation)revisitingadversarialimitationforsampleefficiency": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deterministic and Discriminative Imitation (D2-Imitation): Revisiting Adversarial Imitation for Sample Efficiency",
    "authors": [
      "Mingfei Sun",
      "Sam Devlin",
      "Katja Hofmann",
      "Shimon Whiteson"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20813",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20813/20572",
    "published": "2022-02",
    "summary": "Sample efficiency is crucial for imitation learning methods to be applicable in real-world applications. Many studies improve sample efficiency by extending adversarial imitation to be off-policy regardless of the fact that these off-policy extensions could either change the original objective or involve complicated optimization. We revisit the foundation of adversarial imitation and propose an off-policy sample efficient approach that requires no adversarial training or min-max optimization. Our formulation capitalizes on two key insights: (1) the similarity between the Bellman equation and the stationary state-action distribution equation allows us to derive a novel temporal difference (TD) learning approach; and (2) the use of a deterministic policy simplifies the TD learning. Combined, these insights yield a practical algorithm, Deterministic and Discriminative Imitation (D2-Imitation), which oper- ates by first partitioning samples into two replay buffers and then learning a deterministic policy via off-policy reinforcement learning. Our empirical results show that D2-Imitation is effective in achieving good sample efficiency, outperforming several off-policy extension approaches of adversarial imitation on many control tasks.",
    "code_link": ""
  },
  "aaai2022_main_exploitingmixedunlabeleddatafordetectingsamplesofseenandunseenout-of-distributionclasses": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploiting Mixed Unlabeled Data for Detecting Samples of Seen and Unseen Out-of-Distribution Classes",
    "authors": [
      "Yi-Xuan Sun",
      "Wei Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20814",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20814/20573",
    "published": "2022-02",
    "summary": "Out-of-Distribution (OOD) detection is essential in real-world applications, which has attracted increasing attention in recent years. However, most existing OOD detection methods require many labeled In-Distribution (ID) data, causing a heavy labeling cost. In this paper, we focus on the more realistic scenario, where limited labeled data and abundant unlabeled data are available, and these unlabeled data are mixed with ID and OOD samples. We propose the Adaptive In-Out-aware Learning (AIOL) method, in which we employ the appropriate temperature to adaptively select potential ID and OOD samples from the mixed unlabeled data and consider the entropy over them for OOD detection. Moreover, since the test data in realistic applications may contain OOD samples whose classes are not in the mixed unlabeled data (we call them unseen OOD classes), data augmentation techniques are brought into the method to further improve the performance. The experiments are conducted on various benchmark datasets, which demonstrate the superiority of our method.",
    "code_link": ""
  },
  "aaai2022_main_generalizedequivarianceandpreferentiallabelingforgnnnodeclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generalized Equivariance and Preferential Labeling for GNN Node Classification",
    "authors": [
      "Zeyu Sun",
      "Wenjie Zhang",
      "Lili Mou",
      "Qihao Zhu",
      "Yingfei Xiong",
      "Lu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20815",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20815/20574",
    "published": "2022-02",
    "summary": "Existing graph neural networks (GNNs) largely rely on node embeddings, which represent a node as a vector by its identity, type, or content. However, graphs with unattributed nodes widely exist in real-world applications (e.g., anonymized social networks). Previous GNNs either assign random labels to nodes (which introduces artefacts to the GNN) or assign one embedding to all nodes (which fails to explicitly distinguish one node from another). Further, when these GNNs are applied to unattributed node classification problems, they have an undesired equivariance property, which are fundamentally unable to address the data with multiple possible outputs. In this paper, we analyze the limitation of existing approaches to node classification problems. Inspired by our analysis, we propose a generalized equivariance property and a Preferential Labeling technique that satisfies the desired property asymptotically. Experimental results show that we achieve high performance in several unattributed node classification tasks.",
    "code_link": "https://github.com/zysszy/Preferential-Labeling"
  },
  "aaai2022_main_explainableandlocalcorrectionofclassificationmodelsusingdecisiontrees": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Explainable and Local Correction of Classification Models Using Decision Trees",
    "authors": [
      "Hirofumi Suzuki",
      "Hiroaki Iwashita",
      "Takuya Takagi",
      "Keisuke Goto",
      "Yuta\n      Fujishige",
      "Satoshi Hara"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20816",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20816/20575",
    "published": "2022-02",
    "summary": "In practical machine learning, models are frequently updated, or corrected, to adapt to new datasets. In this study, we pose two challenges to model correction. First, the effects of corrections to the end-users need to be described explicitly, similar to standard software where the corrections are described as release notes. Second, the amount of corrections need to be small so that the corrected models perform similarly to the old models. In this study, we propose the first model correction method for classification models that resolves these two challenges. Our idea is to use an additional decision tree to correct the output of the old models. Thanks to the explainability of decision trees, the corrections are describable to the end-users, which resolves the first challenge. We resolve the second challenge by incorporating the amount of corrections when training the additional decision tree so that the effects of corrections to be small. Experiments on real data confirm the effectiveness of the proposed method compared to existing correction methods.",
    "code_link": ""
  },
  "aaai2022_main_consistencyregularizationforadversarialrobustness": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Consistency Regularization for Adversarial Robustness",
    "authors": [
      "Jihoon Tack",
      "Sihyun Yu",
      "Jongheon Jeong",
      "Minseon Kim",
      "Sung Ju Hwang",
      "Jinwoo\n      Shin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20817",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20817/20576",
    "published": "2022-02",
    "summary": "Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary `consistency' regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial.",
    "code_link": ""
  },
  "aaai2022_main_regularizationguaranteesgeneralizationinbayesianreinforcementlearningthroughalgorithmicstability": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability",
    "authors": [
      "Aviv Tamar",
      "Daniel Soudry",
      "Ev Zisselman"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20818",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20818/20577",
    "published": "2022-02",
    "summary": "In the Bayesian reinforcement learning (RL) setting, a prior distribution over the unknown problem parameters -- the rewards and transitions -- is assumed, and a policy that optimizes the (posterior) expected return is sought. A common approximation, which has been recently popularized as meta-RL, is to train the agent on a sample of N problem instances from the prior, with the hope that for large enough N, good generalization behavior to an unseen test instance will be obtained. In this work, we study generalization in Bayesian RL under the probably approximately correct (PAC) framework, using the method of algorithmic stability. Our main contribution is showing that by adding regularization, the optimal policy becomes uniformly stable in an appropriate sense. Most stability results in the literature build on strong convexity of the regularized loss -- an approach that is not suitable for RL as Markov decision processes (MDPs) are not convex. Instead, building on recent results of fast convergence rates for mirror descent in regularized MDPs, we show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability. This result, which may be of independent interest, allows us to study the effect of regularization on generalization in the Bayesian RL setting.",
    "code_link": ""
  },
  "aaai2022_main_fedprotofederatedprototypelearningacrossheterogeneousclients": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FedProto: Federated Prototype Learning across Heterogeneous Clients",
    "authors": [
      "Yue Tan",
      "Guodong Long",
      "LU LIU",
      "Tianyi Zhou",
      "Qinghua Lu",
      "Jing Jiang",
      "Chengqi Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20819",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20819/20578",
    "published": "2022-02",
    "summary": "Heterogeneity across clients in federated learning (FL) usually hinders the optimization convergence and generalization performance when the aggregation of clients' knowledge occurs in the gradient space. For example, clients may differ in terms of data distribution, network latency, input/output space, and/or model architecture, which can easily lead to the misalignment of their local gradients. To improve the tolerance to heterogeneity, we propose a novel federated prototype learning (FedProto) framework in which the clients and server communicate the abstract class prototypes instead of the gradients. FedProto aggregates the local prototypes collected from different clients, and then sends the global prototypes back to all clients to regularize the training of local models. The training on each client aims to minimize the classification error on the local data while keeping the resulting local prototypes sufficiently close to the corresponding global ones. Moreover, we provide a theoretical analysis to the convergence rate of FedProto under non-convex objectives. In experiments, we propose a benchmark setting tailored for heterogeneous FL, with FedProto outperforming several recent FL approaches on multiple datasets.",
    "code_link": ""
  },
  "aaai2022_main_whataboutinputtingpolicyinvaluefunctionpolicyrepresentationandpolicy-extendedvaluefunctionapproximator": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "What about Inputting Policy in Value Function: Policy Representation and Policy-Extended Value Function Approximator",
    "authors": [
      "Hongyao Tang",
      "Zhaopeng Meng",
      "Jianye Hao",
      "Chen Chen",
      "Daniel Graves",
      "Dong\n      Li",
      "Changmin Yu",
      "Hangyu Mao",
      "Wulong Liu",
      "Yaodong Yang",
      "Wenyuan Tao",
      "Li\n      Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20820",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20820/20579",
    "published": "2022-02",
    "summary": "We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., value generalization among policies. We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or state-action pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks. For a representative instance of algorithm implementation, Proximal Policy Optimization (PPO) re-implemented under the paradigm of GPI with PeVFA achieves about 40% performance improvement on its vanilla counterpart in most environments.",
    "code_link": ""
  },
  "aaai2022_main_optimalsamplinggapsforadaptivesubmodularmaximization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimal Sampling Gaps for Adaptive Submodular Maximization",
    "authors": [
      "Shaojie Tang",
      "Jing Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20821",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20821/20580",
    "published": "2022-02",
    "summary": "Running machine learning algorithms on large and rapidly growing volumes of data is often computationally expensive, one common trick to reduce the size of a data set, and thus reduce the computational cost of machine learning algorithms, is probability sampling. It creates a sampled data set by including each data point from the original data set with a known probability. Although the benefit of running machine learning algorithms on the reduced data set is obvious, one major concern is that the performance of the solution obtained from samples might be much worse than that of the optimal solution when using the full data set.In this paper, we examine the performance loss caused byprobability sampling in the context of adaptive submodular maximization. We consider a simple probability sampling method which selects each data point with probability at least r.If we set r=1, our problem reduces to finding a solution based on the original full data set. We define sampling gap as the largest ratio between the optimal solution obtained from the full data set and the optimal solution obtained from the samples,over independence systems. Our main contribution is to show that if the sampling probability of each data point is at least r and the utility function is policywise submodular, then the sampling gap is both upper bounded and lower bounded by 1/r. We show that the property of policywise submodular can be found in a wide range of real-world applications, including pool-based active learning and adaptive viral marketing.",
    "code_link": ""
  },
  "aaai2022_main_withfalsefriendslikethese,whocannoticemistakes?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "With False Friends Like These, Who Can Notice Mistakes?",
    "authors": [
      "Lue Tao",
      "Lei Feng",
      "Jinfeng Yi",
      "Songcan Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20822",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20822/20581",
    "published": "2022-02",
    "summary": "Adversarial examples crafted by an explicit adversary have attracted significant attention in machine learning. However, the security risk posed by a potential false friend has been largely overlooked. In this paper, we unveil the threat of hypocritical examples---inputs that are originally misclassified yet perturbed by a false friend to force correct predictions. While such perturbed examples seem harmless, we point out for the first time that they could be maliciously used to conceal the mistakes of a substandard (i.e., not as good as required) model during an evaluation. Once a deployer trusts the hypocritical performance and applies the \"well-performed\" model in real-world applications, unexpected failures may happen even in benign environments. More seriously, this security risk seems to be pervasive: we find that many types of substandard models are vulnerable to hypocritical examples across multiple datasets. Furthermore, we provide the first attempt to characterize the threat with a metric called hypocritical risk and try to circumvent it via several countermeasures. Results demonstrate the effectiveness of the countermeasures, while the risk remains non-negligible even after adaptive robust training.",
    "code_link": ""
  },
  "aaai2022_main_poweringfinetuninginfew-shotlearningdomain-agnosticbiasreductionwithselectedsampling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling",
    "authors": [
      "Ran Tao",
      "Han Zhang",
      "Yutong Zheng",
      "Marios Savvides"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20823",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20823/20582",
    "published": "2022-02",
    "summary": "In recent works, utilizing a deep network trained on meta-training set serves as a strong baseline in few-shot learning. In this paper, we move forward to refine novel-class features by finetuning a trained deep network. Finetuning is designed to focus on reducing biases in novel-class feature distributions, which we define as two aspects: class-agnostic and class-specific biases. Class-agnostic bias is defined as the distribution shifting introduced by domain difference, which we propose Distribution Calibration Module(DCM) to reduce. DCM owes good property of eliminating domain difference and fast feature adaptation during optimization. Class-specific bias is defined as the biased estimation using a few samples in novel classes, which we propose Selected Sampling(SS) to reduce. Without inferring the actual class distribution, SS is designed by running sampling using proposal distributions around support-set samples. By powering finetuning with DCM and SS, we achieve state-of-the-art results on Meta-Dataset with consistent performance boosts over ten datasets from different domains. We believe our simple yet effective method demonstrates its possibility to be applied on practical few-shot applications.",
    "code_link": ""
  },
  "aaai2022_main_sminetstate-awaremulti-aspectinterestsrepresentationnetworkforcold-startusersrecommendation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SMINet: State-Aware Multi-Aspect Interests Representation Network for Cold-Start Users Recommendation",
    "authors": [
      "Wanjie Tao",
      "Yu Li",
      "Liangyue Li",
      "Zulong Chen",
      "Hong Wen",
      "Peilin Chen",
      "Tingting Liang",
      "Quan Lu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20824",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20824/20583",
    "published": "2022-02",
    "summary": "Online travel platforms (OTPs), e.g., bookings.com and Ctrip.com, deliver travel experiences to online users by providing travel-related products. Although much progress has been made, the state-of-the-arts for cold-start problems are largely sub-optimal for user representation, since they do not take into account the unique characteristics exhibited from user travel behaviors. In this work, we propose a State-aware Multi-aspect Interests representation Network (SMINet) for cold-start users recommendation at OTPs, which consists of a multi-aspect interests extractor, a co-attention layer, and a state-aware gating layer. The key component of the model is the multi-aspect interests extractor, which is able to extract representations for the user's multi-aspect interests. Furthermore, to learn the interactions between the user behaviors in the current session and the above multi-aspect interests, we carefully design a co-attention layer which allows the cross attentions between the two modules. Additionally, we propose a travel state-aware gating layer to attentively select the multi-aspect interests. The final user representation is obtained by fusing the three components. Comprehensive experiments conducted both offline and online demonstrate the superior performance of the proposed model at user representation, especially for cold-start users, compared with state-of-the-art methods.",
    "code_link": "https://github.com/wanjietao/FliggySMINet-AAAI2022"
  },
  "aaai2022_main_splitfedwhenfederatedlearningmeetssplitlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SplitFed: When Federated Learning Meets Split Learning",
    "authors": [
      "Chandra Thapa",
      "Pathum Chamikara Mahawaga Arachchige",
      "Seyit Camtepe",
      "Lichao\n      Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20825",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20825/20584",
    "published": "2022-02",
    "summary": "Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments. However, SL performs slower than FL due to the relay-based training across multiple clients. In this regard, this paper presents a novel approach, named splitfed learning (SFL), that amalgamates the two approaches eliminating their inherent drawbacks, along with a refined architectural configuration incorporating differential privacy and PixelDP to enhance data privacy and model robustness. Our analysis and empirical results demonstrate that (pure) SFL provides similar test accuracy and communication efficiency as SL while significantly decreasing its computation time per global epoch than in SL for multiple clients. Furthermore, as in SL, its communication efficiency over FL improves with the number of clients. Besides, the performance of SFL with privacy and robustness measures is further evaluated under extended experimental settings.",
    "code_link": ""
  },
  "aaai2022_main_listwiselearningtorankbasedonapproximaterankindicators": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Listwise Learning to Rank Based on Approximate Rank Indicators",
    "authors": [
      "Thibaut Thonet",
      "Yagmur Gizem Cinar",
      "Eric Gaussier",
      "Minghan Li",
      "Jean-Michel\n      Renders"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20826",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20826/20585",
    "published": "2022-02",
    "summary": "We study here a way to approximate information retrieval metrics through a softmax-based approximation of the rank indicator function. Indeed, this latter function is a key component in the design of information retrieval metrics, as well as in the design of the ranking and sorting functions. Obtaining a good approximation for it thus opens the door to differentiable approximations of many evaluation measures that can in turn be used in neural end-to-end approaches. We first prove theoretically that the approximations proposed are of good quality, prior to validate them experimentally on both learning to rank and text-based information retrieval tasks.",
    "code_link": ""
  },
  "aaai2022_main_privatemailsupervisedmanifoldlearningofdeepfeatureswithprivacyforimageretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PrivateMail: Supervised Manifold Learning of Deep Features with Privacy for Image Retrieval",
    "authors": [
      "Praneeth Vepakomma",
      "Julia Balla",
      "Ramesh Raskar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20827",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20827/20586",
    "published": "2022-02",
    "summary": "Differential Privacy offers strong guarantees such as immutable privacy under any post-processing. In this work, we propose a differentially private mechanism called PrivateMail for performing supervised manifold learning. We then apply it to the use case of private image retrieval to obtain nearest matches to a client\u2019s target image from a server\u2019s database. PrivateMail releases the target image as part of a differentially private manifold embedding. We give bounds on the global sensitivity of the manifold learning map in order to obfuscate and release embeddings with differential privacy inducing noise. We show that PrivateMail obtains a substantially better performance in terms of the privacy-utility trade off in comparison to several baselines on various datasets. We share code for applying PrivateMail at http://tiny.cc/PrivateMail.",
    "code_link": ""
  },
  "aaai2022_main_amortizedgenerationofsequentialalgorithmicrecoursesforblack-boxmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models",
    "authors": [
      "Sahil Verma",
      "Keegan Hines",
      "John P. Dickerson"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20828",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20828/20587",
    "published": "2022-02",
    "summary": "Explainable machine learning (ML) has gained traction in recent years due to the increasing adoption of ML-based systems in many sectors. Algorithmic Recourses (ARs) provide \"what if\" feedback of the form \"if an input datapoint were x' instead of x, then an ML-based system's output would be y' instead of y.\" Recourses are attractive due to their actionable feedback, amenability to existing legal frameworks, and fidelity to the underlying ML model. Yet, current recourse approaches are single shot that is, they assume x can change to x' in a single time period. We propose a novel stochastic-control-based approach that generates sequential recourses, that is, recourses that allow x to move stochastically and sequentially across intermediate states to a final state x'. Our approach is model agnostic and black box. Furthermore, the calculation of recourses is amortized such that once trained, it applies to multiple datapoints without the need for re-optimization. In addition to these primary characteristics, our approach admits optional desiderata such as adherence to the data manifold, respect for causal relations, and sparsity identified by past research as desirable properties of recourses. We evaluate our approach using three real-world datasets and show successful generation of sequential recourses that respect other recourse desiderata.",
    "code_link": ""
  },
  "aaai2022_main_robustoptimalclassificationtreesagainstadversarialexamples": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Optimal Classification Trees against Adversarial Examples",
    "authors": [
      "Dani\u00ebl Vos",
      "Sicco Verwer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20829",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20829/20588",
    "published": "2022-02",
    "summary": "Decision trees are a popular choice of explainable model, but just like neural networks, they suffer from adversarial examples. Existing algorithms for fitting decision trees robust against adversarial examples are greedy heuristics and lack approximation guarantees. In this paper we propose ROCT, a collection of methods to train decision trees that are optimally robust against user-specified attack models. We show that the min-max optimization problem that arises in adversarial learning can be solved using a single minimization formulation for decision trees with 0-1 loss. We propose such formulations in Mixed-Integer Linear Programming and Maximum Satisfiability, which widely available solvers can optimize. We also present a method that determines the upper bound on adversarial accuracy for any model using bipartite matching. Our experimental results demonstrate that the existing heuristics achieve close to optimal scores while ROCT achieves state-of-the-art scores.",
    "code_link": "https://github.com/tudelft-cda-lab/ROCT"
  },
  "aaai2022_main_spline-pinnapproachingpdeswithoutdatausingfast,physics-informedhermite-splinecnns": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Spline-PINN: Approaching PDEs without Data Using Fast, Physics-Informed Hermite-Spline CNNs",
    "authors": [
      "Nils Wandel",
      "Michael Weinmann",
      "Michael Neidlin",
      "Reinhard Klein"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20830",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20830/20589",
    "published": "2022-02",
    "summary": "Partial Differential Equations (PDEs) are notoriously difficult to solve. In general, closed form solutions are not available and numerical approximation schemes are computationally expensive. In this paper, we propose to approach the solution of PDEs based on a novel technique that combines the advantages of two recently emerging machine learning based approaches.First, physics-informed neural networks (PINNs) learn continuous solutions of PDEs and can be trained with little to no ground truth data. However, PINNs do not generalize well to unseen domains. Second, convolutional neural networks provide fast inference and generalize but either require large amounts of training data or a physics-constrained loss based on finite differences that can lead to inaccuracies and discretization artifacts.We leverage the advantages of both of these approaches by using Hermite spline kernels in order to continuously interpolate a grid-based state representation that can be handled by a CNN. This allows for training without any precomputed training data using a physics-informed loss function only and provides fast, continuous solutions that generalize to unseen domains.We demonstrate the potential of our method at the examples of the incompressible Navier-Stokes equation and the damped wave equation. Our models are able to learn several intriguing phenomena such as Karman vortex streets, the Magnus effect, Doppler effect, interference patterns and wave reflections. Our quantitative assessment and an interactive real-time demo show that we are narrowing the gap in accuracy of unsupervised ML based methods to industrial solvers for computational fluid dynamics (CFD) while being orders of magnitude faster.",
    "code_link": "https://github.com/aschethor/Spline"
  },
  "aaai2022_main_contextuncertaintyincontextualbanditswithapplicationstorecommendersystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Context Uncertainty in Contextual Bandits with Applications to Recommender Systems",
    "authors": [
      "Hao Wang",
      "Yifei Ma",
      "Hao Ding",
      "Yuyang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20831",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20831/20590",
    "published": "2022-02",
    "summary": "Recurrent neural networks have proven effective in modeling sequential user feedbacks for recommender systems. However, they usually focus solely on item relevance and fail to effectively explore diverse items for users, therefore harming the system performance in the long run. To address this problem, we propose a new type of recurrent neural networks, dubbed recurrent exploration networks (REN), to jointly perform representation learning and effective exploration in the latent space. REN tries to balance relevance and exploration while taking into account the uncertainty in the representations. Our theoretical analysis shows that REN can preserve the rate-optimal sublinear regret even when there exists uncertainty in the learned representations. Our empirical study demonstrates that REN can achieve satisfactory long-term rewards on both synthetic and real-world recommendation datasets, outperforming state-of-the-art models.",
    "code_link": ""
  },
  "aaai2022_main_demystifyingwhylocalaggregationhelpsconvergenceanalysisofhierarchicalsgd": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Demystifying Why Local Aggregation Helps: Convergence Analysis of Hierarchical SGD",
    "authors": [
      "Jiayi Wang",
      "Shiqiang Wang",
      "Rong-Rong Chen",
      "Mingyue Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20832",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20832/20591",
    "published": "2022-02",
    "summary": "Hierarchical SGD (H-SGD) has emerged as a new distributed SGD algorithm for multi-level communication networks. In H-SGD, before each global aggregation, workers send their updated local models to local servers for aggregations. Despite recent research efforts, the effect of local aggregation on global convergence still lacks theoretical understanding. In this work, we first introduce a new notion of \"upward\" and \"downward\" divergences. We then use it to conduct a novel analysis to obtain a worst-case convergence upper bound for two-level H-SGD with non-IID data, non-convex objective function, and stochastic gradient. By extending this result to the case with random grouping, we observe that this convergence upper bound of H-SGD is between the upper bounds of two single-level local SGD settings, with the number of local iterations equal to the local and global update periods in H-SGD, respectively. We refer to this as the \"sandwich behavior\". Furthermore, we extend our analytical approach based on \"upward\" and \"downward\" divergences to study the convergence for the general case of H-SGD with more than two levels, where the \"sandwich behavior\" still holds. Our theoretical results provide key insights of why local aggregation can be beneficial in improving the convergence of H-SGD.",
    "code_link": "https://github.com/C3atUofU/Hierarchical-SGD.git"
  },
  "aaai2022_main_learngenefromopen-worldtoyourlearningtask": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learngene: From Open-World to Your Learning Task",
    "authors": [
      "Qiu-Feng Wang",
      "Xin Geng",
      "Shu-Xia Lin",
      "Shi-Yu Xia",
      "Lei Qi",
      "Ning Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20833",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20833/20592",
    "published": "2022-02",
    "summary": "Although deep learning has made significant progress on fixed large-scale datasets, it typically encounters challenges regarding improperly detecting unknown/unseen classes in the open-world scenario, over-parametrized, and overfitting small samples. Since biological systems can overcome the above difficulties very well, individuals inherit an innate gene from collective creatures that have evolved over hundreds of millions of years and then learn new skills through few examples. Inspired by this, we propose a practical collective-individual paradigm where an evolution (expandable) network is trained on sequential tasks and then recognize unknown classes in real-world. Moreover, the learngene, i.e., the gene for learning initialization rules of the target model, is proposed to inherit the meta-knowledge from the collective model and reconstruct a lightweight individual model on the target task. Particularly, a novel criterion is proposed to discover learngene in the collective model, according to the gradient information. Finally, the individual model is trained only with few samples on the target learning tasks. We demonstrate the effectiveness of our approach in an extensive empirical study and theoretical analysis.",
    "code_link": ""
  },
  "aaai2022_main_boostingactivelearningviaimprovingtestperformance": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Boosting Active Learning via Improving Test Performance",
    "authors": [
      "Tianyang Wang",
      "Xingjian Li",
      "Pengkun Yang",
      "Guosheng Hu",
      "Xiangrui Zeng",
      "Siyu\n      Huang",
      "Cheng-Zhong Xu",
      "Min Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20834",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20834/20593",
    "published": "2022-02",
    "summary": "Central to active learning (AL) is what data should be selected for annotation. Existing works attempt to select highly uncertain or informative data for annotation. Nevertheless, it remains unclear how selected data impacts the test performance of the task model used in AL. In this work, we explore such an impact by theoretically proving that selecting unlabeled data of higher gradient norm leads to a lower upper-bound of test loss, resulting in a better test performance.However, due to the lack of label information, directly computing gradient norm for unlabeled data is infeasible. To address this challenge, we propose two schemes, namely expected-gradnorm and entropy-gradnorm. The former computes the gradient norm by constructing an expected empirical loss while the latter constructs an unsupervised loss with entropy. Furthermore, we integrate the two schemes in a universal AL framework. We evaluate our method on classical image classification and semantic segmentation tasks. To demonstrate its competency in domain applications and its robustness to noise, we also validate our method on a cellular imaging analysis task, namely cryo-Electron Tomography subtomogram classification. Results demonstrate that our method achieves superior performance against the state of the art. We refer readers to https://arxiv.org/pdf/2112.05683.pdf for the full version of this paper which includes the appendix and source code link.",
    "code_link": "https://github.com/kuangliu/pytorch-cifar"
  },
  "aaai2022_main_efficientalgorithmsforgeneralisotoneoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Algorithms for General Isotone Optimization",
    "authors": [
      "Xiwen Wang",
      "Jiaxi Ying",
      "Jos\u00e9 Vin\u00edcius de M. Cardoso",
      "Daniel P. Palomar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20835",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20835/20594",
    "published": "2022-02",
    "summary": "Monotonicity is often a fundamental assumption involved in the modeling of a number of real-world applications. From an optimization perspective, monotonicity is formulated as partial order constraints among the optimization variables, commonly known as isotone optimization. In this paper, we develop an efficient, provable convergent algorithm for solving isotone optimization problems. The proposed algorithm is general in the sense that it can handle any arbitrary isotonic constraints and a wide range of objective functions. We evaluate our algorithm and state-of-the-art methods with experiments involving both synthetic and real-world data. The experimental results demonstrate that our algorithm is more efficient by one to four orders of magnitude than the state-of-the-art methods.",
    "code_link": "https://github.com/Xiwen1997/IsotoneOptimization"
  },
  "aaai2022_main_efficientcausalstructurelearningfrommultipleinterventionaldatasetswithunknowntargets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Causal Structure Learning from Multiple Interventional Datasets with Unknown Targets",
    "authors": [
      "Yunxia Wang",
      "Fuyuan Cao",
      "Kui Yu",
      "Jiye Liang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20836",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20836/20595",
    "published": "2022-02",
    "summary": "We consider the problem of reducing the false discovery rate in multiple high-dimensional interventional datasets under unknown targets. Traditional algorithms merged directly multiple causal graphs learned, which ignores the contradictions of different datasets, leading to lots of inconsistent directions of edges. For reducing the contradictory information, we propose a new algorithm, which first learns an interventional Markov equivalence class (I-MEC) before merging multiple graphs. It utilizes the full power of the constraints available in interventional data and combines ideas from local learning, intervention, and search-and-score techniques in a principled and effective way in different intervention experiments. Specifically, local learning on multiple datasets is used to build a causal skeleton. Perfect intervention destroys some possible triangles, leading to the identification of more possible V-structures. And then a theoretically correct I-MEC is learned. Search and scoring techniques based on the learned I-MEC further identify the remaining unoriented edges. Both theoretical analysis and experiments on benchmark Bayesian networks with the number of variables from 20 to 724 validate that the effectiveness of our algorithm in reducing the false discovery rate in high-dimensional interventional data.",
    "code_link": ""
  },
  "aaai2022_main_continuallearningthroughretrievalandimagination": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Continual Learning through Retrieval and Imagination",
    "authors": [
      "Zhen Wang",
      "Liu Liu",
      "Yiqun Duan",
      "Dacheng Tao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20837",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20837/20596",
    "published": "2022-02",
    "summary": "Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting.",
    "code_link": ""
  },
  "aaai2022_main_max-mingroupedbandits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Max-Min Grouped Bandits",
    "authors": [
      "Zhenlin Wang",
      "Jonathan Scarlett"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20838",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20838/20597",
    "published": "2022-02",
    "summary": "In this paper, we introduce a multi-armed bandit problem termed max-min grouped bandits, in which the arms are arranged in possibly-overlapping groups, and the goal is to find a group whose worst arm has the highest mean reward. This problem is of interest in applications such as recommendation systems, and is also closely related to widely-studied robust optimization problems. We present two algorithms based successive elimination and robust optimization, and derive upper bounds on the number of samples to guarantee finding a max-min optimal or near-optimal group, as well as an algorithm-independent lower bound. We discuss the degree of tightness of our bounds in various cases of interest, and the difficulties in deriving uniformly tight bounds.",
    "code_link": ""
  },
  "aaai2022_main_sample-efficientreinforcementlearningviaconservativemodel-basedactor-critic": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sample-Efficient Reinforcement Learning via Conservative Model-Based Actor-Critic",
    "authors": [
      "Zhihai Wang",
      "Jie Wang",
      "Qi Zhou",
      "Bin Li",
      "Houqiang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20839",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20839/20598",
    "published": "2022-02",
    "summary": "Model-based reinforcement learning algorithms, which aim to learn a model of the environment to make decisions, are more sample efficient than their model-free counterparts. The sample efficiency of model-based approaches relies on whether the model can well approximate the environment. However, learning an accurate model is challenging, especially in complex and noisy environments. To tackle this problem, we propose the conservative model-based actor-critic (CMBAC), a novel approach that achieves high sample efficiency without the strong reliance on accurate learned models. Specifically, CMBAC learns multiple estimates of the Q-value function from a set of inaccurate models and uses the average of the bottom-k estimates---a conservative estimate---to optimize the policy. An appealing feature of CMBAC is that the conservative estimates effectively encourage the agent to avoid unreliable \u201cpromising actions\u201d---whose values are high in only a small fraction of the models. Experiments demonstrate that CMBAC significantly outperforms state-of-the-art approaches in terms of sample efficiency on several challenging control tasks, and the proposed method is more robust than previous methods in noisy environments.",
    "code_link": ""
  },
  "aaai2022_main_controllingunderestimationbiasinreinforcementlearningviaquasi-medianoperation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Controlling Underestimation Bias in Reinforcement Learning via Quasi-median Operation",
    "authors": [
      "Wei Wei",
      "Yujia Zhang",
      "Jiye Liang",
      "Lin Li",
      "Yyuze Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20840",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20840/20599",
    "published": "2022-02",
    "summary": "How to get a good value estimation is one of the key problems in reinforcement learning (RL). Current off-policy methods, such as Maxmin Q-learning, TD3 and TADD, suffer from the underestimation problem when solving the overestimation problem. In this paper, we propose the Quasi-Median Operation, a novel way to mitigate the underestimation bias by selecting the quasi-median from multiple state-action values. Based on the quasi-median operation, we propose Quasi-Median Q-learning (QMQ) for the discrete action tasks and Quasi-Median Delayed Deep Deterministic Policy Gradient (QMD3) for the continuous action tasks. Theoretically, the underestimation bias of our method is improved while the estimation variance is significantly reduced compared to Maxmin Q-learning, TD3 and TADD. We conduct extensive experiments on the discrete and continuous action tasks, and results show that our method outperforms the state-of-the-art methods.",
    "code_link": "https://github.com/ntasfi/PyGame-Learning-Environment"
  },
  "aaai2022_main_symbolicbrittlenessinsequencemodelsonsystematicgeneralizationinsymbolicmathematics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Symbolic Brittleness in Sequence Models: On Systematic Generalization in Symbolic Mathematics",
    "authors": [
      "Sean Welleck",
      "Peter West",
      "Jize Cao",
      "Yejin Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20841",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20841/20600",
    "published": "2022-02",
    "summary": "Neural sequence models trained with maximum likelihood estimation have led to breakthroughs in many tasks, where success is defined by the gap between training and test performance. However, their ability to achieve stronger forms of generalization remains unclear. We consider the problem of symbolic mathematical integration, as it requires generalizing systematically beyond the training set. We develop a methodology for evaluating generalization that takes advantage of the problem domain's structure and access to a verifier. Despite promising in-distribution performance of sequence-to-sequence models in this domain, we demonstrate challenges in achieving robustness, compositionality, and out-of-distribution generalization, through both carefully constructed manual test suites and a genetic algorithm that automatically finds large collections of failures in a controllable manner. Our investigation highlights the difficulty of generalizing well with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across different aspects of generalization.",
    "code_link": "https://github.com/facebookresearch/SymbolicMathematics"
  },
  "aaai2022_main_pruneandtuneensembleslow-costensemblelearningwithsparseindependentsubnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Prune and Tune Ensembles: Low-Cost Ensemble Learning with Sparse Independent Subnetworks",
    "authors": [
      "Tim Whitaker",
      "Darrell Whitley"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20842",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20842/20601",
    "published": "2022-02",
    "summary": "Ensemble Learning is an effective method for improving generalization in machine learning. However, as state-of-the-art neural networks grow larger, the computational cost associated with training several independent networks becomes expensive. We introduce a fast, low-cost method for creating diverse ensembles of neural networks without needing to train multiple models from scratch. We do this by first training a single parent network. We then create child networks by cloning the parent and dramatically pruning the parameters of each child to create an ensemble of members with unique and diverse topologies. We then briefly train each child network for a small number of epochs, which now converge significantly faster when compared to training from scratch. We explore various ways to maximize diversity in the child networks, including the use of anti-random pruning and one-cycle tuning. This diversity enables \"Prune and Tune\" ensembles to achieve results that are competitive with traditional ensembles at a fraction of the training cost. We benchmark our approach against state of the art low-cost ensemble methods and display marked improvement in both accuracy and uncertainty estimation on CIFAR-10 and CIFAR-100.",
    "code_link": ""
  },
  "aaai2022_main_plugenmulti-labelconditionalgenerationfrompre-trainedmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PluGeN: Multi-Label Conditional Generation from Pre-trained Models",
    "authors": [
      "Maciej Wo\u0142czyk",
      "Magdalena Proszewska",
      "\u0141ukasz Maziarka",
      "Maciej Zieba",
      "Patryk Wielopolski",
      "Rafa\u0142 Kurczab",
      "Marek Smieja"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20843",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20843/20602",
    "published": "2022-02",
    "summary": "Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to perform edits of only one attribute while leaving the others unchanged. To overcome these limitations we propose PluGeN (Plugin Generative Network), a simple yet effective generative technique that can be used as a plugin to pre-trained generative models. The idea behind our approach is to transform the entangled latent representation using a flow-based module into a multi-dimensional space where the values of each attribute are modeled as an independent one-dimensional distribution. In consequence, PluGeN can generate new samples with desired attributes as well as manipulate labeled attributes of existing examples. Due to the disentangling of the latent representation, we are even able to generate samples with rare or unseen combinations of attributes in the dataset, such as a young person with gray hair, men with make-up, or women with beards. We combined PluGeN with GAN and VAE models and applied it to conditional generation and manipulation of images and chemical molecule modeling. Experiments demonstrate that PluGeN preserves the quality of backbone models while adding the ability to control the values of labeled attributes. Implementation is available at https://github.com/gmum/plugen.",
    "code_link": "https://github.com/gmum/plugen"
  },
  "aaai2022_main_structurelearning-basedtaskdecompositionforreinforcementlearninginnon-stationaryenvironments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Structure Learning-Based Task Decomposition for Reinforcement Learning in Non-stationary Environments",
    "authors": [
      "Honguk Woo",
      "Gwangpyo Yoo",
      "Minjong Yoo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20844",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20844/20603",
    "published": "2022-02",
    "summary": "Reinforcement learning (RL) agents empowered by deep neural networks have been considered a feasible solution to automate control functions in a cyber-physical system. In this work, we consider an RL-based agent and address the issue of learning via continual interaction with a time-varying dynamic system modeled as a non-stationary Markov decision process (MDP). We view such a non-stationary MDP as a time series of conventional MDPs that can be parameterized by hidden variables. To infer the hidden parameters, we present a task decomposition method that exploits CycleGAN-based structure learning. This method enables the separation of time-variant tasks from a non-stationary MDP, establishing the task decomposition embedding specific to time-varying information. To mitigate the adverse effect due to inherent noises of task embedding, we also leverage continual learning on sequential tasks by adapting the orthogonal gradient descent scheme with a sliding window.Through various experiments, we demonstrate that our approach renders the RL agent adaptable to time-varying dynamic environment conditions, outperforming other methods including state-of-the-art non-stationary MDP algorithms.",
    "code_link": ""
  },
  "aaai2022_main_anefficientcombinatorialoptimizationmodelusinglearning-to-rankdistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Efficient Combinatorial Optimization Model Using Learning-to-Rank Distillation",
    "authors": [
      "Honguk Woo",
      "Hyunsung Lee",
      "Sangwoo Cho"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20845",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20845/20604",
    "published": "2022-02",
    "summary": "Recently, deep reinforcement learning (RL) has proven its feasibility in solving combinatorial optimization problems (COPs). The learning-to-rank techniques have been studied in the field of information retrieval. While several COPs can be formulated as the prioritization of input items, as is common in the information retrieval, it has not been fully explored how the learning-to-rank techniques can be incorporated into deep RL for COPs. In this paper, we present the learning-to-rank distillation-based COP framework, where a high-performance ranking policy obtained by RL for a COP can be distilled into a non-iterative, simple model, thereby achieving a low-latency COP solver. Specifically, we employ the approximated ranking distillation to render a score-based ranking model learnable via gradient descent. Furthermore, we use the efficient sequence sampling to improve the inference performance with a limited delay. With the framework, we demonstrate that a distilled model not only achieves comparable performance to its respective, high-performance RL, but also provides several times faster inferences. We evaluate the framework with several COPs such as priority-based task scheduling and multidimensional knapsack, demonstrating the benefits of the framework in terms of inference latency and performance.",
    "code_link": ""
  },
  "aaai2022_main_pumaperformanceunchangedmodelaugmentationfortrainingdataremoval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PUMA: Performance Unchanged Model Augmentation for Training Data Removal",
    "authors": [
      "Ga Wu",
      "Masoud Hashemi",
      "Christopher Srinivasa"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20846",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20846/20605",
    "published": "2022-02",
    "summary": "Preserving the performance of a trained model while removing unique characteristics of marked training data points is challenging. Recent research usually suggests retraining a model from scratch with remaining training data or refining the model by reverting the model optimization on the marked data points. Unfortunately, aside from their computational inefficiency, those approaches inevitably hurt the resulting model's generalization ability since they remove not only unique characteristics but also discard shared (and possibly contributive) information. To address the performance degradation problem, this paper presents a novel approach called Performance Unchanged Model Augmentation (PUMA). The proposed PUMA framework explicitly models the influence of each training data point on the model's generalization ability with respect to various performance criteria. It then complements the negative impact of removing marked data by reweighting the remaining data optimally. To demonstrate the effectiveness of the PUMA framework, we compared it with multiple state-of-the-art data removal techniques in the experiments, where we show the PUMA can effectively and efficiently remove the unique characteristics of marked training data without retraining the model that can 1) fool a membership attack, and 2) resist performance degradation. In addition, as PUMA estimates the data importance during its operation, we show it could serve to debug mislabelled data points more efficiently than existing approaches.",
    "code_link": ""
  },
  "aaai2022_main_generalizingreinforcementlearningthroughfusingself-supervisedlearningintointrinsicmotivation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generalizing Reinforcement Learning through Fusing Self-Supervised Learning into Intrinsic Motivation",
    "authors": [
      "Keyu Wu",
      "Min Wu",
      "Zhenghua Chen",
      "Yuecong Xu",
      "Xiaoli Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20847",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20847/20606",
    "published": "2022-02",
    "summary": "Despite the great potential of reinforcement learning (RL) in solving complex decision-making problems, generalization remains one of its key challenges, leading to difficulty in deploying learned RL policies to new environments. In this paper, we propose to improve the generalization of RL algorithms through fusing Self-supervised learning into Intrinsic Motivation (SIM). Specifically, SIM boosts representation learning through driving the cross-correlation matrix between the embeddings of augmented and non-augmented samples close to the identity matrix. This aims to increase the similarity between the embedding vectors of a sample and its augmented version while minimizing the redundancy between the components of these vectors. Meanwhile, the redundancy reduction based self-supervised loss is converted to an intrinsic reward to further improve generalization in RL via an auxiliary objective. As a general paradigm, SIM can be implemented on top of any RL algorithm. Extensive evaluations have been performed on a diversity of tasks. Experimental results demonstrate that SIM consistently outperforms the state-of-the-art methods and exhibits superior generalization capability and sample efficiency.",
    "code_link": "https://github.com/KerryWu16/SIM"
  },
  "aaai2022_main_adalossacomputationally-efficientandprovablyconvergentadaptivegradientmethod": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AdaLoss: A Computationally-Efficient and Provably Convergent Adaptive Gradient Method",
    "authors": [
      "Xiaoxia Wu",
      "Yuege Xie",
      "Simon Shaolei Du",
      "Rachel Ward"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20848",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20848/20607",
    "published": "2022-02",
    "summary": "We propose a computationally-friendly adaptive learning rate schedule, ``AdaLoss\", which directly uses the information of the loss function to adjust the stepsize in gradient descent methods. We prove that this schedule enjoys linear convergence in linear regression. Moreover, we extend the to the non-convex regime, in the context of two-layer over-parameterized neural networks. If the width is sufficiently large (polynomially), then AdaLoss converges robustly to the global minimum in polynomial time. We numerically verify the theoretical results and extend the scope of the numerical experiments by considering applications in LSTM models for text clarification and policy gradients for control problems.",
    "code_link": "https://github.com/willway1023yx/adaloss"
  },
  "aaai2022_main_towardsoff-policylearningforrankingpolicieswithloggedfeedback": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Off-Policy Learning for Ranking Policies with Logged Feedback",
    "authors": [
      "Teng Xiao",
      "Suhang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20849",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20849/20608",
    "published": "2022-02",
    "summary": "Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods",
    "code_link": ""
  },
  "aaai2022_main_activelearningfordomainadaptationanenergy-basedapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Active Learning for Domain Adaptation: An Energy-Based Approach",
    "authors": [
      "Binhui Xie",
      "Longhui Yuan",
      "Shuang Li",
      "Chi Harold Liu",
      "Xinjing Cheng",
      "Guoren\n      Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20850",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20850/20609",
    "published": "2022-02",
    "summary": "Unsupervised domain adaptation has recently emerged as an effective paradigm for generalizing deep neural networks to new target domains. However, there is still enormous potential to be tapped to reach the fully supervised performance. In this paper, we present a novel active learning strategy to assist knowledge transfer in the target domain, dubbed active domain adaptation. We start from an observation that energy-based models exhibit free energy biases when training (source) and test (target) data come from different distributions. Inspired by this inherent mechanism, we empirically reveal that a simple yet efficient energy-based sampling strategy sheds light on selecting the most valuable target samples than existing approaches requiring particular architectures or computation of the distances. Our algorithm, Energy-based Active Domain Adaptation (EADA), queries groups of target data that incorporate both domain characteristic and instance uncertainty into every selection round. Meanwhile, by aligning the free energy of target data compact around the source domain via a regularization term, domain gap can be implicitly diminished. Through extensive experiments, we show that EADA surpasses state-of-the-art methods on well-known challenging benchmarks with substantial improvements, making it a useful option in the open world. Code is available at https://github.com/BIT-DA/EADA.",
    "code_link": "https://github.com/BIT-DA/EADA"
  },
  "aaai2022_main_gearnetstepwiseduallearningforweaklysuperviseddomainadaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GearNet: Stepwise Dual Learning for Weakly Supervised Domain Adaptation",
    "authors": [
      "Renchunzi Xie",
      "Hongxin Wei",
      "Lei Feng",
      "Bo An"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20851",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20851/20610",
    "published": "2022-02",
    "summary": "This paper studies a weakly supervised domain adaptation (WSDA) problem, where we only have access to the source domain with noisy labels, from which we need to transfer useful information to the unlabeled target domain. Although there have been a few studies on this problem, most of them only exploit unidirectional relationships from the source domain to the target domain. In this paper, we propose a universal paradigm called GearNet to exploit bilateral relationships between the two domains. Specifically, we take the two domains as different inputs to train two models alternately, and a symmetrical Kullback-Leibler loss is used for selectively matching the predictions of the two models in the same domain. This interactive learning schema enables implicit label noise canceling and exploit correlations between the source and target domains. Therefore, our GearNet has the great potential to boost the performance of a wide range of existing WSDA methods. Comprehensive experimental results show that the performance of existing methods can be significantly improved by equipping with our GearNet.",
    "code_link": ""
  },
  "aaai2022_main_reinforcementlearningaugmentedasymptoticallyoptimalindexpolicyforfinite-horizonrestlessbandits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reinforcement Learning Augmented Asymptotically Optimal Index Policy for Finite-Horizon Restless Bandits",
    "authors": [
      "Guojun Xiong",
      "Jian Li",
      "Rahul Singh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20852",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20852/20611",
    "published": "2022-02",
    "summary": "We study a finite-horizon restless multi-armed bandit problem with multiple actions, dubbed as R(MA)^2B. The state of each arm evolves according to a controlled Markov decision process (MDP), and the reward of pulling an arm depends on both the current state and action of the corresponding MDP. Since finding the optimal policy is typically intractable, we propose a computationally appealing index policy entitled Occupancy-Measured-Reward Index Policy for the finite-horizon R(MA)^2B. Our index policy is well-defined without the requirement of indexability condition and is provably asymptotically optimal as the number of arms tends to infinity. We then adopt a learning perspective where the system parameters are unknown, and propose R(MA)^2B-UCB, a generative model based reinforcement learning augmented algorithm that can fully exploit the structure of Occupancy-Measured-Reward Index Policy. Compared to existing algorithms, R(MA)^2B-UCB performs close to offline optimum, and achieves a sub-linear regret and a low computational complexity all at once. Experimental results show that R(MA)^2B-UCB outperforms existing algorithms in both regret and running time.",
    "code_link": ""
  },
  "aaai2022_main_coordinatingmomentaforcross-silofederatedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Coordinating Momenta for Cross-Silo Federated Learning",
    "authors": [
      "An Xu",
      "Heng Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20853",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20853/20612",
    "published": "2022-02",
    "summary": "Communication efficiency is crucial for federated learning (FL). Conducting local training steps in clients to reduce the communication frequency between clients and the server is a common method to address this issue. However, it leads to the client drift problem due to non-i.i.d. data distributions in different clients which severely deteriorates the performance. In this work, we propose a new method to improve the training performance in cross-silo FL via maintaining double momentum buffers. One momentum buffer tracks the server model updating direction, and the other tracks the local model updating direction. Moreover, we introduce a novel momentum fusion technique to coordinate the server and local momentum buffers. We also provide the first theoretical convergence analysis involving both the server and local standard momentum SGD. Extensive deep FL experimental results show a better training performance than FedAvg and existing standard momentum SGD variants.",
    "code_link": ""
  },
  "aaai2022_main_learning-augmentedalgorithmsforonlinesteinertree": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning-Augmented Algorithms for Online Steiner Tree",
    "authors": [
      "Chenyang Xu",
      "Benjamin Moseley"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20854",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20854/20613",
    "published": "2022-02",
    "summary": "This paper considers the recently popular beyond-worst-case algorithm analysis model which integrates machine-learned predictions with online algorithm design. We consider the online Steiner tree problem in this model for both directed and undirected graphs. Steiner tree is known to have strong lower bounds in the online setting and any algorithm\u2019s worst-case guarantee is far from desirable.This paper considers algorithms that predict which terminal arrives online. The predictions may be incorrect and the algorithms\u2019 performance is parameterized by the number of incorrectly predicted terminals. These guarantees ensure that algorithms break through the online lower bounds with good predictions and the competitive ratio gracefully degrades as the prediction error grows. We then observe that the theory is predictive of what will occur empirically. We show on graphs where terminals are drawn from a distribution, the new online algorithms have strong performance even with modestly correct predictions.",
    "code_link": ""
  },
  "aaai2022_main_constraintspenalizedq-learningforsafeofflinereinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
    "authors": [
      "Haoran Xu",
      "Xianyuan Zhan",
      "Xiangyu Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20855",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20855/20614",
    "published": "2022-02",
    "summary": "We study the problem of safe offline reinforcement learning (RL), the goal is to learn a policy that maximizes long-term reward while satisfying safety constraints given only offline data, without further interaction with the environment. This problem is more appealing for real world RL applications, in which data collection is costly or dangerous. Enforcing constraint satisfaction is non-trivial, especially in offline settings, as there is a potential large discrepancy between the policy distribution and the data distribution, causing errors in estimating the value of safety constraints. We show that na\u00efve approaches that combine techniques from safe RL and offline RL can only learn sub-optimal solutions. We thus develop a simple yet effective algorithm, Constraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits the use of data generated by mixed behavior policies. We present a theoretical analysis and demonstrate empirically that our approach can learn robustly across a variety of benchmark control tasks, outperforming several baselines.",
    "code_link": ""
  },
  "aaai2022_main_deepincompletemulti-viewclusteringviaminingclustercomplementarity": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Incomplete Multi-View Clustering via Mining Cluster Complementarity",
    "authors": [
      "Jie Xu",
      "Chao Li",
      "Yazhou Ren",
      "Liang Peng",
      "Yujie Mo",
      "Xiaoshuang Shi",
      "Xiaofeng Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20856",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20856/20615",
    "published": "2022-02",
    "summary": "Incomplete multi-view clustering (IMVC) is an important unsupervised approach to group the multi-view data containing missing data in some views. Previous IMVC methods suffer from the following issues: (1) the inaccurate imputation or padding for missing data negatively affects the clustering performance, (2) the quality of features after fusion might be interfered by the low-quality views, especially the inaccurate imputed views. To avoid these issues, this work presents an imputation-free and fusion-free deep IMVC framework. First, the proposed method builds a deep embedding feature learning and clustering model for each view individually. Our method then nonlinearly maps the embedding features of complete data into a high-dimensional space to discover linear separability. Concretely, this paper provides an implementation of the high-dimensional mapping as well as shows the mechanism to mine the multi-view cluster complementarity. This complementary information is then transformed to the supervised information with high confidence, aiming to achieve the multi-view clustering consistency for the complete data and incomplete data. Furthermore, we design an EM-like optimization strategy to alternately promote feature learning and clustering. Extensive experiments on real-world multi-view datasets demonstrate that our method achieves superior clustering performance over state-of-the-art methods.",
    "code_link": "https://github.com/SubmissionsIn/DIMVC"
  },
  "aaai2022_main_linearity-awaresubspaceclustering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Linearity-Aware Subspace Clustering",
    "authors": [
      "Yesong Xu",
      "Shuo Chen",
      "Jun Li",
      "Jianjun Qian"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20857",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20857/20616",
    "published": "2022-02",
    "summary": "Obtaining a good similarity matrix is extremely important in subspace clustering. Current state-of-the-art methods learn the similarity matrix through self-expressive strategy. However, these methods directly adopt original samples as a set of basis to represent itself linearly. It is difficult to accurately describe the linear relation between samples in the real-world applications, and thus is hard to find an ideal similarity matrix. To better represent the linear relation of samples, we present a subspace clustering model, Linearity-Aware Subspace Clustering (LASC), which can consciously learn the similarity matrix by employing a linearity-aware metric. This is a new subspace clustering method that combines metric learning and subspace clustering into a joint learning framework. In our model, we first utilize the self-expressive strategy to obtain an initial subspace structure and discover a low-dimensional representation of the original data. Subsequently, we use the proposed metric to learn an intrinsic similarity matrix with linearity-aware on the obtained subspace. Based on such a learned similarity matrix, the inter-cluster distance becomes larger than the intra-cluster distances, and thus successfully obtaining a good subspace cluster result. In addition, to enrich the similarity matrix with more consistent knowledge, we adopt a collaborative learning strategy for self-expressive subspace learning and linearity-aware subspace learning. Moreover, we provide detailed mathematical analysis to show that the metric can properly characterize the linear correlation between samples.",
    "code_link": ""
  },
  "aaai2022_main_gowiderinsteadofdeeper": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Go Wider Instead of Deeper",
    "authors": [
      "Fuzhao Xue",
      "Ziji Shi",
      "Futao Wei",
      "Yuxuan Lou",
      "Yong Liu",
      "Yang You"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20858",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20858/20617",
    "published": "2022-02",
    "summary": "More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference. In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by 1.5% with 0.72 times trainable parameters. Using 0.46 times and 0.13 times parameters, our WideNet can still surpass ViT and ViT-MoE by 0.8% and 2.1%, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by 1.8% on average and surpass BERT using factorized embedding parameterization by 0.8% with fewer parameters.",
    "code_link": ""
  },
  "aaai2022_main_seizingcriticallearningperiodsinfederatedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Seizing Critical Learning Periods in Federated Learning",
    "authors": [
      "Gang Yan",
      "Hao Wang",
      "Jian Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20859",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20859/20618",
    "published": "2022-02",
    "summary": "Federated learning (FL) is a popular technique to train machine learning (ML) models with decentralized data. Extensive works have studied the performance of the global model; however, it is still unclear how the training process affects the final test accuracy. Exacerbating this problem is the fact that FL executions differ significantly from traditional ML with heterogeneous data characteristics across clients, involving more hyperparameters. In this work, we show that the final test accuracy of FL is dramatically affected by the early phase of the training process, i.e., FL exhibits critical learning periods, in which small gradient errors can have irrecoverable impact on the final test accuracy. To further explain this phenomenon, we generalize the trace of the Fisher Information Matrix (FIM) to FL and define a new notation called FedFIM, a quantity reflecting the local curvature of each clients from the beginning of the training in FL. Our findings suggest that the initial learning phase plays a critical role in understanding the FL performance. This is in contrast to many existing works which generally do not connect the final accuracy of FL to the early phase training.Finally, seizing critical learning periods in FL is of independent interest and could be useful for other problems such as the choices of hyperparameters including but not limited to the number of client selected per round, batch size, so as to improve the performance of FL training and testing.",
    "code_link": ""
  },
  "aaai2022_main_learningtoidentifytopeloratingsaduelingbanditsapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Identify Top Elo Ratings: A Dueling Bandits Approach",
    "authors": [
      "Xue Yan",
      "Yali Du",
      "Binxin Ru",
      "Jun Wang",
      "Haifeng Zhang",
      "Xu Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20860",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20860/20619",
    "published": "2022-02",
    "summary": "The Elo rating system is widely adopted to evaluate the skills of (chess) game and sports players. Recently it has been also integrated into machine learning algorithms in evaluating the performance of computerised AI agents. However, an accurate estimation of the Elo rating (for the top players) often requires many rounds of competitions, which can be expensive to carry out. In this paper, to minimize the number of comparisons and to improve the sample efficiency of the Elo evaluation (for top players), we propose an efficient online match scheduling algorithm. Specifically, we identify and match the top players through a dueling bandits framework and tailor the bandit algorithm to the gradient-based update of Elo. We show that it reduces the per-step memory and time complexity to constant, compared to the traditional likelihood maximization approaches requiring O(t) time. Our algorithm has a regret guarantee that is sublinear in the number of competition rounds and has been extended to the multidimensional Elo ratings for handling intransitive games. We empirically demonstrate that our method achieves superior convergence speed and time efficiency on a variety of gaming tasks.",
    "code_link": ""
  },
  "aaai2022_main_q-ballmodelingbasketballgamesusingdeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Q-Ball: Modeling Basketball Games Using Deep Reinforcement Learning",
    "authors": [
      "Chen Yanai",
      "Adir Solomon",
      "Gilad Katz",
      "Bracha Shapira",
      "Lior Rokach"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20861",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20861/20620",
    "published": "2022-02",
    "summary": "Basketball is one of the most popular types of sports in the world. Recent technological developments have made it possible to collect large amounts of data on the game, analyze it, and discover new insights. We propose a novel approach for modeling basketball games using deep reinforcement learning. By analyzing multiple aspects of both the players and the game, we are able to model the latent connections among players' movements, actions, and performance, into a single measure - the Q-Ball. Using Q-Ball, we are able to assign scores to the performance of both players and whole teams. Our approach has multiple practical applications, including evaluating and improving players' game decisions and producing tactical recommendations. We train and evaluate our approach on a large dataset of National Basketball Association games, and show that the Q-Ball is capable of accurately assessing the performance of players and teams. Furthermore, we show that Q-Ball is highly effective in recommending alternatives to players' actions.",
    "code_link": ""
  },
  "aaai2022_main_trainingaresilientq-networkagainstobservationalinterference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Training a Resilient Q-network against Observational Interference",
    "authors": [
      "Chao-Han Huck Yang",
      "I-Te Danny Hung",
      "Yi Ouyang",
      "Pin-Yu Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20862",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20862/20621",
    "published": "2022-02",
    "summary": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications.In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an essential yet challenging task. In this paper, we consider a deep q-network (DQN) framework training with an auxiliary task of observational interferences such as artificial noises. Inspired by causal inference for observational interference, we propose a causal inference based DQN algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DQN environments with different types of interferences as auxiliary labels. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.",
    "code_link": "https://github.com/huckiyang/Obs-Causal-Q-Network"
  },
  "aaai2022_main_policyoptimizationwithstochasticmirrordescent": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Policy Optimization with Stochastic Mirror Descent",
    "authors": [
      "Long Yang",
      "Yu Zhang",
      "Gang Zheng",
      "Qian Zheng",
      "Pengfei Li",
      "Jianhang Huang",
      "Gang Pan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20863",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20863/20622",
    "published": "2022-02",
    "summary": "Improving sample efficiency has been a longstanding goal in reinforcement learning. This paper proposes VRMPO algorithm: a sample efficient policy gradient method with stochastic mirror descent. In VRMPO, a novel variance-reduced policy gradient estimator is presented to improve sample efficiency. We prove that the proposed VRMPO needs only O(\u03b5\u22123) sample trajectories to achieve an \u03b5-approximate first-order stationary point, which matches the best sample complexity for policy optimization. Extensive empirical results demonstrate that VRMP outperforms the state-of-the-art policy gradient methods in various settings.",
    "code_link": ""
  },
  "aaai2022_main_graphpointerneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Graph Pointer Neural Networks",
    "authors": [
      "Tianmeng Yang",
      "Yujing Wang",
      "Zhihan Yue",
      "Yaming Yang",
      "Yunhai Tong",
      "Jing Bai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20864",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20864/20623",
    "published": "2022-02",
    "summary": "Graph Neural Networks (GNNs) have shown advantages in various graph-based applications. Most existing GNNs assume strong homophily of graph structure and apply permutation-invariant local aggregation of neighbors to learn a representation for each node. However, they fail to generalize to heterophilic graphs, where most neighboring nodes have different labels or features, and the relevant nodes are distant. Few recent studies attempt to address this problem by combining multiple hops of hidden representations of central nodes (i.e., multi-hop-based approaches) or sorting the neighboring nodes based on attention scores (i.e., ranking-based approaches). As a result, these approaches have some apparent limitations. On the one hand, multi-hop-based approaches do not explicitly distinguish relevant nodes from a large number of multi-hop neighborhoods, leading to a severe over-smoothing problem. On the other hand, ranking-based models do not joint-optimize node ranking with end tasks and result in sub-optimal solutions. In this work, we present Graph Pointer Neural Networks (GPNN) to tackle the challenges mentioned above. We leverage a pointer network to select the most relevant nodes from a large amount of multi-hop neighborhoods, which constructs an ordered sequence according to the relationship with the central node. 1D convolution is then applied to extract high-level features from the node sequence. The pointer-network-based ranker in GPNN is joint-optimized with other parts in an end-to-end manner. Extensive experiments are conducted on six public node classification datasets with heterophilic graphs. The results show that GPNN significantly improves the classification performance of state-of-the-art methods. In addition, analyses also reveal the privilege of the proposed GPNN in filtering out irrelevant neighbors and reducing over-smoothing.",
    "code_link": ""
  },
  "aaai2022_main_logicdefaninterpretabledefenseframeworkagainstadversarialexamplesviainductivescenegraphreasoning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LOGICDEF: An Interpretable Defense Framework against Adversarial Examples via Inductive Scene Graph Reasoning",
    "authors": [
      "Yuan Yang",
      "James C Kerce",
      "Faramarz Fekri"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20865",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20865/20624",
    "published": "2022-02",
    "summary": "Deep vision models have provided new capability across a spectrum of applications in transportation, manufacturing, agriculture, commerce, and security. However, recent studies have demonstrated that these models are vulnerable to adversarial attack, exposing a risk-of-use in critical applications where untrusted parties have access to the data environment or even directly to the sensor inputs. Existing adversarial defense methods are either limited to specific types of attacks or are too complex to be applied to practical vision models. More importantly, these methods rely on techniques that are not interpretable to humans. In this work, we argue that an effective defense should produce an explanation as to why the system is attacked, and by using a representation that is easily readable by a human user, e.g. a logic formalism. To this end, we propose logic adversarial defense (LogicDef), a defense framework that utilizes the scene graph of the image to provide a contextual structure for detecting and explaining object classification. Our framework first mines inductive logic rules from the extracted scene graph, and then uses these rules to construct a defense model that alerts the user when the vision model violates the consistency rules. The defense model is interpretable and its robustness is further enhanced by incorporating existing relational commonsense knowledge from projects such as ConceptNet. In order to handle the hierarchical nature of such relational reasoning, we use a curriculum learning approach based on object taxonomy, yielding additional improvements to training and performance.",
    "code_link": "https://github.com/gblackout/logic-adversarial-defense"
  },
  "aaai2022_main_exploitinginvarianceintrainingdeepneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Exploiting Invariance in Training Deep Neural Networks",
    "authors": [
      "Chengxi Ye",
      "Xiong Zhou",
      "Tristan McKinney",
      "Yanfeng Liu",
      "Qinggang Zhou",
      "Fedor Zhdanov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20866",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20866/20625",
    "published": "2022-02",
    "summary": "Inspired by two basic mechanisms in animal visual systems, we introduce a feature transform technique that imposes invariance properties in the training of deep neural networks. The resulting algorithm requires less parameter tuning, trains well with an initial learning rate 1.0, and easily generalizes to different tasks. We enforce scale invariance with local statistics in the data to align similar samples at diverse scales. To accelerate convergence, we enforce a GL(n)-invariance property with global statistics extracted from a batch such that the gradient descent solution should remain invariant under basis change. Profiling analysis shows our proposed modifications takes 5% of the computations of the underlying convolution layer. Tested on convolutional networks and transformer networks, our proposed technique requires fewer iterations to train, surpasses all baselines by a large margin, seamlessly works on both small and large batch size training, and applies to different computer vision and language tasks.",
    "code_link": "https://github.com/jeonsworld/ViT-pytorch"
  },
  "aaai2022_main_lifelonggenerativemodellingusingdynamicexpansiongraphmodel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Lifelong Generative Modelling Using Dynamic Expansion Graph Model",
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20867",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20867/20626",
    "published": "2022-02",
    "summary": "Variational Autoencoders (VAEs) suffer from degenerated performance, when learning several successive tasks. This is caused by catastrophic forgetting. In order to address the knowledge loss, VAEs are using either Generative Replay (GR) mechanisms or Expanding Network Architectures (ENA). In this paper we study the forgetting behaviour of VAEs using a joint GR and ENA methodology, by deriving an upper bound on the negative marginal log-likelihood. This theoretical analysis provides new insights into how VAEs forget the previously learnt knowledge during lifelong learning. The analysis indicates the best performance achieved when considering model mixtures, under the ENA framework, where there are no restrictions on the number of components. However, an ENA-based approach may require an excessive number of parameters. This motivates us to propose a novel Dynamic Expansion Graph Model (DEGM). DEGM expands its architecture, according to the novelty associated with each new database, when compared to the information already learnt by the network from previous tasks. DEGM training optimizes knowledge structuring, characterizing the joint probabilistic representations corresponding to the past and more recently learned tasks. We demonstrate that DEGM guarantees optimal performance for each task while also minimizing the required number of parameters.",
    "code_link": "https://github.com/dtuzi123/Expansion-Graph-Model"
  },
  "aaai2022_main_stageconsciousattentionnetwork(scan)ademonstration-conditionedpolicyforfew-shotimitation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stage Conscious Attention Network (SCAN): A Demonstration-Conditioned Policy for Few-Shot Imitation",
    "authors": [
      "Jia-Fong Yeh",
      "Chi-Ming Chung",
      "Hung-Ting Su",
      "Yi-Ting Chen",
      "Winston H. Hsu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20868",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20868/20627",
    "published": "2022-02",
    "summary": "In few-shot imitation learning (FSIL), using behavioral cloning (BC) to solve unseen tasks with few expert demonstrations becomes a popular research direction. The following capabilities are essential in robotics applications: (1) Behaving in compound tasks that contain multiple stages. (2) Retrieving knowledge from few length-variant and misalignment demonstrations. (3) Learning from an expert different from the agent. No previous work can achieve these abilities at the same time. In this work, we conduct FSIL problem under the union of above settings and introduce a novel stage conscious attention network (SCAN) to retrieve knowledge from few demonstrations simultaneously. SCAN uses an attention module to identify each stage in length-variant demonstrations. Moreover, it is designed under demonstration-conditioned policy that learns the relationship between experts and agents. Experiment results show that SCAN can perform in complicated compound tasks without fine-tuning and provide the explainable visualization. Project page is at https://sites.google.com/view/scan-aaai2022.",
    "code_link": ""
  },
  "aaai2022_main_batudebudget-awareneuralnetworkcompressionbasedontuckerdecomposition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "BATUDE: Budget-Aware Neural Network Compression Based on Tucker Decomposition",
    "authors": [
      "Miao Yin",
      "Huy Phan",
      "Xiao Zang",
      "Siyu Liao",
      "Bo Yuan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20869",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20869/20628",
    "published": "2022-02",
    "summary": "Model compression is very important for the efficient deployment of deep neural network (DNN) models on resource-constrained devices. Among various model compression approaches, high-order tensor decomposition is particularly attractive and useful because the decomposed model is very small and fully structured. For this category of approaches, tensor ranks are the most important hyper-parameters that directly determine the architecture and task performance of the compressed DNN models. However, as an NP-hard problem, selecting optimal tensor ranks under the desired budget is very challenging and the state-of-the-art studies suffer from unsatisfied compression performance and timing-consuming search procedures. To systematically address this fundamental problem, in this paper we propose BATUDE, a Budget-Aware TUcker DEcomposition-based compression approach that can efficiently calculate optimal tensor ranks via one-shot training. By integrating the rank selecting procedure to the DNN training process with a specified compression budget, the tensor ranks of the DNN models are learned from the data and thereby bringing very significant improvement on both compression ratio and classification accuracy for the compressed models. The experimental results on ImageNet dataset show that our method enjoys 0.33% top-5 higher accuracy with 2.52X less computational cost as compared to the uncompressed ResNet-18 model. For ResNet-50, the proposed approach enables 0.37% and 0.55% top-5 accuracy increase with 2.97X and 2.04X computational cost reduction, respectively, over the uncompressed model.",
    "code_link": ""
  },
  "aaai2022_main_distributedrandomizedsketchingkernellearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Distributed Randomized Sketching Kernel Learning",
    "authors": [
      "Rong Yin",
      "Yong Liu",
      "Dan Meng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20870",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20870/20629",
    "published": "2022-02",
    "summary": "We investigate the statistical and computational requirements for distributed kernel ridge regression with randomized sketching (DKRR-RS) and successfully achieve the optimal learning rates with only a fraction of computations. More precisely, the proposed DKRR-RS combines sparse randomized sketching, divide-and-conquer and KRR to scale up kernel methods and successfully derives the same learning rate as the exact KRR with greatly reducing computational costs in expectation, at the basic setting, which outperforms previous state of the art solutions. Then, for the sake of the gap between theory and experiments, we derive the optimal learning rate in probability for DKRR-RS to reflect its generalization performance. Finally, to further improve the learning performance, we construct an efficient communication strategy for DKRR-RS and demonstrate the power of communications via theoretical assessment. An extensive experiment validates the effectiveness of DKRR-RS and the communication strategy on real datasets.",
    "code_link": ""
  },
  "aaai2022_main_autogclautomatedgraphcontrastivelearningvialearnableviewgenerators": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators",
    "authors": [
      "Yihang Yin",
      "Qingzhong Wang",
      "Siyu Huang",
      "Haoyi Xiong",
      "Xiang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20871",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20871/20630",
    "published": "2022-02",
    "summary": "Contrastive learning has been widely applied to graph representation learning, where the view generators play a vital role in generating effective contrastive samples. Most of the existing contrastive learning methods employ pre-defined view generation methods, e.g., node drop or edge perturbation, which usually cannot adapt to input data or preserve the original semantic structures well. To address this issue, we propose a novel framework named Automated Graph Contrastive Learning (AutoGCL) in this paper. Specifically, AutoGCL employs a set of learnable graph view generators orchestrated by an auto augmentation strategy, where every graph view generator learns a probability distribution of graphs conditioned by the input. While the graph view generators in AutoGCL preserve the most representative structures of the original graph in generation of every contrastive sample, the auto augmentation learns policies to introduce adequate augmentation variances in the whole contrastive learning procedure. Furthermore, AutoGCL adopts a joint training strategy to train the learnable view generators, the graph encoder, and the classifier in an end-to-end manner, resulting in topological heterogeneity yet semantic similarity in the generation of contrastive samples. Extensive experiments on semi-supervised learning, unsupervised learning, and transfer learning demonstrate the superiority of our AutoGCL framework over the state-of-the-arts in graph contrastive learning. In addition, the visualization results further confirm that the learnable view generators can deliver more compact and semantically meaningful contrastive samples compared against the existing view generation methods. Our code is available at https://github.com/Somedaywilldo/AutoGCL.",
    "code_link": "https://github.com/Somedaywilldo/AutoGCL"
  },
  "aaai2022_main_bm-nasbilevelmultimodalneuralarchitecturesearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "BM-NAS: Bilevel Multimodal Neural Architecture Search",
    "authors": [
      "Yihang Yin",
      "Siyu Huang",
      "Xiang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20872",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20872/20631",
    "published": "2022-02",
    "summary": "Deep neural networks (DNNs) have shown superior performances on various multimodal learning problems. However, it often requires huge efforts to adapt DNNs to individual multimodal tasks by manually engineering unimodal features and designing multimodal feature fusion strategies. This paper proposes Bilevel Multimodal Neural Architecture Search (BM-NAS) framework, which makes the architecture of multimodal fusion models fully searchable via a bilevel searching scheme. At the upper level, BM-NAS selects the inter/intra-modal feature pairs from the pretrained unimodal backbones. At the lower level, BM-NAS learns the fusion strategy for each feature pair, which is a combination of predefined primitive operations. The primitive operations are elaborately designed and they can be flexibly combined to accommodate various effective feature fusion modules such as multi-head attention (Transformer) and Attention on Attention (AoA). Experimental results on three multimodal tasks demonstrate the effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS achieves competitive performances with much less search time and fewer model parameters in comparison with the existing generalized multimodal NAS methods. Our code is available at https://github.com/Somedaywilldo/BM-NAS.",
    "code_link": "https://github.com/Somedaywilldo/BM-NAS"
  },
  "aaai2022_main_early-birdgcnsgraph-networkco-optimizationtowardsmoreefficientgcntrainingandinferenceviadrawingearly-birdlotterytickets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Early-Bird GCNs: Graph-Network Co-optimization towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets",
    "authors": [
      "Haoran You",
      "Zhihan Lu",
      "Zijian Zhou",
      "Yonggan Fu",
      "Yingyan Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20873",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20873/20632",
    "published": "2022-02",
    "summary": "Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep learning model for representation learning on graphs. However, it remains notoriously challenging to train and inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the exploration of deeper and more sophisticated GCN graphs. This is because as the graph size grows, the sheer number of node features and the large adjacency matrix can easily explode the required memory and data movements. To tackle the aforementioned challenges, we explore the possibility of drawing lottery tickets when sparsifying GCN graphs, i.e., subgraphs that largely shrink the adjacency matrix yet are capable of achieving accuracy comparable to or even better than their full graphs. Specifically, we for the first time discover the existence of graph early-bird (GEB) tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify the emergence of such GEB tickets. Furthermore, we advocate graph-model co-optimization and develop a generic efficient GCN early-bird training framework dubbed GEBT that can significantly boost the efficiency of GCN training by (1) drawing joint early-bird tickets between the GCN graphs and models and (2) enabling simultaneously sparsification of both the GCN graphs and models. Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of our GEBT, e.g., our GEBT achieves up to 80.2% ~ 85.6% and 84.6% ~ 87.5% savings of GCN training and inference costs while offering a comparable or even better accuracy as compared to state-of-the-art methods. Our source code and supplementary appendix are available at https://github.com/RICE-EIC/Early-Bird-GCN.",
    "code_link": "https://github.com/RICE-EIC/Early-Bird-GCN"
  },
  "aaai2022_main_hindsightnetworkcreditassignmentefficientcreditassignmentinnetworksofdiscretestochasticunits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hindsight Network Credit Assignment: Efficient Credit Assignment in Networks of Discrete Stochastic Units",
    "authors": [
      "Kenny Young"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20874",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20874/20633",
    "published": "2022-02",
    "summary": "Training neural networks with discrete stochastic variables presents a unique challenge. Backpropagation is not directly applicable, nor are the reparameterization tricks used in networks with continuous stochastic variables. To address this challenge, we present Hindsight Network Credit Assignment (HNCA), a novel gradient estimation algorithm for networks of discrete stochastic units. HNCA works by assigning credit to each unit based on the degree to which its output influences its immediate children in the network. We prove that HNCA produces unbiased gradient estimates with reduced variance compared to the REINFORCE estimator, while the computational cost is similar to that of backpropagation. We first apply HNCA in a contextual bandit setting to optimize a reward function that is unknown to the agent. In this setting, we empirically demonstrate that HNCA significantly outperforms REINFORCE, indicating that the variance reduction implied by our theoretical analysis is significant and impactful. We then show how HNCA can be extended to optimize a more general function of the outputs of a network of stochastic units, where the function is known to the agent. We apply this extended version of HNCA to train a discrete variational auto-encoder and empirically show it compares favourably to other strong methods. We believe that the ideas underlying HNCA can help stimulate new ways of thinking about efficient credit assignment in stochastic compute graphs.",
    "code_link": "https://github.com/kenjyoung/HNCA"
  },
  "aaai2022_main_sailself-augmentedgraphcontrastivelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SAIL: Self-Augmented Graph Contrastive Learning",
    "authors": [
      "Lu Yu",
      "Shichao Pei",
      "Lizhong Ding",
      "Jun Zhou",
      "Longfei Li",
      "Chuxu Zhang",
      "Xiangliang Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20875",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20875/20634",
    "published": "2022-02",
    "summary": "This paper studies learning node representations with graph neural networks (GNNs) for unsupervised scenario. Specifically, we derive a theoretical analysis and provide an empirical demonstration about the non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined. The performance of GNNs depends on both the node feature smoothness and the locality of graph structure. To smooth the discrepancy of node proximity measured by graph topology and node feature, we proposed SAIL - a novel self-augmented graph contrastive learning framework, with two complementary self-distilling regularization modules, i.e., intra- and inter-graph knowledge distillation. We demonstrate the competitive performance of SAIL on a variety of graph applications. Even with a single GNN layer, SAIL has consistently competitive or even better performance on various benchmark datasets, comparing with state-of-the-art baselines.",
    "code_link": ""
  },
  "aaai2022_main_naturalblack-boxadversarialexamplesagainstdeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Natural Black-Box Adversarial Examples against Deep Reinforcement Learning",
    "authors": [
      "Mengran Yu",
      "Shiliang Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20876",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20876/20635",
    "published": "2022-02",
    "summary": "Black-box attacks in deep reinforcement learning usually retrain substitute policies to mimic behaviors of target policies as well as craft adversarial examples, and attack the target policies with these transferable adversarial examples. However, the transferability of adversarial examples is not always guaranteed. Moreover, current methods of crafting adversarial examples only utilize simple pixel space metrics which neglect semantics in the whole images, and thus generate unnatural adversarial examples. To address these problems, we propose an advRL-GAN framework to directly generate semantically natural adversarial examples in the black-box setting, bypassing the transferability requirement of adversarial examples. It formalizes the black-box attack as a reinforcement learning (RL) agent, which explores natural and aggressive adversarial examples with generative adversarial networks and the feedback of target agents. To the best of our knowledge, it is the first RL-based adversarial attack on a deep RL agent. Experimental results on multiple environments demonstrate the effectiveness of advRL-GAN in terms of reward reductions and magnitudes of perturbations, and validate the sparse and targeted property of adversarial perturbations through visualization.",
    "code_link": ""
  },
  "aaai2022_main_regularizationpenaltyoptimizationforaddressingdataqualityvarianceinoodalgorithms": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Regularization Penalty Optimization for Addressing Data Quality Variance in OoD Algorithms",
    "authors": [
      "Runpeng Yu",
      "Hong Zhu",
      "Kaican Li",
      "Lanqing Hong",
      "Rui Zhang",
      "Nanyang Ye",
      "Shao-Lun Huang",
      "Xiuqiang He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20877",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20877/20636",
    "published": "2022-02",
    "summary": "Due to the poor generalization performance of traditional empirical risk minimization (ERM) in the case of distributional shift, Out-of-Distribution (OoD) generalization algorithms receive increasing attention. However, OoD generalization algorithms overlook the great variance in the quality of training data, which significantly compromises the accuracy of these methods. In this paper, we theoretically reveal the relationship between training data quality and algorithm performance, and analyze the optimal regularization scheme for Lipschitz regularized invariant risk minimization. A novel algorithm is proposed based on the theoretical results to alleviate the influence of low quality data at both the sample level and the domain level. The experiments on both the regression and classification benchmarks validate the effectiveness of our method with statistical significance.",
    "code_link": ""
  },
  "aaai2022_main_low-passgraphconvolutionalnetworkforrecommendation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Low-Pass Graph Convolutional Network for Recommendation",
    "authors": [
      "Wenhui Yu",
      "Zixin Zhang",
      "Zheng Qin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20878",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20878/20637",
    "published": "2022-02",
    "summary": "Spectral graph convolution is extremely time-consuming for large graphs, thus existing Graph Convolutional Networks (GCNs) reconstruct the kernel by a polynomial, which is (almost) fixed. To extract features from the graph data by learning kernels, Low-pass Collaborative Filter Network (LCFN) was proposed as a new paradigm with trainable kernels. However, there are two demerits of LCFN: (1) The hypergraphs in LCFN are constructed by mining 2-hop connections of the user-item bipartite graph, thus 1-hop connections are not used, resulting in serious information loss. (2) LCFN follows the general network structure of GCNs, which is suboptimal. To address these issues, we utilize the bipartite graph to define the graph space directly and explore the best network structure based on experiments. Comprehensive experiments on two real-world datasets demonstrate the effectiveness of the proposed model. Codes are available on https://github.com/Wenhui-Yu/LCFN.",
    "code_link": "https://github.com/Wenhui-Yu/LCFN"
  },
  "aaai2022_main_mia-formerefficientandrobustvisiontransformersviamulti-grainedinput-adaptation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-Grained Input-Adaptation",
    "authors": [
      "Zhongzhi Yu",
      "Yonggan Fu",
      "Sicheng Li",
      "Chaojian Li",
      "Yingyan Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20879",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20879/20638",
    "published": "2022-02",
    "summary": "Vision transformers have recently demonstrated great success in various computer vision tasks, motivating a tremendously increased interest in their deployment into many real-world IoT applications. However, powerful ViTs are often too computationally expensive to be fitted onto real-world resource-constrained platforms, due to (1) their quadratically increased complexity with the number of input tokens and (2) their overparameterized self-attention heads and model depth. In parallel, different images are of varied complexity and their different regions can contain various levels of visual information, e.g., a sky background is not as informative as a foreground object in object classification tasks, indicating that treating those regions equally in terms of model complexity is unnecessary while such opportunities for trimming down ViTs' complexity have not been fully exploited.To this end, we propose a Multi-grained Input-Adaptive Vision Transformer framework dubbed MIA-Former that can input-adaptively adjust the structure of ViTs at three coarse-to-fine-grained granularities (i.e., model depth and the number of model heads/tokens).In particular, our MIA-Former adopts a low-cost network trained with a hybrid supervised and reinforcement learning method to skip the unnecessary layers, heads, and tokens in an input adaptive manner, reducing the overall computational cost. Furthermore, an interesting side effect of our MIA-Former is that its resulting ViTs are naturally equipped with improved robustness against adversarial attacks over their static counterparts, because MIA-Former's multi-grained dynamic control improves the model diversity similar to the effect of ensemble and thus increases the difficulty of adversarial attacks against all its sub-models.Extensive experiments and ablation studies validate that the proposed MIA-Former framework can (1) effectively allocate adaptive computation budgets to the difficulty of input images, achieving state-of-the-art (SOTA) accuracy-efficiency trade-offs, e.g., up to 16.5\\% computation savings with the same or even a higher accuracy compared with the SOTA dynamic transformer models, and (2) boost ViTs' robustness accuracy under various adversarial attacks over their vanilla counterparts by 2.4\\% and 3.0\\%, respectively. Our code is available at https://github.com/RICE-EIC/MIA-Former.",
    "code_link": "https://github.com/RICE-EIC/MIA-Former"
  },
  "aaai2022_main_unsupervisedlearningofcompositionalscenerepresentationsfrommultipleunspecifiedviewpoints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Learning of Compositional Scene Representations from Multiple Unspecified Viewpoints",
    "authors": [
      "Jinyang Yuan",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20880",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20880/20639",
    "published": "2022-02",
    "summary": "Visual scenes are extremely rich in diversity, not only because there are infinite combinations of objects and background, but also because the observations of the same scene may vary greatly with the change of viewpoints. When observing a visual scene that contains multiple objects from multiple viewpoints, humans are able to perceive the scene in a compositional way from each viewpoint, while achieving the so-called ``object constancy'' across different viewpoints, even though the exact viewpoints are untold. This ability is essential for humans to identify the same object while moving and to learn from vision efficiently. It is intriguing to design models that have the similar ability. In this paper, we consider a novel problem of learning compositional scene representations from multiple unspecified viewpoints without using any supervision, and propose a deep generative model which separates latent representations into a viewpoint-independent part and a viewpoint-dependent part to solve this problem. To infer latent representations, the information contained in different viewpoints is iteratively integrated by neural networks. Experiments on several specifically designed synthetic datasets have shown that the proposed method is able to effectively learn from multiple unspecified viewpoints.",
    "code_link": ""
  },
  "aaai2022_main_ts2vectowardsuniversalrepresentationoftimeseries": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "TS2Vec: Towards Universal Representation of Time Series",
    "authors": [
      "Zhihan Yue",
      "Yujing Wang",
      "Juanyong Duan",
      "Tianmeng Yang",
      "Congrui Huang",
      "Yunhai Tong",
      "Bixiong Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20881",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20881/20640",
    "published": "2022-02",
    "summary": "This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.",
    "code_link": "https://github.com/yuezhihan/ts2vec"
  },
  "aaai2022_main_fractionaladaptivelinearunits": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fractional Adaptive Linear Units",
    "authors": [
      "Julio Zamora",
      "Anthony D. Rhodes",
      "Lama Nachman"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20882",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20882/20641",
    "published": "2022-02",
    "summary": "This work introduces Fractional Adaptive Linear Units (FALUs), a flexible generalization of adaptive activation functions. Leveraging principles from fractional calculus, FALUs define a diverse family of activation functions (AFs) that encompass many traditional and state-of-the-art activation functions. This family includes the Sigmoid, Gaussian, ReLU, GELU, and Swish functions, as well as a large variety of smooth interpolations between these functions. Our technique requires only a small number of additional trainable parameters, and needs no further specialized optimization or initialization procedures. For this reason, FALUs present a seamless and rich automated solution to the problem of activation function optimization.Through experiments on a variety of conventional tasks and network architectures, we demonstrate the effectiveness of FALUs when compared to traditional and state-of-the-art AFs.To facilitate practical use of this work, we plan to make our code publicly available",
    "code_link": ""
  },
  "aaai2022_main_simsrsimpledistance-basedstaterepresentationsfordeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SimSR: Simple Distance-Based State Representations for Deep Reinforcement Learning",
    "authors": [
      "Hongyu Zang",
      "Xin Li",
      "Mingzhong Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20883",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20883/20642",
    "published": "2022-02",
    "summary": "This work explores how to learn robust and generalizable state representation from image-based observations with deep reinforcement learning methods. Addressing the computational complexity, stringent assumptions and representation collapse challenges in existing work of bisimulation metric, we devise Simple State Representation (SimSR) operator. SimSR enables us to design a stochastic approximation method that can practically learn the mapping functions (encoders) from observations to latent representation space. In addition to the theoretical analysis and comparison with the existing work, we experimented and compared our work with recent state-of-the-art solutions in visual MuJoCo tasks. The results shows that our model generally achieves better performance and has better robustness and good generalization.",
    "code_link": "https://github.com/bit1029public/SimSR"
  },
  "aaai2022_main_efficientdecentralizedstochasticgradientdescentmethodfornonconvexfinite-sumoptimizationproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Decentralized Stochastic Gradient Descent Method for Nonconvex Finite-Sum Optimization Problems",
    "authors": [
      "Wenkang Zhan",
      "Gang Wu",
      "Hongchang Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20884",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20884/20643",
    "published": "2022-02",
    "summary": "Decentralized stochastic gradient descent methods have attracted increasing interest in recent years. Numerous methods have been proposed for the nonconvex finite-sum optimization problem.However, existing methods have a large sample complexity,slowing down the empirical convergence speed. To address this issue, in this paper, we proposed a novel decentralized stochastic gradient descent method for the nonconvex finite-sum optimization problem, which enjoys a better sample and communication complexity than existing methods. To the best of our knowledge, our work is the first one achieving such favorable sample and communication complexities. Finally, we have conducted extensive experiments and the experimental results have confirmed the superior performance of our proposed method.",
    "code_link": ""
  },
  "aaai2022_main_metanodeprototypeoptimizationasaneuralodeforfew-shotlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning",
    "authors": [
      "Baoquan Zhang",
      "Xutao Li",
      "Shanshan Feng",
      "Yunming Ye",
      "Rui Ye"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20885",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20885/20644",
    "published": "2022-02",
    "summary": "Few-Shot Learning (FSL) is a challenging task, i.e., how to recognize novel classes with few examples? Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then predicting novel classes via a cosine nearest neighbor classifier with mean-based prototypes. Nevertheless, due to the data scarcity, the mean-based prototypes are usually biased. In this paper, we attempt to diminish the prototype bias by regarding it as a prototype optimization problem. To this end, we propose a novel meta-learning based prototype optimization framework to rectify prototypes, i.e., introducing a meta-optimizer to optimize prototypes. Although the existing meta-optimizers can also be adapted to our framework, they all overlook a crucial gradient bias issue, i.e., the mean-based gradient estimation is also biased on sparse data. To address the issue, we regard the gradient and its flow as meta-knowledge and then propose a novel Neural Ordinary Differential Equation (ODE)-based meta-optimizer to polish prototypes, called MetaNODE. In this meta-optimizer, we first view the mean-based prototypes as initial prototypes, and then model the process of prototype optimization as continuous-time dynamics specified by a Neural ODE. A gradient flow inference network is carefully designed to learn to estimate the continuous gradient flow for prototype dynamics. Finally, the optimal prototypes can be obtained by solving the Neural ODE. Extensive experiments on miniImagenet, tieredImagenet, and CUB-200-2011 show the effectiveness of our method.",
    "code_link": ""
  },
  "aaai2022_main_statedeviationcorrectionforofflinereinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "State Deviation Correction for Offline Reinforcement Learning",
    "authors": [
      "Hongchang Zhang",
      "Jianzhun Shao",
      "Yuhang Jiang",
      "Shuncheng He",
      "Guanwen Zhang",
      "Xiangyang Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20886",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20886/20645",
    "published": "2022-02",
    "summary": "Offline reinforcement learning aims to maximize the expected cumulative rewards with a fixed collection of data. The basic principle of current offline reinforcement learning methods is to restrict the policy to the offline dataset action space. However, they ignore the case where the dataset's trajectories fail to cover the state space completely. Especially, when the dataset's size is limited, it is likely that the agent would encounter unseen states during test time. Prior policy-constrained methods are incapable of correcting the state deviation, and may lead the agent to its unexpected regions further. In this paper, we propose the state deviation correction (SDC) method to constrain the policy's induced state distribution by penalizing the out-of-distribution states which might appear during the test period. We first perturb the states sampled from the logged dataset, then simulate noisy next states on the basis of a dynamics model and the policy. We then train the policy to minimize the distances between the noisy next states and the offline dataset. In this manner, we allow the trained policy to guide the agent to its familiar regions. Experimental results demonstrate that our proposed method is competitive with the state-of-the-art methods in a GridWorld setup, offline Mujoco control suite, and a modified offline Mujoco dataset with a finite number of valuable samples.",
    "code_link": ""
  },
  "aaai2022_main_multi-agentreinforcementlearningwithgeneralutilitiesviadecentralizedshadowrewardactor-critic": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Agent Reinforcement Learning with General Utilities via Decentralized Shadow Reward Actor-Critic",
    "authors": [
      "Junyu Zhang",
      "Amrit Singh Bedi",
      "Mengdi Wang",
      "Alec Koppel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20887",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20887/20646",
    "published": "2022-02",
    "summary": "We posit a new mechanism for cooperation in multi-agent reinforcement learning (MARL)based upon any nonlinear function of the team's long-term state-action occupancy measure, i.e., a general utility. This subsumes the cumulative return but also allows one to incorporate risk-sensitivity, exploration, and priors. We derive the Decentralized Shadow Reward Actor-Critic (DSAC) in which agents alternate between policy evaluation (critic), weighted averaging with neighbors (information mixing), and local gradient updates for their policy parameters (actor). DSAC augments the classic critic step by requiring agents to (i) estimate their local occupancy measure in order to (ii) estimate the derivative of the local utility with respect to their occupancy measure, i.e., the ``shadow reward\". DSAC converges to \u03f5-stationarity in O(1/\u03f5^2.5) or faster O(1/\u03f5^2) steps with high probability, depending on the amount of communications. We further establish the non-existence of spurious stationary points for this problem, that is, DSAC finds the globally optimal policy. Experiments demonstrate the merits of goals beyond the cumulative return in cooperative MARL.",
    "code_link": ""
  },
  "aaai2022_main_co-promotionpredictionsoffinancingmarketandsalesmarketacooperative-competitiveattentionapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Co-promotion Predictions of Financing Market and Sales Market: A Cooperative-Competitive Attention Approach",
    "authors": [
      "Lei Zhang",
      "Wang Xiang",
      "Chuang Zhao",
      "Hongke Zhao",
      "Rui Li",
      "Runze Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20888",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20888/20647",
    "published": "2022-02",
    "summary": "Market popularity prediction has always been a hot research topic, such as sales prediction and crowdfunding prediction. Most of these studies put the perspective on isolated markets, relying on the knowledge of certain market to maximize the prediction performance. However, these market-specific approaches are restricted by the knowledge limitation of isolated markets and incapable of the complicated and potential relations among different markets, especially some with strong dependence such as the financing market and sales market. Fortunately, we discover potentially symbiotic relations between the financing market and the sales market, which provides us with an opportunity to co-promote the popularity predictions of both markets. Thus, for bridgly learning the knowledge interactions between financing market and sales market, we propose a cross-market approach, namely CATN: Cooperative-competitive Attention Transfer Network, which could effectively transfer knowledge of financing capability from the crowdfunding market and sales prospect from the E-commerce market. Specifically, for capturing the complicated relations especially the cooperation or complement of items and enhancing the knowledge transfer between the two heterogeneous markets, we design a novel Cooperative Attention; meanwhile, for finely computing the relations of items especially the competition in specific same market, we further design Competitive Attentions for the two markets respectively. Besides, we also distinguish aligned features and unique features to adapt the cross-market predictions. With the real-world datasets collected from Indiegogo and Amazon, we construct extensive experiments on three types of datasets from the two markets and the results demonstrate the effectiveness and generalization of our CATN model.",
    "code_link": ""
  },
  "aaai2022_main_categoricalneighbourcorrelationcoefficient(cncor)fordetectingrelationshipsbetweencategoricalvariables": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Categorical Neighbour Correlation Coefficient (CnCor) for Detecting Relationships between Categorical Variables",
    "authors": [
      "Lifeng Zhang",
      "Shimo Yang",
      "Hongxun Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20889",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20889/20648",
    "published": "2022-02",
    "summary": "Categorical data is common and, however, special in that its possible values exist only on a nominal scale so that many statistical operations such as mean, variance, and covariance become not applicable. Following the basic idea of the neighbour correlation coefficient (nCor), in this study, we propose a new measure named the categorical nCor (CnCor) to examine the association between categorical variables through using indicator functions to reform the distance metric and product-moment correlation coefficient. The proposed measure is easy to compute, and enables a direct test of statistical dependence without the need of converting the qualitative variables to quantitative ones. Compare to previous approaches, it is much more robust and effective in dealing with multi-categorical target variables especially when highly nonlinear relationships occurs in the multivariate case. We also applied the CnCor to implementing feature selection by the scheme of backward elimination. Finally, extensive experiments performed on both synthetic and real-world datasets are conducted to demonstrate the outstanding performance of the proposed methods, and draw comparisons with state-of-the-art association measures and feature selection algorithms.",
    "code_link": ""
  },
  "aaai2022_main_interpretabledomainadaptationforhiddensubdomainalignmentinthecontextofpre-trainedsourcemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Interpretable Domain Adaptation for Hidden Subdomain Alignment in the Context of Pre-trained Source Models",
    "authors": [
      "Luxin Zhang",
      "Pascal Germain",
      "Yacine Kessaci",
      "Christophe Biernacki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20890",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20890/20649",
    "published": "2022-02",
    "summary": "Domain adaptation aims to leverage source domain knowledge to predict target domain labels. Most domain adaptation methods tackle a single-source, single-target scenario, whereas source and target domain data can often be subdivided into data from different distributions in real-life applications (e.g., when the distribution of the collected data changes with time). However, such subdomains are rarely given and should be discovered automatically. To this end, some recent domain adaptation works seek separations of hidden subdomains, w.r.t. a known or fixed number of subdomains. In contrast, this paper introduces a new subdomain combination method that leverages a variable number of subdomains. Precisely, we propose to use an inter-subdomain divergence maximization criterion to exploit hidden subdomains. Besides, our proposition stands in a target-to-source domain adaptation scenario, where one exploits a pre-trained source model as a black box; thus, the proposed method is model-agnostic.By providing interpretability at two complementary levels (transformation and subdomain levels), our method can also be easily interpreted by practitioners with or without machine learning backgrounds.Experimental results over two fraud detection datasets demonstrate the efficiency of our method.",
    "code_link": ""
  },
  "aaai2022_main_convergenceandoptimalityofpolicygradientmethodsinweaklysmoothsettings": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings",
    "authors": [
      "Matthew S. Zhang",
      "Murat A Erdogdu",
      "Animesh Garg"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20891",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20891/20650",
    "published": "2022-02",
    "summary": "Policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success, yet existing convergence analysis still relies on non-intuitive, impractical and often opaque conditions. In particular, existing rates are achieved in limited settings, under strict regularity conditions. In this work, we establish explicit convergence rates of policy gradient methods, extending the convergence regime to weakly smooth policy classes with L2 integrable gradient. We provide intuitive examples to illustrate the insight behind these new conditions. Notably, our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions. Lastly we provide performance guarantees for the converged policies.",
    "code_link": ""
  },
  "aaai2022_main_gaussianprocessbanditswithaggregatedfeedback": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Gaussian Process Bandits with Aggregated Feedback",
    "authors": [
      "Mengyan Zhang",
      "Russell Tsuchida",
      "Cheng Soon Ong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20892",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20892/20651",
    "published": "2022-02",
    "summary": "We consider the continuum-armed bandits problem, under a novel setting of recommending the best arms within a fixed budget under aggregated feedback. This is motivated by applications where the precise rewards are impossible or expensive to obtain, while an aggregated reward or feedback, such as the average over a subset, is available. We constrain the set of reward functions by assuming that they are from a Gaussian Process and propose the Gaussian Process Optimistic Optimisation (GPOO) algorithm. We adaptively construct a tree with nodes as subsets of the arm space, where the feedback is the aggregated reward of representatives of a node. We propose a new simple regret notion with respect to aggregated feedback on the recommended arms. We provide theoretical analysis for the proposed algorithm, and recover single point feedback as a special case. We illustrate GPOO and compare it with related algorithms on simulated data.",
    "code_link": ""
  },
  "aaai2022_main_rethinkinginfluencefunctionsofneuralnetworksintheover-parameterizedregime": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Rethinking Influence Functions of Neural Networks in the Over-Parameterized Regime",
    "authors": [
      "Rui Zhang",
      "Shihua Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20893",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20893/20652",
    "published": "2022-02",
    "summary": "Understanding the black-box prediction for neural networks is challenging. To achieve this, early studies have designed influence function (IF) to measure the effect of removing a single training point on neural networks. However, the classic implicit Hessian-vector product (IHVP) method for calculating IF is fragile, and theoretical analysis of IF in the context of neural networks is still lacking. To this end, we utilize the neural tangent kernel (NTK) theory to calculate IF for the neural network trained with regularized mean-square loss, and prove that the approximation error can be arbitrarily small when the width is sufficiently large for two-layer ReLU networks. We analyze the error bound for the classic IHVP method in the over-parameterized regime to understand when and why it fails or not. In detail, our theoretical analysis reveals that (1) the accuracy of IHVP depends on the regularization term, and is pretty low under weak regularization; (2) the accuracy of IHVP has a significant correlation with the probability density of corresponding training points. We further borrow the theory from NTK to understand the IFs better, including quantifying the complexity for influential samples and depicting the variation of IFs during the training dynamics. Numerical experiments on real-world data confirm our theoretical results and demonstrate our findings.",
    "code_link": ""
  },
  "aaai2022_main_amulti-agentreinforcementlearningapproachforefficientclientselectioninfederatedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Multi-Agent Reinforcement Learning Approach for Efficient Client Selection in Federated Learning",
    "authors": [
      "Sai Qian Zhang",
      "Jieyu Lin",
      "Qi Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20894",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20894/20653",
    "published": "2022-02",
    "summary": "Federated learning (FL) is a training technique that enables client devices to jointly learn a shared model by aggregating locally computed models without exposing their raw data. While most of the existing work focuses on improving the FL model accuracy, in this paper, we focus on the improving the training efficiency, which is often a hurdle for adopting FL in real world applications. Specifically, we design an efficient FL framework which jointly optimizes model accuracy, processing latency and communication efficiency, all of which are primary design considerations for real implementation of FL. Inspired by the recent success of Multi Agent Reinforcement Learning (MARL) in solving complex control problems, we present FedMarl, a federated learning framework that relies on trained MARL agents to perform efficient run-time client selection. Experiments show that FedMarl can significantly improve model accuracy with much lower processing latency and communication cost.",
    "code_link": ""
  },
  "aaai2022_main_tailorversatilemulti-modallearningformulti-labelemotionrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition",
    "authors": [
      "Yi Zhang",
      "Mingyuan Chen",
      "Jundong Shen",
      "Chongjun Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20895",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20895/20654",
    "published": "2022-02",
    "summary": "Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), aiming to refine multi-modal representations and enhance discriminative capacity of each label. Specifically, we design an adversarial multi-modal refinement module to sufficiently explore the commonality among different modalities and strengthen the diversity of each modality. To further exploit label-modal dependence, we devise a BERT-like cross-modal encoder to gradually fuse private and common modality representations in a granularity descent way, as well as a label-guided decoder to adaptively generate a tailored representation for each label with the guidance of label semantics. In addition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in both aligned and unaligned settings, which demonstrate the superiority of TAILOR over the state-of-the-arts.",
    "code_link": "https://github.com/kniter1/TAILOR"
  },
  "aaai2022_main_fusionmultiplekernelk-means": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fusion Multiple Kernel K-means",
    "authors": [
      "Yi Zhang",
      "Xinwang Liu",
      "Jiyuan Liu",
      "Sisi Dai",
      "Changwang Zhang",
      "Kai Xu",
      "En\n      Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20896",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20896/20655",
    "published": "2022-02",
    "summary": "Multiple kernel clustering aims to seek an appropriate combination of base kernels to mine inherent non-linear information for optimal clustering. Late fusion algorithms generate base partitions independently and integrate them in the following clustering procedure, improving the overall efficiency. However, the separate base partition generation leads to inadequate negotiation with the clustering procedure and a great loss of beneficial information in corresponding kernel matrices, which negatively affects the clustering performance. To address this issue, we propose a novel algorithm, termed as Fusion Multiple Kernel k-means (FMKKM), which unifies base partition learning and late fusion clustering into one single objective function, and adopts early fusion technique to capture more sufficient information in kernel matrices. Specifically, the early fusion helps base partitions keep more beneficial kernel details, and the base partitions learning further guides the generation of consensus partition in the late fusion stage, while the late fusion provides positive feedback on two former procedures.The close collaboration of three procedures results in a promising performance improvement. Subsequently, an alternate optimization method with promising convergence is developed to solve the resultant optimization problem. Comprehensive experimental results demonstrate that our proposed algorithm achieves state-of-the-art performance on multiple public datasets, validating its effectiveness. The code of this work is publicly available at https://github.com/ethan-yizhang/Fusion-Multiple-Kernel-K-means.",
    "code_link": "https://github.com/ethan-yizhang/Fusion-Multiple-KernelK-means"
  },
  "aaai2022_main_batchactivelearningwithgraphneuralnetworksviamulti-agentdeepreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Batch Active Learning with Graph Neural Networks via Multi-Agent Deep Reinforcement Learning",
    "authors": [
      "Yuheng Zhang",
      "Hanghang Tong",
      "Yinglong Xia",
      "Yan Zhu",
      "Yuejie Chi",
      "Lei Ying"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20897",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20897/20656",
    "published": "2022-02",
    "summary": "Graph neural networks (GNNs) have achieved tremendous success in many graph learning tasks such as node classification, graph classification and link prediction. For the classification task, GNNs' performance often highly depends on the number of labeled nodes and thus could be significantly hampered due to the expensive annotation cost. The sparse literature on active learning for GNNs has primarily focused on selecting only one sample each iteration, which becomes inefficient for large scale datasets. In this paper, we study the batch active learning setting for GNNs where the learning agent can acquire labels of multiple samples at each time. We formulate batch active learning as a cooperative multi-agent reinforcement learning problem and present a novel reinforced batch-mode active learning framework BiGeNe. To avoid the combinatorial explosion of the joint action space, we introduce a value decomposition method that factorizes the total Q-value into the average of individual Q-values. Moreover, we propose a novel multi-agent Q-network consisting of a graph convolutional network (GCN) component and a gated recurrent unit (GRU) component. The GCN component takes both the informativeness and inter-dependences between nodes into account and the GRU component enables the agent to consider interactions between selected nodes in the same batch. Experimental results on multiple public datasets demonstrate the effectiveness and efficiency of our proposed method.",
    "code_link": ""
  },
  "aaai2022_main_protgnntowardsself-explaininggraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
    "authors": [
      "Zaixi Zhang",
      "Qi Liu",
      "Hao Wang",
      "Chengqiang Lu",
      "Cheekong Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20898",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20898/20657",
    "published": "2022-02",
    "summary": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictionsmade by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.",
    "code_link": "https://github.com/zaixizhang/ProtGNN"
  },
  "aaai2022_main_learningtosolvetravellingsalesmanproblemwithhardness-adaptivecurriculum": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Solve Travelling Salesman Problem with Hardness-Adaptive Curriculum",
    "authors": [
      "Zeyang Zhang",
      "Ziwei Zhang",
      "Xin Wang",
      "Wenwu Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20899",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20899/20658",
    "published": "2022-02",
    "summary": "Various neural network models have been proposed to tackle combinatorial optimization problems such as the travelling salesman problem (TSP). Existing learning-based TSP methods adopt a simple setting that the training and testing data are independent and identically distributed. However, the existing literature fails to solve TSP instances when training and testing data have different distributions. Concretely, we find that different training and testing distribution will result in more difficult TSP instances, i.e., the solution obtained by the model has a large gap from the optimal solution. To tackle this problem, in this work, we study learning-based TSP methods when training and testing data have different distributions using adaptive-hardness, i.e., how difficult a TSP instance can be for a solver.This problem is challenging because it is non-trivial to (1) define hardness measurement quantitatively; (2) efficiently and continuously generate sufficiently hard TSP instances upon model training; (3) fully utilize instances with different levels of hardness to learn a more powerful TSP solver. To solve these challenges, we first propose a principled hardness measurement to quantify the hardness of TSP instances. Then, we propose a hardness-adaptive generator to generate instances with different hardness. We further propose a curriculum learner fully utilizing these instances to train the TSP solver. Experiments show that our hardness-adaptive generator can generate instances ten times harder than the existing methods, and our proposed method achieves significant improvement over state-of-the-art models in terms of the optimality gap. The codes are publicly available.",
    "code_link": "https://github.com/wondergo2017/TSP-HAC"
  },
  "aaai2022_main_robustactiongapincreasingwithclippedadvantagelearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Action Gap Increasing with Clipped Advantage Learning",
    "authors": [
      "Zhe Zhang",
      "Yaozhong Gan",
      "Xiaoyang Tan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20900",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20900/20659",
    "published": "2022-02",
    "summary": "Advantage Learning (AL) seeks to increase the action gap between the optimal action and its competitors, so as to improve the robustness to estimation errors. However, the method becomes problematic when the optimal action induced by the approximated value function does not agree with the true optimal action. In this paper, we present a novel method, named clipped Advantage Learning (clipped AL), to address this issue. The method is inspired by our observation that increasing the action gap blindly for all given samples while not taking their necessities into account could accumulate more errors in the performance loss bound, leading to a slow value convergence, and to avoid that, we should adjust the advantage value adaptively. We show that our simple clipped AL operator not only enjoys fast convergence guarantee but also retains proper action gaps, hence achieving a good balance between the large action gap and the fast convergence. The feasibility and effectiveness of the proposed method are verified empirically on several RL benchmarks with promising performance.",
    "code_link": "https://github.com/ntasfi/PyGame-Learning-Environment"
  },
  "aaai2022_main_onlineinfluencemaximizationwithnode-levelfeedbackusingstandardofflineoracles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Influence Maximization with Node-Level Feedback Using Standard Offline Oracles",
    "authors": [
      "Zhijie Zhang",
      "Wei Chen",
      "Xiaoming Sun",
      "Jialin Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20901",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20901/20660",
    "published": "2022-02",
    "summary": "We study the online influence maximization (OIM) problem in social networks, where in multiple rounds the learner repeatedly chooses seed nodes to generate cascades, observes the cascade feedback, and gradually learns the best seeds that generate the largest cascade. We focus on two major challenges in this paper. First, we work with node-level feedback instead of edge-level feedback. The edge-level feedback reveals all edges that pass through information in a cascade, whereas the node-level feedback only reveals the activated nodes with timestamps. The node-level feedback is arguably more realistic since in practice it is relatively easy to observe who is influenced but very difficult to observe from which relationship (edge) the influence comes. Second, we use standard offline oracles instead of offline pair-oracles. To compute a good seed set for the next round, an offline pair-oracle finds the best seed set and the best parameters within the confidence region simultaneously, and such an oracle is difficult to compute due to the combinatorial core of the OIM problem. So we focus on how to use the standard offline influence maximization oracle which finds the best seed set given the edge parameters as input. In this paper, we resolve these challenges for the famous independent cascade (IC) diffusion model. The past research only achieves edge-level feedback, while we present the first optimal regret algorithm for the node-level feedback. For the first challenge above, we apply a novel adaptation of the maximum likelihood estimation (MLE) approach to learn the graph parameters and its confidence region (a confidence ellipsoid). For the second challenge, we adjust the update procedure to dissect the confidence ellipsoid into confidence intervals on each parameter, so that the standard offline influence maximization oracle is enough.",
    "code_link": ""
  },
  "aaai2022_main_clpaclean-labelpoisoningavailabilityattacksusinggenerativeadversarialnets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CLPA: Clean-Label Poisoning Availability Attacks Using Generative Adversarial Nets",
    "authors": [
      "Bingyin Zhao",
      "Yingjie Lao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20902",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20902/20661",
    "published": "2022-02",
    "summary": "Poisoning attacks are emerging threats to deep neural networks where the adversaries attempt to compromise the models by injecting malicious data points in the clean training data. Poisoning attacks target either the availability or integrity of a model. The availability attack aims to degrade the overall accuracy while the integrity attack causes misclassification only for specific instances without affecting the accuracy of clean data. Although clean-label integrity attacks are proven to be effective in recent studies, the feasibility of clean-label availability attacks remains unclear. This paper, for the first time, proposes a clean-label approach, CLPA, for the poisoning availability attack. We reveal that due to the intrinsic imperfection of classifiers, naturally misclassified inputs can be considered as a special type of poisoned data, which we refer to as \"natural poisoned data''. We then propose a two-phase generative adversarial net (GAN) based poisoned data generation framework along with a triplet loss function for synthesizing clean-label poisoned samples that locate in a similar distribution as natural poisoned data. The generated poisoned data are plausible to human perception and can also bypass the singular vector decomposition (SVD) based defense. We demonstrate the effectiveness of our approach on CIFAR-10 and ImageNet dataset over a variety type of models. Codes are available at: https://github.com/bxz9200/CLPA.",
    "code_link": ""
  },
  "aaai2022_main_fedinvbyzantine-robustfederatedlearningbyinversinglocalmodelupdates": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "FedInv: Byzantine-Robust Federated Learning by Inversing Local Model Updates",
    "authors": [
      "Bo Zhao",
      "Peng Sun",
      "Tao Wang",
      "Keyu Jiang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20903",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20903/20662",
    "published": "2022-02",
    "summary": "Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables multiple clients to collaboratively train statistical models without disclosing raw training data. However, the inaccessible local training data and uninspectable local training process make FL susceptible to various Byzantine attacks (e.g., data poisoning and model poisoning attacks), aiming to manipulate the FL model training process and degrade the model performance. Most of the existing Byzantine-robust FL schemes cannot effectively defend against stealthy poisoning attacks that craft poisoned models statistically similar to benign models. Things worsen when many clients are compromised or data among clients are highly non-independent and identically distributed (non-IID). In this work, to address these issues, we propose FedInv, a novel Byzantine-robust FL framework by inversing local model updates. Specifically, in each round of local model aggregation in FedInv, the parameter server first inverses the local model updates submitted by each client to generate a corresponding dummy dataset. Then, the server identifies those dummy datasets with exceptional Wasserstein distances from others and excludes the related local model updates from model aggregation. We conduct an exhaustive experimental evaluation of FedInv. The results demonstrate that FedInv significantly outperforms the existing robust FL schemes in defending against stealthy poisoning attacks under highly non-IID data partitions.",
    "code_link": ""
  },
  "aaai2022_main_well-classifiedexamplesareunderestimatedinclassificationwithdeepneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Well-Classified Examples Are Underestimated in Classification with Deep Neural Networks",
    "authors": [
      "Guangxiang Zhao",
      "Wenkai Yang",
      "Xuancheng Ren",
      "Lei Li",
      "Yunfang Wu",
      "Xu Sun"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20904",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20904/20663",
    "published": "2022-02",
    "summary": "The conventional wisdom behind learning deep classification models is to focus on bad-classified examples and ignore well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically show that this common practice hinders representation learning, energy optimization, and margin growth. To counteract this deficiency, we propose to reward well-classified examples with additive bonuses to revive their contribution to the learning process. This counterexample theoretically addresses these three issues. We empirically support this claim by directly verifying the theoretical results or significant performance improvement with our counterexample on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, this paper shows that we can deal with complex scenarios, such as imbalanced classification, OOD detection, and applications under adversarial attacks because our idea can solve these three issues. Code is available at https://github.com/lancopku/well-classified-examples-are-underestimated.",
    "code_link": "https://github.com/lancopku/well-classified-examples-areunderestimated"
  },
  "aaai2022_main_error-basedknockoffsinferenceforcontrolledfeatureselection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Error-Based Knockoffs Inference for Controlled Feature Selection",
    "authors": [
      "Xuebin Zhao",
      "Hong Chen",
      "Yingjie Wang",
      "Weifu Li",
      "Tieliang Gong",
      "Yulong\n      Wang",
      "Feng Zheng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20905",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20905/20664",
    "published": "2022-02",
    "summary": "Recently, the scheme of model-X knockoffs was proposed as a promising solution to address controlled feature selection under high-dimensional finite-sample settings. However, the procedure of model-X knockoffs depends heavily on the coefficient-based feature importance and only concerns the control of false discovery rate (FDR). To further improve its adaptivity and flexibility, in this paper, we propose an error-based knockoff inference method by integrating the knockoff features, the error-based feature importance statistics, and the stepdown procedure together. The proposed inference procedure does not require specifying a regression model and can handle feature selection with theoretical guarantees on controlling false discovery proportion (FDP), FDR, or k-familywise error rate (k-FWER). Empirical evaluations demonstrate the competitive performance of our approach on both simulated and real data.",
    "code_link": ""
  },
  "aaai2022_main_onlinemissingvalueimputationandchangepointdetectionwiththegaussiancopula": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Missing Value Imputation and Change Point Detection with the Gaussian Copula",
    "authors": [
      "Yuxuan Zhao",
      "Eric Landgrebe",
      "Eliot Shekhtman",
      "Madeleine Udell"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20906",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20906/20665",
    "published": "2022-02",
    "summary": "Missing value imputation is crucial for real-world data science workflows. Imputation is harder in the online setting, as it requires the imputation method itself to be able to evolve over time. For practical applications, imputation algorithms should produce imputations that match the true data distribution, handle data of mixed types, including ordinal, boolean, and continuous variables, and scale to large datasets. In this work we develop a new online imputation algorithm for mixed data using the Gaussian copula. The online Gaussian copula model produces meets all the desiderata: its imputations match the data distribution even for mixed data, improve over its offline counterpart on the accuracy when the streaming data has a changing distribution, and on the speed (up to an order of magnitude) especially on large scale datasets. By fitting the copula model to online data, we also provide a new method to detect change points in the multivariate dependence structure for mixed data with missing values. Experimental results on synthetic and real world data validate the performance of the proposed methods.",
    "code_link": ""
  },
  "aaai2022_main_lassllabel-guidedself-trainingforsemi-supervisedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LaSSL: Label-Guided Self-Training for Semi-supervised Learning",
    "authors": [
      "Zhen Zhao",
      "Luping Zhou",
      "Lei Wang",
      "Yinghuan Shi",
      "Yang Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20907",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20907/20666",
    "published": "2022-02",
    "summary": "The key to semi-supervised learning (SSL) is to explore adequate information to leverage the unlabeled data. Current dominant approaches aim to generate pseudo-labels on weakly augmented instances and train models on their corresponding strongly augmented variants with high-confidence results. However, such methods are limited in excluding samples with low-confidence pseudo-labels and under-utilization of the label information. In this paper, we emphasize the cruciality of the label information and propose a Label-guided Self-training approach to Semi-supervised Learning (LaSSL), which improves pseudo-label generations from two mutually boosted strategies. First, with the ground-truth labels and iteratively-polished pseudo-labels, we explore instance relations among all samples and then minimize a class-aware contrastive loss to learn discriminative feature representations that make same-class samples gathered and different-class samples scattered. Second, on top of improved feature representations, we propagate the label information to the unlabeled samples across the potential data manifold at the feature-embedding level, which can further improve the labelling of samples with reference to their neighbours. These two strategies are seamlessly integrated and mutually promoted across the whole training process. We evaluate LaSSL on several classification benchmarks under partially labeled settings and demonstrate its superiority over the state-of-the-art approaches.",
    "code_link": "https://github.com/zhenzhao/lassl"
  },
  "aaai2022_main_stackelbergactor-criticgame-theoreticreinforcementlearningalgorithms": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms",
    "authors": [
      "Liyuan Zheng",
      "Tanner Fiez",
      "Zane Alumbaugh",
      "Benjamin Chasnov",
      "Lillian J.\n      Ratliff"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20908",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20908/20667",
    "published": "2022-02",
    "summary": "The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation. We adopt this viewpoint and model the actor and critic interaction as a two-player general-sum game with a leader-follower structure known as a Stackelberg game. Given this abstraction, we propose a meta-framework for Stackelberg actor-critic algorithms where the leader player follows the total derivative of its objective instead of the usual individual gradient. From a theoretical standpoint, we develop a policy gradient theorem for the refined update and provide a local convergence guarantee for the Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an empirical standpoint, we demonstrate via simple examples that the learning dynamics we study mitigate cycling and accelerate convergence compared to the usual gradient dynamics given cost structures induced by actor-critic formulations. Finally, extensive experiments on OpenAI gym environments show that Stackelberg actor-critic algorithms always perform at least as well and often significantly outperform the standard actor-critic algorithm counterparts.",
    "code_link": ""
  },
  "aaai2022_main_adaptivepairwiseweightsfortemporalcreditassignment": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adaptive Pairwise Weights for Temporal Credit Assignment",
    "authors": [
      "Zeyu Zheng",
      "Risto Vuorio",
      "Richard Lewis",
      "Satinder Singh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20909",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20909/20668",
    "published": "2022-02",
    "summary": "How much credit (or blame) should an action taken in a state get for a future reward? This is the fundamental temporal credit assignment problem in Reinforcement Learning (RL). One of the earliest and still most widely used heuristics is to assign this credit based on a scalar coefficient, lambda (treated as a hyperparameter), raised to the power of the time interval between the state-action and the reward. In this empirical paper, we explore heuristics based on more general pairwise weightings that are functions of the state in which the action was taken, the state at the time of the reward, as well as the time interval between the two. Of course it isn't clear what these pairwise weight functions should be, and because they are too complex to be treated as hyperparameters we develop a metagradient procedure for learning these weight functions during the usual RL training of a policy. Our empirical work shows that it is often possible to learn these pairwise weight functions during learning of the policy to achieve better performance than competing approaches.",
    "code_link": ""
  },
  "aaai2022_main_programmaticrewarddesignbyexample": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Programmatic Reward Design by Example",
    "authors": [
      "Weichao Zhou",
      "Wenchao Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20910",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20910/20669",
    "published": "2022-02",
    "summary": "Reward design is a fundamental problem in reinforcement learning (RL). A misspecified or poorly designed reward can result in low sample efficiency and undesired behaviors. In this paper, we propose the idea of programmatic reward design, i.e. using programs to specify the reward functions in RL environments. Programs allow human engineers to express sub-goals and complex task scenarios in a structured and interpretable way. The challenge of programmatic reward design, however, is that while humans can provide the high-level structures, properly setting the low-level details, such as the right amount of reward for a specific sub-task, remains difficult. A major contribution of this paper is a probabilistic framework that can infer the best candidate programmatic reward function from expert demonstrations. Inspired by recent generative-adversarial approaches, our framework searches for themost likely programmatic reward function under whichthe optimally generated trajectories cannot be differen-tiated from the demonstrated trajectories. Experimental results show that programmatic reward functions learned using this framework can significantly outperform those learned using existing reward learning algorithms, and enable RL agents to achieve state-of-the-art performance on highly complex tasks.",
    "code_link": "https://github.com/maximecb/gym-minigrid"
  },
  "aaai2022_main_neuralpiecewise-constantdelaydifferentialequations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Neural Piecewise-Constant Delay Differential Equations",
    "authors": [
      "Qunxi Zhu",
      "Yifei Shen",
      "Dongsheng Li",
      "Wei Lin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20911",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20911/20670",
    "published": "2022-02",
    "summary": "Continuous-depth neural networks, such as the Neural Ordinary Differential Equations (ODEs), have aroused a great deal of interest from the communities of machine learning and data science in recent years, which bridge the connection between deep neural networks and dynamical systems. In this article, we introduce a new sort of continuous-depth neural network, called the Neural Piecewise-Constant Delay Differential Equations (PCDDEs). Here, unlike the recently proposed framework of the Neural Delay Differential Equations (DDEs), we transform the single delay into the piecewise-constant delay(s). The Neural PCDDEs with such a transformation, on one hand, inherit the strength of universal approximating capability in Neural DDEs. On the other hand, the Neural PCDDEs, leveraging the contributions of the information from the multiple previous time steps, further promote the modeling capability without augmenting the network dimension. With such a promotion, we show that the Neural PCDDEs do outperform the several existing continuous-depth neural frameworks on the one-dimensional piecewise-constant delay population dynamics and real-world datasets, including MNIST, CIFAR10, and SVHN.",
    "code_link": ""
  },
  "aaai2022_main_structurallandmarkingandinteractionmodellinga\u201cslim\u201dnetworkforgraphclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Structural Landmarking and Interaction Modelling: A \u201cSLIM\u201d Network for Graph Classification",
    "authors": [
      "Yaokang Zhu",
      "Kai Zhang",
      "Jun Wang",
      "Haibin Ling",
      "Jie Zhang",
      "Hongyuan Zha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20912",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20912/20671",
    "published": "2022-02",
    "summary": "Graph neural networks are a promising architecture for learning and inference with graph-structured data. Yet, how to generate informative, fixed dimensional features for graphs with varying size and topology can still be challenging. Typically, this is achieved through graph-pooling, which summarizes a graph by compressing all its nodes into a single vector. Is such a \u201ccollapsing-style\u201d graph-pooling the only choice for graph classification? From complex system\u2019s point of view, properties of a complex system arise largely from the interaction among its components. Therefore, we speculate that preserving the interacting relation between parts, instead of pooling them together, could benefit system level prediction. To verify this, we propose SLIM, a graph neural network model for Structural Landmarking and Interaction Modelling. The main idea is to compute a set of end-to-end optimizable sub-structure landmarks, so that any input graph can be projected onto these (spatially) local structural representatives for a faithful, global characterization. By doing so, explicit interaction between component parts of a graph can be leveraged directly in generating discriminative graph representation. Encouraging results are observed on benchmark datasets for graph classification, demonstrating the value of interaction modelling in the design of graph neural networks.",
    "code_link": ""
  },
  "aaai2022_main_invariantactioneffectmodelforreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Invariant Action Effect Model for Reinforcement Learning",
    "authors": [
      "Zheng-Mao Zhu",
      "Shengyi Jiang",
      "Yu-Ren Liu",
      "Yang Yu",
      "Kun Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20913",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20913/20672",
    "published": "2022-02",
    "summary": "Good representations can help RL agents perform concise modeling of their surroundings, and thus support effective decision-making in complex environments. Previous methods learn good representations by imposing extra constraints on dynamics.However, in the causal perspective, the causation between the action and its effect is not fully considered in those methods, which leads to the ignorance of the underlying relations among the action effects on the transitions. Based on the intuition that the same action always causes similar effects among different states, we induce such causation by taking the invariance of action effects among states as the relation.By explicitly utilizing such invariance, in this paper, we show that a better representation can be learned and potentially improves the sample efficiency and the generalization ability of the learned policy. We propose Invariant Action Effect Model (IAEM) to capture the invariance in action effects, where the effect of an action is represented as the residual of representations from neighboring states.IAEM is composed of two parts:(1) a new contrastive-based loss to capture the underlying invariance of action effects;(2) an individual action effect and provides a self-adapted weighting strategy to tackle the corner cases where the invariance does not hold.The extensive experiments on two benchmarks, i.e. Grid-World and Atari, show that the representations learned by IAEM preserve the invariance of action effects. Moreover, with the invariant action effect, IAEM can accelerate the learning process by 1.6x, rapidly generalize to new environments by fine-tuning on a few components, and outperform other dynamics-based representation methods by 1.4x in limited steps.",
    "code_link": ""
  },
  "aaai2022_main_self-adaptiveimitationlearninglearningtaskswithdelayedrewardsfromsub-optimaldemonstrations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Adaptive Imitation Learning: Learning Tasks with Delayed Rewards from Sub-optimal Demonstrations",
    "authors": [
      "Zhuangdi Zhu",
      "Kaixiang Lin",
      "Bo Dai",
      "Jiayu Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20914",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20914/20673",
    "published": "2022-02",
    "summary": "Reinforcement learning (RL) has demonstrated its superiority in solving sequential decision-making problems. However, heavy dependence on immediate reward feedback impedes the wide application of RL. On the other hand, imitation learning (IL) tackles RL without relying on environmental supervision by leveraging external demonstrations. In practice, however, collecting sufficient expert demonstrations can be prohibitively expensive, yet the quality of demonstrations typically limits the performance of the learning policy. To address a practical scenario, in this work, we propose Self-Adaptive Imitation Learning (SAIL), which, provided with a few demonstrations from a sub-optimal teacher, can perform well in RL tasks with extremely delayed rewards, where the only reward feedback is trajectory-wise ranking. SAIL bridges the advantages of IL and RL by interactively exploiting the demonstrations to catch up with the teacher and exploring the environment to yield demonstrations that surpass the teacher. Extensive empirical results show that not only does SAIL significantly improve the sample efficiency, but it also leads to higher asymptotic performance across different continuous control tasks, compared with the state-of-the-art.",
    "code_link": "https://github.com/openai/mujoco-py"
  },
  "aaai2022_main_localitymattersascalablevaluedecompositionapproachforcooperativemulti-agentreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Locality Matters: A Scalable Value Decomposition Approach for Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Roy Zohar",
      "Shie Mannor",
      "Guy Tennenholtz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20915",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/20915/20674",
    "published": "2022-02",
    "summary": "Cooperative multi-agent reinforcement learning (MARL) faces significant scalability issues due to state and action spaces that are exponentially large in the number of agents. As environments grow in size, effective credit assignment becomes increasingly harder and often results in infeasible learning times. Still, in many real-world settings, there exist simplified underlying dynamics that can be leveraged for more scalable solutions. In this work, we exploit such locality structures effectively whilst maintaining global cooperation. We propose a novel, value-based multi-agent algorithm called LOMAQ, which incorporates local rewards in the Centralized Training Decentralized Execution paradigm. Additionally, we provide a direct reward decomposition method for finding these local rewards when only a global signal is provided. We test our method empirically, showing it scales well compared to other methods, significantly improving performance and convergence speed.",
    "code_link": ""
  },
  "aaai2022_main_hedonicgameswithfixed-sizecoalitions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hedonic Games with Fixed-Size Coalitions",
    "authors": [
      "Vittorio Bil\u00f2",
      "Gianpiero Monaco",
      "Luca Moscardelli"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21156",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21156/20905",
    "published": "2022-02",
    "summary": "In hedonic games, a set of n agents, having preferences over all possible coalition structures, needs to agree on a stable outcome. In this work, we initiate the study of hedonic games with fixed-size coalitions, where the set of possible coalition structures is restricted as follows: there are k coalitions, each coalition has a fixed size, and the sum of the sizes of all coalitions equals n.We focus on the basic model of additively separable hedonic games with symmetric preferences, where an agent's preference is captured by a utility function which sums up a contribution due to any other agent in the same coalition. In this setting, an outcome is stable if no pair of agents can exchange coalitions and improve their utilities. Conditioned on the definition of improvement, three stability notions arise: swap stability under transferable utilities, which requires to improve the sum of the utilities of both agents, swap stability, which requires to improve the utility of one agent without decreasing the utility of the other one, and strict swap stability, requiring to improve the utilities of both agents simultaneously. We analyse the fundamental questions of existence, complexity and efficiency of stable outcomes, and that of complexity of a social optimum.",
    "code_link": ""
  },
  "aaai2022_main_partner-awarealgorithmsindecentralizedcooperativebanditteams": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Partner-Aware Algorithms in Decentralized Cooperative Bandit Teams",
    "authors": [
      "Erdem Biyik",
      "Anusha Lalitha",
      "Rajarshi Saha",
      "Andrea Goldsmith",
      "Dorsa Sadigh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21158",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21158/20908",
    "published": "2022-02",
    "summary": "When humans collaborate with each other, they often make decisions by observing others and considering the consequences that their actions may have on the entire team, instead of greedily doing what is best for just themselves. We would like our AI agents to effectively collaborate in a similar way by capturing a model of their partners. In this work, we propose and analyze a decentralized Multi-Armed Bandit (MAB) problem with coupled rewards as an abstraction of more general multi-agent collaboration. We demonstrate that naive extensions of single-agent optimal MAB algorithms fail when applied for decentralized bandit teams. Instead, we propose a Partner-Aware strategy for joint sequential decision-making that extends the well-known single-agent Upper Confidence Bound algorithm. We analytically show that our proposed strategy achieves logarithmic regret, and provide extensive experiments involving human-AI and human-robot collaboration to validate our theoretical findings. Our results show that the proposed partner-aware strategy outperforms other known methods, and our human subject studies suggest humans prefer to collaborate with AI agents implementing our partner-aware strategy.",
    "code_link": ""
  },
  "aaai2022_main_fixationmaximizationinthepositionalmoranprocess": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fixation Maximization in the Positional Moran Process",
    "authors": [
      "Joachim Brendborg",
      "Panagiotis Karras",
      "Andreas Pavlogiannis",
      "Asger\n      Ullersted Rasmussen",
      "Josef Tkadlec"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21160",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21160/20909",
    "published": "2022-02",
    "summary": "The Moran process is a classic stochastic process that models invasion dynamics on graphs. A single mutant (e.g., a new opinion, strain, social trait etc.) invades a population of residents spread over the nodes of a graph. The mutant fitness advantage \u03b4>=0 determines how aggressively mutants propagate to their neighbors. The quantity of interest is the fixation probability, i.e., the probability that the initial mutant eventually takes over the whole population. However, in realistic settings, the invading mutant has an advantage only in certain locations. E.g., the ability to metabolize a certain sugar is an advantageous trait to bacteria only when the sugar is actually present in their surroundings. In this paper we introduce the positional Moran process, a natural generalization in which the mutant fitness advantage is only realized on specific nodes called active nodes, and study the problem of fixation maximization: given a budget k, choose a set of k active nodes that maximize the fixation probability of the invading mutant. We show that the problem is NP-hard, while the optimization function is not submodular, thus indicating strong computational hardness. We focus on two natural limits. In the limit of \u03b4 to infinity (strong selection), although the problem remains NP-hard, the optimization function becomes submodular and thus admits a constant-factor approximation using a simple greedy algorithm. In the limit of \u03b4 to 0 (weak selection), we show that we can obtain a tight approximation in O(n^{2\u00d7\u03c9}) time, where \u03c9 is the matrix-multiplication exponent. An experimental evaluation of the new algorithms along with some proposed heuristics corroborates our results.",
    "code_link": ""
  },
  "aaai2022_main_flexdistributionforbounded-suboptimalmulti-agentpathfinding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Flex Distribution for Bounded-Suboptimal Multi-Agent Path Finding",
    "authors": [
      "Shao-Hung Chan",
      "Jiaoyang Li",
      "Graeme Gange",
      "Daniel Harabor",
      "Peter J.\n      Stuckey",
      "Sven Koenig"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21162",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21162/20911",
    "published": "2022-02",
    "summary": "Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths for multiple agents that minimize the sum of path costs. EECBS is a leading two-level algorithm that solves MAPF bounded-suboptimally, that is, within some factor w of the minimum sum of path costs C*. It uses focal search to find bounded-suboptimal paths on the low level andExplicit Estimation Search (EES) to resolve collisions on the high level. EES keeps track of a lower bound LB on C* to find paths whose sum of path costs is at most w LB in order to solve MAPF bounded-suboptimally. However, the costs of many paths are often much smaller than w times their minimum path costs, meaning that the sum of path costs is much smaller than w C*. In this paper, we therefore propose Flexible EECBS (FEECBS), which uses a flex(ible) distribution of the path costs (that relaxes the requirement to find bounded-suboptimal paths on the low level) in order to reduce the number of collisions that need to be resolved on the high level while still guaranteeing to solve MAPF bounded suboptimally. We address the drawbacks of flex distribution via techniques such as restrictions on the flex distribution, restarts of the high-level search with EECBS, and low-level focal-A* search. Our empirical evaluation shows that FEECBS substantially improves the efficiency of EECBS on MAPF instances with large maps and large numbers of agents.",
    "code_link": ""
  },
  "aaai2022_main_participatorybudgetingwithdonationsanddiversityconstraints": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Participatory Budgeting with Donations and Diversity Constraints",
    "authors": [
      "Jiehua Chen",
      "Martin Lackner",
      "Jan Maly"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21163",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21163/20912",
    "published": "2022-02",
    "summary": "Participatory budgeting (PB) is a democratic process where citizens jointly decide on how to allocate public funds to indivisible projects. In this work, we focus on PB processes where citizens may provide additional money to projects they want to see funded. We introduce a formal framework for this kind of PB with donations. Our framework also allows for diversity constraints, meaning that each project belongs to one or more types, and there are lower and upper bounds on the number of projects of the same type that can be funded. We propose three general classes of methods for aggregating the citizens\u2019 preferences in the presence of donations and analyze their axiomatic properties. Furthermore, we investigate the computational complexity of determining the outcome of a PB process with donations and of finding a citizen\u2019s optimal donation strategy.",
    "code_link": ""
  },
  "aaai2022_main_pretrainedcostmodelfordistributedconstraintoptimizationproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pretrained Cost Model for Distributed Constraint Optimization Problems",
    "authors": [
      "Yanchen Deng",
      "Shufeng Kong",
      "Bo An"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21164",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21164/20913",
    "published": "2022-02",
    "summary": "Distributed Constraint Optimization Problems (DCOPs) are an important subclass of combinatorial optimization problems, where information and controls are distributed among multiple autonomous agents. Previously, Machine Learning (ML) has been largely applied to solve combinatorial optimization problems by learning effective heuristics. However, existing ML-based heuristic methods are often not generalizable to different search algorithms. Most importantly, these methods usually require full knowledge about the problems to be solved, which are not suitable for distributed settings where centralization is not realistic due to geographical limitations or privacy concerns. To address the generality issue, we propose a novel directed acyclic graph representation schema for DCOPs and leverage the Graph Attention Networks (GATs) to embed graph representations. Our model, GAT-PCM, is then pretrained with optimally labelled data in an offline manner, so as to construct effective heuristics to boost a broad range of DCOP algorithms where evaluating the quality of a partial assignment is critical, such as local search or backtracking search. Furthermore, to enable decentralized model inference, we propose a distributed embedding schema of GAT-PCM where each agent exchanges only embedded vectors, and show its soundness and complexity. Finally, we demonstrate the effectiveness of our model by combining it with a local search or a backtracking search algorithm. Extensive empirical evaluations indicate that the GAT-PCM-boosted algorithms significantly outperform the state-of-the-art methods in various benchmarks.",
    "code_link": "https://github.com/dyc941126/GAT-PCM"
  },
  "aaai2022_main_concentrationnetworkforreinforcementlearningoflarge-scalemulti-agentsystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems",
    "authors": [
      "Qingxu Fu",
      "Tenghai Qiu",
      "Jianqiang Yi",
      "Zhiqiang Pu",
      "Shiguang Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21165",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21165/20914",
    "published": "2022-02",
    "summary": "When dealing with a series of imminent issues, humans can naturally concentrate on a subset of these concerning issues by prioritizing them according to their contributions to motivational indices, e.g., the probability of winning a game. This idea of concentration offers insights into reinforcement learning of sophisticated Large-scale Multi-Agent Systems (LMAS) participated by hundreds of agents. In such an LMAS, each agent receives a long series of entity observations at each step, which can overwhelm existing aggregation networks such as graph attention networks and cause inefficiency. In this paper, we propose a concentration network called ConcNet. First, ConcNet scores the observed entities considering several motivational indices, e.g., expected survival time and state value of the agents, and then ranks, prunes, and aggregates the encodings of observed entities to extract features. Second, distinct from the well-known attention mechanism, ConcNet has a unique motivational subnetwork to explicitly consider the motivational indices when scoring the observed entities. Furthermore, we present a concentration policy gradient architecture that can learn effective policies in LMAS from scratch. Extensive experiments demonstrate that the presented architecture has excellent scalability and flexibility, and significantly outperforms existing methods on LMAS benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_cooperativemulti-agentfairnessandequivariantpolicies": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cooperative Multi-Agent Fairness and Equivariant Policies",
    "authors": [
      "Niko A. Grupen",
      "Bart Selman",
      "Daniel D. Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21166",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21166/20915",
    "published": "2022-02",
    "summary": "We study fairness through the lens of cooperative multi-agent learning. Our work is motivated by empirical evidence that naive maximization of team reward yields unfair outcomes for individual team members. To address fairness in multi-agent contexts, we introduce team fairness, a group-based fairness measure for multi-agent learning. We then prove that it is possible to enforce team fairness during policy optimization by transforming the team's joint policy into an equivariant map. We refer to our multi-agent learning strategy as Fairness through Equivariance (Fair-E) and demonstrate its effectiveness empirically. We then introduce Fairness through Equivariance Regularization (Fair-ER) as a soft-constraint version of Fair-E and show that it reaches higher levels of utility than Fair-E and fairer outcomes than non-equivariant policies. Finally, we present novel findings regarding the fairness-utility trade-off in multi-agent settings; showing that the magnitude of the trade-off is dependent on agent skill.",
    "code_link": ""
  },
  "aaai2022_main_practicalfixed-parameteralgorithmsfordefendingactivedirectorystyleattackgraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs",
    "authors": [
      "Mingyu Guo",
      "Jialiang Li",
      "Aneta Neumann",
      "Frank Neumann",
      "Hung Nguyen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21167",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21167/20916",
    "published": "2022-02",
    "summary": "Active Directory is the default security management system for Windows domain networks. We study the shortest path edge interdiction problem for defending Active Directory style attack graphs. The problem is formulated as a Stackelberg game between one defender and one attacker. The attack graph contains one destination node and multiple entry nodes. The attacker's entry node is chosen by nature. The defender chooses to block a set of edges limited by his budget. The attacker then picks the shortest unblocked attack path. The defender aims to maximize the expected shortest path length for the attacker, where the expectation is taken over entry nodes.We observe that practical Active Directory attack graphs have small maximum attack path length and are structurally close to trees. We first show that even if the maximum attack path length is a constant, the problem is still w[1]-hard with respect to the defender's budget. Having a small maximum attack path length and a small budget is not enough to design fixed-parameter algorithms. If we further assume that the number of entry nodes is small, then we derive a fixed-parameter tractable algorithm.We then propose two other fixed-parameter algorithms by exploiting the tree-like features. One is based on tree decomposition and requires a small tree width. The other assumes a small number of splitting nodes (nodes with multiple out-going edges). Finally, the last algorithm is converted into a graph convolutional neural network based heuristic, which scales to larger graphs with more splitting nodes.",
    "code_link": "https://github.com/BloodHoundAD/BloodHound"
  },
  "aaai2022_main_anytimemulti-agentpathfindingviamachinelearning-guidedlargeneighborhoodsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Anytime Multi-Agent Path Finding via Machine Learning-Guided Large Neighborhood Search",
    "authors": [
      "Taoan Huang",
      "Jiaoyang Li",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21168",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21168/20917",
    "published": "2022-02",
    "summary": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths for a team of agents in a common environment. MAPF is NP-hard to solve optimally and, in some cases, also bounded-suboptimally. It is thus time-consuming for (bounded-sub)optimal solvers to solve large MAPF instances. Anytime algorithms find solutions quickly for large instances and then improve them to close-to-optimal ones over time. In this paper, we improve the current state-of-the-art anytime solver MAPF-LNS, that first finds an initial solution fast and then repeatedly replans the paths of subsets of agents via Large Neighborhood Search (LNS). It generates the subsets of agents for replanning by randomized destroy heuristics, but not all of them increase the solution quality substantially. We propose to use machine learning to learn how to select a subset of agents from a collection of subsets, such that replanning increases the solution quality more. We show experimentally that our solver, MAPF-ML-LNS, significantly outperforms MAPF-LNS on the standard MAPF benchmark set in terms of both the speed of improving the solution and the final solution quality.",
    "code_link": ""
  },
  "aaai2022_main_mdpgtmomentum-baseddecentralizedpolicygradienttracking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MDPGT: Momentum-Based Decentralized Policy Gradient Tracking",
    "authors": [
      "Zhanhong Jiang",
      "Xian Yeow Lee",
      "Sin Yong Tan",
      "Kai Liang Tan",
      "Aditya Balu",
      "Young M Lee",
      "Chinmay Hegde",
      "Soumik Sarkar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21169",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21169/20918",
    "published": "2022-02",
    "summary": "We propose a novel policy gradient method for multi-agent reinforcement learning, which leverages two different variance-reduction techniques and does not require large batches over iterations. Specifically, we propose a momentum-based decentralized policy gradient tracking (MDPGT) where a new momentum-based variance reduction technique is used to approximate the local policy gradient surrogate with importance sampling, and an intermediate parameter is adopted to track two consecutive policy gradient surrogates. MDPGT provably achieves the best available sample complexity of O(N -1 e -3) for converging to an e-stationary point of the global average of N local performance functions (possibly nonconcave). This outperforms the state-of-the-art sample complexity in decentralized model-free reinforcement learning and when initialized with a single trajectory, the sample complexity matches those obtained by the existing decentralized policy gradient methods. We further validate the theoretical claim for the Gaussian policy function. When the required error tolerance e is small enough, MDPGT leads to a linear speed up, which has been previously established in decentralized stochastic optimization, but not for reinforcement learning. Lastly, we provide empirical results on a multi-agent reinforcement learning benchmark environment to support our theoretical findings.",
    "code_link": "https://github.com/xylee95/MD-PGT"
  },
  "aaai2022_main_shardsystemsscalable,robustandpersistentmulti-agentpathfindingwithperformanceguarantees": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shard Systems: Scalable, Robust and Persistent Multi-Agent Path Finding with Performance Guarantees",
    "authors": [
      "Christopher Leet",
      "Jiaoyang Li",
      "Sven Koenig"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21170",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21170/20919",
    "published": "2022-02",
    "summary": "Modern multi-agent robotic systems increasingly require scalable, robust and persistent Multi-Agent Path Finding (MAPF) with performance guarantees. While many MAPF solvers that provide some of these properties exist, none provides them all. To fill this need, we propose a new MAPF framework, the shard system. A shard system partitions the workspace into geographic regions, called shards, linked by a novel system of buffers. Agents are routed optimally within a shard by a local controller to local goals set by a global controller. The buffer system novelly allows shards to plan with perfect parallelism, providing scalability. A novel global controller algorithm can rapidly generate an inter-shard routing plan for thousands of agents while minimizing the traffic routed through any shard. A novel workspace partitioning algorithm produces shards small enough to replan rapidly. These innovations allow a shard system to adjust its routing plan in real time if an agent is delayed or assigned a new goal, enabling robust, persistent MAPF. A shard system's local optimality and optimized inter-shard routing bring the sum-of-costs of its solutions to single-shot MAPF problems to < 20-60% of optimal on a diversity of workspaces. Its scalability allows it to plan paths for 1000s of agents in seconds. If any of their goals change or move actions fails, a shard system can replan in under a second.",
    "code_link": ""
  },
  "aaai2022_main_adeeperunderstandingofstate-basedcriticsinmulti-agentreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning",
    "authors": [
      "Xueguang Lyu",
      "Andrea Baisero",
      "Yuchen Xiao",
      "Christopher Amato"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21171",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21171/20920",
    "published": "2022-02",
    "summary": "Centralized Training for Decentralized Execution, where training is done in a centralized offline fashion, has become a popular solution paradigm in Multi-Agent Reinforcement Learning. Many such methods take the form of actor-critic with state-based critics, since centralized training allows access to the true system state, which can be useful during training despite not being available at execution time. State-based critics have become a common empirical choice, albeit one which has had limited theoretical justification or analysis. In this paper, we show that state-based critics can introduce bias in the policy gradient estimates, potentially undermining the asymptotic guarantees of the algorithm. We also show that, even if the state-based critics do not introduce any bias, they can still result in a larger gradient variance, contrary to the common intuition. Finally, we show the effects of the theories in practice by comparing different forms of centralized critics on a wide range of common benchmarks, and detail how various environmental properties are related to the effectiveness of different types of critics.",
    "code_link": "https://github.com/Bigpig4396/MultiAgent-Reinforcement-Learning-Environment"
  },
  "aaai2022_main_whencanthedefendereffectivelydeceiveattackersinsecuritygames?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "When Can the Defender Effectively Deceive Attackers in Security Games?",
    "authors": [
      "Thanh Nguyen",
      "Haifeng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21172",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21172/20921",
    "published": "2022-02",
    "summary": "This paper studies defender patrol deception in general Stackelberg security games (SSGs), where a defender attempts to alter the attacker's perception of the defender's patrolling intensity so as to influence the attacker's decision making. We are interested in understanding the complexity and effectiveness of optimal defender deception under different attacker behavior models. Specifically, we consider three different attacker strategies of response (to the defender's deception) with increasing sophistication, and design efficient polynomial-time algorithms to compute the equilibrium for each. Moreover, we prove formal separation for the effectiveness of patrol deception when facing an attacker of increasing sophistication, until it becomes even harmful to the defender when facing the most intelligent attacker we consider. Our results shed light on when and how deception should be used in SSGs. We conduct extensive experiments to illustrate our theoretical results in various game settings.",
    "code_link": ""
  },
  "aaai2022_main_generalizationinmeanfieldgamesbylearningmasterpolicies": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generalization in Mean Field Games by Learning Master Policies",
    "authors": [
      "Sarah Perrin",
      "Mathieu Lauri\u00e8re",
      "Julien P\u00e9rolat",
      "Romuald \u00c9lie",
      "Matthieu\n      Geist",
      "Olivier Pietquin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21173",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21173/20922",
    "published": "2022-02",
    "summary": "Mean Field Games (MFGs) can potentially scale multi-agent systems to extremely large populations of agents. Yet, most of the literature assumes a single initial distribution for the agents, which limits the practical applications of MFGs. Machine Learning has the potential to solve a wider diversity of MFG problems thanks to generalizations capacities. We study how to leverage these generalization properties to learn policies enabling a typical agent to behave optimally against any population distribution. In reference to the Master equation in MFGs, we coin the term \u201cMaster policies\u201d to describe them and we prove that a single Master policy provides a Nash equilibrium, whatever the initial distribution. We propose a method to learn such Master policies. Our approach relies on three ingredients: adding the current population distribution as part of the observation, approximating Master policies with neural networks, and training via Reinforcement Learning and Fictitious Play. We illustrate on numerical examples not only the efficiency of the learned Master policy but also its generalization capabilities beyond the distributions used for training.",
    "code_link": ""
  },
  "aaai2022_main_findingnontrivialminimumfixedpointsindiscretedynamicalsystemscomplexity,specialcasealgorithmsandheuristics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems: Complexity, Special Case Algorithms and Heuristics",
    "authors": [
      "Zirou Qiu",
      "Chen Chen",
      "Madhav Marathe",
      "S.S. Ravi",
      "Daniel J. Rosenkrantz",
      "Richard Stearns",
      "Anil Vullikanti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21174",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21174/20923",
    "published": "2022-02",
    "summary": "Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial-time algorithm for approximating a solution to this problem to within the factor n^(1 - epsilon) for any constant epsilon > 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to address the problem for networks of reasonable sizes. For solving the problem on larger networks, we propose a general heuristic framework along with greedy selection methods. Extensive experimental results on real-world networks demonstrate the effectiveness of the proposed heuristics. A full version of the manuscript, source code and data are available at: https://github.com/bridgelessqiu/NMIN-FPE",
    "code_link": "https://github.com/bridgelessqiu/NMIN-FPE"
  },
  "aaai2022_main_howmanyrepresentativesdoweneed?theoptimalsizeofacongressvotingonbinaryissues": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "How Many Representatives Do We Need? The Optimal Size of a Congress Voting on Binary Issues",
    "authors": [
      "Manon Revel",
      "Tao Lin",
      "Daniel Halpern"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21175",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21175/20924",
    "published": "2022-02",
    "summary": "Aggregating opinions of a collection of agents is a question of interest to a broad array of researchers, ranging from ensemble-learning theorists to political scientists designing democratic institutions. This work investigates the optimal number of agents needed to decide on a binary issue under majority rule. We take an epistemic view where the issue at hand has a ground truth ``correct'' outcome and each one of n voters votes correctly with a fixed probability, known as their competence level or competence. These competencies come from a fixed distribution D. Observing the competencies, we must choose a specific group that will represent the population. Finally, voters sample a decision (either correct or not), and the group is correct as long as more than half the chosen representatives voted correctly. Assuming that we can identify the best experts, i.e., those with the highest competence, to form an epistemic congress we find that the optimal congress size should be linear in the population size. This result is striking because it holds even when allowing the top representatives to become arbitrarily accurate, choosing the correct outcome with probabilities approaching 1. We then analyze real-world data, observing that the actual sizes of representative bodies are much smaller than the optimal ones our theoretical results suggest. We conclude by examining under what conditions congresses of sub-optimal sizes would still outperform direct democracy, in which all voters vote. We find that a small congress would beat direct democracy if the rate at which the societal bias towards the ground truth decreases with the population size fast enough, and we quantify the speed needed for constant and polynomial congress sizes.",
    "code_link": ""
  },
  "aaai2022_main_decentralizedmeanfieldgames": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Decentralized Mean Field Games",
    "authors": [
      "Sriram Ganapathi Subramanian",
      "Matthew E. Taylor",
      "Mark Crowley",
      "Pascal\n      Poupart"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21176",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21176/20925",
    "published": "2022-02",
    "summary": "Multiagent reinforcement learning algorithms have not been widely adopted in large scale environments with many agents as they often scale poorly with the number of agents. Using mean field theory to aggregate agents has been proposed as a solution to this problem. However, almost all previous methods in this area make a strong assumption of a centralized system where all the agents in the environment learn the same policy and are effectively indistinguishable from each other. In this paper, we relax this assumption about indistinguishable agents and propose a new mean field system known as Decentralized Mean Field Games, where each agent can be quite different from others. All agents learn independent policies in a decentralized fashion, based on their local observations. We define a theoretical solution concept for this system and provide a fixed point guarantee for a Q-learning based algorithm in this system. A practical consequence of our approach is that we can address a `chicken-and-egg' problem in empirical mean field reinforcement learning algorithms. Further, we provide Q-learning and actor-critic algorithms that use the decentralized mean field learning approach and give stronger performances compared to common baselines in this area. In our setting, agents do not need to be clones of each other and learn in a fully decentralized fashion. Hence, for the first time, we show the application of mean field learning methods in fully competitive environments, large-scale continuous action space environments, and other environments with heterogeneous agents. Importantly, we also apply the mean field method in a ride-sharing problem using a real-world dataset. We propose a decentralized solution to this problem, which is more practical than existing centralized training methods.",
    "code_link": "https://github.com/Sriram94/DMFG"
  },
  "aaai2022_main_incentivizingcollaborationinmachinelearningviasyntheticdatarewards": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Incentivizing Collaboration in Machine Learning via Synthetic Data Rewards",
    "authors": [
      "Sebastian Shenghong Tay",
      "Xinyi Xu",
      "Chuan Sheng Foo",
      "Bryan Kian Hsiang Low"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21177",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21177/20926",
    "published": "2022-02",
    "summary": "This paper presents a novel collaborative generative modeling (CGM) framework that incentivizes collaboration among self-interested parties to contribute data to a pool for training a generative model (e.g., GAN), from which synthetic data are drawn and distributed to the parties as rewards commensurate to their contributions. Distributing synthetic data as rewards (instead of trained models or money) offers task- and model-agnostic benefits for downstream learning tasks and is less likely to violate data privacy regulation. To realize the framework, we firstly propose a data valuation function using maximum mean discrepancy (MMD) that values data based on its quantity and quality in terms of its closeness to the true data distribution and provide theoretical results guiding the kernel choice in our MMD-based data valuation function. Then, we formulate the reward scheme as a linear optimization problem that when solved, guarantees certain incentives such as fairness in the CGM framework. We devise a weighted sampling algorithm for generating synthetic data to be distributed to each party as reward such that the value of its data and the synthetic data combined matches its assigned reward value by the reward scheme. We empirically show using simulated and real-world datasets that the parties' synthetic data rewards are commensurate to their contributions.",
    "code_link": ""
  },
  "aaai2022_main_learningtheoptimalrecommendationfromexplorativeusers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning the Optimal Recommendation from Explorative Users",
    "authors": [
      "Fan Yao",
      "Chuanhao Li",
      "Denis Nekipelov",
      "Hongning Wang",
      "Haifeng Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21178",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21178/20927",
    "published": "2022-02",
    "summary": "We propose a new problem setting to study the sequential interactions between a recommender system and a user. Instead of assuming the user is omniscient, static, and explicit, as the classical practice does, we sketch a more realistic user behavior model, under which the user: 1) rejects recommendations if they are clearly worse than others; 2) updates her utility estimation based on rewards from her accepted recommendations; 3) withholds realized rewards from the system. We formulate the interactions between the system and such an explorative user in a K-armed bandit framework and study the problem of learning the optimal recommendation on the system side. We show that efficient system learning is still possible but is more difficult. In particular, the system can identify the best arm with probability at least 1-delta within O(1/delta) interactions, and we prove this is tight. Our finding contrasts the result for the problem of best arm identification with fixed confidence, in which the best arm can be identified with probability 1-delta within O(log(1/delta)) interactions. This gap illustrates the inevitable cost the system has to pay when it learns from an explorative user's revealed preferences on its recommendations rather than from the realized rewards.",
    "code_link": ""
  },
  "aaai2022_main_multi-agentincentivecommunicationviadecentralizedteammatemodeling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multi-Agent Incentive Communication via Decentralized Teammate Modeling",
    "authors": [
      "Lei Yuan",
      "Jianhao Wang",
      "Fuxiang Zhang",
      "Chenghe Wang",
      "ZongZhang Zhang",
      "Yang\n      Yu",
      "Chongjie Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21179",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21179/20928",
    "published": "2022-02",
    "summary": "Effective communication can improve coordination in cooperative multi-agent reinforcement learning (MARL). One popular communication scheme is exchanging agents' local observations or latent embeddings and using them to augment individual local policy input. Such a communication paradigm can reduce uncertainty for local decision-making and induce implicit coordination. However, it enlarges agents' local policy spaces and increases learning complexity, leading to poor coordination in complex settings. To handle this limitation, this paper proposes a novel framework named Multi-Agent Incentive Communication (MAIC) that allows each agent to learn to generate incentive messages and bias other agents' value functions directly, resulting in effective explicit coordination. Our method firstly learns targeted teammate models, with which each agent can anticipate the teammate's action selection and generate tailored messages to specific agents. We further introduce a novel regularization to leverage interaction sparsity and improve communication efficiency. MAIC is agnostic to specific MARL algorithms and can be flexibly integrated with different value function factorization methods. Empirical results demonstrate that our method significantly outperforms baselines and achieves excellent performance on multiple cooperative MARL tasks.",
    "code_link": ""
  },
  "aaai2022_main_mlinklinkingblack-boxmodelsforcollaborativemulti-modelinference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MLink: Linking Black-Box Models for Collaborative Multi-Model Inference",
    "authors": [
      "Mu Yuan",
      "Lan Zhang",
      "Xiang-Yang Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21180",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21180/20929",
    "published": "2022-02",
    "summary": "The cost efficiency of model inference is critical to real-world machine learning (ML) applications, especially for delay-sensitive tasks and resource-limited devices. A typical dilemma is: in order to provide complex intelligent services (e.g. smart city), we need inference results of multiple ML models, but the cost budget (e.g. GPU memory) is not enough to run all of them. In this work, we study underlying relationships among black-box ML models and propose a novel learning task: model linking. Model linking aims to bridge the knowledge of different black-box models by learning mappings (dubbed model links) between their output spaces. Based on model links, we developed a scheduling algorithm, named MLink. Through collaborative multi-model inference enabled by model links, MLink can improve the accuracy of obtained inference results under the cost budget. We evaluated MLink on a multi-modal dataset with seven different ML models and two real-world video analytics systems with six ML models and 3,264 hours of video. Experimental results show that our proposed model links can be effectively built among various black-box models. Under the budget of GPU memory, MLink can save 66.7% inference computations while preserving 94% inference accuracy, which outperforms multi-task learning, deep reinforcement learning-based scheduler and frame filtering baselines.",
    "code_link": "https://github.com/yuanmu97/MLink"
  },
  "aaai2022_main_equilibriumfindinginnormal-formgamesviagreedyregretminimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Equilibrium Finding in Normal-Form Games via Greedy Regret Minimization",
    "authors": [
      "Hugh Zhang",
      "Adam Lerer",
      "Noam Brown"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21181",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21181/20930",
    "published": "2022-02",
    "summary": "We extend the classic regret minimization framework for approximating equilibria in normal-form games by greedily weighing iterates based on regrets observed at runtime. Theoretically, our method retains all previous convergence rate guarantees. Empirically, experiments on large randomly generated games and normal-form subgames of the AI benchmark Diplomacy show that greedy weights outperforms previous methods whenever sampling is used, sometimes by several orders of magnitude.",
    "code_link": "https://github.com/hughbzhang/greedy-weights"
  },
  "aaai2022_main_whyfairlabelscanyieldunfairpredictionsgraphicalconditionsforintroducedunfairness": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Why Fair Labels Can Yield Unfair Predictions: Graphical Conditions for Introduced Unfairness",
    "authors": [
      "Carolyn Ashurst",
      "Ryan Carey",
      "Silvia Chiappa",
      "Tom Everitt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21182",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21182/20931",
    "published": "2022-02",
    "summary": "In addition to reproducing discriminatory relationships in the training data, machine learning (ML) systems can also introduce or amplify discriminatory effects. We refer to this as introduced unfairness, and investigate the conditions under which it may arise. To this end, we propose introduced total variation as a measure of introduced unfairness, and establish graphical conditions under which it may be incentivised to occur. These criteria imply that adding the sensitive attribute as a feature removes the incentive for introduced variation under well-behaved loss functions. Additionally, taking a causal perspective, introduced path-specific effects shed light on the issue of when specific paths should be considered fair.",
    "code_link": ""
  },
  "aaai2022_main_incorporatingitemfrequencyfordifferentiallyprivatesetunion": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Incorporating Item Frequency for Differentially Private Set Union",
    "authors": [
      "Ricardo Silva Carvalho",
      "Ke Wang",
      "Lovedeep Singh Gondara"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21183",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21183/20932",
    "published": "2022-02",
    "summary": "We study the problem of releasing the set union of users' items subject to differential privacy. Previous approaches consider only the set of items for each user as the input. We propose incorporating the item frequency, which is typically available in set union problems, to boost the utility of private mechanisms. However, using the global item frequency over all users would largely increase privacy loss. We propose to use the local item frequency of each user to approximate the global item frequency without incurring additional privacy loss. Local item frequency allows us to design greedy set union mechanisms that are differentially private, which is impossible for previous greedy proposals. Moreover, while all previous works have to use uniform sampling to limit the number of items each user would contribute to, our construction eliminates the sampling step completely and allows our mechanisms to consider all of the users' items. Finally, we propose to transfer the knowledge of the global item frequency from a public dataset into our mechanism, which further boosts utility even when the public and private datasets are from different domains. We evaluate the proposed methods on multiple real-life datasets.",
    "code_link": "https://github.com/ricardocarvalhods/diff-private-set-union"
  },
  "aaai2022_main_cosinemodelwatermarkingagainstensembledistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cosine Model Watermarking against Ensemble Distillation",
    "authors": [
      "Laurent Charette",
      "Lingyang Chu",
      "Yizhou Chen",
      "Jian Pei",
      "Lanjun Wang",
      "Yong\n      Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21184",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21184/20933",
    "published": "2022-02",
    "summary": "Many model watermarking methods have been developed to prevent valuable deployed commercial models from being stealthily stolen by model distillations.However, watermarks produced by most existing model watermarking methods can be easily evaded by ensemble distillation, because averaging the outputs of multiple ensembled models can significantly reduce or even erase the watermarks.In this paper, we focus on tackling the challenging task of defending against ensemble distillation.We propose a novel watermarking technique named CosWM to achieve outstanding model watermarking performance against ensemble distillation.CosWM is not only elegant in design, but also comes with desirable theoretical guarantees.Our extensive experiments on public data sets demonstrate the excellent performance of CosWM and its advantages over the state-of-the-art baselines.",
    "code_link": ""
  },
  "aaai2022_main_towardsdebiasingdnnmodelsfromspuriousfeatureinfluence": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Debiasing DNN Models from Spurious Feature Influence",
    "authors": [
      "Mengnan Du",
      "Ruixiang Tang",
      "Weijie Fu",
      "Xia Hu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21185",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21185/20934",
    "published": "2022-02",
    "summary": "Recent studies indicate that deep neural networks (DNNs) are prone to show discrimination towards certain demographic groups. We observe that algorithmic discrimination can be explained by the high reliance of the models on fairness sensitive features. Motivated by this observation, we propose to achieve fairness by suppressing the DNN models from capturing the spurious correlation between those fairness sensitive features with the underlying task. Specifically, we firstly train a bias-only teacher model which is explicitly encouraged to maximally employ fairness sensitive features for prediction. The teacher model then counter-teaches a debiased student model so that the interpretation of the student model is orthogonal to the interpretation of the teacher model. The key idea is that since the teacher model relies explicitly on fairness sensitive features for prediction, the orthogonal interpretation loss enforces the student network to reduce its reliance on sensitive features and instead capture more task relevant features for prediction. Experimental analysis indicates that our framework substantially reduces the model's attention on fairness sensitive features. Experimental results on four datasets further validate that our framework has consistently improved the fairness with respect to three group fairness metrics, with a comparable or even better accuracy.",
    "code_link": ""
  },
  "aaai2022_main_path-specificobjectivesforsaferagentincentives": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Path-Specific Objectives for Safer Agent Incentives",
    "authors": [
      "Sebastian Farquhar",
      "Ryan Carey",
      "Tom Everitt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21186",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21186/20935",
    "published": "2022-02",
    "summary": "We present a general framework for training safe agents whose naive incentives are unsafe. As an example, manipulative or deceptive behaviour can improve rewards but should be avoided. Most approaches fail here: agents maximize expected return by any means necessary. We formally describe settings with `delicate' parts of the state which should not be used as a means to an end. We then train agents to maximize the causal effect of actions on the expected return which is not mediated by the delicate parts of state, using Causal Influence Diagram analysis. The resulting agents have no incentive to control the delicate state. We further show how our framework unifies and generalizes existing proposals.",
    "code_link": ""
  },
  "aaai2022_main_algorithmicfairnessverificationwithgraphicalmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Algorithmic Fairness Verification with Graphical Models",
    "authors": [
      "Bishwamittra Ghosh",
      "Debabrota Basu",
      "Kuldeep S Meel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21187",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21187/20936",
    "published": "2022-02",
    "summary": "In recent years, machine learning (ML) algorithms have been deployed in safety-critical and high-stake decision-making, where the fairness of algorithms is of paramount importance. Fairness in ML centers on detecting bias towards certain demographic populations induced by an ML classifier and proposes algorithmic solutions to mitigate the bias with respect to different fairness definitions. To this end, several fairness verifiers have been proposed that compute the bias in the prediction of an ML classifier\u2014essentially beyond a finite dataset\u2014given the probability distribution of input features. In the context of verifying linear classifiers, existing fairness verifiers are limited by accuracy due to imprecise modeling of correlations among features and scalability due to restrictive formulations of the classifiers as SSAT/SMT formulas or by sampling. In this paper, we propose an efficient fairness verifier, called FVGM, that encodes the correlations among features as a Bayesian network. In contrast to existing verifiers, FVGM proposes a stochastic subset-sum based approach for verifying linear classifiers. Experimentally, we show that FVGM leads to an accurate and scalable assessment for more diverse families of fairness-enhancing algorithms, fairness attacks, and group/causal fairness metrics than the state-of-the-art fairness verifiers. We also demonstrate that FVGM facilitates the computation of fairness influence functions as a stepping stone to detect the source of bias induced by subsets of features.",
    "code_link": "https://github.com/meelgroup/justicia"
  },
  "aaai2022_main_achievinglong-termfairnessinsequentialdecisionmaking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Achieving Long-Term Fairness in Sequential Decision Making",
    "authors": [
      "Yaowei Hu",
      "Lu Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21188",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21188/20937",
    "published": "2022-02",
    "summary": "In this paper, we propose a framework for achieving long-term fair sequential decision making. By conducting both the hard and soft interventions, we propose to take path-specific effects on the time-lagged causal graph as a quantitative tool for measuring long-term fairness. The problem of fair sequential decision making is then formulated as a constrained optimization problem with the utility as the objective and the long-term and short-term fairness as constraints. We show that such an optimization problem can be converted to a performative risk optimization. Finally, repeated risk minimization (RRM) is used for model training, and the convergence of RRM is theoretically analyzed. The empirical evaluation shows the effectiveness of the proposed algorithm on synthetic and semi-synthetic temporal datasets.",
    "code_link": "https://github.com/yaoweihu/Achieving-Long-term-Fairness"
  },
  "aaai2022_main_fairnesswithoutimputationadecisiontreeapproachforfairpredictionwithmissingvalues": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fairness without Imputation: A Decision Tree Approach for Fair Prediction with Missing Values",
    "authors": [
      "Haewon Jeong",
      "Hao Wang",
      "Flavio P. Calmon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21189",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21189/20938",
    "published": "2022-02",
    "summary": "We investigate the fairness concerns of training a machine learning model using data with missing values. Even though there are a number of fairness intervention methods in the literature, most of them require a complete training set as input. In practice, data can have missing values, and data missing patterns can depend on group attributes (e.g. gender or race). Simply applying off-the-shelf fair learning algorithms to an imputed dataset may lead to an unfair model. In this paper, we first theoretically analyze different sources of discrimination risks when training with an imputed dataset. Then, we propose an integrated approach based on decision trees that does not require a separate process of imputation and learning. Instead, we train a tree with missing incorporated as attribute (MIA), which does not require explicit imputation, and we optimize a fairness-regularized objective function. We demonstrate that our approach outperforms existing fairness intervention methods applied to an imputed dataset, through several experiments on real-world datasets.",
    "code_link": ""
  },
  "aaai2022_main_shapingnoiseforrobustattributionsinneuralstochasticdifferentialequations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Shaping Noise for Robust Attributions in Neural Stochastic Differential Equations",
    "authors": [
      "Sumit Kumar Jha",
      "Rickard Ewetz",
      "Alvaro Velasquez",
      "Arvind Ramanathan",
      "Susmit Jha"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21190",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21190/20939",
    "published": "2022-02",
    "summary": "Neural SDEs with Brownian motion as noise lead to smoother attributions than traditional ResNets. Various attribution methods such as saliency maps, integrated gradients, DeepSHAP and DeepLIFT have been shown to be more robust for neural SDEs than for ResNets using the recently proposed sensitivity metric. In this paper, we show that neural SDEs with adaptive attribution-driven noise lead to even more robust attributions and smaller sensitivity metrics than traditional neural SDEs with Brownian motion as noise. In particular, attribution-driven shaping of noise leads to 6.7%, 6.9% and 19.4% smaller sensitivity metric for integrated gradients computed on three discrete approximations of neural SDEs with standard Brownian motion noise: stochastic ResNet-50, WideResNet-101 and ResNeXt-101 models respectively. The neural SDE model with adaptive attribution-driven noise leads to 25.7% and 4.8% improvement in the SIC metric over traditional ResNets and Neural SDEs with Brownian motion as noise. To the best of our knowledge, we are the first to propose the use of attributions for shaping the noise injected in neural SDEs, and demonstrate that this process leads to more robust attributions than traditional neural SDEs with standard Brownian motion as noise.",
    "code_link": ""
  },
  "aaai2022_main_certifiedrobustnessofnearestneighborsagainstdatapoisoningandbackdoorattacks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks",
    "authors": [
      "Jinyuan Jia",
      "Yupei Liu",
      "Xiaoyu Cao",
      "Neil Zhenqiang Gong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21191",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21191/20940",
    "published": "2022-02",
    "summary": "Data poisoning attacks and backdoor attacks aim to corrupt a machine learning classifier via modifying, adding, and/or removing some carefully selected training examples, such that the corrupted classifier makes incorrect predictions as the attacker desires. The key idea of state-of-the-art certified defenses against data poisoning attacks and backdoor attacks is to create a majority vote mechanism to predict the label of a testing example. Moreover, each voter is a base classifier trained on a subset of the training dataset. Classical simple learning algorithms such as k nearest neighbors (kNN) and radius nearest neighbors (rNN) have intrinsic majority vote mechanisms. In this work, we show that the intrinsic majority vote mechanisms in kNN and rNN already provide certified robustness guarantees against data poisoning attacks and backdoor attacks. Moreover, our evaluation results on MNIST and CIFAR10 show that the intrinsic certified robustness guarantees of kNN and rNN outperform those provided by state-of-the-art certified defenses. Our results serve as standard baselines for future certified defenses against data poisoning attacks and backdoor attacks.",
    "code_link": "https://github.com/openai/CLIP"
  },
  "aaai2022_main_onthefairnessofcausalalgorithmicrecourse": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Fairness of Causal Algorithmic Recourse",
    "authors": [
      "Julius von K\u00fcgelgen",
      "Amir-Hossein Karimi",
      "Umang Bhatt",
      "Isabel Valera",
      "Adrian Weller",
      "Bernhard Sch\u00f6lkopf"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21192",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21192/20941",
    "published": "2022-02",
    "summary": "Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which\u2014unlike prior work on equalising the average group-wise distance from the decision boundary\u2014explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier.",
    "code_link": "https://github.com/amirhk/recourse"
  },
  "aaai2022_main_deepauthadnnauthenticationframeworkbymodel-uniqueandfragilesignatureembedding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepAuth: A DNN Authentication Framework by Model-Unique and Fragile Signature Embedding",
    "authors": [
      "Yingjie Lao",
      "Weijie Zhao",
      "Peng Yang",
      "Ping Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21193",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21193/20942",
    "published": "2022-02",
    "summary": "Along with the evolution of deep neural networks (DNNs) in many real-world applications, the complexity of model building has also dramatically increased. Therefore, it is vital to protect the intellectual property (IP) of the model builder and ensure the trustworthiness of the deployed models. Meanwhile, adversarial attacks on DNNs (e.g., backdoor and poisoning attacks) that seek to inject malicious behaviors have been investigated recently, demanding a means for verifying the integrity of the deployed model to protect the users. This paper presents a novel DNN authentication framework DeepAuth that embeds a unique and fragile signature to each protected DNN model. Our approach exploits sensitive key samples that are well crafted from the input space to latent space and then to logit space for producing signatures. After embedding, each model will respond distinctively to these key samples, which creates a model-unique signature as a strong tool for authentication and user identity. The signature embedding process is also designed to ensure the fragility of the signature, which can be used to detect malicious modifications such that an illegitimate user or an altered model should not have the intact signature. Extensive evaluations on various models over a wide range of datasets demonstrate the effectiveness and efficiency of the proposed DeepAuth.",
    "code_link": ""
  },
  "aaai2022_main_fastsparsedecisiontreeoptimizationviareferenceensembles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast Sparse Decision Tree Optimization via Reference Ensembles",
    "authors": [
      "Hayden McTavish",
      "Chudi Zhong",
      "Reto Achermann",
      "Ilias Karimalis",
      "Jacques\n      Chen",
      "Cynthia Rudin",
      "Margo Seltzer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21194",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21194/20943",
    "published": "2022-02",
    "summary": "Sparse decision tree optimization has been one of the most fundamental problems in AI since its inception and is a challenge at the core of interpretable machine learning. Sparse decision tree optimization is computationally hard, and despite steady effort since the 1960's, breakthroughs have been made on the problem only within the past few years, primarily on the problem of finding optimal sparse decision trees. However, current state-of-the-art algorithms often require impractical amounts of computation time and memory to find optimal or near-optimal trees for some real-world datasets, particularly those having several continuous-valued features. Given that the search spaces of these decision tree optimization problems are massive, can we practically hope to find a sparse decision tree that competes in accuracy with a black box machine learning model? We address this problem via smart guessing strategies that can be applied to any optimal branch-and-bound-based decision tree algorithm. The guesses come from knowledge gleaned from black box models. We show that by using these guesses, we can reduce the run time by multiple orders of magnitude while providing bounds on how far the resulting trees can deviate from the black box's accuracy and expressive power. Our approach enables guesses about how to bin continuous features, the size of the tree, and lower bounds on the error for the optimal decision tree. Our experiments show that in many cases we can rapidly construct sparse decision trees that match the accuracy of black box models. To summarize: when you are having trouble optimizing, just guess.",
    "code_link": ""
  },
  "aaai2022_main_unsupervisedcausalbinaryconceptsdiscoverywithvaeforblack-boxmodelexplanation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-Box Model Explanation",
    "authors": [
      "Thien Q Tran",
      "Kazuto Fukuchi",
      "Youhei Akimoto",
      "Jun Sakuma"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21195",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21195/20944",
    "published": "2022-02",
    "summary": "We aim to explain a black-box classifier with the form: \"data X is classified as class Y because X has A, B and does not have C\" in which A, B, and C are high-level concepts. The challenge is that we have to discover in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful for explaining the classifier. We first introduce a structural generative model that is suitable to express and discover such concepts. We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output. Our method also allows easy integration of user's prior knowledge to induce high interpretability of concepts. Finally, using multiple datasets, we demonstrate that the proposed method can discover useful concepts for explanation in this form.",
    "code_link": ""
  },
  "aaai2022_main_dofeatureattributionmethodscorrectlyattributefeatures?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Do Feature Attribution Methods Correctly Attribute Features?",
    "authors": [
      "Yilun Zhou",
      "Serena Booth",
      "Marco Tulio Ribeiro",
      "Julie Shah"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21196",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21196/20945",
    "published": "2022-02",
    "summary": "Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of \"attribution\", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code and appendix are available at https://yilunzhou.github.io/feature-attribution-evaluation/.",
    "code_link": ""
  },
  "aaai2022_main_formalsemanticsandformallyverifiedvalidationfortemporalplanning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Formal Semantics and Formally Verified Validation for Temporal Planning",
    "authors": [
      "Mohammad Abdulaziz",
      "Lukas Koller"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21197",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21197/20946",
    "published": "2022-02",
    "summary": "We present a simple and concise semantics for temporal planning. Our semantics are developed and formalised in the logic of the interactive theorem prover Isabelle/HOL. We derive from those semantics a validation algorithm for temporal planning and show, using a formal proof in Isabelle/HOL, that this validation algorithm implements our semantics. We experimentally evaluate our verified validation algorithm and show that it is practical.",
    "code_link": ""
  },
  "aaai2022_main_goalrecognitionasreinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Goal Recognition as Reinforcement Learning",
    "authors": [
      "Leonardo Amado",
      "Reuth Mirsky",
      "Felipe Meneguzzi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21198",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21198/20947",
    "published": "2022-02",
    "summary": "Most approaches for goal recognition rely on specifications of the possible dynamics of the actor in the environment when pursuing a goal. These specifications suffer from two key issues. First, encoding these dynamics requires careful design by a domain expert, which is often not robust to noise at recognition time. Second, existing approaches often need costly real-time computations to reason about the likelihood of each potential goal. In this paper, we develop a framework that combines model-free reinforcement learning and goal recognition to alleviate the need for careful, manual domain design, and the need for costly online executions. This framework consists of two main stages: Offline learning of policies or utility functions for each potential goal, and online inference. We provide a first instance of this framework using tabular Q-learning for the learning stage, as well as three measures that can be used to perform the inference stage. The resulting instantiation achieves state-of-the-art performance against goal recognizers on standard evaluation domains and superior performance in noisy environments.",
    "code_link": ""
  },
  "aaai2022_main_onlinesearchwithbest-priceandquery-basedpredictions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Online Search with Best-Price and Query-Based Predictions",
    "authors": [
      "Spyros Angelopoulos",
      "Shahin Kamali",
      "Dehou Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21199",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21199/20948",
    "published": "2022-02",
    "summary": "In the online (time-series) search problem, a player is presented with a sequence of prices which are revealed in an online manner. In the standard definition of the problem, for each revealed price, the player must decide irrevocably whether to accept or reject it, without knowledge of future prices (other than an upper and a lower bound on their extreme values), and the objective is to minimize the competitive ratio, namely the worst case ratio between the maximum price in the sequence and the one selected by the player. The problem formulates several applications of decision-making in the face of uncertainty on the revealed samples.Previous work on this problem has largely assumed extreme scenarios in which either the player has almost no information about the input, or the player is provided with some powerful, and error-free advice. In this work, we study learning-augmented algorithms, in which there is a potentially erroneous prediction concerning the input. Specifically, we consider two different settings: the setting in which the prediction is related to the maximum price in the sequence, as well as well as the setting in which the prediction is obtained as a response to a number of binary queries. For both settings, we provide tight, or near-tight upper and lower bounds on the worst-case performance of search algorithms as a function of the prediction error. We also provide experimental results on data obtained from stock exchange markets that confirm the theoretical analysis, and explain how our techniques can be applicable to other learning-augmented applications.",
    "code_link": ""
  },
  "aaai2022_main_extendedgoalrecognitiondesignwithfirst-ordercomputationtreelogic": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Extended Goal Recognition Design with First-Order Computation Tree Logic",
    "authors": [
      "Tsz-Chiu Au"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21200",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21200/20949",
    "published": "2022-02",
    "summary": "Goal recognition design (GRD) is the task of modifying environments for aiding observers to recognize the objectives of agents during online observations. The worst case distinctiveness (WCD), a widely used performance measure in GRD research, can fail to provide useful guidance to the redesign process when some goals are too hard to be distinguished.Moreover, the existing WCD-based approaches do not work when an agent aims for a sequence of goals instead of just one goal. The paper presents a new GRD framework called extended goal recognition design (EGRD) for goal recognition that involves multiple goals. The objective of EGRD is to modify an environment to minimize the worst case distinctiveness of a goal condition that describes how an agent can reach a set of goals.A goal condition can be formally expressed in first-order computation tree logic (FO-CTL) that can be evaluated by model checking.We introduce a novel graphical representation of FO-CTL sentences that is suitable for extended goal recognition.Moreover, we present a search algorithm for EGRD with a novel caching mechanism.Our experimental results show that the caching mechanism can greatly speed up our EGRD search algorithm by reusing the previous evaluation of FO-CTL sentences.",
    "code_link": ""
  },
  "aaai2022_main_sampling-basedrobustcontrolofautonomoussystemswithnon-gaussiannoise": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sampling-Based Robust Control of Autonomous Systems with Non-Gaussian Noise",
    "authors": [
      "Thom S. Badings",
      "Alessandro Abate",
      "Nils Jansen",
      "David Parker",
      "Hasan A.\n      Poonawala",
      "Marielle Stoelinga"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21201",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21201/20950",
    "published": "2022-02",
    "summary": "Controllers for autonomous systems that operate in safety-critical settings must account for stochastic disturbances. Such disturbances are often modeled as process noise, and common assumptions are that the underlying distributions are known and/or Gaussian. In practice, however, these assumptions may be unrealistic and can lead to poor approximations of the true noise distribution. We present a novel planning method that does not rely on any explicit representation of the noise distributions. In particular, we address the problem of computing a controller that provides probabilistic guarantees on safely reaching a target. First, we abstract the continuous system into a discrete-state model that captures noise by probabilistic transitions between states. As a key contribution, we adapt tools from the scenario approach to compute probably approximately correct (PAC) bounds on these transition probabilities, based on a finite number of samples of the noise. We capture these bounds in the transition probability intervals of a so-called interval Markov decision process (iMDP). This iMDP is robust against uncertainty in the transition probabilities, and the tightness of the probability intervals can be controlled through the number of samples. We use state-of-the-art verification techniques to provide guarantees on the iMDP, and compute a controller for which these guarantees carry over to the autonomous system. Realistic benchmarks show the practical applicability of our method, even when the iMDP has millions of states or transitions.",
    "code_link": ""
  },
  "aaai2022_main_synthesisfromsatisficingandtemporalgoals": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Synthesis from Satisficing and Temporal Goals",
    "authors": [
      "Suguman Bansal",
      "Lydia Kavraki",
      "Moshe Y. Vardi",
      "Andrew Wells"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21202",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21202/20951",
    "published": "2022-02",
    "summary": "Reactive synthesis from high-level specifications that combine hard constraints expressed in Linear Temporal Logic (LTL) with soft constraints expressed by discounted sum (DS) rewards has applications in planning and reinforcement learning. An existing approach combines techniques from LTL synthesis with optimization for the DS rewards but has failed to yield a sound algorithm. An alternative approach combining LTL synthesis with satisficing DS rewards (rewards that achieve a threshold) is sound and complete for integer discount factors, but, in practice, a fractional discount factor is desired. This work extends the existing satisficing approach, presenting the first sound algorithm for synthesis from LTL and DS rewards with fractional discount factors. The utility of our algorithm is demonstrated on robotic planning domains.",
    "code_link": "https://github.com/suguman/NonIntegerGames"
  },
  "aaai2022_main_makingtranslationstoclassicalplanningcompetitivewithotherhtnplanners": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Making Translations to Classical Planning Competitive with Other HTN Planners",
    "authors": [
      "Gregor Behnke",
      "Florian Pollitt",
      "Daniel H\u00f6ller",
      "Pascal Bercher",
      "Ron Alford"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21203",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21203/20952",
    "published": "2022-02",
    "summary": "Translation-based approaches to planning allow for solving problems in complex and expressive formalisms via the means of highly efficient solvers for simpler formalisms. To be effective, these translations have to be constructed appropriately. The current existing translation of the highly expressive formalism of HTN planning into the more simple formalism of classical planning is not on par with the performance of current dedicated HTN planners. With our contributions in this paper, we close this gap: we describe new versions of the translation that reach the performance of state-of-the-art dedicated HTN planners. We present new translation techniques both for the special case of totally-ordered HTNs as well as for the general partially-ordered case. In the latter, we show that our new translation generates only linearly many actions, while the previous encoding generates and exponential number of actions.",
    "code_link": "https://github.com/panda-planner-dev/pandaPIengine"
  },
  "aaai2022_main_planverbdomain-independentverbalizationandsummaryoftaskplans": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PlanVerb: Domain-Independent Verbalization and Summary of Task Plans",
    "authors": [
      "Gerard Canal",
      "Senka Krivi\u0107",
      "Paul Luff",
      "Andrew Coles"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21204",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21204/20953",
    "published": "2022-02",
    "summary": "For users to trust planning algorithms, they must be able to understand the planner's outputs and the reasons for each action selection. This output does not tend to be user-friendly, often consisting of sequences of parametrised actions or task networks. And these may not be practical for non-expert users who may find it easier to read natural language descriptions. In this paper, we propose PlanVerb, a domain and planner-independent method for the verbalization of task plans. It is based on semantic tagging of actions and predicates. Our method can generate natural language descriptions of plans including causal explanations. The verbalized plans can be summarized by compressing the actions that act on the same parameters. We further extend the concept of verbalization space, previously applied to robot navigation, and apply it to planning to generate different kinds of plan descriptions for different user requirements. Our method can deal with PDDL and RDDL domains, provided that they are tagged accordingly. Our user survey evaluation shows that users can read our automatically generated plan descriptions and that the explanations help them answer questions about the plan.",
    "code_link": "https://github.com/gerardcanal/task_plan_verbalization"
  },
  "aaai2022_main_competingforresourcesestimatingadversarystrategyforeffectiveplangeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Competing for Resources: Estimating Adversary Strategy for Effective Plan Generation",
    "authors": [
      "Luk\u00e1\u0161 Chrpa",
      "Pavel Ryt\u00ed\u0159",
      "Rostislav Hor\u010d\u00edk",
      "Stefan Edelkamp"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21205",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21205/20954",
    "published": "2022-02",
    "summary": "Effective decision making while competing for limited resources in adversarial environments is important for many real-world applications (e.g. two Taxi companies competing for customers). Decision-making techniques such as Automated planning have to take into account possible actions of adversary (or competing) agents. That said, the agent should know what the competitor will likely do and then generate its plan accordingly. In this paper we propose a novel approach for estimating strategies of the adversary (or the competitor), sampling its actions that might hinder agent's goals by interfering with the agent's actions. The estimated competitor strategies are used in plan generation such that agent's actions have to be applied prior to the ones of the competitor, whose estimated times dictate the deadlines. We empirically evaluate our approach leveraging sampling of competitor's actions by comparing it to the naive approach optimising the make-span (not taking the competing agent into account at all) and to Nash Equilibrium (mixed) strategies.",
    "code_link": ""
  },
  "aaai2022_main_theffheuristicforliftedclassicalplanning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The FF Heuristic for Lifted Classical Planning",
    "authors": [
      "Augusto B. Corr\u00eaa",
      "Florian Pommerening",
      "Malte Helmert",
      "Guillem Franc\u00e8s"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21206",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21206/20955",
    "published": "2022-02",
    "summary": "Heuristics for lifted planning are not yet as informed as the best heuristics for ground planning. Recent work introduced the idea of using Datalog programs to compute the additive heuristic over lifted tasks. Based on this work, we show how to compute the more informed FF heuristic in a lifted manner. We extend the Datalog program with executable annotations that can also be used to define other delete-relaxation heuristics. In our experiments, we show that a planner using the lifted FF implementation produces state-of-the-art results for lifted planners. It also reduces the gap to state-of-the-art ground planners in domains where grounding is feasible.",
    "code_link": ""
  },
  "aaai2022_main_inconsistentplanningwhenindoubt,tossacoin!": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Inconsistent Planning: When in Doubt, Toss a Coin!",
    "authors": [
      "Yuriy Dementiev",
      "Fedor Fomin",
      "Artur Ignatiev"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21207",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21207/20956",
    "published": "2022-02",
    "summary": "One of the most widespread human behavioral biases is the present bias -- the tendency to overestimate current costs by a bias factor. Kleinberg and Oren (2014) introduced an elegant graph-theoretical model of inconsistent planning capturing the behavior of a present-biased agent accomplishing a set of actions. The essential measure of the system introduced by Kleinberg and Oren is the cost of irrationality -- the ratio of the total cost of the actions performed by the present-biased agent to the optimal cost. This measure is vital for a task designer to estimate the aftermaths of human behavior related to time-inconsistent planning, including procrastination and abandonment. As we prove in this paper, the cost of irrationality is highly susceptible to the agent's choices when faced with a few possible actions of equal estimated costs. To address this issue, we propose a modification of Kleinberg-Oren's model of inconsistent planning. In our model, when an agent selects from several options of minimum prescribed cost, he uses a randomized procedure. We explore the algorithmic complexity of computing and estimating the cost of irrationality in the new model.",
    "code_link": ""
  },
  "aaai2022_main_robustificationofonlinegraphexplorationmethods": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robustification of Online Graph Exploration Methods",
    "authors": [
      "Franziska Eberle",
      "Alexander Lindermayr",
      "Nicole Megow",
      "Lukas N\u00f6lke",
      "Jens\n      Schl\u00f6ter"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21208",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21208/20957",
    "published": "2022-02",
    "summary": "Exploring unknown environments is a fundamental task in many domains, e.g., robot navigation, network security, and internet search. We initiate the study of a learning-augmented variant of the classical, notoriously hard online graph exploration problem by adding access to machine-learned predictions. We propose an algorithm that naturally integrates predictions into the well-known Nearest Neighbor (NN) algorithm and significantly outperforms any known online algorithm if the prediction is of high accuracy while maintaining good guarantees when the prediction is of poor quality. We provide theoretical worst-case bounds that gracefully degrade with the prediction error, and we complement them by computational experiments that confirm our results. Further, we extend our concept to a general framework to robustify algorithms. By interpolating carefully between a given algorithm and NN, we prove new performance bounds that leverage the individual good performance on particular inputs while establishing robustness to arbitrary inputs.",
    "code_link": ""
  },
  "aaai2022_main_explainableplannerselectionforclassicalplanning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Explainable Planner Selection for Classical Planning",
    "authors": [
      "Patrick Ferber",
      "Jendrik Seipp"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21209",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21209/20958",
    "published": "2022-02",
    "summary": "Since no classical planner consistently outperforms all others, it is important to select a planner that works well for a given classical planning task. The two strongest approaches for planner selection use image and graph convolutional neural networks. They have the drawback that the learned models are complicated and uninterpretable. To obtain explainable models, we identify a small set of simple task features and show that elementary and interpretable machine learning techniques can use these features to solve roughly as many tasks as the complex approaches based on neural networks.",
    "code_link": ""
  },
  "aaai2022_main_operator-potentialheuristicsforsymbolicsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Operator-Potential Heuristics for Symbolic Search",
    "authors": [
      "Daniel Fi\u0161er",
      "\u00c1lvaro Torralba",
      "J\u00f6rg Hoffmann"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21210",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21210/20959",
    "published": "2022-02",
    "summary": "Symbolic search, using Binary Decision Diagrams (BDDs) to represent sets of states, is a competitive approach to optimal planning. Yet heuristic search in this context remains challenging. The many advances on admissible planning heuristics are not directly applicable, as they evaluate one state at a time. Indeed, progress using heuristic functions in symbolic search has been limited and even very informed heuristics have been shown to be detrimental. Here we show how this connection can be made stronger for LP-based potential heuristics. Our key observation is that, for this family of heuristic functions, the change of heuristic value induced by each operator can be precomputed. This facilitates their smooth integration into symbolic search. Our experiments show that this can pay off significantly: we establish a new state of the art in optimal symbolic planning.",
    "code_link": "https://gitlab.com/danfis/cpddl"
  },
  "aaai2022_main_reconfiguringshortestpathsingraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reconfiguring Shortest Paths in Graphs",
    "authors": [
      "Kshitij Gajjar",
      "Agastya Vibhuti Jha",
      "Manish Kumar",
      "Abhiruk Lahiri"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21211",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21211/20960",
    "published": "2022-02",
    "summary": "Reconfiguring two shortest paths in a graph means modifying one shortest path to the other by changing one vertex at a time, so that all the intermediate paths are also shortest paths. This problem has several natural applications, namely: (a) revamping road networks, (b) rerouting data packets in a synchronous multiprocessing setting, (c) the shipping container stowage problem, and (d) the train marshalling problem.When modelled as graph problems, (a) is the most general case while (b), (c) and (d) are restrictions to different graph classes. We show that (a) is intractable, even for relaxed variants of the problem. For (b), (c) and (d), we present efficient algorithms to solve the respective problems. We also generalise the problem to when at most k (for some k >= 2) contiguous vertices on a shortest path can be changed at a time.",
    "code_link": ""
  },
  "aaai2022_main_homomorphismsofliftedplanningtasksthecasefordelete-freerelaxationheuristics": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Homomorphisms of Lifted Planning Tasks: The Case for Delete-Free Relaxation Heuristics",
    "authors": [
      "Rostislav Hor\u010d\u00edk",
      "Daniel Fi\u0161er",
      "\u00c1lvaro Torralba"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21212",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21212/20961",
    "published": "2022-02",
    "summary": "Classical planning tasks are modelled in PDDL which is a schematic language based on first-order logic. Most of the current planners turn this lifted representation into a propositional one via a grounding process. However, grounding may cause an exponential blowup. Therefore it is important to investigate methods for searching for plans on the lifted level. To build a lifted state-based planner, it is necessary to invent lifted heuristics. We introduce maps between PDDL tasks preserving plans allowing to transform a PDDL task into a smaller one. We propose a novel method for computing lifted (admissible) delete-free relaxed heuristics via grounding of the smaller task and computing the (admissible) delete-free relaxed heuristics there. This allows us to transfer the knowledge about relaxed heuristics from the grounded level to the lifted level.",
    "code_link": "https://gitlab.com/danfis/cpddl"
  },
  "aaai2022_main_speedinguptherul\u00afdynamic-controllability-checkingalgorithmforsimpletemporalnetworkswithuncertainty": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Speeding Up the RUL\u00af Dynamic-Controllability-Checking Algorithm for Simple Temporal Networks with Uncertainty",
    "authors": [
      "Luke Hunsberger",
      "Roberto Posenato"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21213",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21213/20962",
    "published": "2022-02",
    "summary": "A Simple Temporal Network with Uncertainty (STNU) includes real-valued variables, called time-points; binary difference constraints on those time-points; and contingent links that represent actions with uncertain durations. STNUs have been used for robot control, web-service composition, and business processes. The most important property of an STNU is called dynamic controllability (DC); and algorithms for checking this property are called DC-checking algorithms. The DC-checking algorithm for STNUs with the best worst-case time-complexity is the RUL\u00af algorithm due to Cairo, Hunsberger and Rizzi. Its complexity is O(mn + k\u00b2n + kn log n), where n is the number of time-points, m is the number of constraints, and k is the number of contingent links. It is expected that this worst-case complexity cannot be improved upon. However, this paper provides a new algorithm, called RUL2021, that improves its performance in practice by an order of magnitude, as demonstrated by a thorough empirical evaluation.",
    "code_link": ""
  },
  "aaai2022_main_learningtosolveroutingproblemsviadistributionallyrobustoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning to Solve Routing Problems via Distributionally Robust Optimization",
    "authors": [
      "Yuan Jiang",
      "Yaoxin Wu",
      "Zhiguang Cao",
      "Jie Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21214",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21214/20963",
    "published": "2022-02",
    "summary": "Recent deep models for solving routing problems always assume a single distribution of nodes for training, which severely impairs their cross-distribution generalization ability. In this paper, we exploit group distributionally robust optimization (group DRO) to tackle this issue, where we jointly optimize the weights for different groups of distributions and the parameters for the deep model in an interleaved manner during training. We also design a module based on convolutional neural network, which allows the deep model to learn more informative latent pattern among the nodes. We evaluate the proposed approach on two types of well-known deep models including GCN and POMO. The experimental results on the randomly synthesized instances and the ones from two benchmark dataset (i.e., TSPLib and CVRPLib) demonstrate that our approach could significantly improve the cross-distribution generalization performance over the original models.",
    "code_link": ""
  },
  "aaai2022_main_learningprobablyapproximatelycompleteandsafeactionmodelsforstochasticworlds": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning Probably Approximately Complete and Safe Action Models for Stochastic Worlds",
    "authors": [
      "Brendan Juba",
      "Roni Stern"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21215",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21215/20964",
    "published": "2022-02",
    "summary": "We consider the problem of learning action models for planning in unknown stochastic environments that can be defined using the Probabilistic Planning Domain Description Language (PPDDL). As input, we are given a set of previously executed trajectories, and the main challenge is to learn an action model that has a similar goal achievement probability to the policies used to create these trajectories. To this end, we introduce a variant of PPDDL in which there is uncertainty about the transition probabilities, specified by an interval for each factor that contains the respective true transition probabilities. Then, we present SAM+, an algorithm that learns such an imprecise-PPDDL environment model. SAM+ has a polynomial time and sample complexity, and guarantees that with high probability, the true environment is indeed captured by the defined intervals. We prove that the action model SAM+ outputs has a goal achievement probability that is almost as good or better than that of the policies used to produced the training trajectories. Then, we show how to produce a PPDDL model based on this imprecise-PPDDL model that has similar properties.",
    "code_link": ""
  },
  "aaai2022_main_boundingqualityindiverseplanning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bounding Quality in Diverse Planning",
    "authors": [
      "Michael Katz",
      "Shirin Sohrabi",
      "Octavian Udrea"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21216",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21216/20965",
    "published": "2022-02",
    "summary": "Diverse planning is an important problem in automated planning with many real world applications. Recently, diverse planning has seen renewed interest, with work that defines a taxonomy of computational problems with respect to both plan quality and solution diversity. However, despite the recent advances in diverse planning, the variety of approaches and the number of available planners are still quite limited, even nonexistent for several computational problems. In this work, we aim to extend the portfolio of planners for various computational problems in diverse planning. To that end, we introduce a novel approach to finding solutions for three computational problems within diverse planning and present planners for these three problems. For one of these problems, our approach is the first one that is able to provide solutions to the problem. For another, we show that top-k and top quality planners can provide, albeit naive, solutions to the problem and we extend these planners to improve the diversity of the solution. Finally, for the third problem, we show that some existing diverse planners already provide solutions to that problem. We suggest another approach and empirically show it to compare favorably with these existing planners.",
    "code_link": "https://github.com/IBM/diversescore"
  },
  "aaai2022_main_a*searchandbound-sensitiveheuristicsforoversubscriptionplanning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A* Search and Bound-Sensitive Heuristics for Oversubscription Planning",
    "authors": [
      "Michael Katz",
      "Emil Keyder"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21217",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21217/20966",
    "published": "2022-02",
    "summary": "Oversubscription planning (OSP) is the problem of finding plans that maximize the utility value of their end state while staying within a specified cost bound. Recently, it has been shown that OSP problems can be reformulated as classical planning problems with multiple cost functions but no utilities. Here we take advantage of this reformulation to show that OSP problems can be solved optimally using the A* search algorithm, in contrast to previous approaches that have used variations on branch-and-bound search. This allows many powerful techniques developed for classical planning to be applied to OSP problems. We also introduce novel bound-sensitive heuristics, which are able to reason about the primary cost of a solution while taking into account secondary cost functions and bounds, to provide superior guidance compared to heuristics that do not take these bounds into account. We propose two such bound-sensitive variants of existing classical planning heuristics, and show experimentally that the resulting search is significantly more informed than with comparable heuristics that do not consider bounds.",
    "code_link": "https://github.com/emilkeyder/fd-2018-osp"
  },
  "aaai2022_main_nicerobustschedulingthroughreinforcementlearning-guidedintegerprogramming": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NICE: Robust Scheduling through Reinforcement Learning-Guided Integer Programming",
    "authors": [
      "Luke Kenworthy",
      "Siddharth Nayak",
      "Christopher Chin",
      "Hamsa Balakrishnan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21218",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21218/20967",
    "published": "2022-02",
    "summary": "Integer programs provide a powerful abstraction for representing a wide range of real-world scheduling problems. Despite their ability to model general scheduling problems, solving large-scale integer programs (IP) remains a computational challenge in practice. The incorporation of more complex objectives such as robustness to disruptions further exacerbates the computational challenge. We present NICE (Neural network IP Coefficient Extraction), a novel technique that combines reinforcement learning and integer programming to tackle the problem of robust scheduling. More specifically, NICE uses reinforcement learning to approximately represent complex objectives in an integer programming formulation. We use NICE to determine assignments of pilots to a flight crew schedule so as to reduce the impact of disruptions. We compare NICE with (1) a baseline integer programming formulation that produces a feasible crew schedule, and (2) a robust integer programming formulation that explicitly tries to minimize the impact of disruptions. Our experiments show that, across a variety of scenarios, NICE produces schedules resulting in 33% to 48% fewer disruptions than the baseline formulation. Moreover, in more severely constrained scheduling scenarios in which the robust integer program fails to produce a schedule within 90 minutes, NICE is able to build robust schedules in less than 2 seconds on average.",
    "code_link": "https://github.com/nsidn98/NICE"
  },
  "aaai2022_main_planningtoavoidsideeffects": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Planning to Avoid Side Effects",
    "authors": [
      "Toryn Q. Klassen",
      "Sheila A. McIlraith",
      "Christian Muise",
      "Jarvis Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21219",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21219/20968",
    "published": "2022-02",
    "summary": "In sequential decision making, objective specifications are often underspecified or incomplete, neglecting to take into account potential (negative) side effects. Executing plans without consideration of their side effects can lead to catastrophic outcomes -- a concern recently raised in relation to the safety of AI. In this paper we investigate how to avoid side effects in a symbolic planning setting. We study the notion of minimizing side effects in the context of a planning environment where multiple independent agents co-exist. We define (classes of) negative side effects in terms of their effect on the agency of those other agents. Finally, we show how plans which minimize side effects of different types can be computed via compilations to cost-optimizing symbolic planning, and investigate experimentally.",
    "code_link": ""
  },
  "aaai2022_main_sample-efficientiterativelowerboundoptimizationofdeepreactivepoliciesforplanningincontinuousmdps": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sample-Efficient Iterative Lower Bound Optimization of Deep Reactive Policies for Planning in Continuous MDPs",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar",
      "Scott Sanner"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21220",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21220/20969",
    "published": "2022-02",
    "summary": "Recent advances in deep learning have enabled optimization of deep reactive policies (DRPs) for continuous MDP planning by encoding a parametric policy as a deep neural network and exploiting automatic differentiation in an end-to-end model-based gradient descent framework. This approach has proven effective for optimizing DRPs in nonlinear continuous MDPs, but it requires a large number of sampled trajectories to learn effectively and can suffer from high variance in solution quality. In this work, we revisit the overall model-based DRP objective and instead take a minorization-maximization perspective to iteratively optimize the DRP w.r.t. a locally tight lower-bounded objective. This novel formulation of DRP learning as iterative lower bound optimization (ILBO) is particularly appealing because (i) each step is structurally easier to optimize than the overall objective, (ii) it guarantees a monotonically improving objective under certain theoretical conditions, and (iii) it reuses samples between iterations thus lowering sample complexity. Empirical evaluation confirms that ILBO is significantly more sample-efficient than the state-of-the-art DRP planner and consistently produces better solution quality with lower variance. We additionally demonstrate that ILBO generalizes well to new problem instances (i.e., different initial states) without requiring retraining.",
    "code_link": "https://github.com/siowmeng/ilbo"
  },
  "aaai2022_main_bridgingltlfinferencetognninferenceforlearningltlfformulae": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bridging LTLf Inference to GNN Inference for Learning LTLf Formulae",
    "authors": [
      "Weilin Luo",
      "Pingjia Liang",
      "Jianfeng Du",
      "Hai Wan",
      "Bo Peng",
      "Delong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21221",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21221/20970",
    "published": "2022-02",
    "summary": "Learning linear temporal logic on finite traces (LTLf) formulae aims to learn a target formula that characterizes the high-level behavior of a system from observation traces in planning. Existing approaches to learning LTLf formulae, however, can hardly learn accurate LTLf formulae from noisy data. It is challenging to design an efficient search mechanism in the large search space in form of arbitrary LTLf formulae while alleviating the wrong search bias resulting from noisy data. In this paper, we tackle this problem by bridging LTLf inference to GNN inference. Our key theoretical contribution is showing that GNN inference can simulate LTLf inference to distinguish traces. Based on our theoretical result, we design a GNN-based approach, GLTLf, which combines GNN inference and parameter interpretation to seek the target formula in the large search space. Thanks to the non-deterministic learning process of GNNs, GLTLf is able to cope with noise. We evaluate GLTLf on various datasets with noise. Our experimental results confirm the effectiveness of GNN inference in learning LTLf formulae and show that GLTLf is superior to the state-of-the-art approaches.",
    "code_link": "https://github.com/a79461378945/Bridging-LTLfInference-to-GNN-Inference-for-Learning-LTLf-Formulae"
  },
  "aaai2022_main_risk-awarestochasticshortestpath": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Risk-Aware Stochastic Shortest Path",
    "authors": [
      "Tobias Meggendorfer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21222",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21222/20971",
    "published": "2022-02",
    "summary": "We treat the problem of risk-aware control for stochastic shortest path (SSP) on Markov decision processes (MDP). Typically, expectation is considered for SSP, which however is oblivious to the incurred risk. We present an alternative view, instead optimizing conditional value-at-risk (CVaR), an established risk measure. We treat both Markov chains as well as MDP and introduce, through novel insights, two algorithms, based on linear programming and value iteration, respectively. Both algorithms offer precise and provably correct solutions. Evaluation of our prototype implementation shows that risk-aware control is feasible on several moderately sized models.",
    "code_link": ""
  },
  "aaai2022_main_differentialassessmentofblack-boxaiagents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Differential Assessment of Black-Box AI Agents",
    "authors": [
      "Rashmeet Kaur Nayyar",
      "Pulkit Verma",
      "Siddharth Srivastava"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21223",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21223/20972",
    "published": "2022-02",
    "summary": "Much of the research on learning symbolic models of AI agents focuses on agents with stationary models. This assumption fails to hold in settings where the agent's capabilities may change as a result of learning, adaptation, or other post-deployment modifications. Efficient assessment of agents in such settings is critical for learning the true capabilities of an AI system and for ensuring its safe usage. In this work, we propose a novel approach to differentially assess black-box AI agents that have drifted from their previously known models. As a starting point, we consider the fully observable and deterministic setting. We leverage sparse observations of the drifted agent's current behavior and knowledge of its initial model to generate an active querying policy that selectively queries the agent and computes an updated model of its functionality. Empirical evaluation shows that our approach is much more efficient than re-learning the agent model from scratch. We also show that the cost of differential assessment using our method is proportional to the amount of drift in the agent's functionality.",
    "code_link": "https://github.com/AAIR-lab/DAAISy"
  },
  "aaai2022_main_solvingdisjunctivetemporalnetworkswithuncertaintyunderrestrictedtime-basedcontrollabilityusingtreesearchandgraphneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability Using Tree Search and Graph Neural Networks",
    "authors": [
      "Kevin Osanlou",
      "Jeremy Frank",
      "Andrei Bursuc",
      "Tristan Cazenave",
      "Eric\n      Jacopin",
      "Christophe Guettier",
      "J. Benton"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21224",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21224/20973",
    "published": "2022-02",
    "summary": "Scheduling under uncertainty is an area of interest in artificial intelligence. We study the problem of Dynamic Controllability (DC) of Disjunctive Temporal Networks with Uncertainty (DTNU), which seeks a reactive scheduling strategy to satisfy temporal constraints in response to uncontrollable action durations. We introduce new semantics for reactive scheduling: Time-based Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We present a tree search approach to determine whether or not a DTNU is R-TDC. Moreover, we leverage the learning capability of a Graph Neural Network (GNN) as a heuristic for tree search guidance. Finally, we conduct experiments on a known benchmark on which we show R-TDC to retain significant completeness with regard to DC, while being faster to prove. This results in the tree search processing fifty percent more DTNU problems in R-TDC than the state-of-the-art DC solver does in DC with the same time budget. We also observe that GNN tree search guidance leads to substantial performance gains on benchmarks of more complex DTNUs, with up to eleven times more problems solved than the baseline tree search.",
    "code_link": ""
  },
  "aaai2022_main_decidingunsolvabilityintemporalplanningunderactionnon-self-overlapping": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deciding Unsolvability in Temporal Planning under Action Non-Self-Overlapping",
    "authors": [
      "Stefan Panjkovic",
      "Andrea Micheli",
      "Alessandro Cimatti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21225",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21225/20974",
    "published": "2022-02",
    "summary": "The field of Temporal Planning (TP) is receiving increasing interest for its many real-world applications. Most of the literature focuses on the TP problem of finding a plan, with algorithms that are not guaranteed to terminate when the problem admits no solution. In this paper, we present sound and complete decision procedures that address the dual problem of proving that no plan exists, which has important applications in oversubscription, model validation and optimization. We focus on the expressive and practically relevant semantics of action non-self-overlapping, recently proved to be PSPACE-complete. For this subclass, we propose two approaches: a reduction of the planning problem to model-checking of Timed Transition Systems, and a heuristic-search algorithm where temporal constraints are represented by Difference Bound Matrices. We implemented the approaches, and carried out an experimental evaluation against other state-of-the-art TP tools. On benchmarks that admit no plans, both approaches dramatically outperform the other planners, while the heuristic-search algorithm remains competitive on solvable benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_adistributionalframeworkforrisk-sensitiveend-to-endplanningincontinuousmdps": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Distributional Framework for Risk-Sensitive End-to-End Planning in Continuous MDPs",
    "authors": [
      "Noah Patton",
      "Jihwan Jeong",
      "Mike Gimelfarb",
      "Scott Sanner"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21226",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21226/20975",
    "published": "2022-02",
    "summary": "Recent advances in efficient planning in deterministic or stochastic high-dimensional domains with continuous action spaces leverage backpropagation through a model of the environment to directly optimize action sequences. However, existing methods typically do not take risk into account when optimizing in stochastic domains, which can be incorporated efficiently in MDPs by optimizing a nonlinear utility function of the return distribution. We bridge this gap by introducing Risk-Aware Planning using PyTorch (RAPTOR), a novel unified framework for risk-sensitive planning through end-to-end optimization of commonly-studied risk-sensitive utility functions such as entropic utility, mean-variance optimization and CVaR. A key technical difficulty of our approach is that direct optimization of general risk-sensitive utility functions by backpropagation is impossible due to the presence of environment stochasticity. The novelty of RAPTOR lies in leveraging reparameterization of the state distribution, leading to a unique distributional perspective of end-to-end planning where the return distribution is utilized for sampling as well as optimizing risk-aware objectives by backpropagation in a unified framework. We evaluate and compare RAPTOR on three highly stochastic MDPs, including nonlinear navigation, HVAC control, and linear reservoir control, demonstrating the ability of RAPTOR to manage risk in complex continuous domains according to different notions of risk-sensitive utility.",
    "code_link": ""
  },
  "aaai2022_main_formulasynthesisinpropositionaldynamiclogicwithshuffle": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Formula Synthesis in Propositional Dynamic Logic with Shuffle",
    "authors": [
      "Sophie Pinchinat",
      "Sasha Rubin",
      "Fran\u00e7ois Schwarzentruber"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21227",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21227/20976",
    "published": "2022-02",
    "summary": "We introduce the formula-synthesis problem for Propositional Dynamic Logic with Shuffle (PDL || ). This problem, which generalises the model-checking problem againsts PDL || is the following: given a finite transition systemand a regular term-grammar that generates (possibly infinitely many) PDL || formulas, find a formula generated by the grammar that is true in the structure (or return that there is none). We prove that the problem is undecidable in general, but add certain restrictions on the input structure or on the input grammar to yield decidability. In particular, we prove that (1) if the grammar only generates formulas in PDL (without shuffle), then the problem is EXPTIME-complete, and a further restriction to linear grammars is PSPACE-complete, and a further restriction to non-recursive grammars is NP-complete,and (2) if one restricts the input structure to have only simple paths then the problem is in 2-EXPTIME. This work is motivated by and opens up connections to other forms of synthesis from hierarchical descriptions, including HTN problems in Planning and Attack-tree Synthesis problems in Security.",
    "code_link": ""
  },
  "aaai2022_main_efficientencodingofcostoptimaldelete-freeplanningassat": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Encoding of Cost Optimal Delete-Free Planning as SAT",
    "authors": [
      "Masood Feyzbakhsh Rankooh",
      "Jussi Rintanen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21228",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21228/20977",
    "published": "2022-02",
    "summary": "We introduce a novel method for encoding cost optimal delete-free STRIPS Planning as SAT. Our method is based on representing relaxed plans as partial functions from the set of propositions to the set of actions. This function can map any proposition to a unique action that adds the proposition during execution of the relaxed plan. We show that a relaxed plan can be produced by maintaining acyclicity in the graph of all causal relations among propositions, represented by the mentioned partial function. We also show that by efficient encoding of action cost propagation and enforcing a series of upper bounds on the total costs of the output plan, an optimal plan can effectively be produced for a given delete-free STRIPS problem. Our empirical results indicate that this method is quite competitive with the state of the art, demonstrating a better coverage compared to that of competing methods on standard STRIPS planning benchmark problems.",
    "code_link": "https://github.com/AI-Planning/classical-domains"
  },
  "aaai2022_main_optimaladmissioncontrolformulticlassqueueswithtime-varyingarrivalratesviastateabstraction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimal Admission Control for Multiclass Queues with Time-Varying Arrival Rates via State Abstraction",
    "authors": [
      "Marc Rigter",
      "Danial Dervovic",
      "Parisa Hassanzadeh",
      "Jason Long",
      "Parisa\n      Zehtabi",
      "Daniele Magazzeni"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21229",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21229/20978",
    "published": "2022-02",
    "summary": "We consider a novel queuing problem where the decision-maker must choose to accept or reject randomly arriving tasks into a no buffer queue which are processed by N identical servers. Each task has a price, which is a positive real number, and a class. Each class of task has a different price distribution, service rate, and arrives according to an inhomogenous Poisson process. The objective is to decide which tasks to accept so that the total price of tasks processed is maximised over a finite horizon. We formulate the problem using a discrete time Markov Decision Process (MDP) with a hybrid state space. We show that the optimal value function has a specific structure, which enables us to solve the hybrid MDP exactly. Moreover, we rigorously prove that as the gap between successive decision epochs grows smaller, the discrete time solution approaches the optimal solution to the original continuous time problem. To improve the scalability of our approach to a greater number of servers and task classes, we present an approximation based on state abstraction. We validate our approach on synthetic data, as well as a real financial fraud data set, which is the motivating application for this work.",
    "code_link": ""
  },
  "aaai2022_main_enhancingcolumngenerationbyamachine-learning-basedpricingheuristicforgraphcoloring": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Enhancing Column Generation by a Machine-Learning-Based Pricing Heuristic for Graph Coloring",
    "authors": [
      "Yunzhuang Shen",
      "Yuan Sun",
      "Xiaodong Li",
      "Andrew Eberhard",
      "Andreas Ernst"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21230",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21230/20979",
    "published": "2022-02",
    "summary": "Column Generation (CG) is an effective method for solving large-scale optimization problems. CG starts by solving a subproblem with a subset of columns (i.e., variables) and gradually includes new columns that can improve the solution of the current subproblem. The new columns are generated as needed by repeatedly solving a pricing problem, which is often NP-hard and is a bottleneck of the CG approach. To tackle this, we propose a Machine-Learning-based Pricing Heuristic (MLPH) that can generate many high-quality columns efficiently. In each iteration of CG, our MLPH leverages an ML model to predict the optimal solution of the pricing problem, which is then used to guide a sampling method to efficiently generate multiple high-quality columns. Using the graph coloring problem, we empirically show that MLPH significantly enhances CG as compared to six state-of-the-art methods, and the improvement in CG can lead to substantially better performance of the branch-and-price exact method.",
    "code_link": "https://github.com/Joey-Shen/MLPH.git"
  },
  "aaai2022_main_qubitroutingusinggraphneuralnetworkaidedmontecarlotreesearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Qubit Routing Using Graph Neural Network Aided Monte Carlo Tree Search",
    "authors": [
      "Animesh Sinha",
      "Utkarsh Azad",
      "Harjinder Singh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21231",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21231/20980",
    "published": "2022-02",
    "summary": "Near-term quantum hardware can support two-qubit operations only on the qubits that can interact with each other. Therefore, to execute an arbitrary quantum circuit on the hardware, compilers have to first perform the task of qubit routing, i.e., to transform the quantum circuit either by inserting additional SWAP gates or by reversing existing CNOT gates to satisfy the connectivity constraints of the target topology. The depth of the transformed quantum circuits is minimized by utilizing the Monte Carlo tree search (MCTS) to perform qubit routing by making it both construct each action and search over the space of all actions. It is aided in performing these tasks by a Graph neural network that evaluates the value function and action probabilities for each state. Along with this, we propose a new method of adding mutex-lock like variables in our state representation which helps factor in the parallelization of the scheduled operations, thereby pruning the depth of the output circuit. Overall, our procedure (referred to as QRoute) performs qubit routing in a hardware agnostic manner, and it outperforms other available qubit routing implementations on various circuit benchmarks.",
    "code_link": ""
  },
  "aaai2022_main_classicalplanningwithavoidconditions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Classical Planning with Avoid Conditions",
    "authors": [
      "Marcel Steinmetz",
      "J\u00f6rg Hoffmann",
      "Alisa Kovtunova",
      "Stefan Borgwardt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21232",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21232/20981",
    "published": "2022-02",
    "summary": "It is often natural in planning to specify conditions that should be avoided, characterizing dangerous or highly undesirable behavior. PDDL3 supports this with temporal-logic state trajectory constraints. Here we focus on the simpler case where the constraint is a non-temporal formula ? - the avoid condition - that must be false throughout the plan. We design techniques tackling such avoid conditions effectively. We show how to learn from search experience which states necessarily lead into ?, and we show how to tailor abstractions to recognize that avoiding ? will not be possible starting from a given state. We run a large-scale experiment, comparing our techniques against compilation methods and against simple state pruning using ?. The results show that our techniques are often superior.",
    "code_link": ""
  },
  "aaai2022_main_stochasticgoalrecognitiondesignproblemswithsuboptimalagents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Stochastic Goal Recognition Design Problems with Suboptimal Agents",
    "authors": [
      "Christabel Wayllace",
      "William Yeoh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21233",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21233/20982",
    "published": "2022-02",
    "summary": "Goal Recognition Design (GRD) problems identify the minimum number of environmental modifications aiming to force an interacting agent to reveal its goal as early as possible. Researchers proposed several extensions to the original model, some of them handling stochastic agent action outcomes. While this generalization is useful, it assumes optimal acting agents, which limits its applicability to more realistic scenarios. This paper presents the Suboptimal Stochastic GRD model, where we consider boundedly rational agents that, due to limited resources, might follow a suboptimal policy. Inspired by theories on human behavior asserting that humans are (close to) optimal when making perceptual decisions, we assume the chosen policy has at most m suboptimal actions. Our contribution includes (I) Extending the stochastic goal recognition design framework by supporting suboptimal agents in cases where an observer has either full or partial observability; (ii) Presenting methods to evaluate the ambiguity of the model under these assumptions; and (iii) Evaluating our approach on a range of benchmark applications.",
    "code_link": "https://github.com/cwayllace/SS-GRD"
  },
  "aaai2022_main_equitypromotioninonlineresourceallocation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Equity Promotion in Online Resource Allocation",
    "authors": [
      "Pan Xu",
      "Yifan Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21234",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21234/20983",
    "published": "2022-02",
    "summary": "We consider online resource allocation under a typical non-profit setting, where limited or even scarce resources are administered by a not-for-profit organization like a government. We focus on the internal-equity by assuming that arriving requesters are homogeneous in terms of their external factors like demands but heterogeneous for their internal attributes like demographics. Specifically, we associate each arriving requester with one or several groups based on their demographics (i.e., race, gender, and age), and we aim to design an equitable distributing strategy such that every group of requesters can receive a fair share of resources proportional to a preset target ratio. We present two LP-based sampling algorithms and investigate them both theoretically (in terms of competitive-ratio analysis) and experimentally based on real COVID-19 vaccination data maintained by the Minnesota Department of Health. Both theoretical and numerical results show that our LP-based sampling strategies can effectively promote equity, especially when the arrival population is disproportionately represented, as observed in the early stage of the COVID-19 vaccine rollout.",
    "code_link": ""
  },
  "aaai2022_main_efficientdeviceschedulingwithmulti-jobfederatedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Device Scheduling with Multi-Job Federated Learning",
    "authors": [
      "Chendi Zhou",
      "Ji Liu",
      "Juncheng Jia",
      "Jingbo Zhou",
      "Yang Zhou",
      "Huaiyu Dai",
      "Dejing Dou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21235",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21235/20984",
    "published": "2022-02",
    "summary": "Recent years have witnessed a large amount of decentralized data in multiple (edge) devices of end-users, while the aggregation of the decentralized data remains difficult for machine learning jobs due to laws or regulations. Federated Learning (FL) emerges as an effective approach to handling decentralized data without sharing the sensitive raw data, while collaboratively training global machine learning models. The servers in FL need to select (and schedule) devices during the training process. However, the scheduling of devices for multiple jobs with FL remains a critical and open problem. In this paper, we propose a novel multi-job FL framework to enable the parallel training process of multiple jobs. The framework consists of a system model and two scheduling methods. In the system model, we propose a parallel training process of multiple jobs, and construct a cost model based on the training time and the data fairness of various devices during the training process of diverse jobs. We propose a reinforcement learning-based method and a Bayesian optimization-based method to schedule devices for multiple jobs while minimizing the cost. We conduct extensive experimentation with multiple jobs and datasets. The experimental results show that our proposed approaches significantly outperform baseline approaches in terms of training time (up to 8.67 times faster) and accuracy (up to 44.6% higher).",
    "code_link": ""
  },
  "aaai2022_main_mapdpcooperativemulti-agentreinforcementlearningtosolvepickupanddeliveryproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MAPDP: Cooperative Multi-Agent Reinforcement Learning to Solve Pickup and Delivery Problems",
    "authors": [
      "Zefang Zong",
      "Meng Zheng",
      "Yong Li",
      "Depeng Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21236",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21236/20985",
    "published": "2022-02",
    "summary": "Cooperative Pickup and Delivery Problem (PDP), as a variant of the typical Vehicle Routing Problems (VRP), is an important formulation in many real-world applications, such as on-demand delivery, industrial warehousing, etc. It is of great importance to efficiently provide high-quality solutions of cooperative PDP. However, it is not trivial to provide effective solutions directly due to two major challenges: 1) the structural dependency between pickup and delivery pairs require explicit modeling and representation. 2) the cooperation between different vehicles is highly related to the solution exploration and difficult to model. In this paper, we propose a novel multi-agent reinforcement learning based framework to solve the cooperative PDP (MAPDP). First, we design a paired context embedding to well measure the dependency of different nodes considering their structural limits. Second, we utilize cooperative multi-agent decoders to leverage the decision dependence among different vehicle agents based on a special communication embedding. Third, we design a novel cooperative A2C algorithm to train the integrated model. We conduct extensive experiments on a randomly generated dataset and a real-world dataset. Experiments result shown that the proposed MAPDP outperform all other baselines by at least 1.64\\% in all settings, and shows significant computation speed during solution inference.",
    "code_link": ""
  },
  "aaai2022_main_entropyestimationvianormalizingflow": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Entropy Estimation via Normalizing Flow",
    "authors": [
      "Ziqiao Ao",
      "Jinglai Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21237",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21237/20986",
    "published": "2022-02",
    "summary": "Entropy estimation is an important problem in information theory and statistical science. Many popular entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high dimensional problems. In this work we propose a transformbased method for high dimensional entropy estimation, which consists of the following two main ingredients. First by modifying the k-NN based entropy estimator, we propose a new estimator which enjoys small estimation bias for samples that are close to a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward a uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward a uniform distribution and then applying the proposed estimator to the transformed samples. Numerical experiments demonstrate the effectiveness of the method for high dimensional entropy estimation problems.",
    "code_link": ""
  },
  "aaai2022_main_fastandmorepowerfulselectiveinferenceforsparsehigh-orderinteractionmodel": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast and More Powerful Selective Inference for Sparse High-Order Interaction Model",
    "authors": [
      "Diptesh Das",
      "Vo Nguyen Le Duy",
      "Hiroyuki Hanada",
      "Koji Tsuda",
      "Ichiro\n      Takeuchi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21238",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21238/20987",
    "published": "2022-02",
    "summary": "Automated high-stake decision-making, such as medical diagnosis, requires models with high interpretability and reliability. We consider the sparse high-order interaction model as an interpretable and reliable model with a good prediction ability. However, finding statistically significant high-order interactions is challenging because of the intrinsically high dimensionality of the combinatorial effects. Another problem in data-driven modeling is the effect of ``cherry-picking\" (i.e., selection bias). Our main contribution is extending the recently developed parametric programming approach for selective inference to high-order interaction models. An exhaustive search over the cherry tree (all possible interactions) can be daunting and impractical, even for small-sized problems. We introduced an efficient pruning strategy and demonstrated the computational efficiency and statistical power of the proposed method using both synthetic and real data.",
    "code_link": "https://github.com/DipteshDas/SI-SHIM"
  },
  "aaai2022_main_generalizedstochasticmatching": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generalized Stochastic Matching",
    "authors": [
      "Alireza Farhadi",
      "Jacob Gilbert",
      "MohammadTaghi Hajiaghayi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21239",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21239/20988",
    "published": "2022-02",
    "summary": "In this paper, we generalize the recently studied stochastic matching problem to more accurately model a significant medical process, kidney exchange, and several other applications. Up until now the stochastic matching problem that has been studied was as follows: given a graph G= (V,E), each edge is included in the realized sub-graph of G independently with probability pe, and the goal is to find a degree-bounded sub-graph Q of G that has an expected maximum matching that approximates the expected maximum matching of G. This model does not account for possibilities of vertex dropouts, which can be found in several applications, e.g. in kidney exchange when donors or patients opt out of the exchange process as well as in online freelancing and online dating when online profiles are found to be faked. Thus, we will study a more generalized model of stochastic matching in which vertices and edges are both realized independently with some probabilities pv, pe, respectively, which more accurately fits important applications than the previously studied model.We will discuss the first algorithms and analysis for this generalization of the stochastic matching model and prove that they achieve good approximation ratios. In particular, we show that the approximation factor of a natural algorithm for this problem is at least 0.6568 in unweighted graphs, and 1/2+\u03b5 in weighted graphs for some constant \u03b5 >0. We further improve our result for unweighted graphs to 2/3 using edge degree constrained sub-graphs (EDCS).",
    "code_link": ""
  },
  "aaai2022_main_robusttestsinonlinedecision-making": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Robust Tests in Online Decision-Making",
    "authors": [
      "Gi-Soo Kim",
      "Jane P Kim",
      "Hyun-Joon Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21240",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21240/20989",
    "published": "2022-02",
    "summary": "Bandit algorithms are widely used in sequential decision problems to maximize the cumulative reward. One potential application is mobile health, where the goal is to promote the user's health through personalized interventions based on user specific information acquired through wearable devices. Important considerations include the type of, and frequency with which data is collected (e.g. GPS, or continuous monitoring), as such factors can severely impact app performance and users\u2019 adherence. In order to balance the need to collect data that is useful with the constraint of impacting app performance, one needs to be able to assess the usefulness of variables. Bandit feedback data are sequentially correlated, so traditional testing procedures developed for independent data cannot apply. Recently, a statistical testing procedure was developed for the actor-critic bandit algorithm. An actor-critic algorithm maintains two separate models, one for the actor, the action selection policy, and the other for the critic, the reward model. The performance of the algorithm as well as the validity of the test are guaranteed only when the critic model is correctly specified. However, misspecification is frequent in practice due to incorrect functional form or missing covariates. In this work, we propose a modified actor-critic algorithm which is robust to critic misspecification and derive a novel testing procedure for the actor parameters in this case.",
    "code_link": ""
  },
  "aaai2022_main_localdifferentialprivacyforbelieffunctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Local Differential Privacy for Belief Functions",
    "authors": [
      "Qiyu Li",
      "Chunlai Zhou",
      "Biao Qin",
      "Zhiqiang Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21241",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21241/20990",
    "published": "2022-02",
    "summary": "In this paper, we propose two new definitions of local differential privacy for belief functions. One is based on Shafer\u2019s semantics of randomly coded messages and the other from the perspective of imprecise probabilities. We show that such basic properties as composition and post-processing also hold for our new definitions. Moreover, we provide a hypothesis testing framework for these definitions and study the effect of \"don\u2019t know\" in the trade-off between privacy and utility in discrete distribution estimation.",
    "code_link": ""
  },
  "aaai2022_main_acompletecriterionforvalueofinformationinsolubleinfluencediagrams": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Complete Criterion for Value of Information in Soluble Influence Diagrams",
    "authors": [
      "Chris van Merwijk",
      "Ryan Carey",
      "Tom Everitt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21242",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21242/20991",
    "published": "2022-02",
    "summary": "Influence diagrams have recently been used to analyse the safety and fairness properties of AI systems. A key building block for this analysis is a graphical criterion for value of information (VoI). This paper establishes the first complete graphical criterion for VoI in influence diagrams with multiple decisions. Along the way, we establish two techniques for proving properties of multi-decision influence diagrams: ID homomorphisms are structure-preserving transformations of influence diagrams, while a Tree of Systems is a collection of paths that captures how information and control can flow in an influence diagram.",
    "code_link": "https://github.com/causalincentives/pycid"
  },
  "aaai2022_main_training-freeuncertaintyestimationfordenseregressionsensitivityasasurrogate": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate",
    "authors": [
      "Lu Mi",
      "Hao Wang",
      "Yonglong Tian",
      "Hao He",
      "Nir N Shavit"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21243",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21243/20992",
    "published": "2022-02",
    "summary": "Uncertainty estimation is an essential step in the evaluation of the robustness for deep learning models in computer vision, especially when applied in risk-sensitive areas. However, most state-of-the-art deep learning models either fail to obtain uncertainty estimation or need significant modification (e.g., formulating a proper Bayesian treatment) to obtain it. Most previous methods are not able to take an arbitrary model off the shelf and generate uncertainty estimation without retraining or redesigning it. To address this gap, we perform a systematic exploration into training-free uncertainty estimation for dense regression, an unrecognized yet important problem, and provide a theoretical construction justifying such estimations. We propose three simple and scalable methods to analyze the variance of outputs from a trained network under tolerable perturbations: infer-transformation, infer-noise, and infer-dropout. They operate solely during the inference, without the need to re-train, re-design, or fine-tune the models, as typically required by state-of-the-art uncertainty estimation methods. Surprisingly, even without involving such perturbations in training, our methods produce comparable or even better uncertainty estimation when compared to training-required state-of-the-art methods. Code is available at https://github.com/lumi9587/train-free-uncertainty.",
    "code_link": "https://github.com/lumi9587/train-free-uncertainty"
  },
  "aaai2022_main_ontheimpactofspuriouscorrelationforout-of-distributiondetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Impact of Spurious Correlation for Out-of-Distribution Detection",
    "authors": [
      "Yifei Ming",
      "Hang Yin",
      "Yixuan Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21244",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21244/20993",
    "published": "2022-02",
    "summary": "Modern neural networks can assign high confidence to inputs drawn from outside the training distribution, posing threats to models in real-world deployments. While much research attention has been placed on designing new out-of-distribution (OOD) detection methods, the precise definition of OOD is often left in vagueness and falls short of the desired notion of OOD in reality. In this paper, we present a new formalization and model the data shifts by taking into account both the invariant and environmental (spurious) features. Under such formalization, we systematically investigate how spurious correlation in the training set impacts OOD detection. Our results suggest that the detection performance is severely worsened when the correlation between spurious features and labels is increased in the training set. We further show insights on detection methods that are more effective in reducing the impact of spurious correlation, and provide theoretical analysis on why reliance on environmental features leads to high OOD detection error. Our work aims to facilitate better understanding of OOD samples and their formalization, as well as the exploration of methods that enhance OOD detection. Code is available at https://github.com/deeplearning-wisc/Spurious_OOD.",
    "code_link": "https://github.com/deeplearning-wisc/Spurious_OOD"
  },
  "aaai2022_main_inferenceandlearningwithmodeluncertaintyinprobabilisticlogicprograms": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Inference and Learning with Model Uncertainty in Probabilistic Logic Programs",
    "authors": [
      "Victor Verreet",
      "Vincent Derkinderen",
      "Pedro Zuidberg Dos Martires",
      "Luc De\n      Raedt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21245",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21245/20994",
    "published": "2022-02",
    "summary": "An issue that has so far received only limited attention in probabilistic logic programming (PLP) is the modelling of so-called epistemic uncertainty, the uncertainty about the model itself. Accurately quantifying this model uncertainty is paramount to robust inference, learning and ultimately decision making. We introduce BetaProbLog, a PLP language that can model epistemic uncertainty. BetaProbLog has sound semantics, an effective inference algorithm that combines Monte Carlo techniques with knowledge compilation, and a parameter learning algorithm. We empirically outperform state-of-the-art methods on probabilistic inference tasks in second-order Bayesian networks, digit classification and discriminative learning in the presence of epistemic uncertainty.",
    "code_link": "https://github.com/ML-KULeuven/betaproblog"
  },
  "aaai2022_main_domain-liftedsamplingforuniversaltwo-variablelogicandextensions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Domain-Lifted Sampling for Universal Two-Variable Logic and Extensions",
    "authors": [
      "Yuanhong Wang",
      "Timothy van Bremen",
      "Yuyi Wang",
      "Ond\u0159ej Ku\u017eelka"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21246",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21246/20995",
    "published": "2022-02",
    "summary": "Given a first-order sentence ? and a domain size n, how can one sample a model of ? on the domain {1, . . . , n} efficiently as n scales? We consider two variants of this problem: the uniform sampling regime, in which the goal is to sample a model uniformly at random, and the symmetric weighted sampling regime, in which models are weighted according to the number of groundings of each predicate appearing in them. Solutions to this problem have applications to the scalable generation of combinatorial structures, as well as sampling in several statistical-relational models such as Markov logic networks and probabilistic logic programs. In this paper, we identify certain classes of sentences that are domain-liftable under sampling, in the sense that they admit a sampling algorithm that runs in time polynomial in n. In particular, we prove that every sentence of the form \u2200x\u2200y: ?(x, y) for some quantifier-free formula ?(x,y) is domain-liftable under sampling. We then further show that this result continues to hold in the presence of one or more cardinality constraints as well as a single tree axiom constraint.",
    "code_link": "https://github.com/lucienwang1009/lifted"
  },
  "aaai2022_main_identifiabilityoflinearampchaingraphmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Identifiability of Linear AMP Chain Graph Models",
    "authors": [
      "Yuhao Wang",
      "Arnab Bhattacharyya"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21247",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21247/20996",
    "published": "2022-02",
    "summary": "We study identifiability of linear Andersson-Madigan-Perlman (AMP) chain graph models, which are a common generalization of linear structural equation models and Gaussian graphical models. AMP models are described by DAGs on chain components which themselves are undirected graphs. For a known chain component decomposition, we show that the DAG on the chain components is identifiable if the determinants of the residual covariance matrices of the chain components are equal (or more generally, monotone non-decreasing in topological order). This condition extends the equal variance identifiability criterion for Bayes nets, and it can be generalized from determinants to any super-additive function on positive semidefinite matrices. When the component decomposition is unknown, we describe conditions that allow recovery of the full structure using a polynomial time algorithm based on submodular function minimization. We also conduct experiments comparing our algorithm's performance against existing baselines.",
    "code_link": "https://github.com/majavid/AMPCGs2019"
  },
  "aaai2022_main_deepstochlogneuralstochasticlogicprogramming": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DeepStochLog: Neural Stochastic Logic Programming",
    "authors": [
      "Thomas Winters",
      "Giuseppe Marra",
      "Robin Manhaeve",
      "Luc De Raedt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21248",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21248/20997",
    "published": "2022-02",
    "summary": "Recent advances in neural-symbolic learning, such as DeepProbLog, extend probabilistic logic programs with neural predicates. Like graphical models, these probabilistic logic programs define a probability distribution over possible worlds, for which inference is computationally hard. We propose DeepStochLog, an alternative neural-symbolic framework based on stochastic definite clause grammars, a kind of stochastic logic program. More specifically, we introduce neural grammar rules into stochastic definite clause grammars to create a framework that can be trained end-to-end. We show that inference and learning in neural stochastic logic programming scale much better than for neural probabilistic logic programs. Furthermore, the experimental evaluation shows that DeepStochLog achieves state-of-the-art results on challenging neural-symbolic learning tasks.",
    "code_link": "https://github.com/ML-KULeuven/deepstochlog"
  },
  "aaai2022_main_towardsrobustoff-policylearningforruntimeuncertainty": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Robust Off-Policy Learning for Runtime Uncertainty",
    "authors": [
      "Da Xu",
      "Yuting Ye",
      "Chuanwei Ruan",
      "Bo Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21249",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21249/20998",
    "published": "2022-02",
    "summary": "Off-policy learning plays a pivotal role in optimizing and evaluating policies prior to the online deployment. However, during the real-time serving, we observe varieties of interventions and constraints that cause inconsistency between the online and offline setting, which we summarize and term as runtime uncertainty. Such uncertainty cannot be learned from the logged data due to its abnormality and rareness nature. To assert a certain level of robustness, we perturb the off-policy estimators along an adversarial direction in view of the runtime uncertainty. It allows the resulting estimators to be robust not only to observed but also unexpected runtime uncertainties. Leveraging this idea, we bring runtime-uncertainty robustness to three major off-policy learning methods: the inverse propensity score method, reward-model method, and doubly robust method. We theoretically justify the robustness of our methods to runtime uncertainty, and demonstrate their effectiveness using both the simulation and the real-world online experiments.",
    "code_link": ""
  },
  "aaai2022_main_improvingbayesianneuralnetworksbyadversarialsampling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Bayesian Neural Networks by Adversarial Sampling",
    "authors": [
      "Jiaru Zhang",
      "Yang Hua",
      "Tao Song",
      "Hao Wang",
      "Zhengui Xue",
      "Ruhui Ma",
      "Haibing\n      Guan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21250",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21250/20999",
    "published": "2022-02",
    "summary": "Bayesian neural networks (BNNs) have drawn extensive interest due to the unique probabilistic representation framework.However, Bayesian neural networks have limited publicized deployments because of the relatively poor model performance in real-world applications.In this paper, we argue that the randomness of sampling in Bayesian neural networks causes errors in the updating of model parameters during training and some sampled models with poor performance in testing.To solve this, we propose to train Bayesian neural networks with Adversarial Distribution as a theoretical solution. To avoid the difficulty of calculating Adversarial Distribution analytically, we further present the Adversarial Sampling method as an approximation in practice. We conduct extensive experiments with multiple network structures on different datasets, e.g., CIFAR-10 and CIFAR-100. Experimental results validate the correctness of the theoretical analysis and the effectiveness of the Adversarial Sampling on improving model performance.Additionally, models trained with Adversarial Sampling still keep their ability to model uncertainties and perform better when predictions are retained according to the uncertainties, which further verifies the generality of the Adversarial Sampling approach.",
    "code_link": "https://github.com/AISIGSJTU/AS"
  },
  "aaai2022_main_efficientoptimaltransportalgorithmbyacceleratedgradientdescent": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Optimal Transport Algorithm by Accelerated Gradient Descent",
    "authors": [
      "Dongsheng An",
      "Na Lei",
      "Xiaoyin Xu",
      "Xianfeng Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21251",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21251/21000",
    "published": "2022-02",
    "summary": "Optimal transport (OT) plays an essential role in various areas like machine learning and deep learning.However, computing discrete optimal transport plan for large scale problems with adequate accuracy and efficiency is still highly challenging. Recently, methods based on the Sinkhorn algorithm add an entropy regularizer to the prime problem and get a trade off between efficiency and accuracy. In this paper, we propose a novel algorithm to further improve the efficiency and accuracy based on Nesterov's smoothing technique. Basically, the non-smooth c-transform of the Kantorovich potential is approximated by the smooth Log-Sum-Exp function, which finally smooths the original non-smooth Kantorovich dual functional. The smooth Kantorovich functional can be optimized by the fast proximal gradient algorithm (FISTA) efficiently. Theoretically, the computational complexity of the proposed method is lower thancurrent estimation of the Sinkhorn algorithm in terms of the precision.Empirically, compared with the Sinkhorn algorithm, our experimental results demonstrate that the proposed method achieves faster convergence and better accuracy with the same parameter.",
    "code_link": ""
  },
  "aaai2022_main_localandgloballinearconvergenceofgenerallow-rankmatrixrecoveryproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Local and Global Linear Convergence of General Low-Rank Matrix Recovery Problems",
    "authors": [
      "Yingjie Bi",
      "Haixiang Zhang",
      "Javad Lavaei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21252",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21252/21001",
    "published": "2022-02",
    "summary": "We study the convergence rate of gradient-based local search methods for solving low-rank matrix recovery problems with general objectives in both symmetric and asymmetric cases, under the assumption of the restricted isometry property. First, we develop a new technique to verify the Polyak-Lojasiewicz inequality in a neighborhood of the global minimizers, which leads to a local linear convergence region for the gradient descent method. Second, based on the local convergence result and a sharp strict saddle property proven in this paper, we present two new conditions that guarantee the global linear convergence of the perturbed gradient descent method. The developed local and global convergence results provide much stronger theoretical guarantees than the existing results. As a by-product, this work significantly improves the existing bounds on the RIP constant required to guarantee the non-existence of spurious solutions.",
    "code_link": ""
  },
  "aaai2022_main_a*+bfhsahybridheuristicsearchalgorithm": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A*+BFHS: A Hybrid Heuristic Search Algorithm",
    "authors": [
      "Zhaoxing Bu",
      "Richard E. Korf"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21253",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21253/21002",
    "published": "2022-02",
    "summary": "We present a new algorithm called A*+BFHS for solving problems with unit-cost operators where A* and IDA* fail due to memory limitations and/or the existence of many distinct paths between the same pair of nodes. A*+BFHS is based on A* and breadth-first heuristic search (BFHS). A*+BFHS combines advantages from both algorithms, namely A*'s node ordering, BFHS's memory savings, and both algorithms' duplicate detection. On easy problems, A*+BFHS behaves the same as A*. On hard problems, it is slower than A* but saves a large amount of memory. Compared to BFIDA*, A*+BFHS reduces the search time and/or memory requirement by several times on a variety of planning domains.",
    "code_link": ""
  },
  "aaai2022_main_nukcpanimprovedlocalsearchalgorithmformaximumk-clubproblem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NukCP: An Improved Local Search Algorithm for Maximum k-Club Problem",
    "authors": [
      "Jiejiang Chen",
      "Yiyuan Wang",
      "Shaowei Cai",
      "Minghao Yin",
      "Yupeng Zhou",
      "Jieyu\n      Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21254",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21254/21003",
    "published": "2022-02",
    "summary": "The maximum k-club problem (MkCP) is an important clique relaxation problem with wide applications. Previous MkCP algorithms only work on small-scale instances and are not applicable for large-scale instances. For solving instances with different scales, this paper develops an efficient local search algorithm named NukCP for the MkCP which mainly includes two novel ideas. First, we propose a dynamic reduction strategy, which makes a good balance between the time efficiency and the precision effectiveness of the upper bound calculation. Second, a stratified threshold configuration checking strategy is designed by giving different priorities for the neighborhood in the different levels. Experiments on a broad range of different scale instances show that NukCP significantly outperforms the state-of-the-art MkCP algorithms on most instances.",
    "code_link": "https://github.com/yiyuanwang1988/NukCP.git"
  },
  "aaai2022_main_fourierrepresentationsforblack-boxoptimizationovercategoricalvariables": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fourier Representations for Black-Box Optimization over Categorical Variables",
    "authors": [
      "Hamid Dadkhahi",
      "Jesus Rios",
      "Karthikeyan Shanmugam",
      "Payel Das"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21255",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21255/21004",
    "published": "2022-02",
    "summary": "Optimization of real-world black-box functions defined over purely categorical variables is an active area of research. In particular, optimization and design of biological sequences with specific functional or structural properties have a profound impact in medicine, materials science, and biotechnology. Standalone search algorithms, such as simulated annealing (SA) and Monte Carlo tree search (MCTS), are typically used for such optimization problems. In order to improve the performance and sample efficiency of such algorithms, we propose to use existing methods in conjunction with a surrogate model for the black-box evaluations over purely categorical variables. To this end, we present two different representations, a group-theoretic Fourier expansion and an abridged one-hot encoded Boolean Fourier expansion. To learn such representations, we consider two different settings to update our surrogate model. First, we utilize an adversarial online regression setting where Fourier characters of each representation are considered as experts and their respective coefficients are updated via an exponential weight update rule each time the black box is evaluated. Second, we consider a Bayesian setting where queries are selected via Thompson sampling and the posterior is updated via a sparse Bayesian regression model (over our proposed representation) with a regularized horseshoe prior. Numerical experiments over synthetic benchmarks as well as real-world RNA sequence optimization and design problems demonstrate the representational power of the proposed methods, which achieve competitive or superior performance compared to state-of-the-art counterparts, while improving the computation cost and/or sample efficiency, substantially.",
    "code_link": ""
  },
  "aaai2022_main_newresultsinbounded-suboptimalsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "New Results in Bounded-Suboptimal Search",
    "authors": [
      "Maximilian Fickert",
      "Tianyi Gu",
      "Wheeler Ruml"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21256",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21256/21005",
    "published": "2022-02",
    "summary": "In bounded-suboptimal heuristic search, one attempts to find a solution that costs no more than a prespecified factor of optimal as quickly as possible. This is an important setting, as it admits faster-than-optimal solving while retaining some control over solution cost. In this paper, we investigate several new algorithms for bounded-suboptimal search, including novel variants of EES and DPS, the two most prominent previous proposals, and methods inspired by recent work in bounded-cost search that leverages uncertainty estimates of the heuristic. We perform what is, to our knowledge, the most comprehensive empirical comparison of bounded-suboptimal search algorithms to date, including both search and planning benchmarks, and we find that one of the new algorithms, a simple alternating queue scheme, significantly outperforms previous work.",
    "code_link": ""
  },
  "aaai2022_main_anexactalgorithmwithnewupperboundsforthemaximumk-defectivecliqueprobleminmassivesparsegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "An Exact Algorithm with New Upper Bounds for the Maximum k-Defective Clique Problem in Massive Sparse Graphs",
    "authors": [
      "Jian Gao",
      "Zhenghang Xu",
      "Ruizhi Li",
      "Minghao Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21257",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21257/21006",
    "published": "2022-02",
    "summary": "The Maximum k-Defective Clique Problem (MDCP), as a clique relaxation model, has been used to solve various problems. Because it is a hard computational task, previous works can hardly solve the MDCP for massive sparse graphs derived from real-world applications. In this work, we propose a novel branch-and-bound algorithm to solve the MDCP based on several new techniques. First, we propose two new upper bounds of the MDCP as well as corresponding reduction rules to remove redundant vertices and edges. The proposed reduction rules are particularly useful for massive graphs. Second, we present another new upper bound by counting missing edges between fixed vertices and an unfixed vertex for cutting branches. We perform extensive computational experiments to evaluate our algorithm. Experimental results show that our reduction rules are very effective for removing redundant vertices and edges so that graphs are reduced greatly. Also, our algorithm can solve benchmark instances efficiently, and it has significantly better performance than state-of-the-art algorithms.",
    "code_link": "https://github.com/chenxiaoyu233/k-defective"
  },
  "aaai2022_main_learningfrommistakes\u2013aframeworkforneuralarchitecturesearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Learning from Mistakes \u2013 a Framework for Neural Architecture Search",
    "authors": [
      "Bhanu Garg",
      "Li Zhang",
      "Pradyumna Sridhara",
      "Ramtin Hosseini",
      "Eric Xing",
      "Pengtao Xie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21258",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21258/21007",
    "published": "2022-02",
    "summary": "Learning from one's mistakes is an effective human learning technique where the learners focus more on the topics where mistakes were made, so as to deepen their understanding. In this paper, we investigate if this human learning strategy can be applied in machine learning. We propose a novel machine learning method called Learning From Mistakes (LFM), wherein the learner improves its ability to learn by focusing more on the mistakes during revision. We formulate LFM as a three-stage optimization problem: 1) learner learns; 2) learner re-learns focusing on the mistakes, and; 3) learner validates its learning. We develop an efficient algorithm to solve the LFM problem. We apply the LFM framework to neural architecture search on CIFAR-10, CIFAR-100, and Imagenet. Experimental results strongly demonstrate the effectiveness of our model.",
    "code_link": ""
  },
  "aaai2022_main_thecomplexityoftemporalvertexcoverinsmall-degreegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The Complexity of Temporal Vertex Cover in Small-Degree Graphs",
    "authors": [
      "Thekla Hamm",
      "Nina Klobas",
      "George B. Mertzios",
      "Paul G. Spirakis"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21259",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21259/21008",
    "published": "2022-02",
    "summary": "Temporal graphs naturally model graphs whose underlying topology changes over time. Recently, the problems Temporal Vertex Cover (or TVC) and Sliding-Window Temporal Vertex Cover (or Delta-TVC for time-windows of a fixed-length Delta) have been established as natural extensions of the classic Vertex Cover problem on static graphs with connections to areas such as surveillance in sensor networks. In this paper we initiate a systematic study of the complexity of TVC and Delta-TVC on sparse graphs. Our main result shows that for every Delta geq 2, Delta-TVC is NP-hard even when the underlying topology is described by a path or a cycle. This resolves an open problem from literature and shows a surprising contrast between Delta-TVC and TVC for which we provide a polynomial-time algorithm in the same setting. To circumvent this hardness, we present a number of exact and approximation algorithms for temporal graphs whose underlying topologies are given by a path, that have bounded vertex degree in every time step, or that admit a small-sized temporal vertex cover.",
    "code_link": ""
  },
  "aaai2022_main_provablesensorsetsforepidemicdetectionovernetworkswithminimumdelay": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Provable Sensor Sets for Epidemic Detection over Networks with Minimum Delay",
    "authors": [
      "Jack Heavey",
      "Jiaming Cui",
      "Chen Chen",
      "B. Aditya Prakash",
      "Anil Vullikanti"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21260",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21260/21009",
    "published": "2022-02",
    "summary": "The efficient detection of outbreaks and other cascading phenomena is a fundamental problem in a number of domains, including disease spread, social networks, and infrastructure networks. In such settings, monitoring and testing a small group of pre-selected nodes from the susceptible population (i.e., a sensor set) is often the preferred testing regime. We study the problem of selecting a sensor set that minimizes the delay in detection---we refer to this as the MinDelSS problem. Prior methods for minimizing the detection time rely on greedy algorithms using submodularity. We show that this approach can sometimes lead to a worse approximation for minimizing the detection time than desired. We also show that MinDelSS is hard to approximate within an O(n^(1-1/g))-factor for any constant g greater than or equal to 2 for a graph with n nodes. This instead motivates seeking a bicriteria approximations. We present the algorithm RoundSensor, which gives a rigorous worst case O(log(n))-factor for the detection time, while violating the budget by a factor of O(log^2(n)). Our algorithm is based on the sample average approximation technique from stochastic optimization, combined with linear programming and rounding. We evaluate our algorithm on several networks, including hospital contact networks, which validates its effectiveness in real settings.",
    "code_link": ""
  },
  "aaai2022_main_towardsautomateddiscoveryofgod-likefolkalgorithmsforrubik\u2019scube": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Automated Discovery of God-Like Folk Algorithms for Rubik\u2019s Cube",
    "authors": [
      "Garrett E. Katz",
      "Naveed Tahir"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21261",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21261/21010",
    "published": "2022-02",
    "summary": "We present a multi-objective meta-search procedure that constructs candidate algorithms for state-space search puzzles like Rubik's cube. The candidate algorithms take the form of macro databases, i.e., rule tables that specify sequences of actions to perform in different states. Rules are repeatedly applied until the puzzle is solved. The objectives favor candidates that are god-like (solving the puzzle in fewer steps) and folk-like (having fewer rules in the macro database). We build each candidate with a non-deterministic rule table construction, and then optimize over the non-deterministic choice points to find candidates near the Pareto-optimal trades-offs between godliness and folksiness. We prove that the rule table construction is correct: it always terminates and solves every state at termination. This is verified empirically on the full 2x2x2 \"pocket\" cube, where correct (but unoptimized) constructions take under one hour and the total number of rules is less than 10% the number of possible states. We also empirically assess the multi-objective optimization on restricted variants of the cube with up to 29K possible states, showing relative improvements in the objectives between 14-20%. Avenues for scaling up the method in future work are discussed.",
    "code_link": "https://github.com/garrettkatz/cubbies"
  },
  "aaai2022_main_mip-gnnadata-drivenframeworkforguidingcombinatorialsolvers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MIP-GNN: A Data-Driven Framework for Guiding Combinatorial Solvers",
    "authors": [
      "Elias B. Khalil",
      "Christopher Morris",
      "Andrea Lodi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21262",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21262/21011",
    "published": "2022-02",
    "summary": "Mixed-integer programming (MIP) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program (MILP) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary MILPs. Our code and appendix are publicly available at https://github.com/lyeskhalil/mipGNN.",
    "code_link": "https://github.com/lyeskhalil/mipGNN"
  },
  "aaai2022_main_banditlimiteddiscrepancysearchandapplicationtomachinelearningpipelineoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bandit Limited Discrepancy Search and Application to Machine Learning Pipeline Optimization",
    "authors": [
      "Akihiro Kishimoto",
      "Djallel Bouneffouf",
      "Radu Marinescu",
      "Parikshit Ram",
      "Ambrish Rawat",
      "Martin Wistuba",
      "Paulito Palmes",
      "Adi Botea"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21263",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21263/21012",
    "published": "2022-02",
    "summary": "Optimizing a machine learning (ML) pipeline has been an important topic of AI and ML. Despite recent progress, pipeline optimization remains a challenging problem, due to potentially many combinations to consider as well as slow training and validation. We present the BLDS algorithm for optimized algorithm selection (ML operations) in a fixed ML pipeline structure. BLDS performs multi-fidelity optimization for selecting ML algorithms trained with smaller computational overhead, while controlling its pipeline search based on multi-armed bandit and limited discrepancy search. Our experiments on well-known classification benchmarks show that BLDS is superior to competing algorithms. We also combine BLDS with hyperparameter optimization, empirically showing the advantage of BLDS.",
    "code_link": ""
  },
  "aaai2022_main_prismarichclassofparameterizedsubmodularinformationmeasuresforguideddatasubsetselection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Data Subset Selection",
    "authors": [
      "Suraj Kothawade",
      "Vishal Kaushal",
      "Ganesh Ramakrishnan",
      "Jeff Bilmes",
      "Rishabh\n      Iyer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21264",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21264/21013",
    "published": "2022-02",
    "summary": "With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i)targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is under performing, and ii)guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided image-collection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.",
    "code_link": ""
  },
  "aaai2022_main_splitmovesformonte-carlotreesearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Split Moves for Monte-Carlo Tree Search",
    "authors": [
      "Jakub Kowalski",
      "Maksymilian Mika",
      "Wojciech Pawlik",
      "Jakub Sutowicz",
      "Marek\n      Szyku\u0142a",
      "Mark H. M. Winands"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21265",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21265/21014",
    "published": "2022-02",
    "summary": "In many games, moves consist of several decisions made by the player. These decisions can be viewed as separate moves, which is already a common practice in multi-action games for efficiency reasons. Such division of a player move into a sequence of simpler / lower level moves is called splitting. So far, split moves have been applied only in forementioned straightforward cases, and furthermore, there was almost no study revealing its impact on agents' playing strength. Taking the knowledge-free perspective, we aim to answer how to effectively use split moves within Monte-Carlo Tree Search (MCTS) and what is the practical impact of split design on agents' strength. This paper proposes a generalization of MCTS that works with arbitrarily split moves. We design several variations of the algorithm and try to measure the impact of split moves separately on efficiency, quality of MCTS, simulations, and action-based heuristics. The tests are carried out on a set of board games and performed using the Regular Boardgames General Game Playing formalism, where split strategies of different granularity can be automatically derived based on an abstract description of the game. The results give an overview of the behavior of agents using split design in different ways. We conclude that split design can be greatly beneficial for single- as well as multi-action games.",
    "code_link": "https://github.com/marekesz/rbg"
  },
  "aaai2022_main_mapf-lns2fastrepairingformulti-agentpathfindingvialargeneighborhoodsearch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MAPF-LNS2: Fast Repairing for Multi-Agent Path Finding via Large Neighborhood Search",
    "authors": [
      "Jiaoyang Li",
      "Zhe Chen",
      "Daniel Harabor",
      "Peter J. Stuckey",
      "Sven Koenig"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21266",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21266/21015",
    "published": "2022-02",
    "summary": "Multi-Agent Path Finding (MAPF) is the problem of planning collision-free paths for multiple agents in a shared environment. In this paper, we propose a novel algorithm MAPF-LNS2 based on large neighborhood search for solving MAPF efficiently. Starting from a set of paths that contain collisions, MAPF-LNS2 repeatedly selects a subset of colliding agents and replans their paths to reduce the number of collisions until the paths become collision-free. We compare MAPF-LNS2 against a variety of state-of-the-art MAPF algorithms, including Prioritized Planning with random restarts, EECBS, and PPS, and show that MAPF-LNS2 runs significantly faster than them while still providing near-optimal solutions in most cases. MAPF-LNS2 solves 80% of the random-scenario instances with the largest number of agents from the MAPF benchmark suite with a runtime limit of just 5 minutes, which, to our knowledge, has not been achieved by any existing algorithms.",
    "code_link": "https://github.com/Jiaoyang-Li/MAPF-LNS2"
  },
  "aaai2022_main_localandglobalconvergenceofgeneralburer-monteirotensoroptimizations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Local and Global Convergence of General Burer-Monteiro Tensor Optimizations",
    "authors": [
      "Shuang Li",
      "Qiuwei Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21267",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21267/21016",
    "published": "2022-02",
    "summary": "Tensor optimization is crucial to massive machine learning and signal processing tasks. In this paper, we consider tensor optimization with a convex and well-conditioned objective function and reformulate it into a nonconvex optimization using the Burer-Monteiro type parameterization. We analyze the local convergence of applying vanilla gradient descent to the factored formulation and establish a local regularity condition under mild assumptions. We also provide a linear convergence analysis of the gradient descent algorithm started in a neighborhood of the true tensor factors.Complementary to the local analysis, this work also characterizes the global geometry of the best rank-one tensor approximation problem and demonstrates that for orthogonally decomposable tensors the problem has no spurious local minima and all saddle points are strict except for the one at zero which is a third-order saddle point.",
    "code_link": ""
  },
  "aaai2022_main_bi-cmrbidirectionalreinforcementguidedhashingforeffectivecross-modalretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bi-CMR: Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval",
    "authors": [
      "Tieying Li",
      "Xiaochun Yang",
      "Bin Wang",
      "Chong Xi",
      "Hanzhong Zheng",
      "Xiangmin\n      Zhou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21268",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21268/21017",
    "published": "2022-02",
    "summary": "Cross-modal hashing has attracted considerable attention for large-scale multimodal data. Recent supervised cross-modal hashing methods using multi-label networks utilize the semantics of multi-labels to enhance retrieval accuracy, where label hash codes are learned independently. However, all these methods assume that label annotations reliably reflect the relevance between their corresponding instances, which is not true in real applications. In this paper, we propose a novel framework called Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval (Bi-CMR), which exploits a bidirectional learning to relieve the negative impact of this assumption. Specifically, in the forward learning procedure, we highlight the representative labels and learn the reinforced multi-label hash codes by intra-modal semantic information, and further adjust similarity matrix. In the backward learning procedure, the reinforced multi-label hash codes and adjusted similarity matrix are used to guide the matching of instances. We construct two datasets with explicit relevance labels that reflect the semantic relevance of instance pairs based on two benchmark datasets. The Bi-CMR is evaluated by conducting extensive experiments over these two datasets. Experimental results prove the superiority of Bi-CMR over four state-of-the-art methods in terms of effectiveness.",
    "code_link": ""
  },
  "aaai2022_main_improvinglocalsearchalgorithmsviaprobabilisticconfigurationchecking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Local Search Algorithms via Probabilistic Configuration Checking",
    "authors": [
      "Weilin Luo",
      "Rongzhen Ye",
      "Hai Wan",
      "Shaowei Cai",
      "Biqing Fang",
      "Delong Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21269",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21269/21018",
    "published": "2022-02",
    "summary": "Configuration checking (CC) has been confirmed to alleviate the cycling problem in local search for combinatorial optimization problems (COPs). When using CC heuristics in local search for graph problems, a critical concept is the configuration of the vertices. All existing CC variants employ either 1- or 2-level neighborhoods of a vertex as its configuration. Inspired by the idea that neighborhoods with different levels should have different contributions to solving COPs, we propose the probabilistic configuration (PC), which introduces probabilities for neighborhoods at different levels to consider the impact of neighborhoods of different levels on the CC strategy. Based on the concept of PC, we first propose probabilistic configuration checking (PCC), which can be developed in an automated and lightweight favor. We then apply PCC to two classic COPs which have been shown to achieve good results by using CC, and our preliminary results confirm that PCC improves the existing algorithms because PCC alleviates the cycling problem.",
    "code_link": ""
  },
  "aaai2022_main_pea*+ida*animprovedhybridmemory-restrictedalgorithm": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "PEA*+IDA*: An Improved Hybrid Memory-Restricted Algorithm",
    "authors": [
      "Frederico Messa",
      "Andr\u00e9 Grahl Pereira"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21270",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21270/21019",
    "published": "2022-02",
    "summary": "It is well-known that the search algorithms A* and Iterative Deepening A* (IDA*) can fail to solve state-space tasks optimally due to time and memory limits. The former typically fails in memory-restricted scenarios and the latter in time-restricted scenarios. Therefore, several algorithms were proposed to solve state-space tasks optimally using less memory than A* and less time than IDA*, such as A*+IDA*, a hybrid memory-restricted algorithm that combines A* and IDA*. In this paper, we present a hybrid memory-restricted algorithm that combines Partial Expansion A* (PEA*) and IDA*. This new algorithm has two phases, the same structure as the A*+IDA* algorithm. The first phase of PEA*+IDA* runs PEA* until it reaches a memory limit, and the second phase runs IDA* without duplicate detection on each node of PEA*'s Open. First, we present a model that shows how PEA*+IDA* can perform better than A*+IDA* although pure PEA* usually makes more expansions than pure A*. Later, we perform an experimental evaluation using three memory limits and show that, compared to A*+IDA* on classical planning domains, PEA*+IDA* has higher coverage and expands fewer nodes. Finally, we experimentally analyze both algorithms and show that having higher F-limits and better priority-queue composition given by PEA* have a considerable impact on the performance of the algorithms.",
    "code_link": ""
  },
  "aaai2022_main_searchstrategiesfortopologicalnetworkoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Search Strategies for Topological Network Optimization",
    "authors": [
      "Michael D. Moffitt"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21271",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21271/21020",
    "published": "2022-02",
    "summary": "We consider an application of combinatorial search to the optimization of topologies in series-parallel networks. We propose a recursive search over the space of decomposition trees, in which partial solutions are obtained by exploring k-way partitionings of expandable nodes. We present two complementary pruning techniques that bound the value of intermediate solutions from above and below, applying monotonic operations to the contents of unresolved leaves. We also develop a means to exploit the convexity of our objective function, so as to prevent the redundant recomputation of subcircuit configurations. Finally, we evaluate our approach on a parameterized benchmark suite of electrical circuits, demonstrating over an order of magnitude improvement in performance as compared to a baseline implementation.",
    "code_link": "https://github.com/google/network-opt"
  },
  "aaai2022_main_hibernatedbackdooramutualinformationempoweredbackdoorattacktodeepneuralnetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks",
    "authors": [
      "Rui Ning",
      "Jiang Li",
      "Chunsheng Xin",
      "Hongyi Wu",
      "Chonggang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21272",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21272/21021",
    "published": "2022-02",
    "summary": "We report a new neural backdoor attack, named Hibernated Backdoor, which is stealthy, aggressive and devastating. The backdoor is planted in a hibernated mode to avoid being detected. Once deployed and fine-tuned on end-devices, the hibernated backdoor turns into the active state that can be exploited by the attacker. To the best of our knowledge, this is the first hibernated neural backdoor attack. It is achieved by maximizing the mutual information (MI) between the gradients of regular and malicious data on the model. We introduce a practical algorithm to achieve MI maximization to effectively plant the hibernated backdoor. To evade adaptive defenses, we further develop a targeted hibernated backdoor, which can only be activated by specific data samples and thus achieves a higher degree of stealthiness. We show the hibernated backdoor is robust and cannot be removed by existing backdoor removal schemes. It has been fully tested on four datasets with two neural network architectures, compared to five existing backdoor attacks, and evaluated using seven backdoor detection schemes. The experiments demonstrate the effectiveness of the hibernated backdoor attack under various settings.",
    "code_link": ""
  },
  "aaai2022_main_planningwithexplanationsforfindingdesiredmeetingpointsongraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Planning with Explanations for Finding Desired Meeting Points on Graphs",
    "authors": [
      "Keisuke Otaki"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21273",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21273/21022",
    "published": "2022-02",
    "summary": "Combinatorial optimization problems are ubiquitous for decision making in planning social infrastructures. In real-world scenarios, a decision-maker needs to solve his/her problem iteratively until he/she satisfies solutions, but such an iterative process remains challenging. This paper studies a new explainable framework, particularly for finding meeting points, which is a key optimization problem for designing facility locations. Our framework automatically fills the gap between its input instance and instances from which a user could obtain the desired outcome, where computed solutions are judged by the user. The framework also provides users with explanations, representing the difference of instances for deeply understanding the process and its inside. Explanations are clues for users to understand their situation and implement suggested results in practice (e.g., designing a coupon for free travel). We experimentally demonstrate that our search-based framework is promising to solve instances with generating explanations in a sequential decision-making process.",
    "code_link": ""
  },
  "aaai2022_main_afastlocalsearchalgorithmforthelatinsquarecompletionproblem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Fast Local Search Algorithm for the Latin Square Completion Problem",
    "authors": [
      "Shiwei Pan",
      "Yiyuan Wang",
      "Minghao Yin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21274",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21274/21023",
    "published": "2022-02",
    "summary": "The Latin square completion (LSC) problem is an important NP-complete problem with numerous applications. Given its theoretical and practical importance, several algorithms are designed for solving the LSC problem. In this work, to further improve the performance, a fast local search algorithm is developed based on three main ideas. Firstly, a reduction reasoning technique is used to reduce the scale of search space. Secondly, we propose a novel conflict value selection heuristic, which considers the history conflicting information of vertices as a selection criterion when more than one vertex have equal values on the primary scoring function. Thirdly, during the search phase, we record previous history search information and then make use of these information to restart the candidate solution. Experimental results show that our proposed algorithm significantly outperforms the state-of-the-art heuristic algorithms on almost all instances in terms of success rate and run time.",
    "code_link": "https://github.com/YanJINFR/Latin-Square-Completion.git"
  },
  "aaai2022_main_sparsificationofdecomposablesubmodularfunctions": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sparsification of Decomposable Submodular Functions",
    "authors": [
      "Akbar Rafiey",
      "Yuichi Yoshida"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21275",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21275/21024",
    "published": "2022-02",
    "summary": "Submodular functions are at the core of many machine learning and data mining tasks. The underlying submodular functions for many of these tasks are decomposable, i.e., they are sum of several simple submodular functions. In many data intensive applications, however, the number of underlying submodular functions in the original function is so large that we need prohibitively large amount of time to process it and/or it does not even fit in the main memory. To overcome this issue, we introduce the notion of sparsification for decomposable submodular functions whose objective is to obtain an accurate approximation of the original function that is a (weighted) sum of only a few submodular functions. Our main result is a polynomial-time randomized sparsification algorithm such that the expected number of functions used in the output is independent of the number of underlying submodular functions in the original function. We also study the effectiveness of our algorithm under various constraints such as matroid and cardinality constraints. We complement our theoretical analysis with an empirical study of the performance of our algorithm.",
    "code_link": ""
  },
  "aaai2022_main_subsetapproximationofparetoregionswithbi-objectivea*": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Subset Approximation of Pareto Regions with Bi-objective A*",
    "authors": [
      "Nicol\u00e1s Rivera",
      "Jorge A. Baier",
      "Carlos Hern\u00e1ndez"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21276",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21276/21025",
    "published": "2022-02",
    "summary": "In bi-objective search, we are given a graph in which each directed arc is associated with a pair of non-negative weights, and the objective is to find the Pareto-optimal solution set. Unfortunately, in many practical settings, this set is too large, and therefore its computation is very time-consuming. In addition, even though bi-objective search algorithms generate the Pareto set incrementally, they do so exhaustively. This means that early during search the solution set covers is not diverse, being concentrated in a small region of the solution set. To address this issue, we present a new approach to subset approximation of the solution set, that can be used as the basis for an anytime bi-objective search algorithm. Our approach transforms the given task into a target bi-objective search task using two real parameters. For each particular parameter setting, the solutions to the target task is a subset of the solution set of the original task. Depending on the parameters used, the solution set of the target task may be computed very quickly. Thisallows us to obtain, in challenging road map benchmarks, a rich variety of solutions in times that may be orders of magnitude smaller than the time needed to compute the solution set. We show that by running the algorithm with an appropriate sequence of parameters, we obtain a growing sequence of solutions that converges to the full solution set. We prove that our approach is correct and that Bi-Objective A* prunes at least as many nodes when run over the target task.",
    "code_link": ""
  },
  "aaai2022_main_onprobabilisticgeneralizationofbackdoorsinbooleansatisfiability": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On Probabilistic Generalization of Backdoors in Boolean Satisfiability",
    "authors": [
      "Alexander Semenov",
      "Artem Pavlenko",
      "Daniil Chivilikhin",
      "Stepan Kochemazov"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21277",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21277/21026",
    "published": "2022-02",
    "summary": "The paper proposes a probabilistic generalization of the well-known Strong Backdoor Set (SBS) concept applied to the Boolean Satisfiability Problem (SAT). We call a set of Boolean variables B a \u03c1-backdoor, if for a fraction of at least \u03c1 of possible assignments of variables from B, assigning their values to variables in a Boolean formula in Conjunctive Normal Form (CNF) results in polynomially solvable formulas. Clearly, a \u03c1-backdoor with \u03c1=1 is an SBS. For a given set B it is possible to efficiently construct an (\u03b5, \u03b4)-approximation of parameter \u03c1 using the Monte Carlo method. Thus, we define an (\u03b5, \u03b4)-SBS as such a set B for which the conclusion \"parameter \u03c1 deviates from 1 by no more than \u03b5\" is true with probability no smaller than 1 - \u03b4. We consider the problems of finding the minimum SBS and the minimum (\u03b5, \u03b4)-SBS. To solve the former problem, one can use the algorithm described by R. Williams, C. Gomes and B. Selman in 2003. In the paper we propose a new probabilistic algorithm to solve the latter problem, and show that the asymptotic estimation of the worst-case complexity of the proposed algorithm is significantly smaller than that of the algorithm by Williams et al. For practical applications, we suggest a metaheuristic optimization algorithm based on the penalty function method to seek the minimal (\u03b5, \u03b4)-SBS. Results of computational experiments show that the use of (\u03b5, \u03b4)-SBSes found by the proposed algorithm allows speeding up solving of test problems related to equivalence checking and hard crafted and combinatorial benchmarks compared to state-of-the-art SAT solvers.",
    "code_link": ""
  },
  "aaai2022_main_anovelapproachtosolvinggoal-achievingproblemsforboardgames": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Novel Approach to Solving Goal-Achieving Problems for Board Games",
    "authors": [
      "Chung-Chin Shih",
      "Ti-Rong Wu",
      "Ting Han Wei",
      "I-Chen Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21278",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21278/21027",
    "published": "2022-02",
    "summary": "Goal-achieving problems are puzzles that set up a specific situation with a clear objective. An example that is well-studied is the category of life-and-death (L&D) problems for Go, which helps players hone their skill of identifying region safety. Many previous methods like lambda search try null moves first, then derive so-called relevance zones (RZs), outside of which the opponent does not need to search. This paper first proposes a novel RZ-based approach, called the RZ-Based Search (RZS), to solving L&D problems for Go. RZS tries moves before determining whether they are null moves post-hoc. This means we do not need to rely on null move heuristics, resulting in a more elegant algorithm, so that it can also be seamlessly incorporated into AlphaZero's super-human level play in our solver. To repurpose AlphaZero for solving, we also propose a new training method called Faster to Life (FTL), which modifies AlphaZero to entice it to win more quickly. We use RZS and FTL to solve L&D problems on Go, namely solving 68 among 106 problems from a professional L&D book while a previous state-of-the-art program TSUMEGO-EXPLORER solves 11 only. Finally, we discuss that the approach is generic in the sense that RZS is applicable to solving many other goal-achieving problems for board games.",
    "code_link": "https://github.com/rockmanray/rzone"
  },
  "aaai2022_main_machinelearningforonlinealgorithmselectionundercensoredfeedback": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Machine Learning for Online Algorithm Selection under Censored Feedback",
    "authors": [
      "Alexander Tornede",
      "Viktor Bengs",
      "Eyke H\u00fcllermeier"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21279",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21279/21028",
    "published": "2022-02",
    "summary": "In online algorithm selection (OAS), instances of an algorithmic problem class are presented to an agent one after another, and the agent has to quickly select a presumably best algorithm from a fixed set of candidate algorithms. For decision problems such as satisfiability (SAT), quality typically refers to the algorithm's runtime. As the latter is known to exhibit a heavy-tail distribution, an algorithm is normally stopped when exceeding a predefined upper time limit. As a consequence, machine learning methods used to optimize an algorithm selection strategy in a data-driven manner need to deal with right-censored samples, a problem that has received little attention in the literature so far. In this work, we revisit multi-armed bandit algorithms for OAS and discuss their capability of dealing with the problem. Moreover, we adapt them towards runtime-oriented losses, allowing for partially censored data while keeping a space- and time-complexity independent of the time horizon. In an extensive experimental evaluation on an adapted version of the ASlib benchmark, we demonstrate that theoretically well-founded methods based on Thompson sampling perform specifically strong and improve in comparison to existing methods.",
    "code_link": "https://github.com/alexandertornede/online_as"
  },
  "aaai2022_main_procrastinatedtreesearchblack-boxoptimizationwithdelayed,noisy,andmulti-fidelityfeedback": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Procrastinated Tree Search: Black-Box Optimization with Delayed, Noisy, and Multi-Fidelity Feedback",
    "authors": [
      "Junxiong Wang",
      "Debabrota Basu",
      "Immanuel Trummer"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21280",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21280/21029",
    "published": "2022-02",
    "summary": "In black-box optimization problems, we aim to maximize an unknown objective function, where the function is only accessible through feedbacks of an evaluation or simulation oracle. In real-life, the feedbacks of such oracles are often noisy and available after some unknown delay that may depend on the computation time of the oracle. Additionally, if the exact evaluations are expensive but coarse approximations are available at a lower cost, the feedbacks can have multi-fidelity. In order to address this problem, we propose a generic extension of hierarchical optimistic tree search (HOO), called ProCrastinated Tree Search (PCTS), that flexibly accommodates a delay and noise-tolerant bandit algorithm. We provide a generic proof technique to quantify regret of PCTS under delayed, noisy, and multi-fidelity feedbacks. Specifically, we derive regret bounds of PCTS enabled with delayed-UCB1 (DUCB1) and delayed-UCB-V (DUCBV) algorithms. Given a horizon T, PCTS retains the regret bound of non-delayed HOO for expected delay of O(log T), and worsens by T^((1-\u03b1)/(d+2)) for expected delays of O(T^(1-\u03b1)) for \u03b1 \u2208 (0,1]. We experimentally validate on multiple synthetic functions and hyperparameter tuning problems that PCTS outperforms the state-of-the-art black-box optimization methods for feedbacks with different noise levels, delays, and fidelity.",
    "code_link": "https://github.com/jxiw/PCTS"
  },
  "aaai2022_main_dpcddiscreteprincipalcoordinatedescentforbinaryvariableproblems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DPCD: Discrete Principal Coordinate Descent for Binary Variable Problems",
    "authors": [
      "Huan Xiong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21281",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21281/21030",
    "published": "2022-02",
    "summary": "Binary optimization, a representative subclass of discrete optimization, plays an important role in mathematical optimization and has various applications in computer vision and machine learning. Generally speaking, binary optimization problems are NP-hard and difficult to solve due to the binary constraints, especially when the number of variables is very large. Existing methods often suffer from high computational costs or large accumulated quantization errors, or are only designed for specific tasks. In this paper, we propose an efficient algorithm, named Discrete Principal Coordinate Descent (DPCD), to find effective approximate solutions for general binary optimization problems. The proposed algorithm iteratively solves optimization problems related to the linear approximation of loss functions, which leads to updating the binary variables that most impact the value of the loss functions at each step. Our method supports a wide range of empirical objective functions with/without restrictions on the numbers of 1s and -1s in the binary variables. Furthermore, the theoretical convergence of our algorithm is proven, and the explicit convergence rates are derived for objective functions with Lipschitz continuous gradients, which are commonly adopted in practice. Extensive experiments on binary hashing tasks and large-scale datasets demonstrate the superiority of the proposed algorithm over several state-of-the-art methods in terms of both effectiveness and efficiency.",
    "code_link": ""
  },
  "aaai2022_main_optimizewhatyouevaluatewithsearchresultdiversificationbasedonmetricoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Optimize What You Evaluate With: Search Result Diversification Based on Metric Optimization",
    "authors": [
      "Hai-Tao Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21282",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21282/21031",
    "published": "2022-02",
    "summary": "Most of the existing methods for search result diversification (SRD) appeal to the greedy strategy for generating diversified results, which is formulated as a sequential process of selecting documents one-by-one, and the locally optimal choice is made at each round. Unfortunately, this strategy suffers from the following shortcomings: (1) Such a one-by-one selection process is rather time-consuming for both training and inference. (2) It works well on the premise that the preceding choices are optimal or close to the optimal solution. (3) The mismatch between the objective function used in training and the final evaluation measure used in testing has not been taken into account. We propose a novel framework through direct metric optimization for SRD (referred to as MO4SRD) based on the score-and-sort strategy. Specifically, we represent the diversity score of each document that determines its rank position based on a probability distribution. These distributions over scores naturally give rise to expectations over rank positions. Armed with this advantage, we can get the differentiable variants of the widely used diversity metrics. Thanks to this, we are able to directly optimize the evaluation measure used in testing. Moreover, we have devised a novel probabilistic neural scoring function. It jointly scores candidate documents by taking into account both cross-document interaction and permutation equivariance, which makes it possible to generate a diversified ranking via a simple sorting. The experimental results on benchmark collections show that the proposed method achieves significantly improved performance over the state-of-the-art results.",
    "code_link": ""
  },
  "aaai2022_main_afirstmathematicalruntimeanalysisofthenon-dominatedsortinggeneticalgorithmii(nsga-ii)": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A First Mathematical Runtime Analysis of the Non-dominated Sorting Genetic Algorithm II (NSGA-II)",
    "authors": [
      "Weijie Zheng",
      "Yufei Liu",
      "Benjamin Doerr"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21283",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21283/21032",
    "published": "2022-02",
    "summary": "The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size larger than the Pareto front size by a constant factor, the NSGA-II with two classic mutation operators and three different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LOTZ benchmark functions. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front (for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front). Our experiments confirm the above findings.",
    "code_link": ""
  },
  "aaai2022_main_pinpointingfine-grainedrelationshipsbetweenhatefultweetsandreplies": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pinpointing Fine-Grained Relationships between Hateful Tweets and Replies",
    "authors": [
      "Abdullah Albanyan",
      "Eduardo Blanco"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21284",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21284/21033",
    "published": "2022-02",
    "summary": "Recent studies in the hate and counter hate domain have provided the grounds for investigating how to detect this pervasive content in social media. These studies mostly work with synthetic replies to hateful content written by annotators on demand rather than replies written by real users. We argue that working with naturally occurring replies to hateful content is key to study the problem. Building on this motivation, we create a corpus of 5,652 hateful tweets and replies. We analyze their fine-grained relationships by indicating whether the reply (a) is hate or counter hate speech, (b) provides a justification, (c) attacks the author of the tweet, and (d) adds additional hate. We also present linguistic insights into the language people use depending on these fine-grained relationships. Experimental results show improvements (a) taking into account the hateful tweet in addition to the reply and (b) pretraining with related tasks.",
    "code_link": ""
  },
  "aaai2022_main_cross-modalcoherencefortext-to-imageretrieval": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Cross-Modal Coherence for Text-to-Image Retrieval",
    "authors": [
      "Malihe Alikhani",
      "Fangda Han",
      "Hareesh Ravi",
      "Mubbasir Kapadia",
      "Vladimir\n      Pavlovic",
      "Matthew Stone"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21285",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21285/21034",
    "published": "2022-02",
    "summary": "Common image-text joint understanding techniques presume that images and the associated text can universally be characterized by a single implicit model. However, co-occurring images and text can be related in qualitatively different ways, and explicitly modeling it could improve the performance of current joint understanding models. In this paper, we train a Cross-Modal Coherence Model for text-to-image retrieval task. Our analysis shows that models trained with image\u2013text coherence relations can retrieve images originally paired with target text more often than coherence-agnostic models. We also show via human evaluation that images retrieved by the proposed coherence-aware model are preferred over a coherence-agnostic baseline by a huge margin. Our findings provide insights into the ways that different modalities communicate and the role of coherence relations in capturing commonsense inferences in text and imagery.",
    "code_link": "https://github.com/klory/Cross-ModalCoherence-for-Text-to-Image-Retrieval"
  },
  "aaai2022_main_enhancedstorycomprehensionforlargelanguagemodelsthroughdynamicdocument-basedknowledgegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs",
    "authors": [
      "Berkeley R Andrus",
      "Yeganeh Nasiri",
      "Shilong Cui",
      "Benjamin Cullen",
      "Nancy\n      Fulda"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21286",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21286/21035",
    "published": "2022-02",
    "summary": "Large transformer-based language models have achieved incredible success at various tasks which require narrative comprehension, including story completion, answering questions about stories, and generating stories ex nihilo. However, due to the limitations of finite context windows, these language models struggle to produce or understand stories longer than several thousand tokens. In order to mitigate the document length limitations that come with finite context windows, we introduce a novel architecture that augments story processing with an external dynamic knowledge graph. In contrast to static commonsense knowledge graphs which hold information about the real world, these dynamic knowledge graphs reflect facts extracted from the story being processed. Our architecture uses these knowledge graphs to create information-rich prompts which better facilitate story comprehension than prompts composed only of story text. We apply our architecture to the tasks of question answering and story completion. To complement this line of research, we introduce two long-form question answering tasks, LF-SQuAD and LF-QUOREF, in which the document length exceeds the size of the language model's context window, and introduce a story completion evaluation method that bypasses the stochastic nature of language model generation. We demonstrate broad improvement over typical prompt formulation methods for both question answering and story completion using GPT-2, GPT-3 and XLNet.",
    "code_link": ""
  },
  "aaai2022_main_diagnostics-guidedexplanationgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Diagnostics-Guided Explanation Generation",
    "authors": [
      "Pepa Atanasova",
      "Jakob Grue Simonsen",
      "Christina Lioma",
      "Isabelle Augenstein"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21287",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21287/21036",
    "published": "2022-02",
    "summary": "Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",
    "code_link": "https://github.com/copenlu/diagnostic-guidedexplanations"
  },
  "aaai2022_main_mitigatingreportingbiasinsemi-supervisedtemporalcommonsenseinferencewithprobabilisticsoftlogic": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic",
    "authors": [
      "Bibo Cai",
      "Xiao Ding",
      "Bowen Chen",
      "Li Du",
      "Ting Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21288",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21288/21037",
    "published": "2022-02",
    "summary": "Acquiring high-quality temporal common sense (TCS) knowledge from free-form text is a crucial but challenging problem for event-centric natural language understanding, due to the language reporting bias problem: people rarely report the commonly observed events but highlight the special cases. For example, one may rarely report \"I get up from bed in 1 minute\", but we can observe \"It takes me an hour to get up from bed every morning'' in text. Models directly trained upon such corpus would capture distorted TCS knowledge, which could influence the model performance. Prior work addresses this issue mainly by exploiting the interactions among temporal dimensions (e.g., duration, temporal relation between events) in a multi-task view. However, this line of work suffers the limitation of implicit, inadequate and unexplainable interactions modeling. In this paper, we propose a novel neural-logic based Soft Logic Enhanced Event Temporal Reasoning (SLEER) model for acquiring unbiased TCS knowledge, in which the complementary relationship among dimensions are explicitly represented as logic rules and modeled by t-norm fuzzy logics. SLEER can utilize logic rules to regularize its inference process. Experimental results on four intrinsic evaluation datasets and two extrinsic datasets show the efficiency of our proposed method.",
    "code_link": "https://github.com/bibocai/SLEER"
  },
  "aaai2022_main_adversarialtrainingforimprovingmodelrobustness?lookatbothpredictionandinterpretation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",
    "authors": [
      "Hanjie Chen",
      "Yangfeng Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21289",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21289/21038",
    "published": "2022-02",
    "summary": "Neural language models show vulnerability to adversarial examples which are semantically similar to their original counterparts with a few words replaced by their synonyms. A common way to improve model robustness is adversarial training which follows two steps\u2014collecting adversarial examples by attacking a target model, and fine-tuning the model on the augmented dataset with these adversarial examples. The objective of traditional adversarial training is to make a model produce the same correct predictions on an original/adversarial example pair. However, the consistency between model decision-makings on two similar texts is ignored. We argue that a robust model should behave consistently on original/adversarial example pairs, that is making the same predictions (what) based on the same reasons (how) which can be reflected by consistent interpretations. In this work, we propose a novel feature-level adversarial training method named FLAT. FLAT aims at improving model robustness in terms of both predictions and interpretations. FLAT incorporates variational word masks in neural networks to learn global word importance and play as a bottleneck teaching the model to make predictions based on important words. FLAT explicitly shoots at the vulnerability problem caused by the mismatch between model understandings on the replaced words and their synonyms in original/adversarial example pairs by regularizing the corresponding global word importance scores. Experiments show the effectiveness of FLAT in improving the robustness with respect to both predictions and interpretations of four neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks on four text classification tasks. The models trained via FLAT also show better robustness than baseline models on unforeseen adversarial examples across different attacks.",
    "code_link": "https://github.com/UVaNLP/FLAT"
  },
  "aaai2022_main_unsupervisededitingforcounterfactualstories": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Editing for Counterfactual Stories",
    "authors": [
      "Jiangjie Chen",
      "Chun Gan",
      "Sijie Cheng",
      "Hao Zhou",
      "Yanghua Xiao",
      "Lei Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21290",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21290/21039",
    "published": "2022-02",
    "summary": "Creating what-if stories requires reasoning about prior statements and possible outcomes of the changed conditions. One can easily generate coherent endings under new conditions, but it would be challenging for current systems to do it with minimal changes to the original story. Therefore, one major challenge is the trade-off between generating a logical story and rewriting with minimal-edits. In this paper, we propose EDUCAT, an editing-based unsupervised approach for counterfactual story rewriting. EDUCAT includes a target position detection strategy based on estimating causal effects of the what-if conditions, which keeps the causal invariant parts of the story. EDUCAT then generates the stories under fluency, coherence and minimal-edits constraints. We also propose a new metric to alleviate the shortcomings of current automatic metrics and better evaluate the trade-off. We evaluate EDUCAT on a public counterfactual story rewriting benchmark. Experiments show that EDUCAT achieves the best trade-off over unsupervised SOTA methods according to both automatic and human evaluation. The resources of EDUCAT are available at: https://github.com/jiangjiechen/EDUCAT.",
    "code_link": "https://github.com/jiangjiechen/EDUCAT"
  },
  "aaai2022_main_lorenlogic-regularizedreasoningforinterpretablefactverification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification",
    "authors": [
      "Jiangjie Chen",
      "Qiaoben Bao",
      "Changzhi Sun",
      "Xinbo Zhang",
      "Jiaze Chen",
      "Hao\n      Zhou",
      "Yanghua Xiao",
      "Lei Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21291",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21291/21040",
    "published": "2022-02",
    "summary": "Given a natural language statement, how to verify its veracity against a large-scale textual knowledge source like Wikipedia? Most existing neural models make predictions without giving clues about which part of a false claim goes wrong. In this paper, we propose LOREN, an approach for interpretable fact verification. We decompose the verification of the whole claim at phrase-level, where the veracity of the phrases serves as explanations and can be aggregated into the final verdict according to logical rules. The key insight of LOREN is to represent claim phrase veracity as three-valued latent variables, which are regularized by aggregation logical rules. The final claim verification is based on all latent variables. Thus, LOREN enjoys the additional benefit of interpretability --- it is easy to explain how it reaches certain results with claim phrase veracity. Experiments on a public fact verification benchmark show that LOREN is competitive against previous approaches while enjoying the merit of faithful and accurate interpretability. The resources of LOREN are available at: https://github.com/jiangjiechen/LOREN.",
    "code_link": "https://github.com/jiangjiechen/LOREN"
  },
  "aaai2022_main_contrastnetacontrastivelearningframeworkforfew-shottextclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification",
    "authors": [
      "Junfan Chen",
      "Richong Zhang",
      "Yongyi Mao",
      "Jie Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21292",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21292/21041",
    "published": "2022-02",
    "summary": "Few-shot text classification has recently been promoted by the meta-learning paradigm which aims to identify target classes with knowledge transferred from source classes with sets of small tasks named episodes. Despite their success, existing works building their meta-learner based on Prototypical Networks are unsatisfactory in learning discriminative text representations between similar classes, which may lead to contradictions during label prediction. In addition, the task-level and instance-level overfitting problems in few-shot text classification caused by a few training examples are not sufficiently tackled. In this work, we propose a contrastive learning framework named ContrastNet to tackle both discriminative representation and overfitting problems in few-shot text classification. ContrastNet learns to pull closer text representations belonging to the same class and push away text representations belonging to different classes, while simultaneously introducing unsupervised contrastive regularization at both task-level and instance-level to prevent overfitting. Experiments on 8 few-shot text classification datasets show that ContrastNet outperforms the current state-of-the-art models.",
    "code_link": ""
  },
  "aaai2022_main_fromgoodtobesttwo-stagetrainingforcross-lingualmachinereadingcomprehension": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "From Good to Best: Two-Stage Training for Cross-Lingual Machine Reading Comprehension",
    "authors": [
      "Nuo Chen",
      "Linjun Shou",
      "Ming Gong",
      "Jian Pei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21293",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21293/21042",
    "published": "2022-02",
    "summary": "Cross-lingual Machine Reading Comprehension (xMRC) is a challenging task due to the lack of training data in low-resource languages. Recent approaches use training data only in a resource-rich language (such as English) to fine-tune large-scale cross-lingual pre-trained language models, which transfer knowledge from resource-rich languages (source) to low-resource languages (target). Due to the big difference between languages, the model fine-tuned only by the source language may not perform well for target languages. In our study, we make an interesting observation that while the top 1 result predicted by the previous approaches may often fail to hit the ground-truth answer, there are still good chances for the correct answer to be contained in the set of top k predicted results. Intuitively, the previous approaches have empowered the model certain level of capability to roughly distinguish good answers from bad ones. However, without sufficient training data, it is not powerful enough to capture the nuances between the accurate answer and those approximate ones. Based on this observation, we develop a two-stage approach to enhance the model performance. The first stage targets at recall; we design a hard-learning (HL) algorithm to maximize the likelihood that the top k predictions contain the accurate answer. The second stage focuses on precision, where an answer-aware contrastive learning (AA-CL) mechanism is developed to learn the minute difference between the accurate answer and other candidates. Extensive experiments show that our model significantly outperforms strong baselines on two cross-lingual MRC benchmark datasets.",
    "code_link": "https://github.com/huggingface/transformers"
  },
  "aaai2022_main_probinglinguisticinformationforlogicalinferenceinpre-trainedlanguagemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Probing Linguistic Information for Logical Inference in Pre-trained Language Models",
    "authors": [
      "Zeming Chen",
      "Qiyue Gao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21294",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21294/21043",
    "published": "2022-02",
    "summary": "Progress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.",
    "code_link": ""
  },
  "aaai2022_main_onthetransferabilityofpre-trainedlanguagemodelsastudyfromartificialdatasets": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets",
    "authors": [
      "Cheng-Han Chiang",
      "Hung-yi Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21295",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21295/21044",
    "published": "2022-02",
    "summary": "Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what specific traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks.We propose to use artificially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have.By fine-tuning the pre-trained models on GLUE benchmark, we can learn how beneficial it is to transfer the knowledge from the model trained on the dataset possessing that specific trait.We define and discuss three different characteristics in the artificial dataset: 1) matching the token's uni-gram or bi-gram distribution between pre-training and downstream fine-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance.Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies.Based on our analysis, we find that models pre-trained with artificial datasets are prone to learn spurious correlation in downstream tasks.Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.",
    "code_link": ""
  },
  "aaai2022_main_c2lcausallycontrastivelearningforrobusttextclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "C2L: Causally Contrastive Learning for Robust Text Classification",
    "authors": [
      "Seungtaek Choi",
      "Myeongho Jeong",
      "Hojae Han",
      "Seung-won Hwang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21296",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21296/21045",
    "published": "2022-02",
    "summary": "Despite the super-human accuracy of recent deep models in NLP tasks, their robustness is reportedly limited due to their reliance on spurious patterns. We thus aim to leverage contrastive learning and counterfactual augmentation for robustness. For augmentation, existing work either requires humans to add counterfactuals to the dataset or machines to automatically matches near-counterfactuals already in the dataset. Unlike existing augmentation is affected by spurious correlations, ours, by synthesizing \u201ca set\u201d of counterfactuals, and making a collective decision on the distribution of predictions on this set, can robustly supervise the causality of each term. Our empirical results show that our approach, by collective decisions, is less sensitive to task model bias of attribution-based synthesis, and thus achieves significant improvements, in diverse dimensions: 1) counterfactual robustness, 2) cross-domain generalization, and 3) generalization from scarce data.",
    "code_link": ""
  },
  "aaai2022_main_noveltycontrolledparaphrasegenerationwithretrievalaugmentedconditionalprompttuning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning",
    "authors": [
      "Jishnu Ray Chowdhury",
      "Yong Zhuang",
      "Shuyi Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21297",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21297/21046",
    "published": "2022-02",
    "summary": "Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.",
    "code_link": ""
  },
  "aaai2022_main_flexibleinstance-specificrationalizationofnlpmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Flexible Instance-Specific Rationalization of NLP Models",
    "authors": [
      "George Chrysostomou",
      "Nikolaos Aletras"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21298",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21298/21047",
    "published": "2022-02",
    "summary": "Recent research on model interpretability in natural language processing extensively uses feature scoring methods for identifying which parts of the input are the most important for a model to make a prediction (i.e. explanation or rationale). However, previous research has shown that there is no clear best scoring method across various text classification tasks while practitioners typically have to make several other ad-hoc choices regarding the length and the type of the rationale (e.g. short or long, contiguous or not). Inspired by this, we propose a simple yet effective and flexible method that allows selecting optimally for each data instance: (1) a feature scoring method; (2) the length; and (3) the type of the rationale. Our method is inspired by input erasure approaches to interpretability which assume that the most faithful rationale for a prediction should be the one with the highest difference between the model's output distribution using the full text and the text after removing the rationale as input respectively. Evaluation on four standard text classification datasets shows that our proposed method provides more faithful, comprehensive and highly sufficient explanations compared to using a fixed feature scoring method, rationale length and type. More importantly, we demonstrate that a practitioner is not required to make any ad-hoc choices in order to extract faithful rationales using our approach.",
    "code_link": ""
  },
  "aaai2022_main_infolmanewmetrictoevaluatesummarization&data2textgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation",
    "authors": [
      "Pierre Jean A. Colombo",
      "Chlo\u00e9 Clavel",
      "Pablo Piantanida"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21299",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21299/21048",
    "published": "2022-02",
    "summary": "Assessing the quality of natural language generation (NLG) systems through human annotation is very expensive. Additionally, human annotation campaigns are time-consuming and include non-reusable human labour. In practice, researchers rely on automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU or ROUGE) have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms. In this paper, we introduce InfoLM a family of untrained metrics that can be viewed as a string-based metric that addresses the aforementioned flaws thanks to a pre-trained masked language model. This family of metrics also makes use of information measures allowing the possibility to adapt InfoLM to different evaluation criteria. Using direct assessment, we demonstrate that InfoLM achieves statistically significant improvement and two figure correlation gains in many configurations compared to existing metrics on both summarization and data2text generation tasks.",
    "code_link": ""
  },
  "aaai2022_main_niceperfume.howlongdidyoumarinateinit?multimodalsarcasmexplanation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Nice Perfume. How Long Did You Marinate in It? Multimodal Sarcasm Explanation",
    "authors": [
      "Poorav Desai",
      "Tanmoy Chakraborty",
      "Md Shad Akhtar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21300",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21300/21049",
    "published": "2022-02",
    "summary": "Sarcasm is a pervading linguistic phenomenon and highly challenging to explain due to its subjectivity, lack of context and deeply-felt opinion. In the multimodal setup, sarcasm is conveyed through the incongruity between the text and visual entities. Although recent approaches deal with sarcasm as a classification problem, it is unclear why an online post is identified as sarcastic. Without proper explanation, end users may not be able to perceive the underlying sense of irony. In this paper, we propose a novel problem -- Multimodal Sarcasm Explanation (MuSE) -- given a multimodal sarcastic post containing an image and a caption, we aim to generate a natural language explanation to reveal the intended sarcasm. To this end, we develop MORE, a new dataset with explanation of 3510 sarcastic multimodal posts. Each explanation is a natural language (English) sentence describing the hidden irony. We benchmark MORE by employing a multimodal Transformer-based architecture. It incorporates a cross-modal attention in the Transformer's encoder which attends to the distinguishing features between the two modalities. Subsequently, a BART-based auto-regressive decoder is used as the generator. Empirical results demonstrate convincing results over various baselines (adopted for MuSE) across five evaluation metrics. We also conduct human evaluation on predictions and obtain Fleiss' Kappa score of 0.4 as a fair agreement among 25 evaluators.",
    "code_link": ""
  },
  "aaai2022_main_zero-shotcommonsensequestionansweringwithclozetranslationandconsistencyoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Zero-Shot Commonsense Question Answering with Cloze Translation and Consistency Optimization",
    "authors": [
      "Zi-Yi Dou",
      "Nanyun Peng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21301",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21301/21050",
    "published": "2022-02",
    "summary": "Commonsense question answering (CQA) aims to test if models can answer questions regarding commonsense knowledge that everyone knows. Prior works that incorporate external knowledge bases have shown promising results, but knowledge bases are expensive to construct and are often limited to a fixed set of relations. In this paper, we instead focus on better utilizing the implicit knowledge stored in pre-trained language models. While researchers have found that the knowledge embedded in pre-trained language models can be extracted by having them fill in the blanks of carefully designed prompts for relation extraction and text classification, it remains unclear if we can adopt this paradigm in CQA where the inputs and outputs take much more flexible forms. To this end, we investigate four translation methods that can translate natural questions into cloze-style sentences to better solicit commonsense knowledge from language models, including a syntactic-based model, an unsupervised neural model, and two supervised neural models. In addition, to combine the different translation methods, we propose to encourage consistency among model predictions on different translated questions with unlabeled data. We demonstrate the effectiveness of our methods on three CQA datasets in zero-shot settings. We show that our methods are complementary to a knowledge base improved model, and combining them can lead to state-of-the-art zero-shot performance. Analyses also reveal distinct characteristics of the different cloze translation methods and provide insights on why combining them can lead to great improvements. Code/dataset is available at https://github.com/PlusLabNLP/zero_shot_cqa.",
    "code_link": ""
  },
  "aaai2022_main_syntheticdisinformationattacksonautomatedfactverificationsystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Synthetic Disinformation Attacks on Automated Fact Verification Systems",
    "authors": [
      "Yibing Du",
      "Antoine Bosselut",
      "Christopher D. Manning"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21302",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21302/21051",
    "published": "2022-02",
    "summary": "Automated fact-checking is a needed technology to curtail the spread of online misinformation. One current framework for such solutions proposes to verify claims by retrieving supporting or refuting evidence from related textual sources. However, the realistic use cases for fact-checkers will require verifying claims against evidence sources that could be affected by the same misinformation. Furthermore, the development of modern NLP tools that can produce coherent, fabricated content would allow malicious actors to systematically generate adversarial disinformation for fact-checkers.In this work, we explore the sensitivity of automated fact-checkers to synthetic adversarial evidence in two simulated settings: ADVERSARIAL ADDITION, where we fabricate documents and add them to the evidence repository available to the fact-checking system, and ADVERSARIAL MODIFICATION, where existing evidence source documents in the repository are automatically altered. Our study across multiple models on three benchmarks demonstrates that these systems suffer significant performance drops against these attacks. Finally, we discuss the growing threat of modern NLG systems as generators of disinformation in the context of the challenges they pose to automated fact-checkers.",
    "code_link": "https://github.com/Yibing-Du/adversarial-factcheck"
  },
  "aaai2022_main_regularizingend-to-endspeechtranslationwithtriangulardecompositionagreement": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement",
    "authors": [
      "Yichao Du",
      "Zhirui Zhang",
      "Weizhi Wang",
      "Boxing Chen",
      "Jun Xie",
      "Tong Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21303",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21303/21052",
    "published": "2022-02",
    "summary": "End-to-end speech-to-text translation (E2E-ST) is becoming increasingly popular due to the potential of its less error propagation, lower latency, and fewer parameters. Given the triplet training corpus\u3008speech, transcription, translation\u3009, the conventional high-quality E2E-ST system leverages the\u3008speech, transcription\u3009pair to pre-train the model and then utilizes the\u3008speech, translation\u3009pair to optimize it further. However, this process only involves two-tuple data at each stage, and this loose coupling fails to fully exploit the association between triplet data. In this paper, we attempt to model the joint probability of transcription and translation based on the speech input to directly leverage such triplet data. Based on that, we propose a novel regularization method for model training to improve the agreement of dual-path decomposition within triplet data, which should be equal in theory. To achieve this goal, we introduce two Kullback-Leibler divergence regularization terms into the model training objective to reduce the mismatch between output probabilities of dual-path. Then the well-trained model can be naturally transformed as the E2E-ST models by a pre-defined early stop tag. Experiments on the MuST-C benchmark demonstrate that our proposed approach significantly outperforms state-of-the-art E2E-ST baselines on all 8 language pairs while achieving better performance in the automatic speech recognition task.",
    "code_link": ""
  },
  "aaai2022_main_playtheshannongamewithlanguagemodelsahuman-freeapproachtosummaryevaluation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Play the Shannon Game with Language Models: A Human-Free Approach to Summary Evaluation",
    "authors": [
      "Nicholas Egan",
      "Oleg Vasilyev",
      "John Bohannon"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21304",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21304/21053",
    "published": "2022-02",
    "summary": "The goal of a summary is to concisely state the most important information in a document. With this principle in mind, we introduce new reference-free summary evaluation metrics that use a pretrained language model to estimate the information content shared between a document and its summary. These metrics are a modern take on the Shannon Game, a method for summary quality scoring proposed decades ago, where we replace human annotators with language models. We also view these metrics as an extension of BLANC, a recently proposed approach to summary quality measurement based on the performance of a language model with and without the help of a summary. Using transformer based language models, we empirically verify that our metrics achieve state-of-the-art correlation with human judgement of the summary quality dimensions of both coherence and relevance, as well as competitive correlation with human judgement of consistency and fluency.",
    "code_link": ""
  },
  "aaai2022_main_fortunately,discoursemarkerscanenhancelanguagemodelsforsentimentanalysis": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis",
    "authors": [
      "Liat Ein-Dor",
      "Ilya Shnayderman",
      "Artem Spector",
      "Lena Dankin",
      "Ranit\n      Aharonov",
      "Noam Slonim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21305",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21305/21054",
    "published": "2022-02",
    "summary": "In recent years, pretrained language models have revolutionized the NLP world, while achieving state of the art performance in various downstream tasks. However, in many cases, these models do not perform well when labeled data is scarce and the model is expected to perform in the zero or few shot setting. Recently, several works have shown that continual pretraining or performing a second phase of pretraining (inter-training) which is better aligned with the downstream task, can lead to improved results, especially in the scarce data setting. Here, we propose to leverage sentiment-carrying discourse markers to generate large-scale weakly-labeled data, which in turn can be used to adapt language models for sentiment analysis. Extensive experimental results show the value of our approach on various benchmark datasets, including the finance domain. Code, models and data are available at https://github.com/ibm/tslm-discourse-markers.",
    "code_link": "https://github.com/ibm/tslm-discourse-markers"
  },
  "aaai2022_main_retrieve,caption,generatevisualgroundingforenhancingcommonsenseintextgenerationmodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models",
    "authors": [
      "Steven Y. Feng",
      "Kevin Lu",
      "Zhuofu Tao",
      "Malihe Alikhani",
      "Teruko Mitamura",
      "Eduard Hovy",
      "Varun Gangal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21306",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21306/21055",
    "published": "2022-02",
    "summary": "We investigate the use of multimodal information contained in images as an effective method for enhancing the commonsense of Transformer models for text generation. We perform experiments using BART and T5 on concept-to-text generation, specifically the task of generative commonsense reasoning, or CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text Generation. VisCTG involves captioning images representing appropriate everyday scenarios, and using these captions to enrich and steer the generation process. Comprehensive evaluation and analysis demonstrate that VisCTG noticeably improves model performance while successfully addressing several issues of the baseline generations, including poor commonsense, fluency, and specificity.",
    "code_link": ""
  },
  "aaai2022_main_languagemodelprimingforcross-lingualeventextraction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Language Model Priming for Cross-Lingual Event Extraction",
    "authors": [
      "Steven Fincke",
      "Shantanu Agarwal",
      "Scott Miller",
      "Elizabeth Boschee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21307",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21307/21056",
    "published": "2022-02",
    "summary": "We present a novel, language-agnostic approach to \"priming\" language models for the task of event extraction, providing particularly effective performance in low-resource and zero-shot cross-lingual settings. With priming, we augment the input to the transformer stack's language model differently depending on the question(s) being asked of the model at runtime. For instance, if the model is being asked to identify arguments for the trigger \"protested\", we will provide that trigger as part of the input to the language model, allowing it to produce different representations for candidate arguments than when it is asked about arguments for the trigger \"arrest\" elsewhere in the same sentence. We show that by enabling the language model to better compensate for the deficits of sparse and noisy training data, our approach improves both trigger and argument detection and classification significantly over the state of the art in a zero-shot cross-lingual setting.",
    "code_link": ""
  },
  "aaai2022_main_languagemodellingvialearningtorank": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Language Modelling via Learning to Rank",
    "authors": [
      "Arvid Frydenlund",
      "Gagandeep Singh",
      "Frank Rudzicz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21308",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21308/21057",
    "published": "2022-02",
    "summary": "We consider language modelling (LM) as a multi-label structured prediction task by re-framing training from solely predicting a single ground-truth word to ranking a set of words which could continue a given context. To avoid annotating top-k ranks, we generate them using pre-trained LMs: GPT-2, BERT, and Born-Again models. This leads to a rank-based form of knowledge distillation (KD). We also develop a method using N-grams to create a non-probabilistic teacher which generates the ranks without the need of a pre-trained LM.We confirm the hypotheses: that we can treat LMing as a ranking task and that we can do so without the use of a pre-trained LM.We show that rank-based KD generally gives a modest improvement to perplexity (PPL) -- though often with statistical significance -- when compared to Kullback\u2013Leibler-based KD. Surprisingly, given the naivety of the method, the N-grams act as competitive teachers and achieve similar performance as using either BERT or a Born-Again model teachers. Unsurprisingly, GPT-2 always acts as the best teacher. Using it and a Transformer-XL student on Wiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and against a KL-based KD of 56.70.",
    "code_link": ""
  },
  "aaai2022_main_nareorthenarrativereorderingproblem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NAREOR: The Narrative Reordering Problem",
    "authors": [
      "Varun Gangal",
      "Steven Y. Feng",
      "Malihe Alikhani",
      "Teruko Mitamura",
      "Eduard\n      Hovy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21309",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21309/21058",
    "published": "2022-02",
    "summary": "Many implicit inferences exist in text depending on how it is structured that can critically impact the text's interpretation and meaning. One such structural aspect present in text with chronology is the order of its presentation. For narratives or stories, this is known as the narrative order. Reordering a narrative can impact the temporal, causal, event-based, and other inferences readers draw from it, which in turn can have strong effects both on its interpretation and interestingness. In this paper, we propose and investigate the task of Narrative Reordering (NAREOR) which involves rewriting a given story in a different narrative order while preserving its plot. We present a dataset, NAREORC, with human rewritings of stories within ROCStories in non-linear orders, and conduct a detailed analysis of it. Further, we propose novel task-specific training methods with suitable evaluation metrics. We perform experiments on NAREORC using state-of-the-art models such as BART and T5 and conduct extensive automatic and human evaluations. We demonstrate that although our models can perform decently, NAREOR is a challenging task with potential for further exploration. We also investigate two applications of NAREOR: generation of more interesting variations of stories and serving as adversarial sets for temporal/event-related tasks, besides discussing other prospective ones, such as for pedagogical setups related to language skills like essay writing and applications to medicine involving clinical narratives.",
    "code_link": "https://github.com/vgtomahawk/NAREORCamReady"
  },
  "aaai2022_main_unisonunpairedcross-lingualimagecaptioning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "UNISON: Unpaired Cross-Lingual Image Captioning",
    "authors": [
      "Jiahui Gao",
      "Yi Zhou",
      "Philip L. H. Yu",
      "Shafiq Joty",
      "Jiuxiang Gu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21310",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21310/21059",
    "published": "2022-02",
    "summary": "Image captioning has emerged as an interesting research field in recent years due to its broad application scenarios. The traditional paradigm of image captioning relies on paired image-caption datasets to train the model in a supervised manner. However, creating such paired datasets for every target language is prohibitively expensive, which hinders the extensibility of captioning technology and deprives a large part of the world population of its benefit. In this work, we present a novel unpaired cross-lingual method to generate image captions without relying on any caption corpus in the source or the target language. Specifically, our method consists of two phases: (1) a cross-lingual auto-encoding process, which utilizing a sentence parallel (bitext) corpus to learn the mapping from the source to the target language in the scene graph encoding space and decode sentences in the target language, and (2) a cross-modal unsupervised feature mapping, which seeks to map the encoded scene graph features from image modality to language modality. We verify the effectiveness of our proposed method on the Chinese image caption generation task. The comparisons against several existing methods demonstrate the effectiveness of our approach.",
    "code_link": "https://github.com/fxsjy/jieba"
  },
  "aaai2022_main_autobert-zeroevolvingbertbackbonefromscratch": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch",
    "authors": [
      "Jiahui Gao",
      "Hang Xu",
      "Han Shi",
      "Xiaozhe Ren",
      "Philip L. H. Yu",
      "Xiaodan Liang",
      "Xin Jiang",
      "Zhenguo Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21311",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21311/21060",
    "published": "2022-02",
    "summary": "Transformer-based pre-trained language models like BERT and its variants have recently achieved promising performance in various natural language processing (NLP) tasks. However, the conventional paradigm constructs the backbone by purely stacking the manually designed global self-attention layers, introducing inductive bias and thus leads to sub-optimal. In this work, we make the first attempt to automatically discover novel pre-trained language model (PLM) backbone on a flexible search space containing the most fundamental operations from scratch. Specifically, we propose a well-designed search space which (i) contains primitive math operations in the intra-layer level to explore novel attention structures, and (ii) leverages convolution blocks to be the supplementary for attentions in the inter-layer level to better learn local dependency. To enhance the efficiency for finding promising architectures, we propose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm, which optimizes both the search algorithm and evaluation of candidate models. Specifically, we propose Operation-Priority (OP) evolution strategy to facilitate model search via balancing exploration and exploitation. Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for fast model evaluation. Extensive experiments show that the searched architecture (named AutoBERT-Zero) significantly outperforms BERT and its variants of different model capacities in various downstream tasks, proving the architecture's transfer and scaling abilities. Remarkably, AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE test set.",
    "code_link": ""
  },
  "aaai2022_main_iseeqinformationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ISEEQ: Information Seeking Question Generation Using Dynamic Meta-Information Retrieval and Knowledge Graphs",
    "authors": [
      "Manas Gaur",
      "Kalpa Gunaratna",
      "Vijay Srinivasan",
      "Hongxia Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21312",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21312/21061",
    "published": "2022-02",
    "summary": "Conversational Information Seeking (CIS) is a relatively new research area within conversational AI that attempts to seek information from end-users in order to understand and satisfy the users' needs. If realized, such a CIS system has far-reaching benefits in the real world; for example, CIS systems can assist clinicians in pre-screening or triaging patients in healthcare. A key open sub-problem in CIS that remains unaddressed in the literature is generating Information Seeking Questions (ISQs) based on a short initial query from the end-user. To address this open problem, we propose Information SEEking Question generator (ISEEQ), a novel approach for generating ISQs from just a short user query, given a large text corpus relevant to the user query. Firstly, ISEEQ uses a knowledge graph to enrich the user query. Secondly, ISEEQ uses the knowledge-enriched query to retrieve relevant context passages to ask coherent ISQs adhering to a conceptual flow. Thirdly, ISEEQ introduces a new deep generative-adversarial reinforcement learning-based approach for generating ISQs. We show that ISEEQ can generate high-quality ISQs to promote the development of CIS agents. ISEEQ significantly outperforms comparable baselines on five ISQ evaluation metrics across four datasets having user queries from diverse domains. Further, we argue that ISEEQ is transferable across domains for generating ISQs, as it shows the acceptable performance when trained and tested on different pairs of domains. A qualitative human evaluation confirms that ISEEQ generated ISQs are comparable in quality to human-generated questions, and it outperformed the best comparable baseline.",
    "code_link": "https://github.com/manasgaur/AAAI-22"
  },
  "aaai2022_main_explainablemetaphoridentificationinspiredbyconceptualmetaphortheory": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Explainable Metaphor Identification Inspired by Conceptual Metaphor Theory",
    "authors": [
      "Mengshi Ge",
      "Rui Mao",
      "Erik Cambria"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21313",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21313/21062",
    "published": "2022-02",
    "summary": "Metaphor is not only a linguistic phenomenon but also reflects the concept projection between source and target domains in human cognition. Previous sequence tagging-based metaphor identification methods could not model the concept projection, resulting in a limitation that the outputs of these models are unexplainable in the predictions of the metaphoricity labels. In this work, we propose the first explainable metaphor identification model, inspired by Conceptual Metaphor Theory. The model is based on statistic learning, a lexical resource, and a novel reward mechanism. Our model can identify the metaphoricity on the word-pair level, and explain the predicted metaphoricity labels via learned concept mappings. The use of the reward mechanism allows the model to learn the optimal concept mappings without knowing their true labels. Our method is also applicable for the concepts that are out of training domains by using the lexical resource. The automatically generated concept mappings demonstrate the implicit human thoughts in metaphoric expressions. Our experiments show the effectiveness of the proposed model in metaphor identification, and concept mapping tasks, respectively.",
    "code_link": ""
  },
  "aaai2022_main_confidencecalibrationforintentdetectionviahypersphericalspaceandrebalancedaccuracy-uncertaintyloss": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Confidence Calibration for Intent Detection via Hyperspherical Space and Rebalanced Accuracy-Uncertainty Loss",
    "authors": [
      "Yantao Gong",
      "Cao Liu",
      "Fan Yang",
      "Xunliang Cai",
      "Guanglu Wan",
      "Jiansong Chen",
      "Weipeng Zhang",
      "Houfeng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21314",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21314/21063",
    "published": "2022-02",
    "summary": "Data-driven methods have achieved notable performance on intent detection, which is a task to comprehend user queries. Nonetheless, they are controversial for over-confident predictions. In some scenarios, users do not only care about the accuracy but also the confidence of model. Unfortunately, mainstream neural networks are poorly calibrated, with a large gap between accuracy and confidence. To handle this problem defined as confidence calibration, we propose a model using the hyperspherical space and rebalanced accuracy-uncertainty loss. Specifically, we project the label vector onto hyperspherical space uniformly to generate a dense label representation matrix, which mitigates over-confident predictions due to overfitting sparse one-hot label matrix. Besides, we rebalance samples of different accuracy and uncertainty to better guide model training. Experiments on the open datasets verify that our model outperforms the existing calibration methods and achieves a significant improvement on the calibration metric.",
    "code_link": ""
  },
  "aaai2022_main_ssastself-supervisedaudiospectrogramtransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SSAST: Self-Supervised Audio Spectrogram Transformer",
    "authors": [
      "Yuan Gong",
      "Cheng-I Lai",
      "Yu-An Chung",
      "James Glass"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21315",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21315/21064",
    "published": "2022-02",
    "summary": "Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for the AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.",
    "code_link": "https://github.com/YuanGongND/ssast"
  },
  "aaai2022_main_block-skimefficientquestionansweringfortransformer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Block-Skim: Efficient Question Answering for Transformer",
    "authors": [
      "Yue Guan",
      "Zhengyi Li",
      "Zhouhan Lin",
      "Yuhao Zhu",
      "Jingwen Leng",
      "Minyi Guo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21316",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21316/21065",
    "published": "2022-02",
    "summary": "Transformer models have achieved promising results on natural language processing (NLP) tasks including extractive question answering (QA). Common Transformer encoders used in NLP tasks process the hidden states of all input tokens in the context paragraph throughout all layers. However, different from other tasks such as sequence classification, answering the raised question does not necessarily need all the tokens in the context paragraph. Following this motivation, we propose Block-skim, which learns to skim unnecessary context in higher hidden layers to improve and accelerate the Transformer performance. The key idea of Block-Skim is to identify the context that must be further processed and those that could be safely discarded early on during inference. Critically, we find that such information could be sufficiently derived from the self-attention weights inside the Transformer model. We further prune the hidden states corresponding to the unnecessary positions early in lower layers, achieving significant inference-time speedup. To our surprise, we observe that models pruned in this way outperform their full-size counterparts. Block-Skim improves QA models' accuracy on different datasets and achieves 3 times speedup on BERT-base model.",
    "code_link": ""
  },
  "aaai2022_main_deepclusteringoftextrepresentationsforsupervision-freeprobingofsyntax": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Clustering of Text Representations for Supervision-Free Probing of Syntax",
    "authors": [
      "Vikram Gupta",
      "Haoyue Shi",
      "Kevin Gimpel",
      "Mrinmaya Sachan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21317",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21317/21066",
    "published": "2022-02",
    "summary": "We explore deep clustering of multilingual text representations for unsupervised model interpretation and induction of syntax. As these representations are high-dimensional, out-of-the-box methods like K-means do not work well. Thus, our approach jointly transforms the representations into a lower-dimensional cluster-friendly space and clusters them. We consider two notions of syntax: Part of Speech Induction (POSI) and Constituency Labelling (CoLab) in this work. Interestingly, we find that Multilingual BERT (mBERT) contains surprising amount of syntactic knowledge of English; possibly even as much as English BERT (E-BERT). Our model can be used as a supervision-free probe which is arguably a less-biased way of probing. We find that unsupervised probes show benefits from higher layers as compared to supervised probes. We further note that our unsupervised probe utilizes E-BERT and mBERT representations differently, especially for POSI. We validate the efficacy of our probe by demonstrating its capabilities as a unsupervised syntax induction technique. Our probe works well for both syntactic formalisms by simply adapting the input representations. We report competitive performance of our probe on 45-tag English POSI, state-of-the-art performance on 12-tag POSI across 10 languages, and competitive results on CoLab. We also perform zero-shot syntax induction on resource impoverished languages and report strong results.",
    "code_link": ""
  },
  "aaai2022_main_few-shotcross-lingualstancedetectionwithsentiment-basedpre-training": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-training",
    "authors": [
      "Momchil Hardalov",
      "Arnav Arora",
      "Preslav Nakov",
      "Isabelle Augenstein"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21318",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21318/21067",
    "published": "2022-02",
    "summary": "The goal of stance detection is to determine the viewpoint expressed in a piece of text towards a target. These viewpoints or contexts are often expressed in many different languages depending on the user and the platform, which can be a local news outlet, a social media platform, a news forum, etc. Most research on stance detection, however, has been limited to working with a single language and on a few limited targets, with little work on cross-lingual stance detection. Moreover, non-English sources of labelled data are often scarce and present additional challenges. Recently, large multilingual language models have substantially improved the performance on many non-English tasks, especially such with a limited number of examples. This highlights the importance of model pre-training and its ability to learn from few examples. In this paper, we present the most comprehensive study of cross-lingual stance detection to date: we experiment with 15 diverse datasets in 12 languages from 6 language families, and with 6 low-resource evaluation settings each. For our experiments, we build on pattern-exploiting training (PET), proposing the addition of a novel label encoder to simplify the verbalisation procedure. We further propose sentiment-based generation of stance data for pre-training, which shows sizeable improvement of more than 6% F1 absolute in few-shot learning settings compared to several strong baselines.",
    "code_link": ""
  },
  "aaai2022_main_attentionbiasingandcontextaugmentationforzero-shotcontrolofencoder-decodertransformersfornaturallanguagegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Attention Biasing and Context Augmentation for Zero-Shot Control of Encoder-Decoder Transformers for Natural Language Generation",
    "authors": [
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Dilek Hakkani-T\u00fcr"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21319",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21319/21068",
    "published": "2022-02",
    "summary": "Controlling neural network-based models for natural language generation (NLG) to realize desirable attributes in the generated outputs has broad applications in numerous areas such as machine translation, document summarization, and dialog systems. Approaches that enable such control in a zero-shot manner would be of great importance as, among other reasons, they remove the need for additional annotated data and training. In this work, we propose novel approaches for controlling encoder-decoder transformer-based NLG models in zero shot. While zero-shot control has previously been observed in massive models (e.g., GPT3), our method enables such control for smaller models. This is done by applying two control knobs, attention biasing and context augmentation, to these models directly during decoding and without additional training or auxiliary models. These knobs control the generation process by directly manipulating trained NLG models (e.g., biasing cross-attention layers). We show that not only are these NLG models robust to such manipulations but also their behavior could be controlled without an impact on their generation performance.",
    "code_link": ""
  },
  "aaai2022_main_galaxyagenerativepre-trainedmodelfortask-orienteddialogwithsemi-supervisedlearningandexplicitpolicyinjection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-supervised Learning and Explicit Policy Injection",
    "authors": [
      "Wanwei He",
      "Yinpei Dai",
      "Yinhe Zheng",
      "Yuchuan Wu",
      "Zheng Cao",
      "Dermot Liu",
      "Peng Jiang",
      "Min Yang",
      "Fei Huang",
      "Luo Si",
      "Jian Sun",
      "Yongbin Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21320",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21320/21069",
    "published": "2022-02",
    "summary": "Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings. For reproducibility, we release the code and data at https://github.com/siat-nlp/GALAXY.",
    "code_link": "https://github.com/siat-nlp/GALAXY"
  },
  "aaai2022_main_protectingintellectualpropertyoflanguagegenerationapiswithlexicalwatermark": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
    "authors": [
      "Xuanli He",
      "Qiongkai Xu",
      "Lingjuan Lyu",
      "Fangzhao Wu",
      "Chenguang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21321",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21321/21070",
    "published": "2022-02",
    "summary": "Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples.",
    "code_link": "https://github.com/xlhex/NLG"
  },
  "aaai2022_main_brosapre-trainedlanguagemodelfocusingontextandlayoutforbetterkeyinformationextractionfromdocuments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents",
    "authors": [
      "Teakgyu Hong",
      "DongHyun Kim",
      "Mingi Ji",
      "Wonseok Hwang",
      "Daehyun Nam",
      "Sungrae\n      Park"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21322",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21322/21071",
    "published": "2022-02",
    "summary": "Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space. Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout. On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout.Specifically, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy. With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features. This paper also reveals two real-world challenges in KIE tasks--(1) minimizing the error from incorrect text ordering and (2) efficient learning from fewer downstream examples--and demonstrates the superiority of BROS over previous methods.",
    "code_link": "https://github.com/microsoft/unilm"
  },
  "aaai2022_main_non-autoregressivetranslationwithlayer-wisepredictionanddeepsupervision": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Non-autoregressive Translation with Layer-Wise Prediction and Deep Supervision",
    "authors": [
      "Chenyang Huang",
      "Hao Zhou",
      "Osmar R. Za\u00efane",
      "Lili Mou",
      "Lei Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21323",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21323/21072",
    "published": "2022-02",
    "summary": "How do we perform efficient inference while retaining high translation quality? Existing neural machine translation models, such as Transformer, achieve high performance, but they decode words one by one, which is inefficient. Recent non-autoregressive translation models speed up the inference, but their quality is still inferior. In this work, we propose DSLP, a highly efficient and high-performance model for machine translation. The key insight is to train a non-autoregressive Transformer with Deep Supervision and feed additional Layer-wise Predictions. We conducted extensive experiments on four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO). Results show that our approach consistently improves the BLEU scores compared with respective base models. Specifically, our best variant outperforms the autoregressive model on three translation tasks, while being 14.8 times more efficient in inference.",
    "code_link": "https://github.com/MANGA-UOFA/DSLP.git"
  },
  "aaai2022_main_wordlevelrobustnessenhancementfightperturbationwithperturbation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Word Level Robustness Enhancement: Fight Perturbation with Perturbation",
    "authors": [
      "Pei Huang",
      "Yuting Yang",
      "Fuqi Jia",
      "Minghao Liu",
      "Feifei Ma",
      "Jian Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21324",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21324/21073",
    "published": "2022-02",
    "summary": "State-of-the-art deep NLP models have achieved impressive improvements on many tasks. However, they are found to be vulnerable to some perturbations. Before they are widely adopted, the fundamental issues of robustness need to be addressed. In this paper, we design a robustness enhancement method to defend against word substitution perturbation, whose basic idea is to fight perturbation with perturbation. We find that: although many well-trained deep models are not robust in the setting of the presence of adversarial samples, they satisfy weak robustness. That means they can handle most non-crafted perturbations well. Taking advantage of the weak robustness property of deep models, we utilize non-crafted perturbations to resist the adversarial perturbations crafted by attackers. Our method contains two main stages. The first stage is using randomized perturbation to conform the input to the data distribution. The second stage is using randomized perturbation to eliminate the instability of prediction results and enhance the robustness guarantee. Experimental results show that our method can significantly improve the ability of deep models to resist the state-of-the-art adversarial attacks while maintaining the prediction performance on the original clean data.",
    "code_link": "https://github.com/YANG-Yuting/fightperturbation-with-perturbation"
  },
  "aaai2022_main_predictingabove-sentencediscoursestructureusingdistantsupervisionfromtopicsegmentation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Predicting Above-Sentence Discourse Structure Using Distant Supervision from Topic Segmentation",
    "authors": [
      "Patrick Huber",
      "Linzi Xing",
      "Giuseppe Carenini"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21325",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21325/21074",
    "published": "2022-02",
    "summary": "RST-style discourse parsing plays a vital role in many NLP tasks, revealing the underlying semantic/pragmatic structure of potentially complex and diverse documents. Despite its importance, one of the most prevailing limitations in modern day discourse parsing is the lack of large-scale datasets. To overcome the data sparsity issue, distantly supervised approaches from tasks like sentiment analysis and summarization have been recently proposed. Here, we extend this line of research by exploiting distant supervision from topic segmentation, which can arguably provide a strong and oftentimes complementary signal for high-level discourse structures. Experiments on two human-annotated discourse treebanks confirm that our proposal generates accurate tree structures on sentence and paragraph level, consistently outperforming previous distantly supervised models on the sentence-to-document task and occasionally reaching even higher scores on the sentence-to-paragraph level.",
    "code_link": "https://github.com/nlpat/MEGA-DT"
  },
  "aaai2022_main_callforcustomizedconversationcustomizedconversationgroundingpersonaandknowledge": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge",
    "authors": [
      "Yoonna Jang",
      "Jungwoo Lim",
      "Yuna Hur",
      "Dongsuk Oh",
      "Suhyune Son",
      "Yeonsoo Lee",
      "Donghoon Shin",
      "Seungryong Kim",
      "Heuiseok Lim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21326",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21326/21075",
    "published": "2022-02",
    "summary": "Humans usually have conversations by making use of prior knowledge about a topic and background information of the people whom they are talking to. However, existing conversational agents and datasets do not consider such comprehensive information, and thus they have a limitation in generating the utterances where the knowledge and persona are fused properly. To address this issue, we introduce a call For Customized conversation (FoCus) dataset where the customized answers are built with the user's persona and Wikipedia knowledge. To evaluate the abilities to make informative and customized utterances of pre-trained language models, we utilize BART and GPT-2 as well as transformer-based models. We assess their generation abilities with automatic scores and conduct human evaluations for qualitative results. We examine whether the model reflects adequate persona and knowledge with our proposed two sub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we show that the utterances of our data are constructed with the proper knowledge and persona through grounding quality assessment.",
    "code_link": "https://github.com/pkchat-focus/FoCus"
  },
  "aaai2022_main_towardsbuildingasrsystemsforthenextbillionusers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Towards Building ASR Systems for the Next Billion Users",
    "authors": [
      "Tahir Javed",
      "Sumanth Doddapaneni",
      "Abhigyan Raman",
      "Kaushal Santosh Bhogale",
      "Gowtham Ramesh",
      "Anoop Kunchukuttan",
      "Pratyush Kumar",
      "Mitesh M. Khapra"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21327",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21327/21076",
    "published": "2022-02",
    "summary": "Recent methods in speech and language technology pretrain very large models which are fine-tuned for specific tasks. However, the benefits of such large models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource languages from the Indian subcontinent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vectors of similar sounding phonemes are shared across languages, representations across layers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent.",
    "code_link": ""
  },
  "aaai2022_main_span-basedsemanticrolelabelingwithargumentpruningandsecond-orderinference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Span-Based Semantic Role Labeling with Argument Pruning and Second-Order Inference",
    "authors": [
      "Zixia Jia",
      "Zhaohui Yan",
      "Haoyi Wu",
      "Kewei Tu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21328",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21328/21077",
    "published": "2022-02",
    "summary": "We study graph-based approaches to span-based semantic role labeling. This task is difficult due to the need to enumerate all possible predicate-argument pairs and the high degree of imbalance between positive and negative samples. Based on these difficulties, high-order inference that considers interactions between multiple arguments and predicates is often deemed beneficial but has rarely been used in span-based semantic role labeling. Because even for second-order inference, there are already O(n^5) parts for a sentence of length n, and exact high-order inference is intractable. In this paper, we propose a framework consisting of two networks: a predicate-agnostic argument pruning network that reduces the number of candidate arguments to O(n), and a semantic role labeling network with an optional second-order decoder that is unfolded from an approximate inference algorithm. Our experiments show that our framework achieves significant and consistent improvement over previous approaches.",
    "code_link": "https://github.com/JZXXX/Span-srl"
  },
  "aaai2022_main_incorporatingconstituentsyntaxforcoreferenceresolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Incorporating Constituent Syntax for Coreference Resolution",
    "authors": [
      "Fan Jiang",
      "Trevor Cohn"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21329",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21329/21078",
    "published": "2022-02",
    "summary": "Syntax has been shown to benefit Coreference Resolution from incorporating long-range dependencies and structured information captured by syntax trees, either in traditional statistical machine learning based systems or recently proposed neural models. However, most leading systems use only dependency trees. We argue that constituent trees also encode important information, such as explicit span-boundary signals captured by nested multi-word phrases, extra linguistic labels and hierarchical structures useful for detecting anaphora. In this work, we propose a simple yet effective graph-based method to incorporate constituent syntactic structures. Moreover, we also explore to utilise higher-order neighbourhood information to encode rich structures in constituent trees. A novel message propagation mechanism is therefore proposed to enable information flow among elements in syntax trees. Experiments on the English and Chinese portions of OntoNotes 5.0 benchmark show that our proposed model either beats a strong baseline or achieves new state-of-the-art performance. Code is available at https://github.com/Fantabulous-J/Coref-Constituent-Graph.",
    "code_link": ""
  },
  "aaai2022_main_xlm-kimprovingcross-linguallanguagemodelpre-trainingwithmultilingualknowledge": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "XLM-K: Improving Cross-Lingual Language Model Pre-training with Multilingual Knowledge",
    "authors": [
      "Xiaoze Jiang",
      "Yaobo Liang",
      "Weizhu Chen",
      "Nan Duan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21330",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21330/21079",
    "published": "2022-02",
    "summary": "Cross-lingual pre-training has achieved great successes using monolingual and bilingual plain text corpora. However, most pre-trained models neglect multilingual knowledge, which is language agnostic but comprises abundant cross-lingual structure alignment. In this paper, we propose XLM-K, a cross-lingual language model incorporating multilingual knowledge in pre-training. XLM-K augments existing multilingual pre-training with two knowledge tasks, namely Masked Entity Prediction Task and Object Entailment Task. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly demonstrate significant improvements over existing multilingual language models. The results on MLQA and NERexhibit the superiority of XLM-K in knowledge related tasks. The success in XNLI shows a better cross-lingual transferability obtained in XLM-K. What is more, we provide a detailed probing analysis to confirm the desired knowledge captured in our pre-training regimen. The code is available at https://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk.",
    "code_link": "https://github.com/microsoft/Unicoder"
  },
  "aaai2022_main_hierarchicalcontexttaggingforutterancerewriting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hierarchical Context Tagging for Utterance Rewriting",
    "authors": [
      "Lisa Jin",
      "Linfeng Song",
      "Lifeng Jin",
      "Dong Yu",
      "Daniel Gildea"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21331",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21331/21080",
    "published": "2022-02",
    "summary": "Utterance rewriting aims to recover coreferences and omitted information from the latest turn of a multi-turn dialogue. Recently, methods that tag rather than linearly generate sequences have proven stronger in both in- and out-of-domain rewriting settings. This is due to a tagger's smaller search space as it can only copy tokens from the dialogue context. However, these methods may suffer from low coverage when phrases that must be added to a source utterance cannot be covered by a single context span. This can occur in languages like English that introduce tokens such as prepositions into the rewrite for grammaticality. We propose a hierarchical context tagger (HCT) that mitigates this issue by predicting slotted rules (e.g., \"besides _\") whose slots are later filled with context spans. HCT (i) tags the source string with token-level edit actions and slotted rules and (ii) fills in the resulting rule slots with spans from the dialogue context. This rule tagging allows HCT to add out-of-context tokens and multiple spans at once; we further cluster the rules to truncate the long tail of the rule distribution. Experiments on several benchmarks show that HCT can outperform state-of-the-art rewriting systems by ~2 BLEU points.",
    "code_link": "https://github.com/lisjin/hct"
  },
  "aaai2022_main_searchandlearnimprovingsemanticcoveragefordata-to-textgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Search and Learn: Improving Semantic Coverage for Data-to-Text Generation",
    "authors": [
      "Shailza Jolly",
      "Zi Xuan Zhang",
      "Andreas Dengel",
      "Lili Mou"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21332",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21332/21081",
    "published": "2022-02",
    "summary": "Data-to-text generation systems aim to generate text descriptions based on input data (often represented in the tabular form). A typical system uses huge training samples for learning the correspondence between tables and texts. However, large training sets are expensive to obtain, limiting the applicability of these approaches in real-world scenarios. In this work, we focus on few-shot data-to-text generation. We observe that, while fine-tuned pretrained language models may generate plausible sentences, they suffer from the low semantic coverage problem in the few-shot setting. In other words, important input slots tend to be missing in the generated text. To this end, we propose a search-and-learning approach that leverages pretrained language models but inserts the missing slots to improve the semantic coverage. We further finetune our system based on the search results to smooth out the search noise, yielding better-quality text and improving inference efficiency to a large extent. Experiments show that our model achieves high performance on E2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E, largely alleviating the low coverage problem.",
    "code_link": ""
  },
  "aaai2022_main_braidweavingsymbolicandneuralknowledgeintocoherentlogicalexplanations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations",
    "authors": [
      "Aditya Kalyanpur",
      "Tom Breloff",
      "David A Ferrucci"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21333",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21333/21082",
    "published": "2022-02",
    "summary": "Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching (unification) of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge (the \u201cknowledge acquisition\u201d problem). To address these issues, we devise a novel logical reasoner called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid, and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query. We use a simple QA example from a children\u2019s story to motivate Braid\u2019s design and explain how the various components work together to produce a coherent logical explanation. Finally, we evaluate Braid on the ROC Story Cloze test and achieve close to state-of-the-art results while providing frame-based explanations.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedaudio-and-textpre-trainingwithextremelylow-resourceparalleldata": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data",
    "authors": [
      "Yu Kang",
      "Tianqiao Liu",
      "Hang Li",
      "Yang Hao",
      "Wenbiao Ding"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21334",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21334/21083",
    "published": "2022-02",
    "summary": "Multimodal pre-training for audio-and-text has recently been proved to be effective and has significantly improved the performance of many downstream speech understanding tasks. However, these state-of-the-art pre-training audio-text models work well only when provided with large amount of parallel audio-and-text data, which brings challenges on many languages that are rich in unimodal corpora but scarce of parallel cross-modal corpus. In this paper, we investigate whether it is possible to pre-train an audio-text multimodal model with extremely low-resource parallel data and extra non-parallel unimodal data. Our pre-training framework consists of the following components: (1) Intra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input text (audio) representations from a noisy version of itself. (2) Cross-modal Denoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input text (audio), given both a noisy version of the input text (audio) and the corresponding translated noisy audio features (text embeddings). (3) Iterative Denoising Process (IDP), which iteratively translates raw audio (text) and the corresponding text embeddings (audio features) translated from previous iteration into the new less-noisy text embeddings (audio features). We adapt a dual cross-modal Transformer as our backbone model which consists of two unimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our method achieves comparable performance on multiple downstream speech understanding tasks compared with the model pre-trained on fully parallel data, demonstrating the great potential of the proposed method.",
    "code_link": ""
  },
  "aaai2022_main_bridgingthegapusingdeepacousticrepresentationstolearngroundedlanguagefromperceptsandrawspeech": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded Language from Percepts and Raw Speech",
    "authors": [
      "Gaoussou Youssouf Kebe",
      "Luke E. Richards",
      "Edward Raff",
      "Francis Ferraro",
      "Cynthia Matuszek"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21335",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21335/21084",
    "published": "2022-02",
    "summary": "Learning to understand grounded language, which connects natural language to percepts, is a critical research area. Prior work in grounded language acquisition has focused primarily on textual inputs. In this work, we demonstrate the feasibility of performing grounded language acquisition on paired visual percepts and raw speech inputs. This will allow human-robot interactions in which language about novel tasks and environments is learned from end-users, reducing dependence on textual inputs and potentially mitigating the effects of demographic bias found in widely available speech recognition systems. We leverage recent work in self-supervised speech representation models and show that learned representations of speech can make language grounding systems more inclusive towards specific groups while maintaining or even increasing general performance.",
    "code_link": "https://github.com/awslabs/speech-representations"
  },
  "aaai2022_main_alpdataaugmentationusinglexicalizedpcfgsforfew-shottextclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ALP: Data Augmentation Using Lexicalized PCFGs for Few-Shot Text Classification",
    "authors": [
      "Hazel H. Kim",
      "Daecheol Woo",
      "Seong Joon Oh",
      "Jeong-Won Cha",
      "Yo-Sub Han"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21336",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21336/21085",
    "published": "2022-02",
    "summary": "Data augmentation has been an important ingredient for boosting performances of learned models. Prior data augmentation methods for few-shot text classification have led to great performance boosts. However, they have not been designed to capture the intricate compositional structure of natural language. As a result, they fail to generate samples with plausible and diverse sentence structures. Motivated by this, we present the data Augmentation using Lexicalized Probabilistic context-free grammars (ALP) that generates augmented samples with diverse syntactic structures with plausible grammar. The lexicalized PCFG parse trees consider both the constituents and dependencies to produce a syntactic frame that maximizes a variety of word choices in a syntactically preservable manner without specific domain experts. Experiments on few-shot text classification tasks demonstrate that ALP enhances many state-of-the-art classification methods. As a second contribution, we delve into the train-val splitting methodologies when a data augmentation method comes into play. We argue empirically that the traditional splitting of training and validation sets is sub-optimal compared to our novel augmentation-based splitting strategies that further expand the training split with the same number of labeled data. Taken together, our contributions on the data augmentation strategies yield a strong training recipe for few-shot text classification tasks.",
    "code_link": ""
  },
  "aaai2022_main_caiseconversationalagentforimagesearchandediting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CAISE: Conversational Agent for Image Search and Editing",
    "authors": [
      "Hyounghun Kim",
      "Doo Soon Kim",
      "Seunghyun Yoon",
      "Franck Dernoncourt",
      "Trung\n      Bui",
      "Mohit Bansal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21337",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21337/21086",
    "published": "2022-02",
    "summary": "Demand for image editing has been increasing as users' desire for expression is also increasing. However, for most users, image editing tools are not easy to use since the tools require certain expertise in photo effects and have complex interfaces. Hence, users might need someone to help edit their images, but having a personal dedicated human assistant for every user is impossible to scale. For that reason, an automated assistant system for image editing is desirable. Additionally, users want more image sources for diverse image editing works, and integrating an image search functionality into the editing tool is a potential remedy for this demand. Thus, we propose a dataset of an automated Conversational Agent for Image Search and Editing (CAISE). To our knowledge, this is the first dataset that provides conversational image search and editing annotations, where the agent holds a grounded conversation with users and helps them to search and edit images according to their requests. To build such a system, we first collect image search and editing conversations between pairs of annotators. The assistant-annotators are equipped with a customized image search and editing tool to address the requests from the user-annotators. The functions that the assistant-annotators conduct with the tool are recorded as executable commands, allowing the trained system to be useful for real-world application execution. We also introduce a generator-extractor baseline model for this task, which can adaptively select the source of the next token (i.e., from the vocabulary or from textual/visual contexts) for the executable command. This serves as a strong starting point while still leaving a large human-machine performance gap for useful future work. Data and code are available: https://github.com/hyounghk/CAISE.",
    "code_link": "https://github.com/hyounghk/CAISE"
  },
  "aaai2022_main_dualtaskframeworkforimprovingpersona-groundeddialoguedataset": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dual Task Framework for Improving Persona-Grounded Dialogue Dataset",
    "authors": [
      "Minju Kim",
      "Beong-woo Kwak",
      "Youngwook Kim",
      "Hong-in Lee",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21338",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21338/21087",
    "published": "2022-02",
    "summary": "This paper introduces a simple yet effective data-centric approach for the task of improving persona-conditioned dialogue agents. Prior model-centric approaches unquestioningly depend on the raw crowdsourced benchmark datasets such as Persona-Chat. In contrast, we aim to fix annotation artifacts in benchmarking, which is orthogonally applicable to any dialogue model. Specifically, we augment relevant personas to improve dialogue dataset/agent, by leveraging the primal-dual structure of the two tasks, predicting dialogue responses and personas based on each other. Experiments on Persona-Chat show that our approach outperforms pre-trained LMs by an 11.7 point gain in terms of accuracy.",
    "code_link": ""
  },
  "aaai2022_main_minimally-supervisedjointlearningofeventvolitionalityandsubjectanimacyclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Minimally-Supervised Joint Learning of Event Volitionality and Subject Animacy Classification",
    "authors": [
      "Hirokazu Kiyomaru",
      "Sadao Kurohashi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21339",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21339/21088",
    "published": "2022-02",
    "summary": "Volitionality and subject animacy are fundamental and closely related properties of an event. Their classification is challenging because it requires contextual text understanding and a huge amount of labeled data. This paper proposes a novel method that jointly learns volitionality and subject animacy at a low cost, heuristically labeling events in a raw corpus. Volitionality labels are assigned using a small lexicon of volitional and non-volitional adverbs such as deliberately and accidentally; subject animacy labels are assigned using a list of animate and inanimate nouns obtained from ontological knowledge. We then consider the problem of learning a classifier from the labeled events so that it can perform well on unlabeled events without the words used for labeling. We view the problem as a bias reduction or unsupervised domain adaptation problem and apply the techniques. We conduct experiments with crowdsourced gold data in Japanese and English and show that our method effectively learns volitionality and subject animacy without manually labeled data.",
    "code_link": ""
  },
  "aaai2022_main_fromfullytrainedtofullyrandomembeddingsimprovingneuralmachinetranslationwithcompactwordembeddingtables": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "From Fully Trained to Fully Random Embeddings: Improving Neural Machine Translation with Compact Word Embedding Tables",
    "authors": [
      "Krtin Kumar",
      "Peyman Passban",
      "Mehdi Rezagholizadeh",
      "Yiusing Lau",
      "Qun Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21340",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21340/21089",
    "published": "2022-02",
    "summary": "Embedding matrices are key components in neural natural language processing (NLP) models that are responsible to provide numerical representations of input tokens (i.e. words or subwords). In this paper, we analyze the impact and utility of such matrices in the context of neural machine translation (NMT). We show that detracting syntactic and semantic information from word embeddings and running NMT systems with random embeddings is not as damaging as it initially sounds. We also show how incorporating only a limited amount of task-specific knowledge from fully-trained embeddings can boost the performance NMT systems. Our findings demonstrate that in exchange for negligible deterioration in performance, any NMT model can be run with partially random embeddings. Working with such structures means a minimal memory requirement as there is no longer need to store large embedding tables, which is a significant gain in industrial and on-device settings. We evaluated our embeddings in translating English into German and French and achieved a 5.3x compression rate. Despite having a considerably smaller architecture, our models in some cases are even able to outperform state-of-the-art baselines.",
    "code_link": "https://github.com/mjpost/sacreBLEU"
  },
  "aaai2022_main_sgd-xabenchmarkforrobustgeneralizationinschema-guideddialoguesystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems",
    "authors": [
      "Harrison Lee",
      "Raghav Gupta",
      "Abhinav Rastogi",
      "Yuan Cao",
      "Bin Zhang",
      "Yonghui\n      Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21341",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21341/21090",
    "published": "2022-02",
    "summary": "Zero/few-shot transfer to unseen services is a critical challenge in task-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset introduced a paradigm for enabling models to support any service in zero-shot through schemas, which describe service APIs to models in natural language. We explore the robustness of dialogue systems to linguistic variations in schemas by designing SGD-X - a benchmark extending SGD with semantically similar yet stylistically diverse variants for every schema. We observe that two top state tracking models fail to generalize well across schema variants, measured by joint goal accuracy and a novel metric for measuring schema sensitivity. Additionally, we present a simple model-agnostic data augmentation method to improve schema robustness.",
    "code_link": "https://github.com/google-research-datasets/dstc8-schema-guideddialogue"
  },
  "aaai2022_main_unifyingmodelexplainabilityandrobustnessforjointtextclassificationandrationaleextraction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",
    "authors": [
      "Dongfang Li",
      "Baotian Hu",
      "Qingcai Chen",
      "Tujie Xu",
      "Jingcong Tao",
      "Yunan Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21342",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21342/21091",
    "published": "2022-02",
    "summary": "Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model\u2019s robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The results indicate that there are connections between robust models and better explanations.",
    "code_link": "https://github.com/crazyofapple/AT-BMC"
  },
  "aaai2022_main_textrevisionbyon-the-flyrepresentationoptimization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Text Revision By On-the-Fly Representation Optimization",
    "authors": [
      "Jingjing Li",
      "Zichao Li",
      "Tao Ge",
      "Irwin King",
      "Michael R. Lyu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21343",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21343/21092",
    "published": "2022-02",
    "summary": "Text revision refers to a family of natural language generation tasks, where the source and target sequences share moderate resemblance in surface form but differentiate in attributes, such as text formality and simplicity. Current state-of-the-art methods formulate these tasks as sequence-to-sequence learning problems, which rely on large-scale parallel training corpus. In this paper, we present an iterative in-place editing approach for text revision, which requires no parallel data. In this approach, we simply fine-tune a pre-trained Transformer with masked language modeling and attribute classification. During inference, the editing at each iteration is realized by two-step span replacement. At the first step, the distributed representation of the text optimizes on the fly towards an attribute function. At the second step, a text span is masked and another new one is proposed conditioned on the optimized representation. The empirical experiments on two typical and important text revision tasks, text formalization and text simplification, show the effectiveness of our approach. It achieves competitive and even better performance than state-of-the-art supervised methods on text simplification, and gains better performance than strong unsupervised methods on text formalization. Our code and model are released at https://github.com/jingjingli01/OREO.",
    "code_link": ""
  },
  "aaai2022_main_unifiednamedentityrecognitionasword-wordrelationclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unified Named Entity Recognition as Word-Word Relation Classification",
    "authors": [
      "Jingye Li",
      "Hao Fei",
      "Jiang Liu",
      "Shengqiong Wu",
      "Meishan Zhang",
      "Chong Teng",
      "Donghong Ji",
      "Fei Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21344",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21344/21093",
    "published": "2022-02",
    "summary": "So far, named entity recognition (NER) has been involved with three major types, including flat, overlapped (aka. nested), and discontinuous NER, which have mostly been studied individually. Recently, a growing interest has been built for unified NER, tackling the above three jobs concurrently with one single model. Current best-performing methods mainly include span-based and sequence-to-sequence models, where unfortunately the former merely focus on boundary identification and the latter may suffer from exposure bias. In this work, we present a novel alternative by modeling the unified NER as word-word relation classification, namely W^2NER. The architecture resolves the kernel bottleneck of unified NER by effectively modeling the neighboring relations between entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-* (THW-*) relations. Based on the W^2NER scheme we develop a neural framework, in which the unified NER is modeled as a 2D grid of word pairs. We then propose multi-granularity 2D convolutions for better refining the grid representations. Finally, a co-predictor is used to sufficiently reason the word-word relations. We perform extensive experiments on 14 widely-used benchmark datasets for flat, overlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our model beats all the current top-performing baselines, pushing the state-of-the-art performances of unified NER.",
    "code_link": ""
  },
  "aaai2022_main_sequence-to-actiongrammaticalerrorcorrectionwithactionguidedsequencegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sequence-to-Action: Grammatical Error Correction with Action Guided Sequence Generation",
    "authors": [
      "Jiquan Li",
      "Junliang Guo",
      "Yongxin Zhu",
      "Xin Sheng",
      "Deqiang Jiang",
      "Bo Ren",
      "Linli Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21345",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21345/21094",
    "published": "2022-02",
    "summary": "The task of Grammatical Error Correction (GEC) has received remarkable attention with wide applications in Natural Language Processing (NLP) in recent years. While one of the key principles of GEC is to keep the correct parts unchanged and avoid over-correction, previous sequence-to-sequence (seq2seq) models generate results from scratch, which are not guaranteed to follow the original sentence structure and may suffer from the over-correction problem. In the meantime, the recently proposed sequence tagging models can overcome the over-correction problem by only generating edit operations, but are conditioned on human designed language-specific tagging labels. In this paper, we combine the pros and alleviate the cons of both models by proposing a novel Sequence-to-Action (S2A) module. The S2A module jointly takes the source and target sentences as input, and is able to automatically generate a token-level action sequence before predicting each token, where each action is generated from three choices named SKIP, COPY and GENerate. Then the actions are fused with the basic seq2seq framework to provide final predictions. We conduct experiments on the benchmark datasets of both English and Chinese GEC tasks. Our model consistently outperforms the seq2seq baselines, while being able to significantly alleviate the over-correction problem as well as holding better generality and diversity in the generation results compared to the sequence tagging models.",
    "code_link": ""
  },
  "aaai2022_main_dynamickey-valuememoryenhancedmulti-stepgraphreasoningforknowledge-basedvisualquestionanswering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Dynamic Key-Value Memory Enhanced Multi-Step Graph Reasoning for Knowledge-Based Visual Question Answering",
    "authors": [
      "Mingxiao Li",
      "Marie-Francine Moens"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21346",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21346/21095",
    "published": "2022-02",
    "summary": "Knowledge-based visual question answering (VQA) is a vision-language task that requires an agent to correctly answer image-related questions using knowledge that is not presented in the given image. It is not only a more challenging task than regular VQA but also a vital step towards building a general VQA system. Most existing knowledge-based VQA systems process knowledge and image information similarly and ignore the fact that the knowledge base (KB) contains complete information about a triplet, while the extracted image information might be incomplete as the relations between two objects are missing or wrongly detected. In this paper, we propose a novel model named dynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which performs explicit and implicit reasoning over a key-value knowledge memory module and a spatial-aware image graph, respectively. Specifically, the memory module learns a dynamic knowledge representation and generates a knowledge-aware question representation at each reasoning step. Then, this representation is used to guide a graph attention operator over the spatial-aware image graph. Our model achieves new state-of-the-art accuracy on the KRVQR and FVQA datasets. We also conduct ablation experiments to prove the effectiveness of each component of the proposed model.",
    "code_link": ""
  },
  "aaai2022_main_knowledgebridgingforempatheticdialoguegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Knowledge Bridging for Empathetic Dialogue Generation",
    "authors": [
      "Qintong Li",
      "Piji Li",
      "Zhaochun Ren",
      "Pengjie Ren",
      "Zhumin Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21347",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21347/21096",
    "published": "2022-02",
    "summary": "Lack of external knowledge makes empathetic dialogue systems difficult to perceive implicit emotions and learn emotional interactions from limited dialogue history. To address the above problems, we propose to leverage external knowledge, including commonsense knowledge and emotional lexical knowledge, to explicitly understand and express emotions in empathetic dialogue generation. We first enrich the dialogue history by jointly interacting with external knowledge and construct an emotional context graph. Then we learn emotional context representations from the knowledge-enriched emotional context graph and distill emotional signals, which are the prerequisites to predicate emotions expressed in responses. Finally, to generate the empathetic response, we propose an emotional cross-attention mechanism to learn the emotional dependencies from the emotional context graph. Extensive experiments conducted on a benchmark dataset verify the effectiveness of the proposed method. In addition, we find the performance of our method can be further improved by integrating with a pre-trained model that works orthogonally.",
    "code_link": "https://github.com/qtli/KEMP"
  },
  "aaai2022_main_contrastandgenerationmakebartagooddialogueemotionrecognizer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contrast and Generation Make BART a Good Dialogue Emotion Recognizer",
    "authors": [
      "Shimin Li",
      "Hang Yan",
      "Xipeng Qiu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21348",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21348/21097",
    "published": "2022-02",
    "summary": "In dialogue systems, utterances with similar semantics may have distinctive emotions under different contexts. Therefore, modeling long-range contextual emotional relationships with speaker dependency plays a crucial part in dialogue emotion recognition. Meanwhile, distinguishing the different emotion categories is non-trivial since they usually have semantically similar sentiments. To this end, we adopt supervised contrastive learning to make different emotions mutually exclusive to identify similar emotions better. Meanwhile, we utilize an auxiliary response generation task to enhance the model's ability of handling context information, thereby forcing the model to recognize emotions with similar semantics in diverse contexts. To achieve these objectives, we use the pre-trained encoder-decoder model BART as our backbone model since it is very suitable for both understanding and generation tasks. The experiments on four datasets demonstrate that our proposed model obtains significantly more favorable results than the state-of-the-art model in dialogue emotion recognition. The ablation study further demonstrates the effectiveness of supervised contrastive loss and generative loss.",
    "code_link": "https://github.com/whatissimondoing/CoG-BART"
  },
  "aaai2022_main_asemi-supervisedlearningapproachwithtwoteacherstoimprovebreakdownidentificationindialogues": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Semi-supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues",
    "authors": [
      "Qian Lin",
      "Hwee Tou Ng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21349",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21349/21098",
    "published": "2022-02",
    "summary": "Identifying breakdowns in ongoing dialogues helps to improve communication effectiveness. Most prior work on this topic relies on human annotated data and data augmentation to learn a classification model. While quality labeled dialogue data requires human annotation and is usually expensive to obtain, unlabeled data is easier to collect from various sources. In this paper, we propose a novel semi-supervised teacher-student learning framework to tackle this task. We introduce two teachers which are trained on labeled data and perturbed labeled data respectively. We leverage unlabeled data to improve classification in student training where we employ two teachers to refine the labeling of unlabeled data through teacher-student learning in a bootstrapping manner. Through our proposed training approach, the student can achieve improvements over single-teacher performance. Experimental results on the Dialogue Breakdown Detection Challenge dataset DBDC5 and Learning to Identify Follow-Up Questions dataset LIF show that our approach outperforms all previous published approaches as well as other supervised and semi-supervised baseline methods.",
    "code_link": "https://github.com/nusnlp/S2T2"
  },
  "aaai2022_main_diffsingersingingvoicesynthesisviashallowdiffusionmechanism": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism",
    "authors": [
      "Jinglin Liu",
      "Chengxi Li",
      "Yi Ren",
      "Feiyang Chen",
      "Zhou Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21350",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21350/21099",
    "published": "2022-02",
    "summary": "Singing voice synthesis (SVS) systems are built to synthesize high-quality and expressive singing voice, in which the acoustic model generates the acoustic features (e.g., mel-spectrogram) given a music score. Previous singing acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial network (GAN) to reconstruct the acoustic features, while they suffer from over-smoothing and unstable training issues respectively, which hinder the naturalness of synthesized singing.In this work, we propose DiffSinger, an acoustic model for SVS based on the diffusion probabilistic model. DiffSinger is a parameterized Markov chain that iteratively converts the noise into mel-spectrogram conditioned on the music score. By implicitly optimizing variational bound, DiffSinger can be stably trained and generate realistic outputs.To further improve the voice quality and speed up inference, we introduce a shallow diffusion mechanism to make better use of the prior knowledge learned by the simple loss. Specifically, DiffSinger starts generation at a shallow step smaller than the total number of diffusion steps, according to the intersection of the diffusion trajectories of the ground-truth mel-spectrogram and the one predicted by a simple mel-spectrogram decoder. Besides, we propose boundary prediction methods to locate the intersection and determine the shallow step adaptively. The evaluations conducted on a Chinese singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS work. Extensional experiments also prove the generalization of our methods on text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io. Codes: https://github.com/MoonInTheRiver/DiffSinger.",
    "code_link": "https://github.com/MoonInTheRiver/DiffSinger"
  },
  "aaai2022_main_kgr4retrieval,retrospect,refineandrethinkforcommonsensegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "KGR4: Retrieval, Retrospect, Refine and Rethink for Commonsense Generation",
    "authors": [
      "Xin Liu",
      "Dayiheng Liu",
      "Baosong Yang",
      "Haibo Zhang",
      "Junwei Ding",
      "Wenqing\n      Yao",
      "Weihua Luo",
      "Haiying Zhang",
      "Jinsong Su"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21351",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21351/21100",
    "published": "2022-02",
    "summary": "Generative commonsense reasoning requires machines to generate sentences describing an everyday scenario given several concepts, which has attracted much attention recently. However, existing models cannot perform as well as humans, since sentences they produce are often implausible and grammatically incorrect. In this paper, inspired by the process of humans creating sentences, we propose a novel Knowledge-enhanced Commonsense Generation framework, termed KGR4, consisting of four stages: Retrieval, Retrospect, Refine, Rethink. Under this framework, we first perform retrieval to search for relevant sentences from external corpus as the prototypes. Then, we train the generator that either edits or copies these prototypes to generate candidate sentences, of which potential errors will be fixed by an autoencoder-based refiner. Finally, we select the output sentence from candidate sentences produced by generators with different hyper-parameters. Experimental results and in-depth analysis on the CommonGen benchmark strongly demonstrate the effectiveness of our framework. Particularly, KGR4 obtains 33.56 SPICE in the official leaderboard, outperforming the previously-reported best result by 2.49 SPICE and achieving state-of-the-art performance. We release the code at https://github.com/DeepLearnXMU/KGR-4.",
    "code_link": "https://github.com/DeepLearnXMU/KGR-4"
  },
  "aaai2022_main_improvingbiomedicalinformationretrievalwithneuralretrievers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Biomedical Information Retrieval with Neural Retrievers",
    "authors": [
      "Man Luo",
      "Arindam Mitra",
      "Tejas Gokhale",
      "Chitta Baral"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21352",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21352/21101",
    "published": "2022-02",
    "summary": "Information retrieval (IR) is essential in search engines and dialogue systems as well as natural language processing tasks such as open-domain question answering. IR serve an important function in the biomedical domain, where content and sources of scientific knowledge may evolve rapidly. Although neural retrievers have surpassed traditional IR approaches such as TF-IDF and BM25 in standard open-domain question answering tasks, they are still found lacking in the biomedical domain. In this paper, we seek to improve information retrieval (IR) using neural retrievers (NR) in the biomedical domain, and achieve this goal using a three-pronged approach. First, to tackle the relative lack of data in the biomedical domain, we propose a template-based question generation method that can be leveraged to train neural retriever models. Second, we develop two novel pre-training tasks that are closely aligned to the downstream task of information retrieval. Third, we introduce the ``Poly-DPR'' model which encodes each context into multiple context vectors. Extensive experiments and analysis on the BioASQ challenge suggest that our proposed method leads to large gains over existing neural approaches and beats BM25 in the small-corpus setting. We show that BM25 and our method can complement each other, and a simple hybrid model leads to further gains in the large corpus setting.",
    "code_link": "https://github.com/luomancs/neural_retrieval"
  },
  "aaai2022_main_thekingisnakedonthenotionofrobustnessfornaturallanguageprocessing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "The King Is Naked: On the Notion of Robustness for Natural Language Processing",
    "authors": [
      "Emanuele La Malfa",
      "Marta Kwiatkowska"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21353",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21353/21102",
    "published": "2022-02",
    "summary": "There is growing evidence that the classical notion of adversarial robustness originally introduced for images has been adopted as a de facto standard by a large part of the NLP research community.We show that this notion is problematic in the context of NLP as it considers a narrow spectrum of linguistic phenomena. In this paper, we argue for semantic robustness, which is better aligned with the human concept of linguistic fidelity. We characterize semantic robustness in terms of biases that it is expected to induce in a model. We study semantic robustness of a range of vanilla and robustly trained architectures using a template-based generative test bed. We complement the analysis with empirical evidence that, despite being harder to implement, semantic robustness can improve performance %gives guarantees for on complex linguistic phenomena where models robust in the classical sense fail.",
    "code_link": "https://github.com/EmanueleLM/the-king-is-naked"
  },
  "aaai2022_main_selectingoptimalcontextsentencesforevent-eventrelationextraction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Selecting Optimal Context Sentences for Event-Event Relation Extraction",
    "authors": [
      "Hieu Man",
      "Nghia Trung Ngo",
      "Linh Ngo Van",
      "Thien Huu Nguyen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21354",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21354/21103",
    "published": "2022-02",
    "summary": "Understanding events entails recognizing the structural and temporal orders between event mentions to build event structures/graphs for input documents. To achieve this goal, our work addresses the problems of subevent relation extraction (SRE) and temporal event relation extraction (TRE) that aim to predict subevent and temporal relations between two given event mentions/triggers in texts. Recent state-of-the-art methods for such problems have employed transformer-based language models (e.g., BERT) to induce effective contextual representations for input event mention pairs. However, a major limitation of existing transformer-based models for SRE and TRE is that they can only encode input texts of limited length (i.e., up to 512 sub-tokens in BERT), thus unable to effectively capture important context sentences that are farther away in the documents. In this work, we introduce a novel method to better model document-level context with important context sentences for event-event relation extraction. Our method seeks to identify the most important context sentences for a given entity mention pair in a document and pack them into shorter documents to be consume entirely by transformer-based language models for representation learning. The REINFORCE algorithm is employed to train models where novel reward functions are presented to capture model performance, and context-based and knowledge-based similarity between sentences for our problem. Extensive experiments demonstrate the effectiveness of the proposed method with state-of-the-art performance on benchmark datasets.",
    "code_link": ""
  },
  "aaai2022_main_semanticparsingintask-orienteddialogwithrecursiveinsertion-basedencoder": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-Based Encoder",
    "authors": [
      "Elman Mansimov",
      "Yi Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21355",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21355/21104",
    "published": "2022-02",
    "summary": "We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for semantic parsing in task-oriented dialog. Our model consists of an encoder network that incrementally builds the semantic parse tree by predicting the non-terminal label and its positions in the linearized tree. At the generation time, the model constructs the semantic parse tree by recursively inserting the predicted non-terminal labels at the predicted positions until termination. RINE achieves state-of-the-art exact match accuracy on low- and high-resource versions of the conversational semantic parsing benchmark TOP, outperforming strong sequence-to-sequence models and transition-based parsers. We also show that our model design is applicable to nested named entity recognition task, where it performs on par with state-of-the-art approach designed for that task. Finally, we demonstrate that our approach is 2-3.5 times faster than the sequence-to-sequence model at inference time.",
    "code_link": ""
  },
  "aaai2022_main_cinscomprehensiveinstructionforfew-shotlearningintask-orienteddialogsystems": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CINS: Comprehensive Instruction for Few-Shot Learning in Task-Oriented Dialog Systems",
    "authors": [
      "Fei Mi",
      "Yasheng Wang",
      "Yitong Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21356",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21356/21105",
    "published": "2022-02",
    "summary": "As the labeling cost for different modules in task-oriented dialog (ToD) systems is high, a major challenge is to learn different tasks with the least amount of labeled data. Recently, pre-trained language models (PLMs) have shown promising results for few-shot learning in ToD. To better utilize the power of PLMs, this paper proposes Comprehensive Instruction (CINS) that exploits PLMs with extra task-specific instructions. We design a schema (definition, constraint, prompt) of instructions and their customized realizations for three important downstream tasks in ToD, ie. intent classification, dialog state tracking, and natural language generation. A sequence-to-sequence model (T5) is adopted to solve these three tasks in a unified framework. Extensive experiments are conducted on these ToD tasks in realistic few-shot learning scenarios with small validation data. Empirical results demonstrate that the proposed CINS approach consistently improves techniques that finetune PLMs with raw input or short prompt.",
    "code_link": ""
  },
  "aaai2022_main_semanticself-segmentationforabstractivesummarizationoflongdocumentsinlow-resourceregimes": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes",
    "authors": [
      "Gianluca Moro",
      "Luca Ragazzi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21357",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21357/21106",
    "published": "2022-02",
    "summary": "The quadratic memory complexity of transformers prevents long document summarization in low computational resource scenarios. State-of-the-art models need to apply input truncation, thus discarding and ignoring potential summary-relevant contents, leading to a performance drop. Furthermore, this loss is generally destructive for semantic text analytics in high-impact domains such as the legal one. In this paper, we propose a novel semantic self-segmentation (Se3) approach for long document summarization to address the critical problems of low-resource regimes, namely to process inputs longer than the GPU memory capacity and produce accurate summaries despite the availability of only a few dozens of training instances. Se3 segments a long input into semantically coherent chunks, allowing transformers to summarize very long documents without truncation by summarizing each chunk and concatenating the results. Experimental outcomes show the approach significantly improves the performance of abstractive summarization transformers, even with just a dozen of labeled data, achieving new state-of-the-art results on two legal datasets of different domains and contents. Finally, we report ablation studies to evaluate each contribution of the components of our method to the performance gain.",
    "code_link": ""
  },
  "aaai2022_main_eyeofthebeholderimprovedrelationgeneralizationfortext-basedreinforcementlearningagents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Eye of the Beholder: Improved Relation Generalization for Text-Based Reinforcement Learning Agents",
    "authors": [
      "Keerthiram Murugesan",
      "Subhajit Chaudhury",
      "Kartik Talamadupula"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21358",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21358/21107",
    "published": "2022-02",
    "summary": "Text-based games (TBGs) have become a popular proving ground for the demonstration of learning-based agents that make decisions in quasi real-world settings. The crux of the problem for a reinforcement learning agent in such TBGs is identifying the objects in the world, and those objects' relations with that world. While the recent use of text-based resources for increasing an agent's knowledge and improving its generalization have shown promise, we posit in this paper that there is much yet to be learned from visual representations of these same worlds. Specifically, we propose to retrieve images that represent specific instances of text observations from the world and train our agents on such images. This improves the agent's overall understanding of the game scene and objects' relationships to the world around them, and the variety of visual representations on offer allow the agent to generate a better generalization of a relationship. We show that incorporating such images improves the performance of agents in various TBG settings.",
    "code_link": ""
  },
  "aaai2022_main_improvingneuralcross-lingualabstractivesummarizationviaemployingoptimaltransportdistanceforknowledgedistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improving Neural Cross-Lingual Abstractive Summarization via Employing Optimal Transport Distance for Knowledge Distillation",
    "authors": [
      "Thong Thanh Nguyen",
      "Anh Tuan Luu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21359",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21359/21108",
    "published": "2022-02",
    "summary": "Current state-of-the-art cross-lingual summarization models employ multi-task learning paradigm, which works on a shared vocabulary module and relies on the self-attention mechanism to attend among tokens in two languages. However, correlation learned by self-attention is often loose and implicit, inefficient in capturing crucial cross-lingual representations between languages. The matter worsens when performing on languages with separate morphological or structural features, making the cross-lingual alignment more challenging, resulting in the performance drop. To overcome this problem, we propose a novel Knowledge-Distillation-based framework for Cross-Lingual Summarization, seeking to explicitly construct cross-lingual correlation by distilling the knowledge of the monolingual summarization teacher into the cross-lingual summarization student. Since the representations of the teacher and the student lie on two different vector spaces, we further propose a Knowledge Distillation loss using Sinkhorn Divergence, an Optimal-Transport distance, to estimate the discrepancy between those teacher and student representations. Due to the intuitively geometric nature of Sinkhorn Divergence, the student model can productively learn to align its produced cross-lingual hidden states with monolingual hidden states, hence leading to a strong correlation between distant languages. Experiments on cross-lingual summarization datasets in pairs of distant languages demonstrate that our method outperforms state-of-the-art models under both high and low-resourced settings.",
    "code_link": ""
  },
  "aaai2022_main_hitkgtowardsgoal-orientedconversationsviamulti-hierarchylearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "HiTKG: Towards Goal-Oriented Conversations via Multi-Hierarchy Learning",
    "authors": [
      "Jinjie Ni",
      "Vlad Pandelea",
      "Tom Young",
      "Haicang Zhou",
      "Erik Cambria"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21360",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21360/21109",
    "published": "2022-02",
    "summary": "Human conversations are guided by short-term and long-term goals. We study how to plan short-term goal sequences as coherently as humans do and naturally direct them to an assigned long-term goal in open-domain conversations. Goal sequences are a series of knowledge graph (KG) entity-relation connections generated by KG walkers that traverse through the KG. The existing recurrent and graph attention based KG walkers either insufficiently utilize the conversation states or lack global guidance. In our work, a hierarchical model learns goal planning in a hierarchical learning framework. We present HiTKG, a hierarchical transformer-based graph walker that leverages multiscale inputs to make precise and flexible predictions on KG paths. Furthermore, we propose a two-hierarchy learning framework that employs two stages to learn both turn-level (short-term) and global-level (long-term) conversation goals. Specifically, at the first stage, HiTKG is trained in a supervised fashion to learn how to plan turn-level goal sequences; at the second stage, HiTKG tries to naturally approach the assigned global goal via reinforcement learning. In addition, we propose MetaPath as the backbone method for KG path representation to exploit the entity and relation information concurrently. We further propose Multi-source Decoding Inputs and Output-level Length Head to improve the decoding controllability. Our experiments show that HiTKG achieves a significant improvement in the performance of turn-level goal learning compared with state-of-the-art baselines. Additionally, both automatic and human evaluation prove the effectiveness of the two-hierarchy learning framework for both short-term and long-term goal planning.",
    "code_link": ""
  },
  "aaai2022_main_isdiscourseroleimportantforemotionrecognitioninconversation?": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Is Discourse Role Important for Emotion Recognition in Conversation?",
    "authors": [
      "Donovan Ong",
      "Jian Su",
      "Bin Chen",
      "Anh Tuan Luu",
      "Ashok Narendranath",
      "Yue Li",
      "Shuqi Sun",
      "Yingzhan Lin",
      "Haifeng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21361",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21361/21110",
    "published": "2022-02",
    "summary": "A conversation is a sequence of utterances, where each utterance plays a specific discourse role while expressing a particular emotion. This paper proposes a novel method to exploit latent discourse role information of an utterance to determine the emotion it conveys in a conversation. Specifically, we use a variant of the Variational-Autoencoder (VAE) to model the context-aware latent discourse roles of each utterance in an unsupervised way. The latent discourse role representation further equips the utterance representation with a salient clue for more accurate emotion recognition. Our experiments show that our proposed method beats the best-reported performances on three public Emotion Recognition in Conversation datasets. This proves that the discourse role information of an utterance plays an important role in the emotion recognition task, which no previous work has studied.",
    "code_link": ""
  },
  "aaai2022_main_improvedtextclassificationviacontrastiveadversarialtraining": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Improved Text Classification via Contrastive Adversarial Training",
    "authors": [
      "Lin Pan",
      "Chung-Wei Hang",
      "Avirup Sil",
      "Saloni Potdar"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21362",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21362/21111",
    "published": "2022-02",
    "summary": "We propose a simple and general method to regularize the fine-tuning of Transformer-based encoders for text classification tasks. Specifically, during fine-tuning we generate adversarial examples by perturbing the word embedding matrix of the model and perform contrastive learning on clean and adversarial examples in order to teach the model to learn noise-invariant representations. By training on both clean and adversarial examples along with the additional contrastive objective, we observe consistent improvement over standard fine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned Bert_Large model outperforms Bert_Large baseline by 1.7% on average, and our fine-tuned Roberta_Large improves over Roberta_Large baseline by 1.3%. We additionally validate our method in different domains using three intent classification datasets, where our fine-tuned Roberta_Large outperforms Roberta_Large baseline by 1-2% on average. For the challenging low-resource scenario, we train our system using half of the training data (per intent) in each of the three intent classification datasets, and achieve similar performance compared to the baseline trained with full training data.",
    "code_link": ""
  },
  "aaai2022_main_lesicinaheterogeneousgraph-basedapproachforautomaticlegalstatuteidentificationfromindianlegaldocuments": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "LeSICiN: A Heterogeneous Graph-Based Approach for Automatic Legal Statute Identification from Indian Legal Documents",
    "authors": [
      "Shounak Paul",
      "Pawan Goyal",
      "Saptarshi Ghosh"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21363",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21363/21112",
    "published": "2022-02",
    "summary": "The task of Legal Statute Identification (LSI) aims to identify the legal statutes that are relevant to a given description of facts or evidence of a legal case. Existing methods only utilize the textual content of facts and legal articles to guide such a task. However, the citation network among case documents and legal statutes is a rich source of additional information, which is not considered by existing models. In this work, we take the first step towards utilising both the text and the legal citation network for the LSI task.We curate a large novel dataset for this task, including facts of cases from several major Indian Courts of Law, and statutes from the Indian Penal Code (IPC). Modeling the statutes and training documents as a heterogeneous graph, our proposed model LeSICiN can learn rich textual and graphical features, and can also tune itself to correlate these features. Thereafter, the model can be used to inductively predict links between test documents (new nodes whose graphical features are not available to the model) and statutes (existing nodes). Extensive experiments on the dataset show that our model comfortably outperforms several state-of-the-art baselines, by exploiting the graphical structure along with textual features.",
    "code_link": ""
  },
  "aaai2022_main_transformeruncertaintyestimationwithhierarchicalstochasticattention": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Transformer Uncertainty Estimation with Hierarchical Stochastic Attention",
    "authors": [
      "Jiahuan Pei",
      "Cheng Wang",
      "Gy\u00f6rgy Szarvas"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21364",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21364/21113",
    "published": "2022-02",
    "summary": "Transformers are state-of-the-art in a wide range of NLP tasks and have also been applied to many real-world products. Understanding the reliability and certainty of transformer models is crucial for building trustable machine learning applications, e.g., medical diagnosis. Although many recent transformer extensions have been proposed, the study of the uncertainty estimation of transformer models is under-explored. In this work, we propose a novel way to enable transformers to have the capability of uncertainty estimation and, meanwhile, retain the original predictive performance. This is achieved by learning hierarchical stochastic self-attention that attends to values and a set of learnable centroids, respectively. Then new attention heads are formed with a mixture of sampled centroids using the Gumbel-Softmax trick. We theoretically show that the self-attention approximation by sampling from a Gumbel distribution is upper bounded. We empirically evaluate our model on two text classification tasks with both in-domain (ID) and out-of-domain (OOD) datasets.The experimental results demonstrate that our approach: (1) achieves the best predictive-uncertainty trade-off among compared methods; (2) exhibits very competitive (in most cases, better) predictive performance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble methods in uncertainty estimation on OOD datasets.",
    "code_link": ""
  },
  "aaai2022_main_stepssemantictypingofeventprocesseswithasequence-to-sequenceapproach": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "STEPS: Semantic Typing of Event Processes with a Sequence-to-Sequence Approach",
    "authors": [
      "Sveva Pepe",
      "Edoardo Barba",
      "Rexhina Blloshmi",
      "Roberto Navigli"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21365",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21365/21114",
    "published": "2022-02",
    "summary": "Enabling computers to comprehend the intent of human actions by processing language is one of the fundamental goals of Natural Language Understanding. An emerging task in this context is that of free-form event process typing, which aims at understanding the overall goal of a protagonist in terms of an action and an object, given a sequence of events.This task was initially treated as a learning-to-rank problem by exploiting the similarity between processes and action/object textual definitions. However, this approach appears to be overly complex, binds the output types to a fixed inventory for possible word definitions and, moreover, leaves space for further enhancements as regards performance.In this paper, we advance the field by reformulating the free-form event process typing task as a sequence generation problem and put forward STEPS, an end-to-end approach for producing user intent in terms of actions and objects only, dispensing with the need for their definitions.In addition to this, we eliminate several dataset constraints set by previous works, while at the same time significantly outperforming them. We release the data and software at https://github.com/SapienzaNLP/steps.",
    "code_link": ""
  },
  "aaai2022_main_sparsestructurelearningviagraphneuralnetworksforinductivedocumentclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification",
    "authors": [
      "Yinhua Piao",
      "Sangseon Lee",
      "Dohoon Lee",
      "Sun Kim"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21366",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21366/21115",
    "published": "2022-02",
    "summary": "Recently, graph neural networks (GNNs) have been widely used for document classification. However, most existing methods are based on static word co-occurrence graphs without sentence-level information, which poses three challenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual dependency. To address these challenges, we propose a novel GNN-based sparse structure learning model for inductive document classification. Specifically, a document-level graph is initially generated by a disjoint union of sentence-level word co-occurrence graphs. Our model collects a set of trainable edges connecting disjoint words between sentences, and employs structure learning to sparsely select edges with dynamic contextual dependencies. Graphs with sparse structure can jointly exploit local and global contextual information in documents through GNNs. For inductive learning, the refined document graph is further fed into a general readout function for graph-level classification and optimization in an end-to-end manner. Extensive experiments on several real-world datasets demonstrate that the proposed model outperforms most state-of-the-art results, and reveal the necessity to learn sparse structures for each document.",
    "code_link": "https://github.com/qkrdmsghk/TextSSL"
  },
  "aaai2022_main_stemunsupervisedstructuralembeddingforstancedetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "STEM: Unsupervised STructural EMbedding for Stance Detection",
    "authors": [
      "Ron Korenblum Pick",
      "Vladyslav Kozhukhov",
      "Dan Vilenchik",
      "Oren Tsur"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21367",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21367/21116",
    "published": "2022-02",
    "summary": "Stance detection is an important task, supporting many downstream tasks such as discourse parsing and modeling the propagation of fake news, rumors, and science denial. In this paper, we propose a novel framework for stance detection.Our framework is unsupervised and domain-independent. Given a claim and a multi-participant discussion -- we construct the interaction network from which we derive topological embedding for each speaker. These speaker embedding enjoy the following property:speakers with the same stance tend to be represented by similar vectors, while antipodal vectors represent speakers with opposing stances. These embedding are then used to divide the speakers into stance-partitions. We evaluate our method on three different datasets from different platforms. Our method outperforms or is comparable with supervised models while providing confidence levels for its output. Furthermore, we demonstrate how the structural embedding relate to the valence expressed by the speakers. Finally, we discuss some limitations inherent to the framework.",
    "code_link": "https://github.com/NasLabBgu/STEM"
  },
  "aaai2022_main_valuenetanewdatasetforhumanvaluedrivendialoguesystem": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "ValueNet: A New Dataset for Human Value Driven Dialogue System",
    "authors": [
      "Liang Qiu",
      "Yizhou Zhao",
      "Jinchao Li",
      "Pan Lu",
      "Baolin Peng",
      "Jianfeng Gao",
      "Song-Chun Zhu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21368",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21368/21117",
    "published": "2022-02",
    "summary": "Building a socially intelligent agent involves many challenges, one of which is to teach the agent to speak guided by its value like a human. However, value-driven chatbots are still understudied in the area of dialogue systems. Most existing datasets focus on commonsense reasoning or social norm modeling. In this work, we present a new large-scale human value dataset called ValueNet, which contains human attitudes on 21,374 text scenarios. The dataset is organized in ten dimensions that conform to the basic human value theory in intercultural research. We further develop a Transformer-based value regression model on ValueNet to learn the utility distribution. Comprehensive empirical results show that the learned value model could benefit a wide range of dialogue tasks. For example, by teaching a generative agent with reinforcement learning and the rewards from the value model, our method attains state-of-the-art performance on the personalized dialog generation dataset: Persona-Chat. With values as additional features, existing emotion recognition models enable capturing rich human emotions in the context, which further improves the empathetic response generation performance in the EmpatheticDialogues dataset. To the best of our knowledge, ValueNet is the first large-scale text dataset for human value modeling, and we are the first one trying to incorporate a value model into emotionally intelligent dialogue systems. The dataset is available at https://liang-qiu.github.io/ValueNet/.",
    "code_link": "https://github.com/facebookresearch/fastText"
  },
  "aaai2022_main_post-ocrdocumentcorrectionwithlargeensemblesofcharactersequence-to-sequencemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Post-OCR Document Correction with Large Ensembles of Character Sequence-to-Sequence Models",
    "authors": [
      "Juan Antonio Ramirez-Orta",
      "Eduardo Xamena",
      "Ana Maguitman",
      "Evangelos\n      Milios",
      "Axel J. Soto"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21369",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21369/21118",
    "published": "2022-02",
    "summary": "In this paper, we propose a novel method to extend sequence-to-sequence models to accurately process sequences much longer than the ones used during training while being sample- and resource-efficient, supported by thorough experimentation. To investigate the effectiveness of our method, we apply it to the task of correcting documents already processed with Optical Character Recognition (OCR) systems using sequence-to-sequence models based on characters. We test our method on nine languages of the ICDAR 2019 competition on post-OCR text correction and achieve a new state-of-the-art performance in five of them. The strategy with the best performance involves splitting the input document in character n-grams and combining their individual corrections into the final output using a voting scheme that is equivalent to an ensemble of a large number of sequence models. We further investigate how to weigh the contributions from each one of the members of this ensemble. Our code for post-OCR correction is shared at https://github.com/jarobyte91/post_ocr_correction.",
    "code_link": "https://github.com/jarobyte91/post"
  },
  "aaai2022_main_mumuqamultimediamulti-hopnewsquestionansweringviacross-mediaknowledgeextractionandgrounding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding",
    "authors": [
      "Revant Gangi Reddy",
      "Xilin Rui",
      "Manling Li",
      "Xudong Lin",
      "Haoyang Wen",
      "Jaemin\n      Cho",
      "Lifu Huang",
      "Mohit Bansal",
      "Avirup Sil",
      "Shih-Fu Chang",
      "Alexander\n      Schwing",
      "Heng Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21370",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21370/21119",
    "published": "2022-02",
    "summary": "Recently, there has been an increasing interest in building question answering (QA) models that reason across multiple modalities, such as text and images. However, QA using images is often limited to just picking the answer from a pre-defined set of options. In addition, images in the real world, especially in news, have objects that are co-referential to the text, with complementary information from both modalities. In this paper, we present a new QA evaluation benchmark with 1,384 questions over news articles that require cross-media grounding of objects in images onto text. Specifically, the task involves multi-hop questions that require reasoning over image-caption pairs to identify the grounded visual object being referred to and then predicting a span from the news body text to answer the question. In addition, we introduce a novel multimedia data augmentation framework, based on cross-media knowledge extraction and synthetic question-answer generation, to automatically augment data that can provide weak supervision for this task. We evaluate both pipeline-based and end-to-end pretraining-based multimedia QA models on our benchmark, and show that they achieve promising performance, while considerably lagging behind human performance hence leaving large room for future work on this challenging new task.",
    "code_link": ""
  },
  "aaai2022_main_pushingthelimitsofrulereasoningintransformersthroughnaturallanguagesatisfiability": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability",
    "authors": [
      "Kyle Richardson",
      "Ashish Sabharwal"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21371",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21371/21120",
    "published": "2022-02",
    "summary": "Investigating the reasoning abilities of transformer models, and discovering new challenging tasks for them, has been a topic of much interest. Recent studies have found these models to be surprisingly strong at performing deductive reasoning over formal logical theories expressed in natural language. A shortcoming of these studies, however, is that they do not take into account that logical theories, when sampled uniformly at random, do not necessarily lead to hard instances. We propose a new methodology for creating challenging algorithmic reasoning datasets that focus on natural language satisfiability (NLSat) problems. The key idea is to draw insights from empirical sampling of hard propositional SAT problems and from complexity-theoretic studies of language. This methodology allows us to distinguish easy from hard instances, and to systematically increase the complexity of existing reasoning benchmarks such as RuleTaker. We find that current transformers, given sufficient training data, are surprisingly robust at solving the resulting NLSat problems of substantially increased difficulty. They also exhibit some degree of scale-invariance\u2014the ability to generalize to problems of larger size and scope. Our results, however, reveal important limitations too: careful sampling of training data is crucial for building models that generalize to larger problems, and transformer models\u2019 limited scale-invariance suggests they are far from learning robust deductive reasoning algorithms.",
    "code_link": "https://github.com/allenai/language"
  },
  "aaai2022_main_sfsrnetsuper-resolutionforsingle-channelaudiosourceseparation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SFSRNet: Super-resolution for Single-Channel Audio Source Separation",
    "authors": [
      "Joel Rixen",
      "Matthias Renz"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21372",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21372/21121",
    "published": "2022-02",
    "summary": "The problem of single-channel audio source separation is to recover (separate) multiple audio sources that are mixed in a single-channel audio signal (e.g. people talking over each other). Some of the best performing single-channel source separation methods utilize downsampling to either make the separation process faster or make the neural networks bigger and increase accuracy. The problem concerning downsampling is that it usually results in information loss. In this paper, we tackle this problem by introducing SFSRNet which contains a super-resolution (SR) network. The SR network is trained to reconstruct the missing information in the upper frequencies of the audio signal by operating on the spectrograms of the output audio source estimations and the input audio mixture. Any separation method where the length of the sequence is a bottleneck in speed and memory can be made faster or more accurate by using the SR network. Based on the WSJ0-2mix benchmark where estimations of the audio signal of two speakers need to be extracted from the mixture, in our experiments our proposed SFSRNet reaches a scale-invariant signal-to-noise-ratio improvement (SI-SNRi) of 24.0 dB outperforming the state-of-the-art solution SepFormer which reaches an SI-SNRi of 22.3 dB.",
    "code_link": "https://github.com/j-rixen/SFSRNet-Audio-samples"
  },
  "aaai2022_main_cemcommonsense-awareempatheticresponsegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "CEM: Commonsense-Aware Empathetic Response Generation",
    "authors": [
      "Sahand Sabour",
      "Chujie Zheng",
      "Minlie Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21373",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21373/21122",
    "published": "2022-02",
    "summary": "A key trait of daily conversations between individuals is the ability to express empathy towards others, and exploring ways to implement empathy is a crucial step towards human-like dialogue systems. Previous approaches on this topic mainly focus on detecting and utilizing the user\u2019s emotion for generating empathetic responses. However, since empathy includes both aspects of affection and cognition, we argue that in addition to identifying the user\u2019s emotion, cognitive understanding of the user\u2019s situation should also be considered. To this end, we propose a novel approach for empathetic response generation, which leverages commonsense to draw more information about the user\u2019s situation and uses this additional information to further enhance the empathy expression in generated responses. We evaluate our approach on EMPATHETICDIALOGUES, which is a widely-used benchmark dataset for empathetic response generation. Empirical results demonstrate that our approach outperforms the baseline models in both automatic and human evaluations and can generate more informative and empathetic responses. Our code is available at https://github.com/Sahandfer/CEM.",
    "code_link": "https://github.com/Sahandfer/CEM"
  },
  "aaai2022_main_weaklysupervisedneuro-symbolicmodulenetworksfornumericalreasoningovertext": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning over Text",
    "authors": [
      "Amrita Saha",
      "Shafiq Joty",
      "Steven C.H. Hoi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21374",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21374/21123",
    "published": "2022-02",
    "summary": "Neural Module Networks (NMNs) have been quite successful in incorporating explicit reasoning as learnable modules in various question answering tasks, including the most generic form of numerical reasoning over text in Machine Reading Comprehension (MRC). However to achieve this, contemporary Neural Module Networks models obtain strong supervision in form of specialized program annotation from the QA pairs through various heuristic parsing and exhaustive computation of all possible discrete operations on discrete arguments. Consequently they fail to generalize to more open-ended settings without such supervision. Hence, we propose Weakly Supervised Neuro-Symbolic Module Network (WNSMN) trained with answers as the sole supervision for numerical reasoning based MRC. WNSMN learns to execute a noisy heuristic program obtained from the dependency parse of the query, as discrete actions over both neural and symbolic reasoning modules and trains it end-to-end in a reinforcement learning framework with discrete reward from answer matching. On the subset of DROP having numerical answers, WNSMN outperforms NMN by 32% and the reasoning-free generative language model GenBERT by 8% in exact match accuracy under comparable weakly supervised settings. This showcases the effectiveness of modular networks that can handle explicit discrete reasoning over noisy programs in an end-to-end manner.",
    "code_link": "https://github.com/raylin1000/drop-bert"
  },
  "aaai2022_main_arevision-languagetransformerslearningmultimodalrepresentations?aprobingperspective": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective",
    "authors": [
      "Emmanuelle Salin",
      "Badreddine Farah",
      "St\u00e9phane Ayache",
      "Benoit Favre"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21375",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21375/21124",
    "published": "2022-02",
    "summary": "In recent years, joint text-image embeddings have significantly improved thanks to the development of transformer-based Vision-Language models. Despite these advances, we still need to better understand the representations produced by those models. In this paper, we compare pre-trained and fine-tuned representations at a vision, language and multimodal level. To that end, we use a set of probing tasks to evaluate the performance of state-of-the-art Vision-Language models and introduce new datasets specifically for multimodal probing. These datasets are carefully designed to address a range of multimodal capabilities while minimizing the potential for models to rely on bias. Although the results confirm the ability of Vision-Language models to understand color at a multimodal level, the models seem to prefer relying on bias in text data for object position and size. On semantically adversarial examples, we find that those models are able to pinpoint fine-grained multimodal differences. Finally, we also notice that fine-tuning a Vision-Language model on multimodal tasks does not necessarily improve its multimodal ability. We make all datasets and code available to replicate experiments.",
    "code_link": "https://github.com/ejsalin/vlm-probing"
  },
  "aaai2022_main_entailmentrelationawareparaphrasegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Entailment Relation Aware Paraphrase Generation",
    "authors": [
      "Abhilasha Sancheti",
      "Balaji Vasan Srinivasan",
      "Rachel Rudinger"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21376",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21376/21125",
    "published": "2022-02",
    "summary": "We introduce a new task of entailment relation aware paraphrase generation which aims at generating a paraphrase conforming to a given entailment relation (e.g. equivalent, forward entailing, or reverse entailing) with respect to a given input. We propose a reinforcement learning-based weakly-supervised paraphrasing system, ERAP, that can be trained using existing paraphrase and natural language inference (NLI) corpora without an explicit task-specific corpus. A combination of automated and human evaluations show that ERAP generates paraphrases conforming to the specified entailment relation and are of good quality as compared to the baselines and uncontrolled paraphrasing systems. Using ERAP for augmenting training data for downstream textual entailment task improves performance over an uncontrolled paraphrasing system, and introduces fewer training artifacts, indicating the benefit of explicit control during paraphrasing.",
    "code_link": ""
  },
  "aaai2022_main_visualdefinitionmodelingchallengingvision&languagemodelstodefinewordsandobjects": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Visual Definition Modeling: Challenging Vision & Language Models to Define Words and Objects",
    "authors": [
      "Bianca Scarlini",
      "Tommaso Pasini",
      "Roberto Navigli"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21377",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21377/21126",
    "published": "2022-02",
    "summary": "Architectures that model language and vision together havereceived much attention in recent years. Nonetheless, most tasks in this field focus on end-to-end applications without providing insights on whether it is the underlying semantics of visual objects or words that is captured. In this paper we draw on the established Definition Modeling paradigm and enhance it by grounding, for the first time, textual definitions to visual representations. We name this new task Visual Definition Modeling and put forward DEMETER and DIONYSUS, two benchmarks where, given an image as context, models have to generate a textual definition for a target being either i) a word that describes the image, or ii) an object patch therein. To measure the difficulty of our tasks we finetuned six different baselines and analyzed their performances, which show that a text-only encoder-decoder model is more effective than models pretrained for handling inputs of both modalities concurrently. This demonstrates the complexity of our benchmarks and encourages more research on text generation conditioned on multimodal inputs. The datasets for both benchmarks are available at https://github.com/SapienzaNLP/visual-definition-modeling as well as the code to reproduce our models.",
    "code_link": ""
  },
  "aaai2022_main_activelearningonpre-trainedlanguagemodelwithtask-independenttripletloss": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Active Learning on Pre-trained Language Model with Task-Independent Triplet Loss",
    "authors": [
      "Seungmin Seo",
      "Donghyun Kim",
      "Youbin Ahn",
      "Kyong-Ho Lee"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21378",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21378/21127",
    "published": "2022-02",
    "summary": "Active learning attempts to maximize a task model\u2019s performance gain by obtaining a set of informative samples from an unlabeled data pool. Previous active learning methods usually rely on specific network architectures or task-dependent sample acquisition algorithms. Moreover, when selecting a batch sample, previous works suffer from insufficient diversity of batch samples because they only consider the informativeness of each sample. This paper proposes a task-independent batch acquisition method using triplet loss to distinguish hard samples in an unlabeled data pool with similar features but difficult to identify labels. To assess the effectiveness of the proposed method, we compare the proposed method with state-of-the-art active learning methods on two tasks, relation extraction and sentence classification. Experimental results show that our method outperforms baselines on the benchmark datasets.",
    "code_link": ""
  },
  "aaai2022_main_onereljointentityandrelationextractionwithonemoduleinonestep": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "OneRel: Joint Entity and Relation Extraction with One Module in One Step",
    "authors": [
      "Yu-Ming Shang",
      "Heyan Huang",
      "Xianling Mao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21379",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21379/21128",
    "published": "2022-02",
    "summary": "Joint entity and relation extraction is an essential task in natural language processing and knowledge graph construction. Existing approaches usually decompose the joint extraction task into several basic modules or processing steps to make it easy to conduct. However, such a paradigm ignores the fact that the three elements of a triple are interdependent and indivisible. Therefore, previous joint methods suffer from the problems of cascading errors and redundant information. To address these issues, in this paper, we propose a novel joint entity and relation extraction model, named OneRel, which casts joint extraction as a fine-grained triple classification problem. Specifically, our model consists of a scoring-based classifier and a relation-specific horns tagging strategy. The former evaluates whether a token pair and a relation belong to a factual triple. The latter ensures a simple but effective decoding process. Extensive experimental results on two widely used datasets demonstrate that the proposed method performs better than the state-of-the-art baselines, and delivers consistent performance gain on complex scenarios of various overlapping patterns and multiple triples.",
    "code_link": ""
  },
  "aaai2022_main_katgkeyword-bias-awareadversarialtextgenerationfortextclassification": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "KATG: Keyword-Bias-Aware Adversarial Text Generation for Text Classification",
    "authors": [
      "Lingfeng Shen",
      "Shoushan Li",
      "Ying Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21380",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21380/21129",
    "published": "2022-02",
    "summary": "Recent work has shown that current text classification models are vulnerable to small adversarial perturbation to inputs, and adversarial training that re-trains the models with the support of adversarial examples is the most popular way to alleviate the impact of the perturbation. However, current adversarial training methods have two principal problems: worse model generalization and ineffective defending against other text attacks. In this paper, we propose a Keyword-bias-aware Adversarial Text Generation model (KATG) that implicitly generates adversarial sentences using a generator-discriminator structure. Instead of using a benign sentence to generate an adversarial sentence, the KATG model utilizes extra multiple benign sentences (namely prior sentences) to guide adversarial sentence generation. Furthermore, to cover more perturbation used in existing attacks, a keyword-bias-aware sampling is proposed to select sentences containing biased words as prior sentences. Besides, to effectively utilize prior sentences, a generative flow mechanism is proposed to construct latent semantic space and learn a latent representation for the prior sentences. Experiments demonstrate that adversarial sentences generated by our KATG model can strengthen the victim model's robustness and generalization.",
    "code_link": ""
  },
  "aaai2022_main_unsuperviseddeepkeyphrasegeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Deep Keyphrase Generation",
    "authors": [
      "Xianjie Shen",
      "Yinghan Wang",
      "Rui Meng",
      "Jingbo Shang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21381",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21381/21130",
    "published": "2022-02",
    "summary": "Keyphrase generation aims to summarize long documents with a collection of salient phrases. Deep neural models have demonstrated remarkable success in this task, with the capability of predicting keyphrases that are even absent from a document. However, such abstractiveness is acquired at the expense of a substantial amount of annotated data. In this paper, we present a novel method for keyphrase generation, AutoKeyGen, without the supervision of any annotated doc-keyphrase pairs. Motivated by the observation that an absent keyphrase in a document may appear in other places, in whole or in part, we construct a phrase bank by pooling all phrases extracted from a corpus.With this phrase bank, we assign phrase candidates to new documents by a simple partial matching algorithm, and then we rank these candidates by their relevance to the document from both lexical and semantic perspectives. Moreover, we bootstrap a deep generative model using these top-ranked pseudo keyphrases to produce more absent candidates. Extensive experiments demonstrate that AutoKeyGen outperforms all unsupervised baselines and can even beat a strong supervised method in certain cases.",
    "code_link": ""
  },
  "aaai2022_main_generation-focusedtable-basedintermediatepre-trainingforfree-formquestionanswering": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Generation-Focused Table-Based Intermediate Pre-training for Free-Form Question Answering",
    "authors": [
      "Peng Shi",
      "Patrick Ng",
      "Feng Nan",
      "Henghui Zhu",
      "Jun Wang",
      "Jiarong Jiang",
      "Alexander Hanbo Li",
      "Rishav Chakravarti",
      "Donald Weidner",
      "Bing Xiang",
      "Zhiguo\n      Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21382",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21382/21131",
    "published": "2022-02",
    "summary": "Question answering over semi-structured tables has attracted significant attention in the NLP community.However, most of the existing work focus on questions that can be answered with short-form answer, i.e. the answer is often a table cell or aggregation of multiple cells. This can mismatch with the intents of users who want to ask more complex questions that require free-form answers such as explanations. To bridge the gap, most recently, pre-trained sequence-to-sequence language models such as T5 are used for generating free-form answers based on the question and table inputs. However, these pre-trained language models have weaker encoding abilities over table cells and schema. To mitigate this issue, in this work, we present an intermediate pre-training framework, Generation-focused Table-based Intermediate Pre-training (GENTAP), that jointly learns representations of natural language questions and tables.GENTAP learns to generate via two training objectives to enhance the question understanding and table representation abilities for complex questions. Based on experimental results, models that leverage GENTAP framework outperform the existing baselines on FETAQA benchmark. The pre-trained models are not only useful for free-form question answering, but also for few-shot data-to-text generation task, thus showing good transfer ability by obtaining new state-of-the-art results.",
    "code_link": ""
  },
  "aaai2022_main_stepgameanewbenchmarkforrobustmulti-hopspatialreasoningintexts": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts",
    "authors": [
      "Zhengxiang Shi",
      "Qiang Zhang",
      "Aldo Lipani"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21383",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21383/21132",
    "published": "2022-02",
    "summary": "Inferring spatial relations in natural language is a crucial ability an intelligent system should possess. The bAbI dataset tries to capture tasks relevant to this domain (task 17 and 19). However, these tasks have several limitations. Most importantly, they are limited to fixed expressions, they are limited in the number of reasoning steps required to solve them, and they fail to test the robustness of models to input that contains irrelevant or redundant information. In this paper, we present a new Question-Answering dataset called StepGame for robust multi-step spatial reasoning in texts. Our experiments demonstrate that state-of-the-art models on the bAbI dataset struggle on the StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental results on both datasets show that our model outperforms all the baselines with superior generalization and robustness performance.",
    "code_link": "https://github.com/ZhengxiangShi/StepGame"
  },
  "aaai2022_main_minimalminingmodelsforuniversaladversarialtriggers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MINIMAL: Mining Models for Universal Adversarial Triggers",
    "authors": [
      "Yaman Kumar Singla",
      "Swapnil Parekh",
      "Somesh Singh",
      "Changyou Chen",
      "Balaji\n      Krishnamurthy",
      "Rajiv Ratn Shah"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21384",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21384/21133",
    "published": "2022-02",
    "summary": "It is well known that natural language models are vulnerable to adversarial attacks, which are mostly input-specific in nature. Recently, it has been shown that there also exist input-agnostic attacks in NLP models, called universal adversarial triggers. However, existing methods to craft universal triggers are data intensive. They require large amounts of data samples to generate adversarial triggers, which are typically inaccessible by attackers. For instance, previous works take 3000 data samples per class for the SNLI dataset to generate adversarial triggers. In this paper, we present a novel data-free approach, MINIMAL, to mine input-agnostic adversarial triggers from models. Using the triggers produced with our data-free algorithm, we reduce the accuracy of Stanford Sentiment Treebank\u2019s positive class from 93.6% to 9.6%. Similarly, for the Stanford Natural LanguageInference (SNLI), our single-word trigger reduces the accuracy of the entailment class from 90.95% to less than 0.6%. Despite being completely data-free, we get equivalent accuracy drops as data-dependent methods",
    "code_link": ""
  },
  "aaai2022_main_hierarchicalheterogeneousgraphattentionnetworkforsyntax-awaresummarization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
    "authors": [
      "Zixing Song",
      "Irwin King"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21385",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21385/21134",
    "published": "2022-02",
    "summary": "The task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models.",
    "code_link": ""
  },
  "aaai2022_main_supervisingmodelattentionwithhumanexplanationsforrobustnaturallanguageinference": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Supervising Model Attention with Human Explanations for Robust Natural Language Inference",
    "authors": [
      "Joe Stacey",
      "Yonatan Belinkov",
      "Marek Rei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21386",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21386/21135",
    "published": "2022-02",
    "summary": "Natural Language Inference (NLI) models are known to learn from biases and artefacts within their training data, impacting how well they generalise to other unseen datasets. Existing de-biasing approaches focus on preventing the models from learning these biases, which can result in restrictive models and lower performance. We instead investigate teaching the model how a human would approach the NLI task, in order to learn features that will generalise better to previously unseen examples. Using natural language explanations, we supervise the model\u2019s attention weights to encourage more attention to be paid to the words present in the explanations, significantly improving model performance. Our experiments show that the in-distribution improvements of this method are also accompanied by out-of-distribution improvements, with the supervised models learning from features that generalise better to other NLI datasets. Analysis of the model indicates that human explanations encourage increased attention on the important words, with more attention paid to words in the premise and less attention paid to punctuation and stopwords.",
    "code_link": "https://github.com/joestacey/NLI"
  },
  "aaai2022_main_hyperbolicdisentangledrepresentationforfine-grainedaspectextraction": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hyperbolic Disentangled Representation for Fine-Grained Aspect Extraction",
    "authors": [
      "Chang-Yu Tai",
      "Ming-Yao Li",
      "Lun-Wei Ku"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21387",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21387/21136",
    "published": "2022-02",
    "summary": "Automatic identification of salient aspects from user reviews is especially useful for opinion analysis. There has been significant progress in utilizing weakly supervised approaches, which require only a small set of seed words for training aspect classifiers. However, there is always room for improvement. First, no weakly supervised approaches fully utilize latent hierarchies between words. Second, each seed word\u2019s representation should have different latent semantics and be distinct when it represents a different aspect. In this paper we propose HDAE, a hyperbolic disentangled aspect extractor in which a hyperbolic aspect classifier captures words\u2019 latent hierarchies, and an aspect-disentangled representation models the distinct latent semantics of each seed word. Compared to previous baselines, HDAE achieves average F1 performance gains of 18.2% and 24.1% on Amazon product review and restaurant review datasets, respectively. In addition, the embedding visualization experience demonstrates that HDAE is a more effective approach to leveraging seed words. An ablation study and a case study further attest the effectiveness of the proposed components.",
    "code_link": "https://github.com/johnnyjana730/HDAE"
  },
  "aaai2022_main_proceduraltextunderstandingviascene-wiseevolution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Procedural Text Understanding via Scene-Wise Evolution",
    "authors": [
      "Jialong Tang",
      "Hongyu Lin",
      "Meng Liao",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun",
      "Weijian Xie",
      "Jin Xu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21388",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21388/21137",
    "published": "2022-02",
    "summary": "Procedural text understanding requires machines to reason about entity states within the dynamical narratives. Current procedural text understanding approaches are commonly entity-wise, which separately track each entity and independently predict different states of each entity. Such an entity-wise paradigm does not consider the interaction between entities and their states. In this paper, we propose a new scene-wise paradigm for procedural text understanding, which jointly tracks states of all entities in a scene-by-scene manner. Based on this paradigm, we propose Scene Graph Reasoner (SGR), which introduces a series of dynamically evolving scene graphs to jointly formulate the evolution of entities, states and their associations throughout the narrative. In this way, the deep interactions between all entities and states can be jointly captured and simultaneously derived from scene graphs. Experiments show that SGR not only achieves the new state-of-the-art performance but also significantly accelerates the speed of reasoning.",
    "code_link": "https://github.com/ytyz1307zzh/NCET-ProPara"
  },
  "aaai2022_main_debiasingnlumodelsviacausalinterventionandcounterfactualreasoning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning",
    "authors": [
      "Bing Tian",
      "Yixin Cao",
      "Yong Zhang",
      "Chunxiao Xing"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21389",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21389/21138",
    "published": "2022-02",
    "summary": "Recent studies have shown that strong Natural Language Understanding (NLU) models are prone to relying on annotation biases of the datasets as a shortcut, which goes against the underlying mechanisms of the task of interest. To reduce such biases, several recent works introduce debiasing methods to regularize the training process of targeted NLU models. In this paper, we provide a new perspective with causal inference to find out the bias. On one hand, we show that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data. To remove such confounder, the backdoor adjustment with causal intervention is utilized to find the true causal effect, which makes the training process fundamentally different from the traditional likelihood estimation. On the other hand, in inference process, we formulate the bias as the direct causal effect and remove it by pursuing the indirect causal effect with counterfactual reasoning. We conduct experiments on large-scale natural language inference and fact verification benchmarks, evaluating on bias sensitive datasets that are specifically designed to assess the robustness of models against known biases in the training data. Experimental results show that our proposed debiasing framework outperforms previous state-of-the-art debiasing methods while maintaining the original in-distribution performance.",
    "code_link": "https://github.com/TalSchuster/FeverSymmetric"
  },
  "aaai2022_main_chessasatestbedforlanguagemodelstatetracking": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Chess as a Testbed for Language Model State Tracking",
    "authors": [
      "Shubham Toshniwal",
      "Sam Wiseman",
      "Karen Livescu",
      "Kevin Gimpel"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21390",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21390/21139",
    "published": "2022-02",
    "summary": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. \u201cfull attention\u201d. Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.",
    "code_link": ""
  },
  "aaai2022_main_contrast-enhancedsemi-supervisedtextclassificationwithfewlabels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Contrast-Enhanced Semi-supervised Text Classification with Few Labels",
    "authors": [
      "Austin Cheng-Yun Tsai",
      "Sheng-Ya Lin",
      "Li-Chen Fu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21391",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21391/21140",
    "published": "2022-02",
    "summary": "Traditional text classification requires thousands of annotated data or an additional Neural Machine Translation (NMT) system, which are expensive to obtain in real applications. This paper presents a Contrast-Enhanced Semi-supervised Text Classification (CEST) framework under label-limited settings without incorporating any NMT systems. We propose a certainty-driven sample selection method and a contrast-enhanced similarity graph to utilize data more efficiently in self-training, alleviating the annotation-starving problem. The graph imposes a smoothness constraint on the unlabeled data to improve the coherence and the accuracy of pseudo-labels. Moreover, CEST formulates the training as a \u201clearning from noisy labels\u201d problem and performs the optimization accordingly. A salient feature of this formulation is the explicit suppression of the severe error propagation problem in conventional semi-supervised learning. With solely 30 labeled data per class for both training and validation dataset, CEST outperforms the previous state-of-the-art algorithms by 2.11% accuracy and only falls within the 3.04% accuracy range of fully-supervised pre-training language model fine-tuning on thousands of labeled data.",
    "code_link": ""
  },
  "aaai2022_main_hybridautoregressiveinferenceforscalablemulti-hopexplanationregeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hybrid Autoregressive Inference for Scalable Multi-Hop Explanation Regeneration",
    "authors": [
      "Marco Valentino",
      "Mokanarangan Thayaparan",
      "Deborah Ferreira",
      "Andr\u00e9 Freitas"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21392",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21392/21141",
    "published": "2022-02",
    "summary": "Regenerating natural language explanations in the scientific domain has been proposed as a benchmark to evaluate complex multi-hop and explainable inference. In this context, large language models can achieve state-of-the-art performance when employed as cross-encoder architectures and fine-tuned on human-annotated explanations. However, while much attention has been devoted to the quality of the explanations, the problem of performing inference efficiently is largely under studied. Cross-encoders, in fact, are intrinsically not scalable, possessing limited applicability to real-world scenarios that require inference on massive facts banks. To enable complex multi-hop reasoning at scale, this paper focuses on bi-encoder architectures, investigating the problem of scientific explanation regeneration at the intersection of dense and sparse models. Specifically, we present SCAR (for Scalable Autoregressive Inference), a hybrid framework that iteratively combines a Transformer-based bi-encoder with a sparse model of explanatory power, designed to leverage explicit inference patterns in the explanations. Our experiments demonstrate that the hybrid framework significantly outperforms previous sparse models, achieving performance comparable with that of state-of-the-art cross-encoders while being approx 50 times faster and scalable to corpora of millions of facts. Further analyses on semantic drift and multi-hop question answering reveal that the proposed hybridisation boosts the quality of the most challenging explanations, contributing to improved performance on downstream inference tasks.",
    "code_link": ""
  },
  "aaai2022_main_detiemultilingualopeninformationextractioninspiredbyobjectdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DetIE: Multilingual Open Information Extraction Inspired by Object Detection",
    "authors": [
      "Michael Vasilkovsky",
      "Anton Alekseev",
      "Valentin Malykh",
      "Ilya Shenbin",
      "Elena\n      Tutubalina",
      "Dmitriy Salikhov",
      "Mikhail Stepnov",
      "Andrey Chertok",
      "Sergey\n      Nikolenko"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21393",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21393/21142",
    "published": "2022-02",
    "summary": "State of the art neural methods for open information extraction (OpenIE) usually extract triplets (or tuples) iteratively in an autoregressive or predicate-based manner in order not to produce duplicates. In this work, we propose a different approach to the problem that can be equally or more successful. Namely, we present a novel single-pass method for OpenIE inspired by object detection algorithms from computer vision. We use an order-agnostic loss based on bipartite matching that forces unique predictions and a Transformer-based encoder-only architecture for sequence labeling. The proposed approach is faster and shows superior or similar performance in comparison with state of the art models on standard benchmarks in terms of both quality metrics and inference time. Our model sets the new state of the art performance of 67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference than previous state of the art. We also evaluate the multilingual version of our model in the zero-shot setting for two languages and introduce a strategy for generating synthetic multilingual data to fine-tune the model for each specific language. In this setting, we show performance improvement of 15% on multilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish languages. Code and models are available at https://github.com/sberbank-ai/DetIE.",
    "code_link": ""
  },
  "aaai2022_main_hybridneuralnetworksforon-devicedirectionalhearing": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hybrid Neural Networks for On-Device Directional Hearing",
    "authors": [
      "Anran Wang",
      "Maruchi Kim",
      "Hao Zhang",
      "Shyamnath Gollakota"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21394",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21394/21143",
    "published": "2022-02",
    "summary": "On-device directional hearing requires audio source separation from a given direction while achieving stringent human-imperceptible latency requirements. While neural nets can achieve significantly better performance than traditional beamformers, all existing models fall short of supporting low-latency causal inference on computationally-constrained wearables. We present DeepBeam, a hybrid model that combines traditional beamformers with a custom lightweight neural net. The former reduces the computational burden of the latter and also improves its generalizability, while the latter is designed to further reduce the memory and computational overhead to enable real-time and low-latency operations. Our evaluation shows comparable performance to state-of-the-art causal inference models on synthetic data while achieving a 5x reduction of model size, 4x reduction of computation per second, 5x reduction in processing time and generalizing better to real hardware data. Further, our real-time hybrid model runs in 8 ms on mobile CPUs designed for low-power wearable devices and achieves an end-to-end latency of 17.5 ms.",
    "code_link": ""
  },
  "aaai2022_main_non-parametriconlinelearningfromhumanfeedbackforneuralmachinetranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Non-parametric Online Learning from Human Feedback for Neural Machine Translation",
    "authors": [
      "Dongqi Wang",
      "Haoran Wei",
      "Zhirui Zhang",
      "Shujian Huang",
      "Jun Xie",
      "Jiajun Chen"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21395",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21395/21144",
    "published": "2022-02",
    "summary": "We study the problem of online learning with human feedback in the human-in-the-loop machine translation, in which the human translators revise the machine-generated translations and then the corrected translations are used to improve the neural machine translation (NMT) system. However, previous methods require online model updating or additional translation memory networks to achieve high-quality performance, making them inflexible and inefficient in practice.In this paper, we propose a novel non-parametric online learning method without changing the model structure.This approach introduces two k-nearest-neighbor (KNN) modules: one module memorizes the human feedback, which is the correct sentences provided by human translators, while the other balances the usage of the history human feedback and original NMT models adaptively. Experiments conducted on EMEA and JRC-Acquis benchmarks demonstrate that our proposed method obtains substantial improvements on translation accuracy and achieves better adaptation performance with less repeating human correction operations.",
    "code_link": ""
  },
  "aaai2022_main_parameterdifferentiationbasedmultilingualneuralmachinetranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Parameter Differentiation Based Multilingual Neural Machine Translation",
    "authors": [
      "Qian Wang",
      "Jiajun Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21396",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21396/21145",
    "published": "2022-02",
    "summary": "Multilingual neural machine translation (MNMT) aims to translate multiple languages with a single model and has been proved successful thanks to effective knowledge transfer among different languages with shared parameters. However, it is still an open question which parameters should be shared and which ones need to be task-specific. Currently, the common practice is to heuristically design or search language-specific modules, which is difficult to find the optimal configuration. In this paper, we propose a novel parameter differentiation based method that allows the model to determine which parameters should be language-speci\ufb01c during training. Inspired by cellular differentiation, each shared parameter in our method can dynamically differentiate into more specialized types. We further de\ufb01ne the differentiation criterion as inter-task gradient similarity. Therefore, parameters with con\ufb02icting inter-task gradients are more likely to be language-specific. Extensive experiments on multilingual datasets have demonstrated that our method signi\ufb01cantly outperforms various strong baselines with different parameter sharing con\ufb01gurations. Further analysis reveals that the parameter sharing configuration obtained by our method correlates well with the linguistic proximities.",
    "code_link": "https://github.com/mjpost/sacrebleu"
  },
  "aaai2022_main_disencitegraph-baseddisentangledrepresentationlearningforcontext-specificcitationgeneration": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DisenCite: Graph-Based Disentangled Representation Learning for Context-Specific Citation Generation",
    "authors": [
      "Yifan Wang",
      "Yiping Song",
      "Shuai Li",
      "Chaoran Cheng",
      "Wei Ju",
      "Ming Zhang",
      "Sheng Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21397",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21397/21146",
    "published": "2022-02",
    "summary": "Citing and describing related literature are crucial to scientific writing. Many existing approaches show encouraging performance in citation recommendation, but are unable to accomplish the more challenging and onerous task of citation text generation. In this paper, we propose a novel disentangled representation based model DisenCite to automatically generate the citation text through integrating paper text and citation graph. A key novelty of our method compared with existing approaches is to generate context-specific citation text, empowering the generation of different types of citations for the same paper. In particular, we first build and make available a graph enhanced contextual citation dataset (GCite) with 25K edges in different types characterized by citation contained sections over 4.8K research papers. Based on this dataset, we encode each paper according to both textual contexts and structure information in the heterogeneous citation graph. The resulted paper representations are then disentangled by the mutual information regularization between this paper and its neighbors in graph. Extensive experiments demonstrate the superior performance of our method comparing to state-of-the-art approaches. We further conduct ablation and case studies to reassure that the improvement of our method comes from generating the context-specific citation through incorporating the citation graph.",
    "code_link": "https://github.com/jamesyifan/DisenCite"
  },
  "aaai2022_main_healaknowledgegraphfordistressmanagementconversations": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "HEAL: A Knowledge Graph for Distress Management Conversations",
    "authors": [
      "Anuradha Welivita",
      "Pearl Pu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21398",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21398/21147",
    "published": "2022-02",
    "summary": "The demands of the modern world are increasingly responsible for causing psychological burdens and bringing adverse impacts on our mental health. As a result, neural conversational agents with empathetic responding and distress management capabilities have recently gained popularity. However, existing end-to-end empathetic conversational agents often generate generic and repetitive empathetic statements such as \"I am sorry to hear that\", which fail to convey specificity to a given situation. Due to the lack of controllability in such models, they also impose the risk of generating toxic responses. Chatbots leveraging reasoning over knowledge graphs is seen as an efficient and fail-safe solution over end-to-end models. However, such resources are limited in the context of emotional distress. To address this, we introduce HEAL, a knowledge graph developed based on 1M distress narratives and their corresponding consoling responses curated from Reddit. It consists of 22K nodes identifying different types of stressors, speaker expectations, responses, and feedback types associated with distress dialogues and forms 104K connections between different types of nodes. Each node is associated with one of 41 affective states. Statistical and visual analysis conducted on HEAL reveals emotional dynamics between speakers and listeners in distress-oriented conversations and identifies useful response patterns leading to emotional relief. Automatic and human evaluation experiments show that HEAL's responses are more diverse, empathetic, and reliable compared to the baselines.",
    "code_link": "https://github.com/anuradha1992/HEAL"
  },
  "aaai2022_main_deepfusingpre-trainedmodelsintoneuralmachinetranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Deep Fusing Pre-trained Models into Neural Machine Translation",
    "authors": [
      "Rongxiang Weng",
      "Heng Yu",
      "Weihua Luo",
      "Min Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21399",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21399/21148",
    "published": "2022-02",
    "summary": "Pre-training and fine-tuning have become the de facto paradigm in many natural language processing (NLP) tasks. However, compared to other NLP tasks, neural machine translation (NMT) aims to generate target language sentences through the contextual representation from the source language counterparts. This characteristic means the optimization objective of NMT is far from that of the universal pre-trained models (PTMs), leading to the standard procedure of pre-training and fine-tuning does not work well in NMT. In this paper, we propose a novel framework to deep fuse the pre-trained representation into NMT, fully exploring the potential of PTMs in NMT. Specifically, we directly replace the randomly initialized Transformer encoder with a pre-trained encoder and propose a layer-wise coordination structure to coordinate PTM and NMT decoder learning. Then, we introduce a partitioned multi-task learning method to fine-tune the pre-trained parameter, reducing the gap between PTM and NMT by progressively learning the task-specific representation. Experimental results show that our approach achieves considerable improvements on WMT14 En2De, WMT14 En2Fr, and WMT16 Ro2En translation benchmarks and outperforms previous work in both autoregressive and non-autoregressive NMT models.",
    "code_link": "https://github.com/google-research/bert"
  },
  "aaai2022_main_vastthevalence-assessingsemanticstestforcontextualizinglanguagemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models",
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21400",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21400/21149",
    "published": "2022-02",
    "summary": "We introduce VAST, the Valence-Assessing Semantics Test, a novel intrinsic evaluation task for contextualized word embeddings (CWEs). Despite the widespread use of contextualizing language models (LMs), researchers have no intrinsic evaluation task for understanding the semantic quality of CWEs and their unique properties as related to contextualization, the change in the vector representation of a word based on surrounding words; tokenization, the breaking of uncommon words into subcomponents; and LM-specific geometry learned during training. VAST uses valence, the association of a word with pleasantness, to measure the correspondence of word-level LM semantics with widely used human judgments, and examines the effects of contextualization, tokenization, and LM-specific geometry. Because prior research has found that CWEs from OpenAI's 2019 English-language causal LM GPT-2 perform poorly on other intrinsic evaluations, we select GPT-2 as our primary subject, and include results showing that VAST is useful for 7 other LMs, and can be used in 7 languages. GPT-2 results show that the semantics of a word are more similar to the semantics of context in layers closer to model output, such that VAST scores diverge between our contextual settings, ranging from Pearson\u2019s rho of .55 to .77 in layer 11. We also show that multiply tokenized words are not semantically encoded until layer 8, where they achieve Pearson\u2019s rho of .46, indicating the presence of an encoding process for multiply tokenized words which differs from that of singly tokenized words, for which rho is highest in layer 0. We find that a few neurons with values having greater magnitude than the rest mask word-level semantics in GPT-2\u2019s top layer, but that word-level semantics can be recovered by nullifying non-semantic principal components: Pearson\u2019s rho in the top layer improves from .32 to .76. Downstream POS tagging and sentence classification experiments indicate that the GPT-2 uses these principal components for non-semantic purposes, such as to represent sentence-level syntax relevant to next-word prediction. After isolating semantics, we show the utility of VAST for understanding LM semantics via improvements over related work on four word similarity tasks, with a score of .50 on SimLex-999, better than the previous best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests, which compare differences in word embedding associations between groups of words, exhibit more stereotype-congruent biases after isolating semantics, indicating that non-semantic structures in LMs also mask social biases.",
    "code_link": "https://github.com/kingoflolz/mesh-transformer-jax"
  },
  "aaai2022_main_alabeldependence-awaresequencegenerationmodelformulti-levelimplicitdiscourserelationrecognition": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Label Dependence-Aware Sequence Generation Model for Multi-Level Implicit Discourse Relation Recognition",
    "authors": [
      "Changxing Wu",
      "Liuwen Cao",
      "Yubin Ge",
      "Yang Liu",
      "Min Zhang",
      "Jinsong Su"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21401",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21401/21150",
    "published": "2022-02",
    "summary": "Implicit discourse relation recognition (IDRR) is a challenging but crucial task in discourse analysis. Most existing methods train multiple models to predict multi-level labels independently, while ignoring the dependence between hierarchically structured labels. In this paper, we consider multi-level IDRR as a conditional label sequence generation task and propose a Label Dependence-aware Sequence Generation Model (LDSGM) for it. Specifically, we first design a label attentive encoder to learn the global representation of an input instance and its level-specific contexts, where the label dependence is integrated to obtain better label embeddings. Then, we employ a label sequence decoder to output the predicted labels in a top-down manner, where the predicted higher-level labels are directly used to guide the label prediction at the current level. We further develop a mutual learning enhanced training method to exploit the label dependence in a bottom-up direction, which is captured by an auxiliary decoder introduced during training. Experimental results on the PDTB dataset show that our model achieves the state-of-the-art performance on multi-level IDRR. We release our code at https://github.com/nlpersECJTU/LDSGM.",
    "code_link": "https://github.com/nlpersECJTU/LDSGM"
  },
  "aaai2022_main_fastandconstrainedabsentkeyphrasegenerationbyprompt-basedlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fast and Constrained Absent Keyphrase Generation by Prompt-Based Learning",
    "authors": [
      "Huanqin Wu",
      "Baijiaxin Ma",
      "Wei Liu",
      "Tao Chen",
      "Dan Nie"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21402",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21402/21151",
    "published": "2022-02",
    "summary": "Generating absent keyphrases, which do not appear in the input document, is challenging in the keyphrase prediction task. Most previous works treat the problem as an autoregressive sequence-to-sequence generation task, which demonstrates promising results for generating grammatically correct and fluent absent keyphrases. However, such an end-to-end process with a complete data-driven manner is unconstrained, which is prone to generate keyphrases inconsistent with the input document. In addition, the existing autoregressive decoding method makes the generation of keyphrases must be done from left to right, leading to slow speed during inference. In this paper, we propose a constrained absent keyphrase generation method in a prompt-based learning fashion. Specifically, the prompt will be created firstly based on the keywords, which are defined as the overlapping words between absent keyphrase and document. Then, a mask-predict decoder is used to complete the absent keyphrase on the constraint of prompt. Experiments on keyphrase generation benchmarks have demonstrated the effectiveness of our approach. In addition, we evaluate the performance of constrained absent keyphrases generation from an information retrieval perspective. The result shows that our approach can generate more consistent keyphrases, which can improve document retrieval performance. What\u2019s more, with a non-autoregressive decoding manner, our model can speed up the absent keyphrase generation by 8.67\u00d7 compared with the autoregressive method.",
    "code_link": ""
  },
  "aaai2022_main_graphmemdialogoptimizingend-to-endtask-orienteddialogsystemsusinggraphmemorynetworks": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "GraphMemDialog: Optimizing End-to-End Task-Oriented Dialog Systems Using Graph Memory Networks",
    "authors": [
      "Jie Wu",
      "Ian G Harris",
      "Hongzhi Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21403",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21403/21152",
    "published": "2022-02",
    "summary": "Effectively integrating knowledge into end-to-end task-oriented dialog systems remains a challenge. It typically requires incorporation of an external knowledge base (KB) and capture of the intrinsic semantics of the dialog history. Recent research shows promising results by using Sequence-to-Sequence models, Memory Networks, and even Graph Convolutional Networks. However, current state-of-the-art models are less effective at integrating dialog history and KB into task-oriented dialog systems in the following ways: 1. The KB representation is not fully context-aware. The dynamic interaction between the dialog history and KB is seldom explored. 2. Both the sequential and structural information in the dialog history can contribute to capturing the dialog semantics, but they are not studied concurrently. In this paper, we propose a novel Graph Memory Network (GMN) based Seq2Seq model, GraphMemDialog, to effectively learn the inherent structural information hidden in dialog history, and to model the dynamic interaction between dialog history and KBs. We adopt a modified graph attention network to learn the rich structural representation of the dialog history, whereas the context-aware representation of KB entities are learnt by our novel GMN. To fully exploit this dynamic interaction, we design a learnable memory controller coupled with external KB entity memories to recurrently incorporate dialog history context into KB entities through a multi-hop reasoning mechanism. Experiments on three public datasets show that our GraphMemDialog model achieves state-of-the-art performance and outperforms strong baselines by a large margin, especially on datatests with more complicated KB information.",
    "code_link": ""
  },
  "aaai2022_main_masteringtheexplicitopinion-roleinteractionsyntax-aidedneuraltransitionsystemforunifiedopinionrolelabeling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Mastering the Explicit Opinion-Role Interaction: Syntax-Aided Neural Transition System for Unified Opinion Role Labeling",
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Fei Li",
      "Meishan Zhang",
      "Yijiang Liu",
      "Chong Teng",
      "Donghong Ji"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21404",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21404/21153",
    "published": "2022-02",
    "summary": "Unified opinion role labeling (ORL) aims to detect all possible opinion structures of 'opinion-holder-target' in one shot, given a text. The existing transition-based unified method, unfortunately, is subject to longer opinion terms and fails to solve the term overlap issue. Current top performance has been achieved by employing the span-based graph model, which however still suffers from both high model complexity and insufficient interaction among opinions and roles. In this work, we investigate a novel solution by revisiting the transition architecture, and augmenting it with a pointer network (PointNet). The framework parses out all opinion structures in linear-time complexity, meanwhile breaks through the limitation of any length of terms with PointNet. To achieve the explicit opinion-role interactions, we further propose a unified dependency-opinion graph (UDOG), co-modeling the syntactic dependency structure and the partial opinion-role structure. We then devise a relation-centered graph aggregator (RCGA) to encode the multi-relational UDOG, where the resulting high-order representations are used to promote the predictions in the vanilla transition system. Our model achieves new state-of-the-art results on the MPQA benchmark. Analyses further demonstrate the superiority of our methods on both efficacy and efficiency.",
    "code_link": "https://github.com/ChocoWu/SyPtrTrans-ORL"
  },
  "aaai2022_main_agraphconvolutionalnetworkwithadaptivegraphgenerationandchannelselectionforeventdetection": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "A Graph Convolutional Network with Adaptive Graph Generation and Channel Selection for Event Detection",
    "authors": [
      "Zhipeng Xie",
      "Yumin Tu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21405",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21405/21154",
    "published": "2022-02",
    "summary": "Graph convolutional networks have been successfully applied to the task of event detection. However, existing works rely heavily on a fixed syntactic parse tree structure from an external parser. In addition, the information content extracted for aggregation is determined simply by the (syntactic) edge direction or type but irrespective of what semantics the vertices have, which is somewhat rigid. With this work, we propose a novel graph convolutional method that combines an adaptive graph generation technique and a multi-channel selection strategy. The adaptive graph generation technique enables the gradients to pass through the graph sampling layer by using the ST-Gumbel-Softmax trick. The multi-channel selection strategy allows two adjacent vertices to automatically determine which information channels to get through for information extraction and aggregation. The proposed method achieves the state-of-the-art performance on ACE2005 dataset.",
    "code_link": "https://github.com/UniversalDependencies/UD"
  },
  "aaai2022_main_leashingtheinnerdemonsself-detoxificationforlanguagemodels": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Leashing the Inner Demons: Self-Detoxification for Language Models",
    "authors": [
      "Canwen Xu",
      "Zexue He",
      "Zhankui He",
      "Julian McAuley"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21406",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21406/21155",
    "published": "2022-02",
    "summary": "Language models (LMs) can reproduce (or amplify) toxic language seen during training, which poses a risk to their practical application. In this paper, we conduct extensive experiments to study this phenomenon. We analyze the impact of prompts, decoding strategies and training corpora on the output toxicity. Based on our findings, we propose a simple yet effective unsupervised method for language models to ``detoxify'' themselves without an additional large corpus or external discriminator. Compared to a supervised baseline, our proposed method shows better toxicity reduction with good generation quality in the generated content under multiple settings. Warning: some examples shown in the paper may contain uncensored offensive content.",
    "code_link": ""
  },
  "aaai2022_main_zero-shotcross-lingualmachinereadingcomprehensionviainter-sentencedependencygraph": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-sentence Dependency Graph",
    "authors": [
      "Liyan Xu",
      "Xuchao Zhang",
      "Bo Zong",
      "Yanchi Liu",
      "Wei Cheng",
      "Jingchao Ni",
      "Haifeng Chen",
      "Liang Zhao",
      "Jinho D. Choi"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21407",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21407/21156",
    "published": "2022-02",
    "summary": "We target the task of cross-lingual Machine Reading Comprehension (MRC) in the direct zero-shot setting, by incorporating syntactic features from Universal Dependencies (UD), and the key features we use are the syntactic relations within each sentence. While previous work has demonstrated effective syntax-guided MRC models, we propose to adopt the inter-sentence syntactic relations, in addition to the rudimentary intra-sentence relations, to further utilize the syntactic dependencies in the multi-sentence input of the MRC task. In our approach, we build the Inter-Sentence Dependency Graph (ISDG) connecting dependency trees to form global syntactic relations across sentences. We then propose the ISDG encoder that encodes the global dependency graph, addressing the inter-sentence relations via both one-hop and multi-hop dependency paths explicitly. Experiments on three multilingual MRC datasets (XQuAD, MLQA, TyDiQA-GoldP) show that our encoder that is only trained on English is able to improve the zero-shot performance on all 14 test sets covering 8 languages, with up to 3.8 F1 / 5.2 EM improvement on-average, and 5.2 F1 / 11.2 EM on certain languages. Further analysis shows the improvement can be attributed to the attention on the cross-linguistically consistent syntactic path. Our code is available at https://github.com/lxucs/multilingual-mrc-isdg.",
    "code_link": ""
  },
  "aaai2022_main_fromdensetosparsecontrastivepruningforbetterpre-trainedlanguagemodelcompression": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression",
    "authors": [
      "Runxin Xu",
      "Fuli Luo",
      "Chengyu Wang",
      "Baobao Chang",
      "Jun Huang",
      "Songfang\n      Huang",
      "Fei Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21408",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21408/21157",
    "published": "2022-02",
    "summary": "Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. It is designed as a general framework, compatible with both structured and unstructured pruning. Unified in contrastive learn- ing, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability.",
    "code_link": ""
  },
  "aaai2022_main_sequencelevelcontrastivelearningfortextsummarization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Sequence Level Contrastive Learning for Text Summarization",
    "authors": [
      "Shusheng Xu",
      "Xingxing Zhang",
      "Yi Wu",
      "Furu Wei"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21409",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21409/21158",
    "published": "2022-02",
    "summary": "Contrastive learning models have achieved great success in unsupervised visual representation learning, which maximize the similarities between feature representations of different views of the same image, while minimize the similarities between feature representations of views of different images. In text summarization, the output summary is a shorter form of the input document and they have similar meanings. In this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a document, its gold summary and its model generated summaries as different views of the same mean representation and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model achieves better faithfulness ratings compared to its counterpart without contrastive objectives. We release our code at https://github.com/xssstory/SeqCo.",
    "code_link": ""
  },
  "aaai2022_main_self-supervisedknowledgeassimilationforexpert-laymantextstyletransfer": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer",
    "authors": [
      "Wenda Xu",
      "Michael Saxon",
      "Misha Sra",
      "William Yang Wang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21410",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21410/21159",
    "published": "2022-02",
    "summary": "Expert-layman text style transfer technologies have the potential to improve communication between members of scientific communities and the general public. High-quality information produced by experts is often filled with difficult jargon laypeople struggle to understand. This is a particularly notable issue in the medical domain, where layman are often confused by medical text online. At present, two bottlenecks interfere with the goal of building high-quality medical expert-layman style transfer systems: a dearth of pretrainedmedical-domain language models spanning both expert and layman terminologies and a lack of parallel corpora for training the transfer task itself. To mitigate the first issue, we propose a novel language model (LM) pretraining task, Knowledge Base Assimilation, to synthesize pretraining data from the edges of a graph of expert- and layman-style medical terminology terms into an LM during self-supervised learning. To mitigate the second issue, we build a large-scale parallel corpus in the medical expert-layman domain using a margin-based criterion. Our experiments show that transformer-based models pretrained on knowledge base assimilation and other well-established pretraining tasks fine-tuning on our new parallel corpus leads to considerable improvement against expert-layman transfer benchmarks, gaining an average relative improvement of our human evaluation, the Overall Success Rate (OSR), by 106%.",
    "code_link": "https://github.com/xu1998hz/SSL_KBA"
  },
  "aaai2022_main_textisnomoreenough!abenchmarkforprofile-basedspokenlanguageunderstanding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Text Is No More Enough! A Benchmark for Profile-Based Spoken Language Understanding",
    "authors": [
      "Xiao Xu",
      "Libo Qin",
      "Kaiji Chen",
      "Guoxing Wu",
      "Linlin Li",
      "Wanxiang Che"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21411",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21411/21160",
    "published": "2022-02",
    "summary": "Current researches on spoken language understanding (SLU) heavily are limited to a simple setting: the plain text-based SLU that takes the user utterance as input and generates its corresponding semantic frames (e.g., intent and slots). Unfortunately, such a simple setting may fail to work in complex real-world scenarios when an utterance is semantically ambiguous, which cannot be achieved by the text-based SLU models. In this paper, we first introduce a new and important task, Profile-based Spoken Language Understanding (ProSLU), which requires the model that not only relies on the plain text but also the supporting profile information to predict the correct intents and slots. To this end, we further introduce a large-scale human-annotated Chinese dataset with over 5K utterances and their corresponding supporting profile information (Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition, we evaluate several state-of-the-art baseline models and explore a multi-level knowledge adapter to effectively incorporate profile information. Experimental results reveal that all existing text-based SLU models fail to work when the utterances are semantically ambiguous and our proposed framework can effectively fuse the supporting information for sentence-level intent detection and token-level slot filling. Finally, we summarize key challenges and provide new points for future directions, which hopes to facilitate the research.",
    "code_link": "https://github.com/LooperXX/ProSLU"
  },
  "aaai2022_main_sasself-augmentationstrategyforlanguagemodelpre-training": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "SAS: Self-Augmentation Strategy for Language Model Pre-training",
    "authors": [
      "Yifei Xu",
      "Jingqiao Zhang",
      "Ru He",
      "Liangzhu Ge",
      "Chao Yang",
      "Cheng Yang",
      "Ying\n      Nian Wu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21412",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21412/21161",
    "published": "2022-02",
    "summary": "The core of self-supervised learning for pre-training language models includes pre-training task design as well as appropriate data augmentation. Most data augmentations in language model pre-training are context-independent. A seminal contextualized augmentation was recently proposed in ELECTRA and achieved state-of-the-art performance by introducing an auxiliary generation network (generator) to produce contextualized data augmentation for the training of a main discrimination network (discriminator). This design, however, introduces extra computation cost of the generator and a need to adjust the relative capability between the generator and the discriminator. In this paper, we propose a self-augmentation strategy (SAS) where a single network is utilized for both regular pre-training and contextualized data augmentation for the training in later epochs. Essentially, this strategy eliminates a separate generator and uses the single network to jointly conduct two pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced Token Detection) heads. It avoids the challenge to search for an appropriate size of the generator, which is critical to the performance as evidenced in ELECTRA and its subsequent variant models. In addition, SAS is a general strategy that can be seamlessly combined with many new techniques emerging recently or in the future, such as the disentangled attention mechanism from DeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other state-of-the-art models in the GLUE tasks with similar or less computation cost.",
    "code_link": "https://github.com/alibaba/self-augmentation-strategy"
  },
  "aaai2022_main_hybridcurriculumlearningforemotionrecognitioninconversation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hybrid Curriculum Learning for Emotion Recognition in Conversation",
    "authors": [
      "Lin Yang",
      "YI Shen",
      "Yue Mao",
      "Longjun Cai"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21413",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21413/21162",
    "published": "2022-02",
    "summary": "Emotion recognition in conversation (ERC) aims to detect the emotion label for each utterance.Motivated by recent studies which have proven that feeding training examples in a meaningful order rather than considering them randomly can boost the performance of models, we propose an ERC-oriented hybrid curriculum learning framework. Our framework consists of two curricula: (1) conversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In CC, we construct a difficulty measurer based on ``emotion shift'' frequency within a conversation, then the conversations are scheduled in an ``easy to hard\" schema according to the difficulty score returned by the difficulty measurer. For UC, it is implemented from an emotion-similarity perspective, which progressively strengthens the model\u2019s ability in identifying the confusing emotions. With the proposed model-agnostic hybrid curriculum learning strategy, we observe significant performance boosts over a wide range of existing ERC models and we are able to achieve new state-of-the-art results on four public ERC datasets.",
    "code_link": ""
  },
  "aaai2022_main_numhtmlnumeric-orientedhierarchicaltransformermodelformulti-taskfinancialforecasting": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-Task Financial Forecasting",
    "authors": [
      "Linyi Yang",
      "Jiazheng Li",
      "Ruihai Dong",
      "Yue Zhang",
      "Barry Smyth"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21414",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21414/21163",
    "published": "2022-02",
    "summary": "Financial forecasting has been an important and active area of machine learning research because of the challenges it presents and the potential rewards that even minor improvements in prediction accuracy or forecasting may entail. Traditionally, financial forecasting has heavily relied on quantitative indicators and metrics derived from structured financial statements. Earnings conference call data, including text and audio, is an important source of unstructured data that has been used for various prediction tasks using deep earning and related approaches. However, current deep learning-based methods are limited in the way that they deal with numeric data; numbers are typically treated as plain-text tokens without taking advantage of their underlying numeric structure. This paper describes a numeric-oriented hierarchical transformer model (NumHTML) to predict stock returns, and financial risk using multi-modal aligned earnings calls data by taking advantage of the different categories of numbers (monetary, temporal, percentages etc.) and their magnitude. We present the results of a comprehensive evaluation of NumHTML against several state-of-the-art baselines using a real-world publicly available dataset. The results indicate that NumHTML significantly outperforms the current state-of-the-art across a variety of evaluation metrics and that it has the potential to offer significant financial gains in a practical trading context.",
    "code_link": ""
  },
  "aaai2022_main_tracingtextprovenanceviacontext-awarelexicalsubstitution": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Tracing Text Provenance via Context-Aware Lexical Substitution",
    "authors": [
      "Xi Yang",
      "Jie Zhang",
      "Kejiang Chen",
      "Weiming Zhang",
      "Zehua Ma",
      "Feng Wang",
      "Nenghai Yu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21415",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21415/21164",
    "published": "2022-02",
    "summary": "Text content created by humans or language models is often stolen or misused by adversaries. Tracing text provenance can help claim the ownership of text content or identify the malicious users who distribute misleading content like machine-generated fake news. There have been some attempts to achieve this, mainly based on watermarking techniques. Specifically, traditional text watermarking methods embed watermarks by slightly altering text format like line spacing and font, which, however, are fragile to cross-media transmissions like OCR. Considering this, natural language watermarking methods represent watermarks by replacing words in original sentences with synonyms from handcrafted lexical resources (e.g., WordNet), but they do not consider the substitution\u2019s impact on the overall sentence's meaning. Recently, a transformer-based network was proposed to embed watermarks by modifying the unobtrusive words (e.g., function words), which also impair the sentence's logical and semantic coherence. Besides, one well-trained network fails on other different types of text content.To address the limitations mentioned above, we propose a natural language watermarking scheme based on context-aware lexical substitution (LS). Specifically, we employ BERT to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Based on this, a selection strategy in terms of synchronicity and substitutability is further designed to test whether a word is exactly suitable for carrying the watermark signal. Extensive experiments demonstrate that, under both objective and subjective metrics, our watermarking scheme can well preserve the semantic integrity of original sentences and has a better transferability than existing methods. Besides, the proposed LS approach outperforms the state-of-the-art approach on the Stanford Word Substitution Benchmark.",
    "code_link": ""
  },
  "aaai2022_main_fusingtask-orientedandopen-domaindialoguesinconversationalagents": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents",
    "authors": [
      "Tom Young",
      "Frank Xing",
      "Vlad Pandelea",
      "Jinjie Ni",
      "Erik Cambria"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21416",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21416/21165",
    "published": "2022-02",
    "summary": "The goal of building intelligent dialogue systems has largely been separately pursued under two paradigms: task-oriented dialogue (TOD) systems, which perform task-specific functions, and open-domain dialogue (ODD) systems, which focus on non-goal-oriented chitchat. The two dialogue modes can potentially be intertwined together seamlessly in the same conversation, as easily done by a friendly human assistant. Such ability is desirable in conversational agents, as the integration makes them more accessible and useful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn dialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset FusedChat, by rewriting the existing TOD turns and adding new ODD turns. This procedure constructs conversation sessions containing exchanges from both dialogue modes. It features inter-mode contextual dependency, i.e., the dialogue turns from the two modes depend on each other. Rich dependency patterns such as co-reference and ellipsis are included. The new dataset, with 60k new human-written ODD turns and 5k re-written TOD turns, offers a benchmark to test a dialogue model's ability to perform inter-mode conversations. This is a more challenging task since the model has to determine the appropriate dialogue mode and generate the response based on the inter-mode context. However, such models would better mimic human-level conversation capabilities. We evaluate two baseline models on this task, including the classification-based two-stage models and the two-in-one fused models. We publicly release FusedChat and the baselines to propel future work on inter-mode dialogue systems.",
    "code_link": "https://github.com/tomyoung903/FusedChat"
  },
  "aaai2022_main_jaketjointpre-trainingofknowledgegraphandlanguageunderstanding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding",
    "authors": [
      "Donghan Yu",
      "Chenguang Zhu",
      "Yiming Yang",
      "Michael Zeng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21417",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21417/21166",
    "published": "2022-02",
    "summary": "Knowledge graphs (KGs) contain rich information about world knowledge, entities, and relations. Thus, they can be great supplements to existing pre-trained language models. However, it remains a challenge to efficiently integrate information from KG into language modeling. And the understanding of a knowledge graph requires related context. We propose a novel joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embeddings for entities and relations in the graph. Our design enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains. Experiment results on several knowledge-aware NLP tasks show that our proposed framework achieves superior performance by effectively leveraging knowledge in language understanding.",
    "code_link": ""
  },
  "aaai2022_main_kid-reviewknowledge-guidedscientificreviewgenerationwithoraclepre-training": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training",
    "authors": [
      "Weizhe Yuan",
      "Pengfei Liu"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21418",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21418/21167",
    "published": "2022-02",
    "summary": "The surge in the number of scientific submissions has brought challenges to the work of peer review. In this paper, as a first step, we explore the possibility of designing an automated system, which is not meant to replace humans, but rather providing a first-pass draft for a machine-assisted human review process. Specifically, we present an end-to-end knowledge-guided review generation framework for scientific papers grounded in cognitive psychology research that a better understanding of text requires different types of knowledge. In practice, we found that this seemingly intuitive idea suffered from training difficulties. In order to solve this problem, we put forward an oracle pre-training strategy, which can not only make the Kid-Review better educated but also make the generated review cover more aspects. Experimentally, we perform a comprehensive evaluation (human and automatic) from different perspectives. Empirical results have shown the effectiveness of different types of knowledge as well as oracle pre-training. We make all code, relevant dataset available: https://github.com/Anonymous4nlp233/KIDReview as well as the Kid-Review system: http://nlpeer.reviews.",
    "code_link": "https://github.com/yyy-Apple/KIDReview"
  },
  "aaai2022_main_reference-basedspeechenhancementviafeaturealignmentandfusionnetwork": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Reference-Based Speech Enhancement via Feature Alignment and Fusion Network",
    "authors": [
      "Huanjing Yue",
      "Wenxin Duo",
      "Xiulian Peng",
      "Jingyu Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21419",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21419/21168",
    "published": "2022-02",
    "summary": "Speech enhancement aims at recovering a clean speech from a noisy input, which can be classified into single speech enhancement and personalized speech enhancement. Personalized speech enhancement usually utilizes the speaker identity extracted from the noisy speech itself (or a clean reference speech) as a global embedding to guide the enhancement process. Different from them, we observe that the speeches of the same speaker are correlated in terms of frame-level short-time Fourier Transform (STFT) spectrogram. Therefore, we propose reference-based speech enhancement via a feature alignment and fusion network (FAF-Net). Given a noisy speech and a clean reference speech spoken by the same speaker, we first propose a feature level alignment strategy to warp the clean reference with the noisy speech in frame level. Then, we fuse the reference feature with the noisy feature via a similarity-based fusion strategy. Finally, the fused features are skipped connected to the decoder, which generates the enhanced results. Experimental results demonstrate that the performance of the proposed FAF-Net is close to state-of-the-art speech enhancement methods on both DNS and Voice Bank+DEMAND datasets. Our code is available at https://github.com/HieDean/FAF-Net.",
    "code_link": "https://github.com/HieDean/FAF-Net"
  },
  "aaai2022_main_mdd-evalself-trainingonaugmenteddataformulti-domaindialogueevaluation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation",
    "authors": [
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Thomas Friedrichs",
      "Haizhou Li"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21420",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21420/21169",
    "published": "2022-02",
    "summary": "Chatbots are designed to carry out human-like conversations across different domains, such as general chit-chat, knowledge exchange, and persona-grounded conversations. To measure the quality of such conversational agents, a dialogue evaluator is expected to conduct assessment across domains as well. However, most of the state-of-the-art automatic dialogue evaluation metrics (ADMs) are not designed for multi-domain evaluation. We are motivated to design a general and robust framework, MDD-Eval, to address the problem. Specifically, we first train a teacher evaluator with human-annotated data to acquire a rating skill to tell good dialogue responses from bad ones in a particular domain and then, adopt a self-training strategy to train a new evaluator with teacher-annotated multi-domain data, that helps the new evaluator to generalize across multiple domains. MDD-Eval is extensively assessed on six dialogue evaluation benchmarks. Empirical results show that the MDD-Eval framework achieves a strong performance with an absolute improvement of 7% over the state-of-the-art ADMs in terms of mean Spearman correlation scores across all the evaluation benchmarks.",
    "code_link": "https://github.com/e0397123/MDD-Eval"
  },
  "aaai2022_main_efficientdialogpolicylearningbyreasoningwithcontextualknowledge": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Efficient Dialog Policy Learning by Reasoning with Contextual Knowledge",
    "authors": [
      "Haodi Zhang",
      "Zhichao Zeng",
      "Keting Lu",
      "Kaishun Wu",
      "Shiqi Zhang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21421",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21421/21170",
    "published": "2022-02",
    "summary": "Goal-oriented dialog policy learning algorithms aim to learn a dialog policy for selecting language actions based on the current dialog state. Deep reinforcement learning methods have been used for dialog policy learning. This work is motivated by the observation that, although dialog is a domain with rich contextual knowledge, reinforcement learning methods are ill-equipped to incorporate such knowledge into the dialog policy learning process. In this paper, we develop a deep reinforcement learning framework for goal-oriented dialog policy learning that learns user preferences from user goal data, while leveraging commonsense knowledge from people. The developed framework has been evaluated using a realistic dialog simulation platform. Compared with baselines from the literature and the ablations of our approach, we see significant improvements in learning efficiency and the quality of the computed action policies.",
    "code_link": "https://github.com/ResearchGroupHdZhang/DPL"
  },
  "aaai2022_main_hierarchicalcross-modalitysemanticcorrelationlearningmodelformultimodalsummarization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization",
    "authors": [
      "Litian Zhang",
      "Xiaoming Zhang",
      "Junshu Pan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21422",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21422/21171",
    "published": "2022-02",
    "summary": "Multimodal summarization with multimodal output (MSMO) generates a summary with both textual and visual content. Multimodal news report contains heterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed that different modalities of data in the news report correlate hierarchically. Traditional MSMO methods indistinguishably handle different modalities of data by learning a representation for the whole data, which is not directly adaptable to the heterogeneous contents and hierarchical correlation. In this paper, we propose a hierarchical cross-modality semantic correlation learning model (HCSCL) to learn the intra- and inter-modal correlation existing in the multimodal data. HCSCL adopts a graph network to encode the intra-modal correlation. Then, a hierarchical fusion framework is proposed to learn the hierarchical correlation between text and images. Furthermore, we construct a new dataset with relevant image annotation and image object label information to provide the supervision information for the learning procedure. Extensive experiments on the dataset show that HCSCL significantly outperforms the baseline methods in automatic summarization metrics and fine-grained diversity tests.",
    "code_link": "https://github.com/LitianD/HCSCL-MSDataset"
  },
  "aaai2022_main_adversarialdataaugmentationfortask-specificknowledgedistillationofpre-trainedtransformers": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers",
    "authors": [
      "Minjia Zhang",
      "Niranjan Uma Naresh",
      "Yuxiong He"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21423",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21423/21172",
    "published": "2022-02",
    "summary": "Deep and large pre-trained language models (e.g., BERT, GPT-3) are state-of-the-art for various natural language processing tasks. However, the huge size of these models brings challenges to fine-tuning and online deployment due to latency and cost constraints. Existing knowledge distillation methods reduce the model size, but they may encounter difficulties transferring knowledge from the teacher model to the student model due to the limited data from the downstream tasks. In this work, we propose AD^2, a novel and effective data augmentation approach to improving the task-specific knowledge transfer when compressing large pre-trained transformer models. Different from prior methods, AD^2 performs distillation by using an enhanced training set that contains both original inputs and adversarially perturbed samples that mimic the output distribution from the teacher. Experimental results show that this method allows better transfer of knowledge from the teacher to the student during distillation, producing student models that retain 99.6\\% accuracy of the teacher model while outperforming existing task-specific knowledge distillation baselines by 1.2 points on average over a variety of natural language understanding tasks. Moreover, compared with alternative data augmentation methods, such as text-editing-based approaches, AD^2 is up to 28 times faster while achieving comparable or higher accuracy. In addition, when AD^2 is combined with more advanced task-agnostic distillation, we can advance the state-of-the-art performance even more. On top of the encouraging performance, this paper also provides thorough ablation studies and analysis. The discovered interplay between KD and adversarial data augmentation for compressing pre-trained Transformers may further inspire more advanced KD algorithms for compressing even larger scale models.",
    "code_link": ""
  },
  "aaai2022_main_text-basedinteractiverecommendationviaofflinereinforcementlearning": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Text-Based Interactive Recommendation via Offline Reinforcement Learning",
    "authors": [
      "Ruiyi Zhang",
      "Tong Yu",
      "Yilin Shen",
      "Hongxia Jin"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21424",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21424/21173",
    "published": "2022-02",
    "summary": "Interactive recommendation with natural-language feedback can provide richer user feedback and has demonstrated advantages over traditional recommender systems. However, the classical online paradigm involves iteratively collecting experience via interaction with users, which is expensive and risky. We consider an offline interactive recommendation to exploit arbitrary experience collected by multiple unknown policies. A direct application of policy learning with such fixed experience suffers from the distribution shift. To tackle this issue, we develop a behavior-agnostic off-policy correction framework to make offline interactive recommendation possible. Specifically, we leverage theconservative Q-function to perform off-policy evaluation, which enables learning effective policies from fixed datasets without further interactions. Empirical results on the simulator derived from real-world datasets demonstrate the effectiveness of our proposed offline training framework.",
    "code_link": ""
  },
  "aaai2022_main_dkplmdecomposableknowledge-enhancedpre-trainedlanguagemodelfornaturallanguageunderstanding": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DKPLM: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural Language Understanding",
    "authors": [
      "Taolin Zhang",
      "Chengyu Wang",
      "Nan Hu",
      "Minghui Qiu",
      "Chengguang Tang",
      "Xiaofeng\n      He",
      "Jun Huang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21425",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21425/21174",
    "published": "2022-02",
    "summary": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained models with relation triples injecting from knowledge graphs to improve language understanding abilities.Experiments show that our model outperforms other KEPLMs significantly over zero-shot knowledge probing tasks and multiple knowledge-aware language understanding tasks. To guarantee effective knowledge injection, previous studies integrate models with knowledge encoders for representing knowledge retrieved from knowledge graphs. The operations for knowledge retrieval and encoding bring significant computational burdens, restricting the usage of such models in real-world applications that require high inference speed. In this paper, we propose a novel KEPLM named DKPLM thatdecomposes knowledge injection process of the pre-trained language models in pre-training, fine-tuning and inference stages, which facilitates the applications of KEPLMs in real-world scenarios. Specifically, we first detect knowledge-aware long-tail entities as the target for knowledge injection, enhancing the KEPLMs' semantic understanding abilities and avoiding injecting redundant information.The embeddings of long-tail entities are replaced by ``pseudo token representations'' formed by relevant knowledge triples. We further design the relational knowledge decoding task for pre-training to force the models to truly understand the injected knowledge by relation triple reconstruction. Experiments show that our model outperforms other KEPLMs significantly over zero-shot knowledge probing tasks and multiple knowledge-aware language understanding tasks. We further show that DKPLM has a higher inference speed than other competing models due to the decomposing mechanism.",
    "code_link": "https://github.com/attardi/wikiextractor"
  },
  "aaai2022_main_frequency-awarecontrastivelearningforneuralmachinetranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Frequency-Aware Contrastive Learning for Neural Machine Translation",
    "authors": [
      "Tong Zhang",
      "Wei Ye",
      "Baosong Yang",
      "Long Zhang",
      "Xingzhang Ren",
      "Dayiheng Liu",
      "Jinan Sun",
      "Shikun Zhang",
      "Haibo Zhang",
      "Wen Zhao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21426",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21426/21175",
    "published": "2022-02",
    "summary": "Low-frequency word prediction remains a challenge in modern neural machine translation (NMT) systems. Recent adaptive training methods promote the output of infrequent words by emphasizing their weights in the overall training objectives. Despite the improved recall of low-frequency words, their prediction precision is unexpectedly hindered by the adaptive objectives. Inspired by the observation that low-frequency words form a more compact embedding space, we tackle this challenge from a representation learning perspective. Specifically, we propose a frequency-aware token-level contrastive learning method, in which the hidden state of each decoding step is pushed away from the counterparts of other target words, in a soft contrastive way based on the corresponding word frequencies. We conduct experiments on widely used NIST Chinese-English and WMT14 English-German translation tasks. Empirical results show that our proposed methods can not only significantly improve the translation quality but also enhance lexical diversity and optimize word representation space. Further investigation reveals that, comparing with related adaptive training strategies, the superiority of our method on low-frequency word prediction lies in the robustness of token-level recall across different frequencies without sacrificing precision.",
    "code_link": ""
  },
  "aaai2022_main_probingwordsyntacticrepresentationsinthebrainbyafeatureeliminationmethod": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Probing Word Syntactic Representations in the Brain by a Feature Elimination Method",
    "authors": [
      "Xiaohan Zhang",
      "Shaonan Wang",
      "Nan Lin",
      "Jiajun Zhang",
      "Chengqing Zong"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21427",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21427/21176",
    "published": "2022-02",
    "summary": "Neuroimaging studies have identified multiple brain regions that are associated with semantic and syntactic processing when comprehending language. However, existing methods cannot explore the neural correlates of fine-grained word syntactic features, such as part-of-speech and dependency relations. This paper proposes an alternative framework to study how different word syntactic features are represented in the brain. To separate each syntactic feature, we propose a feature elimination method, called Mean Vector Null space Projection (MVNP). This method can remove a specific feature from word representations, resulting in one-feature-removed representations. Then we respectively associate one-feature-removed and the original word vectors with brain imaging data to explore how the brain represents the removed feature. This paper for the first time studies the cortical representations of multiple fine-grained syntactic features simultaneously and suggests some possible contributions of several brain regions to the complex division of syntactic processing. These findings indicate that the brain foundations of syntactic information processing might be broader than those suggested by classical studies.",
    "code_link": ""
  },
  "aaai2022_main_unsupervisedsentencerepresentationviacontrastivelearningwithmixingnegatives": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Unsupervised Sentence Representation via Contrastive Learning with Mixing Negatives",
    "authors": [
      "Yanzhao Zhang",
      "Richong Zhang",
      "Samuel Mensah",
      "Xudong Liu",
      "Yongyi Mao"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21428",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21428/21177",
    "published": "2022-02",
    "summary": "Unsupervised sentence representation learning is a fundamental problem in natural language processing. Recently, contrastive learning has made great success on this task. Existing constrastive learning based models usually apply random sampling to select negative examples for training. Previous work in computer vision has shown that hard negative examples help contrastive learning to achieve faster convergency and better optimization for representation learning. However, the importance of hard negatives in contrastive learning for sentence representation is yet to be explored. In this study, we prove that hard negatives are essential for maintaining strong gradient signals in the training process while random sampling negative examples is ineffective for sentence representation. Accordingly, we present a contrastive model, MixCSE, that extends the current state-of-the-art SimCSE by continually constructing hard negatives via mixing both positive and negative features. The superior performance of the proposed approach is demonstrated via empirical studies on Semantic Textual Similarity datasets and Transfer task datasets.",
    "code_link": ""
  },
  "aaai2022_main_retgenajointframeworkforretrievalandgroundedtextgenerationmodeling": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "RetGen: A Joint Framework for Retrieval and Grounded Text Generation Modeling",
    "authors": [
      "Yizhe Zhang",
      "Siqi Sun",
      "Xiang Gao",
      "Yuwei Fang",
      "Chris Brockett",
      "Michel\n      Galley",
      "Jianfeng Gao",
      "Bill Dolan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21429",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21429/21178",
    "published": "2022-02",
    "summary": "Recent advances in large-scale pre-training such as GPT-3 allow seemingly high quality text to be generated from a given prompt. However, such generation systems often suffer from problems of hallucinated facts, and are not inherently designed to incorporate useful external information. Grounded generation models appear to offer remedies, but their training typically relies on rarely-available parallel data where information-relevant documents are provided for context. We propose a framework that alleviates this data constraint by jointly training a grounded generator and document retriever on the language model signal. The model learns to reward retrieval of the documents with the highest utility in generation, and attentively combines them using a Mixture-of-Experts (MoE) ensemble to generate follow-on text.We demonstrate that both generator and retriever can take advantage of this joint training and work synergistically to produce more informative and relevant text in both prose and dialogue generation.",
    "code_link": ""
  },
  "aaai2022_main_birdqaabilingualdatasetforquestionansweringontrickyriddles": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles",
    "authors": [
      "Yunxiang Zhang",
      "Xiaojun Wan"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21430",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21430/21179",
    "published": "2022-02",
    "summary": "A riddle is a question or statement with double or veiled meanings, followed by an unexpected answer. Solving riddle is a challenging task for both machine and human, testing the capability of understanding figurative, creative natural language and reasoning with commonsense knowledge. We introduce BiRdQA, a bilingual multiple-choice question answering dataset with 6614 English riddles and 8751 Chinese riddles. For each riddle-answer pair, we provide four distractors with additional information from Wikipedia. The distractors are automatically generated at scale with minimal bias. Existing monolingual and multilingual QA models fail to perform well on our dataset, indicating that there is a long way to go before machine can beat human on solving tricky riddles. The dataset is publicly available at https://forms.gle/NvT7DfWhAPhvoFvH7.",
    "code_link": "https://github.com/fxsjy/jieba"
  },
  "aaai2022_main_unimsaunifiedframeworkformultimodalsummarizationwithknowledgedistillation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation",
    "authors": [
      "Zhengkun Zhang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Xin Jiang",
      "Qun Liu",
      "Zhenglu\n      Yang"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21431",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21431/21180",
    "published": "2022-02",
    "summary": "With the rapid increase of multimedia data, a large body of literature has emerged to work on multimodal summarization, the majority of which target at refining salient information from textual and image modalities to output a pictorial summary with the most relevant images. Existing methods mostly focus on either extractive or abstractive summarization and rely on the presence and quality of image captions to build image references. We are the first to propose a Unified framework for Multimodal Summarization grounding on BART, UniMS, that integrates extractive and abstractive objectives, as well as selecting the image output. Specially, we adopt knowledge distillation from a vision-language pretrained model to improve image selection, which avoids any requirement on the existence and quality of image captions. Besides, we introduce a visual guided decoder to better integrate textual and visual modalities in guiding abstractive text generation. Results show that our best model achieves a new state-of-the-art result on a large-scale benchmark dataset. The newly involved extractive objective as well as the knowledge distillation technique are proven to bring a noticeable improvement to the multimodal summarization task.",
    "code_link": "https://github.com/openai/CLIP"
  },
  "aaai2022_main_dialoglmpre-trainedmodelforlongdialogueunderstandingandsummarization": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization",
    "authors": [
      "Ming Zhong",
      "Yang Liu",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21432",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21432/21181",
    "published": "2022-02",
    "summary": "Dialogue is an essential part of human communication and cooperation. Existing research mainly focuses on short dialogue scenarios in a one-on-one fashion. However, multi-person interactions in the real world, such as meetings or interviews, are frequently over a few thousand words. There is still a lack of corresponding research and powerful tools to understand and process such long dialogues. Therefore, in this work, we present a pre-training framework for long dialogue understanding and summarization. Considering the nature of long conversations, we propose a window-based denoising approach for generative pre-training. For a dialogue, it corrupts a window of text with dialogue-inspired noise, and guides the model to reconstruct this window based on the content of the remaining conversation. Furthermore, to process longer input, we augment the model with sparse attention which is combined with conventional attention in a hybrid manner. We conduct extensive experiments on five datasets of long dialogues, covering tasks of dialogue summarization, abstractive question answering and topic segmentation. Experimentally, we show that our pre-trained model DialogLM significantly surpasses the state-of-the-art models across datasets and tasks. Source code and all the pre-trained models are available on our GitHub repository (https://github.com/microsoft/DialogLM).",
    "code_link": "https://github.com/microsoft/DialogLM"
  },
  "aaai2022_main_idiomaticexpressionparaphrasingwithoutstrongsupervision": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Idiomatic Expression Paraphrasing without Strong Supervision",
    "authors": [
      "Jianing Zhou",
      "Ziheng Zeng",
      "Hongyu Gong",
      "Suma Bhat"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21433",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21433/21182",
    "published": "2022-02",
    "summary": "Idiomatic expressions (IEs) play an essential role in natural language. In this paper, we study the task of idiomatic sentence paraphrasing (ISP), which aims to paraphrase a sentence with an IE by replacing the IE with its literal paraphrase. The lack of large-scale corpora with idiomatic-literal parallel sentences is a primary challenge for this task, for which we consider two separate solutions. First, we propose an unsupervised approach to ISP, which leverages an IE's contextual information and definition and does not require a parallel sentence training set. Second, we propose a weakly supervised approach using back-translation to jointly perform paraphrasing and generation of sentences with IEs to enlarge the small-scale parallel sentence training dataset. Other significant derivatives of the study include a model that replaces a literal phrase in a sentence with an IE to generate an idiomatic expression and a large scale parallel dataset with idiomatic/literal sentence pairs. The effectiveness of the proposed solutions compared to competitive baselines is seen in the relative gains of over 5.16 points in BLEU, over 8.75 points in METEOR, and over 19.57 points in SARI when the generated sentences are empirically validated on a parallel dataset using automatic and manual evaluations. We demonstrate the practical utility of ISP as a preprocessing step in En-De machine translation.",
    "code_link": "https://github.com/zhjjn/ISP.git"
  },
  "aaai2022_main_multilingualcodesnippetstrainingforprogramtranslation": {
    "conf_id": "AAAI2022",
    "conf_sub_id": "Main",
    "is_workshop": false,
    "conf_name": "AAAI2022",
    "title": "Multilingual Code Snippets Training for Program Translation",
    "authors": [
      "Ming Zhu",
      "Karthik Suresh",
      "Chandan K Reddy"
    ],
    "page_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21434",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/view/21434/21183",
    "published": "2022-02",
    "summary": "Program translation aims to translate source code from one programming language to another. It is particularly useful in applications such as multiple-platform adaptation and legacy code migration. Traditional rule-based program translation methods usually rely on meticulous manual rule-crafting, which is costly both in terms of time and effort. Recently, neural network based methods have been developed to address this problem. However, the absence of high-quality parallel code data is one of the main bottlenecks which impedes the development of program translation models. In this paper, we introduce CoST, a new multilingual Code Snippet Translation dataset that contains parallel data from 7 commonly used programming languages. The dataset is parallel at the level of code snippets, which provides much more fine-grained alignments between different languages than the existing translation datasets. We also propose a new program translation model that leverages multilingual snippet denoising auto-encoding and Multilingual Snippet Translation (MuST) pre-training. Extensive experiments show that the multilingual snippet training is effective in improving program translation performance, especially for low-resource languages. Moreover, our training method shows good generalizability and consistently improves the translation performance of a number of baseline models. The proposed model outperforms the baselines on both snippet-level and program-level translation, and achieves state-of-the-art performance on CodeXGLUE translation task. The code, data, and appendix for this paper can be found at https://github.com/reddy-lab-code-research/MuST-CoST.",
    "code_link": "https://github.com/reddy-labcode-research/MuST-CoST"
  }
}