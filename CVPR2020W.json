{
  "cvpr2020_w42_illumination-basedtransformationsimproveskinlesionsegmentationindermoscopicimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Illumination-Based Transformations Improve Skin Lesion Segmentation in Dermoscopic Images",
    "authors": [
      "Kumar Abhishek",
      "Ghassan Hamarneh",
      "Mark S. Drew"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Abhishek_Illumination-Based_Transformations_Improve_Skin_Lesion_Segmentation_in_Dermoscopic_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Abhishek_Illumination-Based_Transformations_Improve_Skin_Lesion_Segmentation_in_Dermoscopic_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The semantic segmentation of skin lesions is an important and common initial task in the computer aided diagnosis of dermoscopic images. Although deep learning-based approaches have considerably improved the segmentation accuracy, there is still room for improvement by addressing the major challenges, such as variations in lesion shape, size, color and varying levels of contrast. In this work, we propose the first deep semantic segmentation framework for dermoscopic images which incorporates, along with the original RGB images, information extracted using the physics of skin illumination and imaging. In particular, we incorporate information from specific color bands, illumination invariant grayscale images, and shading-attenuated images. We evaluate our method on three datasets: the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset, the DermoFit Image Library, and the PH2 dataset and observe improvements of 12.02%, 4.30%, and 8.86% respectively in the mean Jaccard index over a baseline model trained only with RGB images.\r"
  },
  "cvpr2020_w42_meta-dermdiagnosisfew-shotskindiseaseidentificationusingmeta-learning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Meta-DermDiagnosis: Few-Shot Skin Disease Identification Using Meta-Learning",
    "authors": [
      "Kushagra Mahajan",
      "Monika Sharma",
      "Lovekesh Vig"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Mahajan_Meta-DermDiagnosis_Few-Shot_Skin_Disease_Identification_Using_Meta-Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Mahajan_Meta-DermDiagnosis_Few-Shot_Skin_Disease_Identification_Using_Meta-Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Annotated images for diagnosis of rare or novel diseases are likely to remain scarce due to small affected patient population and limited clinical expertise to annotate images. Deep networks employed for image based diagnosis need to be robust enough to quickly adapt to novel diseases with few annotated images. Further, in case of the frequently occurring long-tailed class distributions in skin lesion and other disease classification datasets, conventional training approaches lead to poor generalization on classes at the tail end of the distribution due to biased class priors. This paper focuses on the problems of disease identification and quick model adaptation in such data-scarce and long-tailed class distribution scenarios by exploiting recent advances in meta-learning. This involves training a neural network on few-shot image classification tasks based on an initial set of class labels / head classes of the distribution, prior to adapting the model for classification on a set of unseen / tail classes. We named the proposed method Meta-DermDiagnosis because it utilizes meta-learning based few-shot learning techniques such as the gradient based Reptile and distance metric based Prototypical networks for identification of diseases in skin lesion datasets. We evaluate the effectiveness of our approach on publicly available skin lesion datasets, namely the ISIC 2018, Derm7pt and SD-198 datasets and obtain significant performance improvement over pre-trained models with just a few annotated examples. Further, we incorporate Group Equivariant convolutions (G-convolutions) for the Meta-DermDiagnosis network to improve disease identification performance as these images generally do not have any prevailing global orientation / canonical structure and G-convolutions make the network equivariant to any discrete transformations like rotation, reflection and translation.\r"
  },
  "cvpr2020_w42_onout-of-distributiondetectionalgorithmswithdeepneuralskincancerclassifiers": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "On Out-of-Distribution Detection Algorithms With Deep Neural Skin Cancer Classifiers",
    "authors": [
      "Andre G. C. Pacheco",
      "Chandramouli S. Sastry",
      "Thomas Trappenberg",
      "Sageev Oore",
      "Renato A. Krohling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Pacheco_On_Out-of-Distribution_Detection_Algorithms_With_Deep_Neural_Skin_Cancer_Classifiers_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Pacheco_On_Out-of-Distribution_Detection_Algorithms_With_Deep_Neural_Skin_Cancer_Classifiers_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Computer-aided skin cancer detection systems built with deep neural networks yield overconfident predictions on out-of-distribution examples. Motivated by the importance of out-of-distribution detection in these systems and the lack of relevant benchmarks targeted for skin cancer classification, we introduce a rich collection of out-of-distribution datasets -- designed to comprehensively evaluate state-of-the-art out-of-distribution algorithms with skin cancer classifiers. In addition, we propose an adaptation in the Gram-Matrix algorithm for out-of-distribution detection that generally performs better and faster than the original algorithm for the considered skin cancer classification task. We also include a detailed discussion comparing the various state-of-the-art out-of-distribution detection algorithms and identify avenues for future research.\r"
  },
  "cvpr2020_w42_interpretingmechanismsofpredictionforskincancerdiagnosisusingmulti-tasklearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Interpreting Mechanisms of Prediction for Skin Cancer Diagnosis Using Multi-Task Learning",
    "authors": [
      "Davide Coppola",
      "Hwee Kuan Lee",
      "Cuntai Guan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Coppola_Interpreting_Mechanisms_of_Prediction_for_Skin_Cancer_Diagnosis_Using_Multi-Task_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Coppola_Interpreting_Mechanisms_of_Prediction_for_Skin_Cancer_Diagnosis_Using_Multi-Task_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " One of the key issues in deep learning is the difficulty in the interpretation of mechanisms for the final predictions. Hence the real-world application of deep learning in skin cancer still proves limited, in spite of the solid performances achieved. We present a way to better interpret predictions on a skin lesion dataset by the use of a multi-task learning framework and a set of learnable gates. The model detects a set of clinically significant attributes in addition to the final diagnosis and learns the association between tasks by selecting which features to share among them. Conventional multi-task learning algorithms generally share all the features among tasks and lack a way of determining the amount of sharing between tasks. On the other hand, this method provides a simple way to inspect which features are being shared between tasks in the form of gates that can be learned in an end-to-end fashion. Experiments have been carried out on the publicly available Derm7pt dataset, which provides diagnosis information as well as the attributes needed for the well-known 7-point checklist method.\r"
  },
  "cvpr2020_w42_agreementbetweensaliencymapsandhuman-labeledregionsofinterestapplicationstoskindiseaseclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Agreement Between Saliency Maps and Human-Labeled Regions of Interest: Applications to Skin Disease Classification",
    "authors": [
      "Nalini Singh",
      "Kang Lee",
      "David Coz",
      "Christof Angermueller",
      "Susan Huang",
      "Aaron Loh",
      "Yuan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Singh_Agreement_Between_Saliency_Maps_and_Human-Labeled_Regions_of_Interest_Applications_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Singh_Agreement_Between_Saliency_Maps_and_Human-Labeled_Regions_of_Interest_Applications_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose to systematically identify potentially problematic patterns in skin disease classification models via quantitative analysis of agreement between saliency maps and human-labeled regions of interest. We further compute summary statistics describing patterns in this agreement for various stratifications of input examples. Through this analysis, we discover candidate spurious associations learned by the classifier and suggest next steps to handle such associations. Our approach can be used as a debugging tool to systematically spot difficult examples and error categories. Insights from this analysis could guide targeted data collection and improve model generalizability.\r"
  },
  "cvpr2020_w42_lessismoresampleselectionandlabelconditioningimproveskinlesionsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Less Is More: Sample Selection and Label Conditioning Improve Skin Lesion Segmentation",
    "authors": [
      "Vinicius Ribeiro",
      "Sandra Avila",
      "Eduardo Valle"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Ribeiro_Less_Is_More_Sample_Selection_and_Label_Conditioning_Improve_Skin_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Ribeiro_Less_Is_More_Sample_Selection_and_Label_Conditioning_Improve_Skin_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Segmenting skin lesions images is relevant both for itself and for assisting in lesion classification, but suffers from the challenge in obtaining annotated data. In this work, we show that segmentation may improve with less data, by selecting the training samples with best inter-annotator agreement, and conditioning the ground-truth masks to remove excessive detail. We perform an exhaustive experimental design considering several sources of variation, including three different test sets, two different deep-learning architectures, and several replications, for a total of 540 experimental runs. We found that sample selection and detail removal may have impacts corresponding, respectively, to 12% and 16% of the one obtained by picking a better deep-learning model.\r"
  },
  "cvpr2020_w42_debiasingskinlesiondatasetsandmodels?notsofast": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Debiasing Skin Lesion Datasets and Models? Not So Fast",
    "authors": [
      "Alceu Bissoto",
      "Eduardo Valle",
      "Sandra Avila"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Bissoto_Debiasing_Skin_Lesion_Datasets_and_Models_Not_So_Fast_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Bissoto_Debiasing_Skin_Lesion_Datasets_and_Models_Not_So_Fast_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Data-driven models are now deployed in a plethora of real-world applications -- including automated diagnosis -- but models learned from data risk learning biases from that same data. When models learn spurious correlations not found in real-world situations, their deployment for critical tasks, such as medical decisions, can be catastrophic. In this work we address this issue for skin-lesion classification models, with two objectives: finding out what are the spurious correlations exploited by biased networks, and debiasing the models by removing such spurious correlations from them. We perform a systematic integrated analysis of 7 visual artifacts (which are possible sources of biases exploitable by networks), employ a state-of-the-art technique to prevent the models from learning spurious correlations, and propose datasets to test models for the presence of bias. We find out that, despite interesting results that point to promising future research, current debiasing methods are not ready to solve the bias issue for skin-lesion models.\r"
  },
  "cvpr2020_w42_howimportantiseachdermoscopyimage?": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "How Important Is Each Dermoscopy Image?",
    "authors": [
      "Catarina Barata",
      "Carlos Santiago"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Barata_How_Important_Is_Each_Dermoscopy_Image_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Barata_How_Important_Is_Each_Dermoscopy_Image_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep neural networks (DNNs) have revolutionized the field of dermoscopy image analysis. Systems based on DNNs are able to achieve impressive diagnostic performances, even outperforming experienced dermatologists. However, DNNs strongly rely on the quantity and quality of the training data. Real world data sets, including those related to dermoscopy, are often severely imbalanced and of reduced dimensions. Thus, models trained on these data sets typically become biased and fail to generalize well to new images. Sample weighting strategies have been proposed to overcome the previous limitations with promising results. Nonetheless, they have been poorly investigated in the context of dermoscopy image analysis. This paper addresses this issue through the extensive comparison of several sample weighting methods, namely class balance and curriculum learning. The results show that each sample weighting strategy influences the performance of the model in different ways, with most finding a compromise between correctly classifying the most common classes or biasing the model towards the less represented classes. Furthermore, the features learned by each model differ significantly, depending on the training strategy.\r"
  },
  "cvpr2020_w42_uncertaintyestimationindeepneuralnetworksfordermoscopicimageclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Uncertainty Estimation in Deep Neural Networks for Dermoscopic Image Classification",
    "authors": [
      "Marc Combalia",
      "Ferran Hueto",
      "Susana Puig",
      "Josep Malvehy",
      "Veronica Vilaplana"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Combalia_Uncertainty_Estimation_in_Deep_Neural_Networks_for_Dermoscopic_Image_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Combalia_Uncertainty_Estimation_in_Deep_Neural_Networks_for_Dermoscopic_Image_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The high performance of machine learning algorithms for the task of skin lesion classification has been proven over the past few years. However, real-world implementations are still scarce. One of the reasons could be that most methods do not quantify the uncertainty in the predictions and are not able to detect data that is anomalous or significantly different from that used in training, which may lead to a lack of confidence in the automated diagnosis or errors in the interpretation of results. In this work, we explore the use of uncertainty estimation techniques and metrics for deep neural networks based on Monte-Carlo sampling and apply them to the problem of skin lesion classification on data from ISIC Challenges 2018 and 2019. Our results show that uncertainty metrics can be successfully used to detect difficult and out-of-distribution samples.\r"
  },
  "cvpr2020_w42_learningameta-ensembletechniqueforskinlesionclassificationandnovelclassdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w42",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Skin Image Analysis",
    "title": "Learning a Meta-Ensemble Technique for Skin Lesion Classification and Novel Class Detection",
    "authors": [
      "Subhranil Bagchi",
      "Anurag Banerjee",
      "Deepti R. Bathula"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w42/Bagchi_Learning_a_Meta-Ensemble_Technique_for_Skin_Lesion_Classification_and_Novel_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w42/Bagchi_Learning_a_Meta-Ensemble_Technique_for_Skin_Lesion_Classification_and_Novel_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The frequency and fatality rates associated with skin Melanoma requires an accurate and efficient detection methodology to enable early medical diagnosis. Artificial Intelligence (AI) augmented detection methods aim at achieving this goal while reducing the costs and time involved in traditional methods. This work utilizes a two-level ensemble learning technique (trained with weighted losses) to improve accuracy over individual classification models. The ensemble technique alleviates over-fitting due to class imbalance in the dataset, achieving a Balanced Multi-class Accuracy (BMA) score of 0.591 without unknown class detection. The algorithm was extended by appending the proposed CS-KSU module collection to detect the presence of images belonging to novel classes during test time. The extended algorithm secured an Area Under the ROC Curve (AUC) score of 0.544 for the unknown class. Our algorithm's performance is at par with the current state-of-the-art for this task.\r"
  },
  "cvpr2020_w60_smokesingle-stagemonocular3dobjectdetectionviakeypointestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation",
    "authors": [
      "Zechen Liu",
      "Zizhang Wu",
      "Roland Toth"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Liu_SMOKE_Single-Stage_Monocular_3D_Object_Detection_via_Keypoint_Estimation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Liu_SMOKE_Single-Stage_Monocular_3D_Object_Detection_via_Keypoint_Estimation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Estimating 3D orientation and translation of objects is essential for infrastructure-less autonomous navigation and driving. In case of monocular vision, successful methods have been mainly based on two ingredients: (i) a network generating 2D region proposals, (ii) a R-CNN structure predicting 3D object pose by utilizing the acquired regions of interest. We argue that the 2D detection network is redundant and introduces non-negligible noise for 3D detection. Hence, we propose a novel 3D object detection method, named SMOKE, in this paper that predicts a 3D bounding box for each detected object by combining a single keypoint estimate with regressed 3D variables. As a second contribution, we propose a multi-step disentangling approach for constructing the 3D bounding box, which significantly improves both training convergence and detection accuracy. In contrast to previous 3D detection techniques, our method does not require complicated pre/post-processing, extra data, and a refinement stage. Despite of its structural simplicity, our proposed SMOKE network outperforms all existing monocular 3D detection methods on the KITTI dataset, giving the best state-of-the-art result on both 3D object detection and Bird's eye view evaluation. The code will be made publicly available.\r"
  },
  "cvpr2020_w60_wassersteinloss-baseddeepobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "Wasserstein Loss-Based Deep Object Detection",
    "authors": [
      "Yuzhuo Han",
      "Xiaofeng Liu",
      "Zhenfei Sheng",
      "Yutao Ren",
      "Xu Han",
      "Jane You",
      "Risheng Liu",
      "Zhongxuan Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Han_Wasserstein_Loss-Based_Deep_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Han_Wasserstein_Loss-Based_Deep_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Object detection locates the objects with bounding boxes and identifies their classes, which is valuable in many computer vision applications (e.g. autonomous driving). Most existing deep learning-based methods output a probability vector for instance classification trained with the one-hot label. However, the limitation of these models lies in attribute perception because they do not take the severity of different misclassifications into consideration. In this paper, we propose a novel method based on the Wasserstein distance called Wasserstein Loss based Model for Object Detection (WLOD). Different from the commonly used distance metric such as cross-entropy (CE), the Wasserstein loss assigns different weights for one sample identified to different classes with different values. Our distance metric is designed by combining the CE or binary cross-entropy (BCE) with Wasserstein distance to learn the detector considering both the discrimination and the seriousness of different misclassifications. The misclassified objects are identified to similar classes with a higher probability to reduce intolerable misclassifications. Finally, the model is tested on the BDD100K and KITTI datasets and reaches state-of-the-art performance.\r"
  },
  "cvpr2020_w60_learningdepth-guidedconvolutionsformonocular3dobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "Learning Depth-Guided Convolutions for Monocular 3D Object Detection",
    "authors": [
      "Mingyu Ding",
      "Yuqi Huo",
      "Hongwei Yi",
      "Zhe Wang",
      "Jianping Shi",
      "Zhiwu Lu",
      "Ping Luo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Ding_Learning_Depth-Guided_Convolutions_for_Monocular_3D_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Ding_Learning_Depth-Guided_Convolutions_for_Monocular_3D_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " 3D object detection from a single image without LiDAR is a challenging task due to the lack of accurate depth information. Conventional 2D convolutions are unsuitable for this task because they fail to capture local object and its scale information, which are vital for 3D object detection. To better represent 3D structure, prior arts typically transform depth maps estimated from 2D images into a pseudo-LiDAR representation, and then apply existing 3D point-cloud based object detectors. However, their results depend heavily on the accuracy of the estimated depth maps, resulting in suboptimal performance. In this work, instead of using pseudo-LiDAR representation, we improve the fundamental 2D fully convolutions by proposing a new local convolutional network (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D4LCN), where the filters and their receptive fields can be automatically learned from image-based depth maps, making different pixels of different images have different filters. D4LCN overcomes the limitation of conventional 2D convolutions and narrows the gap between image representation and 3D point cloud representation. Extensive experiments show that D4LCN outperforms existing works by large margins. For example, the relative improvement of D4LCN against the state-of-the-art on KITTI is 9.1% in the moderate setting.\r"
  },
  "cvpr2020_w60_feudalsteeringhierarchicallearningforsteeringangleprediction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "Feudal Steering: Hierarchical Learning for Steering Angle Prediction",
    "authors": [
      "Faith Johnson",
      "Kristin Dana"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Johnson_Feudal_Steering_Hierarchical_Learning_for_Steering_Angle_Prediction_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Johnson_Feudal_Steering_Hierarchical_Learning_for_Steering_Angle_Prediction_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We consider the challenge of automated steering angle prediction for self driving cars using egocentric road images. In this work, we explore the use of feudal networks, used in hierarchical reinforcement learning (HRL), to devise a vehicle agent to predict steering angles from first person, dash-cam images of the Udacity driving dataset. Our method, Feudal Steering, is inspired by recent work in HRL consisting of a manager network and a worker network that operate on different temporal scales and have different goals. The manager works at a temporal scale that is relatively coarse compared to the worker and has a higher level, task-oriented goal space. Using feudal learning to divide the task into manager and worker sub-networks provides more accurate and robust prediction. Temporal abstraction in driving allows more complex primitives than the steering angle at a single time instance. Composite actions comprise a subroutine or skill that can be re-used throughout the driving sequence. The associated subroutine id is the manager network's goal, so that the manager seeks to succeed at the high level task (e.g. a sharp right turn, a slight right turn, moving straight in traffic, or moving straight unencumbered by traffic). The steering angle at a particular time instance is the worker network output which is regulated by the manager's high level task. We demonstrate state-of-the art steering angle prediction results on the Udacity dataset.\r"
  },
  "cvpr2020_w60_self-supervisedobjectmotionanddepthestimationfromvideo": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "Self-Supervised Object Motion and Depth Estimation From Video",
    "authors": [
      "Qi Dai",
      "Vaishakh Patil",
      "Simon Hecker",
      "Dengxin Dai",
      "Luc Van Gool",
      "Konrad Schindler"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Dai_Self-Supervised_Object_Motion_and_Depth_Estimation_From_Video_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Dai_Self-Supervised_Object_Motion_and_Depth_Estimation_From_Video_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a self-supervised learning framework to estimate the individual object motion and monocular depth from video. We model the object motion as a 6 degree-of-freedom rigid-body transformation. The instance segmentation mask is leveraged to introduce the information of object. Compared with methods which predict dense optical flow map to model the motion, our approach significantly reduces the number of values to be estimated. Our system eliminates the scale ambiguity of motion prediction through imposing a novel geometric constraint loss term. Experiments on KITTI driving dataset demonstrate our system is capable to capture the object motion without external annotation. Our system outperforms previous self-supervised approaches in terms of 3D scene flow prediction, and contribute to the disparity prediction in dynamic area.\r"
  },
  "cvpr2020_w60_end-to-endlanemarkerdetectionviarow-wiseclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "End-to-End Lane Marker Detection via Row-Wise Classification",
    "authors": [
      "Seungwoo Yoo",
      "Hee Seok Lee",
      "Heesoo Myeong",
      "Sungrack Yun",
      "Hyoungwoo Park",
      "Janghoon Cho",
      "Duck Hoon Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Yoo_End-to-End_Lane_Marker_Detection_via_Row-Wise_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Yoo_End-to-End_Lane_Marker_Detection_via_Row-Wise_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In autonomous driving, detecting reliable and accurate lane marker positions is a crucial yet challenging task. The conventional approaches for the lane marker detection problem perform a pixel-level dense prediction task followed by sophisticated post-processing that is inevitable since lane markers are typically represented by a collection of line segments without thickness. In this paper, we propose a method performing direct lane marker vertex prediction in an end-to-end manner, i.e., without any post-processing step that is required in the pixel-level dense prediction task. Specifically, we translate the lane marker detection problem into a row-wise classification task, which takes advantage of the innate shape of lane markers but, surprisingly, has not been explored well. In order to compactly extract sufficient information about lane markers which spread from the left to the right in an image, we devise a novel layer, inspired by [8], which is utilized to successively compress horizontal components so enables an end-to-end lane marker detection system where the final lane marker positions are sim- ply obtained via argmax operations in testing time. Experimental results demonstrate the effectiveness of the proposed method, which is on par or outperforms the state-of-the-art methods on two popular lane marker detection benchmarks, i.e., TuSimple and CULane.\r"
  },
  "cvpr2020_w60_anextensiblemulti-sensorfusionframeworkfor3dimaging": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "An Extensible Multi-Sensor Fusion Framework for 3D Imaging",
    "authors": [
      "Talha Ahmad Siddiqui",
      "Rishi Madhok",
      "Matthew O'Toole"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Siddiqui_An_Extensible_Multi-Sensor_Fusion_Framework_for_3D_Imaging_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Siddiqui_An_Extensible_Multi-Sensor_Fusion_Framework_for_3D_Imaging_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Many autonomous vehicles rely on an array of sensors for safe navigation, where each sensor captures different visual attributes from the surrounding environment. For example, a single conventional camera captures high-resolution images but no 3D information; a LiDAR provides excellent range information but poor spatial resolution; and a prototype single-photon LiDAR (SP-LiDAR) can provide a dense but noisy representation of the 3D scene. Although the outputs of these sensors vary dramatically (e.g., 2D images, point clouds, 3D volumes), they all derive from the same 3D scene. We propose an extensible sensor fusion framework that (1) lifts the sensor output to volumetric representations of the 3D scene, (2) fuses these volumes together, and (3) processes the resulting volume with a deep neural network to generate a depth (or disparity) map. Although our framework can potentially extend to many types of sensors, we focus on fusing combinations of three imaging systems: monocular/stereo cameras, regular LiDARs, and SP-LiDARs. To train our neural network, we generate a synthetic dataset through CARLA that contains the individual measurements. We also conduct various fusion ablation experiments and evaluate the results of different sensor combinations.\r"
  },
  "cvpr2020_w60_topometricimitationlearningforroutefollowingunderappearancechange": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w60",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Autonomous Driving",
    "title": "Topometric Imitation Learning for Route Following Under Appearance Change",
    "authors": [
      "Shaojun Cai",
      "Yingjia Wan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w60/Cai_Topometric_Imitation_Learning_for_Route_Following_Under_Appearance_Change_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w60/Cai_Topometric_Imitation_Learning_for_Route_Following_Under_Appearance_Change_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Traditional navigation models in autonomous driving rely heavily on metric maps, which severely limits their application in large scale environments. In this paper, we introduce a two-level navigation architecture that contains a topological-metric memory structure and a deep image-based controller. The hybrid memory extracts visual features at each location point with a deep convolutional neural network, and stores information about local driving commands at each location point based on metric information estimated from ego-motion information. The topological-metric memory is seamlessly integrated with a conditional imitation learning controller through the navigational commands that drive the vehicle between different vertices without collision. We test the whole system in teach-and-repeat experiments in an urban driving simulator. Results show that after being trained in a separate environment, the system could quickly adapt to novel environments with a single teach trial and follow route successively under various illumination and weather conditions.\r"
  },
  "cvpr2020_w61_two-stagediscriminativere-rankingforlarge-scalelandmarkretrieval": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w61",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Image Matching: Local Features and Beyond",
    "title": "Two-Stage Discriminative Re-Ranking for Large-Scale Landmark Retrieval",
    "authors": [
      "Shuhei Yokoo",
      "Kohei Ozaki",
      "Edgar Simo-Serra",
      "Satoshi Iizuka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w61/Yokoo_Two-Stage_Discriminative_Re-Ranking_for_Large-Scale_Landmark_Retrieval_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w61/Yokoo_Two-Stage_Discriminative_Re-Ranking_for_Large-Scale_Landmark_Retrieval_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose an efficient pipeline for large-scale landmark image retrieval that addresses the diversity of the dataset through two-stage discriminative re-ranking. Our approach is based on embedding the images in a feature-space using a convolutional neural network trained with a cosine softmax loss. Due to the variance of the images, which include extreme viewpoint changes such as having to retrieve images of the exterior of a landmark from images of the interior, this is very challenging for approaches based exclusively on visual similarity. Our proposed re-ranking approach improves the results in two steps: in the sort-step, k-nearest neighbor search with soft-voting to sort the retrieved results based on their label similarity to the query images, and in the insert-step, we add additional samples from the dataset that were not retrieved by image-similarity. This approach allows overcoming the low visual diversity in retrieved images. In-depth experimental results show that the proposed approach significantly outperforms existing approaches on the challenging Google Landmark Datasets. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge on Kaggle. Our code is publicly available here: https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution\r"
  },
  "cvpr2020_w61_matchornomatchkeypointfilteringbasedonmatchingprobability": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w61",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Image Matching: Local Features and Beyond",
    "title": "Match or No Match: Keypoint Filtering Based on Matching Probability",
    "authors": [
      "Alexandra I. Papadaki",
      "Ronny Hansch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w61/Papadaki_Match_or_No_Match_Keypoint_Filtering_Based_on_Matching_Probability_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w61/Papadaki_Match_or_No_Match_Keypoint_Filtering_Based_on_Matching_Probability_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Keypoints that do not meet the needs of a given application are a very common accuracy and efficiency bottleneck in many computer vision tasks, including keypoint matching and 3D reconstruction. Many computer vision and machine learning methods have dealt with this issue, trying to improve keypoint detection or the matching process. We introduce an algorithm that filters detected keypoints before the matching is even attempted, by predicting the probability of each point to be successfully matched. This is realized using a flexible and time efficient Random Forest classifier. Experiments on stereo and multi-view datasets of building facades show that the proposed method decreases the computational cost of a subsequent keypoint matching and 3D reconstruction, by correctly filtering 50% of the points that wouldn't be matched while preserving 73% of the matchable keypoints. This enables a subsequent processing with minimal mismatches, provides reliable matches, and point clouds. The presented filtering leads to an improved 3D reconstruction of the scene, even in the hard case of repetitive patterns and vegetation.\r"
  },
  "cvpr2020_w53_utilizingmaskr-cnnforwaterlinedetectionincanoesprintvideoanalysis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Utilizing Mask R-CNN for Waterline Detection in Canoe Sprint Video Analysis",
    "authors": [
      "Marie-Sophie von Braun",
      "Patrick Frenzel",
      "Christian Kading",
      "Mirco Fuchs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/von_Braun_Utilizing_Mask_R-CNN_for_Waterline_Detection_in_Canoe_Sprint_Video_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/von_Braun_Utilizing_Mask_R-CNN_for_Waterline_Detection_in_Canoe_Sprint_Video_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Determining a waterline in images recorded in canoe sprint training is an important component for the kinematic parameter analysis to assess an athlete's performance. Here, we propose an approach for the automated waterline detection. First, we utilized a pre-trained Mask R-CNN by means of transfer learning for canoe segmentation. Second, we developed a multi-stage approach to estimate a waterline from the outline of the segments. It consists of two linear regression stages and the systematic selection of canoe parts. We then introduced a parameterization of the waterline as a basis for further evaluations. Next, we conducted a study among several experts to estimate the ground truth waterlines. This not only included an average waterline drawn from the individual experts annotations but, more importantly, a measure for the uncertainty between individual results. Finally, we assessed our method with respect to the question whether the predicted waterlines are in accordance with the experts annotations. Our method demonstrated a high performance and provides opportunities for new applications in the field of automated video analysis in canoe sprint.\r"
  },
  "cvpr2020_w53_vralpineskitrainingaugmentationusingvisualcuesofleadingskier": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "VR Alpine Ski Training Augmentation Using Visual Cues of Leading Skier",
    "authors": [
      "Erwin Wu",
      "Takayuki Nozawa",
      "Florian Perteneder",
      "Hideki Koike"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Wu_VR_Alpine_Ski_Training_Augmentation_Using_Visual_Cues_of_Leading_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Wu_VR_Alpine_Ski_Training_Augmentation_Using_Visual_Cues_of_Leading_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Alpine skiing has strong environmental dependencies and the way of teaching the movement is believed to be incremental and cyclical. Training alpine skiing on simulators is a challenging work, especially when supporting experienced learner to improve to higher level. In this paper, we propose several vision augmentations for learning from a recorded expert skier motion in the way to replay the motion as a virtual leading skier. The system uses an stationary indoor ski simulator and a VR System for prototyping, two VR trackers are used to capture the motion of skis so that users can control the skis on the virtual slope. For training, we captured the motion of professional athletes and replay it to let the users follow the experts in the slope. To support users, 6 different visual cues are introduced from different perspectives of learning skiing, such as the feet angle or the lateral position. To explore the utility of visual cues and to study how users could learn the motion patterns from the expert-skier effectively, we performed qualitative and quantitative evaluations. In addition, we also studied several visual feedback aiming to help the learning process. The work provides the basis for developing and understanding the possibilities and limitations of VR ski ski training, which also has the potential to be extended to AR/MR use in real world.\r"
  },
  "cvpr2020_w53_multimodalandmultiviewdistillationforreal-timeplayerdetectiononafootballfield": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Multimodal and Multiview Distillation for Real-Time Player Detection on a Football Field",
    "authors": [
      "Anthony Cioppa",
      "Adrien Deliege",
      "Noor Ul Huda",
      "Rikke Gade",
      "Marc Van Droogenbroeck",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Cioppa_Multimodal_and_Multiview_Distillation_for_Real-Time_Player_Detection_on_a_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Cioppa_Multimodal_and_Multiview_Distillation_for_Real-Time_Player_Detection_on_a_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Monitoring the occupancy of public sports facilities is essential to assess their use and to motivate their construction in new places. In the case of a football field, the area to cover is large, thus several regular cameras should be used, which makes the setup expensive and complex. As an alternative, we developed a system that detects players from a unique cheap and wide-angle fisheye camera assisted by a single narrow-angle thermal camera. In this work, we train a network in a knowledge distillation approach in which the student and the teacher have different modalities and a different view of the same scene. In particular, we design a custom data augmentation combined with a motion detection algorithm to handle the training in the region of the fisheye camera not covered by the thermal one. We show that our solution is effective in detecting players on the whole field filmed by the fisheye camera. We evaluate it quantitatively and qualitatively in the case of an online distillation, where the student detects players in real time while being continuously adapted to the latest video conditions.\r"
  },
  "cvpr2020_w53_eventdetectionincoarselyannotatedsportsvideosviaparallelmulti-receptivefield1dconvolutions": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Event Detection in Coarsely Annotated Sports Videos via Parallel Multi-Receptive Field 1D Convolutions",
    "authors": [
      "Kanav Vats",
      "Mehrnaz Fani",
      "Pascale Walters",
      "David A. Clausi",
      "John Zelek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Vats_Event_Detection_in_Coarsely_Annotated_Sports_Videos_via_Parallel_Multi-Receptive_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Vats_Event_Detection_in_Coarsely_Annotated_Sports_Videos_via_Parallel_Multi-Receptive_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In problems such as sports video analytics, it is difficult to obtain accurate frame-level annotations and exact event duration because of the lengthy videos and sheer volume of video data. This issue is even more pronounced in fast-paced sports such as ice hockey. Obtaining annotations on a coarse scale can be much more practical and time efficient. We propose the task of event detection in coarsely annotated videos. We introduce a multi-tower temporal convolutional network architecture for the proposed task. The network, with the help of multiple receptive fields, processes information at various temporal scales to account for the uncertainty with regard to the exact event location and duration. We demonstrate the effectiveness of the multi-receptive field architecture through appropriate ablation studies. The method is evaluated on two tasks - event detection in coarsely annotated hockey videos in the NHL dataset and event spotting in soccer on the SoccerNet dataset. The two datasets lack frame-level annotations and have very distinct event frequencies. Experimental results demonstrate the effectiveness of the network by obtaining a 55% average F1 score on the NHL dataset and by achieving competitive performance compared to the state of the art on the SoccerNet dataset. We believe our approach will help develop more practical pipelines for event detection in sports video.\r"
  },
  "cvpr2020_w53_ttnetreal-timetemporalandspatialvideoanalysisoftabletennis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "TTNet: Real-Time Temporal and Spatial Video Analysis of Table Tennis",
    "authors": [
      "Roman Voeikov",
      "Nikolay Falaleev",
      "Ruslan Baikulov"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Voeikov_TTNet_Real-Time_Temporal_and_Spatial_Video_Analysis_of_Table_Tennis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Voeikov_TTNet_Real-Time_Temporal_and_Spatial_Video_Analysis_of_Table_Tennis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a neural network TTNet aimed at real-time processing of high-resolution table tennis videos, providing both temporal (events spotting) and spatial (ball detection and semantic segmentation) data. This approach gives core information for reasoning score updates by an auto-referee system. We also publish a multi-task dataset OpenTTGames with videos of table tennis games in 120 fps labeled with events, semantic segmentation masks, and ball coordinates for evaluation of multi-task approaches, primarily oriented on spotting of quick events and small objects tracking. TTNet demonstrated 97.0% accuracy in game events spot-ting along with 2 pixels RMSE in ball detection with 97.5% accuracy on the test part of the presented dataset. The proposed network allows the processing of downscaled full HD videos with inference time below 6 ms per input tensor on a machine with a single consumer-grade GPU. Thus, we are contributing to the development of real-time multi-task deep learning applications and presenting approach, which is potentially capable of substituting manual data collection by sports scouts, providing support for referees' decision-making, and gathering extra information about the game process.\r"
  },
  "cvpr2020_w53_usingplayersbody-orientationtomodelpassfeasibilityinsoccer": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Using Player's Body-Orientation to Model Pass Feasibility in Soccer",
    "authors": [
      "Adria Arbues-Sanguesa",
      "Adrian Martin",
      "Javier Fernandez",
      "Coloma Ballester",
      "Gloria Haro"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Arbues-Sanguesa_Using_Players_Body-Orientation_to_Model_Pass_Feasibility_in_Soccer_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Arbues-Sanguesa_Using_Players_Body-Orientation_to_Model_Pass_Feasibility_in_Soccer_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Given a monocular video of a soccer match, this paper presents a computational model to estimate the most feasible pass at any given time. The method leverages offensive player's orientation (plus their location) and opponents' spatial configuration to compute the feasibility of pass events within players of the same team. Orientation data is gathered from body pose estimations that are properly projected onto the 2D game field; moreover, a geometrical solution is provided, through the definition of a feasibility measure, to determine which players are better oriented towards each other. Once analyzed more than 6000 pass events, results show that, by including orientation as a feasibility measure, a robust computational model can be built, reaching more than 0.7 Top-3 accuracy. Finally, the combination of the orientation feasibility measure with the recently introduced Expected Possession Value metric is studied; promising results are obtained, thus showing that existing models can be refined by using orientation as a key feature. These models could help both coaches and analysts to have a better understanding of the game and to improve the players' decision-making process.\r"
  },
  "cvpr2020_w53_anon-invasivevision-basedapproachtovelocitymeasurementofskeletontraining": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "A Non-Invasive Vision-Based Approach to Velocity Measurement of Skeleton Training",
    "authors": [
      "Murray Evans",
      "Laurie Needham",
      "Steffi L. Colyer",
      "Darren P. Cosker"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Evans_A_Non-Invasive_Vision-Based_Approach_to_Velocity_Measurement_of_Skeleton_Training_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Evans_A_Non-Invasive_Vision-Based_Approach_to_Velocity_Measurement_of_Skeleton_Training_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Skeleton is a winter sport where performance is greatly affected by the velocity an athlete can achieve during their start up to the point where they load themselves onto their sled. As such, it is of interest to athletes and coaching staff to be able to monitor the performance of their athletes and how they respond to different training schedules and techniques. This paper proposes a non-invasive vision based method for measuring the velocity of a skeleton athlete and their sled during the push start. Mean differences in estimated velocity between ground truth data and our proposed system were -0.005 (+/- 0.186) m/s for the athlete mass centre and -0.017 (+/- 0.133) m/s for the sled. The results compare favourably to techniques previously presented in the biomechanics and sport science literature.\r"
  },
  "cvpr2020_w53_asystemforacquisitionandmodellingofice-hockeystickshapedeformationfromplayershotvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "A System for Acquisition and Modelling of Ice-Hockey Stick Shape Deformation From Player Shot Videos",
    "authors": [
      "Kaustubha Mendhurwar",
      "Gaurav Handa",
      "Leixiao Zhu",
      "Sudhir Mudur",
      "Etienne Beauchesne",
      "Marc LeVangie",
      "Aiden Hallihan",
      "Abbas Javadtalab",
      "Tiberiu Popa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Mendhurwar_A_System_for_Acquisition_and_Modelling_of_Ice-Hockey_Stick_Shape_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Mendhurwar_A_System_for_Acquisition_and_Modelling_of_Ice-Hockey_Stick_Shape_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In Ice-Hockey, a player shot significantly deforms the hockey-stick. Since this deformation plays a dynamic role in determining the flight of the puck, it is used in the study of hockey stick shapes, material properties, match to player style, etc. Reconstructing the deformable 3D shape of the stick during the course of a player shot has important applications. In this work we present a new, low cost, portable system to acquire videos of a player shot and to automatically reconstruct the deformation in 3D shape of the stick.The point clouds obtained are low resolution and noisy, as it is difficult to separate players hand geometry from the stick.We use the medial axis to constrain the point cloud to stick only geometry, and then use physics-based co-rotational FEM to determine the stick bend. We have tested the system with different sticks, players and shot styles, and our system yields accurate reconstructions. The results are discussed both qualitatively and where possible, quantitatively.\r"
  },
  "cvpr2020_w53_decouplingvideoandhumanmotiontowardspracticaleventdetectioninathleterecordings": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Decoupling Video and Human Motion: Towards Practical Event Detection in Athlete Recordings",
    "authors": [
      "Moritz Einfalt",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Einfalt_Decoupling_Video_and_Human_Motion_Towards_Practical_Event_Detection_in_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Einfalt_Decoupling_Video_and_Human_Motion_Towards_Practical_Event_Detection_in_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper we address the problem of motion event detection in athlete recordings from individual sports. In contrast to recent end-to-end approaches, we propose to use 2D human pose sequences as an intermediate representation that decouples human motion from the raw video information. Combined with domain-adapted athlete tracking, we describe two approaches to event detection on pose sequences and evaluate them in complementary domains: swimming and athletics. For swimming, we show how robust decision rules on pose statistics can detect different motion events during swim starts, with a F1 score of over 91% despite limited data. For athletics, we use a convolutional sequence model to infer stride-related events in long and triple jump recordings, leading to highly accurate detections with 96% in F1 score at only +/- 5ms temporal deviation. Our approach is not limited to these domains and shows the flexibility of pose-based motion event detection.\r"
  },
  "cvpr2020_w53_asseenontvautomaticbasketballvideoproductionusinggaussian-basedactionnessandgamestatesrecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "As Seen on TV: Automatic Basketball Video Production Using Gaussian-Based Actionness and Game States Recognition",
    "authors": [
      "Julian Quiroga",
      "Henry Carrillo",
      "Edisson Maldonado",
      "John Ruiz",
      "Luis M. Zapata"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Quiroga_As_Seen_on_TV_Automatic_Basketball_Video_Production_Using_Gaussian-Based_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Quiroga_As_Seen_on_TV_Automatic_Basketball_Video_Production_Using_Gaussian-Based_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Automatic video production of sports aims at producing an aesthetic broadcast of sporting events. We present a new video system able to automatically produce a smooth and pleasant broadcast of Basketball games using a single fixed 4K camera. The system automatically detects and localizes players, ball and referees, to recognize main action coordinates and game states yielding to a professional cameraman-like production of the basketball event. We also release a fully annotated dataset consisting of single 4K camera and twelve-camera videos of basketball games.\r"
  },
  "cvpr2020_w53_improvedsocceractionspottingusingbothaudioandvideostreams": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Improved Soccer Action Spotting Using Both Audio and Video Streams",
    "authors": [
      "Bastien Vanderplaetse",
      "Stephane Dupont"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Vanderplaetse_Improved_Soccer_Action_Spotting_Using_Both_Audio_and_Video_Streams_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Vanderplaetse_Improved_Soccer_Action_Spotting_Using_Both_Audio_and_Video_Streams_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a study on multi-modal (audio and video) action spotting and classification in soccer videos. Action spotting and classification are the tasks that consist in finding the temporal anchors of events in a video and determine which event they are. This is an important application of general activity understanding. Here, we propose an experimental study on combining audio and video information at different stages of deep neural network architectures. We used the SoccerNet benchmark dataset, which contains annotated events for 500 soccer game videos from the Big Five European leagues. Through this work, we evaluated several ways to integrate audio stream into video-only-based architectures. We observed an average absolute improvement of the mean Average Precision (mAP) metric of 7.43% for the action classification task and of 4.19% for the action spotting task.\r"
  },
  "cvpr2020_w53_groupactivitydetectionfromtrajectoryandvideodatainsoccer": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "Group Activity Detection From Trajectory and Video Data in Soccer",
    "authors": [
      "Ryan Sanford",
      "Siavash Gorji",
      "Luiz G. Hafemann",
      "Bahareh Pourbabaee",
      "Mehrsan Javan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Sanford_Group_Activity_Detection_From_Trajectory_and_Video_Data_in_Soccer_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Sanford_Group_Activity_Detection_From_Trajectory_and_Video_Data_in_Soccer_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Group activity detection in soccer can be done by using either video data or player and ball trajectory data. In current soccer activity datasets, activities are labelled as atomic events without a duration. Given that the state-of-the-art activity detection methods are not well-defined for atomic actions, these methods cannot be used. In this work, we evaluated the effectiveness of activity recognition models for detecting such events, by using an intuitive non-maximum suppression process and evaluation metrics. We also considered the problem of explicitly modeling interactions between players and ball. For this, we propose self-attention models to learn and extract relevant information from a group of soccer players for activity detection from both trajectory and video data. We conducted an extensive study on the use of visual features and trajectory data for group activity detection in sports using a large scale soccer dataset provided by Sportlogiq. Our results show that most events can be detected using either vision or trajectory-based approaches with a temporal resolution of less than 0.5 seconds, and that each approach has unique challenges\r"
  },
  "cvpr2020_w53_falconsfastlearner-graderforcontortedposesinsports": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w53",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision in Sports",
    "title": "FALCONS: FAst Learner-Grader for CONtorted Poses in Sports",
    "authors": [
      "Mahdiar Nekoui",
      "Fidel Omar Tito Cruz",
      "Li Cheng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w53/Nekoui_FALCONS_FAst_Learner-Grader_for_CONtorted_Poses_in_Sports_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w53/Nekoui_FALCONS_FAst_Learner-Grader_for_CONtorted_Poses_in_Sports_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Isn't it about time to help judges with the challenging task of evaluating athletes' performances in sports with extreme poses? To tackle this problem and inspired by human judges' grading schema, we propose a virtual refereeing network to evaluate the execution of a diving performance. This assessment would be based on visual clues as well as the body joints sequence of the action video. In order to cover the unusual body contortions in such scenarios, we present ExPose: annotated dataset of Extreme Poses. We further introduce a simple yet effective module to assess the difficulty of the performance based on the extracted joints sequence. Finally, the overall score of the performance would be reported as the multiplication of the execution and difficulty scores. The results demonstrate our proposed lightweight network not only achieves state-of-the-art results compared to previous studies in diving but also shows acceptable generalization to other contortive sports.\r"
  },
  "cvpr2020_w66_multi-cameratrajectoryforecastingpedestriantrajectorypredictioninanetworkofcameras": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w66",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Precognition: Seeing Through the Future",
    "title": "Multi-Camera Trajectory Forecasting: Pedestrian Trajectory Prediction in a Network of Cameras",
    "authors": [
      "Olly Styles",
      "Tanaya Guha",
      "Victor Sanchez",
      "Alex Kot"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w66/Styles_Multi-Camera_Trajectory_Forecasting_Pedestrian_Trajectory_Prediction_in_a_Network_of_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w66/Styles_Multi-Camera_Trajectory_Forecasting_Pedestrian_Trajectory_Prediction_in_a_Network_of_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We introduce the task of multi-camera trajectory forecasting (MCTF), where the future trajectory of an object is predicted in a network of cameras. Prior works consider forecasting trajectories in a single camera view. Our work is the first to consider the challenging scenario of forecasting across multiple non-overlapping camera views. This has wide applicability in tasks such as re-identification and multi-target multi-camera tracking. To facilitate research in this new area, we release the Warwick-NTU Multi-camera Forecasting Database (WNMF), a unique dataset of multi-camera pedestrian trajectories from a network of 15 synchronized cameras. To accurately label this large dataset (600 hours of video footage), we also develop a semi-automated annotation method. An effective MCTF model should proactively anticipate where and when a person will re-appear in the camera network. In this paper, we consider the task of predicting the next camera a pedestrian will re-appear after leaving the view of another camera, and present several baseline approaches for this. The labeled database is available online: https://github.com/olly-styles/Multi-Camera-Trajectory-Forecasting.\r"
  },
  "cvpr2020_w66_dynamicneuralrelationalinferenceforforecastingtrajectories": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w66",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Precognition: Seeing Through the Future",
    "title": "Dynamic Neural Relational Inference for Forecasting Trajectories",
    "authors": [
      "Colin Graber",
      "Alexander Schwing"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w66/Graber_Dynamic_Neural_Relational_Inference_for_Forecasting_Trajectories_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w66/Graber_Dynamic_Neural_Relational_Inference_for_Forecasting_Trajectories_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Understanding interactions between entities, e.g., joints of the human body, team sports players, etc., is crucial for tasks like forecasting. However, interactions between entities are commonly not observed and often hard to quantify. To address this challenge, recently, 'Neural Relational Inference' was introduced. It predicts static relations between entities in a system and provides an interpretable representation of the underlying system dynamics that are used for better trajectory forecasting. However, generally, relations between entities change as time progresses. Hence, static relations improperly model the data. In response to this, we develop Dynamic Neural Relational Inference (dNRI), which incorporates insights from sequential latent variable models to predict separate relation graphs for every time-step. We demonstrate on several real-world datasets that modeling dynamic relations improves forecasting of complex trajectories.\r"
  },
  "cvpr2020_w66_glaucomaprecognitionrecognizingpreclinicalvisualfunctionalsignsofglaucoma": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w66",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Precognition: Seeing Through the Future",
    "title": "Glaucoma Precognition: Recognizing Preclinical Visual Functional Signs of Glaucoma",
    "authors": [
      "Krati Gupta",
      "Anshul Thakur",
      "Michael Goldbaum",
      "Siamak Yousefi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w66/Gupta_Glaucoma_Precognition_Recognizing_Preclinical_Visual_Functional_Signs_of_Glaucoma_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w66/Gupta_Glaucoma_Precognition_Recognizing_Preclinical_Visual_Functional_Signs_of_Glaucoma_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep archetypal analysis (DAA) has recently been proposed as an unsupervised approach for discovering latent structures in data. However, while a few approaches have used classical archetypal analysis (AA), DAA has not been incorporated in medical image analysis as yet. The purpose of this study is to develop a precognition framework to identify preclinical signs of glaucomatous vision loss using convex representations derived from DAA. We first develop an AA structure and a novel DAA framework to recognize hidden patterns of visual functional loss, and then project visual field data over the identified patterns to obtain a representation for glaucoma precognition several years prior to disease onset. We then develop a glaucoma classification framework using class-balanced bagging with neural networks to address the class imbalance problem. In contrast to other classification approaches, DAA, applied to a unique prospective longitudinal dataset with approximately eight years of visual field tests from normal eyes that developed glaucoma, has allowed visualization of the early signs of glaucoma and development of a construct for glaucoma precognition. Our findings suggest that our proposed glaucoma precognition approach could significantly advance state-of-the-art glaucoma prediction.\r"
  },
  "cvpr2020_w66_amultimodalpredictiveagentmodelforhumaninteractiongeneration": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w66",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Precognition: Seeing Through the Future",
    "title": "A Multimodal Predictive Agent Model for Human Interaction Generation",
    "authors": [
      "Murchana Baruah",
      "Bonny Banerjee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w66/Baruah_A_Multimodal_Predictive_Agent_Model_for_Human_Interaction_Generation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w66/Baruah_A_Multimodal_Predictive_Agent_Model_for_Human_Interaction_Generation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Perception and action are inextricably tied together. We propose an agent model which consists of perceptual and proprioceptive pathways. The agent actively samples a sequence of percepts from its environment using the perception-action loop. The model predicts to complete the partial percept and propriocept sequences observed till each sampling instant, and learns where and what to sample from the prediction error, without supervision or reinforcement. The model is implemented using a multimodal variational recurrent neural network. The model is exposed to videos of two-person interactions, where one person is the modeled agent and the other person's actions constitute its visual observation. For each interaction class, the model learns to selectively attend to locations in the other person's body. The proposed attention-based agent is the first of its kind to interact with and learn end-to-end from human interactions, and generate realistic interactions with performance comparable to models without attention and using significantly more computational resources.\r"
  },
  "cvpr2020_w66_silaanincrementallearningapproachforpedestriantrajectoryprediction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w66",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Precognition: Seeing Through the Future",
    "title": "SILA: An Incremental Learning Approach for Pedestrian Trajectory Prediction",
    "authors": [
      "Golnaz Habibi",
      "Nikita Jaipuria",
      "Jonathan P. How"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w66/Habibi_SILA_An_Incremental_Learning_Approach_for_Pedestrian_Trajectory_Prediction_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w66/Habibi_SILA_An_Incremental_Learning_Approach_for_Pedestrian_Trajectory_Prediction_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The prediction of pedestrian motion is challenging, especially in crowded roads and intersections. Most of the current approaches apply offline methods to learn motion behaviors, but as a result, they are not able to learn continuously and typically do not generalize well to new environments. This paper presents Similarity-based Incremental Learning Algorithm (SILA) for pedestrian motion prediction with the ability of improving the learned model over the time as data is obtained incrementally. To keep the model size efficient, the motion primitives learned from the new data are compared with the previously known ones, and similar motion primitives are fused while novel motion primitives are added to the model. Results show that the SILA model growth rate is about 1/3 that of an incremental approach that does not fuse motion primitives. SILA is evaluated on different datasets and scenarios including intersections and busy streets. The results show that, even though SILA learns incrementally, it performs comparably to (and sometimes outperforms) state-of-the-art algorithms in pedestrian prediction. Additionally, SILA learning time only depends on the size of the data added incrementally, which makes SILA more efficient in terms of time and space compared to batch learning.\r"
  },
  "cvpr2020_w69_ipg-netimagepyramidguidancenetworkforsmallobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w69",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Catch UAVs That Want to Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild",
    "title": "IPG-Net: Image Pyramid Guidance Network for Small Object Detection",
    "authors": [
      "Ziming Liu",
      "Guangyu Gao",
      "Lin Sun",
      "Li Fang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w69/Liu_IPG-Net_Image_Pyramid_Guidance_Network_for_Small_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w69/Liu_IPG-Net_Image_Pyramid_Guidance_Network_for_Small_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " For Convolutional Neural Network-based object detection, there is a typical dilemma: the spatial information is well kept in the shallow layers which unfortunately do not have enough semantic information, while the deep layers have a high semantic concept but lost a lot of spatial information, resulting in serious information imbalance. To acquire enough semantic information for shallow layers, Feature Pyramid Networks (FPN) is used to build a top-down propagated path. In this paper, except for top-down combining of information for shallow layers, we propose a novel network called Image Pyramid Guidance Network (IPG-Net) to make sure both the spatial information and semantic information are abundant for each layer. Our IPG-Net has two main parts: the image pyramid guidance transformation module and the image pyramid guidance fusion module. Our main idea is to introduce the image pyramid guidance into the backbone stream to solve the information imbalance problem, which alleviates the vanishment of the small object features. This IPG transformation module promises even in the deepest stage of the backbone, there is enough spatial information for bounding box regression and classification. Furthermore, we designed an effective fusion module to fuse the features from the image pyramid and features from the backbone stream. We have tried to apply this novel network to both one-stage and two-stage detection models, state of the art results are obtained on the most popular benchmark data sets, i.e. MS COCO and Pascal VOC.\r"
  },
  "cvpr2020_w69_real-timetrackingwithstabilizedframe": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w69",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Catch UAVs That Want to Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild",
    "title": "Real-Time Tracking With Stabilized Frame",
    "authors": [
      "Zixuan Wang",
      "Zhicheng Zhao",
      "Fei Su"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w69/Wang_Real-Time_Tracking_With_Stabilized_Frame_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w69/Wang_Real-Time_Tracking_With_Stabilized_Frame_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning methods have dramatically increased tracking accuracy benefitting from exquisite features extractor. Among these methods, siamese-based tracker performs well. However, in case of camera shaking, the objects are easily to be lost because of no consideration of camera judder, and the position of each pixel changes drastically between frames. In particular, the tracking performance would degrade dramatically in case that the target is small and moving fast, such as UAV tracking. In this paper, the S-Siam framework is proposed to deal with this problem and improves the performance of real-time tracking. Through stabilizing each frame by estimating where the object is going to move, the camera is adjusted adaptively to keep the object in its original position. Experimental results on the VOT2018 dataset show that the proposed method obtained an EAO score 0.449, and achieved 10% robustness improvement compared with existing three trackers, i.e., SiamFC, SiamMask and SiamRPN++, which demonstrates the effectiveness of the proposed algorithm.\r"
  },
  "cvpr2020_w69_effectofannotationerrorsondronedetectionwithyolov3": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w69",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Catch UAVs That Want to Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild",
    "title": "Effect of Annotation Errors on Drone Detection With YOLOv3",
    "authors": [
      "Aybora Koksal",
      "Kutalmis Gokalp Ince",
      "Aydin Alatan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w69/Koksal_Effect_of_Annotation_Errors_on_Drone_Detection_With_YOLOv3_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w69/Koksal_Effect_of_Annotation_Errors_on_Drone_Detection_With_YOLOv3_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Following the recent advances in deep networks, object detection and tracking algorithms with deep learning backbones have been improved significantly; however, this rapid development resulted in the necessity of large amounts of annotated labels. Even if the details of such semi-automatic annotation processes for most of these datasets are not known precisely, especially for the video annotations, some automated labeling processes are usually employed. Unfortunately, such approaches might result with erroneous annotations. In this work, different types of annotation errors for object detection problem are simulated and the performance of a popular state-of-the-art object detector, YOLOv3, with erroneous annotations during training and testing stages is examined. Moreover, some inevitable annotation errors in CVPR-2020 Anti-UAV Challenge dataset is also examined in this manner, while proposing a solution to correct such annotation errors of this valuable data set.\r"
  },
  "cvpr2020_w69_areal-timerobustapproachfortrackinguavsininfraredvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w69",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Catch UAVs That Want to Watch You: Detection and Tracking of Unmanned Aerial Vehicle in the Wild",
    "title": "A Real-Time Robust Approach for Tracking UAVs in Infrared Videos",
    "authors": [
      "Han Wu",
      "Weiqiang Li",
      "Wanqi Li",
      "Guizhong Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w69/Wu_A_Real-Time_Robust_Approach_for_Tracking_UAVs_in_Infrared_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w69/Wu_A_Real-Time_Robust_Approach_for_Tracking_UAVs_in_Infrared_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Object tracking has been studied for decades, but most of the existing works are focused on the RGB tracking. For an infrared video, the object is often textureless, especially for far-range drone planar targets. Furthermore, motion of camera and unexpected movement of the drones make tracking more difficult, causing existing object tracking algorithms lose the targets. In this paper a robust and real-time tracking algorithm is proposed for infrared drones, in which a feature attention module and an expansion strategy for searching the target are added to the fully convolutional classifier. Experiments on the Anti-UAV infrared dataset show its robustness to the different challenges of real infrared scenes with a high efficiency.\r"
  },
  "cvpr2020_w26_learningintuitivephysicsbyexplainingsurprise": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "Learning Intuitive Physics by Explaining Surprise",
    "authors": [
      "Hung Nguyen",
      "Jay Patravali",
      "Fuxin Li",
      "Alan Fern"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Nguyen_Learning_Intuitive_Physics_by_Explaining_Surprise_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Nguyen_Learning_Intuitive_Physics_by_Explaining_Surprise_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The IntPhys Challenge aims to evaluate how well algorithms capture 'common sense' about the physical world by measuring the ability to detect violations of intuitive physics in dynamic multi-object visual scenes. One approach to this problem is to define or learn a detailed model of the observations and dynamics and to then detect violations of that model. While viable, this approach poses challenges in acquiring an accurate enough model that can handle detailed non-linear object interactions, such as visual occlusion and collisions. In this work, we consider an alternative approach, the Surprise and Explain (SnE) framework, which aims for simplicity while remaining highly flexible. The key idea is to exploit the assumption that, for the vast majority of time, objects follow simple dynamic models, e.g. linear dynamics. Further, when the simple dynamics are occasionally violated ('surprises') due to non-linear interactions, e.g. collisions and occlusion, it is assumed that there is a small set of detectable explanations for the surprise. Violations of intuitive physics then correspond to surprises for which an explanation cannot be inferred. This paper develops an instantiation of the SnE framework and demonstrates its potential in the IntPhys Challenge by placing 2nd at the time of this paper's submission.\r"
  },
  "cvpr2020_w26_storycompletionwithexplicitmodelingofcommonsenseknowledge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "Story Completion With Explicit Modeling of Commonsense Knowledge",
    "authors": [
      "Mingda Zhang",
      "Keren Ye",
      "Rebecca Hwa",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Zhang_Story_Completion_With_Explicit_Modeling_of_Commonsense_Knowledge_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Zhang_Story_Completion_With_Explicit_Modeling_of_Commonsense_Knowledge_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Growing up with bedtime tales, even children could easily tell how a story should develop; but selecting a coherent and reasonable ending for a story is still not easy for machines. To successfully choose an ending requires not only detailed analysis of the context, but also applying commonsense reasoning and basic knowledge. Previous work has shown that language models trained on very large corpora could capture common sense in an implicit and hard-to-interpret way. We explore another direction and present a novel method that explicitly incorporates commonsense knowledge from a structured dataset, and demonstrate the potential for improving story completion.\r"
  },
  "cvpr2020_w26_visualcommonsenserepresentationlearningviacausalinference": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "Visual Commonsense Representation Learning via Causal Inference",
    "authors": [
      "Tan Wang",
      "Jianqiang Huang",
      "Hanwang Zhang",
      "Qianru Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Con-volutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for high-level tasks such as captioning and VQA. Given a set of detected object regions in an image (e.g., using Faster R-CNN), like any other un-supervised feature learning methods (e.g., word2vec), the proxy training objective of VC R-CNN is to predict the con-textual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by using causal intervention: P(Y|do(X)), while others are by using the conventional likelihood:P(Y|X). We extensively apply VC R-CNN features in prevailing models of two popular tasks: Image Captioning and VQA, and observe consistent performance boosts across all the methods, achieving many new state-of-the-arts. Code and feature are available at https://github.com/Wangt-CN/VC-R-CNN. For better clarity, you can also refer to the full version of this paper in [11].\r"
  },
  "cvpr2020_w26_somethingfinderlocalizingundefinedregionsusingreferringexpressions": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "SomethingFinder: Localizing Undefined Regions Using Referring Expressions",
    "authors": [
      "Sungmin Eum",
      "David Han",
      "Gordon Briggs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Eum_SomethingFinder_Localizing_Undefined_Regions_Using_Referring_Expressions_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Eum_SomethingFinder_Localizing_Undefined_Regions_Using_Referring_Expressions_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Previous research on localizing a target region in an image referred to by a natural language expression has occurred within an object-centric paradigm. However, in practice, there may not be any easily named or identifiable objects near a target location. Instead, references may need to rely on basic visual attributes, such as color or geometric clues. An expression like \"a red something beside a blue vertical line\" could still pinpoint a target location. As such, we begin to explore the open challenge of computational object-agnostic reference by constructing a novel dataset and by devising a new set of algorithms that can identify a target region in an image when given a referring expression containing only basic conceptual features.\r"
  },
  "cvpr2020_w26_responsetimeanalysisforexplainabilityofvisualprocessingincnns": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "Response Time Analysis for Explainability of Visual Processing in CNNs",
    "authors": [
      "Eric Taylor",
      "Shashank Shekhar",
      "Graham W. Taylor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Taylor_Response_Time_Analysis_for_Explainability_of_Visual_Processing_in_CNNs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Taylor_Response_Time_Analysis_for_Explainability_of_Visual_Processing_in_CNNs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Explainable artificial intelligence (XAI) methods rely on access to model architecture and parameters that is not always feasible for most users, practitioners, and regulators. Drawing inspiration from cognitive psychology, we present a case for response times (RTs) as a technique for XAI. RTs are observable without access to the model. Moreover, dynamic inference models performing conditional computation generate variable RTs for visual learning tasks depending on hierarchical representations. We show that MSDNet, a conditional computation model with early-exit architecture, exhibits slower RT for images with more complex features in the ObjectNet test set, as well as the human phenomenon of scene grammar, where object recognition depends on intra-scene object-object relationships. These results cast a light on MSDNet's hierarchical feature space without opening the black box and illustrate the promise of RT as a technique for XAI.\r"
  },
  "cvpr2020_w26_hierarchicalcolorlearninginconvolutionalneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "Hierarchical Color Learning in Convolutional Neural Networks",
    "authors": [
      "Chris Hickey",
      "Byoung-Tak Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Hickey_Hierarchical_Color_Learning_in_Convolutional_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Hickey_Hierarchical_Color_Learning_in_Convolutional_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Empirical evidence suggests that color categories emerge in a universal, recurrent, hierarchical pattern across different cultures. This pattern is referred to as the \"Color Hierarchy\". Over two experiments, the present study examines whether there is evidence for such hierarchical color category learning patterns in Convolutional Neural Networks (CNNs). Experiment A investigated whether color categories are learned randomly, or in a fixed, hierarchical fashion. Results show that colors higher up the Color Hierarchy (e.g. red) were generally learned before colors lower down the hierarchy (e.g. brown, orange, gray). Experiment B examined whether object color affects recall in object detection. Similar to Experiment A, results found that object recall was noticeably impacted by color, with colors higher up the Color Hierarchy generally showing better recall. Additionally, objects whose color can be described by adjectives that emphasise colorfulness (e.g. Vivid, Brilliant, Deep) show better recall than those which de-emphasise colorfulness (e.g. Dark, Pale, Light). These results highlight similarities between humans and CNNs in color perception, and provide insight into factors that influence object detection.\r"
  },
  "cvpr2020_w26_understandingknowledgegapsinvisualquestionansweringimplicationsforgapidentificationandtesting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "Understanding Knowledge Gaps in Visual Question Answering: Implications for Gap Identification and Testing",
    "authors": [
      "Goonmeet Bajaj",
      "Bortik Bandyopadhyay",
      "Daniel Schmidt",
      "Pranav Maneriker",
      "Christopher Myers",
      "Srinivasan Parthasarathy"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Bajaj_Understanding_Knowledge_Gaps_in_Visual_Question_Answering_Implications_for_Gap_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Bajaj_Understanding_Knowledge_Gaps_in_Visual_Question_Answering_Implications_for_Gap_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Traditional Visual Question Answering (VQA) datasets typically contain questions related to the spatial information of objects, object attributes, or general scene questions. Recently, researchers have recognized the need to improve the balance of such datasets to reduce the system's dependency on memorized linguistic features and statistical biases, while aiming for enhanced visual understanding. However, it is unclear whether any latent patterns exist to quantify and explain these failures. As an initial step towards better quantifying our understanding of the performance of VQA models, we use a taxonomy of Knowledge Gaps (KGs) to tag questions with one or more types of KGs. Each KG describes the reasoning abilities needed to arrive at a resolution, and failure to resolve gaps indicates an absence of the required reasoning ability. After identifying KGs for each question, we examine the skew in the distribution of questions for each KG. We then introduce a targeted question generation model to reduce this skew, which allows us to generate new types of questions for an image.\r"
  },
  "cvpr2020_w26_3dq-netsvisualconceptsemergeinposeequivariant3dquantizedneuralscenerepresentations": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w26",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Minds vs. Machines: How Far Are We From the Common Sense of a Toddler?",
    "title": "3DQ-Nets: Visual Concepts Emerge in Pose Equivariant 3D Quantized Neural Scene Representations",
    "authors": [
      "Mihir Prabhudesai",
      "Shamit Lal",
      "Hsiao-Yu Fish Tung",
      "Adam W. Harley",
      "Shubhankar Potdar",
      "Katerina Fragkiadaki"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w26/Prabhudesai_3DQ-Nets_Visual_Concepts_Emerge_in_Pose_Equivariant_3D_Quantized_Neural_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w26/Prabhudesai_3DQ-Nets_Visual_Concepts_Emerge_in_Pose_Equivariant_3D_Quantized_Neural_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a framework that learns 3D object concepts without supervision from 3D annotations. Our model detects objects, quantizes their features into prototypes, infers associations across detected objects in different scenes, and uses those to (self) supervise its visual feature representations. Object detection, correspondence inference, representation learning, and object to prototype compression takes place in a 3-dimensional visual feature space, inferred from the input RGB-D images using differentiable inverse graphics architectures, optimized end-to-end for predicting views of scenes. Our 3D feature space learns to be invariant to the camera viewpoint and disentangled from projection artifacts, foreshortenings or cross-object occlusions. As a result, 3D features learn to establish accurate correspondences across objects found under varying camera viewpoints, size and pose, and compressing them into prototypes. Our prototypes are represented similarly by 3-dimensional feature maps. They are rotated and scaled appropriately during matching to explain object instances in a variety of 3D poses and scales. We show this pose and scale equivariance permits much better compressibility of objects into their prototypical representations. Our model is optimized with a mix of end-to-end gradient descent and expectation-maximization iterations. We show 3D object detection, correspondence inference and object-to-prototype clustering improve over time and help one another. We demonstrate the usefulness of our model in few-shot learning: one or few object labels suffice to learn a pose-aware 3D object detector for the object category. To the best of our knowledge, this is the first system that demonstrates that 3D visual concepts emerge, without language annotating, rather, by moving around and relating episodic visual experiences, in a self-paced automated learning process.\r"
  },
  "cvpr2020_w20_evaluatingscalablebayesiandeeplearningmethodsforrobustcomputervision": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision",
    "authors": [
      "Fredrik K. Gustafsson",
      "Martin Danelljan",
      "Thomas B. Schon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Gustafsson_Evaluating_Scalable_Bayesian_Deep_Learning_Methods_for_Robust_Computer_Vision_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Gustafsson_Evaluating_Scalable_Bayesian_Deep_Learning_Methods_for_Robust_Computer_Vision_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, e.g. in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is specifically designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the first properly extensive and conclusive comparison of the two current state-of-the-art scalable methods: ensembling and MC-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates.\r"
  },
  "cvpr2020_w20_improvednoiseandattackrobustnessforsemanticsegmentationbyusingmulti-tasktrainingwithself-superviseddepthestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training With Self-Supervised Depth Estimation",
    "authors": [
      "Marvin Klingner",
      "Andreas Bar",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Klingner_Improved_Noise_and_Attack_Robustness_for_Semantic_Segmentation_by_Using_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Klingner_Improved_Noise_and_Attack_Robustness_for_Semantic_Segmentation_by_Using_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While current approaches for neural network training often aim at improving performance, less focus is put on training methods aiming at robustness towards varying noise conditions or directed attacks by adversarial examples. In this paper, we propose to improve robustness by a multi-task training, which extends supervised semantic segmentation by a self-supervised monocular depth estimation on unlabeled videos. This additional task is only performed during training to improve the semantic segmentation model's robustness at test time under several input perturbations. Moreover, we even find that our joint training approach also improves the performance of the model on the original (supervised) semantic segmentation task. Our evaluation exhibits a particular novelty in that it allows to mutually compare the effect of input noises and adversarial attacks on the robustness of the semantic segmentation. We show the effectiveness of our method on the Cityscapes dataset, where our multi-task training approach consistently outperforms the single-task semantic segmentation baseline in terms of both robustness vs. noise and in terms of adversarial attacks, without the need for depth labels in training.\r"
  },
  "cvpr2020_w20_attentionalbottlenecktowardsaninterpretabledeepdrivingnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Attentional Bottleneck: Towards an Interpretable Deep Driving Network",
    "authors": [
      "Jinkyu Kim",
      "Mayank Bansal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Kim_Attentional_Bottleneck_Towards_an_Interpretable_Deep_Driving_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Kim_Attentional_Bottleneck_Towards_an_Interpretable_Deep_Driving_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep neural networks are a key component of behavior prediction and motion generation for self-driving cars. One of their main drawbacks is a lack of transparency: they should provide easy to interpret rationales for what triggers certain behaviors. We propose an architecture called Attentional Bottleneck with the goal of improving transparency. Our key idea is to combine visual attention, which identifies what aspects of the input the model is using, with an information bottleneck that enables the model to only use aspects of the input which are important. This not only provides sparse and interpretable attention maps (e.g. focusing only on specific vehicles in the scene), but it adds this transparency at no cost to model accuracy. In fact, we find slight improvements in accuracy when applying Attentional Bottleneck to the ChauffeurNet model in comparison to a traditional visual attention model that degrades accuracy.\r"
  },
  "cvpr2020_w20_leveragingcombinatorialtestingforsafety-criticalcomputervisiondatasets": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Leveraging Combinatorial Testing for Safety-Critical Computer Vision Datasets",
    "authors": [
      "Christoph Gladisch",
      "Christian Heinzemann",
      "Martin Herrmann",
      "Matthias Woehrle"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Gladisch_Leveraging_Combinatorial_Testing_for_Safety-Critical_Computer_Vision_Datasets_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Gladisch_Leveraging_Combinatorial_Testing_for_Safety-Critical_Computer_Vision_Datasets_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning-based approaches have gained popularity for environment perception tasks such as semantic segmentation and object detection from images. However, the different nature of a data-driven deep neural nets (DNN) to conventional software is a challenge for practical software verification. In this work, we show how existing methods from software engineering provide benefits for the development of a DNN and in particular for dataset design and analysis. We show how combinatorial testing based on a domain model can be leveraged for generating test sets providing coverage guarantees with respect to important environmental features and their interaction. Additionally, we show how our approach can be used for growing a dataset, i.e. to identify where data is missing and should be collected next. We evaluate our approach on an internal use case and two public datasets.\r"
  },
  "cvpr2020_w20_multivariateconfidencecalibrationforobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Multivariate Confidence Calibration for Object Detection",
    "authors": [
      "Fabian Kuppers",
      "Jan Kronenberger",
      "Amirhossein Shantia",
      "Anselm Haselhoff"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Kuppers_Multivariate_Confidence_Calibration_for_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Kuppers_Multivariate_Confidence_Calibration_for_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Unbiased confidence estimates of neural networks are crucial especially for safety-critical applications. Many methods have been developed to calibrate biased confidence estimates. Though there is a variety of methods for classification, the field of object detection has not been addressed yet. Therefore, we present a novel framework to measure and calibrate biased (or miscalibrated) confidence estimates of object detection methods. The main difference to related work in the field of classifier calibration is that we also use additional information of the regression output of an object detector for calibration. Our approach allows, for the first time, to obtain calibrated confidence estimates with respect to image location and box scale. In addition, we propose a new measure to evaluate miscalibration of object detectors. Finally, we show that our developed methods outperform state-of-the-art calibration models for the task of object detection and provides reliable confidence estimates across different locations and scales.\r"
  },
  "cvpr2020_w20_detectionandretrievalofout-of-distributionobjectsinsemanticsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Detection and Retrieval of Out-of-Distribution Objects in Semantic Segmentation",
    "authors": [
      "Philipp Oberdiek",
      "Matthias Rottmann",
      "Gernot A. Fink"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Oberdiek_Detection_and_Retrieval_of_Out-of-Distribution_Objects_in_Semantic_Segmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Oberdiek_Detection_and_Retrieval_of_Out-of-Distribution_Objects_in_Semantic_Segmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " When deploying deep learning technology in self-driving cars, deep neural networks are constantly exposed to domain shifts. These include, e.g., changes in weather conditions, time of day, and long-term temporal shift. In this work we utilize a deep neural network trained on the Cityscapes dataset containing urban street scenes and infer images from a different dataset, the A2D2 dataset, containing also countryside and highway images. We present a novel pipeline for semantic segmenation that detects out-of-distribution (OOD) segments by means of the deep neural network's prediction and performs image retrieval after feature extraction and dimensionality reduction on image patches. In our experiments we demonstrate that the deployed OOD approach is suitable for detecting out-of-distribution concepts. Furthermore, we evaluate the image patch retrieval qualitatively as well as quantitatively by means of the semi-compatible A2D2 ground truth and obtain mAP values of up to 52.2%.\r"
  },
  "cvpr2020_w20_generatingsociallyacceptableperturbationsforefficientevaluationofautonomousvehicles": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Generating Socially Acceptable Perturbations for Efficient Evaluation of Autonomous Vehicles",
    "authors": [
      "Songan Zhang",
      "Huei Peng",
      "Subramanya Nageshrao",
      "H. Eric Tseng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Zhang_Generating_Socially_Acceptable_Perturbations_for_Efficient_Evaluation_of_Autonomous_Vehicles_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Zhang_Generating_Socially_Acceptable_Perturbations_for_Efficient_Evaluation_of_Autonomous_Vehicles_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep reinforcement learning methods have been considered and implemented for autonomous vehicle's decision-making in recent years. A key issue is that deep neural networks can be fragile to adversarial attacks through unseen inputs, and thus the reinforcement learning policy, that uses deep neural networks would be also fragile to malicious attacks or benign but out of distribution perturbations. In this paper, we address the latter issue: we focus on generating socially acceptable perturbations (SAP), so that the autonomous vehicle (AV agent under evaluation), instead of the challenging vehicle (challenger), is primarily responsible for the crash. In our process, one challenger is added to the environment and trained by deep reinforcement learning to generate the desired perturbation. The reward is designed so that the challenger aims to fail the AV agent in a socially acceptable way. After training the challenger, the AV agent policy is evaluated in both the original naturalistic environment and the environment with one challenger. The results show that the AV agent policy which is safe in the naturalistic environment has many crashes in the perturbed environment.\r"
  },
  "cvpr2020_w20_robustsemanticsegmentationbyredundantnetworkswithalayer-specificlosscontributionandmajorityvote": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Robust Semantic Segmentation by Redundant Networks With a Layer-Specific Loss Contribution and Majority Vote",
    "authors": [
      "Andreas Bar",
      "Marvin Klingner",
      "Serin Varghese",
      "Fabian Huger",
      "Peter Schlicht",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Bar_Robust_Semantic_Segmentation_by_Redundant_Networks_With_a_Layer-Specific_Loss_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Bar_Robust_Semantic_Segmentation_by_Redundant_Networks_With_a_Layer-Specific_Loss_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The lack of robustness shown by deep neural networks (DNNs) questions their deployment in safety-critical tasks, such as autonomous driving. We pick up the recently introduced redundant teacher-student frameworks (3 DNNs) and propose in this work a novel error detection and correction scheme with application to semantic segmentation. It obtains its robustnesss by an online-adapted and therefore hard-to-attack student DNN during vehicle operation, which builds upon a novel layer-dependent inverse feature matching (IFM) loss. We conduct experiments on the Cityscapes dataset showing that this loss renders the adaptive student to be more than 20% absolute mean intersection-over-union (mIoU) better than in previous works. Moreover, the entire error correction virtually always delivers the performance of the best non-attacked network, resulting in an mIoU of about 50% even under strongest attacks (instead of 1...2%), while keeping the performance on clean data at about original level (ca. 75.7%).\r"
  },
  "cvpr2020_w20_self-superviseddomainmismatchestimationforautonomousperception": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Self-Supervised Domain Mismatch Estimation for Autonomous Perception",
    "authors": [
      "Jonas Lohdefink",
      "Justin Fehrling",
      "Marvin Klingner",
      "Fabian Huger",
      "Peter Schlicht",
      "Nico M. Schmidt",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Lohdefink_Self-Supervised_Domain_Mismatch_Estimation_for_Autonomous_Perception_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Lohdefink_Self-Supervised_Domain_Mismatch_Estimation_for_Autonomous_Perception_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Autonomous driving requires self awareness of its perception functions. Technically spoken, this can be realized by observers, which monitor the performance indicators of various perception modules. In this work we choose, exemplarily, a semantic segmentation to be monitored, and propose an autoencoder, trained in a self-supervised fashion on the very same training data as the semantic segmentation to be monitored. While the autoencoder's image reconstruction performance (PSNR) during online inference shows already a good predictive power w.r.t. semantic segmentation performance, we propose a novel domain mismatch metric DM as the earth mover's distance between a pre-stored PSNR distribution on training (source) data, and an online-acquired PSNR distribution on any inference (target) data. We are able to show by experiments that the DM metric has a strong rank order correlation with the semantic segmentation within its functional scope. We also propose a training domain-dependent threshold for the DM metric to define this functional scope.\r"
  },
  "cvpr2020_w20_unsupervisedtemporalconsistencymetricforvideosegmentationinhighly-automateddriving": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Unsupervised Temporal Consistency Metric for Video Segmentation in Highly-Automated Driving",
    "authors": [
      "Serin Varghese",
      "Yasin Bayzidi",
      "Andreas Bar",
      "Nikhil Kapoor",
      "Sounak Lahiri",
      "Jan David Schneider",
      "Nico M. Schmidt",
      "Peter Schlicht",
      "Fabian Huger",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Varghese_Unsupervised_Temporal_Consistency_Metric_for_Video_Segmentation_in_Highly-Automated_Driving_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Varghese_Unsupervised_Temporal_Consistency_Metric_for_Video_Segmentation_in_Highly-Automated_Driving_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Commonly used metrics to evaluate semantic segmentation such as mean intersection over union (mIoU) do not incorporate temporal consistency. A straightforward extension of existing metrics towards evaluating the consistency of segmentation of video sequences does not exist, since labelled videos are rare and very expensive to obtain. For safety-critical applications such as highly automated driving, there is, however, a need for a metric that measures such temporal consistency of video segmentation networks to possibly support safety requirements. In this paper, (a) we introduce a metric which does not require segmentation labels for measuring the stability of the predictions of segmentation networks over a series of images; (b) we perform an in-depth analysis of the proposed metric and observe strong correlations to the supervised mIoU metric; (c) we perform an evaluation of five state-of-the-art networks for semantic segmentation of varying complexities and architectures evaluated on two public datasets, namely, Cityscapes and CamVid. Finally, we perform timing evaluations and propose the use of the metric as either an online observer for identification of possibly unstable segmentation predictions, or as an offline method to evaluate or to improve semantic segmentation networks, e.g., by selecting additional training data with critical temporal consistency.\r"
  },
  "cvpr2020_w20_mindthegap-abenchmarkfordensedepthpredictionbeyondlidar": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Mind the Gap - A Benchmark for Dense Depth Prediction Beyond Lidar",
    "authors": [
      "Hendrik Schilling",
      "Marcel Gutsche",
      "Alexander Brock",
      "Dane Spath",
      "Carsten Rother",
      "Karsten Krispin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Schilling_Mind_the_Gap_-_A_Benchmark_for_Dense_Depth_Prediction_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Schilling_Mind_the_Gap_-_A_Benchmark_for_Dense_Depth_Prediction_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The large interest in autonomous vehicles is a significant driver for computer vision research. Current deep learning approaches are capable of impressive feats, like dense full frame depth prediction from a single image. While impressive results have been achieved, it is not yet clear if they are sufficient for autonomous driving. The problem remains that existing evaluation benchmarks and metrics are not yet capable of fully addressing this issue. This work takes a step towards answering this question. Current evaluation methods are incapable of proving or refuting suitability for potentially hazardous real world situations. This is due to a) the large gaps in the currently used Lidar ground truth data, which cannot test many difficult and relevant cases and b) the global summary metrics used, which are intangible with respect to rigorous performance guarantees. In this work we provide a new benchmark based on commercially available dense light-field depth data, which closes these gaps in the evaluation. We implement domain specific and interpretable error metrics, which allow for strict assertions over the performance of tested methods. The leaderboard for dense depth prediction is publicly available. The approach is also transferable to other depth estimation tasks. Such stringent evaluations are indispensable when testing and demonstrating performance for potentially hazardous applications like autonomous driving, and are a critical aspect for the assessment of autonomous systems by regulatory bodies as well as for public acceptance.\r"
  },
  "cvpr2020_w20_explainingautonomousdrivingbylearningend-to-endvisualattention": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Explaining Autonomous Driving by Learning End-to-End Visual Attention",
    "authors": [
      "Luca Cultrera",
      "Lorenzo Seidenari",
      "Federico Becattini",
      "Pietro Pala",
      "Alberto Del Bimbo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Current deep learning based autonomous driving approaches yield impressive results also leading to in-production deployment in certain controlled scenarios. One of the most popular and fascinating approaches relies on learning vehicle controls directly from data perceived by sensors. This end-to-end learning paradigm can be applied both in classical supervised settings and using reinforcement learning. Nonetheless the main drawback of this approach as also in other learning problems is the lack of explainability. Indeed, a deep network will act as a black-box outputting predictions depending on previously seen driving patterns without giving any feedback on why such decisions were taken. While to obtain optimal performance it is not critical to obtain explainable outputs from a learned agent, especially in such a safety critical field, it is of paramount importance to understand how the network behaves. This is particularly relevant to interpret failures of such systems. In this work we propose to train an imitation learning based agent equipped with an attention model. The attention model allows us to understand what part of the image has been deemed most important. Interestingly, the use of attention also leads to superior performance in a standard benchmark using the CARLA driving simulator.\r"
  },
  "cvpr2020_w20_usingmixtureofexpertmodelstogaininsightsintosemanticsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w20",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Safe Artificial Intelligence for Automated Driving",
    "title": "Using Mixture of Expert Models to Gain Insights Into Semantic Segmentation",
    "authors": [
      "Svetlana Pavlitskaya",
      "Christian Hubschneider",
      "Michael Weber",
      "Ruby Moritz",
      "Fabian Huger",
      "Peter Schlicht",
      "Marius Zollner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w20/Pavlitskaya_Using_Mixture_of_Expert_Models_to_Gain_Insights_Into_Semantic_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w20/Pavlitskaya_Using_Mixture_of_Expert_Models_to_Gain_Insights_Into_Semantic_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Not only correct scene understanding, but also ability to understand the decision making process of neural networks is essential for safe autonomous driving. Current work mainly focuses on uncertainty measures, often based on Monte Carlo dropout, to gain at least some insight into a models confidence. We investigate a mixture of experts architecture to achieve additional interpretability while retaining comparable result quality. By being able to use both the overall model output as well as retaining the possibility to take into account individual expert outputs, the agreement or disagreement between those individual outputs can be used to gain insights into the decision process. Expert networks are trained by splitting the input data into semantic subsets, e.g. corresponding to different driving scenarios, to become experts in those domains. An additional gating network that is also trained on the same input data is consequently used to weight the output of individual experts. We evaluate this mixture of expert setup on the A2D2 dataset and achieve similar results to a baseline FRRN network trained on all available data, while getting additional information.\r"
  },
  "cvpr2020_w21_thevertigoeffectonyoursmartphonedollyzoomviasingleshotviewsynthesis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w21",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Dynamic Scene Reconstruction",
    "title": "The \"Vertigo Effect\" on Your Smartphone: Dolly Zoom via Single Shot View Synthesis",
    "authors": [
      "Yangwen Liang",
      "Rohit Ranade",
      "Shuangquan Wang",
      "Dongwoon Bai",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w21/Liang_The_Vertigo_Effect_on_Your_Smartphone_Dolly_Zoom_via_Single_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w21/Liang_The_Vertigo_Effect_on_Your_Smartphone_Dolly_Zoom_via_Single_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Dolly zoom is a technique where the camera is moved either forwards or backwards from the subject under focus while simultaneously adjusting the field of view in order to maintain the size of the subject in the frame. This results in perspective effect so that the subject in focus appears stationary while the background field of view changes. The effect is frequently used in films and requires skill, practice and equipment. This paper presents a novel technique to model the effect given a single shot capture from a single camera. The proposed synthesis pipeline based on camera geometry simulates the effect by producing a sequence of synthesized views. The technique is also extended to allow simultaneous captures from multiple cameras as inputs and can be easily extended to video sequence captures. Our pipeline consists of efficient image warping along with depth-aware image inpainting making it suitable for smartphone applications. The proposed method opens up new avenues for view synthesis applications in modern smartphones.\r"
  },
  "cvpr2020_w21_bilinearparameterizationfordifferentiablerank-regularization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w21",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Dynamic Scene Reconstruction",
    "title": "Bilinear Parameterization for Differentiable Rank-Regularization",
    "authors": [
      "Marcus Valtonen Ornhag",
      "Carl Olsson",
      "Anders Heyden"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w21/Ornhag_Bilinear_Parameterization_for_Differentiable_Rank-Regularization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w21/Ornhag_Bilinear_Parameterization_for_Differentiable_Rank-Regularization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Low rank approximation is a commonly occurring problem in many computer vision and machine learning applications. There are two common ways of optimizing the resulting models. Either the set of matrices with a given rank can be explicitly parametrized using a bilinear factorization, or low rank can be implicitly enforced using regularization terms penalizing non-zero singular values. While the former approach results in differentiable problems that can be efficiently optimized using local quadratic approximation, the latter is typically not differentiable (sometimes even discontinuous) and requires first order subgradient or splitting methods. It is well known that gradient based methods exhibit slow convergence for ill-conditioned problems. In this paper we show how many non-differentiable regularization methods can be reformulated into smooth objectives using bilinear parameterization. This allows us to use standard second order methods, such as Levenberg-Marquardt (LM) and Variable Projection (VarPro), to achieve accurate solutions for ill-conditioned cases. We show on several real and synthetic experiments that our second order formulation converges to substantially more accurate solutions than competing state-of-the-art methods.\r"
  },
  "cvpr2020_w21_semi-supervised3dfacerepresentationlearningfromunconstrainedphotocollections": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w21",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Dynamic Scene Reconstruction",
    "title": "Semi-Supervised 3D Face Representation Learning From Unconstrained Photo Collections",
    "authors": [
      "Zhongpai Gao",
      "Juyong Zhang",
      "Yudong Guo",
      "Chao Ma",
      "Guangtao Zhai",
      "Xiaokang Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w21/Gao_Semi-Supervised_3D_Face_Representation_Learning_From_Unconstrained_Photo_Collections_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w21/Gao_Semi-Supervised_3D_Face_Representation_Learning_From_Unconstrained_Photo_Collections_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recovering 3D geometry shape, albedo, and lighting from a single image is a typical ill-posed problem. To address this challenging problem, we propose to utilize the joint constraints from unconstrained photo collections of one person to recover his or her identity shape and albedo. Unconstrained photo collections include one's photos captured under different times, backgrounds, and expressions, e.g., photos posted on Instagram. We train our model in a semi-supervised manner with adversarial loss to exploit large amounts of unconstrained facial images. A novel center loss is introduced to make sure that facial images from the same subject have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.\r"
  },
  "cvpr2020_w22_deeplearningforautomaticpneumoniadetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "Deep Learning for Automatic Pneumonia Detection",
    "authors": [
      "Tatiana Gabruseva",
      "Dmytro Poplavskiy",
      "Alexandr Kalinin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Gabruseva_Deep_Learning_for_Automatic_Pneumonia_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Gabruseva_Deep_Learning_for_Automatic_Pneumonia_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Pneumonia is the leading cause of death among young children and one of the top mortality causes worldwide. The pneumonia detection is usually performed through examine of chest X-Ray radiograph by highly-trained specialists. This process is tedious and often leads to a disagreement between radiologists. Computer-aided diagnosis systems showed the potential for improving diagnostic accuracy. In this work, we develop the computational approach for pneumonia regions detection based on single-shot detectors, squeeze-and-extinction deep convolution neural networks, augmentations and multi-task learning. The proposed approach was evaluated in the context of the Radiological Society of North America Pneumonia Detection Challenge, achieving one of the best results in the challenge.\r"
  },
  "cvpr2020_w22_reinflexiblemeshgenerationfrompointclouds": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "REIN: Flexible Mesh Generation From Point Clouds",
    "authors": [
      "Rangel Daroya",
      "Rowel Atienza",
      "Rhandley Cajote"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Daroya_REIN_Flexible_Mesh_Generation_From_Point_Clouds_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Daroya_REIN_Flexible_Mesh_Generation_From_Point_Clouds_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " 3D reconstruction from sparse point clouds is a challenging problem. Existing methods interpolate from point clouds to produce meshes, but the performance decreases with the number of points. To address this, we propose an algorithm that looks at the global structure while reconstructing the surface one vertex at a time. Experimental results on ShapeNet and ModelNet10 show 81.5% Chamfer Distance and 14% Point Normal Similarity average improvement compared to Ball Pivoting Algorithm (BPA) and Poisson Surface Reconstruction (PSR). Qualitatively, the generated meshes have a closer similarity to the ground truth. Results on ShapeNet Patched illustrate significant improvement in mesh quality compared to BPA and PSR. The code is available at https://github.com/rangeldaroya/rein.\r"
  },
  "cvpr2020_w22_indefenseofthetripletlossagainlearningrobustpersonre-identificationwithfastapproximatedtripletlossandlabeldistillation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification With Fast Approximated Triplet Loss and Label Distillation",
    "authors": [
      "Ye Yuan",
      "Wuyang Chen",
      "Yang Yang",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Yuan_In_Defense_of_the_Triplet_Loss_Again_Learning_Robust_Person_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Yuan_In_Defense_of_the_Triplet_Loss_Again_Learning_Robust_Person_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The comparative losses (typically, triplet loss) are appealing choices for learning person re-identification (ReID) features. However, the triplet loss is computationally much more expensive than the (practically more popular) classification loss, limiting their wider usage in massive datasets. Moreover, the abundance of label noise and outliers in ReID datasets may also put the margin-based loss in jeopardy. This work addresses the above two shortcomings of triplet loss, extending its effectiveness to large-scale ReID datasets with potentially noisy labels. We propose a fast-approximated triplet (FAT) loss, which provably converts the point-wise triplet loss into its upper bound form, consisting of a point-to-set loss term plus cluster compactness regularization. It preserves the effectiveness of triplet loss, while leading to linear complexity to the training set size. A label distillation strategy is further designed to learn refined soft-labels in place of the potentially noisy labels, from only an identified subset of confident examples, through teacher-student networks. We conduct extensive experiments on three most popular ReID benchmarks (Market-1501, DukeMTMC-reID, and MSMT17), and demonstrate that FAT loss with distilled labels lead to ReID features with remarkable accuracy, efficiency, robustness, and direct transferability to unseen datasets.\r"
  },
  "cvpr2020_w22_salientobjectdetectionbycontextualrefinement": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "Salient Object Detection by Contextual Refinement",
    "authors": [
      "Sayanti Bardhan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Bardhan_Salient_Object_Detection_by_Contextual_Refinement_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Bardhan_Salient_Object_Detection_by_Contextual_Refinement_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Context plays an important role in the saliency prediction task. In this work, we propose a saliency detection framework that not only extracts visual features but also models two kinds of context including object-object relationships within a single image and scene contextual information. Specifically, we develop a novel saliency detection framework with a Contextual Refinement Module (CRM) which consists of two sub-networks, Object Relation Unit (ORU) and Scene Context Unit (SCU). ORU encodes the object-object relationship based on object relative position and object co-occurrence pattern in an image, by graphical approach, while SCU incorporates the scene contextual information of an image. Object Relation Unit (ORU) and Scene Context Unit (SCU) captures complementary contextual information to give a holistic estimation of salient regions. Extensive experiments show the effectiveness of modelling object relations and scene context in boosting the performance of saliency prediction. In particular, our frame-work outperforms the state-of-the-art models on challenging benchmark datasets.\r"
  },
  "cvpr2020_w22_eff-unetanovelarchitectureforsemanticsegmentationinunstructuredenvironment": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured Environment",
    "authors": [
      "Bhakti Baheti",
      "Shubham Innani",
      "Suhas Gajre",
      "Sanjay Talbar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Since the last few decades, the number of road causalities has seen continuous growth across the globe. Nowadays intelligent transportation systems are being developed to enable safe and relaxed driving and scene understanding of the surrounding environment is an integral part of it. While several approaches are being developed for semantic scene segmentation based on deep learning and Convolutional Neural Network (CNN), these approaches assume well structured road infrastructure and driving environment. We focus our work on recent India Driving Lite Dataset (IDD), which contains data from unstructured driving environment and was hosted as an online challenge in NCVPRIPG 2019. We propose a novel architecture named as Eff-UNet which combines the effectiveness of compound scaled EfficientNet as the encoder for feature extraction with UNet decoder for reconstructing the fine-grained segmentation map. High level feature information as well as low level spatial information useful for precise segmentation are combined. The proposed architecture achieved 0.7376 and 0.6276 mean Intersection over Union (mIoU) on validation and test dataset respectively and won first prize in IDD lite segmentation challenge outperforming other approaches in the literature.\r"
  },
  "cvpr2020_w22_usingsinusoidally-modulatednoiseasasurrogateforslow-wavesleeptoaccomplishstableunsuperviseddictionarylearninginaspike-basedsparsecodingmodel": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "Using Sinusoidally-Modulated Noise as a Surrogate for Slow-Wave Sleep to Accomplish Stable Unsupervised Dictionary Learning in a Spike-Based Sparse Coding Model",
    "authors": [
      "Yijing Watkins",
      "Edward Kim",
      "Andrew Sornborger",
      "Garrett T. Kenyon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Watkins_Using_Sinusoidally-Modulated_Noise_as_a_Surrogate_for_Slow-Wave_Sleep_to_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Watkins_Using_Sinusoidally-Modulated_Noise_as_a_Surrogate_for_Slow-Wave_Sleep_to_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Sparse coding algorithms have been used to model the acquisition of V1 simple cell receptive fields as well as to accomplish the unsupervised acquisition of features for a variety of machine learning applications. The Locally Competitive Algorithm (LCA) provides a biologically plausible implementation of sparse coding based on lateral inhibition. LCA can be reformulated to support dictionary learning via an online local Hebbian rule that reduces predictive coding error. Although originally formulated in terms of leaky integrator rate-coded neurons, LCA based on lateral inhibition between leaky integrate-and-fire (LIF) neurons has been implemented on spiking neuromorphic processors but such implementations preclude local online learning. We previously reported that spiking LCA can be expressed in terms of predictive coding error in a manner that allows for unsupervised dictionary learning via a local Hebbian rule but the issue of stability has not previously been addressed. Here, we use the Nengo simulator to show that unsupervised dictionary learning in a spiking LCA model can be made stable by incorporating epochs of sinusoidally-modulated noise that we hypothesize are analogous to slow-wave sleep. In the absence of slow-wave sleep epochs, the |L|_2 norm of individual features tends to increase over time during unsupervised dictionary learning until the corresponding neurons can be activated by random Gaussian noise. By inserting epochs of sinusoidally-modulated Gaussian noise, however, the |L|_2 norms of any activated neurons are down regulated such that individual neurons are no longer activated by noise. Our results suggest that slow-wave sleep may act, in part, to ensure that cortical neurons do not \"hallucinate\" their target features in pure noise, thus helping to maintain dynamical stability.\r"
  },
  "cvpr2020_w22_rit-18anoveldatasetforcompositionalgroupactivityunderstanding": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "RIT-18: A Novel Dataset for Compositional Group Activity Understanding",
    "authors": [
      "Junwen Chen",
      "Haiting Hao",
      "Hanbin Hong",
      "Yu Kong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Chen_RIT-18_A_Novel_Dataset_for_Compositional_Group_Activity_Understanding_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Chen_RIT-18_A_Novel_Dataset_for_Compositional_Group_Activity_Understanding_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Group activity understanding is a challenging task as multiple people are involved, and their relations may vary over time. Currently, the literature of group activity is limited to group activity recognition, because videos are trimmed in very short duration and focus on a single activity. This slows down the progress in the group activity domain. In this paper, we propose a new large-scale untrimmed compositional group activity dataset RIT-18 based on the volleyball games captured from YouTube. Each clip in our dataset depicts an entire rally which spans the duration from serve to a point being scored. Comprehensive annotations including group activity labels, temporal boundaries of activities, key persons, and winning teams are provided. We describe group activity recognition, future activity anticipation, and rally-level winner prediction challenges, and evaluate several baseline methods over these challenges. We report their performance on our dataset and demonstrate further efforts need to be made. The dataset is available at https://pht180.rit.edu/actionlab/rit-18.\r"
  },
  "cvpr2020_w22_squeezeu-netamemoryandenergyefficientimagesegmentationnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "Squeeze U-Net: A Memory and Energy Efficient Image Segmentation Network",
    "authors": [
      "Nazanin Beheshti",
      "Lennart Johnsson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Beheshti_Squeeze_U-Net_A_Memory_and_Energy_Efficient_Image_Segmentation_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Beheshti_Squeeze_U-Net_A_Memory_and_Energy_Efficient_Image_Segmentation_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " To facilitate implementation of deep neural networks on embedded systems keeping memory and computation requirements low is critical, particularly for real-time mobile use. In this work, we propose a SqueezeNet inspired version of U-Net for image segmentation that achieves a 12X reduction in model size to 32MB, and 3.2X reduction in Multiplication Accumulation operations (MACs) from 287 billion ops to 88 billion ops for inference on the CamVid data set. Our proposed Squeeze U-Net is efficient in both low MACs and memory use. Our performance results using Tensorflow 1.14 with Python 3.6 and CUDA 10.1.243 on an NVIDIA K40 GPU shows that Squeeze U-Net is 17% faster for inference and 52% faster for training than U-Net.\r"
  },
  "cvpr2020_w22_learningfurniturecompatibilitywithgraphneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w22",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Women in Computer Vision",
    "title": "Learning Furniture Compatibility With Graph Neural Networks",
    "authors": [
      "Luisa F. Polania",
      "Mauricio Flores",
      "Matthew Nokleby",
      "Yiran Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w22/Polania_Learning_Furniture_Compatibility_With_Graph_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w22/Polania_Learning_Furniture_Compatibility_With_Graph_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose a graph neural network (GNN) approach to the problem of predicting the stylistic compatibility of a set of furniture items from images. While most existing results are based on siamese networks which evaluate pairwise compatibility between items, the proposed GNN architecture exploits relational information among groups of items. We present two GNN models, both of which comprise a deep CNN that extracts a feature representation for each image, a gated recurrent unit (GRU) network that models interactions between the furniture items in a set, and an aggregation function that calculates the compatibility score. In the first model, a generalized contrastive loss function that promotes the generation of clustered embeddings for items belonging to the same furniture set is introduced. Also, in the first model, the edge function between nodes in the GRU and the aggregation function are fixed in order to limit model complexity and allow training on smaller datasets; in the second model, the edge function and aggregation function are learned directly from the data. We demonstrate state-of-the art accuracy for compatibility prediction and \"fill in the blank\" tasks on the Bonn and Singapore furniture datasets. We further introduce a new dataset, called the Target Furniture Collections dataset, which contains over 6000 furniture items that have been hand-curated by stylists to make up 1632 compatible sets. We also demonstrate superior prediction accuracy on this dataset.\r"
  },
  "cvpr2020_w23_inferringtemporalcompositionsofactionsusingprobabilisticautomata": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w23",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Compositionality in Computer Vision",
    "title": "Inferring Temporal Compositions of Actions Using Probabilistic Automata",
    "authors": [
      "Rodrigo Santa Cruz",
      "Anoop Cherian",
      "Basura Fernando",
      "Dylan Campbell",
      "Stephen Gould"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w23/Santa_Cruz_Inferring_Temporal_Compositions_of_Actions_Using_Probabilistic_Automata_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w23/Santa_Cruz_Inferring_Temporal_Compositions_of_Actions_Using_Probabilistic_Automata_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents a framework to recognize temporal compositions of atomic actions in videos. Specifically, we propose to express temporal compositions of actions as semantic regular expressions and derive an inference framework using probabilistic automata to recognize complex actions as satisfying these expressions on the input video features. Our approach is different from existing works that either predict long-range complex activities as unordered sets of atomic actions, or retrieve videos using natural language sentences. Instead, the proposed approach allows recognizing complex fine-grained activities using only pretrained action classifiers, without requiring any additional data, annotations or neural network training. To evaluate the potential of our approach, we provide experiments on synthetic datasets and challenging real action recognition datasets, such as MultiTHUMOS and Charades. We conclude that the proposed approach can extend state-of-the-art primitive action classifiers to vastly more complex activities without large performance degradation.\r"
  },
  "cvpr2020_w23_understandingactionrecognitioninstillimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w23",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Compositionality in Computer Vision",
    "title": "Understanding Action Recognition in Still Images",
    "authors": [
      "Deeptha Girish",
      "Vineeta Singh",
      "Anca Ralescu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w23/Girish_Understanding_Action_Recognition_in_Still_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w23/Girish_Understanding_Action_Recognition_in_Still_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Action recognition in still images is closely related to various other computer vision tasks like pose estimation, object recognition, image retrieval, video action recognition and frame tagging in videos. This problem is focused on recognizing a person's action or behavior using a single frame. Unlike action recognition in videos, which is a relatively very well established area of research, spatio-temporal features are not available to characterize actions in still images which makes a more challenging problem. In this work only actions that involve objects are considered. The complex action is broken down into components based on semantics. The importance of each of these components in action recognition is systematically studied.\r"
  },
  "cvpr2020_w23_decomposingimagegenerationintolayoutpredictionandconditionalsynthesis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w23",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Compositionality in Computer Vision",
    "title": "Decomposing Image Generation Into Layout Prediction and Conditional Synthesis",
    "authors": [
      "Anna Volokitin",
      "Ender Konukoglu",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w23/Volokitin_Decomposing_Image_Generation_Into_Layout_Prediction_and_Conditional_Synthesis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w23/Volokitin_Decomposing_Image_Generation_Into_Layout_Prediction_and_Conditional_Synthesis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Learning the distribution of multi-object scenes with Generative Adversarial Networks (GAN) is challenging. Guiding the learning using semantic intermediate representations, which are less complex than images, can be a solution. In this article, we investigate splitting the optimisation of generative adversarial networks into two parts, by first generating a semantic segmentation mask from noise and then translating that segmentation mask into an image. We performed experiments using images from the CityScapes dataset and compared our approach to Progressive Growing of GANs (PGGAN), which uses multiscale growing of networks to guide the learning. Using the lens of a segmentation algorithm to examine the structure of generated images, we find that our method achieves higher structural consistency in latent space interpolations and yields generations with better differentiation between distinct objects, while achieving the same image quality as PGGAN as judged by a user study and a standard GAN evaluation metric.\r"
  },
  "cvpr2020_w47_noiseisinsideme!generatingadversarialperturbationswithnoisederivedfromnaturalfilters": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Noise Is Inside Me! Generating Adversarial Perturbations With Noise Derived From Natural Filters",
    "authors": [
      "Akshay Agarwal",
      "Mayank Vatsa",
      "Richa Singh",
      "Nalini K. Ratha"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Agarwal_Noise_Is_Inside_Me_Generating_Adversarial_Perturbations_With_Noise_Derived_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Agarwal_Noise_Is_Inside_Me_Generating_Adversarial_Perturbations_With_Noise_Derived_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning solutions are vulnerable to adversarial perturbations and can lead a \"frog\" image to be misclassified as a \"deer\" or random pattern into \"guitar\". Adversarial attack generation algorithms generally utilize the knowledge of database and CNN model to craft the noise. In this research, we present a novel scheme termed as Camera Inspired Perturbations to generate adversarial noise. The proposed approach relies on the noise embedded in the image due to environmental factors or camera noise incorporated. We extract these noise patterns using image filtering algorithms and incorporate them into images to generate adversarial images. Unlike most of the existing algorithms that require learning of noise, the proposed adversarial noise can be applied in real-time. It is model-agnostic and can be utilized to fool multiple deep learning classifiers on various databases. The effectiveness of the proposed approach is evaluated on five different databases with five different convolutional neural networks such as ResNet-50, VGG-16, and VGG-Face. The proposed attack reduces the classification accuracy of every network, for instance, the performance of VGG-16 on the Tiny ImageNet database is reduced by more than 33%. The robustness of the proposed adversarial noise is also evaluated against different adversarial defense algorithms.\r"
  },
  "cvpr2020_w47_learningorderedtop-kadversarialattacksviaadversarialdistillation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Learning Ordered Top-k Adversarial Attacks via Adversarial Distillation",
    "authors": [
      "Zekun Zhang",
      "Tianfu Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Zhang_Learning_Ordered_Top-k_Adversarial_Attacks_via_Adversarial_Distillation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Zhang_Learning_Ordered_Top-k_Adversarial_Attacks_via_Adversarial_Distillation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially white-box targeted attacks. This paper studies the problem of how aggressive white-box targeted attacks can be to go beyond widely used Top-1 attacks. We propose to learn ordered Top-k attacks (k >=1), which enforce the Top-k predicted labels of an adversarial example to be the k (randomly) selected and ordered labels (the ground-truth label is exclusive). Two methods are presented. First, we extend the vanilla Carlini-Wagner (C&W) method and use it as a strong baseline. Second, we present an Adversarial Distillation (AD) framework consisting of two components: (i) Computing an adversarial probability distribution for a given ordered Top-k targeted labels. (ii) Learning adversarial examples by minimizing the Kullback-Leibler (KL) divergence between the adversarial distribution and the predicted distribution, together with the perturbation energy penalty. In computing adversarial distributions, we explore how to leverage label semantic similarities, leading to knowledge-oriented attacks. In experiments, we test Top-k (k=1,2,5,10) attacks in the ImageNet-1000 val. dataset using three representative DNNs trained with the clean ImageNet-1000 train dataset, ResNet-50, DenseNet-121 and AOGNet-12M. Overall, the proposed AD approach obtains the best results, especially by a large margin when the computation budget is limited. It reduces the perturbation energy consistently with the same attack success rate on all the four k's, and improves the attack success rate by large margin against the modified C&W method for k=10.\r"
  },
  "cvpr2020_w47_adversarialfoolingbeyondflippingthelabel": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Adversarial Fooling Beyond \"Flipping the Label\"",
    "authors": [
      "Konda Reddy Mopuri",
      "Vaisakh Shaj",
      "R. Venkatesh Babu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent advancements in CNNs have shown remarkable achievements in various CV/AI applications. Though CNNs show near human or better than human performance in many critical tasks, they are quite vulnerable to adversarial attacks. These attacks are potentially dangerous in real-life deployments. Though there have been many adversarial attacks proposed in recent years, there is no proper way of quantifying the effectiveness of these attacks. As of today, mere fooling rate is used for measuring the susceptibility of the models, or the effectiveness of adversarial attacks. Fooling rate just considers label flipping and does not consider the cost of such flipping, for instance, in some deployments, flipping between two species of dogs may not be as severe as confusing a dog category with that of a vehicle. Therefore, the metric to quantify the vulnerability of the models should capture the severity of the flipping as well. In this work we first bring out the drawbacks of the existing evaluation and propose novel metrics to capture various aspects of the fooling. Further, for the first time, we present a comprehensive analysis of several important adversarial attacks over a set of distinct CNN architectures. We believe that the presented analysis brings valuable insights about the current adversarial attacks and the CNN models.\r"
  },
  "cvpr2020_w47_improvingtheaffordabilityofrobustnesstrainingfordnns": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Improving the Affordability of Robustness Training for DNNs",
    "authors": [
      "Sidharth Gupta",
      "Parijat Dube",
      "Ashish Verma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Gupta_Improving_the_Affordability_of_Robustness_Training_for_DNNs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Gupta_Improving_the_Affordability_of_Robustness_Training_for_DNNs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Projected Gradient Descent (PGD) based adversarial training has become one of the most prominent methods for building robust deep neural network models. However, the computational complexity associated with this approach, due to the maximization of the loss function when finding adversaries, is a longstanding problem and may be prohibitive when using larger and more complex models. In this paper we show that the initial phase of adversarial training is redundant and can be replaced with natural training which significantly improves the computational efficiency. We demonstrate that this efficiency gain can be achieved without any loss in accuracy on natural and adversarial test samples. We support our argument with insights on the nature of the adversaries and their relative strength during the training process. We show that our proposed method can reduce the training time by a factor of up to 2.5 with comparable or better model test accuracy and generalization on various strengths of adversarial attacks.\r"
  },
  "cvpr2020_w47_acyclically-trainedadversarialnetworkforinvariantrepresentationlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "A Cyclically-Trained Adversarial Network for Invariant Representation Learning",
    "authors": [
      "Jiawei Chen",
      "Janusz Konrad",
      "Prakash Ishwar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Chen_A_Cyclically-Trained_Adversarial_Network_for_Invariant_Representation_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Chen_A_Cyclically-Trained_Adversarial_Network_for_Invariant_Representation_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent studies show that deep neural networks are vulnerable to adversarial examples which can be generated via certain types of transformations. Being robust to a desired family of adversarial attacks is then equivalent to being invariant to a family of transformations. Learning invariant representations then naturally emerges as an important goal to achieve which we explore in this paper within specific application contexts. Specifically, we propose a cyclically-trained adversarial network to learn a mapping from image space to latent representation space and back such that the latent representation is invariant to a specified factor of variation (e.g., identity). The learned mapping assures that the synthesized image is not only realistic, but has the same values for unspecified factors (e.g., pose and illumination) as the original image and a desired value of the specified factor. Unlike disentangled representation learning, which requires two latent spaces, one for specified and another for unspecified factors, invariant representation learning needs only one such space. We encourage invariance to a specified factor by applying adversarial training using a variational autoencoder in the image space as opposed to the latent space. We strengthen this invariance by introducing a cyclic training process (forward and backward cycle). We also propose a new method to evaluate conditional generative networks. It compares how well different factors of variation can be predicted from the synthesized, as opposed to real, images. In quantitative terms, our approach attains state-of-the-art performance in experiments spanning three datasets with factors such as identity, pose, illumination or style. Our method produces sharp, high-quality synthetic images with little visible artefacts compared to previous approaches.\r"
  },
  "cvpr2020_w47_roleofspatialcontextinadversarialrobustnessforobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Role of Spatial Context in Adversarial Robustness for Object Detection",
    "authors": [
      "Aniruddha Saha",
      "Akshayvarun Subramanya",
      "Koninika Patil",
      "Hamed Pirsiavash"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Saha_Role_of_Spatial_Context_in_Adversarial_Robustness_for_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Saha_Role_of_Spatial_Context_in_Adversarial_Robustness_for_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The benefits of utilizing spatial context in fast object detection algorithms have been studied extensively. Detectors increase inference speed by doing a single forward pass per image which means they implicitly use contextual reasoning for their predictions. However, one can show that an adversary can design adversarial patches which do not overlap with any objects of interest in the scene and exploit contextual reasoning to fool standard detectors. In this paper, we examine this problem and design category specific adversarial patches which make a widely used object detector like YOLO blind to an attacker chosen object category. We also show that limiting the use of spatial context during object detector training improves robustness to such adversaries. We believe the existence of context based adversarial attacks is concerning since the adversarial patch can affect predictions without being in vicinity of any objects of interest. Hence, defending against such attacks becomes challenging and we urge the research community to give attention to this vulnerability.\r"
  },
  "cvpr2020_w47_extensionsandlimitationsofrandomizedsmoothingforrobustnessguarantees": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Extensions and Limitations of Randomized Smoothing for Robustness Guarantees",
    "authors": [
      "Jamie Hayes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Hayes_Extensions_and_Limitations_of_Randomized_Smoothing_for_Robustness_Guarantees_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Hayes_Extensions_and_Limitations_of_Randomized_Smoothing_for_Robustness_Guarantees_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Randomized smoothing, a method to certify a classifier's decision on an input is invariant under adversarial noise, offers attractive advantages over other certification methods. It operates in a black-box and so certification is not constrained by the size of the classifier's architecture. Here, we extend the work of Li et al. (2019), studying how the choice of divergence between smoothing measures affects the final robustness guarantee, and how the choice of smoothing measure itself can lead to guarantees in differing threat models. To this end, we develop a method to certify robustness against any Lp norm minimized adversarial perturbation. We then demonstrate a negative result, that randomized smoothing suffers from the curse of dimensionality; as p increases, the effective radius around an input one can certify vanishes.\r"
  },
  "cvpr2020_w47_systematicevaluationofbackdoordatapoisoningattacksonimageclassifiers": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers",
    "authors": [
      "Loc Truong",
      "Chace Jones",
      "Brian Hutchinson",
      "Andrew August",
      "Brenda Praggastis",
      "Robert Jasper",
      "Nicole Nichols",
      "Aaron Tuor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Backdoor data poisoning attacks have recently been demonstrated in computer vision research as a potential safety risk for machine learning (ML) systems. Traditional data poisoning attacks manipulate training data to induce unreliability of an ML model, whereas backdoor data poisoning attacks maintain system performance unless the ML model is presented with an input containing an embedded \"trigger\" that provides a predetermined response advantageous to the adversary. Our work builds upon prior backdoor data-poisoning research for ML image classifiers and systematically assesses different experimental conditions including types of trigger patterns, persistence of trigger patterns during retraining, poisoning strategies, architectures (ResNet-50, NasNet, NasNet-Mobile), datasets (Flowers, CIFAR-10), and potential defensive regularization techniques (Contrastive Loss, Logit Squeezing, Manifold Mixup, Soft-Nearest-Neighbors Loss). Experiments yield four key findings. First, the success rate of backdoor poisoning attacks varies widely, depending on several factors, including model architecture, trigger pattern and regularization technique. Second, we find that poisoned models are hard to detect through performance inspection alone. Third, regularization typically reduces backdoor success rate, although it can have no effect or even slightly increase it, depending on the form of regularization. Finally, backdoors inserted through data poisoning can be rendered ineffective after just a few epochs of additional training on a small set of clean data without affecting the model's performance.\r"
  },
  "cvpr2020_w47_probingforartifactsdetectingimagenetmodelevasions": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Probing for Artifacts: Detecting Imagenet Model Evasions",
    "authors": [
      "Jeremiah Rounds",
      "Addie Kingsland",
      "Michael J. Henry",
      "Kayla R. Duskin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Rounds_Probing_for_Artifacts_Detecting_Imagenet_Model_Evasions_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Rounds_Probing_for_Artifacts_Detecting_Imagenet_Model_Evasions_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While deep learning models have made incredible progress across a variety of machine learning tasks, they remain vulnerable to adversarial examples crafted to fool otherwise trustworthy models. Previous work has proposed examining the internal activation of Imagenet models to detect adversarial examples. Our work expands the scale and scope of previous research by simultaneously probing every activation within an Imagenet model using a novel probe block. This probe block model is trained against multiple adversarial algorithms to create a more robust detector. Parameterization of the probe block and adversarial classification networks that utilize probe block output are examined in an ablation experiment with probes of Resnet-50, Inception-v3 and Xception. Considered adversarial classification networks include examples built with Mobilenet-v2 which is shown to be better than a VGG alternative for detecting adversarial artifacts. Results are compared to logistic regression feature squeezing results, which we suggest is an improvement to feature squeezing.\r"
  },
  "cvpr2020_w47_robustassessmentofreal-worldadversarialexamples": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Robust Assessment of Real-World Adversarial Examples",
    "authors": [
      "Brett Jefferson",
      "Carlos Ortiz Marrero"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Jefferson_Robust_Assessment_of_Real-World_Adversarial_Examples_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Jefferson_Robust_Assessment_of_Real-World_Adversarial_Examples_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We explore rigorous, systematic, and controlled experimental evaluation of adversarial examples in the real world and propose a testing regimen for evaluation of real-world adversarial objects. We show that for small scene/ environmental perturbations, large adversarial performance differences exist. Current state of adversarial reporting exists largely as a frequency count over a dynamic collections of scenes. Our work underscores the need for either a more complete report or a score that incorporates scene changes and baseline performance for models and environments tested by adversarial developers. We put forth a score that attempts to address the above issues in a straight-forward exemplar application for multiple generated adversary examples. We contribute the following: 1. a testbed for adversarial assessment, 2. a score for adversarial examples, and 3. a collection of additional evaluations on testbed data.\r"
  },
  "cvpr2020_w47_vulnerabilityofpersonre-identificationmodelstometricadversarialattacks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Vulnerability of Person Re-Identification Models to Metric Adversarial Attacks",
    "authors": [
      "Quentin Bouniot",
      "Romaric Audigier",
      "Angelique Loesch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Bouniot_Vulnerability_of_Person_Re-Identification_Models_to_Metric_Adversarial_Attacks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Bouniot_Vulnerability_of_Person_Re-Identification_Models_to_Metric_Adversarial_Attacks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Person re-identification (re-ID) is a key problem in smart supervision of camera networks. Over the past years, models using deep learning have become state of the art. However, it has been shown that deep neural networks are flawed with adversarial examples, i.e. human-imperceptible perturbations. Extensively studied for the task of image closed-set classification, this problem can also appear in the case of open-set retrieval tasks. Indeed, recent work has shown that we can also generate adversarial examples for metric learning systems such as re-ID ones. These models remain vulnerable: when faced with adversarial examples, they fail to correctly recognize a person, which represents a security breach. These attacks are all the more dangerous as they are impossible to detect for a human operator. Attacking a metric consists in altering the distances between the feature of an attacked image and those of reference images, i.e. guides. In this article, we investigate different possible attacks depending on the number and type of guides available. From this metric attack family, two particularly effective attacks stand out. The first one, called Self Metric Attack, is a strong attack that does not need any image apart from the attacked image. The second one, called Furthest-Negative Attack, makes full use of a set of images. Attacks are evaluated on commonly used datasets: Market1501 and DukeMTMC. Finally, we propose an efficient extension of adversarial training protocol adapted to metric learning as a defense that increases the robustness of re-ID models.\r"
  },
  "cvpr2020_w47_livetrojanattacksondeepneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w47",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Adversarial Machine Learning in Computer Vision",
    "title": "Live Trojan Attacks on Deep Neural Networks",
    "authors": [
      "Robby Costales",
      "Chengzhi Mao",
      "Raphael Norwitz",
      "Bryan Kim",
      "Junfeng Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w47/Costales_Live_Trojan_Attacks_on_Deep_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w47/Costales_Live_Trojan_Attacks_on_Deep_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Like all software systems, the execution of deep learning models is dictated in part by logic represented as data in memory. For decades, attackers have exploited traditional software programs by manipulating this data. We propose a live attack on deep learning systems that patches model parameters in memory to achieve predefined malicious behavior on a certain set of inputs. By minimizing the size and number of these patches, the attacker can reduce the amount of network communication and memory overwrites, with minimal risk of system malfunctions or other detectable side effects. We demonstrate the feasibility of this attack by computing efficient patches on multiple deep learning models. We show that the desired trojan behavior can be induced with a few small patches and with limited access to training data. We describe the details of how this attack is carried out on real systems and provide sample code for patching TensorFlow model parameters in Windows and in Linux. Lastly, we present a technique for effectively manipulating entropy on perturbed inputs to bypass STRIP, a state-of-the-art run-time trojan detection technique.\r"
  },
  "cvpr2020_w45_homogeneouslinearinequalityconstraintsforneuralnetworkactivations": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Homogeneous Linear Inequality Constraints for Neural Network Activations",
    "authors": [
      "Thomas Frerix",
      "Matthias Niessner",
      "Daniel Cremers"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Frerix_Homogeneous_Linear_Inequality_Constraints_for_Neural_Network_Activations_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Frerix_Homogeneous_Linear_Inequality_Constraints_for_Neural_Network_Activations_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose a method to impose homogeneous linear inequality constraints of the form Ax <= 0 on neural network activations. The proposed method allows a data-driven training approach to be combined with modeling prior knowledge about the task. One way to achieve this task is by means of a projection step at test time after unconstrained training. However, this is an expensive operation. By directly incorporating the constraints into the architecture, we can significantly speed-up inference at test time; for instance, our experiments show a speed-up of up to two orders of magnitude over a projection method. Our algorithm computes a suitable parameterization of the feasible set at initialization and uses standard variants of stochastic gradient descent to find solutions to the constrained network. Thus, the modeling constraints are always satisfied during training. Crucially, our approach avoids to solve an optimization problem at each training step or to manually trade-off data and constraint fidelity with additional hyperparameters. We consider constrained generative modeling as an important application domain and experimentally demonstrate the proposed method by constraining a variational autoencoder.\r"
  },
  "cvpr2020_w45_suw-learnjointsupervised,unsupervised,weaklysuperviseddeeplearningformonoculardepthestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "SUW-Learn: Joint Supervised, Unsupervised, Weakly Supervised Deep Learning for Monocular Depth Estimation",
    "authors": [
      "Haoyu Ren",
      "Aman Raj",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Ren_SUW-Learn_Joint_Supervised_Unsupervised_Weakly_Supervised_Deep_Learning_for_Monocular_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Ren_SUW-Learn_Joint_Supervised_Unsupervised_Weakly_Supervised_Deep_Learning_for_Monocular_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We introduce SUW-Learn: A framework for deep-learning with joint supervised learning (S), unsupervised learning (U), and weakly-supervised learning (W). We deploy SUW-Learn for deep learning of the monocular depth from images and video sequences. The supervised learning module optimizes a depth estimation network by knowledge of the ground-truth depth. In contrast, the unsupervised learning module has no knowledge of the ground-truth depth, but optimizes the depth estimation network by predicting the current frame from the estimated 3D geometry. The weakly supervised module optimizes the depth estimation by evaluating the consistency between the estimated depth and weak labels derived from other information, such as the semantic information. SUW-Learn trains the deep-learning networks end-to-end with joint optimization of the desired SUW objectives. To improve the performance of monocular depth networks on scenes with people subjects, we construct the M&M dataset, by combining two recent datasets with different domain knowledge and from different sources, the Megadepth dataset with images of people around landmarks, and the Mannequin Challenge dataset with video sequences of frozen people. We demonstrate the benefits of joint SUW learning in improving the generalization capability on the M&M dataset. We benchmark SUW-Learn on the proposed M&M dataset and the KITTI driving-scene dataset, and achieve the state-of-the-art performance.\r"
  },
  "cvpr2020_w45_top-downnetworksacoarse-to-finereimaginationofcnns": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Top-Down Networks: A Coarse-to-Fine Reimagination of CNNs",
    "authors": [
      "Ioannis Lelekas",
      "Nergis Tomen",
      "Silvia L. Pintea",
      "Jan C. van Gemert"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Lelekas_Top-Down_Networks_A_Coarse-to-Fine_Reimagination_of_CNNs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Lelekas_Top-Down_Networks_A_Coarse-to-Fine_Reimagination_of_CNNs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Biological vision adopts a coarse-to-fine information processing pathway, from initial visual detection and binding of salient features of a visual scene, to the enhanced and preferential processing given relevant stimuli. On the contrary, CNNs employ a fine-to-coarse processing, moving from local, edge-detecting filters to more global ones extracting abstract representations of the input. In this paper we reverse the feature extraction part of standard bottom-up architectures and turn them upside-down: We propose top-down networks. Our proposed coarse-to-fine pathway, by blurring higher frequency information and restoring it only at later stages, offers a line of defence against adversarial attacks that introduce high frequency noise. Moreover, since we increase image resolution with depth, the high resolution of the feature map in the final convolutional layer contributes to the explainability of the network's decision making process. This favors object-driven decisions over context driven ones, and thus provides better localized class activation maps. This paper offers empirical evidence for the applicability of the top-down resolution processing to various existing architectures on multiple visual tasks.\r"
  },
  "cvpr2020_w45_muteinter-classambiguitydrivenmulti-hottargetencodingfordeepneuralnetworkdesign": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "MUTE: Inter-Class Ambiguity Driven Multi-Hot Target Encoding for Deep Neural Network Design",
    "authors": [
      "Mayoore S. Jaiswal",
      "Bumsoo Kang",
      "Jinho Lee",
      "Minsik Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Jaiswal_MUTE_Inter-Class_Ambiguity_Driven_Multi-Hot_Target_Encoding_for_Deep_Neural_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Jaiswal_MUTE_Inter-Class_Ambiguity_Driven_Multi-Hot_Target_Encoding_for_Deep_Neural_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Target encoding is an effective technique to boost performance of classical and deep neural networks based classification models. However, the existing target encoding approaches require significant increase in the learning capacity, thus demand higher computation power and more training data. In this paper, we present a novel and efficient target encoding method, Inter-class Ambiguity Driven Multi-hot Target Encoding (MUTE), to improve both generalizability and robustness of a classification model by understanding the inter-class characteristics of a target dataset. By evaluating ambiguity between the target classes in a dataset, MUTE strategically optimizes the Hamming distances among target encoding. Such optimized target encoding offers higher classification strength for neural network models with negligible computation overhead and without increasing the model size. When MUTE is applied to the popular image classification networks and datasets, our experimental results show that MUTE offers better generalization and defense against the noises and adversarial attacks over the existing solutions.\r"
  },
  "cvpr2020_w45_smoothmixasimpleyeteffectivedataaugmentationtotrainrobustclassifiers": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "SmoothMix: A Simple Yet Effective Data Augmentation to Train Robust Classifiers",
    "authors": [
      "Jin-Ha Lee",
      "Muhammad Zaigham Zaheer",
      "Marcella Astrid",
      "Seung-Ik Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Data augmentation has been proven effective which, by preventing overfitting, can not only enhances the performance of a deep neural network but also leads to a better generalization even with limited dataset. Recently introduced regional dropout based data augmentation strategies remove (or replace) some parts of an input image with a desideratum to make the network focus on less discriminative portions of an image, which results in an improved performance. However, such approaches usually possess 'strong-edge' problem caused by an obvious change in the pixels at the positions where the image is manipulated. It may not only impact on the local convolution operation but can also provide clues for the network to latch on to, which do not align well with the basic purpose of augmentation. In order to minimize such peculiarities, we introduce SmoothMix in which blending of images is done based on soft edges and the training labels are computed accordingly. Extensive analysis performed on CIFAR-10, CIFAR-100 and ImageNet for image classification demonstrate state-of-the-art results. Furthermore, SmoothMix significantly increases robustness of a network against image corruption. Results on CIFAR-100-C & ImageNet-C corruption datasets also shows superiority of our proposed approach.\r"
  },
  "cvpr2020_w45_s2ldsemi-supervisedlandmarkdetectioninlow-resolutionimagesandimpactonfaceverification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "S2LD: Semi-Supervised Landmark Detection in Low-Resolution Images and Impact on Face Verification",
    "authors": [
      "Amit Kumar",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Kumar_S2LD_Semi-Supervised_Landmark_Detection_in_Low-Resolution_Images_and_Impact_on_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Kumar_S2LD_Semi-Supervised_Landmark_Detection_in_Low-Resolution_Images_and_Impact_on_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Landmark detection algorithms trained on high resolution images perform poorly on datasets containing low resolution images. This degrades the performance of facial verification, recognition and modeling that rely on accurate detection of landmarks. To the best of our knowledge, there is no dataset consisting of low resolution face images along with their annotated landmarks, making supervised training infeasible. In this paper, we present a semi-supervised approach to predict landmarks on low resolution images by learning them from labeled high resolution images. The objective of this work is to show that predicting landmarks directly on low resolution images is more effective than the current practice of aligning images after rescaling or superresolution. In a two-step process, the proposed approach first learns to generate low resolution images by modeling the distribution of target low resolution images. In the second stage, the model learns to predict landmarks for target low resolution images from generated low resolution images. With extensive experimentation, we study the impact of the various design choices and also show that prediction of landmarks directly in low resolution, improves performance on the critical task of face verification in low resolution images. As a byproduct, the proposed method also achieves state of the art land mark detection results for high resolution images.\r"
  },
  "cvpr2020_w45_p2lpredictingtransferlearningforimagesandsemanticrelations": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "P2L: Predicting Transfer Learning for Images and Semantic Relations",
    "authors": [
      "Bishwaranjan Bhattacharjee",
      "John R. Kender",
      "Matthew Hill",
      "Parijat Dube",
      "Siyu Huo",
      "Michael R. Glass",
      "Brian Belgodere",
      "Sharath Pankanti",
      "Noel Codella",
      "Patrick Watson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Bhattacharjee_P2L_Predicting_Transfer_Learning_for_Images_and_Semantic_Relations_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Bhattacharjee_P2L_Predicting_Transfer_Learning_for_Images_and_Semantic_Relations_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We describe an efficient method to accurately estimate the effectiveness of a previously trained deep learning model for use in a new learning task. We use this method, \"Predict To Learn\" (P2L), to predict the most likely \"source\" dataset to produce effective transfer for training on a \"target\" dataset. We validate our approach extensively across 21 tasks, including image classification tasks and semantic relationship prediction tasks in the linguistic domain. The P2L approach selects the best transfer learning model on 62% of the tasks,compared with a baseline of 48% of cases when using a heuristic of selecting the largest source dataset and 52% of cases when using a distance measure between source and target datasets. Further, our work results in an 8% reduction in error rate. Finally, we also show that a model trained from merging multiple source model datasets does not necessarily result in improved transfer learning. This suggests that performance of the target model depends upon the relative composition of the source dataset as well as their absolute scale, as measured by our novel method we term 'P2L'\r"
  },
  "cvpr2020_w45_semi-supervisedlearningwithscarceannotations": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Semi-Supervised Learning With Scarce Annotations",
    "authors": [
      "Sylvestre-Alvise Rebuffi",
      "Sebastien Ehrhardt",
      "Kai Han",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Rebuffi_Semi-Supervised_Learning_With_Scarce_Annotations_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Rebuffi_Semi-Supervised_Learning_With_Scarce_Annotations_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While semi-supervised learning (SSL) algorithms provide an efficient way to make use of both labelled and unlabelled data, they generally struggle when the number of annotated samples is very small. In this work, we consider the problem of SSL multi-class classification with very few labelled instances. We introduce two key ideas. The first is a simple but effective one: we leverage the power of transfer learning among different tasks and self-supervision to initialize a good representation of the data without making use of any label. The second idea is a new algorithm for SSL that can exploit well such a pre-trained representation. The algorithm works by alternating two phases, one fitting the labelled points and one fitting the unlabelled ones, with carefully-controlled information flow between them. The benefits are greatly reducing overfitting of the labelled data and avoiding issue with balancing labelled and unlabelled losses during training. We show empirically that this method can successfully train competitive models with as few as 10 labelled data points per class. More in general, we show that the idea of bootstrapping features using self-supervised learning always improves SSL on standard benchmarks. We show that our algorithm works increasingly well compared to other methods when refining from other tasks or datasets.\r"
  },
  "cvpr2020_w45_spatio-temporalactiondetectionandlocalizationusingahierarchicallstm": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Spatio-Temporal Action Detection and Localization Using a Hierarchical LSTM",
    "authors": [
      "Akshaya Ramaswamy",
      "Karthik Seemakurthy",
      "Jayavardhana Gubbi",
      "Balamuralidhar Purushothaman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Ramaswamy_Spatio-Temporal_Action_Detection_and_Localization_Using_a_Hierarchical_LSTM_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Ramaswamy_Spatio-Temporal_Action_Detection_and_Localization_Using_a_Hierarchical_LSTM_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Video analysis is gaining importance in the recent past due to its usefulness in a wide variety of applications. The efficiency of a video analytics engine primarily depends on its ability to extract the spatio-temporal features, which has enough discriminative. Inspired by the way the human visual system operates, we propose a hierarchical architecture to capture the spatio-temporal information from a given input video at different time scales. The proposed architecture has a 3D Inception module followed by two layers of modified Convolutional Long Short Term Memory (ConvLSTM) as the fundamental unit. At each level, we consolidate the LSTM cell and hidden states to the next level by using an visual attention-based pooling approach. The proposed network is used for video action detection and localization application that is the foundational element for video analysis. UCF101 and AVA datasets are used to show that the recognition accuracy achieved by the proposed algorithm advances the state-of-the-art in spatio-temporal action detection and localization application.\r"
  },
  "cvpr2020_w45_canwelearnheuristicsforgraphicalmodelinferenceusingreinforcementlearning?": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Can We Learn Heuristics for Graphical Model Inference Using Reinforcement Learning?",
    "authors": [
      "Safa Messaoud",
      "Maghav Kumar",
      "Alexander G. Schwing"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Messaoud_Can_We_Learn_Heuristics_for_Graphical_Model_Inference_Using_Reinforcement_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Messaoud_Can_We_Learn_Heuristics_for_Graphical_Model_Inference_Using_Reinforcement_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Combinatorial optimization is frequently used in computer vision. For instance, in applications like semantic segmentation, human pose estimation and action recognition, programs are formulated for solving inference in Conditional Random Fields (CRFs) to produce a structured output that is consistent with visual features of the image. However, solving inference in CRFs is in general intractable, and approximation methods are computationally demanding and limited to unary, pairwise and hand-crafted forms of higher order potentials. In this paper, we show that we can learn program heuristics, i.e., policies, for solving inference in higher order CRFs for the task of semantic segmentation, using reinforcement learning. Our method solves inference tasks efficiently without imposing any constraints on the form of the potentials. We show compelling results on the Pascal VOC and MOTS datasets.\r"
  },
  "cvpr2020_w45_distillingknowledgefromrefinementinmultipleinstancedetectionnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Distilling Knowledge From Refinement in Multiple Instance Detection Networks",
    "authors": [
      "Luis Felipe Zeni",
      "Claudio R. Jung"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Zeni_Distilling_Knowledge_From_Refinement_in_Multiple_Instance_Detection_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Zeni_Distilling_Knowledge_From_Refinement_in_Multiple_Instance_Detection_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Weakly supervised object detection (WSOD) aims to tackle the object detection problem using only labeled image categories as supervision. A common approach used in WSOD to deal with the lack of localization information is Multiple Instance Learning, and in recent years methods started adopting Multiple Instance Detection Networks (MIDN), which allows training in an end-to-end fashion. In general, these methods work by selecting the best instance from a pool of candidates and then aggregating other instances based on similarity. In this work, we claim that carefully selecting the aggregation criteria can considerably improve the accuracy of the learned detector. We start by proposing an additional refinement step to an existing approach (OICR), which we call refinement knowledge distillation. Then, we present an adaptive supervision aggregation function that dynamically changes the aggregation criteria for selecting boxes related to one of the ground-truth classes, background, or even ignored during the generation of each refinement module supervision. Experiments in Pascal VOC 2007 demonstrate that our Knowledge Distillation and smooth aggregation function significantly improves the performance of OICR in the weakly supervised object detection and weakly supervised object localization tasks. These improvements make the Boosted-OICR competitive again versus other state-of-the-art approaches.\r"
  },
  "cvpr2020_w45_robustoneshotaudiotovideogeneration": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Robust One Shot Audio to Video Generation",
    "authors": [
      "Neeraj Kumar",
      "Srishti Goel",
      "Ankur Narang",
      "Mujtaba Hasan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Audio to Video generation is an interesting problem that has numerous applications across industry verticals including film making, multi-media, marketing, education and others. High-quality video generation with expressive facial movements is a challenging problem that involves complex learning steps for generative adversarial networks. Further, enabling one-shot learning for an unseen single image increases the complexity of the problem while simultaneously making it more applicable to practical scenarios. In the paper, we propose a novel approach OneShotA2V to synthesize a talking person video of arbitrary length using as input: an audio signal and a single unseen image of a person. OneShotA2V leverages curriculum learning to learn movements of expressive facial components and hence generates a high-quality talking head video of the given person. Further, it feeds the features generated from the audio input directly into a generative adversarial network and it adapts to any given unseen selfie by applying fewshot learning with only a few output updation epochs. OneShotA2V leverages spatially adaptive normalization based multi-level generator and multiple multi-level discriminators based architecture. The input audio clip is not restricted to any specific language, which gives the method multilingual applicability. Experimental evaluation demonstrates superior performance of OneShotA2V as compared to Realistic Speech-Driven Facial Animation with GANs(RSDGAN) [43], Speech2Vid [8], and other approaches, on multiple quantitative metrics including: SSIM (structural similarity index), PSNR (peak signal to noise ratio) and CPBD (image sharpness). Further, qualitative evaluation and Online Turing tests demonstrate the efficacy of our approach.\r"
  },
  "cvpr2020_w45_deflatingdatasetbiasusingsyntheticdataaugmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w45",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - DeepVision",
    "title": "Deflating Dataset Bias Using Synthetic Data Augmentation",
    "authors": [
      "Nikita Jaipuria",
      "Xianling Zhang",
      "Rohan Bhasin",
      "Mayar Arafa",
      "Punarjay Chakravarty",
      "Shubham Shrivastava",
      "Sagar Manglani",
      "Vidya N. Murali"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w45/Jaipuria_Deflating_Dataset_Bias_Using_Synthetic_Data_Augmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w45/Jaipuria_Deflating_Dataset_Bias_Using_Synthetic_Data_Augmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-specific environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the benefits of gaming engine simulations and sim2real style transfer techniques - for filling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs - parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a significant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.\r"
  },
  "cvpr2020_w28_cspnetanewbackbonethatcanenhancelearningcapabilityofcnn": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "CSPNet: A New Backbone That Can Enhance Learning Capability of CNN",
    "authors": [
      "Chien-Yao Wang",
      "Hong-Yuan Mark Liao",
      "Yueh-Hua Wu",
      "Ping-Yang Chen",
      "Jun-Wei Hsieh",
      "I-Hau Yeh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet.\r"
  },
  "cvpr2020_w28_enablingmonoculardepthperceptionattheveryedge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "Enabling Monocular Depth Perception at the Very Edge",
    "authors": [
      "Valentino Peluso",
      "Antonio Cipolletta",
      "Andrea Calimera",
      "Matteo Poggi",
      "Fabio Tosi",
      "Filippo Aleotti",
      "Stefano Mattoccia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Peluso_Enabling_Monocular_Depth_Perception_at_the_Very_Edge_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Peluso_Enabling_Monocular_Depth_Perception_at_the_Very_Edge_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Depth estimation is crucial in several computer vision applications, and a recent trend aims at inferring such a cue from a single camera through computationally demanding CNNs -- precluding their practical deployment in several application contexts characterized by low-power constraints. Purposely, we develop a tiny network tailored to microcontrollers, processing low-resolution images to obtain a coarse depth map of the observed scene. Our solution enables depth perception with minimal power requirements (a few hundreds of mW), accurately enough to pave the way to several high-level applications at-the-edge.\r"
  },
  "cvpr2020_w28_effectivedeep-learning-baseddepthdataanalysisonlow-powerhardwareforsupportingelderlycare": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "Effective Deep-Learning-Based Depth Data Analysis on Low-Power Hardware for Supporting Elderly Care",
    "authors": [
      "Christopher Pramerdorfer",
      "Rainer Planinc",
      "Martin Kampel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Pramerdorfer_Effective_Deep-Learning-Based_Depth_Data_Analysis_on_Low-Power_Hardware_for_Supporting_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Pramerdorfer_Effective_Deep-Learning-Based_Depth_Data_Analysis_on_Low-Power_Hardware_for_Supporting_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a detailed technical insight into a commercial vision-based sensor for monitoring residents in elderly care facilities and alerting caretakers in case of dangerous situations such as falls or residents not returning to their beds during nighttime. We focus on aspects that enable deep-learning-based object classification in realtime on low-end ARM-based hardware, which is prerequisite for a solution that is performant yet affordable, low-power, and unobtrusive. To this end, we introduce an efficient vision pipeline that maps the input depth data to concise virtual top-views. These views are then processed by a set of convolutional neural networks, with a scheduler selecting the most appropriate one based on the current operating conditions and available hardware resources. In order to overcome the challenge of acquiring large amounts of training data in this privacy-critical environment, we pretrain these networks on a large set of synthetic depth data. These concepts are general and applicable to similar vision tasks.\r"
  },
  "cvpr2020_w28_enablingincrementalknowledgetransferforobjectdetectionattheedge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "Enabling Incremental Knowledge Transfer for Object Detection at the Edge",
    "authors": [
      "Mohammad Farhadi",
      "Mehdi Ghasemi",
      "Sarma Vrudhula",
      "Yezhou Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Farhadi_Enabling_Incremental_Knowledge_Transfer_for_Object_Detection_at_the_Edge_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Farhadi_Enabling_Incremental_Knowledge_Transfer_for_Object_Detection_at_the_Edge_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Object detection using deep neural networks (DNNs) involves a huge amount of computation which impedes its implementation on resource/energy-limited user-end devices. The reason for the success of DNNs is due to having knowledge over all different domains of observed environments. However, we need a limited knowledge of the observed environment at inference time which can be learned using a shallow neural network (SHNN). In this paper, a system-level design is proposed to improve the energy consumption of object detection on the user-end device. An SHNN is deployed on the user-end device to detect objects in the observing environment. Also, a knowledge transfer mechanism has been implemented to update the SHNN model using the DNN knowledge when there is a change in the object domain. DNN knowledge can be obtained from a powerful edge device connected to the user-end device through LAN or WiFi. Experiments demonstrate that the energy consumption of the user-end device and the inference time can be improved by 78% and 40% compared with running the deep model on the user end device.\r"
  },
  "cvpr2020_w28_ahardwareprototypetargetingdistributeddeeplearningforon-deviceinference": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "A Hardware Prototype Targeting Distributed Deep Learning for On-Device Inference",
    "authors": [
      "Allen-Jasmin Farcas",
      "Guihong Li",
      "Kartikeya Bhardwaj",
      "Radu Marculescu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Farcas_A_Hardware_Prototype_Targeting_Distributed_Deep_Learning_for_On-Device_Inference_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Farcas_A_Hardware_Prototype_Targeting_Distributed_Deep_Learning_for_On-Device_Inference_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents a hardware prototype and a framework for a new communication-aware model compression for distributed on-device inference. Our approach relies on Knowledge Distillation (KD) and achieves orders of magnitude compression ratios on a large pre-trained teacher model. The distributed hardware prototype consists of multiple student models deployed on Raspberry-Pi 3 nodes that run Wide ResNet and VGG models on the CIFAR10 dataset for real-time image classification. We observe significant reductions in memory footprint (50x), energy consumption (14x), latency (33x) and an increase in performance (12x) without any significant accuracy loss compared to the initial teacher model. This is an important step towards deploying deep learning models for IoT applications.\r"
  },
  "cvpr2020_w28_challengesinenergy-efficientdeepneuralnetworktrainingwithfpga": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "Challenges in Energy-Efficient Deep Neural Network Training With FPGA",
    "authors": [
      "Yudong Tao",
      "Rui Ma",
      "Mei-Ling Shyu",
      "Shu-Ching Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Tao_Challenges_in_Energy-Efficient_Deep_Neural_Network_Training_With_FPGA_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Tao_Challenges_in_Energy-Efficient_Deep_Neural_Network_Training_With_FPGA_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In recent years, it is highly demanding to deploy Deep Neural Networks (DNNs) on edge devices, such as mobile phones, drones, robotics, and wearable devices, to process visual data collected by the cameras embedded in these systems. In addition to the model inference, training DNNs locally can benefit model customization and data privacy protection. Since many edge systems are powered by batteries or have limited energy budgets, Field-Programmable Gate Array (FPGA) is commonly used as the primary processing engine to satisfy both demands in performance and energy-efficiency. Although many recent research papers have been published on the topic of DNN inference with FPGAs, training a DNN with FPGAs has not been well exploited by the community. This paper summarizes the current status of adopting FPGA for DNN computation and identifies the main challenges in deploying DNN training on FPGAs. Moreover, a performance metric and evaluation workflow are proposed to compare the FPGA-based systems for DNN training in terms of (1) usage of on-chip resources, (2) training efficiency, (3) energy efficiency, and (4) model performance for specific computer vision tasks.\r"
  },
  "cvpr2020_w28_recursivehybridfusionpyramidnetworkforreal-timesmallobjectdetectiononembeddeddevices": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w28",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Low-Power Computer Vision Challenge",
    "title": "Recursive Hybrid Fusion Pyramid Network for Real-Time Small Object Detection on Embedded Devices",
    "authors": [
      "Ping-Yang Chen",
      "Jun-Wei Hsieh",
      "Chien-Yao Wang",
      "Hong-Yuan Mark Liao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w28/Chen_Recursive_Hybrid_Fusion_Pyramid_Network_for_Real-Time_Small_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w28/Chen_Recursive_Hybrid_Fusion_Pyramid_Network_for_Real-Time_Small_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper proposes a novel RHF-Net (Recursive Hybrid Fusion pyramid network) to solve the problem of small object detection on real-time embedded devices. Though the object detection accuracy rate is improved by a large margin with state-of-the-art models, e.g., SSD, YOLO, RetinaNet, and RefineDet, they are still problematic for small object detection and inefficient on embedded systems. One novelty of the RHF-Net is a bidirectional fusion module) that allows to fuse feature maps with both the top-down and bottom-up directions to generate flexible FPs for small object detection. This module can be easily integrated to any feature pyramid based object detection model. Another novelty of this net is a recursive concatenation and reshaping module which can recursively concatenate not only high-level semantic features from deep layers but also reshape spatially richer features from shallower layers to prevent small objects from disappearing. RHF-Net net adopts computationally low-cost and feature preserving operations in the fusion, thus it is efficient and accurate even on embedded devices. The superiority of RHF-Net is investigated on the COCO benchmark and UAVDT dataset in terms of mAP and FPS.\r"
  },
  "cvpr2020_w29_challengesinrecognizingspontaneousandintentionallyexpressedreactionstopositiveandnegativeimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w29",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Challenges and Promises to Inferring Emotion From Images and Video",
    "title": "Challenges in Recognizing Spontaneous and Intentionally Expressed Reactions to Positive and Negative Images",
    "authors": [
      "Jennifer Healey",
      "Haoliang Wang",
      "Niyati Chhaya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w29/Healey_Challenges_in_Recognizing_Spontaneous_and_Intentionally_Expressed_Reactions_to_Positive_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w29/Healey_Challenges_in_Recognizing_Spontaneous_and_Intentionally_Expressed_Reactions_to_Positive_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents a preliminary exploration of the challenges of automatically recognizing positive and negative facial expressions in both spontaneous and intentionally expressed conditions. Instead of recognizing iconic basic emotion states, which we have found to be less common in typical human computer interaction, we instead attempted to recognize only positive versus negative states. Our hypothesis was that this would prove more accurate if participants intentionally expressed their feelings. Our study consisted of analyzing video from seven participants, each participating in two sessions. Participants were asked to view 20 images, 10 positive and 10 negative, selected from the OASIS image data set. In the first session participants were instructed to react normally, while in the second session they were asked to intentionally express the emotion they felt when looking at each image. We extracted facial action coding units (AUs) from the recorded video and found that on average, intentionally expressed emotions generated 33% more AU intensity across action units associated with both negative emotions (AU1, AU2, AU4 and AU5) and 117% more intensity for AUs associated with positive emotions (AU6 and AU12). We also show that wide variation exists both in average participant responses across images and in individual reactions to images and that simply taking a ration of our identified action units is not sufficient to determine if a response is positive or negative even in the intentionally expressed case.\r"
  },
  "cvpr2020_w29_discriminantdistribution-agnosticlossforfacialexpressionrecognitioninthewild": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w29",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Challenges and Promises to Inferring Emotion From Images and Video",
    "title": "Discriminant Distribution-Agnostic Loss for Facial Expression Recognition in the Wild",
    "authors": [
      "Amir Hossein Farzaneh",
      "Xiaojun Qi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w29/Farzaneh_Discriminant_Distribution-Agnostic_Loss_for_Facial_Expression_Recognition_in_the_Wild_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w29/Farzaneh_Discriminant_Distribution-Agnostic_Loss_for_Facial_Expression_Recognition_in_the_Wild_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Facial Expression Recognition (FER) has demonstrated remarkable progress due to the advancement of deep Convolutional Neural Networks (CNNs). FER's goal as a visual recognition problem is to learn a mapping from the facial embedding space to a set of fixed expression categories using a supervised learning algorithm. Softmax loss as the de facto standard in practice fails to learn discriminative features for efficient learning. Center loss and its variants as promising solutions increase deep feature discriminability in the embedding space and enable efficient learning. They fundamentally aim to maximize intra-class similarity and inter-class separation in the embedding space. However, center loss and its variants ignore the underlying extreme class imbalance in challenging wild FER datasets. As a result, they lead to a separation bias toward majority classes and leave minority classes overlapped in the embedding space. In this paper, we propose a novel Discriminant Distribution-Agnostic loss (DDA loss) to optimize the embedding space for extreme class imbalance scenarios. Specifically, DDA loss enforces inter-class separation of deep features for both majority and minority classes. Any CNN model can be trained with the DDA loss to yield well separated deep feature clusters in the embedding space. We conduct experiments on two popular large-scale wild FER datasets (RAF-DB and AffectNet) to show the discriminative power of the proposed loss function.\r"
  },
  "cvpr2020_w29_predictingsentimentsinimageadvertisementsusingsemanticrelationsamongsentimentlabels": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w29",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Challenges and Promises to Inferring Emotion From Images and Video",
    "title": "Predicting Sentiments in Image Advertisements Using Semantic Relations Among Sentiment Labels",
    "authors": [
      "Stephen Pilli",
      "Manasi Patwardhan",
      "Niranjan Pedanekar",
      "Shirish Karande"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w29/Pilli_Predicting_Sentiments_in_Image_Advertisements_Using_Semantic_Relations_Among_Sentiment_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w29/Pilli_Predicting_Sentiments_in_Image_Advertisements_Using_Semantic_Relations_Among_Sentiment_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Understanding the sentiments evoked by advertisements is crucial in serving them appropriately to consumers. Advertisements often use images to evoke sentiments. An image can convey multiple sentiments of different nature. Automatically predicting these multiple sentiments can help serve better advertisements to consumers, especially in an online scenario at scale. In this paper, we present a neural network model based on graph convolution to predict such sentiments, which exploits the semantic relationship among the sentiment labels. We use it to predict multiple sentiment labels using an annotated dataset of 30,340 image-based advertisements. We also find a distance metric that best represents the distribution of sentiments in the dataset and utilizes it in a loss function that separates applicable sentiments from the non-applicable ones. We report an improvement in mean average precision and overall F1 score over a multi-modal multi-task state-of-the-art model.\r"
  },
  "cvpr2020_w29_facialactionunitrecognitioninthewildwithmulti-taskcnnself-trainingfortheemotionetchallenge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w29",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Challenges and Promises to Inferring Emotion From Images and Video",
    "title": "Facial Action Unit Recognition in the Wild With Multi-Task CNN Self-Training for the EmotioNet Challenge",
    "authors": [
      "Philipp Werner",
      "Frerk Saxen",
      "Ayoub Al-Hamadi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w29/Werner_Facial_Action_Unit_Recognition_in_the_Wild_With_Multi-Task_CNN_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w29/Werner_Facial_Action_Unit_Recognition_in_the_Wild_With_Multi-Task_CNN_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Automatic understanding of facial behavior is hampered by factors such as occlusion, illumination, non-frontal head pose, low image resolution, or limitations in labeled training data. The EmotioNet 2020 Challenge addresses these issues through a competition on recognizing facial action units on in-the-wild data. We propose to combine multi-task and self-training to make best use of the small manually / fully labeled and the large weakly / partially labeled training datasets provided by the challenge organizers. With our approach (and without using additional data) we achieve the second place in the 2020 challenge -- with a performance gap of only 0.05% to the challenge winner and of 5.9% to the third place. On the 2018 challenge evaluation data our method outperforms all other known results.\r"
  },
  "cvpr2020_w29_talemotionetchallenge2020rethinkingthemodelchosenprobleminmulti-tasklearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w29",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Challenges and Promises to Inferring Emotion From Images and Video",
    "title": "TAL EmotioNet Challenge 2020 Rethinking the Model Chosen Problem in Multi-Task Learning",
    "authors": [
      "Pengcheng Wang",
      "Zihao Wang",
      "Zhilong Ji",
      "Xiao Liu",
      "Songfan Yang",
      "Zhongqin Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w29/Wang_TAL_EmotioNet_Challenge_2020_Rethinking_the_Model_Chosen_Problem_in_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w29/Wang_TAL_EmotioNet_Challenge_2020_Rethinking_the_Model_Chosen_Problem_in_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper introduces our approach to the EmotioNet Challenge 2020. We pose the AU recognition problem as a multi-task learning problem, where the non-rigid facial muscle motion (mainly the first 17 AUs) and the rigid head motion (the last 6 AUs) are modeled separately. The co-occurrence of the expression features and the head pose features are explored. We observe that different AUs converge at various speed. By choosing the optimal checkpoint for each AU, the recognition results are improved. We are able to obtain a final score of 0.746 in validation set and 0.7306 in the test set of the challenge.\r"
  },
  "cvpr2020_w29_multipletransferlearningandmulti-labelbalancedtrainingstrategiesforfacialaudetectioninthewild": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w29",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Challenges and Promises to Inferring Emotion From Images and Video",
    "title": "Multiple Transfer Learning and Multi-Label Balanced Training Strategies for Facial AU Detection in the Wild",
    "authors": [
      "Sijie Ji",
      "Kai Wang",
      "Xiaojiang Peng",
      "Jianfei Yang",
      "Zhaoyang Zeng",
      "Yu Qiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w29/Ji_Multiple_Transfer_Learning_and_Multi-Label_Balanced_Training_Strategies_for_Facial_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w29/Ji_Multiple_Transfer_Learning_and_Multi-Label_Balanced_Training_Strategies_for_Facial_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents SIAT-NTU solution and results of facial action unit (AU) detection in the EmotiNet Challenge 2020. The task aims to detect 23 AUs from facial images in the wild, and its main difficulties lie in the imbalanced AU distribution and discriminative feature learning. We tackle these difficulties from the following aspects. First, to address the unconstrained heterogeneity of in-the-wild images, we detect and align faces with multi-task convolutional neural networks (MTCNN). Second, by using multiple transfer strategies, we pre-train large CNNs on multiple related datasets, e.g. face recognition datasets and facial expression datasets, and fine-tune them on the EmotiNetdataset. Third, we employ a multi-label balanced sampling strategy and a weighted loss to mitigate the imbalance problem. Last but not the least, to further improve performance, we ensemble multiple models and optimize the thresholds for each AU. Our proposed solution achieves an accuracy of 90.13% and F1 of 44.10% in the final test phase. Our Code is available at:https://github.com/kaiwang960112/ENC2020_AU_Detection\r"
  },
  "cvpr2020_w40_bamsprodasteptowardsgeneralizingtheadaptiveoptimizationmethodstodeepbinarymodel": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "BAMSProd: A Step Towards Generalizing the Adaptive Optimization Methods to Deep Binary Model",
    "authors": [
      "Junjie Liu",
      "Dongchao Wen",
      "Deyu Wang",
      "Wei Tao",
      "Tse-Wei Chen",
      "Kinya Osa",
      "Masami Kato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Liu_BAMSProd_A_Step_Towards_Generalizing_the_Adaptive_Optimization_Methods_to_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Liu_BAMSProd_A_Step_Towards_Generalizing_the_Adaptive_Optimization_Methods_to_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent methods have significantly reduced the performance degradation of Binary Neural Networks (BNNs), but guaranteeing the effective and efficient training of BNNs is an unsolved problem. The main reason is that the estimated gradients produced by the Straight-Through-Estimator (STE) mismatches with the gradients of the real derivatives. In this paper, we provide an explicit convex optimization example where training the BNNs with the traditionally adaptive optimization methods still faces the risk of non-convergence, and identify that constraining the range of gradients is critical for optimizing the deep binary model to avoid highly suboptimal solutions. Besides, we propose a BAMSProd algorithm with a key observation that the convergence property of optimizing deep binary model is strongly related to the quantization errors. In brief, it employs an adaptive range constraint via an errors measurement for smoothing the gradients transition while follows the exponential moving strategy from AMSGrad to avoid errors accumulation during the optimization. The experiments verify the corollary of theoretical convergence analysis, and further demonstrate that our optimization method can speed up the convergence about 1.2x and boost the performance of BNNs to a significant level than the specific binary optimizer about 3.7%, even in a highly non-convex optimization problem.\r"
  },
  "cvpr2020_w40_dynamicinferenceanewapproachtowardefficientvideoactionrecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Dynamic Inference: A New Approach Toward Efficient Video Action Recognition",
    "authors": [
      "Wenhao Wu",
      "Dongliang He",
      "Xiao Tan",
      "Shifeng Chen",
      "Yi Yang",
      "Shilei Wen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Though action recognition in videos has achieved great success recently, it remains a challenging task due to the massive computational cost. Designing lightweight networks is a possible solution, but it may degrade the recognition performance. In this paper, we innovatively propose a general dynamic inference idea to improve inference efficiency by leveraging the variation in the distinguishability of different videos. The dynamic inference approach can be achieved from aspects of the network depth and the number of input video frames, or even in a joint input-wise and network depth-wise manner. In a nutshell, we treat input frames and network depth of the computational graph as a 2-dimensional grid, and several checkpoints are placed on this grid in advance with a prediction module. The inference is carried out progressively on the grid by following some predefined route, whenever the inference process comes across a checkpoint, an early prediction can be made depending on whether the early stop criteria meets. For the proof-of-concept purpose, we instantiate several dynamic inference frameworks. In these instances, we overcome the drawback of limited temporal coverage resulted from an early prediction by a novel frame permutation scheme, and alleviate the conflict between progressive computation and video temporal relation modeling by introducing the online temporal shift module. Extensive experiments are conducted to thoroughly analyze the effectiveness of our ideas and to inspire future research efforts. Results on various datasets also evident the superiority of our approach.\r"
  },
  "cvpr2020_w40_learninglow-rankdeepneuralnetworksviasingularvectororthogonalityregularizationandsingularvaluesparsification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Learning Low-Rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification",
    "authors": [
      "Huanrui Yang",
      "Minxue Tang",
      "Wei Wen",
      "Feng Yan",
      "Daniel Hu",
      "Ang Li",
      "Hai Li",
      "Yiran Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Yang_Learning_Low-Rank_Deep_Neural_Networks_via_Singular_Vector_Orthogonality_Regularization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Yang_Learning_Low-Rank_Deep_Neural_Networks_via_Singular_Vector_Orthogonality_Regularization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Modern deep neural networks (DNNs) often require high memory consumption and large computational loads. In order to deploy DNN algorithms efficiently on edge or mobile devices, a series of DNN compression algorithms have been explored, including factorization methods. Factorization methods approximate the weight matrix of a DNN layer with the multiplication of two or multiple low-rank matrices. However, it is hard to measure the ranks of DNN layers during the training process. Previous works mainly induce low-rank through implicit approximations or via costly singular value decomposition (SVD) process on every training step. The former approach usually induces a high accuracy loss while the latter has a low efficiency. In this work, we propose SVD training, the first method to explicitly achieve low-rank DNNs during training without applying SVD on every step. SVD training first decomposes each layer into the form of its full-rank SVD, then performs training directly on the decomposed weights. We add orthogonality regularization to the singular vectors, which ensure the valid form of SVD and avoid gradient vanishing/exploding. Low-rank is encouraged by applying sparsity-inducing regularizers on the singular values of each layer. Singular value pruning is applied at the end to explicitly reach a low-rank model. We empirically show that SVD training can significantly reduce the rank of DNN layers and achieve higher reduction on computation load under the same accuracy, comparing to not only previous factorization methods but also state-of-the-art filter pruning methods.\r"
  },
  "cvpr2020_w40_low-bitquantizationneedsgooddistribution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Low-Bit Quantization Needs Good Distribution",
    "authors": [
      "Haibao Yu",
      "Tuopu Wen",
      "Guangliang Cheng",
      "Jiankai Sun",
      "Qi Han",
      "Jianping Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Yu_Low-Bit_Quantization_Needs_Good_Distribution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Yu_Low-Bit_Quantization_Needs_Good_Distribution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Low-bit quantization is challenging to maintain high performance with limited model capacity (e.g., 4-bit for both weights and activations). Naturally, the distribution of both weights and activations in deep neural network are Gaussian-like. Nevertheless, due to the limited bitwidth of low-bit model, uniform-like distributed weights and activations have been proved to be more friendly to quantization while preserving accuracy. Motivated by this, we propose Scale-Clip, a Distribution Reshaping technique that can reshape weights or activations into a uniform-like distribution in a dynamic manner. Furthermore, to increase the model capability for a low-bit model, a novel Group-based Quantization algorithm is proposed to split the filters into several groups. Different groups can learn different quantization parameters, which can be elegantly merged into batch normalization layer without extra computational cost in the inference stage. Finally, we integrate Scale-Clip technique with Group-based Quantization algorithm and propose the Group-based Distribution Reshaping Quantization (GDRQ) framework to further improve the quantization performance. Experiments on various networks (e.g. VGGNet and ResNet) and vision tasks (e.g. classification, detection, and segmentation) demonstrate that our framework achieves much better performance than state-of-the-art quantization methods. Specifically, the ResNet-50 model with 2-bit weights and 4-bit activations obtained by our framework achieves less than 1% accuracy drop on ImageNet classification task, which is a new state-of-the-art to our best knowledge.\r"
  },
  "cvpr2020_w40_attentivesemanticpreservationnetworkforzero-shotlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Attentive Semantic Preservation Network for Zero-Shot Learning",
    "authors": [
      "Ziqian Lu",
      "Yunlong Yu",
      "Zhe-Ming Lu",
      "Feng-Li Shen",
      "Zhongfei Zhang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Lu_Attentive_Semantic_Preservation_Network_for_Zero-Shot_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Lu_Attentive_Semantic_Preservation_Network_for_Zero-Shot_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While promising progress has been achieved in the Zero-Shot Learning (ZSL) task , the existing generated approaches still suffer from overly plain pseudo features, resulting in poor discrimination of the generated visual features. To improve the quality of the generated features, we propose a novel Attentive Semantic Preservation Network (ASPN) to encode more discriminative as well as semantic-related information into the generated features with the category self-attention cues. Specifically, the feature generation and the semantic inference modules are formulated into a unified process to promote each other, which can effectively align the crossmodality semantic relation. The category attentive strategy encourages model to focus more on intrinsic information of the noisy generated features to alleviate the confusion of generated features. Moreover, prototype-based classification mechanism is introduced in an efficient way of leveraging known semantic information to further boost discriminative of the generated features. Experiments on four popular benchmarks, i.e., AWA1, AWA2, CUB, and FLO verify that our proposed approach outperforms state-of-the-art methods with obvious improvements under both the Traditional ZSL (TZSL) and the Generalized ZSL (GZSL) settings.\r"
  },
  "cvpr2020_w40_mimictherawdomainacceleratingactionrecognitioninthecompresseddomain": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Mimic the Raw Domain: Accelerating Action Recognition in the Compressed Domain",
    "authors": [
      "Barak Battash",
      "Haim Barad",
      "Hanlin Tang",
      "Amit Bleiweiss"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Battash_Mimic_the_Raw_Domain_Accelerating_Action_Recognition_in_the_Compressed_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Battash_Mimic_the_Raw_Domain_Accelerating_Action_Recognition_in_the_Compressed_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Video understanding usually requires expensive computation that prohibits its deployment, yet videos contain significant spatiotemporal redundancy that can be exploited. In particular, operating directly on the motion vectors and residuals in the compressed video domain can significantly accelerate the compute, by not using the raw videos which demand colossal storage capacity. Existing methods approach this task as a multiple modalities problem. In this paper we are approaching the task in a completely different way; we are looking at the data from the compressed stream as a one unit clip and propose that the residual frames can replace the original RGB frames from the raw domain. Furthermore, we are using the teacher-student method to aid the network in the compressed domain to mimic the teacher network in the raw domain. We show experiments on three leading datasets (HMDB51, UCF1, and Kinetics) that approach state-of-the-art accuracy on raw video data by using compressed data. Our model MFCD-Net outperforms prior methods in the compressed domain and more importantly, our model has 11X fewer parameters and 3X fewer Flops, dramatically improving the efficiency of video recognition inference. This approach enables applying neural networks exclusively in the compressed domain without compromising accuracy while accelerating performance.\r"
  },
  "cvpr2020_w40_constraint-awareimportanceestimationforglobalfilterpruningundermultipleresourceconstraints": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Constraint-Aware Importance Estimation for Global Filter Pruning Under Multiple Resource Constraints",
    "authors": [
      "Yu-Cheng Wu",
      "Chih-Ting Liu",
      "Bo-Ying Chen",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Wu_Constraint-Aware_Importance_Estimation_for_Global_Filter_Pruning_Under_Multiple_Resource_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Wu_Constraint-Aware_Importance_Estimation_for_Global_Filter_Pruning_Under_Multiple_Resource_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Filter pruning is an efficient way to structurally remove the redundant parameters in convolutional neural network, where at the same time reduces the computation, memory storage and transfer cost. Recent state-of-the-art methods globally estimate the importance of each filter based on its impact to the loss and iteratively remove those with smaller values until the pruned network meets some resource constraints, such as the commonly used number (or ratio) of filter left. However, when there is a more practical constraint like the total number of FLOPs, they ignore its relation to the estimation of filter importance. We propose a novel method called Constraint-Aware Importance Estimation (CAIE) that integrates information of the impact on the given resource into the original importance estimation only based on loss when pruning each filter. Moreover, our CAIE can be generalized to the pruning problem under multiple resource constraints simultaneously. Extensive experiments show that under the same multiple resource constraints, the model pruned with our CAIE method can not only accurately meet the constraints but also achieve the optimal performance results when comparing to existing state-of-the-art methods.\r"
  },
  "cvpr2020_w40_fonetamemory-efficientfourier-basedorthogonalnetworkforobjectrecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "FoNet: A Memory-Efficient Fourier-Based Orthogonal Network for Object Recognition",
    "authors": [
      "Feng Wei",
      "Uyen Trang Nguyen",
      "Hui Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Wei_FoNet_A_Memory-Efficient_Fourier-Based_Orthogonal_Network_for_Object_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Wei_FoNet_A_Memory-Efficient_Fourier-Based_Orthogonal_Network_for_Object_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The memory consumption of most Convolutional Neural Network (CNN) architectures grows rapidly with the increasing depth of the network, which is a major constraint for efficient network training and inference on modern GPUs with limited memory. Several studies show that the feature maps (as generated after the convolutional layers) are the big bottleneck in this memory problem. Often, these feature maps mimic natural photographs in the sense that their energy is concentrated in the spectral domain. In this paper, we propose a \\underline F ourier-based \\underline O rthogonal \\underline Net work (FoNet) that incorporates orthogonal representations and performs both the convolution and the activation operations in the spectral domain to achieve memory reduction. The performance of our FoNet is evaluated on four standard object recognition benchmarks (i.e., MNIST, CIFAR-10, SVHN, and ImageNet), and compared with four state-of-the-art implementations (i.e., LeNet, AlexNet, VGG, and DenseNet). Encouragingly, FoNet is able to reduce memory consumption by about 60% without significant loss of performance for all tested network architectures.\r"
  },
  "cvpr2020_w40_computer-aideddiagnosissystemoflungcarcinomausingconvolutionalneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Computer-Aided Diagnosis System of Lung Carcinoma Using Convolutional Neural Networks",
    "authors": [
      "Fangjian Han",
      "Li Yu",
      "Yi Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Han_Computer-Aided_Diagnosis_System_of_Lung_Carcinoma_Using_Convolutional_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Han_Computer-Aided_Diagnosis_System_of_Lung_Carcinoma_Using_Convolutional_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " With the number of lung cancers' morbidity and mortality is showing a trend of increasing year by year, the demand for pathologists is increasing rapidly, so in this study, we aimed to design a medical pathologically assistant diagnostic system to help pathologists complete diagnostic analysis tasks. A Deep Convolutional Neural Network(DCNN) is adopted to automatically distinguish tumor tissues from normal tissues in digitized hematoxylin and eosin (H&E) stained lung cell pathological slides that collected from The Cancer Genome Atlas (TCGA) and collaborate hospitals, we trained and evaluate WSIs(the whole slide images) captured at 10x magnification and other higher magnification, results show the difference are negligible. Moreover, we also compared the training effect of different models on same level magnification WSIs, the results show that performance of Resnet-18 network model and Resnet-50 network model is nearly consistent. Actually processing time based on Resnet-18 model is shorter than Resnet-50 model, so we don't need deeper network for study. Our system was shown to enormous advantages in accuracy, sensitivity and efficiency, could reduce the burden on pathologists, enable them to spend more time on advanced decision-making tasks, would be widely applied to pathological diagnosis, clinical practice, scientific research and so on.\r"
  },
  "cvpr2020_w40_fasthardware-awareneuralarchitecturesearch": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Fast Hardware-Aware Neural Architecture Search",
    "authors": [
      "Li Lyna Zhang",
      "Yuqing Yang",
      "Yuhang Jiang",
      "Wenwu Zhu",
      "Yunxin Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Zhang_Fast_Hardware-Aware_Neural_Architecture_Search_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Zhang_Fast_Hardware-Aware_Neural_Architecture_Search_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Designing accurate and efficient convolutional neural architectures for vast amount of hardware is challenging because hardware designs are complex and diverse. This paper addresses the hardware diversity challenge in Neural Architecture Search (NAS). Unlike previous approaches that apply search algorithms on a small, human-designed search space without considering hardware diversity, we propose HURRICANE that explores the automatic hardware-aware search over a much larger search space and a two-stage search algorithm, to efficiently generate tailored models for different types of hardware. Extensive experiments on ImageNet demonstrate that our algorithm outperforms state-of-the-art hardware-aware NAS methods under the same latency constraint on three types of hardware. Moreover, the discovered architectures achieve much lower latency and higher accuracy than current state-of-the-art efficient models. Remarkably, HURRICANE achieves a 76.67% top-1 accuracy on ImageNet with a inference latency of only 16.5 ms for DSP, which is a 3.47% higher accuracy and a 6.35x inference speedup than FBNet-iPhoneX, respectively. For VPU, we achieve a 0.53% higher top-1 accuracy than Proxyless-mobile with a 1.49x speedup. Even for well-studied mobile CPU, we achieve a 1.63% higher top-1 accuracy than FBNet-iPhoneX with a comparable inference latency. HURRICANE also reduces the training time by 30.4% compared to SPOS.\r"
  },
  "cvpr2020_w40_learningsparseneuralnetworksthroughmixture-distributedregularization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Learning Sparse Neural Networks Through Mixture-Distributed Regularization",
    "authors": [
      "Chang-Ti Huang",
      "Jun-Cheng Chen",
      "Ja-Ling Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Huang_Learning_Sparse_Neural_Networks_Through_Mixture-Distributed_Regularization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Huang_Learning_Sparse_Neural_Networks_Through_Mixture-Distributed_Regularization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " L0-norm regularization is one of the most efficient approaches to learn a sparse neural network. Due to its discrete nature, differentiable and approximate regularizations based on the concrete distribution or its variants are proposed as alternatives; however, the concrete relaxation suffers from high-variance gradient estimates and is limited to its own concrete distribution. To address these issues, in this paper, we propose a more general framework for relaxing binary gates through mixture distributions. With the proposed method, any mixture pair of distributions converging to d(0) and d(1) can be applied to construct smoothed binary gates. We further introduce a reparameterization method for the smoothed binary gates drawn from mixture distributions to enable efficient gradient gradient-based optimization under the proposed deep learning algorithm. Extensive experiments are conducted, and the results show that the proposed approach achieves better performance in terms of pruned architectures, structured sparsity and the reduced number of floating point operations (FLOPs) as compared with other state-of-the-art sparsity-inducing methods.\r"
  },
  "cvpr2020_w40_lsq+improvinglow-bitquantizationthroughlearnableoffsetsandbetterinitialization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "LSQ+: Improving Low-Bit Quantization Through Learnable Offsets and Better Initialization",
    "authors": [
      "Yash Bhalgat",
      "Jinwon Lee",
      "Markus Nagel",
      "Tijmen Blankevoort",
      "Nojun Kwak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Bhalgat_LSQ_Improving_Low-Bit_Quantization_Through_Learnable_Offsets_and_Better_Initialization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Bhalgat_LSQ_Improving_Low-Bit_Quantization_Through_Learnable_Offsets_and_Better_Initialization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Unlike ReLU, newer activation functions (like Swish, H-swish, Mish) that are frequently employed in popular efficient architectures can also result in negative activation values, with skewed positive and negative ranges. Typical learnable quantization schemes [PACT, LSQ] assume unsigned quantization for activations and quantize all negative activations to zero which leads to significant loss in performance. Naively using signed quantization to accommodate these negative values requires an extra sign bit which is expensive for low-bit (2-, 3-, 4-bit) quantization. To solve this problem, we propose LSQ+, a natural extension of LSQ, wherein we introduce a general asymmetric quantization scheme with trainable scale and offset parameters that can learn to accommodate the negative activations. Gradient-based learnable quantization schemes also commonly suffer from high instability or variance in the final training performance, hence requiring a great deal of hyper-parameter tuning to reach a satisfactory performance. LSQ+ alleviates this problem by using an MSE-based initialization scheme for the quantization parameters. We show that this initialization leads to significantly lower variance in final performance across multiple training runs. Overall, LSQ+ shows state-of-the-art results for EfficientNet and MixNet and also significantly outperforms LSQ for low-bit quantization of neural nets with Swish activations (e.g.: 1.8% gain with W4A4 quantization and upto 5.6% gain with W2A2 quantization of EfficientNet-B0 on ImageNet dataset). To the best of our knowledge, ours is the first work to quantize such architectures to extremely low bit-widths.\r"
  },
  "cvpr2020_w40_leastsquaresbinaryquantizationofneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Least Squares Binary Quantization of Neural Networks",
    "authors": [
      "Hadi Pouransari",
      "Zhucheng Tu",
      "Oncel Tuzel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Pouransari_Least_Squares_Binary_Quantization_of_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Pouransari_Least_Squares_Binary_Quantization_of_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Quantizing weights and activations of deep neural networks results in significant improvement in inference efficiency at the cost of lower accuracy. A source of the accuracy gap between full precision and quantized models is the quantization error. In this work, we focus on the binary quantization, in which values are mapped to -1 and 1. We provide a unified framework to analyze different scaling strategies. Inspired by the pareto-optimality of 2-bits versus 1-bit quantization, we introduce a novel 2-bits quantization with provably least squares error. Our quantization algorithms can be implemented efficiently on the hardware using bitwise operations. We present proofs to show that our proposed methods are optimal, and also provide empirical error analysis. We conduct experiments on the ImageNet dataset and show a reduced accuracy gap when using the proposed least squares quantization algorithms.\r"
  },
  "cvpr2020_w40_refinedetlitealightweightone-stageobjectdetectionframeworkforcpu-onlydevices": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "RefineDetLite: A Lightweight One-Stage Object Detection Framework for CPU-Only Devices",
    "authors": [
      "Chen Chen",
      "Mengyuan Liu",
      "Xiandong Meng",
      "Wanpeng Xiao",
      "Qi Ju"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Chen_RefineDetLite_A_Lightweight_One-Stage_Object_Detection_Framework_for_CPU-Only_Devices_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Chen_RefineDetLite_A_Lightweight_One-Stage_Object_Detection_Framework_for_CPU-Only_Devices_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Previous state-of-the-art real-time object detectors have been reported on GPUs which are extremely expensive for processing massive data and in resource-restricted scenarios. Therefore, high efficiency object detectors on CPU-only devices are urgently-needed in industry. The floating-point operations (FLOPs) of networks are not strictly proportional to the running speed on CPU devices, which inspires the design of an exactly \"fast\" and \"accurate\" object detector. After investigating the concern gaps between classification networks and detection backbones, and following the design principles of efficient networks, we propose a lightweight residual-like backbone with large receptive fields and wide dimensions for low-level features, which are crucial for detection tasks. Correspondingly, we also design a light-head detection part to match the backbone capability. Furthermore, by analyzing the drawbacks of current one-stage detector training strategies, we also propose three orthogonal training strategies--IOU-guided loss, classes-aware weighting method and balanced multitask training approach. Without bells and whistles, our proposed RefineDetLite achieves 26.8 mAP on the MSCOCO benchmark at a speed of 130 ms/pic on a single-thread CPU. The detection accuracy can be further increased to 29.6 mAP by integrating all the proposed training strategies, without apparent speed drop.\r"
  },
  "cvpr2020_w40_randaugmentpracticalautomateddataaugmentationwithareducedsearchspace": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Randaugment: Practical Automated Data Augmentation With a Reduced Search Space",
    "authors": [
      "Ekin D. Cubuk",
      "Barret Zoph",
      "Jonathon Shlens",
      "Quoc V. Le"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Cubuk_Randaugment_Practical_Automated_Data_Augmentation_With_a_Reduced_Search_Space_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Cubuk_Randaugment_Practical_Automated_Data_Augmentation_With_a_Reduced_Search_Space_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent work on automated augmentation strategies has led to state-of-the-art results in image classification and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated augmentation strategies. We find that while previous work required a search for both magnitude and probability of each operation independently, it is sufficient to only search for a single distortion magnitude that jointly controls all operations. We hence propose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task. Despite the simplifications, our method achieves equal or better performance over previous automated augmentation strategies on on CIFAR-10/100, SVHN, ImageNet and COCO datasets. EfficientNet-B7, we achieve 85.0% accuracy, a 1.0% increase over baseline augmentation, a 0.6% improvement over AutoAugment on the ImageNet dataset. With EfficientNet-B8, we achieve 85.4% accuracy on ImageNet, which matches a previous result that used 3.5B extra images. On object detection, the same method as classification leads to 1.0-1.3% improvement over baseline augmentation. Code will be made available online.\r"
  },
  "cvpr2020_w40_any-widthnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Any-Width Networks",
    "authors": [
      "Thanh Vu",
      "Marc Eder",
      "True Price",
      "Jan-Michael Frahm"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Vu_Any-Width_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Vu_Any-Width_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Despite remarkable improvements in speed and accuracy, convolutional neural networks (CNNs) still typically operate as monolithic entities at inference time. This poses a challenge for resource-constrained practical applications, where both computational budgets and performance needs can vary with the situation. To address these constraints, we propose the Any-Width Network (AWN), an adjustable-width CNN architecture and associated training routine that allow for fine-grained control over speed and accuracy during inference. Our key innovation is the use of lower-triangular weight matrices which explicitly address width-varying batch statistics while being naturally suited for multi-width operations. We also show that this design facilitates an efficient training routine based on random width sampling. We empirically demonstrate that our proposed AWNs compare favorably to existing methods while providing maximally granular control during inference.\r"
  },
  "cvpr2020_w40_adamt-netanadaptiveweightlearningbasedmulti-tasklearningmodelforsceneunderstanding": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "AdaMT-Net: An Adaptive Weight Learning Based Multi-Task Learning Model for Scene Understanding",
    "authors": [
      "Ankit Jha",
      "Awanish Kumar",
      "Biplab Banerjee",
      "Subhasis Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Jha_AdaMT-Net_An_Adaptive_Weight_Learning_Based_Multi-Task_Learning_Model_for_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Jha_AdaMT-Net_An_Adaptive_Weight_Learning_Based_Multi-Task_Learning_Model_for_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We tackle the problem of deep end-to-end multi-task learning (MTL) for visual scene understanding from monocular images in this paper. It is proven that learning several related tasks together helps in attaining improved performance per-task than training them independently. This is due to the fact that related tasks share important feature properties among themselves, which the MTL techniques can effectively explore for improved joint training. Following the same, we are interested in judiciously segregating the task-centric feature learning stage from a learnable task-generic feature space. To this end, we propose a typical U-Net based encoder-decoder architecture calledAdaMT-Netwhere the densely-connected deep convolutional neural network (CNN) based feature encoder is shared among the tasks while the soft-attention based task-specific decoder modules produce the desired outcomes at the end. One major issue in MTL is to select the weights for the task-specific loss-terms in the final optimization function. As opposed to manual weight selection, we propose a novel adaptive weight learning strategy by carefully exploring the loss-gradients per-task in different training iterations. We validate AdaMT-Net on the challenging CityScapes, NYUv2, and ISPRS datasets, where consistently improved performance can be observed.\r"
  },
  "cvpr2020_w40_ternarymobilenetsviaper-layerhybridfilterbanks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Ternary MobileNets via Per-Layer Hybrid Filter Banks",
    "authors": [
      "Dibakar Gope",
      "Jesse Beu",
      "Urmish Thakker",
      "Matthew Mattina"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Gope_Ternary_MobileNets_via_Per-Layer_Hybrid_Filter_Banks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Gope_Ternary_MobileNets_via_Per-Layer_Hybrid_Filter_Banks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. New applications with stringent real-time requirements on highly constrained devices require further compression of MobileNets-like compute-efficient networks. Model quantization is a widely used technique to compress and accelerate neural network inference and prior works have quantized MobileNets to 4-6 bits, albeit with a modest to significant drop in accuracy. While quantization to sub-byte values (i.e. precision <= 8 bits) has been valuable, even further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, we propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. Using this proposed quantization method, we quantize a substantial portion of weight filters of MobileNets to ternary values resulting in 27.98% savings in energy, and a 51.07% reduction in the model size, while achieving comparable accuracy and no degradation in throughput on specialized hardware in comparison to the baseline full-precision MobileNets. Finally, we demonstrate the generalizability and effectiveness of hybrid filter banks to other neural network architectures.\r"
  },
  "cvpr2020_w40_data-freenetworkquantizationwithadversarialknowledgedistillation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Data-Free Network Quantization With Adversarial Knowledge Distillation",
    "authors": [
      "Yoojin Choi",
      "Jihwan Choi",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Choi_Data-Free_Network_Quantization_With_Adversarial_Knowledge_Distillation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Choi_Data-Free_Network_Quantization_With_Adversarial_Knowledge_Distillation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Network quantization is an essential procedure in deep learning for development of efficient fixed-point inference models on mobile or edge platforms. However, as datasets grow larger and privacy regulations become stricter, data sharing for model compression gets more difficult and restricted. In this paper, we consider data-free network quantization with synthetic data. The synthetic data are generated from a generator, while no data are used in training the generator and in quantization. To this end, we propose data-free adversarial knowledge distillation, which minimizes the maximum distance between the outputs of the teacher and the (quantized) student for any adversarial samples from a generator. To generate adversarial samples similar to the original data, we additionally propose matching statistics from the batch normalization layers for generated data and the original data in the teacher. Furthermore, we show the gain of producing diverse adversarial samples by using multiple generators and multiple students. Our experiments show the state-of-the-art data-free model compression and quantization results for (wide) residual networks and MobileNet on SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The accuracy losses compared to using the original datasets are shown to be very minimal.\r"
  },
  "cvpr2020_w40_nowthaticansee,icanimproveenablingdata-drivenfinetuningofcnnsontheedge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Now That I Can See, I Can Improve: Enabling Data-Driven Finetuning of CNNs on the Edge",
    "authors": [
      "Aditya Rajagopal",
      "Christos-Savvas Bouganis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Rajagopal_Now_That_I_Can_See_I_Can_Improve_Enabling_Data-Driven_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Rajagopal_Now_That_I_Can_See_I_Can_Improve_Enabling_Data-Driven_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In today's world, a vast amount of data is being generated by edge devices that can be used as valuable training data to improve the performance of machine learning algorithms in terms of the achieved accuracy or to reduce the compute requirements of the model. However, due to user data privacy concerns as well as storage and communication bandwidth limitations, this data cannot be moved from the device to the data centre for further improvement of the model and subsequent deployment. As such there is a need for increased edge intelligence, where the deployed models can be fine-tuned on the edge, leading to improved accuracy and/or reducing the model's workload as well as its memory and power footprint. In the case of Convolutional Neural Networks (CNNs), both the weights of the network as well as its topology can be tuned to adapt to the data that it processes. This paper provides a first step towards enabling CNN finetuning on an edge device based on structured pruning. It explores the performance gains and costs of doing so and presents an extensible open-source framework that allows the deployment of such approaches on a wide range of network architectures and devices. The results show that on average, data-aware pruning with retraining can provide 10.2pp increased accuracy over a wide range of subsets, networks and pruning levels with a maximum improvement of 42.0pp over pruning and retraining in a manner agnostic to the data being processed by the network.\r"
  },
  "cvpr2020_w40_structuredweightunificationandencodingforneuralnetworkcompressionandacceleration": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Structured Weight Unification and Encoding for Neural Network Compression and Acceleration",
    "authors": [
      "Wei Jiang",
      "Wei Wang",
      "Shan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Jiang_Structured_Weight_Unification_and_Encoding_for_Neural_Network_Compression_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Jiang_Structured_Weight_Unification_and_Encoding_for_Neural_Network_Compression_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We investigate structured joint weight unification and weight encoding to compress deep neural network models for reduced storage and computation. A structured weight unification method is proposed, where weight coefficients are unified according to a hardware-friendly structure, so that the unified weights can be effectively encoded and the inference computation can be accelerated. Our method can be seen as a generalization of structured weight pruning, where we unify weights of a selected structure to share some value instead of removing them. A 3D pyramid-based encoding method is further proposed to team up with the structurally learned weights, providing a systematic solution for compressing neural network models while preserving the network capacity and the original prediction performance. Also, we develop a training framework to iteratively optimize the subproblems of weight unification and target prediction, which ensures the unification rate with little prediction loss. Experiments over several benchmark models and datasets of different tasks demonstrate the effectiveness of our approach.\r"
  },
  "cvpr2020_w40_neuralnetworkcompressionusinghigher-orderstatisticsandauxiliaryreconstructionlosses": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Neural Network Compression Using Higher-Order Statistics and Auxiliary Reconstruction Losses",
    "authors": [
      "Christos Chatzikonstantinou",
      "Georgios Th. Papadopoulos",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Chatzikonstantinou_Neural_Network_Compression_Using_Higher-Order_Statistics_and_Auxiliary_Reconstruction_Losses_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Chatzikonstantinou_Neural_Network_Compression_Using_Higher-Order_Statistics_and_Auxiliary_Reconstruction_Losses_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, the problem of pruning and compressing the weights of various layers of deep neural networks is investigated. The proposed method aims to remove redundant filters from the network in order to reduce computational complexity and storage requirements, while improving the performance of the original network. More specifically, a novel filter selection criterion is introduced based on the fact that filters whose weights follow a Gaussian distribution correspond to hidden units that do not capture important aspects of data. To this end, Higher Order Statistics (HOS) are used and filters with low cumulant values that do not deviate significantly from Gaussian distribution are identified and removed from the network. In addition, a novel pruning strategy is proposed aiming to decide on the pruning ratio of each individual layer using the Shapiro-Wilk normality test. The use of auxiliary MSE losses (intermediate and after the softmax layer) during the fine-tuning phase further improves the overall performance of the compressed network. Extensive experiments with different network architectures and comparison with state-of-the-art approaches on well-known public datasets, such as CIFAR-10, CIFAR-100 and ILSCVR-12, demonstrate the great potential of the proposed approach.\r"
  },
  "cvpr2020_w40_montecarlogradientquantization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Monte Carlo Gradient Quantization",
    "authors": [
      "Goncalo Mordido",
      "Matthijs Van Keirsbilck",
      "Alexander Keller"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Mordido_Monte_Carlo_Gradient_Quantization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Mordido_Monte_Carlo_Gradient_Quantization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose Monte Carlo methods to leverage both sparsity and quantization to compress gradients of neural networks throughout training. On top of reducing the communication exchanged between multiple workers in a distributed setting, we also improve the computational efficiency of each worker. Our method, called Monte Carlo Gradient Quantization (MCGQ), shows faster convergence and higher performance than existing quantization methods on image classification and language modeling. Using both low-bit-width-quantization and high sparsity levels, our method more than doubles the rates of existing compression methods from 200xto 520xand 462xto more than 1200xon different language modeling tasks.\r"
  },
  "cvpr2020_w40_ditheredbackpropasparseandquantizedbackpropagationalgorithmformoreefficientdeepneuralnetworktraining": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Dithered Backprop: A Sparse and Quantized Backpropagation Algorithm for More Efficient Deep Neural Network Training",
    "authors": [
      "Simon Wiedemann",
      "Temesgen Mehari",
      "Kevin Kepp",
      "Wojciech Samek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Wiedemann_Dithered_Backprop_A_Sparse_and_Quantized_Backpropagation_Algorithm_for_More_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Wiedemann_Dithered_Backprop_A_Sparse_and_Quantized_Backpropagation_Algorithm_for_More_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep Neural Networks are successful but highly computationally expensive learning systems. One of the main sources of time and energy drains is the well known backpropagation (backprop) algorithm, which roughly accounts for 2/3 of the computational cost of training. In this work we propose a method for reducing the computational complexity of backprop, which we named dithered backprop. It consists on applying a stochastic quantization scheme to intermediate results of the method. The particular quantisation scheme, called non-subtractive dither (NSD), induces sparsity which can be exploited by computing efficient sparse matrix multiplications. Experiments on popular image classification tasks show that it induces 92% sparsity on average across a wide set of models at no or negligible accuracy drop in comparison to state-of-the-art approaches, thus significantly reducing the computational complexity of the backward pass. Moreover, we show that our method is fully compatible to state-of-the-art training methods that reduce the bit-precision of training down to 8-bits, as such being able to further reduce the computational requirements. Finally we discuss and show potential benefits of applying dithered backprop on a distributed training settings, in that communication as well as compute efficiency may increase simultaneously with the number of participant nodes.\r"
  },
  "cvpr2020_w40_learningsparse&ternaryneuralnetworkswithentropy-constrainedtrainedternarization(ec2t)": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Learning Sparse & Ternary Neural Networks With Entropy-Constrained Trained Ternarization (EC2T)",
    "authors": [
      "Arturo Marban",
      "Daniel Becking",
      "Simon Wiedemann",
      "Wojciech Samek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Marban_Learning_Sparse__Ternary_Neural_Networks_With_Entropy-Constrained_Trained_Ternarization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Marban_Learning_Sparse__Ternary_Neural_Networks_With_Entropy-Constrained_Trained_Ternarization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep neural networks (DNN) have shown remarkable success in a variety of machine learning applications. The capacity of these models (i.e., number of parameters), endows them with expressive power and allows them to reach the desired performance. In recent years, there is an increasing interest in deploying DNNs to resource-constrained devices (i.e., mobile devices) with limited energy, memory, and computational budget. To address this problem, we propose Entropy-Constrained Trained Ternarization (EC2T), a general framework to create sparse and ternary neural networks which are efficient in terms of storage (e.g., at most two binary-masks and two full-precision values are required to save a weight matrix) and computation (e.g., MAC operations are reduced to a few accumulations plus two multiplications). This approach consists of two steps. First, a super-network is created by scaling the dimensions of a pre-trained model (i.e., its width and depth). Subsequently, this super-network is simultaneously pruned (using an entropy constraint) and quantized (that is, ternary values are assigned layer-wise) in a training process, resulting in a sparse and ternary network representation. We validate the proposed approach in CIFAR-10, CIFAR-100, and ImageNet datasets, showing its effectiveness in image classification tasks.\r"
  },
  "cvpr2020_w40_intelligentscenecachingtoimproveaccuracyforenergy-constrainedembeddedvision": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Intelligent Scene Caching to Improve Accuracy for Energy-Constrained Embedded Vision",
    "authors": [
      "Benjamin Simpson",
      "Ekdeep Lubana",
      "Yuchen Liu",
      "Robert Dick"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Simpson_Intelligent_Scene_Caching_to_Improve_Accuracy_for_Energy-Constrained_Embedded_Vision_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Simpson_Intelligent_Scene_Caching_to_Improve_Accuracy_for_Energy-Constrained_Embedded_Vision_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We describe an efficient method of improving the performance of vision algorithms operating on video streams by reducing the amount of data captured and transferred from image sensors to analysis servers in a data-aware manner. The key concept is to combine guided, highly heterogeneous sampling with an intelligent Scene Cache. This enables the system to adapt to spatial and temporal patterns in the scene, thus reducing redundant data capture and processing. A software prototype of our framework running on a general-purpose embedded processor enables superior object detection accuracy (by 56%) at similar energy consumption (slight improvement of 4%) compared to an H.264 hardware accelerator.\r"
  },
  "cvpr2020_w40_adaptivepositparameterawarenumericalformatfordeeplearninginferenceontheedge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w40",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Efficient Deep Learning for Computer Vision",
    "title": "Adaptive Posit: Parameter Aware Numerical Format for Deep Learning Inference on the Edge",
    "authors": [
      "Hamed F. Langroudi",
      "Vedant Karia",
      "John L. Gustafson",
      "Dhireesha Kudithipudi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w40/Langroudi_Adaptive_Posit_Parameter_Aware_Numerical_Format_for_Deep_Learning_Inference_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w40/Langroudi_Adaptive_Posit_Parameter_Aware_Numerical_Format_for_Deep_Learning_Inference_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Ultra low-precision (<8-bit width) arithmetic is a discernible approach to deploy deep learning networks on to edge devices. Recent findings show that posit with linear quantization has a similar dynamic range as the weight and activation values across the deep neural network layers. This characteristic can benefit the data representation of deep neural networks without impacting the overall accuracy. When capturing the full dynamic range of weights and activations, posit with mixed precision or linear quantization leads to a surge in hardware resource requirements. We propose adaptive posit, which has the ability to capture the non-homogeneous dynamic range of weights and activations across the deep neural network layers. A fine granular control is achieved by embedding the hyperparameters in the numerical format. To evaluate the overall system efficiency, we design a parameterized ASIC softcore for the adaptive posit encoder and decoder. Benchmarking and evaluation of the adaptive posit are performed on three datasets: Fashion-MNIST, CIFAR-10, and ImageNet. Results assert that on average the performance on inference with<8-bitadaptive posits surpasses (2% to 10%) that of posit.\r"
  },
  "cvpr2020_w1_facerecognitiontoobias,ornottoobias?": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Face Recognition: Too Bias, or Not Too Bias?",
    "authors": [
      "Joseph P. Robinson",
      "Gennady Livitz",
      "Yann Henon",
      "Can Qin",
      "Yun Fu",
      "Samson Timoner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Robinson_Face_Recognition_Too_Bias_or_Not_Too_Bias_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Robinson_Face_Recognition_Too_Bias_or_Not_Too_Bias_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We reveal critical insights into problems of bias in state-of-the-art facial recognition (FR) systems using a novel Balanced Faces in the Wild (BFW) dataset: data balanced for gender and ethnic groups. We show variations in the optimal scoring threshold for face-pairs across different subgroups. Thus, the conventional approach of learning a global threshold for all pairs results in performance gaps between subgroups. By learning subgroup-specific thresholds, we reduce performance gaps, and also show a notable boost in overall performance. Furthermore, we do a human evaluation to measure bias in humans, which supports the hypothesis that an analogous bias exists in human perception. For the BFW database, source code, and more, visit https://github.com/visionjo/facerec-bias-bfw.\r"
  },
  "cvpr2020_w1_samthesensitivityofattributionmethodstohyperparameters": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "SAM: The Sensitivity of Attribution Methods to Hyperparameters",
    "authors": [
      "Naman Bansal",
      "Chirag Agarwal",
      "Anh Nguyen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Bansal_SAM_The_Sensitivity_of_Attribution_Methods_to_Hyperparameters_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Bansal_SAM_The_Sensitivity_of_Attribution_Methods_to_Hyperparameters_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Attribution methods can provide powerful insights into the reasons for a classifier's decision. We argue that a key desideratum of an explanation method is its robustness to input hyperparameters which are often randomly set or empirically tuned. High sensitivity to arbitrary hyperparameter choices does not only impede reproducibility but also questions the correctness of an explanation and impairs the trust of end-users. In this paper, we provide a thorough empirical study on the sensitivity of existing attribution methods. We found an alarming trend that many methods are highly sensitive to changes in their common hyperparameters e.g. even changing a random seed can yield a different explanation! Interestingly, such sensitivity is not reflected in the average explanation accuracy scores over the dataset as commonly reported in the literature. In addition, explanations generated for robust classifiers (i.e. which are trained to be invariant to pixel-wise perturbations) are surprisingly more robust than those generated for regular classifiers.\r"
  },
  "cvpr2020_w1_revisitingtheevaluationofuncertaintyestimationanditsapplicationtoexploremodelcomplexity-uncertaintytrade-off": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Revisiting the Evaluation of Uncertainty Estimation and Its Application to Explore Model Complexity-Uncertainty Trade-Off",
    "authors": [
      "Yukun Ding",
      "Jinglan Liu",
      "Jinjun Xiong",
      "Yiyu Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Ding_Revisiting_the_Evaluation_of_Uncertainty_Estimation_and_Its_Application_to_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Ding_Revisiting_the_Evaluation_of_Uncertainty_Estimation_and_Its_Application_to_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Accurately estimating uncertainties in neural network predictions is of great importance in building trusted DNNs-based models, and there is an increasing interest in providing accurate uncertainty estimation on many tasks, such as security cameras and autonomous driving vehicles. In this paper, we focus on the two main use cases of uncertainty estimation, i.e., selective prediction and confidence calibration. We first reveal potential issues of commonly used quality metrics for uncertainty estimation in both use cases, and propose our new metrics to mitigate them. We then apply these new metrics to explore the trade-off between model complexity and uncertainty estimation quality, a critically missing work in the literature. Our empirical experiment results validate the superiority of the proposed metrics, and some interesting trends about the complexity-uncertainty trade-off are observed.\r"
  },
  "cvpr2020_w1_ananalyticalframeworkfortrustedmachinelearningandcomputervisionrunningwithblockchain": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "An Analytical Framework for Trusted Machine Learning and Computer Vision Running With Blockchain",
    "authors": [
      "Tao Wang",
      "Maggie Du",
      "Xinmin Wu",
      "Taiping He"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Wang_An_Analytical_Framework_for_Trusted_Machine_Learning_and_Computer_Vision_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Wang_An_Analytical_Framework_for_Trusted_Machine_Learning_and_Computer_Vision_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Machine learning algorithms often use data from databases that are mutable; therefore, the data and the results of machine learning cannot be fully trusted. Also, the learning process is often difficult to automate. A unified analytical framework for trusted machine learning has been presented in the literature to address both issues. It proposed building a trusted machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts on blockchain are used to automate the machine learning process. However, in such a blockchain framework, data efficiency is a big concern, because it is very expensive to store a large amount of data on blockchain. On the other hand, machine learning-based computer vision systems often rely on a lot of data. Therefore, to fully leverage a blockchain-based machine learning framework for computer vision systems, data efficiency issues must be addressed. This paper investigates how to enhance data efficiency in such a framework to bring computer vision systems to the edge. It presents a three-step approach. First, a lightweight machine learning model is trained on the server layer. Second, the trained model is saved in a special binary data format for data efficiency. Finally, the streaming layer takes these binary data as input and scores incoming new data in an online fashion. Real-time semantic segmentation for autonomous driving is used as an example to demonstrate how this approach works. This paper makes the following contributions. First, it improves the analytical framework for fair and trusted computer vision systems based on blockchain. Second, the real-time semantic segmentation example shows how data-efficient learning for computer vision can be performed on the edge.\r"
  },
  "cvpr2020_w1_identitypreservetransformunderstandwhatactivityclassificationmodelshavelearnt": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Identity Preserve Transform: Understand What Activity Classification Models Have Learnt",
    "authors": [
      "Jialing Lyu",
      "Weichao Qiu",
      "Alan Yuille"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Lyu_Identity_Preserve_Transform_Understand_What_Activity_Classification_Models_Have_Learnt_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Lyu_Identity_Preserve_Transform_Understand_What_Activity_Classification_Models_Have_Learnt_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Activity classification has observed great success recently. The performance on small dataset is almost saturated and people are moving towards larger datasets. What leads to the performance gain on the model and what the model has learnt? In this paper we propose identity preserve transform (IPT) to study this problem. IPT manipulates the nuisance factors (background, viewpoint, etc.) of the data while keeping those factors related to the task (human motion) unchanged. To our surprise, we found popular models are using highly correlated information (background, object) to achieve high classification accuracy, rather than using the essential information (human motion). This can explain why an activity classification model usually fails to generalize to datasets it is not trained on. We implement IPT in two forms, i.e. image-space transform and 3D transform, using synthetic images. The tool will be made open-source to help study model and dataset design.\r"
  },
  "cvpr2020_w1_interpretinginterpretationsorganizingattributionmethodsbycriteria": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Interpreting Interpretations: Organizing Attribution Methods by Criteria",
    "authors": [
      "Zifan Wang",
      "Piotr Mardziel",
      "Anupam Datta",
      "Matt Fredrikson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Wang_Interpreting_Interpretations_Organizing_Attribution_Methods_by_Criteria_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Wang_Interpreting_Interpretations_Organizing_Attribution_Methods_by_Criteria_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Motivated by distinct, though related, criteria, a growing number of attribution methods have been developed to interpret deep learning. While each relies on the interpretability of the concept of \"importance\" and our ability to visualize patterns, explanations produced by the methods often differ. In this work we expand the foundations of human-understandable concepts with which attributions can be interpreted beyond \"importance\" and its visualization; we incorporate the logical concepts of necessity and sufficiency, and the concept of proportionality. We define metrics to represent these concepts as quantitative aspects of an attribution. We evaluate our measures on a collection of methods explaining convolutional neural networks (CNN) for image classification. We conclude that some attribution methods are more appropriate for interpretation in terms of necessity while others are in terms of sufficiency, while no method is always the most appropriate in terms of both.\r"
  },
  "cvpr2020_w1_explainingfailureinvestigationofsurpriseandexpectationincnns": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Explaining Failure: Investigation of Surprise and Expectation in CNNs",
    "authors": [
      "Thomas Hartley",
      "Kirill Sidorov",
      "Christopher Willis",
      "David Marshall"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Hartley_Explaining_Failure_Investigation_of_Surprise_and_Expectation_in_CNNs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Hartley_Explaining_Failure_Investigation_of_Surprise_and_Expectation_in_CNNs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " As Convolutional Neural Networks (CNNs) have expanded into every day use, more rigorous methods of explaining their inner workings are required. Current popular techniques, such as saliency maps, show how a network interprets an input image at a simple level by scoring pixels according to their importance. In this paper, we introduce the concept of surprise and expectation as means for exploring and visualising how a network learns to model the training data through the understanding of filter activations. We show that this is a powerful technique for understanding how the network reacts to an unseen image compared to the training data. We also show that the insights provided by our technique allows us to `fix' misclassifications. Our technique can be used with nearly all types of CNN. We evaluate our method both qualitatively and quantitatively using ImageNet.\r"
  },
  "cvpr2020_w1_enhancingfacialdatadiversitywithstyle-basedfaceaging": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Enhancing Facial Data Diversity With Style-Based Face Aging",
    "authors": [
      "Markos Georgopoulos",
      "James Oldfield",
      "Mihalis A. Nicolaou",
      "Yannis Panagakis",
      "Maja Pantic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Georgopoulos_Enhancing_Facial_Data_Diversity_With_Style-Based_Face_Aging_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Georgopoulos_Enhancing_Facial_Data_Diversity_With_Style-Based_Face_Aging_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " A significant limiting factor in training fair classifiers relates to the presence of dataset bias. In particular, face datasets are typically biased in terms of attributes such as gender, age, and race. If not mitigated, bias leads to algorithms that exhibit unfair behaviour towards such groups. In this work, we address the problem of increasing the diversity of face datasets with respect to age. Concretely, we propose a novel, generative style-based architecture for data augmentation that captures fine-grained aging patterns by conditioning on multi-resolution age-discriminative representations. By evaluating on several age-annotated datasets in both single- and cross-database experiments, we show that the proposed method outperforms state-of-the-art algorithms for age transfer, especially in the case of age groups that lie in the tails of the label distribution. We further show significantly increased diversity in the augmented datasets, outperforming all compared methods according to established metrics.\r"
  },
  "cvpr2020_w1_impartingfairnesstopre-trainedbiasedrepresentations": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Imparting Fairness to Pre-Trained Biased Representations",
    "authors": [
      "Bashir Sadeghi",
      "Vishnu Naresh Boddeti"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Sadeghi_Imparting_Fairness_to_Pre-Trained_Biased_Representations_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Sadeghi_Imparting_Fairness_to_Pre-Trained_Biased_Representations_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the \"linear\"\" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for \"imparting\"\" provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches. Code is available at https://github.com/human-analysis/kernel-adversarial representation-learning.git\r"
  },
  "cvpr2020_w1_exploringracialbiaswithinfacerecognitionviaper-subjectadversarially-enableddataaugmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w1",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Fair, Data-Efficient and Trusted Computer Vision",
    "title": "Exploring Racial Bias Within Face Recognition via Per-Subject Adversarially-Enabled Data Augmentation",
    "authors": [
      "Seyma Yucer",
      "Samet Akcay",
      "Noura Al-Moubayed",
      "Toby P. Breckon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w1/Yucer_Exploring_Racial_Bias_Within_Face_Recognition_via_Per-Subject_Adversarially-Enabled_Data_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w1/Yucer_Exploring_Racial_Bias_Within_Face_Recognition_via_Per-Subject_Adversarially-Enabled_Data_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Whilst face recognition applications are becoming increasingly prevalent within our daily lives, leading approaches in the field still suffer from performance bias to the detriment of some racial profiles within society. In this study, we propose a novel adversarial derived data augmentation methodology that aims to enable dataset balance at a per-subject level via the use of image-to-image transformation for the transfer of sensitive racial characteristic facial features. Our aim is to automatically construct a synthesised dataset by transforming facial images across varying racial domains, while still preserving identity-related features, such that racially dependant features subsequently become irrelevant within the determination of subject identity. We construct our experiments on three significant face recognition variants: Softmax, CosFace and ArcFace loss over a common convolutional neural network backbone. In a side-by-side comparison, we show the positive impact our proposed technique can have on the recognition performance for (racial) minority groups within an originally unbalanced training dataset by reducing the per-race variance in performance.\r"
  },
  "cvpr2020_w3_dynamicattention-basedvisualodometry": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w3",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Long Term Visual Localization, Visual Odometry and Geometric and Learning-Based SLAM",
    "title": "Dynamic Attention-Based Visual Odometry",
    "authors": [
      "Xin-Yu Kuo",
      "Chien Liu",
      "Kai-Chen Lin",
      "Chun-Yi Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w3/Kuo_Dynamic_Attention-Based_Visual_Odometry_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w3/Kuo_Dynamic_Attention-Based_Visual_Odometry_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper proposes a dynamic attention-based visual odometry framework (DAVO), a learning-based VO method, for estimating the ego-motion of a monocular camera. DAVO dynamically adjusts the attention weights on different semantic categories for different motion scenarios based on optical flow maps. These weighted semantic categories can then be used to generate attention maps that highlight the relative importance of different semantic regions in input frames for pose estimation. In order to examine the proposed DAVO, we perform a number of experiments on the KITTI Visual Odometry and SLAM benchmark suite to quantitatively and qualitatively inspect the impacts of the dynamically adjusted weights on the accuracy of the evaluated trajectories. Moreover, we design a set of ablation analyses to justify each of our design choices, and validate the effectiveness as well as the advantages of DAVO. Our experiments on the KITTI dataset shows that the proposed DAVO framework does provide satisfactory performance in ego-motion estimation, and is able deliver competitive performance when compared to the contemporary VO methods.\r"
  },
  "cvpr2020_w3_extendingabsoluteposeregressiontomultiplescenes": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w3",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Long Term Visual Localization, Visual Odometry and Geometric and Learning-Based SLAM",
    "title": "Extending Absolute Pose Regression to Multiple Scenes",
    "authors": [
      "Hunter Blanton",
      "Connor Greenwell",
      "Scott Workman",
      "Nathan Jacobs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w3/Blanton_Extending_Absolute_Pose_Regression_to_Multiple_Scenes_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w3/Blanton_Extending_Absolute_Pose_Regression_to_Multiple_Scenes_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Direct pose regression using deep convolutional neural networks has become a highly active research area. However, even with significant improvements in performance in recent years, the best performance comes from training distinct, scene-specific networks. We propose a novel architecture, Multi-Scene PoseNet (MSPN), that allows for a single network to be used on an arbitrary number of scenes with only a small scene-specific component. Using our approach, we achieve competitive performance for two benchmark 6DOF datasets, Microsoft 7Scenes and Cambridge Landmarks, while reducing the total number of network parameters significantly. Additionally, we demonstrate that our trained model serves as a better initialization for fine-tuning on new scenes compared to the standard ImageNet initialization, converging to lower error solutions within only a few epochs.\r"
  },
  "cvpr2020_w3_reconstruct,rasterizeandbackpropdenseshapeandposeestimationfromasingleimage": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w3",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Long Term Visual Localization, Visual Odometry and Geometric and Learning-Based SLAM",
    "title": "Reconstruct, Rasterize and Backprop: Dense Shape and Pose Estimation From a Single Image",
    "authors": [
      "Aniket Pokale",
      "Aditya Aggarwal",
      "Krishna Murthy Jatavallabhula",
      "Madhava Krishna"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w3/Pokale_Reconstruct_Rasterize_and_Backprop_Dense_Shape_and_Pose_Estimation_From_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w3/Pokale_Reconstruct_Rasterize_and_Backprop_Dense_Shape_and_Pose_Estimation_From_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents a new system to obtain dense object reconstructions along with 6-DoF poses from a single image. Geared towards high fidelity reconstruction, several recent approaches leverage implicit surface representations and deep neural networks to estimate a 3D mesh of an object, given a single image. However, all such approaches recover only the shape of an object; the reconstruction is often in a canonical frame, unsuitable for downstream robotics tasks. To this end, we leverage recent advances in differentiable rendering (in particular, rasterization) to close the loop with 3D reconstruction in camera frame. We demonstrate that our approach---dubbed reconstruct, rasterize and backprop (RRB) achieves significantly lower pose estimation errors compared to prior art, and is able to recover dense object shapes and poses from imagery. We further extend our results to an (offline) setup, where we demonstrate a dense monocular object-centric egomotion estimation system.\r"
  },
  "cvpr2020_w3_viprvisual-odometry-aidedposeregressionfor6dofcameralocalization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w3",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Long Term Visual Localization, Visual Odometry and Geometric and Learning-Based SLAM",
    "title": "ViPR: Visual-Odometry-Aided Pose Regression for 6DoF Camera Localization",
    "authors": [
      "Felix Ott",
      "Tobias Feigl",
      "Christoffer Loffler",
      "Christopher Mutschler"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w3/Ott_ViPR_Visual-Odometry-Aided_Pose_Regression_for_6DoF_Camera_Localization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w3/Ott_ViPR_Visual-Odometry-Aided_Pose_Regression_for_6DoF_Camera_Localization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for long-term 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state of the art in long-term navigation tasks.\r"
  },
  "cvpr2020_w5_multi-viewself-constructinggraphconvolutionalnetworkswithadaptiveclassweightinglossforsemanticsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Multi-View Self-Constructing Graph Convolutional Networks With Adaptive Class Weighting Loss for Semantic Segmentation",
    "authors": [
      "Qinghui Liu",
      "Michael C. Kampffmeyer",
      "Robert Jenssen",
      "Arnt-Borre Salberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Liu_Multi-View_Self-Constructing_Graph_Convolutional_Networks_With_Adaptive_Class_Weighting_Loss_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Liu_Multi-View_Self-Constructing_Graph_Convolutional_Networks_With_Adaptive_Class_Weighting_Loss_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose a novel architecture called the Multi-view Self-Constructing Graph Convolutional Networks (MSCG-Net) for semantic segmentation. Building on the recently proposed Self-Constructing Graph (SCG) module, which makes use of learnable latent variables to self-construct the underlying graphs directly from the input features without relying on manually built prior knowledge graphs, we leverage multiple views in order to explicitly exploit the rotational invariance in airborne images. We further develop an adaptive class weighting loss to address the class imbalance. We demonstrate the effectiveness and flexibility of the proposed method on the Agriculture-Vision challenge dataset and our model achieves very competitive results (0.547 mIoU) with much fewer parameters and at a lower computational cost compared to related pure-CNN based work.\r"
  },
  "cvpr2020_w5_reducingthefeaturedivergenceofrgbandnear-infraredimagesusingswitchablenormalization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Reducing the Feature Divergence of RGB and Near-Infrared Images Using Switchable Normalization",
    "authors": [
      "Siwei Yang",
      "Shaozuo Yu",
      "Bingchen Zhao",
      "Yin Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Yang_Reducing_the_Feature_Divergence_of_RGB_and_Near-Infrared_Images_Using_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Yang_Reducing_the_Feature_Divergence_of_RGB_and_Near-Infrared_Images_Using_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Visual pattern recognition over agricultural areas is an important application of aerial image processing. In this paper, we consider the multi-modality nature of agricultural aerial images and show that naively combining different modalities together without taking the feature divergence into account can lead to sub-optimal results. Thus, we apply a SwitchableNormalization block to ourDeepLabV3+ segmentation model to alleviate the feature divergence. Using the popular symmetric Kullback-Leibler divergence measure, we show that our model can greatly reduce the divergence between RGB and near-infrared channels. Together with a hybrid loss function, our model achieves nearly 10% improvements in mean IoU over previously published baseline.\r"
  },
  "cvpr2020_w5_the1stagriculture-visionchallengemethodsandresults": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "The 1st Agriculture-Vision Challenge: Methods and Results",
    "authors": [
      "Mang Tik Chiu",
      "Xingqian Xu",
      "Kai Wang",
      "Jennifer Hobbs",
      "Naira Hovakimyan",
      "Thomas S. Huang",
      "Honghui Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Chiu_The_1st_Agriculture-Vision_Challenge_Methods_and_Results_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Chiu_The_1st_Agriculture-Vision_Challenge_Methods_and_Results_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The first Agriculture-Vision Challenge aims to encourage research in developing novel and effective algorithms for agricultural pattern recognition from aerial images, especially for the semantic segmentation task associated with our challenge dataset. Around 57 participating teams from various countries compete to achieve state-of-the-art in aerial agriculture semantic segmentation. The Agriculture-Vision Challenge Dataset was employed, which comprises of 21,061 aerial and multi-spectral farmland images. This paper provides a summary of notable methods and results in the challenge. Our submission server and leaderboard will continue to open for researchers that are interested in this challenge dataset and task; the link can be found here.\r"
  },
  "cvpr2020_w5_findingberriessegmentationandcountingofcranberriesusingpointsupervisionandshapepriors": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Finding Berries: Segmentation and Counting of Cranberries Using Point Supervision and Shape Priors",
    "authors": [
      "Peri Akiva",
      "Kristin Dana",
      "Peter Oudemans",
      "Michael Mars"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Akiva_Finding_Berries_Segmentation_and_Counting_of_Cranberries_Using_Point_Supervision_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Akiva_Finding_Berries_Segmentation_and_Counting_of_Cranberries_Using_Point_Supervision_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Precision agriculture has become a key factor for increasing crop yields by providing essential information to decision makers. In this work, we present a deep learning method for simultaneous segmentation and counting of cranberries to aid in yield estimation and sun exposure predictions. Notably, supervision is done using low cost center point annotations. The approach, named Triple-S Network, incorporates a three-part loss with shape priors to promote better fitting to objects of known shape typical in agricultural scenes. Our results improve overall segmentation performance by more than 6.74% and counting results by 22.91% when compared to state-of-the-art. To train and evaluate the network, we have collected the CRanberry Aerial Imagery Dataset (CRAID), the largest dataset of aerial drone imagery from cranberry fields. This dataset will be made publicly available.\r"
  },
  "cvpr2020_w5_leafspotattentionnetworkforappleleafdiseaseidentification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Leaf Spot Attention Network for Apple Leaf Disease Identification",
    "authors": [
      "Hee-Jin Yu",
      "Chang-Hwan Son"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Yu_Leaf_Spot_Attention_Network_for_Apple_Leaf_Disease_Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Yu_Leaf_Spot_Attention_Network_for_Apple_Leaf_Disease_Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Although new deep learning approaches have recently been introduced for leaf disease identification, existing deep learning models such as VGG and ResNet have been used previously. Therefore, a new deep learning architecture is proposed to consider the leaf spot attention mechanism. The primary idea is that leaf disease symptoms appear in the leaf area, whereas the background region does not contain any useful information regarding leaf diseases. To realize this, two subnetworks are designed. The first is a feature segmentation subnetwork to provide more discriminative features for the separated background, leaf areas, and spot areas in the feature map. The other is a spot-aware classification subnetwork to increase the classification accuracy. To train the proposed leaf spot attention network, the feature segmentation subnetwork is first learned with a new image set, where the background, leaf area, and spot area are annotated. Subsequently, the spot-aware classification subnetwork is connected to the feature segmentation subnetwork and then trained through early and later fusions to produce the semantic-level spot feature information. The experimental results confirm that the proposed network can increase the discriminative power by modeling the leaf spot attention mechanism. The results prove that the proposed method outperforms conventional state-of-the-art deep learning models.\r"
  },
  "cvpr2020_w5_visual3dreconstructionanddynamicsimulationoffruittreesforroboticmanipulation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Visual 3D Reconstruction and Dynamic Simulation of Fruit Trees for Robotic Manipulation",
    "authors": [
      "Francisco Yandun",
      "Abhisesh Silwal",
      "George Kantor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Yandun_Visual_3D_Reconstruction_and_Dynamic_Simulation_of_Fruit_Trees_for_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Yandun_Visual_3D_Reconstruction_and_Dynamic_Simulation_of_Fruit_Trees_for_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Modern agriculture is facing a series of challenges to adopt new technologies to improve sustainability, profitability and resilience. One of them is the use of robotic applications to assist or even replace manual workers for the complex task of interaction with the vegetation. For example, harvesting and pruning are tasks that need certain dexterity to not only make the cuts, but also to move branches or foliage in the canopy to reach hidden objects or locations. For such capability, first the robot should be able to perceive the vegetation and estimate the dynamics for the interaction. This work mainly focuses on the perception problem, aiming to digitize commercial tree fruit canopies and estimating how it moves when force is applied to the branches. We studied the suitability of two known algorithms, viz. the space colonization and the Laplace based contraction algorithms, to build a geometric model of the tree using point cloud data from stereo cameras. Such model is then used to estimate the dynamics of the tree, by considering the branches as links articulated by spring-damper joints. The geometric model was evaluated for topological and morphological correctness by comparing it with the ground truth, obtaining better results with the Laplace based contraction algorithm. Furthermore, results of the dynamics estimation showed that by adjusting the parameters for the spring-damper model, the motion prediction is promising, with a maximum mean squared error of 0.073m in the tracking of the movement of the branches.\r"
  },
  "cvpr2020_w5_cross-regionaloilpalmtreedetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Cross-Regional Oil Palm Tree Detection",
    "authors": [
      "Wenzhao Wu",
      "Juepeng Zheng",
      "Haohuan Fu",
      "Weijia Li",
      "Le Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Wu_Cross-Regional_Oil_Palm_Tree_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Wu_Cross-Regional_Oil_Palm_Tree_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " As oil palm has become one of the most rapidly expanding tropical crops in the world, detecting and counting oil palms have received considerable attention. Although deep learning has been widely applied to remote sensing image processing including tree crown detection, the large size and the variety of the data make it extremely difficult for cross-regional and large-scale scenarios. In this paper, we propose a cross-regional oil palm tree detection (CROPTD) method. CROPTD contains a local domain discriminator and a global domain discriminator, both of which are generated by adversarial learning. Additionally, since the local alignment does not take full advantages of its transferability information, we improve the local module with the local attention mechanism, taking more attention on more transferable regions. We evaluate our CROPTD on two large-scale high-resolution satellite images located in Peninsular Malaysia. CROPTD improves the detection accuracy by 8.69% in terms of average F1-score compared with the Baseline method (Faster R-CNN) and performs 4.99-2.21% better than other two state-of-the-art domain adaptive object detection approaches. Experimental results demonstrate the great potential of our CROPTD for large-scale, cross-regional oil palm tree detection, guaranteeing a high detection accuracy as well as saving the manual annotation efforts. Our training and validation dataset are available on https://github.com/rs-dl/CROPTD.\r"
  },
  "cvpr2020_w5_multi-streamcnnforspatialresourceallocationacropmanagementapplication": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Multi-Stream CNN for Spatial Resource Allocation: A Crop Management Application",
    "authors": [
      "Alexandre Barbosa",
      "Thiago Marinho",
      "Nicolas Martin",
      "Naira Hovakimyan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Barbosa_Multi-Stream_CNN_for_Spatial_Resource_Allocation_A_Crop_Management_Application_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Barbosa_Multi-Stream_CNN_for_Spatial_Resource_Allocation_A_Crop_Management_Application_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Modeling the spatial structure of crop inputs is of great importance for accurate yield prediction. It is a fundamental step towards optimizing the spatial allocation of resources such as seed and fertilizer. We propose two distinct architectures of Multi-Stream Convolutional Neural Network (MSCNN) - Late Fusion (LF) and Early Fusion (EF) - to model yield response to seed and nutrient management. A study presents a comparison between proposed models with conventional 2D and 3D CNN architectures, and existing agronomy methods. The dataset used to train and test the models is constructed using on-farm experiment data from nine cornfields across the US together with multispectral satellite images. Results show that the MSCNN-LF achieved a 20% reduction of the prediction's RMSE value when compared to a 3D CNN, and a 26% reduction when compared to a 2D CNN. An optimization algorithm uses the MSCNN-LF model's gradient to change the manageable inputs variables in a way the expected profit is maximized subject to resource constraints. It is shown that an increase of up to 5.2% on expected crop yield return is obtained when compared to usual management practices.\r"
  },
  "cvpr2020_w5_effectivedatafusionwithgeneralizedvegetationindexevidencefromlandcoversegmentationinagriculture": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Effective Data Fusion With Generalized Vegetation Index: Evidence From Land Cover Segmentation in Agriculture",
    "authors": [
      "Hao Sheng",
      "Xiao Chen",
      "Jingyi Su",
      "Ram Rajagopal",
      "Andrew Ng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Sheng_Effective_Data_Fusion_With_Generalized_Vegetation_Index_Evidence_From_Land_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Sheng_Effective_Data_Fusion_With_Generalized_Vegetation_Index_Evidence_From_Land_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " How can we effectively leverage the domain knowledge from remote sensing to better segment agriculture land cover from satellite images? In this paper, we propose a novel, model-agnostic, data-fusion approach for vegetation-related computer vision tasks. Motivated by the various Vegetation Indices (VIs), which are introduced by domain experts, we systematically reviewed the VIs that are widely used in remote sensing and their feasibility to be incorporated in deep neural networks. To fully leverage the Near-Infrared channel, the traditional Red-Green-Blue channels, and Vegetation Index or its variants, we propose a Generalized Vegetation Index (GVI), a lightweight module that can be easily plugged into many neural network architectures to serve as an additional information input. To smoothly train models with our GVI, we developed an Additive Group Normalization (AGN) module that does not require extra parameters of the prescribed neural networks. Our approach has improved the IoUs of vegetation-related classes by 0.9-1.3 percent and consistently improves the overall mIoU by 2 percent on our baseline.\r"
  },
  "cvpr2020_w5_deeptransferlearningforplantcenterlocalization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Deep Transfer Learning for Plant Center Localization",
    "authors": [
      "Enyu Cai",
      "Sriram Baireddy",
      "Changye Yang",
      "Melba Crawford",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Cai_Deep_Transfer_Learning_for_Plant_Center_Localization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Cai_Deep_Transfer_Learning_for_Plant_Center_Localization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Plant phenotyping focuses on the measurement of plant characteristics throughout the growing season, typically with the goal of evaluating genotypes for plant breeding. Estimating plant location is important for identifying genotypes which have low emergence, which is also related to the environment and management practices such as fertilizer applications. The goal of this paper is to investigate methods that estimate plant locations for a field-based crop using RGB aerial images captured using Unmanned Aerial Vehicles (UAVs). Deep learning approaches provide promising capability for locating plants observed in RGB images, but they require large quantities of labeled data (ground truth) for training. Using a deep learning architecture fine-tuned on a single field or a single type of crop on fields in other geographic areas or with other crops may not have good results. The problem of generating ground truth for each new field is labor-intensive and tedious. In this paper, we propose a method for estimating plant centers by transferring an existing model to a new scenario using limited ground truth data. We describe the use of transfer learning using a model fine-tuned for a single field or a single type of plant on a varied set of similar crops and fields. We show that transfer learning provides promising results for detecting plant locations.\r"
  },
  "cvpr2020_w5_segmentationanddetectionfromorganised3dpointcloudsacasestudyinbroccoliheaddetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Segmentation and Detection From Organised 3D Point Clouds: A Case Study in Broccoli Head Detection",
    "authors": [
      "Justin Le Louedec",
      "Hector A. Montes",
      "Tom Duckett",
      "Grzegorz Cielniak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Le_Louedec_Segmentation_and_Detection_From_Organised_3D_Point_Clouds_A_Case_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Le_Louedec_Segmentation_and_Detection_From_Organised_3D_Point_Clouds_A_Case_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Autonomous harvesting is becoming an important challenge and necessity in agriculture, because of the lack of labour and the growth of population needing to be fed. Perception is a key aspect of autonomous harvesting and is very challenging due to difficult lighting conditions, limited sensing technologies, occlusions, plant growth, etc. 3D vision approaches can bring several benefits addressing the aforementioned challenges such as localisation, size estimation, occlusion handling and shape analysis. In this paper, we propose a novel approach using 3D information for detecting broccoli heads based on Convolutional Neural Networks (CNNs), exploiting the organised nature of the point clouds originating from the RGBD sensors. The proposed algorithm, tested on real-world datasets, achieves better performances than the state-of-the-art, with better accuracy and generalisation in unseen scenarios, whilst significantly reducing inference time, making it better suited for real-time in-field applications.\r"
  },
  "cvpr2020_w5_deeplearningbasedcornkernelclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Deep Learning Based Corn Kernel Classification",
    "authors": [
      "Henry O. Velesaca",
      "Raul Mira",
      "Patricia L. Suarez",
      "Christian X. Larrea",
      "Angel D. Sappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Velesaca_Deep_Learning_Based_Corn_Kernel_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Velesaca_Deep_Learning_Based_Corn_Kernel_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents a full pipeline to classify sample sets of corn kernels. The proposed approach follows a segmentation-classification scheme. The image segmentation is performed through a well known deep learning-based approach, the Mask R-CNN architecture, while the classification is performed through a novel-lightweight network specially designed for this task---good corn kernel, defective corn kernel and impurity categories are considered. As a second contribution, a carefully annotated multi-touching corn kernel dataset has been generated. This dataset has been used for training the segmentation and the classification modules. Quantitative evaluations have been performed and comparisons with other approaches are provided showing improvements with the proposed pipeline.\r"
  },
  "cvpr2020_w5_improvingin-fieldcassavawhiteflypestsurveillancewithmachinelearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Improving In-Field Cassava Whitefly Pest Surveillance With Machine Learning",
    "authors": [
      "Jeremy Francis Tusubira",
      "Solomon Nsumba",
      "Flavia Ninsiima",
      "Benjamin Akera",
      "Guy Acellam",
      "Joyce Nakatumba",
      "Ernest Mwebaze",
      "John Quinn",
      "Tonny Oyana"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Tusubira_Improving_In-Field_Cassava_Whitefly_Pest_Surveillance_With_Machine_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Tusubira_Improving_In-Field_Cassava_Whitefly_Pest_Surveillance_With_Machine_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Whiteflies are the major vector responsible for the transmission of cassava related diseases in tropical environments, and knowing the numbers of whiteflies is key in detecting and identifying their spread and prevention. However, the current approach for counting whiteflies is a simple visual inspection, where a cassava leaf is turned upside down to reveal the underside where the whiteflies reside to enable a manual count. Repeated across many cassava farms, this task is quite tedious and time-consuming. In this paper, we propose a method to automatically count whiteflies using computer vision techniques. To implement this approach, we collected images of infested cassava leaves and trained a computer vision detector using Haar Cascade and DeepLearning techniques. The two techniques were used to identify the pest in images and return a count. Our results show that this novel method produces a whitefly count with high precision. This method could be applied to similar object detection scenarios similar to the whitefly problem with minor adjustments.\r"
  },
  "cvpr2020_w5_weaklysupervisedlearningguidedbyactivationmappingappliedtoanovelcitruspestbenchmark": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Weakly Supervised Learning Guided by Activation Mapping Applied to a Novel Citrus Pest Benchmark",
    "authors": [
      "Edson Bollis",
      "Helio Pedrini",
      "Sandra Avila"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Bollis_Weakly_Supervised_Learning_Guided_by_Activation_Mapping_Applied_to_a_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Bollis_Weakly_Supervised_Learning_Guided_by_Activation_Mapping_Applied_to_a_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Pests and diseases are relevant factors for production losses in agriculture and, therefore, promote a huge investment in the prevention and detection of its causative agents. In many countries, Integrated Pest Management is the most widely used process to prevent and mitigate the damages caused by pests and diseases in citrus crops. However, its results are credited by humans who visually inspect the orchards in order to identify the disease symptoms, insects and mite pests. In this context, we design a weakly supervised learning process guided by saliency maps to automatically select regions of interest in the images, significantly reducing the annotation task. In addition, we create a large citrus pest benchmark composed of positive samples (six classes of mite species) and negative samples. Experiments conducted on two large datasets demonstrate that our results are very promising for the problem of pest and disease classification in the agriculture field.\r"
  },
  "cvpr2020_w5_fine-grainedrecognitioninhigh-throughputphenotyping": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Fine-Grained Recognition in High-Throughput Phenotyping",
    "authors": [
      "Beichen Lyu",
      "Stuart D. Smith",
      "Keith A. Cherkauer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Lyu_Fine-Grained_Recognition_in_High-Throughput_Phenotyping_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Lyu_Fine-Grained_Recognition_in_High-Throughput_Phenotyping_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Fine-Grained Recognition aims to classify sub-category objects such as bird species and car models from imagery. In High-throughput Phenotyping, the required task is to classify individual plant cultivars to assist plant breeding, which has posed three challenges: 1) it is easy to overfit complex features and models, 2) visual conditions change during and between image collection opportunities, and 3) analysis of thousands of cultivars require high-throughput data collection and analysis. To tackle these challenges, we propose a simple but intuitive descriptor, Radial Object Descriptor, to represent plant cultivar objects based on contour. This descriptor is invariant under scaling, rotation, and translation, as well as robust under changes to the plant's growth stage and camera's view angle. Furthermore, we complement this mid-level feature by fusing it with the low-level features (Histogram of Oriented Gradients) and deep features (ResNet-18), respectively. We extensively test our fusion approaches using two real world experiments. One experiment is on a novel benchmark dataset (HTP-Soy) in which we collect 2,000 high-resolution aerial images of outdoor soybean plots. Another experiment is on three datasets of indoor rosette plants. For both experiments, our fusion approaches achieve superior accuracies while maintaining better generalization as compared with traditional approaches.\r"
  },
  "cvpr2020_w5_anoveltechniquecombiningimageprocessing,plantdevelopmentproperties,andthehungarianalgorithm,toimproveleafdetectioninmaize": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "A Novel Technique Combining Image Processing, Plant Development Properties, and the Hungarian Algorithm, to Improve Leaf Detection in Maize",
    "authors": [
      "Nazifa Azam Khan",
      "Oliver A.S. Lyon",
      "Mark Eramian",
      "Ian McQuillan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Khan_A_Novel_Technique_Combining_Image_Processing_Plant_Development_Properties_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Khan_A_Novel_Technique_Combining_Image_Processing_Plant_Development_Properties_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Manual determination of plant phenotypic properties such as plant architecture, growth, and health is very time consuming and sometimes destructive. Automatic image analysis has become a popular approach. This research aims to identify the position (and number) of leaves from a temporal sequence of high-quality indoor images consisting of multiple views, focussing in particular of images of maize. The procedure used a segmentation on the images, using the convex hull to pick the best view at each time step, followed by a skeletonization of the corresponding image. To remove skeleton spurs, a discrete skeleton evolution pruning process was applied. Pre-existing statistics regarding maize development was incorporated to help differentiate between true leaves and false leaves. Furthermore, for each time step, leaves were matched to those of the previous and next three days using the graph-theoretic Hungarian algorithm. This matching algorithm can be used to both remove false positives, and also to predict true leaves, even if they were completely occluded from the image itself. The algorithm was evaluated using an open dataset consisting of 13 maize plants across 27 days from two different views. The total number of true leaves from the dataset was 1843, and our proposed techniques detect a total of 1690 leaves including 1674 true leaves, and only 16 false leaves, giving a recall of 90.8%, and a precision of 99.0%.\r"
  },
  "cvpr2020_w5_farmparceldelineationusingspatio-temporalconvolutionalnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Farm Parcel Delineation Using Spatio-Temporal Convolutional Networks",
    "authors": [
      "Han Lin Aung",
      "Burak Uzkent",
      "Marshall Burke",
      "David Lobell",
      "Stefano Ermon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Aung_Farm_Parcel_Delineation_Using_Spatio-Temporal_Convolutional_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Aung_Farm_Parcel_Delineation_Using_Spatio-Temporal_Convolutional_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Farm parcel delineation (delineation of boundaries of farmland parcels/segmentation of farmland areas) provides cadastral data that is important in developing and managing climate change policies. Specifically, farm parcel delineation informs applications in downstream governmental policies of land allocation, irrigation, fertilization, greenhouse gases (GHG's), etc. This data can also be useful for the agricultural insurance sector for assessing compensations following damages associated with extreme weather events - a growing trend related to climate change. Using satellite imaging can be a scalable and cost-effective manner to perform the task of farm parcel delineation to collect this valuable data. In this paper, we break down this task using satellite imaging into two approaches: 1) Segmentation of parcel boundaries, and 2) Segmentation of parcel areas. We implemented variations of U-Nets, one of which takes into account temporal information, which achieved the best results on our dataset on farm parcels in France in 2017.\r"
  },
  "cvpr2020_w5_climateadaptationreliablypredictingfromimbalancedsatellitedata": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w5",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Agriculture-Vision: Challenges & Opportunities for Computer Vision in Agriculture",
    "title": "Climate Adaptation: Reliably Predicting From Imbalanced Satellite Data",
    "authors": [
      "Ruchit Rawal",
      "Prabhu Pradhan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w5/Rawal_Climate_Adaptation_Reliably_Predicting_From_Imbalanced_Satellite_Data_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w5/Rawal_Climate_Adaptation_Reliably_Predicting_From_Imbalanced_Satellite_Data_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The utility of aerial imagery (Satellite, Drones) has become an invaluable information source for cross-disciplinary applications, especially for crisis management. Most of the mapping and tracking efforts are manual which is resource-intensive and often lead to delivery delays. Deep Learning methods have boosted the capacity of relief efforts via recognition, detection, and are now being used for non-trivial applications. However the data commonly available is highly imbalanced (similar to other real-life applications) which severely hampers the neural network's capabilities, this reduces robustness and trust. We give an overview on different kinds of techniques being used for handling such extreme settings and present solutions aimed at maximizing performance on minority classes using a diverse set of methods (ranging from architectural tuning to augmentation) which as a combination generalizes for all minority classes. We hope to amplify cross-disciplinary efforts by enhancing model reliability.\r"
  },
  "cvpr2020_w7_improveimagecodecsperformancebyvariatingpostenhancingneuralnetworksubmissionofzxwforclic2020": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Improve Image Codec's Performance by Variating Post Enhancing Neural Network: Submission of zxw for CLIC2020",
    "authors": [
      "Ming Li",
      "Yundong Zhang",
      "Changsheng Xia",
      "Jinwen Zan",
      "Zhangming Huang",
      "Dekai Chen",
      "Guoxin Li",
      "Jing Nie"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Li_Improve_Image_Codecs_Performance_by_Variating_Post_Enhancing_Neural_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Li_Improve_Image_Codecs_Performance_by_Variating_Post_Enhancing_Neural_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Adding post enhancing filter after traditional image decoder to improve reconstruction quality is nowadays a very common method [??],[??],[??],[??]. Researchers use a large network filter or repeatedly stack single/multiple types of relative simple filters together. They all achieved better results. On the other hand the training materials and training time increases exponentially for a larger network scale, and the performance improvement becomes less and little. We learn this experience from the CLIC2019 low-rate track, where we proposed the VimicroABCnet and VimicroSpeed[??], with 2 post filters of different scale. The later one(5 time larger than the small one) achieved the final test's PSNR by improvement of only 0.02db@0.15bpp. In this paper, we propose a method to variate an existing post network filter(base filter). The base filter is altered into different ones, alternation only happens to weights. The key of the method is to divide the training data into different groups. Based on the pre-trained base filter, different altered filters are individually fine-trained with different group of training data. There are different ways to divide the training data, and we use a relative simple one. Sort by compression rate(with traditional codec) and bin the training images in to 4/8 group of training data subsets. With the new filters plus the base one, we now have 5/9 filters candidates in encoding phase and choose the best. The CLIC2019 test data show that PSNR increases 0.04db@0.15bpp and 0.06db@0.15bpp than the one filter VimicroSpeed method. This method requires the same training data and perfectly suitable for multi-GPU training scheme, and retraining the altered filters is much easier and consuming less time than training a relative large network filter. Also the result is better(5 filters scheme@0.04db vs VimicroABCnet@0.02db)\r"
  },
  "cvpr2020_w7_3-dcontextentropymodelforimprovedpracticalimagecompression": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "3-D Context Entropy Model for Improved Practical Image Compression",
    "authors": [
      "Zongyu Guo",
      "Yaojun Wu",
      "Runsen Feng",
      "Zhizheng Zhang",
      "Zhibo Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Guo_3-D_Context_Entropy_Model_for_Improved_Practical_Image_Compression_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Guo_3-D_Context_Entropy_Model_for_Improved_Practical_Image_Compression_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we present our image compression framework designed for CLIC 2020 competition. Our method is based on Variational AutoEncoder (VAE) architecture which is strengthened with residual structures. In short, we make three noteworthy improvements here. First, we propose a 3-D context entropy model which can take advantage of known latent representation in current spatial locations for better entropy estimation. Second, a light-weighted residual structure is adopted for feature learning during entropy estimation. Finally, an effective training strategy is introduced for practical adaptation with different resolutions. Experiment results indicate our image compression method achieves 0.9775 MS-SSIM on CLIC validation set and 0.9809 MS-SSIM on test set.\r"
  },
  "cvpr2020_w7_ultralowbitratelearnedimagecompressionbyselectivedetaildecoding": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Ultra Low Bitrate Learned Image Compression by Selective Detail Decoding",
    "authors": [
      "Hiroaki Akutsu",
      "Akifumi Suzuki",
      "Zhisheng Zhong",
      "Kiyoharu Aizawa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Akutsu_Ultra_Low_Bitrate_Learned_Image_Compression_by_Selective_Detail_Decoding_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Akutsu_Ultra_Low_Bitrate_Learned_Image_Compression_by_Selective_Detail_Decoding_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Neural network-based learned image compression has a special feature in that a differentiable image quality index can be used as a loss function directly, and a decoder and an encoder can be optimized by the quality index through end-to-end learning. From a perceptual view, we hypothesized that there were detailed important parts in pictures. For those parts, we applied an additional decoder and weighted loss function to achieve both low bitrate image compression and perceptual quality. Furthermore, our approach can automatically determine which region an additional decoder will take for an input image. Experiments visually showed that the proposed method can recognize important parts, such as text and faces, and we show that our method can decode images more clearly than the simple MS-SSIM training model.\r"
  },
  "cvpr2020_w7_learnedvideocompressionwithfeature-levelresiduals": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Learned Video Compression With Feature-Level Residuals",
    "authors": [
      "Runsen Feng",
      "Yaojun Wu",
      "Zongyu Guo",
      "Zhizheng Zhang",
      "Zhibo Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Feng_Learned_Video_Compression_With_Feature-Level_Residuals_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Feng_Learned_Video_Compression_With_Feature-Level_Residuals_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we present an end-to-end video compression network for P-frame challenge on CLIC. We focus on deep neural network (DNN) based video compression, and improve the current frameworks from three aspects. First, we notice that pixel space residuals is sensitive to the prediction errors of optical flow based motion compensation. To suppress the relative influence, we propose to compress the residuals of image feature rather than the residuals of image pixels. Furthermore, we combine the advantages of both pixel-level and feature-level residual compression methods by model ensembling. Finally, we propose a step-by-step training strategy to improve the training efficiency of the whole framework. Experiment results on the CLIC validation dataset show that the proposed method achieves 0.9968 MS-SSIM score.\r"
  },
  "cvpr2020_w7_variablerateimagecompressionwithcontentadaptiveoptimization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Variable Rate Image Compression With Content Adaptive Optimization",
    "authors": [
      "Tiansheng Guo",
      "Jing Wang",
      "Ze Cui",
      "Yihui Feng",
      "Yunying Ge",
      "Bo Bai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Guo_Variable_Rate_Image_Compression_With_Content_Adaptive_Optimization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Guo_Variable_Rate_Image_Compression_With_Content_Adaptive_Optimization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a variable rate image compression framework for low bit-rate image compression task. Unlike most of the variational auto-encoder (VAE) based methods, our proposal is able to achieve continuously variable rate in a single model by introducing a pair of gain units into VAE. Besides, a content adaptive optimization is applied to adapt the latent representation to the specific content while keeping the parameters of the network and the predictive model fixed. After that, due to the variable rate characteristics of our method, each image can be compressed into any quality level through a unified codec. Finally, an efficient rate control algorithm is designed to find the optimal bit allocation scheme under the constraint of the low rate challenge.\r"
  },
  "cvpr2020_w7_sr-cl-dmcp-framecodingwithsuper-resolution,colorlearning,anddeepmotioncompensation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "SR-CL-DMC: P-Frame Coding With Super-Resolution, Color Learning, and Deep Motion Compensation",
    "authors": [
      "Man M. Ho",
      "Jinjia Zhou",
      "Gang He",
      "Muchen Li",
      "Lei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Ho_SR-CL-DMC_P-Frame_Coding_With_Super-Resolution_Color_Learning_and_Deep_Motion_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Ho_SR-CL-DMC_P-Frame_Coding_With_Super-Resolution_Color_Learning_and_Deep_Motion_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper proposes a deep learning based video coding framework to greatly increase the compression ratio and keep the video quality by efficiently leveraging the information from a reference. In the encoder, the input frame is compressed by down-sampling to a lower resolution, eliminating color information, and then encoding residual between the current frame and the reference frame using Versatile Video Coding (VVC). The decoder consists of two main parts: Super-Resolution with Color Learning (SR-CL), and Deep Motion Compensation (DMC). For the SR-CL part, we adopt Restoration-Reconstruction Deep Neural Network to firstly restore the missing information from compression at low resolution and compression without color. And then, the sampling degradation at high-resolution is compensated. For the DMC part, we adopt recursive-feedback architectures to propose an optical flow estimation and refinement using Dilated Inception Blocks. As a result, the work achieves 64:1 compression ratio with 41.81/41.34 dB PSNR and 0.9959/0.9962 MS-SSIM on the validation/test set provided by the CLIC P-frame track challenge.\r"
  },
  "cvpr2020_w7_lowbitrateimagecompressionwithdiscretizedgaussianmixturelikelihoods": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Low Bitrate Image Compression With Discretized Gaussian Mixture Likelihoods",
    "authors": [
      "Zhengxue Cheng",
      "Heming Sun",
      "Jiro Katto"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Cheng_Low_Bitrate_Image_Compression_With_Discretized_Gaussian_Mixture_Likelihoods_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Cheng_Low_Bitrate_Image_Compression_With_Discretized_Gaussian_Mixture_Likelihoods_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we provide a detailed description on our submitted method Kattolab to Workshop and Challenge on Learned Image Compression (CLIC) 2020. Our method mainly incorporates discretized Gaussian Mixture Likelihoods to previous state-of-the-art learned compression algorithms. Besides, we also describes the acceleration strategies and bit optimization with the rate constraint. Experimental results have demonstrated that our approach Kattolab achieves 0.9761 in terms of MS-SSIM at the rate constraint of 0.15 bpp during the validation phase.\r"
  },
  "cvpr2020_w7_post-processingnetworkbasedondenseinceptionattentionforvideocompression": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Post-Processing Network Based on Dense Inception Attention for Video Compression",
    "authors": [
      "Hao Tao",
      "Jian Qian",
      "Li Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Tao_Post-Processing_Network_Based_on_Dense_Inception_Attention_for_Video_Compression_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Tao_Post-Processing_Network_Based_on_Dense_Inception_Attention_for_Video_Compression_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Traditional video coding standards, such as HEVC and VVC, have achieved significant compression performance. To further improve the coding efficiency, a post-processing network is proposed to enhance the compressed frames in this paper. Specifically, the proposed network, namely DIA Net, contains multiple inception blocks, attention mechanism and dense residual structure. The DIA Net can efficiently extract information of multiple scale and fully exploit the extracted feature to improve image quality. In addition, the DIA Net is integrated into the latest test model of VVC (VTM-8.0) to post-process the reconstructed frames of the decoder for better compression performance. The proposed scheme has achieved the best performance in the sense of PSNR at the similar bitrate in the validation sets of challenge on learned image compression (CLIC), which demonstrates the superiority of our approach.\r"
  },
  "cvpr2020_w7_efficientcontext-awarelossyimagecompression": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Efficient Context-Aware Lossy Image Compression",
    "authors": [
      "Jan Xu",
      "Alexander Lytchier",
      "Ciro Cursio",
      "Dimitrios Kollias",
      "Christian Besenbruch",
      "Arsalan Zafar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Xu_Efficient_Context-Aware_Lossy_Image_Compression_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Xu_Efficient_Context-Aware_Lossy_Image_Compression_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present an efficient context-aware lossy image compression system to participate in the Low Rate track of the CLIC 2020 Image Compression challenge. Our method is based on an auto-encoder pipeline augmented with a nested hyperprior model, a PixelCNN-based context model and an adversarial loss to remove artefacts.\r"
  },
  "cvpr2020_w7_compressionartifactremovalwithensemblelearningofneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Compression Artifact Removal With Ensemble Learning of Neural Networks",
    "authors": [
      "Yueyu Hu",
      "Haichuan Ma",
      "Dong Liu",
      "Jiaying Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Hu_Compression_Artifact_Removal_With_Ensemble_Learning_of_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Hu_Compression_Artifact_Removal_With_Ensemble_Learning_of_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose to improve the reconstruction quality of DLVC intra coding based on an ensemble of deep restoration neural networks. Different ways are proposed to generate diversity models, and based on these models, the behavior of different integration methods for model ensemble is explored. The experimental results show that model ensemble can bring additional performance gains to post-processing on the basis that deep neural networks have shown great performance improvements. Besides, we observe that both averaging and selection approaches for model ensemble can bring performance gains, and they can be used in combination to pursue better results.\r"
  },
  "cvpr2020_w7_jointlearnedandtraditionalvideocompressionforpframe": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Joint Learned and Traditional Video Compression for P Frame",
    "authors": [
      "Zhao Wang",
      "Ru-Ling Liao",
      "Yan Ye"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Wang_Joint_Learned_and_Traditional_Video_Compression_for_P_Frame_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Wang_Joint_Learned_and_Traditional_Video_Compression_for_P_Frame_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a joint learned and traditional video compression framework for the P frame track on learned image compression hosted at CVPR2020. The main difference between video compression and image compression is that the former has high degree of similarity between the successive frames which can be utilized to reduce the temporal redundancy. Therefore, we first introduce a decoder-side template-based inter prediction method as an efficient way to obtain reference blocks without the need to signal the motion vectors. Secondly, a CNN post filter is proposed to suppress visual artifacts and improve the decoded image quality. Specifically, the spatial and temporal information is jointly exploited by taking both the current block and similar block in reference frame into consideration. Furthermore, an advanced SSIM based rate-distortion optimization model is proposed to achieve best balance between the coding bits and the decoded image quality. Experimental results show that the proposed P frame compression scheme achieves higher reconstruction quality in terms of both PSNR and MS-SSIM.\r"
  },
  "cvpr2020_w7_towardstheperceptualqualityenhancementoflowbit-ratecompressedimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Towards the Perceptual Quality Enhancement of Low Bit-Rate Compressed Images",
    "authors": [
      "Younhee Kim",
      "Seunghyun Cho",
      "Jooyoung Lee",
      "Se-Yoon Jeong",
      "Jin Soo Choi",
      "Jihoon Do"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Kim_Towards_the_Perceptual_Quality_Enhancement_of_Low_Bit-Rate_Compressed_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Kim_Towards_the_Perceptual_Quality_Enhancement_of_Low_Bit-Rate_Compressed_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, a low bit-rate compressed image quality enhancement framework is presented. A recent image/video coding method and a deep learning based quality enhancement method are integrated to improve the perceptual quality of compressed images. The proposed architecture is designed to reduce the coding artifact and restore the blurred texture details. To show that the reconstructed images has enhanced visual quality, we have used the objective quality metric. The experimental results presents that the proposed framework shows significant improvement in the human visual quality and a 33% improvement in the objective evaluation criterion of the perceptual quality.\r"
  },
  "cvpr2020_w7_ahybridimagecodecwithlearnedresidualcoding": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "A Hybrid Image Codec With Learned Residual Coding",
    "authors": [
      "Wei-Cheng Lee",
      "Hsueh-Ming Hang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Lee_A_Hybrid_Image_Codec_With_Learned_Residual_Coding_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Lee_A_Hybrid_Image_Codec_With_Learned_Residual_Coding_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose a three-layer image compression system consisting of a base-layer VVC (intra) codec, a learning-based residual layer codec, and a learnable hyperprior. This proposal is submitted to the Challenge on Learned Image Compression (CLIC) in March 2020. Our contribution is developing a data fusion attention module and integrating several known components together to form an efficient image codec, which has a higher compression performance than the standard VVC coding scheme. Unlike the conventional residual image coding, both our encoder and decoder take inputs also from the base-layer output. Also, we construct a refinement neural network to merge the residual-layer decoded residual image and the base-layer decoded image together to form the final reconstructed image. We tested two autoencoder structures for the encoder and decoder, namely, CNN with GDN block , and the generalized octave CNN. Our results show that the transmitted latent representations are very efficient in coding the residuals because the object boundary information can be provided by the proposed spatial attention module. The experiments indicate that the proposed system achieves better performance than the single-layer VVC at both PSNR and subjective quality at around 0.15 bit-per-pixel.\r"
  },
  "cvpr2020_w7_learnedlowbit-rateimagecompressionwithadversarialmechanism": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Learned Low Bit-Rate Image Compression With Adversarial Mechanism",
    "authors": [
      "Jiayu Yang",
      "Chunhui Yang",
      "Yi Ma",
      "Shiyi Liu",
      "Ronggang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Yang_Learned_Low_Bit-Rate_Image_Compression_With_Adversarial_Mechanism_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Yang_Learned_Low_Bit-Rate_Image_Compression_With_Adversarial_Mechanism_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Adversarial mechanism is introduced to learned image compression system in this paper. Our motivation is that the number of quantization levels is limited with the constraint of low bit-rate, resulting in severe distortion in details after reconstruction. The adversarial training manner enhances the ability of Decoder/Generator to enrich textures and details in the reconstructed image. Channel-spatial attention mechanism is used to refine the intermediate features implicitly to boost the representation power of CNNs. As for entropy model, we jointly take hyperpriors and autoregressive priors for accurate probability estimation. Moreover, an EDSR-like post-processing subnetwork is concatenated after Decoder for further quality enhancement. The proposed approach demonstrates competitive performance when evaluated with multi-scale structural similarity (MSSSIM) and favorably visual quality at low bit-rate.\r"
  },
  "cvpr2020_w7_end-to-endlearningforvideoframecompressionwithself-attention": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "End-to-End Learning for Video Frame Compression With Self-Attention",
    "authors": [
      "Nannan Zou",
      "Honglei Zhang",
      "Francesco Cricri",
      "Hamed R. Tavakoli",
      "Jani Lainema",
      "Emre Aksu",
      "Miska Hannuksela",
      "Esa Rahtu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Zou_End-to-End_Learning_for_Video_Frame_Compression_With_Self-Attention_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Zou_End-to-End_Learning_for_Video_Frame_Compression_With_Self-Attention_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " One of the core components of conventional (i.e., non-learned) video codecs consists of predicting a frame from a previously-decoded frame, by leveraging temporal correlations. In this paper, we propose an end-to-end learned system for compressing video frames. Instead of relying on pixel-space motion (as with optical flow), our system learns deep embeddings of frames and encodes their difference in latent space. At decoder-side, an attention mechanism is designed to attend to the latent space of frames to decide how different parts of the previous and current frame are combined to form the final predicted current frame. Spatially-varying channel allocation is achieved by using importance masks acting on the feature-channels. The model is trained to reduce the bitrate by minimizing a loss on importance maps and a loss on the probability output by a context model for arithmetic coding. In our experiments, we show that the proposed system achieves high compression rates and high objective visual quality as measured by MS-SSIM and PSNR. Furthermore, we provide ablation studies where we highlight the contribution of different components.\r"
  },
  "cvpr2020_w7_atrainingmethodforimagecompressionnetworkstoimproveperceptualqualityofreconstructions": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "A Training Method for Image Compression Networks to Improve Perceptual Quality of Reconstructions",
    "authors": [
      "Jooyoung Lee",
      "Donghyun Kim",
      "Younhee Kim",
      "Hyoungjin Kwon",
      "Jongho Kim",
      "Taejin Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Lee_A_Training_Method_for_Image_Compression_Networks_to_Improve_Perceptual_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Lee_A_Training_Method_for_Image_Compression_Networks_to_Improve_Perceptual_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recently, neural-network based lossy image compression methods have been actively studied and they have achieved remarkable performance. However, the classical evaluation metrics, such as PSNR and MS-SSIM, that the recent approaches have been using in their objective function yield sub-optimal coding efficiency in terms of human perception, although they are very dominant metrics in research and standardization fields. Taking into account that improving the perceptual quality is one of major goals in lossy image compression, we propose a new training method that allows the existing image compression networks to reconstruct perceptually enhanced images. By experiments, we show the effectiveness of our method, both quantitatively and qualitatively.\r"
  },
  "cvpr2020_w7_jointmotionandresidualinformationlatentrepresentationforp-framecoding": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Joint Motion and Residual Information Latent Representation for P-Frame Coding",
    "authors": [
      "Renam Castro da Silva",
      "Nilson Donizete Guerin Jr.",
      "Pedro Sanches",
      "Henrique Costa Jung",
      "Eduardo Peixoto",
      "Bruno Macchiavello",
      "Edson M. Hung",
      "Vanessa Testoni",
      "Pedro Garcia Freitas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/da_Silva_Joint_Motion_and_Residual_Information_Latent_Representation_for_P-Frame_Coding_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/da_Silva_Joint_Motion_and_Residual_Information_Latent_Representation_for_P-Frame_Coding_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper proposes an inter-frame prediction frame encoding for the P-frame video compression challenge of the Workshop and Challenge on Learned Image Compression (CLIC). For this challenge, we use an uncompressed reference (previous) frame to compress the current frame. So, this is not a complete solution for learning-based video compression. The main goal is to represent a set of frames with an average of 0.075 bpp (bits per pixel), which is a very low bitrate. A restriction on the model size is also requested to avoid overfitting. Here we propose an autoencoder architecture that jointly represents the motion and residue information at the latent space. Three trained models were used to achieve the target bpp and a bit allocation algorithm is also proposed to optimize the quality performance of the encoded dataset.\r"
  },
  "cvpr2020_w7_avideocompressionframeworkusinganoverfittedrestorationneuralnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "A Video Compression Framework Using an Overfitted Restoration Neural Network",
    "authors": [
      "Gang He",
      "Chang Wu",
      "Lei Li",
      "Jinjia Zhou",
      "Xianglin Wang",
      "Yunfei Zheng",
      "Bing Yu",
      "Weiying Xie"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/He_A_Video_Compression_Framework_Using_an_Overfitted_Restoration_Neural_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/He_A_Video_Compression_Framework_Using_an_Overfitted_Restoration_Neural_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Many existing deep learning based video compression approaches apply deep neural networks (DNNs) to enhance the decoded video by learning the mapping between de- coded video and raw video (ground truth). The big chal- lenge is to train one well-fitted model (one mapping) for various video sequences. Different with the other applica- tions such as image enhancement whose ground truth can only be obtained in the training process, the video encoder can always get the ground truth which is the raw video. It means we can train the model together with video com- pression and use one model for each sequence or even for each frame. The main idea of our approach is building a video compression framework (VCOR) using overfitted restoration neural network (ORNN). A lightweight ORNN is trained for a group of consecutive frames, so that it is overfitted to this group and achieves a strong restoration ability. After that, parameters of ORNN are transmitted to the decoder as a part of the encoded bitstream. At the de- coder side, ORNN can perform the same strong restoration operation to the reconstructed frames. We participate in the CLIC2020 challenge on P-frame track as the team \"WestWorld\".\r"
  },
  "cvpr2020_w7_p-framecodingproposalbynctuparametricvideopredictionthroughbackprop-basedmotionestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "P-Frame Coding Proposal by NCTU: Parametric Video Prediction Through Backprop-Based Motion Estimation",
    "authors": [
      "Yung-Han Ho",
      "Chih-Chun Chan",
      "David Alexandre",
      "Wen-Hsiao Peng",
      "Chih-Peng Chang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Ho_P-Frame_Coding_Proposal_by_NCTU_Parametric_Video_Prediction_Through_Backprop-Based_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Ho_P-Frame_Coding_Proposal_by_NCTU_Parametric_Video_Prediction_Through_Backprop-Based_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents a parametric video prediction scheme with backprop-based motion estimation, in response to the CLIC challenge on P-frame compression. Recognizing that most learning-based video codecs rely on optical flow-based temporal prediction and suffer from having to signal a large amount of motion information, we propose to perform parametric overlapped block motion compensation on a sparse motion field. In forming this sparse motion field, we conduct the steepest descent algorithm on a loss function for identifying critical pixels, of which the motion vectors are communicated to the decoder. Moreover, we introduce a critical pixel dropout mechanism to strike a good balance between motion overhead and prediction quality. Compression results with HEVC-based residual coding on CLIC validation sequences show that our parametric video prediction achieves higher PSNR and MS-SSIM than optical flow-based warping. Moreover, our critical pixel dropout mechanism is found beneficial in terms of rate-distortion performance. Our scheme offers the potential for working with learned residual coding.\r"
  },
  "cvpr2020_w7_animagecompressionframeworkwithlearning-basedfilter": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "An Image Compression Framework With Learning-Based Filter",
    "authors": [
      "Heming Sun",
      "Chao Liu",
      "Jiro Katto",
      "Yibo Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Sun_An_Image_Compression_Framework_With_Learning-Based_Filter_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Sun_An_Image_Compression_Framework_With_Learning-Based_Filter_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, a coding framework VIP-ICT-Codec is introduced. Our method is based on the VTM (Versatile Video Coding Test Model). First, we propose a color space conversion from RGB to YUV domain by using a PCA-like operation. A method for the PCA mean calculation is proposed to de-correlate the residual components of YUV channels. In addition, the correlation of UV components are compensated considering that they share the same coding tree in VVC. We also learn a residual mapping to alleviate the over-filtered and under-filtered problem of specific images. Finally, we regard the rate control as an unconstraint Lagrangian problem to reach the target bpp. The results show that we achieve 32.625dB at the validation phase.\r"
  },
  "cvpr2020_w7_low-rateimagecompressionwithsuper-resolutionlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Low-Rate Image Compression With Super-Resolution Learning",
    "authors": [
      "Wei Gao",
      "Lvfang Tao",
      "Linjie Zhou",
      "Dinghao Yang",
      "Xiaoyu Zhang",
      "Zixuan Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Gao_Low-Rate_Image_Compression_With_Super-Resolution_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Gao_Low-Rate_Image_Compression_With_Super-Resolution_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose an end-to-end learned image compression framework for low-rate scenarios. Based on variational autoencoder, our method features a pair of compact-resolution and super-resolution networks, a set of hyper and main codec networks, and a conditional context model. The learning process of this framework is facilitated with integrated non-local attention modules and phase congruency priors. Multiple models are obtained from training with different hyper-parameters, and are jointly used in the image-level model selection process for rate control, which ensures that the bit-rate constraint of the CLIC challenge is satisfied. Experimental results demonstrate that the proposed method can achieve an averaged multi-scale structural similarity (MS-SSIM) score of 0.9648 with bit-rate consumption of 0.1499 bits per pixel, which outperforms the BPG image coding method significantly.\r"
  },
  "cvpr2020_w7_end-to-endoptimizedvideocompressionwithmv-residualprediction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "End-to-End Optimized Video Compression With MV-Residual Prediction",
    "authors": [
      "Xiangji Wu",
      "Ziwen Zhang",
      "Jie Feng",
      "Lei Zhou",
      "Junmin Wu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Wu_End-to-End_Optimized_Video_Compression_With_MV-Residual_Prediction_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Wu_End-to-End_Optimized_Video_Compression_With_MV-Residual_Prediction_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present an end-to-end trainable framework for P-frame compression in this paper. A joint motion vector (MV) and residual prediction network MV-Residual is designed to extract the ensembled features of motion representations and residual information by treating the two successive frames as inputs. The prior probability of the latent representations is modeled by a hyperprior autoencoder and trained jointly with the MV-Residual network. Specially, the spatially-displaced convolution is applied for video frame prediction, in which a motion kernel for each pixel is learned to generate predicted pixel by applying the kernel at a displaced location in the source image. Finally, novel rate allocation and post-processing strategies are used to produce the final compressed bits, considering the bits constraint of the challenge. The experimental results on validation set show that the proposed optimized framework can generate the highest MS-SSIM for P-frame compression competition.\r"
  },
  "cvpr2020_w7_multi-scalegroupeddensenetworkforvvcintracoding": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Multi-Scale Grouped Dense Network for VVC Intra Coding",
    "authors": [
      "Xin Li",
      "Simeng Sun",
      "Zhizheng Zhang",
      "Zhibo Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Li_Multi-Scale_Grouped_Dense_Network_for_VVC_Intra_Coding_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Li_Multi-Scale_Grouped_Dense_Network_for_VVC_Intra_Coding_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Versatile Video Coding (H.266/VVC) standard achieves better image quality when keeping the same bits than any other conventional image codecs, such as BPG, JPEG, and etc. However, it is still attractive and challenging to improve the image quality with high compression ratio on the basis of traditional coding techniques. In this paper, we design the multi-scale grouped dense network (MSGDN) to further reduce the compression artifacts by combining the multi-scale and grouped dense block, which are integrated as the post-process network of VVC intra coding. Besides, to improve the subjective quality of compressed image, we also present a generative adversarial network (MSGDN-GAN) by utilizing our MSGDN as generator. Across the extensive experiments on validation set, our MSGDN trained by MSE losses yields the PSNR of 32.622 on average with teams \"\"IMC\"\" and \"\"haha\"\" at the bit-rate of 0.15 in Low-rate track. Moreover, our MSGDN-GAN could achieve the better subjective performance.\r"
  },
  "cvpr2020_w7_imagecompressionwithencoder-decodermatchedsemanticsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Image Compression With Encoder-Decoder Matched Semantic Segmentation",
    "authors": [
      "Trinh Man Hoang",
      "Jinjia Zhou",
      "Yibo Fan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Hoang_Image_Compression_With_Encoder-Decoder_Matched_Semantic_Segmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Hoang_Image_Compression_With_Encoder-Decoder_Matched_Semantic_Segmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In recent years, the layered image compression is demonstrated to be a promising direction, which encodes a compact representation of the input image and apply an up-sampling network to reconstruct the image. To further improve the quality of the reconstructed image, some works transmit the semantic segment together with the compressed image data. Consequently, the compression ratio is also decreased because extra bits are required for transmitting the semantic segment. To solve this problem, we propose a new layered image compression framework with encoder-decoder matched semantic segmentation (EDMS). And then, followed by the semantic segmentation, a special convolution neural network is used to enhance the inaccurate semantic segment. As a result, the accurate semantic segment can be obtained in the decoder without requiring extra bits. The experimental results show that the proposed EDMS framework can get up to 35.31% BD-rate reduction over the HEVC-based (BPG) codec, 5% bitrate and 24% encoding time saving compare to the state-of-the-art semantic-based image codec.\r"
  },
  "cvpr2020_w7_variablerateimagecompressionmethodwithdead-zonequantizer": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Variable Rate Image Compression Method With Dead-Zone Quantizer",
    "authors": [
      "Jing Zhou",
      "Akira Nakagawa",
      "Keizo Kato",
      "Sihan Wen",
      "Kimihiko Kazui",
      "Zhiming Tan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Zhou_Variable_Rate_Image_Compression_Method_With_Dead-Zone_Quantizer_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Zhou_Variable_Rate_Image_Compression_Method_With_Dead-Zone_Quantizer_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning based image compression methods have achieved superior performance compared with transform based conventional codec. With end-to-end Rate-Distortion Optimization (RDO) in the codec, compression model is optimized with Lagrange multiplier l. For conventional codec, signal is decorrelated with orthonormal transformation, and uniform quantizer is introduced. We propose a variable rate image compression method with dead-zone quantizer. Firstly, the autoencoder network is trained with RaDOGAGA [6] framework, which can make the latents isometric to the metric space, such as SSIM and MSE. Then the conventional dead-zone quantization method with arbitrary step size is used in the common trained network to provide the flexible rate control. With dead-zone quantizer, the experimental results show that our method performs comparably with independently optimized models within a wide range of bitrate.\r"
  },
  "cvpr2020_w7_adaptingjpegxsgainsandprioritiestotasksandcontents": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Adapting JPEG XS Gains and Priorities to Tasks and Contents",
    "authors": [
      "Benoit Brummer",
      "Christophe de Vleeschouwer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Brummer_Adapting_JPEG_XS_Gains_and_Priorities_to_Tasks_and_Contents_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Brummer_Adapting_JPEG_XS_Gains_and_Priorities_to_Tasks_and_Contents_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Most current research in the domain of image compression focuses solely on achieving state of the art compression ratio, but that is not always usable in today's workflow due to the constraints on computing resources. Constant market requirements for a low-complexity image codec have led to the recent development and standardization of a lightweight image codec named JPEG XS. In this work we show that JPEG XS compression can be adapted to a specific given task and content, such as preserving visual quality on desktop content or maintaining high accuracy in neural network segmentation tasks, by optimizing its gain and priority parameters using the covariance matrix adaptation evolution strategy.\r"
  },
  "cvpr2020_w7_lossycompressionwithdistortionconstrainedoptimization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Lossy Compression With Distortion Constrained Optimization",
    "authors": [
      "Ties van Rozendaal",
      "Guillaume Sautiere",
      "Taco S. Cohen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/van_Rozendaal_Lossy_Compression_With_Distortion_Constrained_Optimization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/van_Rozendaal_Lossy_Compression_With_Distortion_Constrained_Optimization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " When training end-to-end learned models for lossy compression, one has to balance the rate and distortion losses. This is typically done by manually setting a tradeoff parameter b, an approach called b-VAE. Using this approach it is difficult to target a specific rate or distortion value, because the result can be very sensitive to b, and the approriate value for b depends on the model and problem setup. As a result, model comparison requires extensive per-model b-tuning, and producing a whole rate-distortion curve (by varying b) for each model to be compared. We argue that the constrained optimization method of Rezende and Viola, 2018 is a lot more appropriate for training lossy compression models because it allows us to obtain the best possible rate subject to a distortion constraint. This enables pointwise model comparisons, by training two models with the same distortion target and comparing their rate. We show that the method does manage to satisfy the constraint on a realistic image compression task, outperforms a constrained optimization method based on a hinge-loss, and is more practical to use for model selection than a b-VAE.\r"
  },
  "cvpr2020_w7_adversarialdistortionforlearnedvideocompression": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w7",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learned Image Compression",
    "title": "Adversarial Distortion for Learned Video Compression",
    "authors": [
      "Vijay Veerabadran",
      "Reza Pourreza",
      "Amirhossein Habibian",
      "Taco S. Cohen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w7/Veerabadran_Adversarial_Distortion_for_Learned_Video_Compression_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w7/Veerabadran_Adversarial_Distortion_for_Learned_Video_Compression_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we present a novel adversarial lossy video compression model. At extremely low bit-rates, standard video coding schemes suffer from unpleasant reconstruction artifacts such as blocking, ringing etc. Existing learned neural approaches to video compression have achieved reasonable success on reducing the bit-rate for efficient transmission and reduce the impact of artifacts to an extent. However, they still tend to produce blurred results under extreme compression. In this paper, we present a deep adversarial learned video compression model that minimizes an auxiliary adversarial distortion objective. We find this adversarial objective to correlate better with human perceptual quality judgement relative to traditional quality metrics such as MS-SSIM and PSNR. Our experiments using a state-of-the-art learned video compression system demonstrate a reduction of perceptual artifacts and reconstruction of detail lost especially under extremely high compression.\r"
  },
  "cvpr2020_w6_c-sureshrinkageestimatorandprototypeclassifierforcomplex-valueddeeplearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "C-SURE: Shrinkage Estimator and Prototype Classifier for Complex-Valued Deep Learning",
    "authors": [
      "Rudrasis Chakraborty",
      "Yifei Xing",
      "Minxuan Duan",
      "Stella X. Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Chakraborty_C-SURE_Shrinkage_Estimator_and_Prototype_Classifier_for_Complex-Valued_Deep_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Chakraborty_C-SURE_Shrinkage_Estimator_and_Prototype_Classifier_for_Complex-Valued_Deep_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The James-Stein shrinkage estimator is a biased estimator that captures the mean of Gaussian random vectors. Recognized by its dominance over the maximum likelihood estimator (MLE) in terms of mean squared error (MSE), the James-Stein estimator has gained huge interests from the statistical field. However, little progress is made for extending the estimator onto complex manifold-valued data. In this work, we propose a novel Stein's unbiased risk estimator (SURE) on the complex field with theoretically proven optimum over the MLE. We empirically compare and analyze results of our proposed model on a publicly available complex-valued dataset where we can achieve better results than other state-of-the-art methods.\r"
  },
  "cvpr2020_w6_sofeaanon-iterativeandrobustopticalflowestimationalgorithmfordynamicvisionsensors": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "SOFEA: A Non-Iterative and Robust Optical Flow Estimation Algorithm for Dynamic Vision Sensors",
    "authors": [
      "Weng Fei Low",
      "Zhi Gao",
      "Cheng Xiang",
      "Bharath Ramesh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Low_SOFEA_A_Non-Iterative_and_Robust_Optical_Flow_Estimation_Algorithm_for_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Low_SOFEA_A_Non-Iterative_and_Robust_Optical_Flow_Estimation_Algorithm_for_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We introduce the single-shot optical flow estimation algorithm (SOFEA) to non-iteratively compute the continuous-time flow information of events produced from bio-inspired cameras such as the dynamic vision sensor (DVS). The output of a DVS is a stream of asynchronous spikes (\"events\"), transmitted at very minimal latency (1-10 ms), caused by local brightness changes. Due to this unconventional output, a continuous representation of events over time is invaluable to most applications using the DVS. To this end, SOFEA consolidates the spatio-temporal information on the surface of active events for flow estimation in a single-shot manner, as opposed to iterative methods in the literature. In contrast to previous works, this is also the first principled method towards finding locally optimal set of neighboring events for plane fitting using an adaptation of Prim's algorithm. Consequently, SOFEA produces flow estimates that are more accurate across a wide variety of scenes compared to state-of-the-art methods. A direct application of such flow estimation is rendering sharp event images using the set of active events at a given time, which is further demonstrated and compared to existing works (source code will be made available at our homepage after the review process).\r"
  },
  "cvpr2020_w6_mosaicsuper-resolutionviasequentialfeaturepyramidnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Mosaic Super-Resolution via Sequential Feature Pyramid Networks",
    "authors": [
      "Mehrdad Shoeiby",
      "Ali Armin",
      "Sadegh Aliakbarian",
      "Saeed Anwar",
      "Lars Petersson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Shoeiby_Mosaic_Super-Resolution_via_Sequential_Feature_Pyramid_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Shoeiby_Mosaic_Super-Resolution_via_Sequential_Feature_Pyramid_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Advances in the design of multi-spectral cameras have led to great interests in a wide range of applications, from astronomy to autonomous driving. However, such cameras inherently suffer from a trade-off between the spatial and spectral resolution. In this paper, we propose to address this limitation by introducing a novel method to carry out super-resolution on raw mosaic images, multi-spectral or RGB Bayer, captured by modern real-time single-shot mosaic sensors. To this end, we design a deep super-resolution architecture that benefits from a sequential feature pyramid along the depth of the network. This, in fact, is achieved by utilizing a convolutional LSTM (ConvLSTM) to learn the inter-dependencies between features at different receptive fields. Additionally, by investigating the effect of different attention mechanisms in our framework, we show that a ConvLSTM inspired module is able to provide superior attention in our context. Our extensive experiments and analyses evidence that our approach yields significant super-resolution quality, outperforming current state-of-the-art mosaic super-resolution methods on both Bayer and multi-spectral images. Additionally, to the best of our knowledge, our method is the first specialized method to super-resolve mosaic images, whether it be multi-spectral or Bayer.\r"
  },
  "cvpr2020_w6_therisurnet-acomputationallyefficientthermalimagesuper-resolutionnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "TherISuRNet - A Computationally Efficient Thermal Image Super-Resolution Network",
    "authors": [
      "Vishal Chudasama",
      "Heena Patel",
      "Kalpesh Prajapati",
      "Kishor P. Upla",
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Chudasama_TherISuRNet_-_A_Computationally_Efficient_Thermal_Image_Super-Resolution_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Chudasama_TherISuRNet_-_A_Computationally_Efficient_Thermal_Image_Super-Resolution_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Human perception is limited to perceive the objects beyond the range of visible wavelengths in the Electromagnetic (EM) spectrum. This prevents them to recognize objects in different conditions such as poor illumination or severe weather (e.g., under fog or smoke). The technological advancement in thermographic imaging enables the visualization of objects beyond visible range which enables it's use in many applications such as military, medical, agriculture, etc. However, due to the hardware constraints, the thermal cameras are limited with poor spatial resolution when compared to similar visible range RGB cameras. In this paper, we propose a Super-Resolution (SR) of thermal images using a deep neural network architecture which we refer to as TherISuRNet. We use progressive upscaling strategy with asymmetrical residual learning in the network which is computationally efficient for different upscaling factors such as x2, x3 and x4. The proposed architecture consists of different modules for low and high-frequency feature extraction along with upsampling blocks. The effectiveness of the proposed architecture in TherISuRNet is verified by evaluating it with different datasets. The obtained results indicate superior results as compared to other state-of-the-art SR methods.\r"
  },
  "cvpr2020_w6_low-resolutionoverheadthermaltripwireforoccupancyestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Low-Resolution Overhead Thermal Tripwire for Occupancy Estimation",
    "authors": [
      "Mertcan Cokbas",
      "Prakash Ishwar",
      "Janusz Konrad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Cokbas_Low-Resolution_Overhead_Thermal_Tripwire_for_Occupancy_Estimation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Cokbas_Low-Resolution_Overhead_Thermal_Tripwire_for_Occupancy_Estimation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Smart buildings use people counts for various tasks ranging from energy-efficient HVAC and lighting to space-utilization analysis and emergency-response. We propose a people counting system which uses a low-resolution thermal sensor. Unlike previous thermal sensor based people counting systems, we use an overhead tripwire configuration at entryways to detect and track transient entries or exits. We develop two people counting algorithms for this system configuration. To evaluate our algorithms we have collected and labeled a low-resolution thermal video dataset with the proposed system configuration. The dataset, which is the largest of its kind, will be published alongside the paper. We also propose new evaluation metrics that are more suitable for systems that are subject to drift and jitter.\r"
  },
  "cvpr2020_w6_unsupervisedobjectdetectionvialwir/rgbtranslation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Unsupervised Object Detection via LWIR/RGB Translation",
    "authors": [
      "Rachael Abbott",
      "Neil M. Robertson",
      "Jesus Martinez del Rincon",
      "Barry Connor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Abbott_Unsupervised_Object_Detection_via_LWIRRGB_Translation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Abbott_Unsupervised_Object_Detection_via_LWIRRGB_Translation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this work, we present two new methods to overcome the lack of annotated long-wavelength infrared (LWIR) data by exploiting the abundance of similar RGB imagery. We introduce a novel unsupervised adaptation to the cycleGAN architecture for translating non-corresponding LWIR/RGB datasets. Our ultimate goal is high detection rates in the real LWIR imagery using only RGB labelled imagery for training detection algorithms. In our first experiment, we translate LWIR imagery to RGB, allowing us to use an RGB trained detection algorithm. We, thereby remove the need for labelled LWIR imagery for training detection algorithms. Experimental results show that our adaption helps to create synthetic RGB imagery with higher detection rates across two different datasets. We also find that combining the synthetic RGB and real LWIR imagery produces higher F1 scores on the RGB trained detection network. In our second experiment, we translate RGB to LWIR to fine-tune a network for detection in real LWIR imagery. This method produces the highest F1 scores out of the two methods with detection reaching up to 85.6%.\r"
  },
  "cvpr2020_w6_fusatnetdualattentionbasedspectrospatialmultimodalfusionnetworkforhyperspectralandlidarclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "FusAtNet: Dual Attention Based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification",
    "authors": [
      "Satyam Mohla",
      "Shivam Pande",
      "Biplab Banerjee",
      "Subhasis Chaudhuri"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " With recent advances in sensing, multimodal data is becoming easily available for various applications, especially in remote sensing (RS), where many data types like multispectral (MSI), hyperspectral (HSI), LiDAR etc. are available. Effective fusion of these multisource datasets is becoming important, for these multimodality features have been shown to generate highly accurate land-cover maps. However, fusion in the context of RS is non-trivial considering the redundancy involved in the data and the large domain differences among multiple modalities. In addition, the feature extraction modules for different modalities hardly interact among themselves, which further limits their semantic relatedness. As a remedy, we propose a feature fusion and extraction framework, namely FusAtNet, for collective land-cover classification of HSIs and LiDAR data in this paper. The proposed framework effectively utilizses HSI modality to generate an attention map using \"self-attention\" mechanism that highlights its own spectral features. Similarly, a \"cross-attention\" approach is simultaneously used to harness the LiDAR derived attention map that accentuates the spatial features of HSI. These attentive spectral and spatial representations are then explored further along with the original data to obtain modality-specific feature embeddings. The modality oriented joint spectro-spatial information thus obtained, is subsequently utilized to carry out the land-cover classification task. Experimental evaluations on three HSI-LiDAR datasets show that the proposed method achieves the state-of-the-art classification performance, including on the largest HSI-LiDAR dataset available, Houston, opening new avenues in multimodality feature fusion classification.\r"
  },
  "cvpr2020_w6_amulti-levelsupervisionmodelanovelapproachforthermalimagesuperresolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "A Multi-Level Supervision Model: A Novel Approach for Thermal Image Super Resolution",
    "authors": [
      "Priya Kansal",
      "Sabari Nathan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Kansal_A_Multi-Level_Supervision_Model_A_Novel_Approach_for_Thermal_Image_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Kansal_A_Multi-Level_Supervision_Model_A_Novel_Approach_for_Thermal_Image_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper proposes a novel architecture for thermal image super-resolution. A very large dataset is provided by PBVS 2020 in their super-resolution challenge. This dataset contains the images with three different resolution scales(low, medium, high) [1]. This dataset is used to train the proposed architecture to generate the super-resolution images in x2, x3, x4 scales. The proposed architecture is based on the residual blocks as the base units of the network. Along with this, the coordinate convolution layer and the convolutional block attention Module (CBAM) are also used in the architecture. Further, the multi-level supervision is implemented to supervise the output image resolution similarity with the real image at each block during training. To test the robustness of the proposed model, we evaluated our model on the Thermal-6 dataset [13]. The results show that our model is efficient to achieve the state of art results on the PBVS'2020 dataset. Further the results on the Thermal-6 dataset show that the model has a decent generalization capacity.\r"
  },
  "cvpr2020_w6_thermalimagesuper-resolutionchallenge-pbvs2020": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Thermal Image Super-Resolution Challenge - PBVS 2020",
    "authors": [
      "Rafael E. Rivadeneira",
      "Angel D. Sappa",
      "Boris X. Vintimilla",
      "Lin Guo",
      "Jiankun Hou",
      "Armin Mehri",
      "Parichehr Behjati Ardakani",
      "Heena Patel",
      "Vishal Chudasama",
      "Kalpesh Prajapati",
      "Kishor P. Upla",
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch",
      "Feras Almasri",
      "Olivier Debeir",
      "Sabari Nathan",
      "Priya Kansal",
      "Nolan Gutierrez",
      "Bardia Mojra",
      "William J. Beksi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_-_PBVS_2020_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Rivadeneira_Thermal_Image_Super-Resolution_Challenge_-_PBVS_2020_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper summarizes the top contributions to the first challenge on thermal image super-resolution (TISR), which was organized as part of the Perception Beyond the Visible Spectrum (PBVS) 2020 workshop. In this challenge, a novel thermal image dataset is considered together with state-of-the-art approaches evaluated under a common framework. The dataset used in the challenge consists of 1021 thermal images, obtained from three distinct thermal cameras at different resolutions (low-resolution, mid-resolution, and high-resolution), resulting in a total of 3063 thermal images. From each resolution, 951 images are used for training and 50 for testing while the 20 remaining images are used for two proposed evaluations. The first evaluation consists of downsampling the low-resolution, mid-resolution, and high-resolution thermal images by x2, x3 and x4 respectively, and comparing their super-resolution results with the corresponding ground truth images. The second evaluation is comprised of obtaining the x2 super-resolution from a given mid-resolution thermal image and comparing it with the corresponding semi-registered high-resolution thermal image. Out of 51 registered participants, 6 teams reached the final validation phase.\r"
  },
  "cvpr2020_w6_low-latencyhandgesturerecognitionwithalow-resolutionthermalimager": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Low-Latency Hand Gesture Recognition With a Low-Resolution Thermal Imager",
    "authors": [
      "Maarten Vandersteegen",
      "Wouter Reusen",
      "Kristof Van Beeck",
      "Toon Goedeme"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Vandersteegen_Low-Latency_Hand_Gesture_Recognition_With_a_Low-Resolution_Thermal_Imager_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Vandersteegen_Low-Latency_Hand_Gesture_Recognition_With_a_Low-Resolution_Thermal_Imager_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Using hand gestures to answer a call or to control the radio while driving a car, is nowadays an established feature in more expensive cars. High resolution time-of-flight cameras and powerful embedded processors usually form the heart of these gesture recognition systems. This however comes with a price tag. We therefore investigate the possibility to design an algorithm that predicts hand gestures using a cheap low-resolution thermal camera with only 32x24 pixels, which is light-weight enough to run on a low-cost processor. We recorded a new dataset of over 1300 video clips for training and evaluation and propose a light-weight low-latency prediction algorithm. Our best model achieves 95.9% classification accuracy and 83% mAP detection accuracy while its processing pipeline has a latency of only one frame.\r"
  },
  "cvpr2020_w6_high-resolutionradardatasetforsemi-supervisedlearningofdynamicobjects": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "High-Resolution Radar Dataset for Semi-Supervised Learning of Dynamic Objects",
    "authors": [
      "Mohammadreza Mostajabi",
      "Ching Ming Wang",
      "Darsh Ranjan",
      "Gilbert Hsyu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Mostajabi_High-Resolution_Radar_Dataset_for_Semi-Supervised_Learning_of_Dynamic_Objects_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Mostajabi_High-Resolution_Radar_Dataset_for_Semi-Supervised_Learning_of_Dynamic_Objects_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Current automotive radars output sparse point clouds with very low angular resolution. Such output lacks semantic information of the environment and has prevented radars from providing reliable redundancy when combined with cameras. This paper introduces the first true imaging-radar dataset for a diverse urban driving environments, with resolution matching that of lidar. To illustrate the need of having high resolution semantic information in modern radar applications, we show an unsupervised pretraining algorithm for deep neural networks to detect moving vehicles in radar data with limited ground-truth labels. We envision that the details seen in this type of high-resolution radar image allow us to borrow from decades of computer vision research and develop radar applications that were not previously possible, such as mapping, localization and drivable area detection. This dataset is our first attempt to introduce such data to the vision community, and we will continue to provide datasets with improved features in the future.\r"
  },
  "cvpr2020_w6_probabilisticorientedobjectdetectioninautomotiveradar": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Probabilistic Oriented Object Detection in Automotive Radar",
    "authors": [
      "Xu Dong",
      "Pengluo Wang",
      "Pengyue Zhang",
      "Langechuan Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Dong_Probabilistic_Oriented_Object_Detection_in_Automotive_Radar_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Dong_Probabilistic_Oriented_Object_Detection_in_Automotive_Radar_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Autonomous radar has been an integral part of advanced driver assistance systems due to its robustness to adverse weather and various lighting conditions. Conventional automotive radars use digital signal processing (DSP) algorithms to process raw data into sparse radar pins which do not provide information regarding the size and orientation of the objects. In this paper we propose a deep-learning based algorithm for radar object detection. The algorithm takes in radar data in its raw tensor representation and places probabilistic oriented bounding boxes (oriented bounding boxes with uncertainty estimate) around the detected objects in bird's-eye-view space. We created a new multimodal dataset with 102,544 frames of raw radar and synchronized LiDAR data. To reduce human annotation effort we developed a scalable pipeline to automatically annotate ground truth using LiDAR as reference. Based on this dataset we developed a vehicle detection pipeline using raw radar data as the only input. Our best performing radar detection model achieves 77.28% AP under oriented IoU of 0.3. To the best of our knowledge this is the first attempt to investigate object detection with raw radar data for conventional corner automotive radars.\r"
  },
  "cvpr2020_w6_vifbavisibleandinfraredimagefusionbenchmark": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "VIFB: A Visible and Infrared Image Fusion Benchmark",
    "authors": [
      "Xingchen Zhang",
      "Ping Ye",
      "Gang Xiao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Zhang_VIFB_A_Visible_and_Infrared_Image_Fusion_Benchmark_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Zhang_VIFB_A_Visible_and_Infrared_Image_Fusion_Benchmark_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Visible and infrared image fusion is an important area in image processing due to its numerous applications. While much progress has been made in recent years with efforts on developing image fusion algorithms, there is a lack of code library and benchmark which can gauge the state-of-the-art. In this paper, after briefly reviewing recent advances of visible and infrared image fusion, we present a visible and infrared image fusion benchmark (VIFB) which consists of 21 image pairs, a code library of 20 fusion algorithms and 13 evaluation metrics. We also carry out extensive experiments within the benchmark to understand the performance of these algorithms. By analyzing qualitative and quantitative results, we identify effective algorithms for robust image fusion and give some observations on the status and future prospects of this field.\r"
  },
  "cvpr2020_w6_fasthumanheadandshoulderdetectionusingconvolutionalnetworksandrgbddata": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Fast Human Head and Shoulder Detection Using Convolutional Networks and RGBD Data",
    "authors": [
      "Wassim A. El Ahmar",
      "Farzan Erlik Nowruzi",
      "Robert Laganiere"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Ahmar_Fast_Human_Head_and_Shoulder_Detection_Using_Convolutional_Networks_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Ahmar_Fast_Human_Head_and_Shoulder_Detection_Using_Convolutional_Networks_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We introduce a new real-time approach for human head and shoulder detection from RGB-D data based on a combination of image processing and deep learning approaches. Candidate head-top locations (CHL) are generated from a fast and accurate image processing algorithm that operates on depth data. We propose enhancements to the CHL algorithm making it three times faster. Various deep learning models are then evaluated for the tasks of classification and detection on the candidate head-top locations to regress the head bounding boxes and detect shoulder keypoints. We propose three different models based on convolutional neural networks for this problem. Experimental results for different architectures of our model are discussed. We also compare the performance of our models to other state of the art methods in terms of accuracy of detections and computational cost and show that our proposed models are on par with the state of the art in terms of precision-recall of head detection and precision of shoulders detection, with the biggest advantage of our models being in terms of computation time. We also analyze the effect of adding the depth channel on the performance of the network.\r"
  },
  "cvpr2020_w6_anevaluationofobjectiveimagequalityassessmentforthermalinfraredvideotonemapping": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "An Evaluation of Objective Image Quality Assessment for Thermal Infrared Video Tone Mapping",
    "authors": [
      "Michael Teutsch",
      "Simone Sedelmaier",
      "Sebastian Moosbauer",
      "Gabriel Eilertsen",
      "Thomas Walter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Teutsch_An_Evaluation_of_Objective_Image_Quality_Assessment_for_Thermal_Infrared_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Teutsch_An_Evaluation_of_Objective_Image_Quality_Assessment_for_Thermal_Infrared_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " State-of-the-art thermal infrared cameras produce high quality images with a bit depth of up to 16 bits per pixel (bpp). In practice, the data often reach a bit depth of 14 bpp, which cannot be displayed naively to a standard monitor that is limited to 8 bpp. Therefore, the dynamic range of these images has to be compressed. This can be done with an operator called tone mapping. There are many methods available for tone mapping, but the quality of the results can be extremely different. In this paper, we discuss and evaluate image quality assessment measures for tone mapping taken from the literature using thermal infrared videos. The usefulness of the measures is analyzed and effectively demonstrated by utilizing various reference Tone Mapping Operators (TMOs) based on traditional algorithm engineering on the one hand and deep learning on the other hand. We conclude that the chosen measures can objectively assess the quality of TMOs in thermal infrared videos.\r"
  },
  "cvpr2020_w6_calibratedvehiclepaintsignaturesforsimulatinghyperspectralimagery": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Calibrated Vehicle Paint Signatures for Simulating Hyperspectral Imagery",
    "authors": [
      "Zachary Mulhollan",
      "Aneesh Rangnekar",
      "Timothy Bauch",
      "Matthew J. Hoffman",
      "Anthony Vodacek"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Mulhollan_Calibrated_Vehicle_Paint_Signatures_for_Simulating_Hyperspectral_Imagery_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Mulhollan_Calibrated_Vehicle_Paint_Signatures_for_Simulating_Hyperspectral_Imagery_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We investigate a procedure for rapidly adding calibrated vehicle visible-near infrared (VNIR) paint signatures to an existing hyperspectral simulator - The Digital Imaging and Remote Sensing Image Generation (DIRSIG) model - to create more diversity in simulated urban scenes. The DIRSIG model can produce synthetic hyperspectral imagery with user-specified geometry, atmospheric conditions, and ground target spectra. To render an object pixel's spectral signature, DIRSIG uses a large database of reflectance curves for the corresponding object material and a bidirectional reflectance model to introduce s due to orientation and surface structure. However, this database contains only a few spectral curves for vehicle paints and generates new paint signatures by combining these curves internally. In this paper we demonstrate a method to rapidly generate multiple paint spectra, flying a drone carrying a pushbroom hyperspectral camera to image a university parking lot. We then process the images to convert them from the digital count space to spectral reflectance without the need of calibration panels in the scene, and port the paint signatures into DIRSIG for successful integration into the newly rendered sets of synthetic VNIR hyperspectral scenes.\r"
  },
  "cvpr2020_w6_unsupervisedensemble-kernelprincipalcomponentanalysisforhyperspectralanomalydetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w6",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Perception Beyond the Visible Spectrum",
    "title": "Unsupervised Ensemble-Kernel Principal Component Analysis for Hyperspectral Anomaly Detection",
    "authors": [
      "Nicholas Merrill",
      "Colin C. Olson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w6/Merrill_Unsupervised_Ensemble-Kernel_Principal_Component_Analysis_for_Hyperspectral_Anomaly_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w6/Merrill_Unsupervised_Ensemble-Kernel_Principal_Component_Analysis_for_Hyperspectral_Anomaly_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Unsupervised anomaly detection--which aims to identify outliers in data sets without the use of labeled training data--is critically important across a variety of domains including medicine, security, defense, finance, and imaging. In particular, detection of anomalous pixels within hyperspectral images is used for purposes ranging from the detection of military targets to the location of invasive plant species. Kernel methods have frequently been employed for this unsupervised learning task but are limited by their sensitivity to parameter choices and the absence of a validation step. Here, we use reconstruction error in the kernel Principal Component Analysis (kPCA) feature space as a metric for anomaly detection and propose, via batch gradient descent minimization of a novel loss function, to automate the selection of the Gaussian RBF kernel parameter, sigma. In addition, we leverage an ensemble of learned models to reduce computational cost and improve detection performance. We describe how to select the model ensemble and show that our method yields better detection accuracy relative to competing algorithms on a pair of data sets.\r"
  },
  "cvpr2020_w8_syntharchinteractiveimagesearchwithattribute-conditionedsynthesis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "Syntharch: Interactive Image Search With Attribute-Conditioned Synthesis",
    "authors": [
      "Zac Yu",
      "Adriana Kovashka"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Yu_Syntharch_Interactive_Image_Search_With_Attribute-Conditioned_Synthesis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Yu_Syntharch_Interactive_Image_Search_With_Attribute-Conditioned_Synthesis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The use of interactive systems has been found to be a promising approach for content-based image retrieval, the task of retrieving a specific image from a database based on its content. These systems allow the user to refine the set of results iteratively until the target is reached. In order to proceed with the search efficiently, conventional methods rely on some shared knowledge between the user and the system, such as semantic visual attributes of the images. Those approaches demand the images to be semantically labeled and introduce a semantic gap between the two parties' understanding. In this paper, we explore an alternative approach to interactive image search where feedback is elicited exclusively in visual forms, therefore eliminating the semantic gap and allowing for a generalized version of the method to operate on unlabeled databases. We present Syntharch, a novel interactive image search approach which uses synthesized images as options for feedback, instead of asking textual questions to gain information on the relative attribute values of the target image. We further demonstrate that by using synthesized images rather than real images retrieved from the database as feedback options, Syntharch causes less confusion to the user. Finally, we establish that our proposed search method performs similarly or better in comparison to the conventional approach.\r"
  },
  "cvpr2020_w8_learningspatialrelationshipsbetweensamplesofpatentimageshapes": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "Learning Spatial Relationships Between Samples of Patent Image Shapes",
    "authors": [
      "Juan Castorena",
      "Manish Bhattarai",
      "Diane Oyen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Castorena_Learning_Spatial_Relationships_Between_Samples_of_Patent_Image_Shapes_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Castorena_Learning_Spatial_Relationships_Between_Samples_of_Patent_Image_Shapes_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Binary image based classification and retrieval of documents of an intellectual nature is a very challenging problem. Variations in the binary image generation mechanisms which are subject to the document artisan designer including drawing style, view-point, inclusion of multiple image components are plausible causes for increasing the complexity of the problem. In this work, we propose a method suitable to binary images which bridges some of the successes of deep learning (DL) to alleviate the problems introduced by the aforementioned variations. The method consists on extracting the shape of interest from the binary image and applying a non-Euclidean geometric neural-net architecture to learn the local and global spatial relationships of the shape. Empirical results show that our method is in some sense invariant to the image generation mechanism variations and achieves results outperforming existing methods in a patent image dataset benchmark.\r"
  },
  "cvpr2020_w8_diagramimageretrievalusingsketch-baseddeeplearningandtransferlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "Diagram Image Retrieval Using Sketch-Based Deep Learning and Transfer Learning",
    "authors": [
      "Manish Bhattarai",
      "Diane Oyen",
      "Juan Castorena",
      "Liping Yang",
      "Brendt Wohlberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Bhattarai_Diagram_Image_Retrieval_Using_Sketch-Based_Deep_Learning_and_Transfer_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Bhattarai_Diagram_Image_Retrieval_Using_Sketch-Based_Deep_Learning_and_Transfer_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Resolution of the complex problem of image retrieval for diagram images has yet to be reached. Deep learning methods continue to excel in the fields of object detection and image classification applied to natural imagery. However, the application of such methodologies applied to binary imagery remains limited due to lack of crucial features such as textures,color and intensity information. This paper presents a deep learning based method for image-based search for binary patent images by taking advantage of existing large natural image repositories for image search and sketch-based methods. (Sketches are not identical to diagrams, but they do share some characteristics; for example, they both are gray scale (binary), composed of contours, and lacking in texture are key features for both imagery types.) We begin by using deep learning to generate sketches from natural images for image retrieval and then train a second deep learning model on the sketches. We then use our small set of manually labeled patent diagram images via transfer learning to adapt the image search from sketches of natural images to diagrams. Our experiment results show the effectiveness of deep learning with transfer learning for detecting near-identical copies in patent images and querying similar images based on content.\r"
  },
  "cvpr2020_w8_automaticdigitizationofengineeringdiagramsusingdeeplearningandgraphsearch": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "Automatic Digitization of Engineering Diagrams Using Deep Learning and Graph Search",
    "authors": [
      "Shouvik Mani",
      "Michael A. Haddad",
      "Dan Constantini",
      "Willy Douhard",
      "Qiwei Li",
      "Louis Poirier"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Mani_Automatic_Digitization_of_Engineering_Diagrams_Using_Deep_Learning_and_Graph_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Mani_Automatic_Digitization_of_Engineering_Diagrams_Using_Deep_Learning_and_Graph_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " A Piping and Instrumentation Diagram (P&ID) is a type of engineering diagram that uses symbols, text, and lines to represent the components and flow of an industrial process. Although used universally across industries such as manufacturing and oil & gas, P&IDs are usually trapped in image files with limited metadata, making their contents unsearchable and siloed from operational or enterprise systems. In order to extract the information contained in these diagrams, we propose a pipeline for automatically digitizing P&IDs. Our pipeline combines a series of computer vision techniques to detect symbols in a diagram, match symbols with associated text, and detect connections between symbols through lines. For the symbol detection task, we train a Convolutional Neural Network to classify certain common symbols with over 90% precision and recall. To detect connections between symbols, we use a graph search approach to traverse a diagram through its lines and discover interconnected symbols. By transforming unstructured diagrams into structured information, our pipeline enables applications such as diagram search, equipment-to-sensor mapping, and asset hierarchy creation. When integrated with operational and enterprise data, the extracted asset hierarchy serves as the foundation for a facility-wide digital twin, enabling advanced applications such as machine learning-based predictive maintenance.\r"
  },
  "cvpr2020_w8_structuredquery-basedimageretrievalusingscenegraphs": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "Structured Query-Based Image Retrieval Using Scene Graphs",
    "authors": [
      "Brigit Schroeder",
      "Subarna Tripathi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Schroeder_Structured_Query-Based_Image_Retrieval_Using_Scene_Graphs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Schroeder_Structured_Query-Based_Image_Retrieval_Using_Scene_Graphs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " A structured query can capture the complexity of object interactions in images (e.g. 'woman rides motorcycle') unlike single objects (e.g. 'woman' or 'motorcycle'). Image retrieval using structured queries therefore is much more useful than single object retrieval, but a much more challenging problem. In this paper we present a method which uses scene graph embeddings as the basis of our image retrieval approach. We examine how visual relationships, derived from scene graphs, can be used as structured queries. Notably, we are able to achieve high recall even on low to medium frequency objects found in the long-tailed COCO-Stuff dataset, and find adding a visual relationship-inspired loss boosts our recall by 10% in the best case.\r"
  },
  "cvpr2020_w8_diagramimageretrievalandanalysischallengesandopportunities": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "Diagram Image Retrieval and Analysis: Challenges and Opportunities",
    "authors": [
      "Liping Yang",
      "Ming Gong",
      "Vijayan K. Asari"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Yang_Diagram_Image_Retrieval_and_Analysis_Challenges_and_Opportunities_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Yang_Diagram_Image_Retrieval_and_Analysis_Challenges_and_Opportunities_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning has achieved significant advances for tasks such as image classification, segmentation, and retrieval; this advance has not yet been realized on scientific and technical drawing images. Research for technical diagram image analysis and retrieval retain much less well developed compared to natural images; one major reason is that the dominant features in scientific diagram images are shape and topology, no color and intensity features, which are essential in retrieval and analysis of natural images. One important purpose of this review, along with some challenges and opportunities, is to draw the attention of researchers and practitioners in the Computer Vision community to the strong needs of advancing research for diagram image retrieval and analysis, beyond the current focus on natural images, in order to move machine vision closer to artificial general intelligence. This paper investigates recent research on diagram image retrieval and analysis, with an emphasis on methods using content-based image retrieval (CBIR), textures, shapes, topology and geometry. Based on our systematic review of key research on diagram image retrieval and analysis, we then demonstrate and discuss some of the main technical challenges to be overcome for diagram image retrieval and analysis, and point out future research opportunities from technical and application perspectives.\r"
  },
  "cvpr2020_w8_asimplifiedframeworkforzero-shotcross-modalsketchdataretrieval": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w8",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Diagram Image Retrieval and Analysis: Representation, Learning, and Similarity Metrics",
    "title": "A Simplified Framework for Zero-Shot Cross-Modal Sketch Data Retrieval",
    "authors": [
      "Ushasi Chaudhuri",
      "Biplab Banerjee",
      "Avik Bhattacharya",
      "Mihai Datcu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w8/Chaudhuri_A_Simplified_Framework_for_Zero-Shot_Cross-Modal_Sketch_Data_Retrieval_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w8/Chaudhuri_A_Simplified_Framework_for_Zero-Shot_Cross-Modal_Sketch_Data_Retrieval_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We deal with the problem of zero-shot cross-modal image retrieval involving color and sketch images through a novel deep representation learning technique. The problem of a sketch to image retrieval and vice-versa is of practical importance, and a trained model in this respect is expected to generalize beyond the training classes, e.g., the zero-shot learning scenario. Nonetheless, considering the drastic distributions-gap between both the modalities, a feature alignment is necessary to learn a shared feature space where retrieval can efficiently be carried out. Additionally, it should also be guaranteed that the shared space is semantically meaningful in order to aid in the zero-shot retrieval task. The very few existing techniques for zero-shot sketch-RGB image retrieval deploy the deep generative models for learning the embedding space; however, training a typical GAN like model for multi-modal image data may be non-trivial at times. To this end, we propose a multi-stream encoder-decoder model that simultaneously ensures improved mapping between the RGB and sketch image spaces and high discrimination in the shared semantics-driven encoded feature space. Further, it is guaranteed that the class topology of the original semantic space is preserved in the encoded feature space, which subsequently reduces the model bias towards the training classes. Experimental results obtained on the benchmark Sketchy and TU-Berlin datasets establish the efficacy of our model as we outperform the existing state-of-the-art techniques by a considerable margin.\r"
  },
  "cvpr2020_w70_lighttrackagenericframeworkforonlinetop-downhumanposetracking": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w70",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Towards Human-Centric Image/Video Synthesis and the Look-Into-Person Challenge",
    "title": "LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking",
    "authors": [
      "Guanghan Ning",
      "Jian Pei",
      "Heng Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w70/Ning_LightTrack_A_Generic_Framework_for_Online_Top-Down_Human_Pose_Tracking_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w70/Ning_LightTrack_A_Generic_Framework_for_Online_Top-Down_Human_Pose_Tracking_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a simple yet effective framework, named LightTrack, for online human pose tracking. Existing methods usually perform human detection, pose estimation and tracking in sequential stages, where pose tracking is regarded as an offline bipartite matching problem. Our proposed framework is designed to be generic, efficient and truly online for top-down approaches. For efficiency, Single-Person Pose Tracking (SPT) and Visual Object Tracking (VOT) are incorporated as a unified online functioning entity, easily implemented by a replaceable single-person pose estimator. To mitigate offline optimization costs, the framework also unifies SPT with online identity association and sheds first light upon bridging multi-person keypoint tracking with Multi-Target Object Tracking (MOT). Specifically, we propose a Siamese Graph Convolution Network (SGCN) for human pose matching as a Re-ID module. In contrary to other Re-ID modules, we use a graphical representation of human joints for matching. The skeleton-based representation effectively captures human pose similarity and is computationally inexpensive. It is robust to sudden camera shifts that introduce human drifting. The proposed framework is general enough to fit other pose estimators and candidate matching mechanisms. Extensive experiments show that our method outperforms other online methods and is very competitive with offline state-of-the-art methods while maintaining higher frame rates. Code and models are publicly available at https://github.com/Guanghan/lighttrack.\r"
  },
  "cvpr2020_w70_epipolartransformerformulti-viewhumanposeestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w70",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Towards Human-Centric Image/Video Synthesis and the Look-Into-Person Challenge",
    "title": "Epipolar Transformer for Multi-View Human Pose Estimation",
    "authors": [
      "Yihui He",
      "Rui Yan",
      "Katerina Fragkiadaki",
      "Shoou-I Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w70/He_Epipolar_Transformer_for_Multi-View_Human_Pose_Estimation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w70/He_Epipolar_Transformer_for_Multi-View_Human_Pose_Estimation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " A common way to localize 3D human joints in a synchronized and calibrated multi-view setup is a two-step process: (1) apply a 2D detector separately on each view to localize joints in 2D, and (2) robust triangulation on 2D detections from each view to acquire 3D joint locations. However, in step 1, the 2D detector is constrained to solve challenging cases which could be better resolved in 3D, such as occlusions and oblique viewing angles, purely in 2D without leveraging any 3D information. Therefore, we propose the differentiable \"epipolar transformer\", which empowers the 2D detector to leverage 3D-aware intermediate features to improve 2D pose estimation. The intuition is: given a 2D location p in reference view, we would like to first find its corresponding point p' in source view, then combine the features at p' with the features at p, thus leading to a more 3D-aware intermediate feature at p. Inspired by stereo matching, the epipolar transformer leverages epipolar constraints and feature matching to approximate the features at p'. The key advantages of the epipolar transformer is: (1) it has minimal learnable parameters, (2) it can be easily plugged into existing networks, moreover (3) it is easily interpretable, i.e., we can analyze the location p' to understand whether matching over the epipolar line was successful. Experiments on InterHand and Human3.6M show that our approach has consistent improvements over the baselines. Specifically, in the condition where no external data is used, our Human3.6M model trained with ResNet-50 and image size 256x256 outperforms state-of-the-art by a large margin and achieves MPJPE 26.9 mm. Code is available. This is the workshop version of our CVPR 2020 paper [8]\r"
  },
  "cvpr2020_w70_yoga-82anewdatasetforfine-grainedclassificationofhumanposes": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w70",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Towards Human-Centric Image/Video Synthesis and the Look-Into-Person Challenge",
    "title": "Yoga-82: A New Dataset for Fine-Grained Classification of Human Poses",
    "authors": [
      "Manisha Verma",
      "Sudhakar Kumawat",
      "Yuta Nakashima",
      "Shanmuganathan Raman"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w70/Verma_Yoga-82_A_New_Dataset_for_Fine-Grained_Classification_of_Human_Poses_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w70/Verma_Yoga-82_A_New_Dataset_for_Fine-Grained_Classification_of_Human_Poses_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Human pose estimation is a well-known problem in computer vision to locate joint positions. Existing datasets for learning of poses are observed to be not challenging enough in terms of pose diversity, object occlusion and view points. This makes the pose annotation process relatively simple and restricts the application of the models that have been trained on them. To handle more variety in human poses, we propose the concept of fine-grained hierarchical pose classification, in which we formulate the pose estimation as a classification task, and propose a dataset, Yoga-82, for large-scale yoga pose recognition with 82 classes. Yoga-82 consists of complex poses where fine annotations may not be possible. To resolve this, we provide hierarchical labels for yoga poses based on the body configuration of the pose. The dataset contains a three-level hierarchy including body positions, variations in body positions, and the actual pose names. We present the classification accuracy of the state-of-the-art convolutional neural network architectures on Yoga-82. We also present several hierarchical variants of DenseNet in order to utilize the hierarchical labels.\r"
  },
  "cvpr2020_w70_fine-grainedpointingrecognitionfornaturaldroneguidance": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w70",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Towards Human-Centric Image/Video Synthesis and the Look-Into-Person Challenge",
    "title": "Fine-Grained Pointing Recognition for Natural Drone Guidance",
    "authors": [
      "Oscar L. Barbed",
      "Pablo Azagra",
      "Lucas Teixeira",
      "Margarita Chli",
      "Javier Civera",
      "Ana C. Murillo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w70/Barbed_Fine-Grained_Pointing_Recognition_for_Natural_Drone_Guidance_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w70/Barbed_Fine-Grained_Pointing_Recognition_for_Natural_Drone_Guidance_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Human action recognition systems are typically focused on identifying different actions, rather than fine grained variations of the same action. This work explores strategies to identify different pointing directions in order to build a natural interaction system to guide autonomous systems such as drones. Commanding a drone with hand-held panels or tablets is common practice but intuitive user-drone interfaces might have significant benefits. The system proposed in this work just requires the user to provide occasional high-level navigation commands by pointing the drone towards the desired motion direction. Due to the lack of data on these settings, we present a new benchmarking video dataset to validate our framework and facilitate future research on the area. Our results show good accuracy for pointing direction recognition, while running at interactive rates and exhibiting robustness to variability in user appearance, viewpoint, camera distance and scenery.\r"
  },
  "cvpr2020_w70_themtadatasetformulti-targetmulti-camerapedestriantrackingbyweighteddistanceaggregation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w70",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Towards Human-Centric Image/Video Synthesis and the Look-Into-Person Challenge",
    "title": "The MTA Dataset for Multi-Target Multi-Camera Pedestrian Tracking by Weighted Distance Aggregation",
    "authors": [
      "Philipp Kohl",
      "Andreas Specker",
      "Arne Schumann",
      "Jurgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w70/Kohl_The_MTA_Dataset_for_Multi-Target_Multi-Camera_Pedestrian_Tracking_by_Weighted_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w70/Kohl_The_MTA_Dataset_for_Multi-Target_Multi-Camera_Pedestrian_Tracking_by_Weighted_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Existing multi target multi camera tracking (MTMCT) datasets are small in terms of the number of identities and video lengths. The creation of new real world datasets is hard as privacy has to be guaranteed and the labeling is tedious. Therefore in the scope of this work a mod for GTA V to record a MTMCT dataset has been developed which also has been used to record a simulated MTMCT dataset called Multi Camera Track Auto (MTA). The MTA dataset contains over 2400 identities, 6 cameras and a video length of over 100 minutes per camera. Additionally a MTMCT system has been implemented to be able to provide a baseline for the created dataset. The system's pipeline looks as follows: Person detection, person re-identification, single camera multi target tracking, track distance calculation, track association. The track distance calculation is a weighted sum of the following distances: A single camera time constraint, a multi camera time constraint using convex camera overlapping areas, an appearance feature distance, a homography matching with pairwise camera homographies and a linear prediction based on the velocity and the time difference of tracks. When using all partial distances, we were able to surpass the results of state-of-the-art single camera trackers by +20% IDF1 score.\r"
  },
  "cvpr2020_w70_reposinghumansbywarping3dfeatures": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w70",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Towards Human-Centric Image/Video Synthesis and the Look-Into-Person Challenge",
    "title": "Reposing Humans by Warping 3D Features",
    "authors": [
      "Markus Knoche",
      "Istvan Sarandi",
      "Bastian Leibe"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w70/Knoche_Reposing_Humans_by_Warping_3D_Features_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w70/Knoche_Reposing_Humans_by_Warping_3D_Features_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We address the problem of reposing an image of a human into any desired novel pose. This conditional image-generation task requires reasoning about the 3D structure of the human, including self-occluded body parts. Most prior works are either based on 2D representations or require fitting and manipulating an explicit 3D body mesh. Based on the recent success in deep learning-based volumetric representations, we propose to implicitly learn a dense feature volume from human images, which lends itself to simple and intuitive manipulation through explicit geometric warping. Once the latent feature volume is warped according to the desired pose change, the volume is mapped back to RGB space by a convolutional decoder. Our state-of-the-art results on the DeepFashion and the iPER benchmarks indicate that dense volumetric human representations are worth investigating in more detail.\r"
  },
  "cvpr2020_w11_resdepthlearnedresidualstereoreconstruction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "ResDepth: Learned Residual Stereo Reconstruction",
    "authors": [
      "Corinne Stucker",
      "Konrad Schindler"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Stucker_ResDepth_Learned_Residual_Stereo_Reconstruction_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Stucker_ResDepth_Learned_Residual_Stereo_Reconstruction_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose an embarrassingly simple but very effective scheme for high-quality dense stereo reconstruction: (i) generate an approximate reconstruction with your favourite stereo matcher; (ii) rewarp the input images with that approximate model; (iii) with the initial reconstruction and the warped images as input, train a deep network to enhance the reconstruction by regressing a residual correction; and (iv) if desired, iterate the refinement with the new, improved reconstruction. The strategy to only learn the residual greatly simplifies the learning problem. A standard Unet without bells and whistles is enough to reconstruct even small surface details, like dormers and roof substructures in satellite images. We also investigate residual reconstruction with less information and find that even a single image is enough to greatly improve an approximate reconstruction. Our full model reduces the mean absolute error of state-of-the-art stereo reconstruction systems by >50%, both in our target domain of satellite stereo and on stereo pairs from the ETH3D benchmark.\r"
  },
  "cvpr2020_w11_dalesalarge-scaleaeriallidardatasetforsemanticsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "DALES: A Large-Scale Aerial LiDAR Data Set for Semantic Segmentation",
    "authors": [
      "Nina Varney",
      "Vijayan K. Asari",
      "Quinn Graehling"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Varney_DALES_A_Large-Scale_Aerial_LiDAR_Data_Set_for_Semantic_Segmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Varney_DALES_A_Large-Scale_Aerial_LiDAR_Data_Set_for_Semantic_Segmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present the Dayton Annotated LiDAR Earth Scan (DALES) data set, a new large-scale aerial LiDAR data set with over a half-billion hand-labeled points spanning 10 square kilometers of area and eight object categories. Large annotated point cloud data sets have become the standard for evaluating deep learning methods. However, most of the existing data sets focus on data collected from a mobile or terrestrial scanner with few focusing on aerial data. Point cloud data collected from an Aerial Laser Scanner (ALS) presents a new set of challenges and applications in areas such as 3D urban modeling and large-scale surveillance. DALES is the most extensive publicly available ALS data set with over 400 times the number of points and six times the resolution of other currently available annotated aerial point cloud data sets. This data set gives a critical number of expert verified hand-labeled points for the evaluation of new 3D deep learning algorithms, helping to expand the focus of current algorithms to aerial data. We describe the nature of our data, annotation workflow, and provide a benchmark of current state-of-the-art algorithm performance on the DALES data set.\r"
  },
  "cvpr2020_w11_s2awassersteinganwithspatio-spectrallaplacianattentionformulti-spectralbandsynthesis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "S2A: Wasserstein GAN With Spatio-Spectral Laplacian Attention for Multi-Spectral Band Synthesis",
    "authors": [
      "Litu Rout",
      "Indranil Misra",
      "S Manthira Moorthi",
      "Debajyoti Dhar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Rout_S2A_Wasserstein_GAN_With_Spatio-Spectral_Laplacian_Attention_for_Multi-Spectral_Band_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Rout_S2A_Wasserstein_GAN_With_Spatio-Spectral_Laplacian_Attention_for_Multi-Spectral_Band_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Intersection of adversarial learning and satellite image processing is an emerging field in remote sensing. In this study, we intend to address synthesis of high resolution multi-spectral satellite imagery using adversarial learning. Guided by the discovery of attention mechanism, we regulate the process of band synthesis through spatio-spectral Laplacian attention. Further, we use Wasserstein GAN with gradient penalty norm to improve training and stability of adversarial learning. In this regard, we introduce a new cost function for the discriminator based on spatial attention and domain adaptation loss. We critically analyze the qualitative and quantitative results compared with state-of-the-art methods using widely adopted evaluation metrics. Our experiments on datasets of three different sensors, namely LISS-3, LISS-4, and WorldView-2 show that attention learning performs favorably against state-of-the-art methods. Using the proposed method we provide an additional data product in consistent with existing high resolution bands. Furthermore, we synthesize over 4000 high resolution scenes covering various terrains to analyze scientific fidelity. At the end, we demonstrate plausible large scale real world applications of the synthesized band.\r"
  },
  "cvpr2020_w11_densitymapguidedobjectdetectioninaerialimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Density Map Guided Object Detection in Aerial Images",
    "authors": [
      "Changlin Li",
      "Taojiannan Yang",
      "Sijie Zhu",
      "Chen Chen",
      "Shanyue Guan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Li_Density_Map_Guided_Object_Detection_in_Aerial_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Li_Density_Map_Guided_Object_Detection_in_Aerial_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Object detection in high-resolution aerial images is a challenging problem because of 1) the large variation in object size, and 2) non-uniform distribution of objects. A common solution is to divide the large aerial image into small (uniform) chips and then apply object detection on each small crop. In this paper, we investigate the effective image cropping strategy to address these challenges. Specifically, we propose a Density-Map guided object detection Network (DMNet), which is inspired from the observation that density map presents how objects distribute in terms of pixel intensity. As pixel intensity varies, it is able to tell whether a region has objects or not, which in turn provide guidance to crop image statistically. DMNet has three key components: a density map generation module, an image cropping module and an object detector. DMNet generates density map and learns scale of categories by utilizing pixel intensity as the guidance to form an implicit boundary as tentative cropping region, which is affected by objects in the region. Compared with ClusDet [??], DMNet puts more emphasis on spatial relation between objects. Extensive experiments show that the proposed method achieves state-of-the-art performance on two popular aerial image datasets, i.e. VisionDrone [??] and UAVDT [??].\r"
  },
  "cvpr2020_w11_standardganmulti-sourcedomainadaptationforsemanticsegmentationofveryhighresolutionsatelliteimagesbydatastandardization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "StandardGAN: Multi-Source Domain Adaptation for Semantic Segmentation of Very High Resolution Satellite Images by Data Standardization",
    "authors": [
      "Onur Tasar",
      "Yuliya Tarabalka",
      "Alain Giros",
      "Pierre Alliez",
      "Sebastien Clerc"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Tasar_StandardGAN_Multi-Source_Domain_Adaptation_for_Semantic_Segmentation_of_Very_High_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Tasar_StandardGAN_Multi-Source_Domain_Adaptation_for_Semantic_Segmentation_of_Very_High_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Domain adaptation for semantic segmentation has recently been actively studied to increase the generalization capabilities of deep learning models. The vast majority of the domain adaptation methods tackle single-source case, where the model trained on a single source domain is adapted to a target domain. However, these methods have limited practical real world applications, since usually one has multiple source domains with different data distributions. In this work, we deal with the multi-source domain adaptation problem. Our method, namely StandardGAN, standardizes each source and target domains so that all the data have similar data distributions. We then use the standardized source domains to train a classifier and segment the standardized target domain. We conduct extensive experiments on two remote sensing data sets, in which the first one consists of multiple cities from a single country, and the other one contains multiple cities from different countries. Our experimental results show that the standardized data generated by StandardGAN allow the classifiers to generate significantly better segmentation.\r"
  },
  "cvpr2020_w11_monte-carlosiamesepolicyonactorforsatelliteimagesuperresolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Monte-Carlo Siamese Policy on Actor for Satellite Image Super Resolution",
    "authors": [
      "Litu Rout",
      "Saumyaa Shah",
      "S Manthira Moorthi",
      "Debajyoti Dhar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Rout_Monte-Carlo_Siamese_Policy_on_Actor_for_Satellite_Image_Super_Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Rout_Monte-Carlo_Siamese_Policy_on_Actor_for_Satellite_Image_Super_Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In the past few years supervised and adversarial learning have been widely adopted in various complex computer vision tasks. It seems natural to wonder whether another branch of artificial intelligence, commonly known as Reinforcement Learning (RL) can benefit such complex vision tasks. In this study, we explore the plausible usage of RL in super resolution of remote sensing imagery. Guided by recent advances in super resolution, we propose a theoretical framework that leverages the benefits of supervised and reinforcement learning. We argue that a straightforward implementation of RL is not adequate to address ill-posed super resolution as the action variables are not fully known. To tackle this issue, we propose to parameterize action variables by matrices, and train our policy network using Monte-Carlo sampling. We study the implications of parametric action space in a model-free environment from theoretical and empirical perspective. Furthermore, we analyze the quantitative and qualitative results on both remote sensing and non-remote sensing datasets. Based on our experiments, we report considerable improvement over state-of-the-art methods by encapsulating supervised models in a reinforcement learning framework.\r"
  },
  "cvpr2020_w11_spacenet6multi-sensorallweathermappingdataset": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "SpaceNet 6: Multi-Sensor All Weather Mapping Dataset",
    "authors": [
      "Jacob Shermeyer",
      "Daniel Hogan",
      "Jason Brown",
      "Adam Van Etten",
      "Nicholas Weir",
      "Fabio Pacifici",
      "Ronny Hansch",
      "Alexei Bastidas",
      "Scott Soenen",
      "Todd Bacastow",
      "Ryan Lewis"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Shermeyer_SpaceNet_6_Multi-Sensor_All_Weather_Mapping_Dataset_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Shermeyer_SpaceNet_6_Multi-Sensor_All_Weather_Mapping_Dataset_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Within the remote sensing domain, a diverse set of acquisition modalities exist, each with their own unique strengths and weaknesses. Yet, most of the current literature and open datasets only deal with electro-optical (optical) data for different detection and segmentation tasks at high spatial resolutions. optical data is often the preferred choice for geospatial applications, but requires clear skies and little cloud cover to work well. Conversely, Synthetic Aperture Radar (SAR) sensors have the unique capability to penetrate clouds and collect during all weather, day and night conditions. Consequently, SAR data are particularly valuable in the quest to aid disaster response, when weather and cloud cover can obstruct traditional optical sensors. Despite all of these advantages, there is little open data available to researchers to explore the effectiveness of SAR for such applications, particularly at very-high spatial resolutions, i.e. <1m Ground Sample Distance (GSD). To address this problem, we present an open Multi-Sensor All Weather Mapping (MSAW) dataset and challenge, which features two collection modalities (both SAR and optical). The dataset and challenge focus on mapping and building footprint extraction using a combination of these data sources. MSAW covers 120 km^2 over multiple overlapping collects and is annotated with over 48,000 unique building footprints labels, enabling the creation and evaluation of mapping algorithms for multi-modal data. We present a baseline and benchmark for building footprint extraction with SAR data and find that state-of-the-art segmentation models pre-trained on optical data, and then trained on SAR (F1 score of 0.21) outperform those trained on SAR data alone (F1 score of 0.135).\r"
  },
  "cvpr2020_w11_fgcndeepfeature-basedgraphconvolutionalnetworkforsemanticsegmentationofurban3dpointclouds": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "FGCN: Deep Feature-Based Graph Convolutional Network for Semantic Segmentation of Urban 3D Point Clouds",
    "authors": [
      "Saqib Ali Khan",
      "Yilei Shi",
      "Muhammad Shehzad",
      "Xiao Xiang Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Khan_FGCN_Deep_Feature-Based_Graph_Convolutional_Network_for_Semantic_Segmentation_of_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Khan_FGCN_Deep_Feature-Based_Graph_Convolutional_Network_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Directly processing 3D point clouds using convolutional neural networks (CNNs) is a highly challenging task primarily due to the lack of explicit neighborhood relationship between points in 3D space. Several researchers have tried to cope with this problem using a preprocessing step of voxelization. Although, this allows to translate the existing CNN architectures to process 3D point clouds but, in addition to computational and memory constraints, it poses quantization artifacts which limits the accurate inference of the underlying object's structure in the illuminated scene. In this paper, we have introduced a more stable and effective end-to-end architecture to classify raw 3D point clouds from indoor and outdoor scenes. In the proposed methodology, we encode the spatial arrangement of neighbouring 3D points inside an undirected symmetrical graph, which is passed along with features extracted from a 2D CNN to a Graph Convolutional Network (GCN) that contains three layers of localized graph convolutions to generate a complete segmentation map. The proposed network achieves on par or even better than state-of-the-art results on tasks like semantic scene parsing, part segmentation and urban classification on three standard benchmark datasets.\r"
  },
  "cvpr2020_w11_meta-learningforfew-shotlandcoverclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Meta-Learning for Few-Shot Land Cover Classification",
    "authors": [
      "Marc Russwurm",
      "Sherrie Wang",
      "Marco Korner",
      "David Lobell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Russwurm_Meta-Learning_for_Few-Shot_Land_Cover_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Russwurm_Meta-Learning_for_Few-Shot_Land_Cover_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The representations of the Earth's surface vary from one geographic region to another. For instance, the appearance of urban areas differs between continents, and seasonality influences the appearance of vegetation. To capture the diversity within a single category, such as urban or vegetation, requires a large model capacity and, consequently, large datasets. In this work, we propose a different perspective and view this diversity as an inductive transfer learning problem where few data samples from one region allow a model to adapt to an unseen region. We evaluate the model-agnostic meta-learning (MAML) algorithm on classification and segmentation tasks using globally and regionally distributed datasets. We find that few-shot model adaptation outperforms pre-training with regular gradient descent and fine-tuning on the (1) Sen12MS dataset and (2) DeepGlobe dataset when the source domain and target domain differ. This indicates that model optimization with meta-learning may benefit tasks in the Earth sciences whose data show a high degree of diversity from region to region, while traditional gradient-based supervised learning remains suitable in the absence of a feature or label shift.\r"
  },
  "cvpr2020_w11_toronto-3dalarge-scalemobilelidardatasetforsemanticsegmentationofurbanroadways": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Toronto-3D: A Large-Scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways",
    "authors": [
      "Weikai Tan",
      "Nannan Qin",
      "Lingfei Ma",
      "Ying Li",
      "Jing Du",
      "Guorong Cai",
      "Ke Yang",
      "Jonathan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Semantic segmentation of large-scale outdoor point clouds is essential for urban scene understanding in various applications, especially autonomous driving and urban high-definition (HD) mapping. With rapid developments of mobile laser scanning (MLS) systems, massive point clouds are available for scene understanding, but publicly accessible large-scale labeled datasets, which are essential for developing learning-based methods, are still limited. This paper introduces Toronto-3D, a large-scale urban outdoor point cloud dataset acquired by a MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of point clouds and consists of about 78.3 million points with 8 labeled object classes. Baseline experiments for semantic segmentation were conducted and the results confirmed the capability of this dataset to train deep learning models effectively. Toronto-3D is released to encourage new research, and the labels will be improved and updated with feedback from the research community.\r"
  },
  "cvpr2020_w11_deepregressionforimagingsolarmagnetogramsusingpyramidgenerativeadversarialnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Deep Regression for Imaging Solar Magnetograms Using Pyramid Generative Adversarial Networks",
    "authors": [
      "Rasha Alshehhi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Alshehhi_Deep_Regression_for_Imaging_Solar_Magnetograms_Using_Pyramid_Generative_Adversarial_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Alshehhi_Deep_Regression_for_Imaging_Solar_Magnetograms_Using_Pyramid_Generative_Adversarial_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Monitoring a large active region in the farside of the Sun is important for space weather forecasting. However, direct imaging of the farside is currently not available and usually physicists rely on seismic holography to infer farside magnetograms. On other hand, mapping between holography and magnetic images is non-trivial. In this work, Generative Adversarial Network (GAN) is used; which consists of a pyramid of modified pixel2pixel architectures to capture internal distributions at different scales with higher quality. Generative model is trained and evaluated using frontside of Solar Dynamic Observatory (SDO): Atmospheric Imaging Assembly (AIA) and Helioseismic and Magnetic Imager (HMI) magnetograms. Farside solar magnetograms from Extreme UltraViolet Imager (EUVI) farside data is also generated. The generative model successfully generates frontside solar magnetograms and outperforms state-of-the art method. It also help to monitor the magnetic changes from farside to frontside using generated solar magnetograms.\r"
  },
  "cvpr2020_w11_multi-imagesuper-resolutionforremotesensingusingdeeprecurrentnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Multi-Image Super-Resolution for Remote Sensing Using Deep Recurrent Networks",
    "authors": [
      "Md Rifat Arefin",
      "Vincent Michalski",
      "Pierre-Luc St-Charles",
      "Alfredo Kalaitzis",
      "Sookyung Kim",
      "Samira E. Kahou",
      "Yoshua Bengio"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Arefin_Multi-Image_Super-Resolution_for_Remote_Sensing_Using_Deep_Recurrent_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Arefin_Multi-Image_Super-Resolution_for_Remote_Sensing_Using_Deep_Recurrent_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " High-resolution satellite imagery is critical for various earth observation applications related to environment monitoring, geoscience, forecasting, and land use analysis. However, the acquisition cost of such high-quality imagery due to the scarcity of providers and needs for high-frequency revisits restricts its accessibility in many fields. In this work, we present a data-driven, multi-image super resolution approach to alleviate these problems. Our approach is based on an end-to-end deep neural network that consists of an encoder, a fusion module, and a decoder. The encoder extracts co-registered highly efficient feature representations from low-resolution images of a scene. A Gated Recurrent Unit (GRU)-based module acts as the fusion module, aggregating features into a combined representation. Finally, a decoder reconstructs the super-resolved image. The proposed model is evaluated on the PROBA-V dataset released in a recent competition held by the European Space Agency. Our results show that it performs among the top contenders and offers a new practical solution for real-world applications.\r"
  },
  "cvpr2020_w11_rasternetmodelingfree-flowspeedusinglidarandoverheadimagery": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "RasterNet: Modeling Free-Flow Speed Using LiDAR and Overhead Imagery",
    "authors": [
      "Armin Hadzic",
      "Hunter Blanton",
      "Weilian Song",
      "Mei Chen",
      "Scott Workman",
      "Nathan Jacobs"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Hadzic_RasterNet_Modeling_Free-Flow_Speed_Using_LiDAR_and_Overhead_Imagery_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Hadzic_RasterNet_Modeling_Free-Flow_Speed_Using_LiDAR_and_Overhead_Imagery_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Roadway free-flow speed captures the typical vehicle speed in low traffic conditions. Modeling free-flow speed is an important problem in transportation engineering with applications to a variety of design, operation, planning, and policy decisions of highway systems. Unfortunately, collecting large-scale historical traffic speed data is expensive and time consuming. Traditional approaches for estimating free-flow speed use geometric properties of the underlying road segment, such as grade, curvature, lane width, lateral clearance and access point density, but for most roads such features are often unavailable. We propose a fully automated approach, RasterNet, for estimating free-flow speed without the need for explicit geometric features. RasterNet is a neural network that fuses large-scale overhead imagery and aerial LiDAR point clouds using a geospatially consistent raster structure. To support training and evaluation, we introduce a novel dataset combining free-flow speeds of road segments, overhead imagery, and LiDAR point clouds across the state of Kentucky. Our method achieves state-of-the-art results on a benchmark dataset.\r"
  },
  "cvpr2020_w11_sen1floods11ageoreferenceddatasettotrainandtestdeeplearningfloodalgorithmsforsentinel-1": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w11",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - EarthVision: Large Scale Computer Vision for Remote Sensing Imagery",
    "title": "Sen1Floods11: A Georeferenced Dataset to Train and Test Deep Learning Flood Algorithms for Sentinel-1",
    "authors": [
      "Derrick Bonafilia",
      "Beth Tellman",
      "Tyler Anderson",
      "Erica Issenberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Accurate flood mapping at global scales can support disaster relief and recovery efforts. Improving flood relief with more accurate data is of great importance due to expected increases in the frequency and magnitude of flood events with climatic and demographic changes. To assist efforts to operationalize deep learning algorithms for flood mapping at global scales, we introduce Sen1Floods11, a surface water data set including classified permanent water, flood water, and raw Sentinel-1 imagery. This dataset consists of 4,831 512x512 chips covering 120,406 km\\textsuperscript 2and spans all 14 biomes, 357 ecoregions, and 6 continents of the world across 11 flood events. We used Sen1Floods11 to train, validate, and test fully convolutional neural networks (FCNN) to segment permanent and flood water. We compare results of classifying permanent, flood, and total surface water from training four FCNN models: i) 446 hand labeled chips of surface water from flood events; ii) 814 chips of publicly available permanent water data labels from Landsat (JRC surface water dataset); iii) 4385 chips of surface water classified from Sentinel-2 images from flood events and iv) 4385 chips of surface water classified from Sentinel-1 imagery from flood events. We compare these four models to a common remote sensing approach of thresholding radar backscatter to identify surface water. Future research to operationalize computer vision approaches to mapping flood and surface water could build new models from Sen1Floods11 and expand this dataset to include additional sensors and flood events. We provide Sen1Floods11, as well as our training and evaluation code at: https://github.com/cloudtostreet/Sen1Floods11\r"
  },
  "cvpr2020_w39_detectingcnn-generatedfacialimagesinreal-worldscenarios": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Detecting CNN-Generated Facial Images in Real-World Scenarios",
    "authors": [
      "Nils Hulzebosch",
      "Sarah Ibrahimi",
      "Marcel Worring"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Hulzebosch_Detecting_CNN-Generated_Facial_Images_in_Real-World_Scenarios_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Hulzebosch_Detecting_CNN-Generated_Facial_Images_in_Real-World_Scenarios_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Artificial, CNN-generated images are now of such high quality that humans have trouble distinguishing them from real images. Several algorithmic detection methods have been proposed, but these appear to generalize poorly to data from unknown sources, making them infeasible for real-world scenarios. In this work, we present a framework for evaluating detection methods under real-world conditions, consisting of cross-model, cross-data, and post-processing evaluation, and we evaluate state-of-the-art detection methods using the proposed framework. Furthermore, we examine the usefulness of commonly used image pre-processing methods. Lastly, we evaluate human performance on detecting CNN-generated images, along with factors that influence this performance, by conducting an online survey. Our results suggest that CNN-based detection methods are not yet robust enough to be used in real-world scenarios.\r"
  },
  "cvpr2020_w39_pipenetselectivemodalpipelineoffusionnetworkformulti-modalfaceanti-spoofing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "PipeNet: Selective Modal Pipeline of Fusion Network for Multi-Modal Face Anti-Spoofing",
    "authors": [
      "Qing Yang",
      "Xia Zhu",
      "Jong-Kae Fwu",
      "Yun Ye",
      "Ganmei You",
      "Yuan Zhu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Yang_PipeNet_Selective_Modal_Pipeline_of_Fusion_Network_for_Multi-Modal_Face_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Yang_PipeNet_Selective_Modal_Pipeline_of_Fusion_Network_for_Multi-Modal_Face_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Face anti-spoofing has become an increasingly important and critical security feature for authentication systems, due to rampant and easily launchable presentation attacks. Addressing the shortage of multi-modal face dataset, CASIA recently released the largest up-to-date CASIA-SURF Cross-ethnicity Face Anti-spoofing(CeFA) dataset, covering 3 ethnicities, 3 modalities, 1607 subjects, and 2D plus 3D attack types in four protocols, and focusing on the challenge of improving the generalization capability of face anti-spoofing in cross-ethnicity and multi-modal continuous data. In this paper, we propose a novel pipeline-based multi-stream CNN architecture called PipeNet for multi-modal face anti-spoofing. Unlike previous works, Selective Modal Pipeline (SMP) is designed to enable a customized pipeline for each data modality to take full advantage of multi-modal data. Limited Frame Vote (LFV) is designed to ensure stable and accurate predictions for video classification. The proposed method wins third place in the final ranking of Chalearn Multi-modal Cross-ethnicity Face Anti-spoofing Recognition Challenge@CVPR2020. Our final submission achieves the Average Classification Error Rate (ACER) of 2.21+-1.26 on the test set.\r"
  },
  "cvpr2020_w39_theroleofsignanddirectionofgradientontheperformanceofcnn": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "The Role of 'Sign' and 'Direction' of Gradient on the Performance of CNN",
    "authors": [
      "Akshay Agarwal",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Agarwal_The_Role_of_Sign_and_Direction_of_Gradient_on_the_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Agarwal_The_Role_of_Sign_and_Direction_of_Gradient_on_the_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " State-of-the-art deep learning models have achieved superlative performance across multiple computer vision applications such as object recognition, face recognition, and digits/character classification. Most of these models highly rely on the gradient information flows through the network for learning. By utilizing this gradient information, a simple gradient sign method based attack is developed to fool the deep learning models. However, the primary concern with this attack is the perceptibility of noise for large degradation in classification accuracy. This research address the question of whether an imperceptible gradient noise can be generated to fool the deep neural networks? For this, the role of sign function in the gradient attack is analyzed. The analysis shows that without-sign function, i.e. gradient magnitude, not only leads to a successful attack mechanism but the noise is also imperceptible to the human observer. Extensive quantitative experiments performed using two convolutional neural networks validate the above observation. For instance, AlexNet architecture yields 63.54% accuracy on the CIFAR-10 database which reduces to 0.0% and 26.39% when sign (i.e., perceptible) and without-sign (i.e., imperceptible) of the gradient is utilized, respectively. Further, the role of the direction of the gradient for image manipulation is studied. When an image is manipulated in the positive direction of the gradient, an adversarial image is generated. On the other hand, if the opposite direction of the gradient is utilized for image manipulation, it is observed that the classification error rate of the CNN model is reduced. On AlexNet, the error rate of 36.46% reduces to 4.29% when images of CIFAR-10 are manipulated in the negative direction of the gradient.\r"
  },
  "cvpr2020_w39_adversarialattackondeeplearning-basedsplicelocalization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Adversarial Attack on Deep Learning-Based Splice Localization",
    "authors": [
      "Andras Rozsa",
      "Zheng Zhong",
      "Terrance E. Boult"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Rozsa_Adversarial_Attack_on_Deep_Learning-Based_Splice_Localization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Rozsa_Adversarial_Attack_on_Deep_Learning-Based_Splice_Localization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Regarding image forensics, researchers have proposed various approaches to detect and/or localize manipulations, such as splices. Recent best performing image-forensics algorithms greatly benefit from the application of deep learning, but such tools can be vulnerable to adversarial attacks. Due to the fact that most of the proposed adversarial example generation techniques can be used only on end-to-end classifiers, the adversarial robustness of image-forensics methods that utilize deep learning only for feature extraction has not been studied yet. Using a novel algorithm capable of directly adjusting the underlying representations of patches we demonstrate on three non end-to-end deep learning-based splice localization tools that hiding manipulations of images is feasible via adversarial attacks. While the tested image-forensics methods, EXIF-SC, SpliceRadar, and Noiseprint, rely on feature extractors that were trained on different surrogate tasks, we find that the formed adversarial perturbations can be transferable among them regarding the deterioration of their localization performance.\r"
  },
  "cvpr2020_w39_multi-modalfaceanti-spoofingbasedoncentraldifferencenetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Multi-Modal Face Anti-Spoofing Based on Central Difference Networks",
    "authors": [
      "Zitong Yu",
      "Yunxiao Qin",
      "Xiaobai Li",
      "Zezheng Wang",
      "Chenxu Zhao",
      "Zhen Lei",
      "Guoying Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Yu_Multi-Modal_Face_Anti-Spoofing_Based_on_Central_Difference_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Yu_Multi-Modal_Face_Anti-Spoofing_Based_on_Central_Difference_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Face anti-spoofing (FAS) plays a vital role in securing face recognition systems from presentation attacks. Existing multi-modal FAS methods rely on stacked vanilla convolutions, which is weak in describing detailed intrinsic information from modalities and easily being ineffective when the domain shifts (e.g., cross attack and cross ethnicity). In this paper, we extend the central difference convolutional networks (CDCN) [??] to a multi-modal version, intending to capture intrinsic spoofing patterns among three modalities (RGB, depth and infrared). Meanwhile, we also give an elaborate study about single-modal based CDCN. Our approach won the first place in \"Track Multi-Modal\" as well as the second place in \"Track Single-Modal (RGB)\" of ChaLearn Face Anti-spoofing Attack Detection Challenge@CVPR2020 [??]. Our final submission obtains 1.02\\pm0.59% and 4.84\\pm1.79% ACER in \"Track Multi-Modal\" and \"Track Single-Modal (RGB)\", respectively.\r"
  },
  "cvpr2020_w39_fakenewsdetectionusinghigher-orderusertousermutual-attentionprogressioninpropagationpaths": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Fake News Detection Using Higher-Order User to User Mutual-Attention Progression in Propagation Paths",
    "authors": [
      "Rahul Mishra"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Mishra_Fake_News_Detection_Using_Higher-Order_User_to_User_Mutual-Attention_Progression_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Mishra_Fake_News_Detection_Using_Higher-Order_User_to_User_Mutual-Attention_Progression_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Social media has become a very prominent source of news consumption. It brings forth multifaceted, multimodal and real-time information on a silver platter for the users. Fake news or rumor mongering on social media is one of the most challenging issues pertaining to present web. Previously, researchers have tried to classify news propagation paths on social media (e.g. Twitter) to detect fake news. However, they do not utilize latent relationships among users efficiently to model the influence of the users with high prestige on the other users, which is a very significant factor in information propagation. In this paper, we propose a novel Higher-order User to User Mutual-attention Progression (HiMaP) method to capture the cues related to authority or influence of the users by modelling direct and indirect (multi-hop) influence relationships among each pair of users, present in the propagation sequence. The proposed higher order attention trick is a novel contribution which can also be very effective in case of transformer architectures. Our model not only outperforms the state-of-the-art methods on two publicly available Twitter datasets but also explains the propagation patterns pertaining to fake news by visualizing higher order mutual-attentions.\r"
  },
  "cvpr2020_w39_towardsuntrustedsocialvideoverificationtocombatdeepfakesviafacegeometryconsistency": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Towards Untrusted Social Video Verification to Combat Deepfakes via Face Geometry Consistency",
    "authors": [
      "Eleanor Tursman",
      "Marilyn George",
      "Seny Kamara",
      "James Tompkin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Tursman_Towards_Untrusted_Social_Video_Verification_to_Combat_Deepfakes_via_Face_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Tursman_Towards_Untrusted_Social_Video_Verification_to_Combat_Deepfakes_via_Face_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deepfakes can spread misinformation, defamation, and propaganda by faking videos of public speakers. We assume that future deepfakes will be visually indistinguishable from real video, and will also fool current deepfake detection methods. As such, we posit a social verification system that instead validates the truth of an event via a set of videos. To confirm which, if any, videos are being faked at any point in time, we check for consistent facial geometry across videos. We demonstrate that by comparing mouth movement across views using a combination of PCA and hierarchical clustering, we can detect a deepfake with subtle mouth manipulations out of a set of six videos at high accuracy. Using our new multi-view dataset of 25 speakers, we show that our performance gracefully decays as we increase the number of identically faked videos from different input views.\r"
  },
  "cvpr2020_w39_oc-fakedectclassifyingdeepfakesusingone-classvariationalautoencoder": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "OC-FakeDect: Classifying Deepfakes Using One-Class Variational Autoencoder",
    "authors": [
      "Hasam Khalid",
      "Simon S. Woo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Khalid_OC-FakeDect_Classifying_Deepfakes_Using_One-Class_Variational_Autoencoder_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Khalid_OC-FakeDect_Classifying_Deepfakes_Using_One-Class_Variational_Autoencoder_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " An image forgery method called Deepfakes can cause security and privacy issues by changing the identity of a person in a photo through the replacement of his/her face with a computer-generated image or another person's face. Therefore, a new challenge of detecting Deepfakes arises to protect individuals from potential misuses. Many researchers have proposed various binary-classification based detection approaches to detect deepfakes. However, binary-classification based methods generally require a large amount of both real and fake face images for training, and it is challenging to collect sufficient fake images data in advance. Besides, when new deepfakes generation methods are introduced, little deepfakes data will be available, and the detection performance may be mediocre. To overcome these data scarcity limitations, we formulate deepfakes detection as a one-class anomaly detection problem. We propose OC-FakeDect, which uses a one-class Variational Autoencoder (VAE) to train only on real face images and detects non-real images such as deepfakes by treating them as anomalies. Our preliminary result shows that our one class-based approach can be promising when detecting Deepfakes, achieving a 97.5% accuracy on the NeuralTextures data of the well-known FaceForensics++ benchmark dataset without using any fake images for the training process.\r"
  },
  "cvpr2020_w39_evadingdeepfake-imagedetectorswithwhite-andblack-boxattacks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Evading Deepfake-Image Detectors With White- and Black-Box Attacks",
    "authors": [
      "Nicholas Carlini",
      "Hany Farid"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Carlini_Evading_Deepfake-Image_Detectors_With_White-_and_Black-Box_Attacks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Carlini_Evading_Deepfake-Image_Detectors_With_White-_and_Black-Box_Attacks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " It is now possible to synthesize highly realistic images of people who do not exist. Such content has, for example, been implicated in the creation of fraudulent social-media profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content. We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near-0% accuracy. We develop five attack case studies on a state-of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.\r"
  },
  "cvpr2020_w39_detectingdeep-fakevideosfromphoneme-visememismatches": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Detecting Deep-Fake Videos From Phoneme-Viseme Mismatches",
    "authors": [
      "Shruti Agarwal",
      "Hany Farid",
      "Ohad Fried",
      "Maneesh Agrawala"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Agarwal_Detecting_Deep-Fake_Videos_From_Phoneme-Viseme_Mismatches_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Agarwal_Detecting_Deep-Fake_Videos_From_Phoneme-Viseme_Mismatches_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent advances in machine learning and computer graphics have made it easier to convincingly manipulate video and audio. These so-called deep-fake videos range from complete full-face synthesis and replacement (face-swap), to complete mouth and audio synthesis and replacement (lip-sync), and partial word-based audio and mouth synthesis and replacement. Detection of deep fakes with only a small spatial and temporal manipulation is particularly challenging. We describe a technique to detect such manipulated videos by exploiting the fact that the dynamics of the mouth shape -- visemes -- are occasionally inconsistent with a spoken phoneme. We focus on the visemes associated with words having the sound M (mama), B (baba), or P (papa) in which the mouth must completely close in order to pronounce these phonemes. We observe that this is not the case in many deep-fake videos. Such phoneme-viseme mismatches can, therefore, be used to detect even spatially small and temporally localized manipulations. We demonstrate the efficacy and robustness of this approach to detect different types of deep-fake videos, including in-the-wild deep fakes.\r"
  },
  "cvpr2020_w39_forgerydetectioninhyperspectraldocumentimagesusinggraphorthogonalnonnegativematrixfactorization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Forgery Detection in Hyperspectral Document Images Using Graph Orthogonal Nonnegative Matrix Factorization",
    "authors": [
      "Abderrahmane Rahiche",
      "Mohamed Cheriet"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Rahiche_Forgery_Detection_in_Hyperspectral_Document_Images_Using_Graph_Orthogonal_Nonnegative_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Rahiche_Forgery_Detection_in_Hyperspectral_Document_Images_Using_Graph_Orthogonal_Nonnegative_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The analysis of inks plays a crucial role in the examination process of questioned documents. To address this issue, we propose a new approach for ink mismatch detection in Hyperspectral document (HSD) images based on a new orthogonal and graph regularized Nonnegative Matrix Factorization (NMF) model. Although some previous works have proposed orthogonality constraints to solve clustering problems in different contexts, the application of such constraints is not straightforward due to the sum-to-one assumption related to the physical nature of Hyperspectral images. In this work, we demonstrate that under some acquisition protocols, latent factors in HSD images can be constrained to be orthogonal. We also incorporate a graph regularized term to exploit the geometric information lost by the matricization of HSD images. Furthermore, we propose an efficient alternating direction method of multipliers based algorithm to solve the proposed method. Our empirical validation demonstrates the competitiveness of the proposed algorithm compared to the state-of-the-art methods. It shows a high potential to be used as a reliable tool for quick investigation of questioned documents.\r"
  },
  "cvpr2020_w39_manipulationdetectioninsatelliteimagesusingdeepbeliefnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Manipulation Detection in Satellite Images Using Deep Belief Networks",
    "authors": [
      "Janos Horvath",
      "Daniel Mas Montserrat",
      "Hanxiang Hao",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Horvath_Manipulation_Detection_in_Satellite_Images_Using_Deep_Belief_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Horvath_Manipulation_Detection_in_Satellite_Images_Using_Deep_Belief_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Satellite images are more accessible with the increase of commercial satellites being orbited. These images are used in a wide range of applications including agricultural management, meteorological prediction, damage assessment from natural disasters and cartography. Image manipulation tools including both manual editing tools and automated techniques can be easily used to tamper and modify satellite imagery. One type of manipulation that we examine in this paper is the splice attack where a region from one image (or the same image) is inserted (\"spliced\") into an image. In this paper, we present a one-class detection method based on deep belief networks (DBN) for splicing detection and localization without using any prior knowledge of the manipulations. We evaluate the performance of our approach and show that it provides good detection and localization accuracies in small forgeries compared to other approaches.\r"
  },
  "cvpr2020_w39_deepfakedetectionbyanalyzingconvolutionaltraces": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "DeepFake Detection by Analyzing Convolutional Traces",
    "authors": [
      "Luca Guarnera",
      "Oliver Giudice",
      "Sebastiano Battiato"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Guarnera_DeepFake_Detection_by_Analyzing_Convolutional_Traces_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Guarnera_DeepFake_Detection_by_Analyzing_Convolutional_Traces_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The Deepfake phenomenon has become very popular nowadays thanks to the possibility to create incredibly realistic images using deep learning tools, based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we focus on the analysis of Deepfakes of human faces with the objective of creating a new detection method able to detect a forensics trace hidden in images: a sort of fingerprint left in the image generation process. The proposed technique, by means of an Expectation Maximization (EM) algorithm, extracts a set of local features specifically addressed to model the underlying convolutional generative process. Ad-hoc validation has been employed through experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as ground-truth for non-fakes. Results demonstrated the effectiveness of the technique in distinguishing the different architectures and the corresponding generation process.\r"
  },
  "cvpr2020_w39_deepfakesdetectionwithautomaticfaceweighting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Deepfakes Detection With Automatic Face Weighting",
    "authors": [
      "Daniel Mas Montserrat",
      "Hanxiang Hao",
      "Sri K. Yarlagadda",
      "Sriram Baireddy",
      "Ruiting Shao",
      "Janos Horvath",
      "Emily Bartusiak",
      "Justin Yang",
      "David Guera",
      "Fengqing Zhu",
      "Edward J. Delp"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Montserrat_Deepfakes_Detection_With_Automatic_Face_Weighting_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Montserrat_Deepfakes_Detection_With_Automatic_Face_Weighting_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Altered and manipulated multimedia is increasingly present and widely distributed via social media platforms. Advanced video manipulation tools enable the generation of highly realistic-looking altered multimedia. While many methods have been presented to detect manipulations, most of them fail when evaluated with data outside of the datasets used in research environments. In order to address this problem, the Deepfake Detection Challenge (DFDC) provides a large dataset of videos containing realistic manipulations and an evaluation system that ensures that methods work quickly and accurately, even when faced with challenging data. In this paper, we introduce a method based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that extracts visual and temporal features from faces present in videos to accurately detect manipulations. The method is evaluated with the DFDC dataset, providing competitive results compared to other techniques.\r"
  },
  "cvpr2020_w39_detectingvideospeedmanipulation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Detecting Video Speed Manipulation",
    "authors": [
      "Brian C. Hosler",
      "Matthew C. Stamm"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Hosler_Detecting_Video_Speed_Manipulation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Hosler_Detecting_Video_Speed_Manipulation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Manipulated videos are frequently an important part of misinformation campaigns. While much attention has recently focused on sophisticated threats such as deepfakes, the majority of videos used in misinformation campaigns have been created using relatively simple manipulations that can be produced by commonly available editing software. One important manipulation that has been used in previous misinformation attempts is altering the speed of a video. In the previous 18 months, widely circulated videos have had their speed manipulated to make Speaker of the House Nancy Pelosi appear disoriented, and to make reporter Jim Acosta appear to act aggressively toward a White House staffer. Currently, however, there are no approaches to accurately detect video speed manipulation that can be deployed at scale. In this paper, we propose new algorithms to detect video speed manipulation and to estimate the rate by which a video's speed has been modified. To do this, we identify a new trace left by video speed manipulation and show how it can be extracted from a video. Our approaches to trace extraction, speed manipulation detection, and manipulation rate estimation are computationally efficient and can be run in a matter of milliseconds. We present experimental results that show that our proposed approach can detect manipulated videos with up to 99% accuracy.\r"
  },
  "cvpr2020_w39_detectinggansandretouchingbaseddigitalalterationsviadad-hcnn": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w39",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Media Forensics",
    "title": "Detecting GANs and Retouching Based Digital Alterations via DAD-HCNN",
    "authors": [
      "Anubhav Jain",
      "Puspita Majumdar",
      "Richa Singh",
      "Mayank Vatsa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w39/Jain_Detecting_GANs_and_Retouching_Based_Digital_Alterations_via_DAD-HCNN_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w39/Jain_Detecting_GANs_and_Retouching_Based_Digital_Alterations_via_DAD-HCNN_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While image generation and editing technologies such as Generative Adversarial Networks and Photoshop are being used for creative and positive applications, the misuse of these technologies to create negative applications including Deep-nude and fake news is also increasing at a rampant pace. Therefore, detecting digitally created and digitally altered images is of paramount importance. This paper proposes a hierarchical approach termed as DAD-HCNN which performs two-fold task: (i) it differentiates between digitally generated images and digitally retouched images from the original unaltered images, and (ii) to increase the explainability of the decision, it also identifies the GAN architecture used to create the image. The effectiveness of the model is demonstrated on a database generated by combining face images generated from four different GAN architectures along with the retouched images and original images from existing benchmark databases.\r"
  },
  "cvpr2020_w38_towardreal-worldpanoramicimageenhancement": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Toward Real-World Panoramic Image Enhancement",
    "authors": [
      "Yupeng Zhang",
      "Hengzhi Zhang",
      "Daojing Li",
      "Liyan Liu",
      "Hong Yi",
      "Wei Wang",
      "Hiroshi Suitoh",
      "Makoto Odamaki"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Zhang_Toward_Real-World_Panoramic_Image_Enhancement_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Zhang_Toward_Real-World_Panoramic_Image_Enhancement_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Panoramic images captured by the fisheye lens cameras cover very wide field of view (FoV) ranging from 180deg to 360deg, but the image quality is very low compared to that of high-end cameras such as DSLR or compact cameras with APS-C or full frame sensors. In this paper, we aim to use deep neural network (DNN) based methods to improve panoramic image quality. Specifically, we enhance low quality panoramic images of 5K resolution (5376x2688) to high-end camera quality at the same resolution, which is good for applications that requires limited resources, low-cost but high image quality. We build a Panoramic-High-end dataset which is the first real world panoramic image dataset as far as we know. Based on the generative adversarial network (GAN) architecture, we also design a compact network employing multi-frequency structure with compressed Residual-in-Residual Dense Blocks (RRDBs) and convolution layers from each dense block. Experiments show that our method surpasses several state-of-the-art DNN based methods in both no-reference and full-reference evaluations as well as the processing speed. Our results show that it's practical to integrate DNN based image enhancer into optics design to achieve a balance between optical cost and image quality.\r"
  },
  "cvpr2020_w38_adeepphysicalmodelforsolarirradianceforecastingwithfisheyeimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "A Deep Physical Model for Solar Irradiance Forecasting With Fisheye Images",
    "authors": [
      "Vincent Le Guen",
      "Nicolas Thome"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Le_Guen_A_Deep_Physical_Model_for_Solar_Irradiance_Forecasting_With_Fisheye_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Le_Guen_A_Deep_Physical_Model_for_Solar_Irradiance_Forecasting_With_Fisheye_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a new deep learning approach for short-term solar irradiance forecasting based on fisheye images. Our architecture, based on recent works on video prediction with partial differential equations, extracts spatio-temporal features modelling cloud motion to accurately anticipate future solar irradiance. Our method obtains state-of-the-art results on video prediction and 5 min ahead irradiance forecasting against strong recent baselines, highlighting the benefits of incorporating physical knowledge in deep models for real-world physical process forecasting.\r"
  },
  "cvpr2020_w38_uprightandstabilizedomnidirectionaldepthestimationforwide-baselinemulti-camerainertialsystems": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Upright and Stabilized Omnidirectional Depth Estimation for Wide-Baseline Multi-Camera Inertial Systems",
    "authors": [
      "Changhee Won",
      "Hochang Seok",
      "Jongwoo Lim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Won_Upright_and_Stabilized_Omnidirectional_Depth_Estimation_for_Wide-Baseline_Multi-Camera_Inertial_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Won_Upright_and_Stabilized_Omnidirectional_Depth_Estimation_for_Wide-Baseline_Multi-Camera_Inertial_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper presents an upright and stabilized omnidirectional depth estimation for an arbitrarily rotated wide-baseline multi-camera inertial system. By aligning the reference rig coordinate system with the gravity direction acquired from an inertial measurement unit, we sample depth hypotheses for omnidirectional stereo matching by sweeping global spheres whose equators are parallel to the ground plane. Then, unary features extracted from each input image by 2D convolutional neural networks (CNN) are warped onto the swept spheres, and the final omnidirectional depth map is output through cost computation by a 3D CNN-based hourglass module and a softargmax operation. This can eliminate wavy or unrecognizable visual artifacts in equirectangular depth maps which can cause failures in scene understanding. We show the capability of our upright and stabilized omnidirectional depth estimation through experiments on real data.\r"
  },
  "cvpr2020_w38_arucomnidetectionofhighlyreliablefiducialmarkersinpanoramicimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "ArUcOmni: Detection of Highly Reliable Fiducial Markers in Panoramic Images",
    "authors": [
      "Jaouad Hajjami",
      "Jordan Caracotte",
      "Guillaume Caron",
      "Thibault Napoleon"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Hajjami_ArUcOmni_Detection_of_Highly_Reliable_Fiducial_Markers_in_Panoramic_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Hajjami_ArUcOmni_Detection_of_Highly_Reliable_Fiducial_Markers_in_Panoramic_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose an adaptation of marker detection algorithm for panoramic cameras such as catadioptric and fisheye sensors. Due to distortions and non-uniform resolution of such sensors, the methods that are commonly used in perspective images cannot be applied directly. This work is in contrast with the existing marker detection framework: Automatic reliable fiducial markers Under occlusion (ArUco) for a conventional camera. To keep the same performance for panoramic cameras, our method is based on a spherical representation of the image that allows the marker to be detected and to estimate its 3D pose. We evaluate our approach on a new shared dataset that consists of a 3D rig of markers taken with two different sensors: a catadioptric camera and a fisheye camera. The evaluation has been performed against ArUco algorithm without rectification and with one of the rectified approaches based on the fisheye model.\r"
  },
  "cvpr2020_w38_rapidrotation-awarepeopledetectioninoverheadfisheyeimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "RAPiD: Rotation-Aware People Detection in Overhead Fisheye Images",
    "authors": [
      "Zhihao Duan",
      "Ozan Tezcan",
      "Hayato Nakamura",
      "Prakash Ishwar",
      "Janusz Konrad"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Duan_RAPiD_Rotation-Aware_People_Detection_in_Overhead_Fisheye_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Duan_RAPiD_Rotation-Aware_People_Detection_in_Overhead_Fisheye_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent methods for people detection in overhead, fisheye images either use radially-aligned bounding boxes to represent people, assuming people always appear along image radius or require significant pre-/post-processing which radically increases computational complexity. In this work, we develop an end-to-end rotation-aware people detection method, named RAPiD, that detects people using arbitrarily-oriented bounding boxes. Our fully convolutional neural network directly regresses the angle of each bounding box using a periodic loss function, which accounts for angle periodicities. We have also created a new dataset with spatio-temporal annotations of rotated bounding boxes, for people detection as well as other vision tasks in overhead fisheye videos. We show that our simple, yet effective method outperforms state-of-the-art results on three fisheye-image datasets. The source code for RAPiD is publicly available.\r"
  },
  "cvpr2020_w38_unsupervisedlearningofmetricrepresentationswithslowfeaturesfromomnidirectionalviews": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Unsupervised Learning of Metric Representations With Slow Features From Omnidirectional Views",
    "authors": [
      "Mathias Franzius",
      "Benjamin Metka",
      "Muhammad Haris",
      "Ute Bauer-Wersing"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Franzius_Unsupervised_Learning_of_Metric_Representations_With_Slow_Features_From_Omnidirectional_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Franzius_Unsupervised_Learning_of_Metric_Representations_With_Slow_Features_From_Omnidirectional_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Unsupervised learning of Self-Localization with Slow Feature Analysis (SFA) using omnidirectional camera input has been shown to be a viable alternative to established SLAM approaches. Previous models for SFA self-localization purely relied on omnidirectional visual input. The model led to globally consistent localization in SFA space but the lack of odometry integration reduced the local accuracy. However, odometry integration and other downstream usage of localization require a common coordinate system, which previously was based on an external metric ground truth measurement system. Here, we show an autonomous unsupervised approach to generate accurate metric representations from SFA outputs without external sensors. We assume locally linear trajectories of a robot, which is consistent with, for example, driving patterns of robotic lawn mowers. This geometric constraint allows a formulation of an optimization problem for the regression from slow feature values to the robot's position. We show that the resulting accuracy on test data is comparable to supervised regression based on external sensors. Based on this result, using a Kalman filter for fusion of SFA localization and odometry is shown to further increase localization accuracy over the supervised regression model.\r"
  },
  "cvpr2020_w38_deeplightingenvironmentmapestimationfromsphericalpanoramas": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w38",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Omnidirectional Computer Vision in Research and Industry",
    "title": "Deep Lighting Environment Map Estimation From Spherical Panoramas",
    "authors": [
      "Vasileios Gkitsas",
      "Nikolaos Zioulis",
      "Federico Alvarez",
      "Dimitrios Zarpalas",
      "Petros Daras"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w38/Gkitsas_Deep_Lighting_Environment_Map_Estimation_From_Spherical_Panoramas_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w38/Gkitsas_Deep_Lighting_Environment_Map_Estimation_From_Spherical_Panoramas_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Estimating a scene's lighting is a very important task when compositing synthetic content within real environments, with applications in mixed reality and post-production. In this work we present a data-driven model that estimates an HDR lighting environment map from a single LDR monocular spherical panorama. In addition to being a challenging and ill-posed problem, the lighting estimation task also suffers from a lack of facile illumination ground truth data, a fact that hinders the applicability of data-driven methods. We approach this problem differently, exploiting the availability of surface geometry to employ image-based relighting as a data generator and supervision mechanism. This relies on a global Lambertian assumption that helps us overcome issues related to pre-baked lighting. We relight our training data and complement the model's supervision with a photometric loss, enabled by a differentiable image-based relighting technique. Finally, since we predict spherical spectral coefficients, we show that by imposing a distribution prior on the predicted coefficients, we can greatly boost performance. Code and models available at https://vcl3d.github.io/DeepPanoramaLighting.\r"
  },
  "cvpr2020_w15_lifelongmachinelearningwithdeepstreaminglineardiscriminantanalysis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Lifelong Machine Learning With Deep Streaming Linear Discriminant Analysis",
    "authors": [
      "Tyler L. Hayes",
      "Christopher Kanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated, which prohibits many applications. An alternative paradigm is online learning in a single pass through the training dataset on a resource constrained budget, which is known as streaming learning. Streaming learning has been much less studied in the deep learning community. In streaming learning, an agent learns instances one-by-one and can be tested at any time, rather than only after learning a large batch. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet ILSVRC-2012 and CORe50, a dataset that involves learning to classify from temporally ordered samples.\r"
  },
  "cvpr2020_w15_cognitively-inspiredmodelforincrementallearningusingafewexamples": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Cognitively-Inspired Model for Incremental Learning Using a Few Examples",
    "authors": [
      "Ali Ayub",
      "Alan R. Wagner"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Ayub_Cognitively-Inspired_Model_for_Incremental_Learning_Using_a_Few_Examples_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Ayub_Cognitively-Inspired_Model_for_Incremental_Learning_Using_a_Few_Examples_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Incremental learning attempts to develop a classifier which learns continuously from a stream of data segregated into different classes. Deep learning approaches suffer from catastrophic forgetting when learning classes incrementally, while most incremental learning approaches require a large amount of training data per class. We examine the problem of incremental learning using only a few training examples, referred to as Few-Shot Incremental Learning (FSIL). To solve this problem, we propose a novel approach inspired by the concept learning model of the hippocampus and the neocortex that represents each image class as centroids and does not suffer from catastrophic forgetting. We evaluate our approach on three class-incremental learning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental and few-shot incremental learning and show that our approach achieves state-of-the-art results in terms of classification accuracy over all learned classes.\r"
  },
  "cvpr2020_w15_continuallearningofobjectinstances": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Continual Learning of Object Instances",
    "authors": [
      "Kishan Parshotam",
      "Mert Kilickaya"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Parshotam_Continual_Learning_of_Object_Instances_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Parshotam_Continual_Learning_of_Object_Instances_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose continual instance learning - a method that applies the concept of continual learning to the task of distinguishing instances of the same object category. We specifically focus on the car object, and incrementally learn to distinguish car instances from each other with metric learning. We begin our paper by evaluating current techniques. Establishing that catastrophic forgetting is evident in existing methods, we then propose two remedies. Firstly, we regularise metric learning via Normalised Cross-Entropy. Secondly, we augment existing models with synthetic data transfer. Our extensive experiments on three large-scale datasets, using two different architectures for five different continual learning methods, reveal that Normalised cross-entropy and synthetic transfer leads to less forgetting in existing techniques.\r"
  },
  "cvpr2020_w15_generativefeaturereplayforclass-incrementallearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Generative Feature Replay for Class-Incremental Learning",
    "authors": [
      "Xialei Liu",
      "Chenshen Wu",
      "Mikel Menta",
      "Luis Herranz",
      "Bogdan Raducanu",
      "Andrew D. Bagdanov",
      "Shangling Jui",
      "Joost van de Weijer"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Liu_Generative_Feature_Replay_for_Class-Incremental_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Liu_Generative_Feature_Replay_for_Class-Incremental_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Humans are capable of learning new tasks without forgetting previous ones, while neural networks fail due to catastrophic forgetting between new and previously-learned tasks. We consider a class-incremental setting which means that the task-ID is unknown at inference time. The imbalance between old and new classes typically results in a bias of the network towards the newest ones. This imbalance problem can either be addressed by storing exemplars from previous tasks, or by using image replay methods. However, the latter can only be applied to toy datasets since image generation for complex datasets is a hard problem. We propose a solution to the imbalance problem based on generative feature replay which does not require any exemplars. To do this, we split the network into two parts: a feature extractor and a classifier. To prevent forgetting, we combine generative feature replay in the classifier with feature distillation in the feature extractor. Through feature generation, our method reduces the complexity of generative replay and prevents the imbalance problem. Our approach is computationally efficient and scalable to large datasets. Experiments confirm that our approach achieves state-of-the-art results on CIFAR-100 and ImageNet, while requiring only a fraction of the storage needed for exemplar-based continual learning.\r"
  },
  "cvpr2020_w15_stream-51streamingclassificationandnoveltydetectionfromvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Stream-51: Streaming Classification and Novelty Detection From Videos",
    "authors": [
      "Ryne Roady",
      "Tyler L. Hayes",
      "Hitesh Vaidya",
      "Christopher Kanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep neural networks are popular for visual perception tasks such as image classification and object detection. Once trained and deployed in a real-time environment, these models struggle to identify novel inputs not initially represented in the training distribution. Further, they cannot be easily updated on new information or they will catastrophically forget previously learned knowledge. While there has been much interest in developing models capable of overcoming forgetting, most research has focused on incrementally learning from common image classification datasets broken up into large batches. Online streaming learning is a more realistic paradigm where a model must learn one sample at a time from temporally correlated data streams. Although there are a few datasets designed specifically for this protocol, most have limitations such as few classes or poor image quality. In this work, we introduce Stream-51, a new dataset for streaming classification consisting of temporally correlated images from 51 distinct object categories and additional evaluation classes outside of the training distribution to test novelty recognition. We establish unique evaluation protocols, experimental metrics, and baselines for our dataset in the streaming paradigm.\r"
  },
  "cvpr2020_w15_catnetclassincremental3dconvnetsforlifelongegocentricgesturerecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture Recognition",
    "authors": [
      "Zhengwei Wang",
      "Qi She",
      "Tejo Chalasani",
      "Aljosa Smolic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Wang_CatNet_Class_Incremental_3D_ConvNets_for_Lifelong_Egocentric_Gesture_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Wang_CatNet_Class_Incremental_3D_ConvNets_for_Lifelong_Egocentric_Gesture_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Egocentric gestures are the most natural form of communication for humans to interact with wearable devices such as VR/AR helmets and glasses. A major issue in such scenarios for real-world applications is that may easily become necessary to add new gestures to the system e.g., a proper VR system should allow users to customize gestures incrementally. Traditional deep learning methods require storing all previous class samples in the system and training the model again from scratch by incorporating previous samples and new samples, which costs humongous memory and significantly increases computation over time. In this work, we demonstrate a lifelong 3D convolutional framework -- c(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal information in videos and enables lifelong learning for egocentric gesture video recognition by learning the feature representation of an exemplar set selected from previous class samples. Importantly, we propose a two-stream CatNet, which deploys RGB and depth modalities to train two separate networks. We evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and show that CatNets can learn many classes incrementally over a long period of time. Results also demonstrate that the two-stream architecture achieves the best performance on both joint training and class incremental training compared to 3 other one-stream architectures. The codes and pre-trained models used in this work will be shortly available.\r"
  },
  "cvpr2020_w15_dropoutasanimplicitgatingmechanismforcontinuallearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Dropout as an Implicit Gating Mechanism for Continual Learning",
    "authors": [
      "Seyed Iman Mirzadeh",
      "Mehrdad Farajtabar",
      "Hassan Ghasemzadeh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Mirzadeh_Dropout_as_an_Implicit_Gating_Mechanism_for_Continual_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Mirzadeh_Dropout_as_an_Implicit_Gating_Mechanism_for_Continual_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In recent years, neural networks have demonstrated an outstanding ability to achieve complex learning tasks across various domains. However, they suffer from the \"catastrophic forgetting\" problem when they face a sequence of learning tasks, where they forget the old ones as they learn new tasks. This problem is also highly related to the \"stability-plasticity dilemma\". The more plastic the network, the easier it can learn new tasks, but the faster it also forgets previous ones. Conversely, a stable network cannot learn new tasks as fast as a very plastic network. However, it is more reliable to preserve the knowledge it has learned from the previous tasks. Several solutions have been proposed to overcome the forgetting problem by making the neural network parameters more stable, and some of them have mentioned the significance of dropout in continual learning. However, their relationship has not been sufficiently studied yet. In this paper, we investigate this relationship and show that a stable network with dropout learns a gating mechanism such that for different tasks, different paths of the network are active. Our experiments show that the stability achieved by this implicit gating plays a very critical role in leading to performance comparable to or better than other involved continual learning algorithms to overcome catastrophic forgetting.\r"
  },
  "cvpr2020_w15_whatishappeninginsideacontinuallearningmodel?arepresentation-basedevaluationofrepresentationalforgetting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "What Is Happening Inside a Continual Learning Model? A Representation-Based Evaluation of Representational Forgetting",
    "authors": [
      "Kengo Murata",
      "Tetsuya Toyota",
      "Kouzou Ohara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Murata_What_Is_Happening_Inside_a_Continual_Learning_Model_A_Representation-Based_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Murata_What_Is_Happening_Inside_a_Continual_Learning_Model_A_Representation-Based_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recently, many continual learning methods have been proposed, and their performance is usually evaluated based on their final output such as the class they predicted. However, this output-based evaluation cannot tell us anything about how representations the model learned from given tasks are forgotten during learning process inside the model although understanding it is important to devise a robust algorithm to catastrophic forgetting that is an intrinsic problem in continual learning. In this work, we propose a representation-based evaluation framework and demonstrate it can help us better understand the representational forgetting through intensive experiments on three benchmark datasets, which eventually brought us the following findings: 1) non-negligible amount of representational forgetting appears at shallow layers of a deep neural network model, and 2) which tasks are more accurately learned when representational forgetting occurred depends on the depth of the layer at which the representational forgetting is observed.\r"
  },
  "cvpr2020_w15_m2sgdlearningtolearnimportantweights": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "M2SGD: Learning to Learn Important Weights",
    "authors": [
      "Nicholas I-Hsien Kuo",
      "Mehrtash Harandi",
      "Nicolas Fourrier",
      "Christian Walder",
      "Gabriela Ferraro",
      "Hanna Suominen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Kuo_M2SGD_Learning_to_Learn_Important_Weights_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Kuo_M2SGD_Learning_to_Learn_Important_Weights_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Meta-learning concerns rapid knowledge acquisition. One popular approach cast optimisation as a learning problem and it has been shown that learnt neural optimisers update base learners more quickly than their handcrafted counterparts. In this paper, we learn an optimisation rule that sparsely updates the learner parameters and removes redundant weights. We present Masked Meta-SGD (M2SGD), a neural optimiser which is not only capable of updating learners quickly, but also capable of removing 83.71% weights for ResNet20s.\r"
  },
  "cvpr2020_w15_few-shotimagerecognitionforuavsportscinematography": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Few-Shot Image Recognition for UAV Sports Cinematography",
    "authors": [
      "Emmanouil Patsiouras",
      "Anastasios Tefas",
      "Ioannis Pitas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Patsiouras_Few-Shot_Image_Recognition_for_UAV_Sports_Cinematography_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Patsiouras_Few-Shot_Image_Recognition_for_UAV_Sports_Cinematography_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The goal of few-shot image learning is to utilize a very small amount of training examples in order to train a machine learning model to recognize a given number of image classes. While humans can perform such a task pretty much effortlessly, applying the same mechanism to deep learning visual recognition systems is a much more difficult task, having a wide range of real-world visual recognition applications. In this paper, we investigate the behavior of such few-shot methods in the context of drone vision cinematography for sports event filming, in order to recognize new image classes by taking into consideration the fact that this new class we wish to identify is a subclass of an already known class. More specifically we use UAV footage to recognize certain types of athletes, belonging to a subset of an original athlete class, utilizing only a handful of recorded images of this athlete subclass. We examine the effects of such methods on image recognition accuracy while proposing a novel approach for accuracy optimizations. The overall task is evaluated on actual cycling race UAV footage.\r"
  },
  "cvpr2020_w15_generalizedclassincrementallearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Generalized Class Incremental Learning",
    "authors": [
      "Fei Mi",
      "Lingjing Kong",
      "Tao Lin",
      "Kaicheng Yu",
      "Boi Faltings"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Mi_Generalized_Class_Incremental_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Mi_Generalized_Class_Incremental_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Many real-world machine learning systems require the ability to continually learn new knowledge. Class incremental learning receives increasing attention recently as a solution towards this goal. However, existing methods often introduce some assumptions to simplify the problem setting, which rarely holds in real-world scenarios. In this paper, we formulate a Generalized Class Incremental Learning (GCIL) framework to systematically alleviate these restrictions, and introduce several novel realistic incremental learning scenarios. In addition, we propose a simple yet effective method, namely ReMix, which combines Exemplar Replay (ER) and Mixup to deal with different challenges in realistic GCIL setups. We demonstrate on CIFAR-100 that ReMix outperforms the state-of-the-art methods in different GCIL setups by significant margins without introducing additional computation cost.\r"
  },
  "cvpr2020_w15_stacknetstackingfeaturemapsforcontinuallearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "StackNet: Stacking Feature Maps for Continual Learning",
    "authors": [
      "Jangho Kim",
      "Jeesoo Kim",
      "Nojun Kwak"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Kim_StackNet_Stacking_Feature_Maps_for_Continual_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Kim_StackNet_Stacking_Feature_Maps_for_Continual_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Training a neural network for a classification task typically assumes that the data to train are given from the beginning. However, in the real world, additional data accumulate gradually and the model requires additional training without accessing the old training data. This usually leads to the catastrophic forgetting problem which is inevitable for the traditional training methodology of neural networks. In this paper, we propose a continual learning method that is able to learn additional tasks while retaining the performance of previously learned tasks by stacking parameters. Composed of two complementary components, the index module and the StackNet, our method estimates the index of the corresponding task for an input sample with the index module and utilizes a particular portion of StackNet with this index. The StackNet guarantees no degradation in the performance of the previously learned tasks and the index module shows high confidence in finding the origin of an input sample. Compared to the previous work of PackNet, our method is competitive and highly intuitive.\r"
  },
  "cvpr2020_w15_noise-basedselectionofrobustinheritedmodelforaccuratecontinuallearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Noise-Based Selection of Robust Inherited Model for Accurate Continual Learning",
    "authors": [
      "Xiaocong Du",
      "Zheng Li",
      "Jae-sun Seo",
      "Frank Liu",
      "Yu Cao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Du_Noise-Based_Selection_of_Robust_Inherited_Model_for_Accurate_Continual_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Du_Noise-Based_Selection_of_Robust_Inherited_Model_for_Accurate_Continual_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " There is a growing demand for an intelligent system to continually learn knowledge from a data stream. Continual learning requires both the preservation of previous knowledge (i.e., avoiding catastrophic forgetting) and the acquisition of new knowledge. Different from previous works that focus only on model adaptation (e.g., regularization, network expansion, memory rehearsal, etc.), we propose a novel training scheme named acquisitive learning (AL), which emphasizes both the knowledge inheritance and knowledge acquisition. AL starts from an elaborately selected model with pre-trained knowledge (the inherited model) and then adapts it to new data using segmented training. The selection is achieved by injecting random noise to various inherited models for better model robustness, which promises higher accuracy in further knowledge acquisition. The approach is validated by the visualization of the loss landscape and quantitative roughness measurement. The combination of the selective inherited model and knowledge acquisition reduces catastrophic forgetting by 10X on the CIFAR-100 dataset.\r"
  },
  "cvpr2020_w15_continualreinforcementlearningin3dnon-stationaryenvironments": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Continual Reinforcement Learning in 3D Non-Stationary Environments",
    "authors": [
      "Vincenzo Lomonaco",
      "Karan Desai",
      "Eugenio Culurciello",
      "Davide Maltoni"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Lomonaco_Continual_Reinforcement_Learning_in_3D_Non-Stationary_Environments_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Lomonaco_Continual_Reinforcement_Learning_in_3D_Non-Stationary_Environments_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " High-dimensional always-changing environments constitute a hard challenge for current reinforcement learning techniques. Artificial agents, nowadays, are often trained off-line in very static and controlled conditions in simulation such that training observations can be thought as sampled i.i.d. from the entire observations space. However, in real world settings, the environment is often non-stationary and subject to unpredictable, frequent changes. In this paper we propose and openly release CRLMaze, a new benchmark for learning continually through reinforcement in a complex 3D non-stationary task based on ViZDoom and subject to several environmental changes. Then, we introduce an end-to-end model-free continual reinforcement learning strategy showing competitive results with respect to four different baselines and not requiring any access to additional supervised signals, previously encountered environmental conditions or observations.\r"
  },
  "cvpr2020_w15_relationshipmattersrelationguidedknowledgetransferforincrementallearningofobjectdetectors": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Relationship Matters: Relation Guided Knowledge Transfer for Incremental Learning of Object Detectors",
    "authors": [
      "Kandan Ramakrishnan",
      "Rameswar Panda",
      "Quanfu Fan",
      "John Henning",
      "Aude Oliva",
      "Rogerio Feris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Ramakrishnan_Relationship_Matters_Relation_Guided_Knowledge_Transfer_for_Incremental_Learning_of_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Ramakrishnan_Relationship_Matters_Relation_Guided_Knowledge_Transfer_for_Incremental_Learning_of_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Standard deep learning based object detectors suffer from catastrophic forgetting, which results in performance degradation on old classes as new classes are incrementally added. There has been a few recent methods that attempt to address this problem by minimizing the discrepancy between individual object proposal responses for old classes from the original and the updated networks. Different from these methods, we introduce a novel approach that not only focuses on what knowledge to transfer but also how to effectively transfer for minimizing the effect of catastrophic forgetting in incremental learning of object detectors. Towards this, we first propose a proposal selection mechanism using ground truth objects from the new classes and then a relation guided transfer loss function that aims to preserve the relations of selected proposals between the base network and the new network trained on additional classes. Experiments on three standard datasets demonstrate the efficacy of our proposed approach over state-of-the-art methods.\r"
  },
  "cvpr2020_w15_reducingcatastrophicforgettingwithlearningonsyntheticdata": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Reducing Catastrophic Forgetting With Learning on Synthetic Data",
    "authors": [
      "Wojciech Masarczyk",
      "Ivona Tautkute"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Masarczyk_Reducing_Catastrophic_Forgetting_With_Learning_on_Synthetic_Data_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Masarczyk_Reducing_Catastrophic_Forgetting_With_Learning_on_Synthetic_Data_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Catastrophic forgetting is a problem caused by neural networks' inability to learn data in sequence. After learning two tasks in sequence, performance on the first one drops significantly. This is a serious disadvantage that prevents many deep learning applications to real-life problems where not all object classes are known beforehand; or change in data requires adjustments to the model. To reduce this problem we investigate the use of synthetic data, namely we answer a question: Is it possible to generate such data synthetically which learned in sequence does not result in catastrophic forgetting? We propose a method to generate such data in two-step optimisation process via meta-gradients. Our experimental results on Split-MNIST dataset show that training a model on such synthetic data in sequence does not result in catastrophic forgetting. We also show that our method of generating data is robust to different learning scenarios.\r"
  },
  "cvpr2020_w15_continuallearningforanomalydetectioninsurveillancevideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Continual Learning for Anomaly Detection in Surveillance Videos",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Doshi_Continual_Learning_for_Anomaly_Detection_in_Surveillance_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Doshi_Continual_Learning_for_Anomaly_Detection_in_Surveillance_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Anomaly detection in surveillance videos has been recently gaining attention. A challenging aspect of high-dimensional applications such as video surveillance is continual learning. While current state-of-the-art deep learning approaches perform well on existing public datasets, they fail to work in a continual learning framework due to computational and storage issues. Furthermore, online decision making is an important but mostly neglected factor in this domain. Motivated by these research gaps, we propose an online anomaly detection method for surveillance videos using transfer learning and continual learning, which in turn significantly reduces the training complexity and provides a mechanism for continually learning from recent data without suffering from catastrophic forgetting. Our proposed algorithm leverages the feature extraction power of neural network-based models for transfer learning, and the continual learning capability of statistical detection methods.\r"
  },
  "cvpr2020_w15_generatingaccuratepseudoexamplesforcontinuallearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w15",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Continual Learning in Computer Vision",
    "title": "Generating Accurate Pseudo Examples for Continual Learning",
    "authors": [
      "Daniel L. Silver",
      "Sazia Mahfuz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w15/Silver_Generating_Accurate_Pseudo_Examples_for_Continual_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w15/Silver_Generating_Accurate_Pseudo_Examples_for_Continual_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Continual learning (CL) is concerned with the persistent and cumulative nature of learning. This requires a method of successfully consolidating new knowledge into long-term memory without the loss of prior knowledge. Prior research has addressed this CL retention problem through the efficient rehearsal of prior examples while learning the examples of a new task within a long-term Multiple Task Learning (MTL) network. The approach maintains or improves prior knowledge while allowing its representation to remain plastic for the integration of new task examples. Preferably, rehearsal is done using pseudo examples synthesized by the MTL network; eliminating the need to retain prior task training examples or a generate them with an additional model. Previous work has shown that to properly retain knowledge the pseudo examples must adhere to the input probability distribution of those original examples. Two approaches are investigated for creating appropriate pseudo examples from a Restricted Boltzmann Machine (RBM) autoencoder, which can reside in the lowest layers of the long-term MTL Deep Belief network. We show that appropriate pseudo examples can be reconstructed by passing uniform random examples to a generative RBM model and selecting only those with reconstruction error less than the mean training error. These pseudo examples are shown to adhere to the probability distribution of the input variables of the original training examples and retain prior task knowledge during rehearsal as well as those examples. As part of the research, we develop and test a new metric called the Autoencoder Divergence Measure for comparing the probability distributions of two datasets given to a generative RBM network based on their reconstruction mean squared error.\r"
  },
  "cvpr2020_w14_hideganahyperspectral-guidedimagedehazinggan": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w14",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "HIDeGan: A Hyperspectral-Guided Image Dehazing GAN",
    "authors": [
      "Aditya Mehta",
      "Harsh Sinha",
      "Pratik Narang",
      "Murari Mandal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w14/Mehta_HIDeGan_A_Hyperspectral-Guided_Image_Dehazing_GAN_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w14/Mehta_HIDeGan_A_Hyperspectral-Guided_Image_Dehazing_GAN_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Haze removal in images captured from a diverse set of scenarios is a very challenging problem. The existing dehazing methods either reconstruct the transmission map or directly estimate the dehazed image in RGB color space. In this paper, we make a first attempt to propose a Hyperspectral-guided Image Dehazing Generative Adversarial Network (HIDEGAN). The HIDEGAN architecture is formulated by designing an enhanced version of CYCLEGAN named R2HCYCLE and an enhanced conditional GAN named H2RGAN. The R2HCYCLE makes use of the hyperspectral-image (HSI) in combination with cycle-consistency and skeleton losses in order to improve the quality of information recovery by analyzing the entire spectrum. The H2RGAN estimates the clean RGB image from the hazy hyperspectral image generated by the R2HCYCLE. The models designed for spatial-spectral-spatial mapping generate visually better haze-free images. To facilitate HSI generation, datasets from spectral reconstruction challenge at NTIRE 2018 and NTIRE 2020 are used. A comprehensive set of experiments were conducted on the D-Hazy, and the recent RESIDE-Standard (SOTS), RESIDE-b (OTS) and RESIDE-Standard (HSTS) datasets. The proposed HIDEGAN outperforms the existing state-of-the-art in all these datasets.\r"
  },
  "cvpr2020_w14_removalofimageobstaclesforvehicle-mountedsurroundingmonitoringcamerasbyreal-timevideoinpainting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w14",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "Removal of Image Obstacles for Vehicle-Mounted Surrounding Monitoring Cameras by Real-Time Video Inpainting",
    "authors": [
      "Yoshihiro Hirohashi",
      "Kenichi Narioka",
      "Masanori Suganuma",
      "Xing Liu",
      "Yukimasa Tamatsu",
      "Takayuki Okatani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w14/Hirohashi_Removal_of_Image_Obstacles_for_Vehicle-Mounted_Surrounding_Monitoring_Cameras_by_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w14/Hirohashi_Removal_of_Image_Obstacles_for_Vehicle-Mounted_Surrounding_Monitoring_Cameras_by_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " One of the practical problems with surrounding view cameras (SMCs) of a vehicle is the degradation of image quality due to obstacles by substances adherent to their lens surface, such as raindrops and mud. Such image degradation could be improved by image restoration techniques that have been studied in the field of computer vision. However, to assist the driver, real time processing and fidelity of the recovered image are essential, which disqualifies most of the existing methods. In this study, we propose to adopt a recently developed video-inpainting method that can restore high-fidelity images in real time. It estimates optical flows using a CNN and use them to match occluded regions in the current frame to unoccluded regions in previous frames, restoring the former. Although the direct application does not lead to satisfactory results due to the peculiarities of the SMC videos, we show that two improvements make it possible to obtain good results that are useful in practice. One is to use a model-based flow estimation method to obtain target flows for training the CNN, and the other is to improve how the estimated flows are used to match the current and previous frames. We conducted experiments using real images mainly of parking spaces in urban areas. The results, including subjective evaluation, show the effectiveness of our approach.\r"
  },
  "cvpr2020_w14_anewmultimodalrgbandpolarimetricimagedatasetforroadscenesanalysis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w14",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "A New Multimodal RGB and Polarimetric Image Dataset for Road Scenes Analysis",
    "authors": [
      "Rachel Blin",
      "Samia Ainouz",
      "Stephane Canu",
      "Fabrice Meriaudeau"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w14/Blin_A_New_Multimodal_RGB_and_Polarimetric_Image_Dataset_for_Road_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w14/Blin_A_New_Multimodal_RGB_and_Polarimetric_Image_Dataset_for_Road_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Road scene analysis is a fundamental task for both autonomous vehicles and ADAS systems. Nowadays, one can find autonomous vehicles that are able to properly detect objects present in the scene in good weather conditions but some improvements are left to be done when the visibility is altered. People claim that using some non conventional sensors (infra-red, Lidar, etc.) along with classical vision enhances road scene analysis but still when conditions are optimal. In this work, we present the improvements achieved using polarimetric imaging in the complex situation of adverse weather conditions. This rich modality is known for its ability to describe an object not only by its intensity but also by its physical information, even under poor illumination and strong reflection. The experimental results have shown that, using our new multimodal dataset, polarimetric imaging was able to provide generic features for both good weather conditions and adverse weather ones. By combining polarimetric images with an adapted learning model, the different detection tasks in adverse weather conditions were improved by about 27%.\r"
  },
  "cvpr2020_w14_impliciteulerodenetworksforsingle-imagedehazing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w14",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Vision for All Seasons: Adverse Weather and Lighting Conditions",
    "title": "Implicit Euler ODE Networks for Single-Image Dehazing",
    "authors": [
      "Jiawei Shen",
      "Zhuoyan Li",
      "Lei Yu",
      "Gui-Song Xia",
      "Wen Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w14/Shen_Implicit_Euler_ODE_Networks_for_Single-Image_Dehazing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w14/Shen_Implicit_Euler_ODE_Networks_for_Single-Image_Dehazing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep convolutional neural networks (CNN) have been applied for image dehazing tasks, where the residual network (ResNet) is often adopted as the basic component to avoid the vanishing gradient problem. Recently, many works indicate that the ResNet can be considered as the explicit Euler forward approximation of an ordinary differential equation (ODE). In this paper, we extend the explicit forward approximation to the implicit backward counterpart, which can be realized via a recursive neural network, named IM-block. Given that, we propose an efficient end-to-end multi-level implicit network (MI-Net) for the single image dehazing problem. Moreover, multi-level fusing (MLF) mechanism and residual channel attention block (RCA-block) are adopted to boost performance of our network. Experiments on several dehazing benchmark datasets demonstrate that our method outperforms existing methods and achieves the state-of-the-art performance.\r"
  },
  "cvpr2020_w17_voronoinetgeneralfunctionalapproximatorswithlocalsupport": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w17",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learning 3D Generative Models",
    "title": "VoronoiNet: General Functional Approximators With Local Support",
    "authors": [
      "Francis Williams",
      "Jerome Parent-Levesque",
      "Derek Nowrouzezahrai",
      "Daniele Panozzo",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w17/Williams_VoronoiNet_General_Functional_Approximators_With_Local_Support_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w17/Williams_VoronoiNet_General_Functional_Approximators_With_Local_Support_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Voronoi diagrams are highly compact representations that are used in various Graphics applications. In this work, we show how to embed a differentiable version of it - via a novel deep architecture - into a generative deep network. By doing so, we achieve a highly compact latent embedding that is able to provide much more detailed reconstructions, both in 2D and 3D, for various shapes. In this tech report, we introduce our representation and present a set of preliminary results comparing it with recently proposed implicit occupancy networks.\r"
  },
  "cvpr2020_w17_deepoctree-basedcnnswithoutput-guidedskipconnectionsfor3dshapeandscenecompletion": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w17",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learning 3D Generative Models",
    "title": "Deep Octree-Based CNNs With Output-Guided Skip Connections for 3D Shape and Scene Completion",
    "authors": [
      "Peng-Shuai Wang",
      "Yang Liu",
      "Xin Tong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w17/Wang_Deep_Octree-Based_CNNs_With_Output-Guided_Skip_Connections_for_3D_Shape_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w17/Wang_Deep_Octree-Based_CNNs_With_Output-Guided_Skip_Connections_for_3D_Shape_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Acquiring complete and clean 3D shape and scene data is challenging due to geometric occlusion and insufficient views during 3D capturing. We present a simple yet effective deep learning approach for completing the input noisy and incomplete shapes or scenes. Our network is built upon the octree-based CNNs (O-CNN) with U-Net like structures, which enjoys high computational and memory efficiency and supports to construct a very deep network structure for 3D CNNs. A novel output-guided skip-connection is introduced to the network structure for better preserving the input geometry and learning geometry prior from data effectively. We show that with these simple adaptions --- output-guided skip-connection and deeper O-CNN (up to 70 layers), our network achieves state-of-the-art results in 3D shape completion and semantic scene computation.\r"
  },
  "cvpr2020_w17_generalizedautoencoderforvolumetricshapegeneration": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w17",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learning 3D Generative Models",
    "title": "Generalized Autoencoder for Volumetric Shape Generation",
    "authors": [
      "Yanran Guan",
      "Tansin Jahan",
      "Oliver van Kaick"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w17/Guan_Generalized_Autoencoder_for_Volumetric_Shape_Generation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w17/Guan_Generalized_Autoencoder_for_Volumetric_Shape_Generation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We introduce a 3D generative shape model based on the generalized autoencoder (GAE). GAEs learn a manifold latent space from data relations explicitly provided during training. In our work, we train a GAE for volumetric shape generation from data similarities derived from the Chamfer distance, and with a loss function which is the combination of the traditional autoencoder loss and the GAE loss. We show that this shape model is able to learn more meaningful structures for the latent manifolds of different categories of shapes, and provides better interpolations between shapes when compared to previous approaches such as autoencoders and variational autoencoders.\r"
  },
  "cvpr2020_w17_topology-awaresingle-image3dshapereconstruction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w17",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learning 3D Generative Models",
    "title": "Topology-Aware Single-Image 3D Shape Reconstruction",
    "authors": [
      "Qimin Chen",
      "Vincent Nguyen",
      "Feng Han",
      "Raimondas Kiveris",
      "Zhuowen Tu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w17/Chen_Topology-Aware_Single-Image_3D_Shape_Reconstruction_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w17/Chen_Topology-Aware_Single-Image_3D_Shape_Reconstruction_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We make an attempt to address topology-awareness for 3D shape reconstruction. Two types of high-level shape typologies are being studied here, namely genus (number of cuttings/holes) and connectivity (number of connected components), which are of great importance in 3D object reconstruction/understanding but have been thus far disjoint from the existing dense voxel-wise prediction literature. We propose a topology-aware shape autoencoder component (TPWCoder) by approximating topology property functions such as genus and connectivity with neural networks from the latent variables. TPWCoder can be directly combined with the existing 3D shape reconstruction pipelines for end-to-end training and prediction. On the challenging A Big CAD Model Dataset (ABC), TPWCoder demonstrates a noticeable quantitative and qualitative improvement over the competing methods, and it also shows improved quantitative result on the ShapeNet dataset.\r"
  },
  "cvpr2020_w17_geometrytotherescue3dinstancereconstructionfromaclutteredscene": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w17",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learning 3D Generative Models",
    "title": "Geometry to the Rescue: 3D Instance Reconstruction From a Cluttered Scene",
    "authors": [
      "Lin Li",
      "Salman Khan",
      "Nick Barnes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w17/Li_Geometry_to_the_Rescue_3D_Instance_Reconstruction_From_a_Cluttered_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w17/Li_Geometry_to_the_Rescue_3D_Instance_Reconstruction_From_a_Cluttered_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " 3D object instance reconstruction from a cluttered 2D scene image is an ill-posed problem. The main challenge is posed by the lack of geometric information in color images and heavy occlusions that lead to incomplete shape details. To deal with this problem, existing works on 3D instance reconstruction directly learn the mapping between the intensity image and the corresponding 3D volume model. Different from these works, we propose to explicitly incorporate 2.5D geometric cues, such as the surface normal, relative depth, and height, while generating full 3D shapes from 2D images. With an intermediate step focused on estimating these 2.5D geometric features, we propose a novel convolutional neural network design that progressively moves from 2D to full 3D estimation. Our model automatically generates instance-specific surface normal maps, relative depth, and height that are compactly encoded within our network design and consequently used to improve the 3D instance reconstruction. Our experimental results on the large-scale synthetic SUNCG dataset and the real-world NYU depth v2 dataset demonstrate the effectiveness of the proposed approach where it beats the state-of-the-art Factored3D network.\r"
  },
  "cvpr2020_w17_meshvariationalautoencoderswithedgecontractionpooling": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w17",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Learning 3D Generative Models",
    "title": "Mesh Variational Autoencoders With Edge Contraction Pooling",
    "authors": [
      "Yu-Jie Yuan",
      "Yu-Kun Lai",
      "Jie Yang",
      "Qi Duan",
      "Hongbo Fu",
      "Lin Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w17/Yuan_Mesh_Variational_Autoencoders_With_Edge_Contraction_Pooling_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w17/Yuan_Mesh_Variational_Autoencoders_With_Edge_Contraction_Pooling_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " 3D shape analysis is an important research topic in computer vision and graphics. While existing methods have generalized image-based deep learning to meshes using graph-based convolutions, the lack of an effective pooling operation restricts the learning capability of their networks. In this paper, we propose a novel pooling operation for mesh datasets with the same connectivity but different geometry, by building a mesh hierarchy using mesh simplification. For this purpose, we develop a modified mesh simplification method to avoid generating highly irregularly sized triangles. Our pooling operation effectively encodes the correspondence between coarser and finer meshes in the hierarchy. We then present a variational auto-encoder (VAE) structure with the edge contraction pooling and graph-based convolutions, to explore probability latent spaces of 3D surfaces and perform 3D shape generation. Our network requires far fewer parameters than the original mesh VAE and thus can handle denser models thanks to our new pooling operation and convolutional kernels. Our evaluation also shows that our method has better generalization ability and is more reliable in various applications, including shape generation and shape interpolation.\r"
  },
  "cvpr2020_w16_subpixeldenserefinementnetworkforskeletonization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w16",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Deep Learning for Geometric Computing",
    "title": "Subpixel Dense Refinement Network for Skeletonization",
    "authors": [
      "Sohom Dey"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w16/Dey_Subpixel_Dense_Refinement_Network_for_Skeletonization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w16/Dey_Subpixel_Dense_Refinement_Network_for_Skeletonization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Skeletonization is the process of reducing a shape image to its approximate medial axis representation while preserving the topology and geometry of the image. Skeletonization is an important step for topological and geometric shape analysis. In this paper a novel skeleton extraction architecture - Subpixel Dense Refinement Network is introduced which is trained and evaluated on the Pixel SkelNetOn Challenge dataset. The proposed architecture is a three-stage encoder-decoder network with dense interconnections between the decoder networks of each stage. The architecture replaces general up-sampling layers and transposed convolution layers with subpixel convolutions for minimizing the information loss during up-sampling of the encoded features. The deep network is trained end-to-end with intermediate supervision in each stage. The proposed single architecture achieved an F1-score of 0.7708 on the validation set of the Pixel SkelNetOn Challenge dataset.\r"
  },
  "cvpr2020_w16_capturingcellulartopologyinmulti-gigapixelpathologyimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w16",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Deep Learning for Geometric Computing",
    "title": "Capturing Cellular Topology in Multi-Gigapixel Pathology Images",
    "authors": [
      "Wenqi Lu",
      "Simon Graham",
      "Mohsin Bilal",
      "Nasir Rajpoot",
      "Fayyaz Minhas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w16/Lu_Capturing_Cellular_Topology_in_Multi-Gigapixel_Pathology_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w16/Lu_Capturing_Cellular_Topology_in_Multi-Gigapixel_Pathology_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In computational pathology, multi-gigapixel whole slide images (WSIs) are typically divided into small patches because of their extremely large size and memory requirements. However, following this strategy, one risks losing visual context which is very important in the development of machine learning models aimed at diagnostic and prognostic assessment of WSIs. In this paper, we propose a novel graph convolutional neural network based model (called Slide Graph) which overcomes these limitations by building a graph representation of the cellular architecture in an entire WSI in a bottom-up manner. We evaluate Slide Graph for prediction of the status of human epidermal growth factor receptor 2 (HER2) and progesterone receptor (PR) expression from WSIs of H&E stained tissue slides of breast cancer. We demonstrate that the proposed model outperforms previous state-of-the-art methods and is more computationally efficient. The proposed paradigm of WSI-level graphs can potentially be applied to other problems in computational pathology as well.\r"
  },
  "cvpr2020_w16_anovellocalgeometrycaptureinpointnet++for3dclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w16",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Deep Learning for Geometric Computing",
    "title": "A Novel Local Geometry Capture in PointNet++ for 3D Classification",
    "authors": [
      "Shivanand Venkanna Sheshappanavar",
      "Chandra Kambhamettu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w16/Sheshappanavar_A_Novel_Local_Geometry_Capture_in_PointNet_for_3D_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w16/Sheshappanavar_A_Novel_Local_Geometry_Capture_in_PointNet_for_3D_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Few of the recent deep learning models for 3D point sets classification are dependent on how well the model captures the local geometric structures. PointNet++ model made remarkable progress in learning local geometric structures than its predecessor PointNet. It recursively applies PointNet on nested partitions of the input 3D point set. PointNet++ model was able to extract the local region features from points by ball querying the local neighborhoods. However, ball querying is less effective in capturing local neighborhoods of high curvature surfaces or regions. In this paper, we demonstrate improvement in the 3D classification results by using ellipsoid querying around centroids, capturing more points in the local neighborhood. We extend the ellipsoid querying technique by orienting it in the direction of principal axes of the local neighborhood for better capture of the local geometry. We then take the union of points grouped by ball querying and ellipsoid querying with re-orientation to improve the PointNet++ classification results by 1.1%. Furthermore, we demonstrate the impact of re-oriented ellipsoid querying on a state-of-the-art ball query-based model, Relation-Shape Convolutional Neural Network (RS-CNN), with a 0.8% improvement in classification accuracy on ModelNet40 dataset.\r"
  },
  "cvpr2020_w19_photoplethysmographybasedstratificationofbloodpressureusingmulti-informationfusionartificialneuralnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Photoplethysmography Based Stratification of Blood Pressure Using Multi-Information Fusion Artificial Neural Network",
    "authors": [
      "Dingliang Wang",
      "Xuezhi Yang",
      "Xuenan Liu",
      "Shuai Fang",
      "Likun Ma",
      "Longwei Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Wang_Photoplethysmography_Based_Stratification_of_Blood_Pressure_Using_Multi-Information_Fusion_Artificial_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Wang_Photoplethysmography_Based_Stratification_of_Blood_Pressure_Using_Multi-Information_Fusion_Artificial_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Regular monitoring of blood pressure (BP) is an effective way to prevent cardiovascular diseases, especially for elderly people. At present, BP measurement mainly relies on cuff-based devices which are inconvenient for users and may cause discomfort. Therefore, many new approaches have been proposed to achieve cuff-less BP detection in recent years. However, the accuracy of the existing approaches still needs to be improved. In this study, holistic-based PPG and its first and second derivative features are extracted and a new multi information fusion artificial neural network (MIF-ANN) is designed to effectively fuse and exploit multiple input data. Experimental results on a public database which contains 12000 subjects show that the proposed network can model the relation between Photoplethysmography (PPG) and BP well, achieving averagely accuracy of 91.33% for 5-category BP stratification. Additionally, this study verified that multi information fusion based on meticulously designed network plays an important role in improving the accuracy of BP detection.\r"
  },
  "cvpr2020_w19_remoteestimationofheartratebasedonmulti-scalefacialrois": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Remote Estimation of Heart Rate Based on Multi-Scale Facial ROIs",
    "authors": [
      "Changchen Zhao",
      "Weiran Han",
      "Zan Chen",
      "Yongqiang Li",
      "Yuanjing Feng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Zhao_Remote_Estimation_of_Heart_Rate_Based_on_Multi-Scale_Facial_ROIs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Zhao_Remote_Estimation_of_Heart_Rate_Based_on_Multi-Scale_Facial_ROIs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While most rPPG approaches extract the pulse signals based on single facial region of interest (ROI), this research proposes a new method to extract pulse signals from ROIs with multiple scales. The idea is that rich pulse features can be extracted by varying ROI scales and combining these features would contribute to the accuracy improvement. The proposed framework consists of three main steps: 1) constructing facial ROI pyramid with multiple scale levels, 2) blood volume pulse (BVP) signals extraction, and 3) signal fusion using convex combination with Gaussian and uniform priors, respectively. This paper also investigates how the commonly used algorithms perform under multi-scale ROIs. Experiments were conducted using one publicly available dataset and one self-collected dataset. The results show that the ROI with a size slightly smaller than the face boundary achieves on average higher measurement accuracy. The high-quality pulse signal appears not consistently in one scale level but rather in multiple levels according to measurement environments and motion statuses. Therefore, the fusion of multiple pulse signals is beneficial to the measurement accuracy improvement.\r"
  },
  "cvpr2020_w19_enhancingremote-ppgpulseextractionindisturbancescenariosutilizingspectralcharacteristics": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Enhancing Remote-PPG Pulse Extraction in Disturbance Scenarios Utilizing Spectral Characteristics",
    "authors": [
      "Kai Zhou",
      "Simon Krause",
      "Timon Blocher",
      "Wilhelm Stork"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Zhou_Enhancing_Remote-PPG_Pulse_Extraction_in_Disturbance_Scenarios_Utilizing_Spectral_Characteristics_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Zhou_Enhancing_Remote-PPG_Pulse_Extraction_in_Disturbance_Scenarios_Utilizing_Spectral_Characteristics_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In recent years, several approaches for remote Photoplethysmography (rPPG) have been proposed, and the recently proposed methods have achieved substantial improvement in measurement accuracy. However, none of the methods has investigated the possibility of using the spectral characteristics for the design of rPPG signal extraction algorithms. In this paper, we propose a new rPPG measurement method which exploits the spectral characteristics of rPPG signals. We validated the freshly proposed method on a benchmark dataset including seven scenarios and 26 participants. The results of the validation experiment demonstrates the feasibility to use spectral characteristics to extract rPPG signal. By combining with the constraint plane, the new proposed method provides better overall performance.\r"
  },
  "cvpr2020_w19_predictingbrainwavesfromfacevideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Predicting Brainwaves From Face Videos",
    "authors": [
      "Christian S. Pilz",
      "Ibtissem Ben Makhlouf",
      "Ute Habel",
      "Steffen Leonhardt"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Pilz_Predicting_Brainwaves_From_Face_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Pilz_Predicting_Brainwaves_From_Face_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We investigate the regulation of human brain arousal in the central nervous system and its synchronization with the autonomic nervous system affecting the facial dynamics and its behavioral gestalt. A major focus is made on the sensing observable during natural human eye to eye communication. Although the inner state of the autopoietic system is deterministic, its outer facial behavioral component non-deterministic. Beside the introduction of general validity of the classical empirical interpretation of the vigilance continuum during open eyes, we show that the facial behavior can be used as suitable surrogate measurement for specific states of mind. As a consequence we predict brainwaves from face videos formulated as inverse problem of the underlying stochastic process. Finally, we discuss the impact and range of application field.\r"
  },
  "cvpr2020_w19_ameta-analysisoftheimpactofskintoneandgenderonnon-contactphotoplethysmographymeasurements": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "A Meta-Analysis of the Impact of Skin Tone and Gender on Non-Contact Photoplethysmography Measurements",
    "authors": [
      "Ewa M. Nowara",
      "Daniel McDuff",
      "Ashok Veeraraghavan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Nowara_A_Meta-Analysis_of_the_Impact_of_Skin_Tone_and_Gender_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Nowara_A_Meta-Analysis_of_the_Impact_of_Skin_Tone_and_Gender_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " It is well established that many datasets used for computer vision tasks are not representative and may be biased towards some demographic groups. The result of this is that performance evaluation may not reflect that in the real-world and might expose some groups (often minorities) to greater risks than others. Imaging photoplethysmography is a set of techniques that enables non-contact measurement of vital signs using imaging devices. While these methods hold great promise for low-cost and scalable physiological monitoring, it is important that performance is characterized accurately over diverse populations. We perform a meta-analysis across three datasets, including 73 people and over 400 videos featuring a broad range of skin types. While heart rate measurement can be performed on all skin types under certain conditions, we find that average performance drops significantly for the darkest skin type. We compare supervised and unsupervised learning algorithms and find that skin type does not impact all methods equally. The imaging photoplethysmography community should devote greater efforts to addressing these disparities and collecting representative datasets.\r"
  },
  "cvpr2020_w19_analysisofpulsetransittimederivedfromimagingphotoplethysmographyandmicrowavesensor-basedballistocardiography": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Analysis of Pulse Transit Time Derived From Imaging Photoplethysmography and Microwave Sensor-Based Ballistocardiography",
    "authors": [
      "Mototaka Yoshioka",
      "Souksakhone Bounyong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Yoshioka_Analysis_of_Pulse_Transit_Time_Derived_From_Imaging_Photoplethysmography_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Yoshioka_Analysis_of_Pulse_Transit_Time_Derived_From_Imaging_Photoplethysmography_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a basic analysis of the pulse transit time derived from imaging photoplethysmography and ballistocardiography. The pulse transit time is considered as an indicator of blood pressure. For convenient estimation of the blood pressure, previous studies have used the time delay between electrocardiography and photoplethysmography using contact based sensors, which is known as the pulse arrival time. In this paper, we propose a noncontact system to measure the pulse transit time, which consists of microwave and image sensors. The microwave sensor allows ballistocardiography from tiny body movements generated by the human heartbeat using reflected wave signal from a subject's chest, and the image sensor enables imaging photoplethysmography of subject's face. By temporally synchronizing two noncontact sensors, the proposed system is able to provide an estimate of the pulse transit time remotely. We conducted experiments on 16 subjects (age range of 69 to 79 years old) with a supine posture. The correlation coefficient between the noncontact pulse transit time and systolic blood pressure was -0.64 (P<0.05). The pulse transit time had a better correlation with systolic blood pressure than the pulse arrival time, which was -0.20. This result indicates that the pre-ejection period influences the pulse arrival time. The pre-ejection period calculated from electrocardiography to ballistocardiography ranges from 54 to 130 ms owing to individual differences. This is an important finding for noncontact blood-pressure estimation in the future.\r"
  },
  "cvpr2020_w19_hearttrackconvolutionalneuralnetworkforremotevideo-basedheartratemonitoring": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "HeartTrack: Convolutional Neural Network for Remote Video-Based Heart Rate Monitoring",
    "authors": [
      "Olga Perepelkina",
      "Mikhail Artemyev",
      "Marina Churikova",
      "Mikhail Grinenko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Perepelkina_HeartTrack_Convolutional_Neural_Network_for_Remote_Video-Based_Heart_Rate_Monitoring_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Perepelkina_HeartTrack_Convolutional_Neural_Network_for_Remote_Video-Based_Heart_Rate_Monitoring_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Detection and continuous monitoring of heart rate can help us identify clinical relevance of some cardiac symptoms. Over the last decade, a lot of attention has been paid to the development of the algorigthms for remote photoplethysmography (rPPG). As a result, we can now accurately monitor heart rate of still sitting subjects using data extracted from video feed. Aside from methods based on hand-crafted features, there have also been developed the more advanced learning-based rPPG algorithms. Deep learning methods usually require large amounts of data for training, however, biomedical data often suffers from lack of real-life data. To address these issues, we have developed a HeartTrack convolutional neural network for remote video-based heart rate tracking. This learning-based method has been trained on synthetic data to accurately estimate heart rate in different conditions. Moreover, here we provide two new rPPG datasets - MoLi-ppg-1 and MoLi-ppg-2 - that were recorded in complicated conditions that were close to the natural ones. The datasets include videos that feature moving and talking subjects, different types of lighting, various equipment, etc. We have used our new MoLi-ppg-1 and MoLi-ppg-2 datasets for algorithm training and testing, and the existing UBFC-RPPG dataset for the algorithm testing and comparison with other approaches. Our HeartTrack neural network shows state-of-the-art results on the UBFC-RPPG database (MAE=2.412, RMSE=3.368, R=0.983).\r"
  },
  "cvpr2020_w19_continuousestimationofemotionalchangeusingmultimodalaffectiveresponses": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Continuous Estimation of Emotional Change Using Multimodal Affective Responses",
    "authors": [
      "Kenta Masui",
      "Takumi Nagasawa",
      "Hirokazu Doi",
      "Norimichi Tsumura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Masui_Continuous_Estimation_of_Emotional_Change_Using_Multimodal_Affective_Responses_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Masui_Continuous_Estimation_of_Emotional_Change_Using_Multimodal_Affective_Responses_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Emotions have a significant effect on our daily behavior, such as perception, memory, and decision making. For this reason, interest in considering the emotions of the user in a human-computer interface has recently increased. This is important for future interface applications, which are expected to operate in harmony with humans. In this paper, we present our approach to instantaneously detecting the emotions of video viewers from remote measurement using an RGB camera. Facial expression and physiological responses, such as heart rate and pupil diameter, were measured by analyzing facial videos. We also verified the effectiveness of the contactless measurement by acquiring electroencephalogram signals using a contact-type electroencephalograph. By combining the measured responses into multimodal features and using machine learning, we showed that the results of emotion estimation were better than estimates made from only single-mode features.\r"
  },
  "cvpr2020_w19_stressestimationusingmultimodalbiosignalinformationfromrgbfacialvideo": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Stress Estimation Using Multimodal Biosignal Information From RGB Facial Video",
    "authors": [
      "Takumi Nagasawa",
      "Ryo Takahashi",
      "Chawan Koopipat",
      "Norimichi Tsumura"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Nagasawa_Stress_Estimation_Using_Multimodal_Biosignal_Information_From_RGB_Facial_Video_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Nagasawa_Stress_Estimation_Using_Multimodal_Biosignal_Information_From_RGB_Facial_Video_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In the present paper, we propose a method for acquiring multiple biological information inputs from a red-green-blue (RGB) facial video footage and using their feature values to estimate stress levels. Such estimations are important because if left unchecked, stress can cause severe mental illness and/or physical damage to the human body. Accordingly, it is important to understand the onset of stress at an early stage and take measures to counteract it. However, since it is difficult for us to accurately gauge our stress levels, it would be desirable to establish an objective and accurate estimation method. Additionally, while the commonly used questionnaire method is easy to implement, it lacks both objectivity and accuracy. In a recent study, many methods that use biological information were proposed. In the present study, we estimate stress using three biological signals captured using an RGB camera: pulse, blinking rate, and pupil diameter. Our results show that stress estimation accuracy is improved by using these biological signals, thereby indicating that it is possible to estimate stress more accurately by using biological information in a multimodal manner.\r"
  },
  "cvpr2020_w19_automateddepthvideomonitoringforfallreductionacasestudy": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Automated Depth Video Monitoring for Fall Reduction: A Case Study",
    "authors": [
      "Josh Brown Kramer",
      "Lucas Sabalka",
      "Ben Rush",
      "Katherine Jones",
      "Tegan Nolte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Kramer_Automated_Depth_Video_Monitoring_for_Fall_Reduction_A_Case_Study_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Kramer_Automated_Depth_Video_Monitoring_for_Fall_Reduction_A_Case_Study_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Patient falls are a common, costly, and serious safety problem in hospitals and health care facilities. We have created a system that reduces falls by using computer vision to monitor fall risk patients and alert staff of unsafe behavior before a fall happens. This paper is a companion and followup to \"Modeling bed exit likelihood in a camera-based automated video monitoring application,\" in which we describe the Ocuvera system. Here additional details are provided on that system and its processes. We report clinical results, detail practices used to iterate rapidly and effectively on a massive video database, discuss details of our people tracking algorithms, and discuss the engineering effort required to support the new Azure Kinect depth camera.\r"
  },
  "cvpr2020_w19_remotephotoplethysmographyrarelyconsideredfactors": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Remote Photoplethysmography: Rarely Considered Factors",
    "authors": [
      "Yuriy Mironenko",
      "Konstantin Kalinin",
      "Mikhail Kopeliovich",
      "Mikhail Petrushan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Mironenko_Remote_Photoplethysmography_Rarely_Considered_Factors_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Mironenko_Remote_Photoplethysmography_Rarely_Considered_Factors_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Remote Photoplethysmography (rPPG) is a fast-growing technique of vital sign estimation by analyzing video of a person. Several major phenomena affecting rPPG signals were studied (e.g. video compression, distance from person to camera, skin tone, head motions). However, to develop a highly accurate rPPG method, new, minor, factors should be studied. First considered factor is irregular frame rate of video recordings. Despite of PPG signal transformation by frame rate irregularity, no significant distortion of PPG signal spectra was found in the experiments. Second factor is rolling shutter effect which generates tiny phase shift of the same PPG signal in different parts of the frame caused by progressive scanning. In particular conditions effect of this artifact could be of the same order of magnitude as physiologically caused phase shifts. Third factor is a size of temporal windows, which could significantly influence the estimated error of vital sign evaluation. Short series of experiments were conducted to estimate importance of these phenomena and to determine necessity of their further comprehensive study.\r"
  },
  "cvpr2020_w19_insearchoflifelearningfromsyntheticdatatodetectvitalsignsinvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "In Search of Life: Learning From Synthetic Data to Detect Vital Signs in Videos",
    "authors": [
      "Florin Condrea",
      "Victor-Andrei Ivan",
      "Marius Leordeanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Condrea_In_Search_of_Life_Learning_From_Synthetic_Data_to_Detect_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Condrea_In_Search_of_Life_Learning_From_Synthetic_Data_to_Detect_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Automatically detecting vital signs in videos, such as the estimation of heart and respiration rates, is a challenging research problem in computer vision with important applications in the medical field. One of the key difficulties in tackling this task is the lack of sufficient supervised training data, which severely limits the use of powerful deep neural networks. In this paper we address this limitation through a novel deep learning approach, in which a recurrent deep neural network is trained to detect vital signs in the infrared thermal domain from purely synthetic data. What is most surprising is that our novel method for synthetic training data generation is general, relatively simple and uses almost no prior medical domain knowledge. Moreover, our system, which is trained in a purely automatic manner and needs no human annotation, also learns to predict the respiration or heart intensity signal for each moment in time and to detect the region of interest that is most relevant for the given task, e.g. the nose area in the case of respiration. We demonstrate the effectiveness of our proposed system on the recent LCAS dataset, where it obtains state-of-the-art performance.\r"
  },
  "cvpr2020_w19_convulsivemovementdetectionusinglow-resolutionthermopilesensorarray": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Convulsive Movement Detection Using Low-Resolution Thermopile Sensor Array",
    "authors": [
      "Ouday Hanosh",
      "Rashid Ansari",
      "Naoum P. Issa",
      "A. Enis Cetin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Hanosh_Convulsive_Movement_Detection_Using_Low-Resolution_Thermopile_Sensor_Array_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Hanosh_Convulsive_Movement_Detection_Using_Low-Resolution_Thermopile_Sensor_Array_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Sudden Unexplained Death in Epilepsy (SUDEP) is a fatal threat to patients who suffer from convulsive seizures. The causes of the SUDEP are still ambiguous, and the patients who suffer from epileptic seizures may face death in well sleeping, likely after an unwitnessed convulsive seizure. An important step towards SUDEP prevention is reliable seizure detection during sleep that is inexpensive and unobtrusive. In this work, we developed a non-contact, non-intrusive, privacy-preserving system that can detect convulsive movements experienced by human subjects. Detection is accomplished by a combination of uncooled low-cost, low-power, low-resolution 8 x 8 IR array sensor, and a deep learning algorithm implemented with a Convolutional Neural Network (CNN). The thermopile sensor array is placed 1m from subjects who are reclining in bed. The CNN training set consists of thermal video streams from 40 healthy subjects mimicking convulsive movements or lying in bed without making convulsive movements. After training, the CNN was tested on thermal video streams not included in the training set and had a 99.2% accuracy in classifying convulsive movements and non-convulsive episodes, with no false negatives to distinguish between the occurrence and non-occurrence of convulsive movements. The performance results show that the thermopile sensor array has the potential to detect convulsive seizures while maintaining patient privacy and not requiring direct patient contact.\r"
  },
  "cvpr2020_w19_predictingfallprobabilitybasedonavalidatedbalancescale": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Predicting Fall Probability Based on a Validated Balance Scale",
    "authors": [
      "Alaa Masalha",
      "Nadav Eichler",
      "Shmuel Raz",
      "Adi Toledano-Shubi",
      "Daphna Niv",
      "Ilan Shimshoni",
      "Hagit Hel-Or"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Masalha_Predicting_Fall_Probability_Based_on_a_Validated_Balance_Scale_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Masalha_Predicting_Fall_Probability_Based_on_a_Validated_Balance_Scale_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Accidental falls are the most frequent injury of old age and have dramatic implications on the individual, family, and the society as a whole. To date, fall prediction estimation is clinical, relying on the expertise of the physiotherapist for performing the diagnosis based on standard scales, such as the highly common and validated Berg Balance Scale (BBS). Unfortunately, the BBS is a time consuming subjective score, prone to variability and inconsistency between examiners. In this study, we developed an objective, computational tool, which automates the BBS fall assessment process and allows easy, efficient and accessible assessment of fall risk. The tool is based on a novel multi depth-camera human motion tracking system integrated with Machine Learning algorithms. The system enables large scale screening of the general public at very little cost while significantly reducing physiotherapist resources. The system was pilot tested in the physiotherapy unit at a major hospital and showed high rates of fall risk predictions as well as correlation with physiotherapists BBS scores on individual BBS motion tasks.\r"
  },
  "cvpr2020_w19_anassessmentofalgorithmstoestimaterespiratoryratefromtheremotephotoplethysmogram": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "An Assessment of Algorithms to Estimate Respiratory Rate From the Remote Photoplethysmogram",
    "authors": [
      "Duncan Luguern",
      "Simon Perche",
      "Yannick Benezeth",
      "Virginie Moser",
      "L. Andrea Dunbar",
      "Fabian Braun",
      "Alia Lemkaddem",
      "Keisuke Nakamura",
      "Randy Gomez",
      "Julien Dubois"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Luguern_An_Assessment_of_Algorithms_to_Estimate_Respiratory_Rate_From_the_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Luguern_An_Assessment_of_Algorithms_to_Estimate_Respiratory_Rate_From_the_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The respiratory rate is important information in the healthcare environment. Consequently, research is done to develop a device that could measure the respiratory rate continuously with non-contact devices. Various methods were tried, such as radio-based, thermal imaging or remote photoplethysmography (rPPG). The rPPG method uses a video recording of the skin in ambient light conditions. It measures the small variations of light reflection induced by the amount of blood in vessels. This method allows the extraction of physiological parameters such as the heart rate or respiratory rate without any contact with the skin. The main issue with the rPPG technique is the lower signal quality compared with contact-based methods. In this paper, we assess the performance of the respiratory rate estimation algorithms with rPPG signals. The tested algorithms were designed for contact-PPG signals input. The use of the algorithms designed for contact PPG on remote PPG signals can lead to respiratory rate estimations with a mean absolute error below 3 breaths-per-minute. We benchmark our results using this standard and some other metrics to interpret the quality of the assessment.\r"
  },
  "cvpr2020_w19_longshort-termmemorydeep-filterinremotephotoplethysmography": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Long Short-Term Memory Deep-Filter In Remote Photoplethysmography",
    "authors": [
      "Deivid Botina-Monsalve",
      "Yannick Benezeth",
      "Richard Macwan",
      "Paul Pierrart",
      "Federico Parra",
      "Keisuke Nakamura",
      "Randy Gomez",
      "Johel Miteran"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Botina-Monsalve_Long_Short-Term_Memory_Deep-Filter_In_Remote_Photoplethysmography_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Botina-Monsalve_Long_Short-Term_Memory_Deep-Filter_In_Remote_Photoplethysmography_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Remote photoplethysmography (rPPG) is a recent technique for estimating heart rate by analyzing subtle skin color variations using regular cameras. As multiple noise sources can pollute the estimated signal, post-processing techniques, such as bandpass filtering, are generally used. However, it is often possible to see alterations in the filtered signal that have not been suppressed, although an experienced eye can easily identify them. From this observation, we propose in this work to use an LSTM network to filter the rPPG signal. The network is able to learn the characteristic shape of the rPPG signal and especially its temporal structure, which is not possible with the usual signal processing-based filtering methods. The results of this study, obtained on a public database, have demonstrated that the proposed deep-learning-based filtering method outperforms the regular post-processing ones in terms of signal quality and accuracy of heart rate estimation.\r"
  },
  "cvpr2020_w19_detectingdeepfakevideosusingattribution-basedconfidencemetric": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Detecting Deepfake Videos Using Attribution-Based Confidence Metric",
    "authors": [
      "Steven Fernandes",
      "Sunny Raj",
      "Rickard Ewetz",
      "Jodh Singh Pannu",
      "Sumit Kumar Jha",
      "Eddy Ortiz",
      "Iustina Vintila",
      "Margaret Salter"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Fernandes_Detecting_Deepfake_Videos_Using_Attribution-Based_Confidence_Metric_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Fernandes_Detecting_Deepfake_Videos_Using_Attribution-Based_Confidence_Metric_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent advances in generative adversarial networks have made detecting fake videos a challenging task. In this paper, we propose the application of the state-of-the-art attribution based confidence (ABC) metric for detecting deepfake videos. The ABC metric does not require access to the training data or training the calibration model on the validation data. The ABC metric can be used to draw inferences even when only the trained model is available. Here, we utilize the ABC metric to characterize whether a video is original or fake. The deep learning model is trained only on original videos. The ABC metric uses the trained model to generate confidence values. For, original videos, the confidence values are greater than 0.94.\r"
  },
  "cvpr2020_w19_onindirectassessmentofheartrateinvideo": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "On Indirect Assessment of Heart Rate in Video",
    "authors": [
      "Mikhail Kopeliovich",
      "Konstantin Kalinin",
      "Yuriy Mironenko",
      "Mikhail Petrushan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Kopeliovich_On_Indirect_Assessment_of_Heart_Rate_in_Video_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Kopeliovich_On_Indirect_Assessment_of_Heart_Rate_in_Video_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Problem of indirect assessment of heart rate in video is addressed. Several methods of indirect evaluations (adaptive baselines) were examined on Remote Physiological Signal Sensing challenge. Particularly, regression models of dependency of heart rate on estimated age and motion intensity were obtained on challenge's train set. Accounting both motion and age in regression model led to top-quarter position in the leaderboard. Practical value of such adaptive baseline approaches is discussed. Although such approaches are considered as non-applicable in medicine, they are valuable as baseline for the photoplethysmography problem.\r"
  },
  "cvpr2020_w19_skinsegmentationusingactivecontoursandgaussianmixturemodelsforheartratedetectioninvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Skin Segmentation Using Active Contours and Gaussian Mixture Models for Heart Rate Detection in Videos",
    "authors": [
      "Alexander Woyczyk",
      "Vincent Fleischhauer",
      "Sebastian Zaunseder"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Woyczyk_Skin_Segmentation_Using_Active_Contours_and_Gaussian_Mixture_Models_for_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Woyczyk_Skin_Segmentation_Using_Active_Contours_and_Gaussian_Mixture_Models_for_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Current research focuses on non-contact means to capture physiological signals like the heart rate. One promising approach uses videos (imaging PPG, iPPG). The common procedure to derive the heart rate by iPPG comprises three steps: segmentation of a region of interest, usage of colour information from that region to yield a pulse signal and analysis of that signal to estimate the heart rate. This contribution proposes a novel approach to yield a region of interest using a Gaussian mixture model based level set formulation. The proposed method aims to segment a homogeneous region on an individual basis. To that end, we model the probability distributions for the pixel skin and non-skin class by two separate Gaussian mixture models. The proportion of the posterior probabilities are then included in the formulation of the level set function. The procedure yields a region of interest, which is used to derive a pulse signal from its average intensity or additional processing steps. We tested the method on own data and data of the 1st Challenge on Remote Physiological Signal Sensing. It is shown that the proposed method can improve the results for heart rate estimation on moving subjects. The potential of our approach is underlined by the promising result in the challenge.\r"
  },
  "cvpr2020_w19_the1stchallengeonremotephysiologicalsignalsensing(repss)": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "The 1st Challenge on Remote Physiological Signal Sensing (RePSS)",
    "authors": [
      "Xiaobai Li",
      "Hu Han",
      "Hao Lu",
      "Xuesong Niu",
      "Zitong Yu",
      "Antitza Dantcheva",
      "Guoying Zhao",
      "Shiguang Shan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Li_The_1st_Challenge_on_Remote_Physiological_Signal_Sensing_RePSS_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Li_The_1st_Challenge_on_Remote_Physiological_Signal_Sensing_RePSS_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Remote measurement of physiological signals from videos is an emerging topic. The topic draws great interests,but the lack of publicly available benchmark databases and a fair validation platform are hindering its further development. Forthisconcern,weorganizethefirstchallenge on Remote Physiological Signal Sensing (RePSS), in which two databases of VIPL and OBF are provided as the bench mark for kin researchers to evaluate their approaches. The 1st challenge of RePSS focuses on measuring the average heart rate from facial videos, which is the basic problem of remote physiological measurement. This paper presents an overview of the challenge, including data, protocol, analysis of results and discussion. The top ranked solutions are highlighted to provide insights for researchers, and future directions are outlined for this topic and this challenge.\r"
  },
  "cvpr2020_w19_neurodatalabsapproachtothechallengeoncomputervisionforphysiologicalmeasurement": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w19",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Physiological Measurement",
    "title": "Neurodata Lab's Approach to the Challenge on Computer Vision for Physiological Measurement",
    "authors": [
      "Mikhail Artemyev",
      "Marina Churikova",
      "Mikhail Grinenko",
      "Olga Perepelkina"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w19/Artemyev_Neurodata_Labs_Approach_to_the_Challenge_on_Computer_Vision_for_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w19/Artemyev_Neurodata_Labs_Approach_to_the_Challenge_on_Computer_Vision_for_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper introduces the Neurodata Lab's approach presented at the 1st Challenge on Remote Physiological Signal Sensing (RePSS) organized within CVPR2020. The RePSS challenge was focused on measuring the average heart rate from color facial videos, which is one of the most fundamental problems in the field of computer vision. Our deep learning-based approach includes 3D spatiotemporal attention convolutional neural network for photoplethysmogram extraction and 1D convolutional neural network pre-trained on synthetic data for time series analysis. It provides state-of-the-art results outperforming those of other participants on a mixture of VIPL and OBF databases: MAE=6.94 (12.3% improvement compared to the top-2 result), RMSE=10.68 (24.6% improvement), Pearson R = 0.755 (28.2% improvement).\r"
  },
  "cvpr2020_w54_activity-awareattributesforzero-shotdriverbehaviorrecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Activity-Aware Attributes for Zero-Shot Driver Behavior Recognition",
    "authors": [
      "Simon Reiss",
      "Alina Roitberg",
      "Monica Haurilet",
      "Rainer Stiefelhagen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Reiss_Activity-Aware_Attributes_for_Zero-Shot_Driver_Behavior_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Reiss_Activity-Aware_Attributes_for_Zero-Shot_Driver_Behavior_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In real-world environments, such as the vehicle cabin, we have to deal with novel concepts as they arise. To this end, we introduce ZS-Drive&Act - the first zero-shot activity classification benchmark specifically aimed at recognizing previously unseen driver behaviors. ZS-Drive&Act is unique due to its focus on fine-grained activities and presence of activity-driven attributes, which are automatically derived from a hierarchical annotation scheme. We adopt and evaluate multiple off-the-shelf zero-shot learning methods on our benchmark, showcasing the difficulties of such models when moving to our application-specific task. We further extend the prominent method based on feature generating Wasserstein GANs with a fusion strategy for linking semantic attributes and word vectors representing the behavior labels. Our experiments demonstrate the effectiveness of leveraging both semantic spaces simultaneously, improving the recognition rate by 2.79%.\r"
  },
  "cvpr2020_w54_diagnosingrarityinhuman-objectinteractiondetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Diagnosing Rarity in Human-Object Interaction Detection",
    "authors": [
      "Mert Kilickaya",
      "Arnold Smeulders"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Kilickaya_Diagnosing_Rarity_in_Human-Object_Interaction_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Kilickaya_Diagnosing_Rarity_in_Human-Object_Interaction_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Human-object interaction (HOI) detection is a core task in computer vision. The goal is to localize all human-object pairs and recognize their interactions. An interaction defined by a  tuple leads to a long-tailed visual recognition challenge since many combinations are rarely represented. The performance of the proposed models is limited especially for the tail categories, but little has been done to understand the reason. To that end, in this paper, we propose to diagnose rarity in HOI detection. We propose a three-steps strategy, namely Detection, Identification and Recognition where we carefully analyse the limiting factors by studying state-of-the-art models. Our findings indicate that detection and identification steps are altered by the interaction signals like occlusion and relative location, as a result limiting the recognition accuracy.\r"
  },
  "cvpr2020_w54_pose-guidedknowledgetransferforobjectpartsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Pose-Guided Knowledge Transfer for Object Part Segmentation",
    "authors": [
      "Shujon Naha",
      "Qingyang Xiao",
      "Prianka Banik",
      "Md Alimoor Reza",
      "David J. Crandall"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Naha_Pose-Guided_Knowledge_Transfer_for_Object_Part_Segmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Naha_Pose-Guided_Knowledge_Transfer_for_Object_Part_Segmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Object part segmentation is an important problem for many applications, but generating the annotations to train a part segmentation model is typically quite labor-intensive.Recently, Fang et al. [6] augmented object part segmentation datasets by using keypoint locations as weak supervision to transfer a source object instance's part annotations to an unlabeled target object. We show that while their approach works well when the source and target objects have clearly visible keypoints, it often fails for severely articulated poses. Also, their model does not generalize well across multiple object classes, even if they are very similar. In this paper, we propose and evaluate a new model for transferring part segmentations using keypoints, even for complex object poses and across different object classes.\r"
  },
  "cvpr2020_w54_ma3modelagnosticadversarialaugmentationforfewshotlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "MA3: Model Agnostic Adversarial Augmentation for Few Shot Learning",
    "authors": [
      "Rohit Jena",
      "Shirsendu Sukanta Halder",
      "Katia Sycara"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Jena_MA3_Model_Agnostic_Adversarial_Augmentation_for_Few_Shot_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Jena_MA3_Model_Agnostic_Adversarial_Augmentation_for_Few_Shot_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Despite the recent developments in vision-related problems using deep neural networks, there still remains a wide scope in the improvement of generalizing these models to unseen examples. In this paper, we explore the domain of few-shot learning with a novel augmentation technique. In contrast to other generative augmentation techniques, where the distribution over input images are learnt, we propose to learn the probability distribution over the image transformation parameters which are easier and quicker to learn. Our technique is fully differentiable which enables its extension to versatile data-sets and base models. We evaluate our proposed method on multiple base-networks and 2 data-sets to establish the robustness and efficiency of this method. We obtain an improvement of nearly 4% by adding our augmentation module without making any change in network architectures. We also make the code readily available for usage by the community.\r"
  },
  "cvpr2020_w54_epilliddatasetalow-shotfine-grainedbenchmarkforpillidentification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill Identification",
    "authors": [
      "Naoto Usuyama",
      "Natalia Larios Delgado",
      "Amanda K. Hall",
      "Jessica Lundin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Usuyama_ePillID_Dataset_A_Low-Shot_Fine-Grained_Benchmark_for_Pill_Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Usuyama_ePillID_Dataset_A_Low-Shot_Fine-Grained_Benchmark_for_Pill_Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Identifying prescription medications is a frequent task for patients and medical professionals; however, this is an error-prone task as many pills have similar appearances (e.g. white round pills), which increases the risk of medication errors. In this paper, we introduce ePillID, the largest public benchmark on pill image recognition, composed of 13k images representing 8184 appearance classes (two sides for 4092 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting. We present our experimental setup and evaluation results of various baseline models on the benchmark. The best baseline using a multi-head metric-learning approach with bilinear features performed remarkably well; however, our error analysis suggests that they still fail to distinguish particularly confusing classes.\r"
  },
  "cvpr2020_w54_image2audiofacilitatingsemi-supervisedaudioemotionrecognitionwithfacialexpressionimage": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Image2Audio: Facilitating Semi-Supervised Audio Emotion Recognition With Facial Expression Image",
    "authors": [
      "Gewen He",
      "Xiaofeng Liu",
      "Fangfang Fan",
      "Jane You"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/He_Image2Audio_Facilitating_Semi-Supervised_Audio_Emotion_Recognition_With_Facial_Expression_Image_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/He_Image2Audio_Facilitating_Semi-Supervised_Audio_Emotion_Recognition_With_Facial_Expression_Image_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " There is a large amount of public available labeled image-based facial expression recognition datasets. How could these images help for the audio emotion recognition with limited labeled data according to their inherent correlations can be a meaningful and challenging task. In this paper, we propose a semi-supervised adversarial network that allows the knowledge transfer from the labeled videos to the heterogeneous labeled audio domain hence enhancing the audio emotion recognition performance. Specifically, face image samples are translated to the spectrograms class-wisely. To harness the translated samples in a sparsely distributed area and construct a tighter decision boundary, we propose to precisely estimate the density on feature space and incorporate the reliable low-density sample with an annealing scheme. Moreover, the unlabeled audios are collected with the high-density path in a graph representation. As a possible \"recognition via generation\" framework, we empirically demonstrated its effectiveness on several audio emotional recognition benchmarks.\r"
  },
  "cvpr2020_w54_auto-annotationqualitypredictionforsemi-supervisedlearningwithensembles": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Auto-Annotation Quality Prediction for Semi-Supervised Learning With Ensembles",
    "authors": [
      "Dror Simon",
      "Miriam Farber",
      "Roman Goldenberg"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Simon_Auto-Annotation_Quality_Prediction_for_Semi-Supervised_Learning_With_Ensembles_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Simon_Auto-Annotation_Quality_Prediction_for_Semi-Supervised_Learning_With_Ensembles_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Auto-annotation by an ensemble of models is an efficient method of learning on unlabeled data. However, wrong or inaccurate annotations generated by the ensemble may lead to performance degradation of the trained model. We propose filtering the auto-labeled data using a trained model that predicts the quality of the annotation from the degree of consensus between ensemble models. Using semantic segmentation as an example, we demonstrate the advantage of the proposed auto-annotation filtering over training on data contaminated with inaccurate labels. We show that the performance of a state-of-the-art model can be achieved by training it with only a fraction (30%) of the original manually labeled samples, and replacing the rest with auto-annotated, quality filtered labels.\r"
  },
  "cvpr2020_w54_clarelclassificationviaretrievallossforzero-shotlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "CLAREL: Classification via Retrieval Loss for Zero-Shot Learning",
    "authors": [
      "Boris N. Oreshkin",
      "Negar Rostamzadeh",
      "Pedro O. Pinheiro",
      "Christopher Pal"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Oreshkin_CLAREL_Classification_via_Retrieval_Loss_for_Zero-Shot_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Oreshkin_CLAREL_Classification_via_Retrieval_Loss_for_Zero-Shot_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We address the problem of learning cross-modal representations. We propose an instance-based deep metric learning approach in joint visual and textual space. The key novelty of this paper is that it shows that using per-image semantic supervision leads to substantial improvement in zero-shot performance over using class-only supervision. We also provide a probabilistic justification and empirical validation for a metric rescaling approach to balance the seen/unseen accuracy in the GZSL task. We evaluate our approach on two fine-grained zero-shot datasets: CUB and FLOWERS.\r"
  },
  "cvpr2020_w54_unsupervisedbatchnormalization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Unsupervised Batch Normalization",
    "authors": [
      "Mustafa Taha Kocyigit",
      "Laura Sevilla-Lara",
      "Timothy M. Hospedales",
      "Hakan Bilen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Kocyigit_Unsupervised_Batch_Normalization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Kocyigit_Unsupervised_Batch_Normalization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Batch Normalization is a widely used tool in neural networks to improve the generalization and convergence of training. However, on small datasets due to the difficulty of obtaining unbiased batch statistics it cannot be applied effectively. In some cases, even if there is only a small labeled dataset available, there are larger unlabeled datasets from the same distribution. We propose using such unlabeled examples to calculate batch normalization statistics, which we call Unsupervised Batch Normalization (UBN). We show that using unlabeled examples for batch statistic calculations results in a reduction of the bias of the statistics, as well as regularization leveraging the data manifold. UBN is easy to implement, computationally inexpensive and can be applied to a variety problems. We report results on monocular depth estimation, where obtaining dense labeled examples is difficult and expensive. Using unlabeled samples, and UBN, we obtain an increase in accuracy of more than 6% on the KITTI dataset, compared to using traditional batch normalization only on the labeled samples.\r"
  },
  "cvpr2020_w54_takethescenicrouteimprovinggeneralizationinvision-and-languagenavigation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Take the Scenic Route: Improving Generalization in Vision-and-Language Navigation",
    "authors": [
      "Felix Yu",
      "Zhiwei Deng",
      "Karthik Narasimhan",
      "Olga Russakovsky"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Yu_Take_the_Scenic_Route_Improving_Generalization_in_Vision-and-Language_Navigation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Yu_Take_the_Scenic_Route_Improving_Generalization_in_Vision-and-Language_Navigation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In the Vision-and-Language Navigation (VLN) task, an agent with egocentric vision navigates to a destination given natural language instructions. The act of manually annotating these instructions is timely and expensive, such that many existing approaches automatically generate additional samples to improve agent performance. However, these approaches still have difficulty generalizing their performance to new environments. In this work, we investigate the popular Room-to-Room (R2R) VLN benchmark and discover that it's not only about the amount data you synthesize, but how you do it. We find that shortest path sampling, which is used by both the R2R benchmark and existing augmentation methods, encode biases in the action space of the agent which we dub as action priors. We then show that these action priors offer one explanation toward the poor generalization of existing works. To mitigate such priors, we propose a path sampling method based on random walks to augment the data. By training with this augmentation strategy, our agent is able to generalize better to unknown environments compared to the baseline, significantly improving model performance in the process.\r"
  },
  "cvpr2020_w54_anembarrassinglysimplebaselinetoone-shotlearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "An Embarrassingly Simple Baseline to One-Shot Learning",
    "authors": [
      "Chen Liu",
      "Chengming Xu",
      "Yikai Wang",
      "Li Zhang",
      "Yanwei Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Liu_An_Embarrassingly_Simple_Baseline_to_One-Shot_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Liu_An_Embarrassingly_Simple_Baseline_to_One-Shot_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose an embarrassingly simple approach for one-shot learning. Our insight is that the one-shot tasks have domain gap to the network pretrained tasks and thus some features from the pretrained network are not relevant, or harmful to the specific one-shot task. Therefore, we propose to directly prune the features from the pretrained network for a specific one-shot task rather than update it via an optimized scheme with complex network structure. Without bells and whistles, our simple yet effective method achieves leading performances on miniImageNet (60.63%) and tieredImageNet (69.02%) for 5-way one-shot setting. The best trial can hit to 66.83% on miniImageNet and 74.04% on tieredImageNet, establishing a new state-of-the-art. We strongly advocate that our method can serve as a strong baseline for one-shot learning. The codes and trained models will be released at http://github.com/corwinliu9669/embarrassingly-simple-baseline.\r"
  },
  "cvpr2020_w54_towardsfine-grainedsamplingforactivelearninginobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Towards Fine-Grained Sampling for Active Learning in Object Detection",
    "authors": [
      "Sai Vikas Desai",
      "Vineeth N Balasubramanian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Desai_Towards_Fine-Grained_Sampling_for_Active_Learning_in_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Desai_Towards_Fine-Grained_Sampling_for_Active_Learning_in_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We study the problem of using active learning to reduce annotation effort in training object detectors. Existing efforts in this space ignore the fact that image annotation costs are variable, depending on the number of objects present in a single image. In this regard, we examine a fine-grained sampling based approach for active learning in object detection. Over an unlabeled pool of images, our method aims to selectively pick the most informative subset of bounding boxes (as opposed to full images) to query an annotator. We measure annotation efforts in terms of the number of ground truth bounding boxes obtained. We study the effects of our method on the Feature Pyramid Network and RetinaNet models, and show promising savings in labeling effort to obtain good detection performance.\r"
  },
  "cvpr2020_w54_zero-shotlearninginthepresenceofhierarchicallycoarsenedlabels": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Zero-Shot Learning in the Presence of Hierarchically Coarsened Labels",
    "authors": [
      "Colin Samplawski",
      "Erik Learned-Miller",
      "Heesung Kwon",
      "Benjamin M. Marlin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Samplawski_Zero-Shot_Learning_in_the_Presence_of_Hierarchically_Coarsened_Labels_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Samplawski_Zero-Shot_Learning_in_the_Presence_of_Hierarchically_Coarsened_Labels_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Zero-shot image classification leverages side information including label attributes and semantic class hierarchies to transfer knowledge about fine-grained training classes to fine-grained zero-shot classes. In this paper, we consider the problem of zero-shot learning of fine-grained classes given a mixture of images with fine-grained and coarsened labels. We show how probabilistic hierarchical classification models can be used to simultaneously accommodate fine and coarse-grained labels in the zero-shot learning setting. We show that this approach is robust even to significant levels of coarsening.\r"
  },
  "cvpr2020_w54_cross-domainknowledgetransferforpredictionofchemosensitivityinovariancancerpatients": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Cross-Domain Knowledge Transfer for Prediction of Chemosensitivity in Ovarian Cancer Patients",
    "authors": [
      "Asfand Yaar",
      "Amina Asif",
      "Shan E Ahmed Raza",
      "Nasir Rajpoot",
      "Fayyaz Minhas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Yaar_Cross-Domain_Knowledge_Transfer_for_Prediction_of_Chemosensitivity_in_Ovarian_Cancer_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Yaar_Cross-Domain_Knowledge_Transfer_for_Prediction_of_Chemosensitivity_in_Ovarian_Cancer_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we report a novel deep neural network framework for prediction of chemo-sensitivity in ovarian cancer patients. The proposed model is based on Multiple Instance Learning (MIL) and a novel variant of Learning using Privileged Information (LUPI). LUPI allows knowledge transfer from highly informative privileged features that are available only at training time to give improved generalization performance on input space features which are available in both training and inference. The proposed model is trained on image patches from Hematoxylin and Eosin (H&E) stained multi-gigapixel whole-slide images (WSIs, the input space) of ovarian cancer tissue sections and their associated gene expression profiles, the privileged feature space. Through cross-domain knowledge transfer with a novel combination of MIL and LUPI, we achieve improved generalization with a limited number of labeled examples in the input space. Informed by the privileged space model output based on relatively expensive and time-consuming gene expression profiles in its training, the proposed LUPI model can generate accurate predictions using routine WSI data alone at the time of inference. The proposed method paves the way for further applications of LUPI in computational pathology and medical image analysis by cross-domain learning especially in cases with a limited number of labeled examples in training.\r"
  },
  "cvpr2020_w54_selectingauxiliarydatausingknowledgegraphsforimageclassificationwithlimitedlabels": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Selecting Auxiliary Data Using Knowledge Graphs for Image Classification With Limited Labels",
    "authors": [
      "Elaheh Raisi",
      "Stephen H. Bach"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Raisi_Selecting_Auxiliary_Data_Using_Knowledge_Graphs_for_Image_Classification_With_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Raisi_Selecting_Auxiliary_Data_Using_Knowledge_Graphs_for_Image_Classification_With_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a learning algorithm for training deep neural networks when there is not sufficient labeled data. To improve the generalization capabilities of the deep model, we adopt a learning scheme to train two related tasks simultaneously. One is the original task (target), and the other is an auxiliary task (source). In order to create a related auxiliary task, we leverage an available knowledge graph to query for semantically related concepts that are grounded in labeled images; hence we call our method KGAuxLearn. We jointly train the target and source tasks in a multi-task architecture. We evaluate our method on two fine-grained visual categorization benchmarks: Oxford Flowers 102 and CUB-200-2011. Our experiments demonstrate that the error rate reduced by at least 2.1% over fine tuning for both datasets. We also improve error rate by 1.36% and 2.93% over using randomly selected concepts as an auxiliary task for Oxford Flowers 102 and CUB-200-2011, respectively. In addition, comparing our method with auxiliary data selection methods that do not use a knowledge graph, the error rate improves by 0.69% and 2.57% on Oxford Flowers 102 and CUB-200-2011, respectively.\r"
  },
  "cvpr2020_w54_relativepositionandmapnetworksinfew-shotlearningforimageclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Relative Position and Map Networks in Few-Shot Learning for Image Classification",
    "authors": [
      "Zhiyu Xue",
      "Zhenshan Xie",
      "Zheng Xing",
      "Lixin Duan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Xue_Relative_Position_and_Map_Networks_in_Few-Shot_Learning_for_Image_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Xue_Relative_Position_and_Map_Networks_in_Few-Shot_Learning_for_Image_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " few-shot learning is an important research topic in image classification, which aims to train robust classifiers to categorize images coming from new classes where only a few labeled samples are available. Recently, metric learning based methods have achieved promising performance, and in those methods a distance metric is learned to directly compare query images against training samples. In this work, we consider finer information from image feature maps and propose a new approach. Specifically, we newly develop Relative Position Network (RPN) based on the attention mechanism to compare different pairs of activation cells from each query and training images, which captures their intrinsic correspondences. Moreover, we introduce Relative Map Network (RMN) to learn a distance metric based on the attention maps obtained from RPN, which better measures the similarity between query and training images.\r"
  },
  "cvpr2020_w54_any-shotsequentialanomalydetectioninsurveillancevideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Any-Shot Sequential Anomaly Detection in Surveillance Videos",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Doshi_Any-Shot_Sequential_Anomaly_Detection_in_Surveillance_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Doshi_Any-Shot_Sequential_Anomaly_Detection_in_Surveillance_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Anomaly detection in surveillance videos has been recently gaining attention. Even though the performance of state-of-the-art methods on publicly available data sets has been competitive, they demand a massive amount of training data. Also, they lack a concrete approach for continuously updating the trained model once new data is available. Furthermore, online decision making is an important but mostly neglected factor in this domain. Motivated by these research gaps, we propose an online anomaly detection method for surveillance videos using transfer learning and any-shot learning, which in turn significantly reduces the training complexity and provides a mechanism which can detect anomalies using only a few labeled nominal examples. Our proposed algorithm leverages the feature extraction power of neural network-based models for transfer learning, and the any-shot learning capability of statistical detection methods.\r"
  },
  "cvpr2020_w54_alleviatingsemantic-levelshiftasemi-superviseddomainadaptationmethodforsemanticsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Alleviating Semantic-Level Shift: A Semi-Supervised Domain Adaptation Method for Semantic Segmentation",
    "authors": [
      "Zhonghao Wang",
      "Yunchao Wei",
      "Rogerio Feris",
      "Jinjun Xiong",
      "Wen-mei Hwu",
      "Thomas S. Huang",
      "Honghui Shi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Wang_Alleviating_Semantic-Level_Shift_A_Semi-Supervised_Domain_Adaptation_Method_for_Semantic_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Wang_Alleviating_Semantic-Level_Shift_A_Semi-Supervised_Domain_Adaptation_Method_for_Semantic_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Utilizing synthetic data for semantic segmentation can significantly relieve human efforts in labelling pixel-level masks. A key challenge of this task is how to alleviate the data distribution discrepancy between the source and target domains, i.e. reducing domain shift. The common approach to this problem is to minimize the discrepancy between feature distributions from different domains through adversarial training. However, directly aligning the feature distribution globally cannot guarantee consistency from a local view (i.e. semantic-level). To tackle this issue, we propose a semi-supervised approach named Alleviating Semantic-level Shift (ASS), which can promote the distribution consistency from both global and local views. We apply our ASS to two domain adaptation tasks, from GTA5 to Cityscapes and from Synthia to Cityscapes. Extensive experiments demonstrate that: (1) ASS can significantly outperform the current unsupervised state-of-the-arts by employing a small number of annotated samples from the target domain; (2) ASS can beat the oracle model trained on the whole target dataset by over 3 points by augmenting the synthetic source data with annotated samples from the target domain without suffering from the prevalent problem of overfitting to the source domain.\r"
  },
  "cvpr2020_w54_self-supervisedlearningoflocalfeaturesin3dpointclouds": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Self-Supervised Learning of Local Features in 3D Point Clouds",
    "authors": [
      "Ali Thabet",
      "Humam Alwassel",
      "Bernard Ghanem"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Thabet_Self-Supervised_Learning_of_Local_Features_in_3D_Point_Clouds_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Thabet_Self-Supervised_Learning_of_Local_Features_in_3D_Point_Clouds_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a self-supervised task on point clouds, in order to learn meaningful point-wise features that encode local structure around each point. Our self-supervised network, operates directly on unstructured/unordered point clouds. Using a multi-layer RNN, our architecture predicts the next point in a point sequence created by a popular and fast Space Filling Curve, the Morton-order curve. The final RNN state (coined Morton feature) is versatile and can be used in generic 3D tasks on point clouds. Our experiments show how our self-supervised task results in features that are useful for 3D segmentation tasks, and generalize well between datasets. We show how Morton features can be used to significantly improve performance (+3% for 2 popular algorithms) in semantic segmentation of point clouds on the challenging and large-scale S3DIS dataset. We also show how our self-supervised network pretrained on S3DIS transfers well to another large-scale dataset, vKITTI, leading to 11% improvement. Our code is publicly available.\r"
  },
  "cvpr2020_w54_asimplediscriminativedualsemanticauto-encoderforzero-shotclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "A Simple Discriminative Dual Semantic Auto-Encoder for Zero-Shot Classification",
    "authors": [
      "Yang Liu",
      "Jin Li",
      "Xinbo Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Liu_A_Simple_Discriminative_Dual_Semantic_Auto-Encoder_for_Zero-Shot_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Liu_A_Simple_Discriminative_Dual_Semantic_Auto-Encoder_for_Zero-Shot_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Most existing ZSL models focus on searching the mapping between visual space and semantic space directly. However, few models study whether the human-designed semantic information is discriminative enough to recognize different categories. On the other hand, one-way mapping typically suffers from the project domain shift problem. Inspired by the encoder-decoder paradigm, we propose a novel solution to ZSL based on learning a Discriminative Dual Semantic Auto-encoder (DDSA). DDSA aims to build an aligned space to bridge the visual space and the semantic space by learning two bidirectional mappings, which provides us the required discriminative information about the visual and semantic features in the aligned space. The key to the proposed model is that we implicitly exact the principal information from visual and semantic space to construct aligned features, which is not only semantic-preserving but also discriminative. Extensive experiments on five benchmark data sets demonstrate the effectiveness of the proposed approach.\r"
  },
  "cvpr2020_w54_viservisualself-regularization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "ViSeR: Visual Self-Regularization",
    "authors": [
      "Hamid Izadinia",
      "Pierre Garrigues"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Izadinia_ViSeR_Visual_Self-Regularization_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Izadinia_ViSeR_Visual_Self-Regularization_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose using large set of unlabeled images as a source of regularization data for learning robust representation. Given a visual model trained in a supervised fashion, we augment our training samples by incorporating large number of unlabeled data and train a semi-supervised model. We demonstrate that our proposed learning approach leverages an abundance of unlabeled images and boosts the visual recognition performance which alleviates the need to rely on large labeled datasets for learning robust representation. In our approach, each labeled image propagates its label to its nearest unlabeled image instances. These retrieved unlabeled images serve as local perturbations of each labeled image to perform Visual Self-Regularization VISER. Using the labeled instances and our regularizers we show that we significantly improve object categorization and localization on the MS COCO and Visual Genome datasets.\r"
  },
  "cvpr2020_w54_context-guidedsuper-classinferenceforzero-shotdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Context-Guided Super-Class Inference for Zero-Shot Detection",
    "authors": [
      "Yanan Li",
      "Yilan Shao",
      "Donghui Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Li_Context-Guided_Super-Class_Inference_for_Zero-Shot_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Li_Context-Guided_Super-Class_Inference_for_Zero-Shot_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Zero-shot object detection (ZSD) is a newly proposed research problem, which aims to simultaneously locate and recognize objects of previously unseen classes. Existing algorithms usually formulate it as a simple combination of a typical detection framework and zero-shot classifier, by learning a visual-semantic mapping from the visual features of bounding box proposals to semantic embeddings of class labels. In this paper, we propose a novel ZSD approach that leverages the context information surrounding objects in the image, following the principle that objects tend to be found in certain contexts. It also incorporates the semantic relations between seen and unseen classes to help recognize located instances. Comprehensive experiments on PASCAL VOC and MS COCO datasets show that context and class hierarchy truly improve the performance of detection.\r"
  },
  "cvpr2020_w54_rethinkingsegmentationguidanceforweaklysupervisedobjectdetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w54",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Visual Learning With Limited Labels: Zero-Shot, Few-Shot, Any-Shot, and Cross-Domain Few-Shot Learning",
    "title": "Rethinking Segmentation Guidance for Weakly Supervised Object Detection",
    "authors": [
      "Ke Yang",
      "Peng Zhang",
      "Peng Qiao",
      "Zhiyuan Wang",
      "Huadong Dai",
      "Tianlong Shen",
      "Dongsheng Li",
      "Yong Dou"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w54/Yang_Rethinking_Segmentation_Guidance_for_Weakly_Supervised_Object_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w54/Yang_Rethinking_Segmentation_Guidance_for_Weakly_Supervised_Object_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Weakly supervised object detection aims at learning object detectors with only image-level category labels. Most existing methods tend to solve this problem by using a multiple instance learning detector which is usually trapped to discriminate object parts, rather than the entire object. In order to select high-quality proposals, recent works leverage objectness scores derived from weakly-supervised segmentation maps to rank the object proposals. Base our observation, this kind of segmentation guided method always fails due to neglect of the fact that objectness of all proposals inside the ground-truth box should be consistent. In this paper, we propose a novel object representation named Objectness Consistent Representation (OCR) to meet the consistency criterion of objectness. Specifically, we project the segmentation confidence scores into two orthogonal directions, namely vertical and horizontal, to get the OCR. With the novel object representation, more high-quality proposals can be mined for learning a much stronger object detector.\r"
  },
  "cvpr2020_w31_ntire2020challengeonimageandvideodeblurring": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Image and Video Deblurring",
    "authors": [
      "Seungjun Nah",
      "Sanghyun Son",
      "Radu Timofte",
      "Kyoung Mu Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Nah_NTIRE_2020_Challenge_on_Image_and_Video_Deblurring_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Nah_NTIRE_2020_Challenge_on_Image_and_Video_Deblurring_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Motion blur is one of the most common degradation artifacts in dynamic scene photography. This paper reviews the NTIRE 2020 Challenge on Image and Video Deblurring. In this challenge, we present the evaluation results from 3 competition tracks as well as the proposed solutions. Track 1 aims to develop single-image deblurring methods focusing on restoration quality. On Track 2, the image deblurring methods are executed on a mobile platform to find the balance of the running speed and the restoration accuracy. Track 3 targets developing video deblurring methods that exploit the temporal relation between input frames. In each competition, there were 163, 135, and 102 registered participants and in the final testing phase, 9, 4, and 7 teams competed. The winning methods demonstrate the state-of-the-art performance on image and video deblurring tasks.\r"
  },
  "cvpr2020_w31_renderingnaturalcamerabokeheffectwithdeeplearning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Rendering Natural Camera Bokeh Effect With Deep Learning",
    "authors": [
      "Andrey Ignatov",
      "Jagruti Patel",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ignatov_Rendering_Natural_Camera_Bokeh_Effect_With_Deep_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ignatov_Rendering_Natural_Camera_Bokeh_Effect_With_Deep_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Bokeh is an important artistic effect used to highlight the main object of interest on the photo by blurring all out-of-focus areas. While DSLR and system camera lenses can render this effect naturally, mobile cameras are unable to produce shallow depth-of-field photos due to a very small aperture diameter of their optics. Unlike the current solutions simulating bokeh by applying Gaussian blur to image background, in this paper we propose to learn a realistic shallow focus technique directly from the photos produced by DSLR cameras. For this, we present a large-scale bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR with 50mm f/1.8 lenses. We use these images to train a deep learning model to reproduce a natural bokeh effect based on a single narrow-aperture image. The experimental results show that the proposed approach is able to render a plausible non-uniform bokeh even in case of complex input data with multiple objects. The dataset, pre-trained models and codes used in this paper are available on the project website: https://people.ee.ethz.ch/ ihnatova/pynet-bokeh.html\r"
  },
  "cvpr2020_w31_deepwaveletnetworkwithdomainadaptationforsingleimagedemoireing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Wavelet Network With Domain Adaptation for Single Image Demoireing",
    "authors": [
      "Xiaotong Luo",
      "Jiangtao Zhang",
      "Ming Hong",
      "Yanyun Qu",
      "Yuan Xie",
      "Cuihua Li"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Luo_Deep_Wavelet_Network_With_Domain_Adaptation_for_Single_Image_Demoireing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Luo_Deep_Wavelet_Network_With_Domain_Adaptation_for_Single_Image_Demoireing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Convolutional neural networks have made a prominent progress in low-level image restoration tasks. Moire is a kind of high-frequency and irregular interference stripe that appears on the photosensitive element of digital cameras or scanners. It can bring in unpleasant colorful artifacts on images. In this paper, we propose a deep wavelet network with domain adaptation mechanism for single image demoireing, dubbed AWUDN. The feature mapping is mainly performed in the wavelet domain, which can not only cut down computation complexity, but also reduce information loss. Moreover, considering that the images provided by the challenge organizers have strong self-similarity, the global context block is adopted for the learning of feature dependency in different positions. Finally, we introduce the domain adaptation mechanism to fine-tune the pretrained model for reducing the domain gap between training moire dataset and testing moire dataset. Benefiting from these improvements, the proposed method can achieve superior accuracy on the public testing dataset in the NTIRE 2020 Single Image Demoireing Challenge.\r"
  },
  "cvpr2020_w31_hierarchicalregressionnetworkforspectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Hierarchical Regression Network for Spectral Reconstruction From RGB Images",
    "authors": [
      "Yuzhi Zhao",
      "Lai-Man Po",
      "Qiong Yan",
      "Wei Liu",
      "Tingyu Lin"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Zhao_Hierarchical_Regression_Network_for_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Zhao_Hierarchical_Regression_Network_for_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Capturing visual image with a hyperspectral camera has been successfully applied to many areas due to its narrow-band imaging technology. Hyperspectral reconstruction from RGB images denotes a reverse process of hyperspectral imaging by discovering an inverse response function. Current works mainly map RGB images directly to corresponding spectrum but do not consider context information explicitly. Moreover, the use of encoder-decoder pair in current algorithms leads to loss of information. To address these problems, we propose a 4-level Hierarchical Regression Network (HRNet) with PixelShuffle layer as inter-level interaction. Furthermore, we adopt a residual dense block to remove artifacts of real world RGB images and a residual global block to build attention mechanism for enlarging perceptive field. We evaluate proposed HRNet with other architectures and techniques by participating in NTIRE 2020 Challenge on Spectral Reconstruction from RGB Images. The HRNet is the winning method of track 2 - real world images and ranks 3rd on track 1 - clean images.\r"
  },
  "cvpr2020_w31_investigatinglossfunctionsforextremesuper-resolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Investigating Loss Functions for Extreme Super-Resolution",
    "authors": [
      "Younghyun Jo",
      "Sejong Yang",
      "Seon Joo Kim"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Jo_Investigating_Loss_Functions_for_Extreme_Super-Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Jo_Investigating_Loss_Functions_for_Extreme_Super-Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The performance of image super-resolution (SR) has been greatly improved by using convolutional neural networks. Most of the previous SR methods have been studied up to x4 upsampling, and few were studied for x16 upsampling. The general approach for perceptual x4 SR is using GAN with VGG based perceptual loss, however, we found that it creates inconsistent details for perceptual x16 SR. To this end, we have investigated loss functions and we propose to use GAN with LPIPS loss for perceptual extreme SR. In addition, we use U-net structure discriminator together to consider both the global and local context of an input image. Experimental results show that our method outperforms the conventional perceptual loss, and we achieved second place in preliminary results of NTIRE 2020 perceptual extreme SR challenge.\r"
  },
  "cvpr2020_w31_c3netdemoireingnetworkattentiveinchannel,colorandconcatenation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "C3Net: Demoireing Network Attentive in Channel, Color and Concatenation",
    "authors": [
      "Sangmin Kim",
      "Hyungjoon Nam",
      "Jisu Kim",
      "Jechang Jeong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Kim_C3Net_Demoireing_Network_Attentive_in_Channel_Color_and_Concatenation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Kim_C3Net_Demoireing_Network_Attentive_in_Channel_Color_and_Concatenation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Attentive neural networks for image restoration are in the spotlight because they got remarkable results both qualitatively and quantitatively. Networks attentive in RGB channels were effective in fields such as single image super-resolution and RAW to RGB mapping. In addition, networks attentive in positions of pixels were used in image denoising. However, networks attentive in positions of pixels, so called spatial attention or pixel attention algorithm, were not as effective in image restoration because the number of pixels in patches of an image is so many that the weights by sigmoid function are insignificant. Also, networks attentive in positions of pixels were mainly used in high-level vision such as image classification and image captioning where there is no need to restore an image itself. In this paper, we propose a demoireing network attentive in channel, color, and concatenation, named C3Net. The proposed algorithm uses residual blocks attentive in RGB channels to take advantage of channel attention algorithm. In addition, we introduce a L1 color loss for demoireing to solve moire patterns caused by color-striped patterns. Also, we transferred multi-scale information by concatenation, not multiplying with the insignificant weights by sigmoid function. As a result, our proposed C3Net showed state-of-the-art results in the benchmark dataset on NTIRE 2020 demoireing challenge.\r"
  },
  "cvpr2020_w31_guidedfrequencyseparationnetworkforreal-worldsuper-resolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Guided Frequency Separation Network for Real-World Super-Resolution",
    "authors": [
      "Yuanbo Zhou",
      "Wei Deng",
      "Tong Tong",
      "Qinquan Gao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Zhou_Guided_Frequency_Separation_Network_for_Real-World_Super-Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Zhou_Guided_Frequency_Separation_Network_for_Real-World_Super-Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Training image pairs are unavailable generally in real-world super-resolution. Although the LR images can be down-scaled from HR images, some real-world characteristics (such as artifacts or sensor noise) have been removed from the degraded images. Therefore, most of state-of-the-art super-resolved methods often fail in real-world scenes. In order to address aforementioned problem, we proposed an unsupervised super-resolved solution. The method can be divided into two stages: domain transformation and super-resolution. A color-guided domain mapping network was proposed to alleviate the color shift in domain transformation process. In particular, we proposed the Color Attention Residual Block (CARB) as the basic unit of the domain mapping network. The CARB which can dynamically regulate the parameters is driven by input data. Therefore, the domain mapping network can result in the powerful generalization performance. Moreover, we modified the discriminator of the super-resolution stage so that the network not only keeps the high frequency features, but also maintains the low frequency features. Finally, we constructed an EdgeLoss to improve the texture details. Experimental results show that our solution can achieve a competitive performance on NTIRE 2020 real-world super-resolution challenge.\r"
  },
  "cvpr2020_w31_tridentdehazingnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Trident Dehazing Network",
    "authors": [
      "Jing Liu",
      "Haiyan Wu",
      "Yuan Xie",
      "Yanyun Qu",
      "Lizhuang Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liu_Trident_Dehazing_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liu_Trident_Dehazing_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Most existing dehazing methods are not robust to nonhomogeneous haze. Meanwhile, the information of dense haze region is usually unknown and hard to estimate, leading to blurry in dehaze result for those regions. Focusing on these two issues, we propose a novel coarse-to-fine model, namely Trident Dehazing Network (TDN), to learn the hazy to hazy-free image mapping with automatic haze density recognition. In detail, TDN is composed of three sub-nets: the Encoder-Decoder Net (EDN) is the main net of TDN to reconstruct the coarse hazy-free feature; the Detail Refinement sub-Net (DRN) helps to refine the high frequency details that was easily lost in the pooling layers in the encoder; and the Haze Density Map Generation sub-Net (HDMGN) can automatically distinguish the thick haze region with thin one, preventing over-dehazing or under-dehazing in regions of different haze density. Moreover, we propose a frequency domain loss function to make supervision of different frequency band more uniform. Extensive experimental results on synthetic and real datasets demonstrate that our proposed TDN outperforms the state-of-the-arts with better fidelity and perceptual, generalizing well on both dense haze and nonhomogeneous haze scene. Our method won the first place in NTIRE2020 nonhomogeneous dehazing challenge.\r"
  },
  "cvpr2020_w31_denselyself-guidedwaveletnetworkforimagedenoising": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Densely Self-Guided Wavelet Network for Image Denoising",
    "authors": [
      "Wei Liu",
      "Qiong Yan",
      "Yuzhi Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liu_Densely_Self-Guided_Wavelet_Network_for_Image_Denoising_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liu_Densely_Self-Guided_Wavelet_Network_for_Image_Denoising_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " During the past years, deep convolutional neural networks have achieved impressive success in image denoising. In this paper, we propose a densely self-guided wavelet network (DSWN) for real world image denoising. The basic structure of DSWN is a top-down self-guidance architecture which is able to efficiently incorporate multi-scale information and extract good local features to recover clean images. Moreover, such a structure requires a smaller number of parameters and enables us to achieve better effectiveness than Unet structure. To avoid information loss and achieve a better receptive field size, we embed wavelet transform into DSWN. In addition, we apply densely residual learning to convolution blocks to enhance the feature extraction capability of the proposed network. At the full resolution level of DSWN, we adopt a double branch structure to generate the final output. One branch of them tends to pay attention to dark areas and the other performs better on bright areas. Such a double branch strategy is able to handle the noise at different exposures. The proposed network is validated by BSD68, Kodak24 and SIDD+ benchmark. Additional experimental results show that the proposed network outperforms most state-of-the-art image denoising solutions.\r"
  },
  "cvpr2020_w31_mmdmmulti-frameandmulti-scaleforimagedemoireing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "MMDM: Multi-Frame and Multi-Scale for Image Demoireing",
    "authors": [
      "Shuai Liu",
      "Chenghua Li",
      "Nan Nan",
      "Ziyao Zong",
      "Ruixia Song"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liu_MMDM_Multi-Frame_and_Multi-Scale_for_Image_Demoireing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liu_MMDM_Multi-Frame_and_Multi-Scale_for_Image_Demoireing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The imaging characteristics of digital sensors often lead to the moire patterns, which are widely distributed over the frequency domain and have irregular colors and shapes. The images with moire patterns could lead to a serious decline in the visual quality. The difficulty of demoireing lies in that the moire patterns mix both low and high frequency information to be processed. In this paper, we propose MMDM, an effective image demoireing network, which uses multiple images as inputs and multi-scale feature encoding module as low-frequency information enhancement. Our MMDM has three key modules: the newly designed multi-frame spatial transformer networks (M-STN), the multi-scale feature encoding module (MSFE), and the enhanced asymmetric convolution block (EACB). Especially, the M-STN aims to align the multiple input images simultaneously. The MSFE is for multiple frequency information encoding, which is built on the efficient EACB module. Experiments prove the effectiveness of MMDM. Also, our model achieves the 2nd place on both demoiring track and denoising track in the NTIRE2020 Challenge. Code is avaliable at: https://github.com/q935970314/MMDM\r"
  },
  "cvpr2020_w31_real-worldsuper-resolutionusinggenerativeadversarialnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Real-World Super-Resolution Using Generative Adversarial Networks",
    "authors": [
      "Haoyu Ren",
      "Amin Kheradmand",
      "Mostafa El-Khamy",
      "Shuangquan Wang",
      "Dongwoon Bai",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ren_Real-World_Super-Resolution_Using_Generative_Adversarial_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ren_Real-World_Super-Resolution_Using_Generative_Adversarial_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Robust real-world super-resolution (SR) aims to generate perception-oriented high-resolution (HR) images from the corresponding low-resolution (LR) ones, without access to the paired LR-HR ground-truth. In this paper, we investigate how to advance the state of the art in real-world SR. Our method involves deploying an ensemble of generative adversarial networks (GANs) for robust real-world SR. The ensemble deploys different GANs trained with different adversarial objectives. Due to the lack of knowledge about the ground-truth blur and noise models, we design a generic training set with the LR images generated by various degradation models from a set of HR images. We achieve good perceptual quality by super resolving the LR images whose degradation was caused by unknown image processing artifacts. For real-world SR on images captured by mobile devices, the GANs are trained by weak supervision of a mobile SR training set having LR-HR image pairs, which we construct from the DPED dataset which provides registered mobile-DSLR images at the same scale. Our ensemble of GANs uses cues from the image luminance and adjusts to generate better HR images at low-illumination. Experiments on the NTIRE 2020 real-world super-resolution dataset show that our proposed SR approach achieves good perceptual quality.\r"
  },
  "cvpr2020_w31_deepgenerativeadversarialresidualconvolutionalnetworksforreal-worldsuper-resolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deep Generative Adversarial Residual Convolutional Networks for Real-World Super-Resolution",
    "authors": [
      "Rao Muhammad Umer",
      "Gian Luca Foresti",
      "Christian Micheloni"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Umer_Deep_Generative_Adversarial_Residual_Convolutional_Networks_for_Real-World_Super-Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Umer_Deep_Generative_Adversarial_Residual_Convolutional_Networks_for_Real-World_Super-Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Most current deep learning based single image super-resolution (SISR) methods focus on designing deeper / wider models to learn the non-linear mapping between low-resolution (LR) inputs and the high-resolution (HR) outputs from a large number of paired (LR/HR) training data. They usually take as assumption that the LR image is a bicubic down-sampled version of the HR image. However, such degradation process is not available in real-world settings i.e. inherent sensor noise, stochastic noise, compression artifacts, possible mismatch between image degradation process and camera device. It reduces significantly the performance of current SISR methods due to real-world image corruptions. To address these problems, we propose a deep Super-Resolution Residual Convolutional Generative Adversarial Network (SRResCGAN) to follow the real-world degradation settings by adversarial training the model with pixel-wise supervision in the HR domain from its generated LR counterpart. The proposed network exploits the residual learning by minimizing the energy-based objective function with powerful image regularization and convex optimization techniques. We demonstrate our proposed approach in quantitative and qualitative experiments that generalize robustly to real input and it is easy to deploy for other down-scaling operators and mobile/embedded devices.\r"
  },
  "cvpr2020_w31_perceptualextremesuper-resolutionnetworkwithreceptivefieldblock": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Perceptual Extreme Super-Resolution Network With Receptive Field Block",
    "authors": [
      "Taizhang Shang",
      "Qiuju Dai",
      "Shengchen Zhu",
      "Tong Yang",
      "Yandong Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Shang_Perceptual_Extreme_Super-Resolution_Network_With_Receptive_Field_Block_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Shang_Perceptual_Extreme_Super-Resolution_Network_With_Receptive_Field_Block_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Perceptual Extreme Super-Resolution for single image is extremely difficult, because the texture details of different images vary greatly. To tackle this difficulty, we develop a super resolution network with receptive field block based on Enhanced SRGAN. We call our network RFB-ESRGAN. The key contributions are listed as follows. First, for the purpose of extracting multi-scale information and enhance the feature discriminability, we applied receptive field block (RFB) to super resolution. RFB has achieved competitive results in object detection and classification. Second, instead of using large convolution kernels in multi-scale receptive field block, several small kernels are used in RFB, which makes us be able to extract detailed features and reduce the computation complexity. Third, we alternately use different upsampling methods in the upsampling stage to reduce the high computation complexity and still remain satisfactory performance. Fourth, we use the ensemble of 10 models of different iteration to improve the robustness of model and reduce the noise introduced by each individual model. Our experimental results show the superior performance of RFB-ESRGAN. According to the preliminary results of NTIRE 2020 Perceptual Extreme Super-Resolution Challenge, our solution ranks first among all the participants.\r"
  },
  "cvpr2020_w31_unsupervisedrealimagesuper-resolutionviagenerativevariationalautoencoder": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unsupervised Real Image Super-Resolution via Generative Variational AutoEncoder",
    "authors": [
      "Zhi-Song Liu",
      "Zhi-Song Liu",
      "Wan-Chi Siu",
      "Li-Wen Wang",
      "Chu-Tak Li",
      "Marie-Paule Cani"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liu_Unsupervised_Real_Image_Super-Resolution_via_Generative_Variational_AutoEncoder_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liu_Unsupervised_Real_Image_Super-Resolution_via_Generative_Variational_AutoEncoder_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Benefited from the deep learning, image Super-Resolution has been one of the most developing research fields in computer vision. Depending upon whether using a discriminator or not, a deep convolutional neural network can provide an image with high fidelity or better perceptual quality. Due to the lack of ground truth images in real life, people prefer a photo-realistic image with low fidelity to a blurry image with high fidelity. In this paper, we revisit the classic example based image super-resolution approaches and come up with a novel generative model for perceptual image super-resolution. Given that real images contain various noise and artifacts, we propose a joint image denoising and super-resolution model via Variational AutoEncoder. We come up with a conditional variational autoencoder to encode the reference for dense feature vector which can then be transferred to the decoder for target image denoising. With the aid of the discriminator, an additional overhead of super-resolution subnetwork is attached to super-resolve the denoised image with photo-realistic visual quality. We participated the NTIRE2020 Real Image Super-Resolution Challenge. Experimental results show that by using the proposed approach, we can obtain enlarged images with clean and pleasant features compared to other supervised methods. We also compared our approach with state-of-the-art methods on various datasets to demonstrate the efficiency of our proposed unsupervised super-resolution model.\r"
  },
  "cvpr2020_w31_nh-hazeanimagedehazingbenchmarkwithnon-homogeneoushazyandhaze-freeimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NH-HAZE: An Image Dehazing Benchmark With Non-Homogeneous Hazy and Haze-Free Images",
    "authors": [
      "Codruta O. Ancuti",
      "Cosmin Ancuti",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ancuti_NH-HAZE_An_Image_Dehazing_Benchmark_With_Non-Homogeneous_Hazy_and_Haze-Free_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ancuti_NH-HAZE_An_Image_Dehazing_Benchmark_With_Non-Homogeneous_Hazy_and_Haze-Free_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Image dehazing is an ill-posed problem that has been extensively studied in the recent years. The objective performance evaluation of the dehazing methods is one of the major obstacles due to the lacking of a reference dataset. While the synthetic datasets have shown important limitations, the few realistic datasets introduced recently assume homogeneous haze over the entire scene. Since in many real cases haze is not uniformly distributed we introduce NH-HAZE, a non-homogeneous realistic dataset with pairs of real hazy and corresponding haze-free images. This is the first non-homogeneous image dehazing dataset and contains 55 outdoor scenes. The non-homogeneous haze has been introduced in the scene using a professional haze generator that imitates the real conditions of hazy scenes. Additionally, this work presents an objective assessment of several state-of-the-art single image dehazing methods that were evaluated using NH-HAZE dataset.\r"
  },
  "cvpr2020_w31_ntire2020challengeonspectralreconstructionfromanrgbimage": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Spectral Reconstruction From an RGB Image",
    "authors": [
      "Boaz Arad",
      "Radu Timofte",
      "Ohad Ben-Shahar",
      "Yi-Tun Lin",
      "Graham D. Finlayson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Arad_NTIRE_2020_Challenge_on_Spectral_Reconstruction_From_an_RGB_Image_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Arad_NTIRE_2020_Challenge_on_Spectral_Reconstruction_From_an_RGB_Image_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the second challenge on spectral reconstruction from RGB images, i.e., the recovery of whole-scene hyperspectral (HS) information from a 3-channel RGB image. As in the previous challenge, two tracks were provided: (i) a \"\"Clean\"\" track where HS images are estimated from noise-free RGBs, the RGB images are themselves calculated numerically using the ground-truth HS images and supplied spectral sensitivity functions (ii) a \"\"Real World\"\" track, simulating capture by an uncalibrated and unknown camera, where the HS images are recovered from noisy JPEG-compressed RGB images. A new, larger-than-ever, natural hyperspectral image data set is presented, containing a total of 510 HS images. The Clean and Real World tracks had 103 and 78 registered participants respectively, with 14 teams competing in the final testing phase. A description of the proposed methods, alongside their challenge scores and an extensive evaluation of top performing methods is also provided. They gauge the state-of-the-art in spectral reconstruction from an RGB image.\r"
  },
  "cvpr2020_w31_realimagedenoisingbasedonmulti-scaleresidualdenseblockandcascadedu-netwithblock-connection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Real Image Denoising Based on Multi-Scale Residual Dense Block and Cascaded U-Net With Block-Connection",
    "authors": [
      "Long Bao",
      "Zengli Yang",
      "Shuangquan Wang",
      "Dongwoon Bai",
      "Jungwon Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Bao_Real_Image_Denoising_Based_on_Multi-Scale_Residual_Dense_Block_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Bao_Real_Image_Denoising_Based_on_Multi-Scale_Residual_Dense_Block_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Benefiting from the recent real image dataset, learning-based approaches have achieved good performance for real-image denoising. To further improve the performance for Bayer raw data denoising, this paper introduces two new networks, which are multi-scale residual dense network (MRDN) and multi-scale residual dense cascaded U-Net with block-connection (MCU-Net). Both networks are built upon a newly designed multi-scale residual dense block (MRDB), and MCU-Net uses MRDB to connect the encoder and decoder of the U-Net. To better exploit the multi-scale feature of the images, the MRDB adds another branch of atrous spatial pyramid pooling (ASPP) based on residual dense block (RDB). Compared to the skip connection, the block-connection using MRDB can adaptively transform the features of the encoder and transfer them to the decoder of the U-Net. In addition, a novel noise permutation algorithm is introduced to avoid model overfitting. The superior performance of these new networks in removing noise within Bayer images has been demonstrated by comparison results on the SIDD benchmark, and the top ranking of SSIM in the NTIRE 2020 Challenge on Real Image Denoising - Track1: rawRGB.\r"
  },
  "cvpr2020_w31_ensembledehazingnetworksfornon-homogeneoushaze": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Ensemble Dehazing Networks for Non-Homogeneous Haze",
    "authors": [
      "Mingzhao Yu",
      "Venkateswararao Cherukuri",
      "Tiantong Guo",
      "Vishal Monga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Yu_Ensemble_Dehazing_Networks_for_Non-Homogeneous_Haze_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Yu_Ensemble_Dehazing_Networks_for_Non-Homogeneous_Haze_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Image dehazing is one of the most challenging imaging inverse problems. Although deep learning methods produce compelling results, one of the most crucial practical challenge is that of non-homogeneous haze, which remains an open problem. To address this challenge, we propose 3 models that are inspired by ensemble techniques. First, we propose a DenseNet based single-encoder four-decoders structure denoted as EDN-3J, wherein among the four decoders, three of them output estimates of dehazed images J1, J2, J3 that are then weighted and combined via weight maps learned by the fourth decoder. In our second model called EDN-AT, the single-encoder four-decoders structure is maintained while three decoders are transformed to jointly estimate two physical inverse haze models that share a common transmission map t with two distinct ambient light maps A1, A2. The two inverse haze models are then weighed and combined for the final dehazed image. To endow two sub-models flexibility and to induce the capability of modeling non-homogeneous haze, we apply attention masks to ambient lights. Both the weight maps and attention maps are generated from the fourth decoder. Finally, in contrast to the above two ensemble models, we propose an encoder-decoder-U-net structure called EDN-EDU, which is a sequential hierarchical ensemble of two different dehazing networks with different modeling capacities. Experiments performed on challenging benchmark image datasets of NTIRE'20 and NTIRE'19 demonstrate that the proposed models outperform many state-of-the-art methods and this fact is particularly demonstrated in the NTIRE-2020 contest where the EDN-AT model achieves the best result in the sense of the perceptual quality metric LPIPS.\r"
  },
  "cvpr2020_w31_nonlocalchannelattentionfornonhomogeneousimagedehazing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NonLocal Channel Attention for NonHomogeneous Image Dehazing",
    "authors": [
      "Kareem Metwaly",
      "Xuelu Li",
      "Tiantong Guo",
      "Vishal Monga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Metwaly_NonLocal_Channel_Attention_for_NonHomogeneous_Image_Dehazing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Metwaly_NonLocal_Channel_Attention_for_NonHomogeneous_Image_Dehazing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The emergence of deep learning methods that complement traditional model-based methods has helped achieve a new state-of-the-art for image dehazing. Many recent methods design deep networks that either estimate the haze-free image (J) directly or estimate physical parameters in the haze model, i.e. ambient light (A) and transmission map (t) followed by using the inverse of the haze model to estimate the dehazed image. However, both kinds of methods fail in dealing with non-homogeneous haze images where some parts of the image are covered with denser haze and the other parts with shallower haze. In this work, we develop a novel neural network architecture that can take benefits of the aforementioned two kinds of dehazed images simultaneously by estimating a new quantity -- a spatially varying weight map (w). w can then be used to combine the directly estimated J and the results obtained by the inverse model. In our work, we utilize a shared DenseNet-based encoder, and four distinct DenseNet-based decoders that estimate J, A, t, and w jointly. A channel attention structure is added to facilitate the generation of distinct feature maps of different decoders. Furthermore, we propose a novel dilation inception module in the architecture to utilize the non-local features to make up the missing information during the learning process. Experiments performed on challenging benchmark datasets of NTIRE'20 and NTIRE'18 demonstrate that the proposed method -namely, AtJwD- can outperform many state-of-the-art alternatives in the sense of quality metrics such as SSIM, especially in recovering images under non-homogeneous haze.\r"
  },
  "cvpr2020_w31_residualchannelattentiongenerativeadversarialnetworkforimagesuper-resolutionandnoisereduction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Residual Channel Attention Generative Adversarial Network for Image Super-Resolution and Noise Reduction",
    "authors": [
      "Jie Cai",
      "Zibo Meng",
      "Chiu Man Ho"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Cai_Residual_Channel_Attention_Generative_Adversarial_Network_for_Image_Super-Resolution_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Cai_Residual_Channel_Attention_Generative_Adversarial_Network_for_Image_Super-Resolution_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Image super-resolution is one of the important computer vision techniques aiming to reconstruct high-resolution images from corresponding low-resolution ones. Most recently, deep learning based approaches have been demonstrated for image super-resolution. However, as the deep networks go deeper, they become more difficult to train and more difficult to restore the finer texture details, especially under real-world settings. In this paper, we propose a Residual Channel Attention-Generative Adversarial Network (RCA-GAN) to solve these problems. Specifically, a novel residual channel attention block is proposed to form RCA-GAN, which consists of a set of residual blocks with shortcut connections, and a channel attention mechanism to model the interdependence and interaction of the feature representations among different channels. Besides, a generative adversarial network (GAN) is employed to further produce realistic and highly detailed results. Benefiting from these improvements, the proposed RCA-GAN yields consistently better visual quality with more detailed and natural textures than baseline models; and achieves comparable or better performance compared with the state-of-the-art methods for real-world image super-resolution.\r"
  },
  "cvpr2020_w31_unsupervisedreal-worldsuperresolutionwithcyclegenerativeadversarialnetworkanddomaindiscriminator": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unsupervised Real-World Super Resolution With Cycle Generative Adversarial Network and Domain Discriminator",
    "authors": [
      "Gwantae Kim",
      "Jaihyun Park",
      "Kanghyu Lee",
      "Junyeop Lee",
      "Jeongki Min",
      "Bokyeung Lee",
      "David K. Han",
      "Hanseok Ko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Kim_Unsupervised_Real-World_Super_Resolution_With_Cycle_Generative_Adversarial_Network_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Kim_Unsupervised_Real-World_Super_Resolution_With_Cycle_Generative_Adversarial_Network_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper proposes an unsupervised single-image Super-Resolution(SR) model using cycleGAN and domain discriminator to solve the problem of SR with unknown degradation using unpaired dataset. In previous approaches, paired dataset is required for training with assumed levels of image degradation. In real world SR applications, however, training sets are typically not of low and high resolution image pairs, but only low resolution images with unknown degradation are provided as inputs. To address the problem, we introduce a cycle-in-cycle GAN based unsupervised learning model using an unpaired dataset. In addition, we combine several losses attributed to image contents, such as pixel-wise loss, VGG feature loss and SSIM loss, for stable learning and performance improvement. We also propose a domain discriminator, which consists of noise discriminator, texture discriminator and color discriminator, to guide generated images to follow target domain distribution rather than source domain. We validate effectiveness of our model in quantitative and qualitative experiments using NTIRE2020 real-world SR challenge dataset.\r"
  },
  "cvpr2020_w31_high-resolutiondual-stagemulti-levelfeatureaggregationforsingleimageandvideodeblurring": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "High-Resolution Dual-Stage Multi-Level Feature Aggregation for Single Image and Video Deblurring",
    "authors": [
      "Stephan Brehm",
      "Sebastian Scherer",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Brehm_High-Resolution_Dual-Stage_Multi-Level_Feature_Aggregation_for_Single_Image_and_Video_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Brehm_High-Resolution_Dual-Stage_Multi-Level_Feature_Aggregation_for_Single_Image_and_Video_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper we address the problem of dynamic scene motion deblurring. We present a model that combines high resolution processing with a multi-resolution feature aggregation method for single frame and video deblurring. Our proposed model consists of 2 stages. In the first stage, single image deblurring is performed at a very high-resolution. For this purpose, we propose a novel network building block that employs multiple atrous convolutions in parallel. We carefully tune the atrous rate of each of these convolutions to achieve complete coverage of a rectangular area of the input. In this way we obtain a large receptive field at a high spatial resolution. The second stage aggregates information across multiple consecutive frames of a video sequence. Here we maintain a high-resolution, but also use multi-resolution features to mitigate the effects of large movements of objects between images. The presented models rank first and fourth in the NTIRE2020 challenges for single image deblurring and video deblurring, respectively. We apply our framework on current benchmarks and challenges and show that our model provides state-of-the art results.\r"
  },
  "cvpr2020_w31_ntire2020challengeonimagedemoireingmethodsandresults": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Image Demoireing: Methods and Results",
    "authors": [
      "Shanxin Yuan",
      "Radu Timofte",
      "Ales Leonardis",
      "Gregory Slabaugh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Yuan_NTIRE_2020_Challenge_on_Image_Demoireing_Methods_and_Results_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Yuan_NTIRE_2020_Challenge_on_Image_Demoireing_Methods_and_Results_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the Challenge on Image Demoireing that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2020. Demoireing is a difficult task of removing moire patterns from an image to reveal an underlying clean image. The challenge was divided into two tracks. Track 1 targeted the single image demoireing problem, which seeks to remove moire patterns from a single image. Track 2 focused on the burst demoireing problem, where a set of degraded moire images of the same scene were provided as input, with the goal of producing a single demoired image as output. The methods were ranked in terms of their fidelity, measured using the peak signal-to-noise ratio (PSNR) between the ground truth clean images and the restored images produced by the participants' methods. The tracks had 142 and 99 registered participants, respectively, with a total of 14 and 6 submissions in the final testing stage. The entries span the current state-of-the-art in image and burst image demoireing problems.\r"
  },
  "cvpr2020_w31_adaptiveweightedattentionnetworkwithcameraspectralsensitivitypriorforspectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Adaptive Weighted Attention Network With Camera Spectral Sensitivity Prior for Spectral Reconstruction From RGB Images",
    "authors": [
      "Jiaojiao Li",
      "Chaoxiong Wu",
      "Rui Song",
      "Yunsong Li",
      "Fei Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Li_Adaptive_Weighted_Attention_Network_With_Camera_Spectral_Sensitivity_Prior_for_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Li_Adaptive_Weighted_Attention_Network_With_Camera_Spectral_Sensitivity_Prior_for_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent promising effort for spectral reconstruction (SR) focuses on learning a complicated mapping through using a deeper and wider convolutional neural networks (CNNs). Nevertheless, most CNN-based SR algorithms neglect to explore the camera spectral sensitivity (CSS) prior and interdependencies among intermediate features, thus limiting the representation ability of the network and performance of SR. To conquer these issues, we propose a novel adaptive weighted attention network (AWAN) for SR, whose backbone is stacked with multiple dual residual attention blocks (DRAB) decorating with long and short skip connections to form the dual residual learning. Concretely, we investigate an adaptive weighted channel attention (AWCA) module to reallocate channel-wise feature responses via integrating correlations between channels. Furthermore, a patch-level second-order non-local (PSNL) module is developed to capture long-range spatial contextual information by second-order non-local operations for more powerful feature representations. Based on the fact that the recovered RGB images can be projected by the reconstructed hyperspectral image (HSI) and the given CSS function, we incorporate the discrepancies of the RGB images and HSIs as a finer constraint for more accurate reconstruction. Experimental results demonstrate the effectiveness of our proposed AWAN network in terms of quantitative comparison and perceptual quality over other state-of-the-art SR methods. In the NTIRE 2020 Spectral Reconstruction Challenge, our entries obtain the 1st ranking on the \"Clean\" track and the 3rd place on the \"Real World\" track. Codes are available at https://github.com/Deep-imagelab/AWAN.\r"
  },
  "cvpr2020_w31_unsupervisedsingleimagesuper-resolutionnetwork(usisresnet)forreal-worlddatausinggenerativeadversarialnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unsupervised Single Image Super-Resolution Network (USISResNet) for Real-World Data Using Generative Adversarial Network",
    "authors": [
      "Kalpesh Prajapati",
      "Vishal Chudasama",
      "Heena Patel",
      "Kishor Upla",
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Prajapati_Unsupervised_Single_Image_Super-Resolution_Network_USISResNet_for_Real-World_Data_Using_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Prajapati_Unsupervised_Single_Image_Super-Resolution_Network_USISResNet_for_Real-World_Data_Using_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Current state-of-the-art Single Image Super-Resolution (SISR) techniques rely largely on supervised learning where Low-Resolution (LR) images are synthetically generated with known degradation (e.g., bicubic downsampling). The deep learning models trained with such synthetic dataset generalize poorly on the real-world or natural data where the degradation characteristics cannot be fully modelled. As an implication, the super-resolved images obtained for real LR images do not produce optimal Super Resolution (SR) images. We propose a new SR approach to mitigate such an issue using unsupervised learning in Generative Adversarial Network (GAN) framework - USISResNet. In an attempt to provide high quality SR image for perceptual inspection, we also introduce a new loss function based on the Mean Opinion Score (MOS). The effectiveness of the proposed architecture is validated with extensive experiments on NTIRE-2020 Real-world SR Challenge validation (Track-1) set along with testing datasets (Track-1 and Track-2). We demonstrate the generalizable nature of proposed network by evaluating real-world images as against other state-of-the-art methods which employ synthetically downsampled LR images. The proposed network has further been evaluated on NTIRE 2020 Real-world SR Challenge dataset where the approach has achieved reliable accuracy.\r"
  },
  "cvpr2020_w31_real-worldsuper-resolutionviakernelestimationandnoiseinjection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Real-World Super-Resolution via Kernel Estimation and Noise Injection",
    "authors": [
      "Xiaozhong Ji",
      "Yun Cao",
      "Ying Tai",
      "Chengjie Wang",
      "Jilin Li",
      "Feiyue Huang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent state-of-the-art super-resolution methods have achieved impressive performance on ideal datasets regardless of blur and noise. However, these methods always fail in real-world image super-resolution, since most of them adopt simple bicubic downsampling from high-quality images to construct Low-Resolution (LR) and High-Resolution (HR) pairs for training which may lose track of frequency-related details. To address this issue, we focus on designing a novel degradation framework for real-world images by estimating various blur kernels as well as real noise distributions. Based on our novel degradation framework, we can acquire LR images sharing a common domain with real-world images. Then, we propose a real-world super-resolution model aiming at better perception. Extensive experiments on synthetic noise data and real-world images demonstrate that our method outperforms the state-of-the-art methods, resulting in lower noise and better visual quality. In addition, our method is the winner of NTIRE 2020 Challenge on both tracks of Real-World Super-Resolution, which significantly outperforms other competitors by large margins.\r"
  },
  "cvpr2020_w31_unsupervisedimagesuper-resolutionwithanindirectsupervisedpath": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Unsupervised Image Super-Resolution With an Indirect Supervised Path",
    "authors": [
      "Shuaijun Chen",
      "Zhen Han",
      "Enyan Dai",
      "Xu Jia",
      "Ziluan Liu",
      "Liu Xing",
      "Xueyi Zou",
      "Chunjing Xu",
      "Jianzhuang Liu",
      "Qi Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Chen_Unsupervised_Image_Super-Resolution_With_an_Indirect_Supervised_Path_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Chen_Unsupervised_Image_Super-Resolution_With_an_Indirect_Supervised_Path_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The task of single image super-resolution (SISR) aims at reconstructing a high-resolution (HR) image from a low-resolution (LR) image. Although significant progress has been made with deep learning models, they are trained on synthetic paired data in a supervised way and do not perform well on real cases. There are several attempts that directly apply unsupervised image translation models to address such a problem. However, unsupervised image translation models need to be modified to adapt to unsupervised low-level vision task which poses higher requirement on the accuracy of translation. In this work, we propose a novel framework which is composed of two stages: 1) unsupervised image translation between real LR and synthetic LR images; 2) supervised super-resolution from approximated real LR images to the paired HR images. It takes the synthetic LR images as a bridge and creates an indirect supervised path. We show that our framework is so flexible that any unsupervised translation model and deep learning based super-resolution model can be integrated into it. Besides, a collaborative training strategy is proposed to encourage the two stages collaborate with each other for better degradation learning and super-resolution performance. The proposed method achieves very good performance on datasets of NTIRE 2017, NTIRE 2018 and NTIRE 2020, even comparable with supervised methods.\r"
  },
  "cvpr2020_w31_dual-domaindeepconvolutionalneuralnetworksforimagedemoireing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Dual-Domain Deep Convolutional Neural Networks for Image Demoireing",
    "authors": [
      "An Gia Vien",
      "Hyunkook Park",
      "Chul Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Vien_Dual-Domain_Deep_Convolutional_Neural_Networks_for_Image_Demoireing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Vien_Dual-Domain_Deep_Convolutional_Neural_Networks_for_Image_Demoireing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We develop deep convolutional neural networks (CNNs) for moire artifacts removal by exploiting the complex properties of moire patterns in multiple complementary domains, i.e., the pixel and frequency domains. In the pixel domain, we employ multi-scale features to remove the moire artifacts associated with specific frequency bands using multi-resolution feature maps. In the frequency domain, we design a network that processes discrete cosine transform (DCT) coefficients to remove moire artifacts. Next, we develop a dynamic filter generation network that learns dynamic blending filters. Finally, the results from the pixel and frequency domains are combined using the blending filters to yield moire-free images. In addition, we extend the proposed approach to arbitrary-length burst image demoireing. Specifically, we develop a new attention network to effectively extract useful information from each image in the burst and to align them with the reference image. We demonstrate the effectiveness of the proposed demoireing algorithm by evaluating on the test set in the NTIRE 2020 Demoireing Challenge: Track 1 (Single image) and Track 2 (Burst).\r"
  },
  "cvpr2020_w31_moirepatternremovalviaattentivefractalnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Moire Pattern Removal via Attentive Fractal Network",
    "authors": [
      "Dejia Xu",
      "Yihao Chu",
      "Qingyan Sun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Xu_Moire_Pattern_Removal_via_Attentive_Fractal_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Xu_Moire_Pattern_Removal_via_Attentive_Fractal_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Moire patterns are commonly seen artifacts when taking photos of screens and other objects with high-frequency textures. It's challenging to remove the moire patterns considering its complex color and shape. In this work, we propose an Attentive Fractal Network to effectively solve this problem. First, we construct each Attentive Fractal Block with progressive feature fusion and channel-wise attention guidance. The network is then fractally stacked with the block on each of its levels. Second, to further boost the performance, we adopt a two-stage augmented refinement strategy. With these designs, our method wins the burst demoireing track and achieves second place in single image demoireing and single image deblurring tracks in NTIRE20 Challenges. Extensive experiments demonstrate the superiority of our method for moire pattern removal compared to existing state-of-the-art methods, and prove the effectiveness of its each component. We will publicly release our code and trained weights on https://github.com/ir1d/AFN.\r"
  },
  "cvpr2020_w31_simusrasimplebutstrongbaselineforunsupervisedimagesuper-resolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "SimUSR: A Simple but Strong Baseline for Unsupervised Image Super-Resolution",
    "authors": [
      "Namhyuk Ahn",
      "Jaejun Yoo",
      "Kyung-Ah Sohn"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ahn_SimUSR_A_Simple_but_Strong_Baseline_for_Unsupervised_Image_Super-Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ahn_SimUSR_A_Simple_but_Strong_Baseline_for_Unsupervised_Image_Super-Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we tackle a fully unsupervised super-resolution problem, i.e., neither paired images nor ground truth HR images. We assume that low resolution (LR) images are relatively easy to collect compared to high resolution (HR) images. By allowing multiple LR images, we build a set of pseudo pairs by denoising and downsampling LR images and cast the original unsupervised problem into a supervised learning problem but in one level lower. Though this line of study is easy to think of and thus should have been investigated prior to any complicated unsupervised methods, surprisingly, there are currently none. Even more, we show that this simple method outperforms the state-of-the-art unsupervised method with a dramatically shorter latency at runtime, and significantly reduces the gap to the HR supervised models. We submitted our method in NTIRE 2020 super-resolution challenge and won 1st in PSNR, 2nd in SSIM, and 13th in LPIPS. This simple method should be used as the baseline to beat in the future, especially when multiple LR images are allowed during the training phase. However, even in the zero-shot condition, we argue that this method can serve as a useful baseline to see the gap between supervised and unsupervised frameworks.\r"
  },
  "cvpr2020_w31_ntire2020challengeonvideoqualitymappingmethodsandresults": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results",
    "authors": [
      "Dario Fuoli",
      "Zhiwu Huang",
      "Martin Danelljan",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Fuoli_NTIRE_2020_Challenge_on_Video_Quality_Mapping_Methods_and_Results_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Fuoli_NTIRE_2020_Challenge_on_Video_Quality_Mapping_Methods_and_Results_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the NTIRE 2020 challenge on video quality mapping (VQM), which addresses the issues of quality mapping from source video domain to target video domain. The challenge includes both a supervised track (track 1) and a weakly-supervised track (track 2) for two benchmark datasets. In particular, track 1 offers a new Internet video benchmark, requiring algorithms to learn the map from more compressed videos to less compressed videos in a supervised training manner. In track 2, algorithms are required to learn the quality mapping from one device to another when their quality varies substantially and weakly-aligned video pairs are available. For track 1, in total 7 teams competed in the final test phase, demonstrating novel and effective solutions to the problem. For track 2, some existing methods are evaluated, showing promising solutions to the weakly-supervised video quality mapping problem.\r"
  },
  "cvpr2020_w31_knowledgetransferdehazingnetworkfornonhomogeneousdehazing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Knowledge Transfer Dehazing Network for NonHomogeneous Dehazing",
    "authors": [
      "Haiyan Wu",
      "Jing Liu",
      "Yuan Xie",
      "Yanyun Qu",
      "Lizhuang Ma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Wu_Knowledge_Transfer_Dehazing_Network_for_NonHomogeneous_Dehazing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Wu_Knowledge_Transfer_Dehazing_Network_for_NonHomogeneous_Dehazing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Single image dehazing is an ill-posed problem that has recently drawn important attention. It is a challenging image process task, especially in nonhomogeneous scene. However, the existing dehazing methods are commonly designed to handle homogeneous haze which is easily violated in practice, due to the unknown haze distribution of real world. In this paper, we propose a knowledge transfer method that utilizes abundant clear images to train a teacher network to provide strong and Single image dehazing is an ill-posed problem that has recently drawn important attention. It is a challenging image process task, especially in nonhomogeneous scene. However, the existing dehazing methods are commonly designed to handle homogeneous haze which is easily violated in practice, due to the unknown haze distribution of real world. In this paper, we propose a knowledge transfer method that utilizes abundant clear images to train a teacher network to provide strong and robust image prior. The derived architecture is referred to as the Knowledge Transform Dehaze Network (KTDN), which consists of the teacher network and the dehazing network with identical architecture. Through the supervision between intermediate features, the dehazing network is encouraged to imitate the teacher network. In addition, we use attention mechanism to combine channel attention with pixel attention to capture effective information, and employ an enhancing module to refine detail textures. Extensive experimental results on synthetic and real scene datasets demonstrates that the proposed method outperforms the state-of-the-arts in both quantitative and qualitative evaluations. The KTDN ranks 2nd in NTIRE-2020 NonHomogeneous Dehazing Challenge.\r"
  },
  "cvpr2020_w31_rgbtospectralreconstructionvialearnedbasisfunctionsandweights": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "RGB to Spectral Reconstruction via Learned Basis Functions and Weights",
    "authors": [
      "Biebele Joslyn Fubara",
      "Mohamed Sedky",
      "David Dyke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Fubara_RGB_to_Spectral_Reconstruction_via_Learned_Basis_Functions_and_Weights_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Fubara_RGB_to_Spectral_Reconstruction_via_Learned_Basis_Functions_and_Weights_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Single RGB image hyperspectral reconstruction has seen a boost in performance and research attention with the emergence of CNNs and more availability of RGB/hyperspectral datasets. This work proposes a CNN-based strategy for learning RGB to hyperspectral cube mapping by learning a set of basis functions and weights in a combined manner and using them both to reconstruct the hyperspectral signatures of RGB data. Further to this, an unsupervised learning strategy is also proposed which extends the supervised model with an unsupervised loss function that enables it to learn in an end-to-end fully self supervised manner. The supervised model outperforms a baseline model of the same CNN model architecture and the unsupervised learning model shows promising results.\r"
  },
  "cvpr2020_w31_fastdeepmulti-patchhierarchicalnetworkfornonhomogeneousimagedehazing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Fast Deep Multi-Patch Hierarchical Network for Nonhomogeneous Image Dehazing",
    "authors": [
      "Sourya Dipta Das",
      "Saikat Dutta"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Das_Fast_Deep_Multi-Patch_Hierarchical_Network_for_Nonhomogeneous_Image_Dehazing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Das_Fast_Deep_Multi-Patch_Hierarchical_Network_for_Nonhomogeneous_Image_Dehazing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recently, CNN based end-to-end deep learning methods achieve superiority in Image Dehazing but they tend to fail drastically in Non-homogeneous dehazing. Apart from that, existing popular Multi-scale approaches are runtime intensive and memory inefficient. In this context, we proposed a fast Deep Multi-patch Hierarchical Network to restore Non-homogeneous hazed images by aggregating features from multiple image patches from different spatial sections of the hazed image with fewer number of network parameters. Our proposed method is quite robust for different environments with various density of the haze or fog in the scene and very lightweight as the total size of the model is around 21.7 MB. It also provides faster runtime compared to current multi-scale methods with an average runtime of 0.0145s to process 1200 x 1600 HD quality image. Finally, we show the superiority of this network on Dense Haze Removal to other state-of-the-art models.\r"
  },
  "cvpr2020_w31_superkernelneuralarchitecturesearchforimagedenoising": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Superkernel Neural Architecture Search for Image Denoising",
    "authors": [
      "Marcin Mozejko",
      "Tomasz Latkowski",
      "Lukasz Treszczotko",
      "Michal Szafraniuk",
      "Krzysztof Trojanowski"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Mozejko_Superkernel_Neural_Architecture_Search_for_Image_Denoising_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Mozejko_Superkernel_Neural_Architecture_Search_for_Image_Denoising_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recent advancements in Neural Architecture Search (NAS) resulted in finding new state-of-the-art Artificial Neural Network (ANN) solutions for tasks like image classification, object detection, or semantic segmentation without substantial human supervision. In this paper, we focus on exploring NAS for a dense prediction task that is image denoising. Due to a costly training procedure, most NAS solutions for image enhancement rely on reinforcement learning or evolutionary algorithm exploration, which usually take weeks (or even months) to train. Therefore, we introduce a new efficient implementation of various superkernel techniques that enable fast (6-8 RTX2080 GPU hours) single-shot training of models for dense predictions. We demonstrate the effectiveness of our method on the SIDD+ benchmark for image denoising.\r"
  },
  "cvpr2020_w31_residualpixelattentionnetworkforspectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Residual Pixel Attention Network for Spectral Reconstruction From RGB Images",
    "authors": [
      "Hao Peng",
      "Xiaomei Chen",
      "Jie Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Peng_Residual_Pixel_Attention_Network_for_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Peng_Residual_Pixel_Attention_Network_for_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In recent years, hyperspectral reconstruction based on RGB imaging has made significant progress of deep learning, which greatly improves the accuracy of the reconstructed hyperspectral images. In this paper, we proposed a convolution neural network of the hyperspectral reconstruction from a single RGB image, called Residual Pixel Attention Network (RPAN). Specifically, we proposed a Pixel Attention (PA) module, which was applied to each pixel of all feature maps, to adaptively rescale pixel-wise features in all feature maps. The RPAN was trained on the hyperspectral dataset provided by NTIRE 2020 Spectral Reconstruction Challenge and compared with previous state-of-the-art method HSCNN+. The results showed our RPAN network had achieved superior performance in terms of MRAE and RMSE.\r"
  },
  "cvpr2020_w31_fbrnnfeedbackrecurrentneuralnetworkforextremeimagesuper-resolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "FBRNN: Feedback Recurrent Neural Network for Extreme Image Super-Resolution",
    "authors": [
      "Junyeop Lee",
      "Jaihyun Park",
      "Kanghyu Lee",
      "Jeongki Min",
      "Gwantae Kim",
      "Bokyeung Lee",
      "Bonhwa Ku",
      "David K. Han",
      "Hanseok Ko"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Lee_FBRNN_Feedback_Recurrent_Neural_Network_for_Extreme_Image_Super-Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Lee_FBRNN_Feedback_Recurrent_Neural_Network_for_Extreme_Image_Super-Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Single image extreme Super Resolution(SR) is a difficult task as scale factors in the order of 10X or greater is typically attempted. For instance, in the case of 16x upscale of an image,a single pixel from a low resolution image gets expanded to a 16x16 image patch. Such attempts often result fuzzy quality and loss in details in reconstructed images. To handle this difficulties, we propose a network architecture composed of a series of connected blocks in recurrent and feedback fashions for enhanced SR reconstruction. By use of a recurrent network, an SR image is refined over a sequence of enhancement stages in a coarse to fine manner. Additionally, each stage involves back projection of SR image to LR images for continuously being refined during the sequence. According to the preliminary results of NTIRE 2020 perceptual extreme challenge, our team (KU ISPLB) secured 6th place by PSNR and 7th place by SSIM among all participants.\r"
  },
  "cvpr2020_w31_ntire2020challengeonnonhomogeneousdehazing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on NonHomogeneous Dehazing",
    "authors": [
      "Codruta O. Ancuti",
      "Cosmin Ancuti",
      "Florin-Alexandru Vasluianu",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ancuti_NTIRE_2020_Challenge_on_NonHomogeneous_Dehazing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ancuti_NTIRE_2020_Challenge_on_NonHomogeneous_Dehazing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the NTIRE 2020 Challenge on NonHomogeneous Dehazing of images (restoration of rich details in hazy image). We focus on the proposed solutions and their results evaluated on NH-Haze, a novel dataset consisting of 55 pairs of real haze free and nonhomogeneous hazy images recorded outdoor. NH-Haze is the first realistic nonhomogeneous haze dataset that provides ground truth images. The nonhomogeneous haze has been produced using a professional haze generator that imitates the real conditions of haze scenes. 168 participants registered in the challenge and 27 teams competed in the final testing phase. The proposed solutions gauge the state-of-the-art in image dehazing.\r"
  },
  "cvpr2020_w31_ntire2020challengeonperceptualextremesuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Perceptual Extreme Super-Resolution: Methods and Results",
    "authors": [
      "Kai Zhang",
      "Shuhang Gu",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Zhang_NTIRE_2020_Challenge_on_Perceptual_Extreme_Super-Resolution_Methods_and_Results_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Zhang_NTIRE_2020_Challenge_on_Perceptual_Extreme_Super-Resolution_Methods_and_Results_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the NTIRE 2020 challenge on perceptual extreme super-resolution with focus on proposed solutions and results. The challenge task was to super-resolve an input image with a magnification factor 16 based on a set of prior examples of low and corresponding high resolution images. The goal is to obtain a network design capable to produce high resolution results with the best perceptual quality and similar to the ground truth. The track had 280 registered participants, and 19 teams submitted the final results. They gauge the state-of-the-art in single image superresolution.\r"
  },
  "cvpr2020_w31_ntire2020challengeonreal-worldimagesuper-resolutionmethodsandresults": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and Results",
    "authors": [
      "Andreas Lugmayr",
      "Martin Danelljan",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Lugmayr_NTIRE_2020_Challenge_on_Real-World_Image_Super-Resolution_Methods_and_Results_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Lugmayr_NTIRE_2020_Challenge_on_Real-World_Image_Super-Resolution_Methods_and_Results_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the NTIRE 2020 challenge on real world super-resolution. It focuses on the participating methods and final results. The challenge addresses the real world setting, where paired true high and low-resolution images are unavailable. For training, only one set of source input images is therefore provided along with a set of unpaired high-quality target images. In Track 1: Image Processing artifacts, the aim is to super-resolve images with synthetically generated image processing artifacts. This allows for quantitative benchmarking of the approaches \\wrt a ground-truth image. In Track 2: Smartphone Images, real low-quality smart phone images have to be super-resolved. In both tracks, the ultimate goal is to achieve the best perceptual quality, evaluated using a human study. This is the second challenge on the subject, following AIM 2019, targeting to advance the state-of-the-art in super-resolution. To measure the performance we use the benchmark protocol from AIM 2019. In total 22 teams competed in the final testing phase, demonstrating new and innovative solutions to the problem.\r"
  },
  "cvpr2020_w31_ntire2020challengeonrealimagedenoisingdataset,methodsandresults": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "NTIRE 2020 Challenge on Real Image Denoising: Dataset, Methods and Results",
    "authors": [
      "Abdelrahman Abdelhamed",
      "Mahmoud Afifi",
      "Radu Timofte",
      "Michael S. Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Abdelhamed_NTIRE_2020_Challenge_on_Real_Image_Denoising_Dataset_Methods_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Abdelhamed_NTIRE_2020_Challenge_on_Real_Image_Denoising_Dataset_Methods_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper reviews the NTIRE 2020 challenge on real image denoising with focus on the newly introduced dataset, the proposed methods and their results. The challenge is a new version of the previous NTIRE 2019 challenge on real image denoising that was based on the SIDD benchmark. This challenge is based on a newly collected validation and testing image datasets, and hence, named SIDD+. This challenge has two tracks for quantitatively evaluating image denoising performance in (1) the Bayer-pattern rawRGB and (2) the standard RGB (sRGB) color spaces. Each track250 registered participants. A total of 22 teams, proposing 24 methods, competed in the final phase of the challenge. The proposed methods by the participating teams represent the current state-of-the-art performance in image denoising targeting real noisy images. The newly collected SIDD+ datasets are publicly available at: https://bit.ly/siddplus_data.\r"
  },
  "cvpr2020_w31_da-cganaframeworkforindoorradiodesignusingadimension-awareconditionalgenerativeadversarialnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "DA-cGAN: A Framework for Indoor Radio Design Using a Dimension-Aware Conditional Generative Adversarial Network",
    "authors": [
      "Chun-Hao Liu",
      "Hun Chang",
      "Taesuh Park"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liu_DA-cGAN_A_Framework_for_Indoor_Radio_Design_Using_a_Dimension-Aware_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liu_DA-cGAN_A_Framework_for_Indoor_Radio_Design_Using_a_Dimension-Aware_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " A novel \"physics-free\" approach of designing indoor radio dot layout for a floor plan is introduced by formulating it as an image-to-image translation problem and solved with customized dimension-aware conditional generative adversarial networks (DA-cGANs). The proposed model generates a desirable radio heatmap and its respective radio dot layout from a given floor plan with wall types, physical dimension, and macro-cell interference, by learning from the accumulated indoor radio designs by human experts. Considering the nature of radio propagation, two new loss functions and a two-stage training strategy are proposed for the generator to learn the right direction of signal propagation and precise dot locations, in addition to a sectional analysis for dealing with large floor plans. Experimental results show that the new model is effectively generating acceptable dot layout designs and that dimension-awareness is a key enabler for this type of prediction.\r"
  },
  "cvpr2020_w31_jointlearningofblindvideodenoisingandopticalflowestimation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Joint Learning of Blind Video Denoising and Optical Flow Estimation",
    "authors": [
      "Songhyun Yu",
      "Bumjun Park",
      "Junwoo Park",
      "Jechang Jeong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Yu_Joint_Learning_of_Blind_Video_Denoising_and_Optical_Flow_Estimation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Yu_Joint_Learning_of_Blind_Video_Denoising_and_Optical_Flow_Estimation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Many deep-learning-based image/video denoising models have been developed, and recently, several approaches for training a denoising neural network without using clean images have been proposed. However, Noise2Noise method requires paired noisy data, and obtaining them is occasionally difficult, whereas other existing models trained using unpaired noisy data deliver limited performance. Obtaining an accurate optical flow from noisy videos is also a difficult task because conven-tional optical flow estimation methods are primarily focused on estimating the optical flow using clean videos. This study proposes a new framework to fine-tune video denoising and optical flow estimation networks using unpaired noisy videos. These two networks are jointly tra-ined to realize synergy; an improvement in the denoising performance increases the accuracy of the flow estimation, and an improvement in the flow-estimation performance enhances the quality of the training data for the denoiser. Our experimental results reveal that proposed approach outperforms the existing training schemes in video denoising and also provides accurate optical flows even when the videos contain a considerable amount of noise.\r"
  },
  "cvpr2020_w31_deployingimagedeblurringacrossmobiledevicesaperspectiveofqualityandlatency": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Deploying Image Deblurring Across Mobile Devices: A Perspective of Quality and Latency",
    "authors": [
      "Cheng-Ming Chiang",
      "Yu Tseng",
      "Yu-Syuan Xu",
      "Hsien-Kai Kuo",
      "Yi-Min Tsai",
      "Guan-Yu Chen",
      "Koan-Sin Tan",
      "Wei-Ting Wang",
      "Yu-Chieh Lin",
      "Shou-Yao Roy Tseng",
      "Wei-Shiang Lin",
      "Chia-Lin Yu",
      "BY Shen",
      "Kloze Kao",
      "Chia-Ming Cheng",
      "Hung-Jen Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Chiang_Deploying_Image_Deblurring_Across_Mobile_Devices_A_Perspective_of_Quality_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Chiang_Deploying_Image_Deblurring_Across_Mobile_Devices_A_Perspective_of_Quality_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recently, image enhancement and restoration have become important applications on mobile devices, such as super-resolution and image deblurring. However, most state-of-the-art networks present extremely high computational complexity. This makes them difficult to be deployed on mobile devices with acceptable latency. Moreover, when deploying to different mobile devices, there is a large latency variation due to the difference and limitation of deep learning accelerators on mobile devices. In this paper, we conduct a search of portable network architectures for better quality-latency trade-off across mobile devices. We further present the effectiveness of widely used network optimizations for image deblurring task. This paper provides comprehensive experiments and comparisons to uncover the in-depth analysis for both latency and image quality. Through all the above works, we demonstrate the successful deployment of image deblurring application on mobile devices with the acceleration of deep learning accelerators. To the best of our knowledge, this is the first paper that addresses all the deployment issues of image deblurring task across mobile devices. This paper provides practical deployment-guidelines, and is adopted by the championship-winning team in NTIRE 2020 Image Deblurring Challenge on Smartphone Track.\r"
  },
  "cvpr2020_w31_msfsramulti-stagefacesuper-resolutionwithaccuratefacialrepresentationviaenhancedfacialboundaries": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "MSFSR: A Multi-Stage Face Super-Resolution With Accurate Facial Representation via Enhanced Facial Boundaries",
    "authors": [
      "Yunchen Zhang",
      "Yi Wu",
      "Liang Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Zhang_MSFSR_A_Multi-Stage_Face_Super-Resolution_With_Accurate_Facial_Representation_via_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Zhang_MSFSR_A_Multi-Stage_Face_Super-Resolution_With_Accurate_Facial_Representation_via_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The majority of Face Super-Resolution (FSR) approaches apply specific facial priors as guidance in super-resolving the given low-resolution (LR) into high-resolution (HR) images. To improve the FSR performance, various kinds of facial representations were explored in the past decades. Nevertheless, there remains a challenge in estimating high-quality facial representations for LR images. To address this problem, we propose novel facial representation - enhanced facial boundaries. By semantically connecting the facial landmark points, enhanced facial boundaries retain rich semantic information and are robust to different spatial resolution scales. Based on the enhanced facial boundaries, we design a novel Multi-Stage FSR (MSFSR) approach, which applies the multi-stage strategy to recover high-quality face images progressively. The enhanced facial boundaries and the coarse-to-fine supervision facilitate the facial boundaries estimation process in producing high quality facial representation. The one-time projection of the FSR task is decomposed into multiple simpler sub-processes. In these ways, the MSFSR estimates a more robust facial representation and achieves better performance. Experimental results indicate the superiority of our approach to the state-of-the-art approaches in both qualitative and quantitative measurements.\r"
  },
  "cvpr2020_w31_color-wiseattentionnetworkforlow-lightimageenhancement": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Color-Wise Attention Network for Low-Light Image Enhancement",
    "authors": [
      "Yousef Atoum",
      "Mao Ye",
      "Liu Ren",
      "Ying Tai",
      "Xiaoming Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Atoum_Color-Wise_Attention_Network_for_Low-Light_Image_Enhancement_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Atoum_Color-Wise_Attention_Network_for_Low-Light_Image_Enhancement_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Absence of nearby light sources while capturing an image will degrade the visibility and quality of the captured image, making computer vision tasks difficult. In this paper, a color-wise attention network (CWAN) is proposed for low-light image enhancement based on convolutional neural networks. Motivated by the human visual system when looking at dark images, CWAN learns an end-to-end mapping between low-light and enhanced images while searching for any useful color cues in the low-light image to aid in the color enhancement process. Once these regions are identified, CWAN attention will be mainly focused to synthesize these local regions, as well as the global image. Both quantitative and qualitative experiments on challenging datasets demonstrate the advantages of our method in comparison with state-of-the-art methods.\r"
  },
  "cvpr2020_w31_gradnetimagedenoising": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "GradNet Image Denoising",
    "authors": [
      "Yang Liu",
      "Saeed Anwar",
      "Liang Zheng",
      "Qi Tian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liu_GradNet_Image_Denoising_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liu_GradNet_Image_Denoising_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " High-frequency regions like edges compromise the image denoising performance. In traditional hand-crafted systems, image edges/textures were regularly used to restore the frequencies in these regions. However, this practice seems to be left forgotten in the deep learning era. In this paper, we revisit this idea of using the image gradient and introduce the GradNet. Our major contribution is fusing the image gradient in the network. Specifically, the image gradient is computed from the denoised network input and is subsequently concatenated with the feature maps extracted from the shallow layers. In this step, we argue that image gradient shares intrinsically similar nature with features from the shallow layers, and thus that our fusion strategy is superior. One minor contribution in this work is proposing a gradient consistency regularization, which enforces the gradient difference of the denoised image and the clean ground-truth to be minimized. Putting the two techniques together, the proposed GradNet allows us to achieve competitive denoising accuracy on three synthetic datasets and three real-world datasets. We show through ablation studies that the two techniques are indispensable. Moreover, we verify that our system is particularly capable of removing noise from textured regions.\r"
  },
  "cvpr2020_w31_photosequencingofmotionblurusingshortandlongexposures": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Photosequencing of Motion Blur Using Short and Long Exposures",
    "authors": [
      "Vijay Rengarajan",
      "Shuo Zhao",
      "Ruiwen Zhen",
      "John Glotzbach",
      "Hamid Sheikh",
      "Aswin C. Sankaranarayanan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Rengarajan_Photosequencing_of_Motion_Blur_Using_Short_and_Long_Exposures_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Rengarajan_Photosequencing_of_Motion_Blur_Using_Short_and_Long_Exposures_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Photosequencing aims to transform a motion blurred image to a sequence of sharp images. This problem is challenging due to the inherent ambiguities in temporal ordering as well as the recovery of lost spatial textures due to blur. Adopting a computational photography approach, we propose to capture two short exposure images, along with the original blurred long exposure image to aid in the aforementioned challenges. Post-capture, we recover the sharp photosequence using a novel blur decomposition strategy that recursively splits the long exposure image into smaller exposure intervals. We validate the approach by capturing a variety of scenes with interesting motions using machine vision cameras programmed to capture short and long exposure sequences. Our experimental results show that the proposed method resolves both fast and fine motions better than prior works.\r"
  },
  "cvpr2020_w31_multi-stepreinforcementlearningforsingleimagesuper-resolution": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Multi-Step Reinforcement Learning for Single Image Super-Resolution",
    "authors": [
      "Kyle Vassilo",
      "Cory Heatwole",
      "Tarek Taha",
      "Asif Mehmood"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Vassilo_Multi-Step_Reinforcement_Learning_for_Single_Image_Super-Resolution_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Vassilo_Multi-Step_Reinforcement_Learning_for_Single_Image_Super-Resolution_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep Learning (DL) has become prevalent in today's image processing research due to its power and versatility. It has dominated the Single Image Super-Resolution (SISR) field with its ability to obtain High-Resolution (HR) images from their Low-Resolution (LR) counterparts, particularly using Generative Adversarial Networks (GANs). Interest in SISR comes from its potential to increase the performance of supplementary image processing tasks such as object detection, localization, and classification. This research applies a multi-agent Reinforcement Learning (RL) algorithm to SISR, creating an advanced ensemble approach for combining powerful GANs. In our implementation each agent chooses a particular action from a fixed action set comprised of results from existing GAN SISR algorithms to update its pixel values. The pixel-wise or patch-wise arrangement of agents and rewards encourages the algorithm to learn a strategy to increase the resolution of an image by choosing the best pixel values from each option.\r"
  },
  "cvpr2020_w31_areviewofanolddilemmademosaickingfirst,ordenoisingfirst?": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "A Review of an Old Dilemma: Demosaicking First, or Denoising First?",
    "authors": [
      "Qiyu Jin",
      "Gabriele Facciolo",
      "Jean-Michel Morel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Jin_A_Review_of_an_Old_Dilemma_Demosaicking_First_or_Denoising_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Jin_A_Review_of_an_Old_Dilemma_Demosaicking_First_or_Denoising_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Image denoising and demosaicking are the first two crucial steps in digital camera pipelines. In most of the literature, denoising and demosaicking are treated as two independent problems, without considering their interaction, or asking which should be applied first. Several recent works have started addressing them jointly in works that involve heavy weight neural networks, thus incompatible with low power portable imaging devices. Hence, the question of how to combine denoising and demosaicking to reconstruct full color images remains very relevant: Is denoising to be applied first, or should that be demosaicking first? In this paper, we review the main variants of these strategies and carry-out an extensive evaluation to find the best way to reconstruct full color images from a noisy mosaic. We conclude that demosaicking should applied first, followed by denoising. Yet we prove that this requires an adaptation of classic denoising algorithms to demosaicked noise, which we justify and specify.In this paper, we review the main variants of these strategies and carry-out an extensive evaluation to find the best way to reconstruct full color images from a noisy mosaic. We conclude that demosaicking should applied first, followed by denoising. Yet we prove that this requires an adaptation of classic denoising algorithms to demosaicked noise, which we justify and specify.\r"
  },
  "cvpr2020_w31_sensor-realisticsyntheticdataengineformulti-framehighdynamicrangephotography": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Sensor-Realistic Synthetic Data Engine for Multi-Frame High Dynamic Range Photography",
    "authors": [
      "Jinhan Hu",
      "Gyeongmin Choe",
      "Zeeshan Nadir",
      "Osama Nabil",
      "Seok-Jun Lee",
      "Hamid Sheikh",
      "Youngjun Yoo",
      "Michael Polley"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Hu_Sensor-Realistic_Synthetic_Data_Engine_for_Multi-Frame_High_Dynamic_Range_Photography_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Hu_Sensor-Realistic_Synthetic_Data_Engine_for_Multi-Frame_High_Dynamic_Range_Photography_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning-based mobile imaging applications are often limited by the lack of training data. To this end, researchers have resorted to using synthetic training data. However, pure synthetic data does not accurately mimic the distribution of the real data. To improve the utility of synthetic data, we present a systematic pipeline that takes synthetic data coming purely from a game engine and then produces synthetic data with real sensor characteristics such as noise and color gamut. We validate the utility of our sensor-realistic synthetic data for multi-frame high dynamic range (HDR) photography using a Samsung Galaxy S10 Plus smartphone. The result of training two baseline neural networks using our sensor realistic synthetic data modeled for the S10 Plus show that our sensor realistic synthetic data improves the quality of HDR photography on the modeled device. The synthetic dataset is publicly available at https://github.com/nadir-zeeshan/sensor-realistic-synthetic-data.\r"
  },
  "cvpr2020_w31_imagepairsrealisticsuperresolutiondatasetviabeamsplittercamerarig": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "ImagePairs: Realistic Super Resolution Dataset via Beam Splitter Camera Rig",
    "authors": [
      "Hamid Reza Vaezi Joze",
      "Ilya Zharkov",
      "Karlton Powell",
      "Carl Ringler",
      "Luming Liang",
      "Andy Roulston",
      "Moshe Lutz",
      "Vivek Pradeep"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Joze_ImagePairs_Realistic_Super_Resolution_Dataset_via_Beam_Splitter_Camera_Rig_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Joze_ImagePairs_Realistic_Super_Resolution_Dataset_via_Beam_Splitter_Camera_Rig_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Super Resolution is the problem of recovering a high-resolution image from a single or multiple low-resolution images of the same scene. It is an ill-posed problem since high frequency visual details of the scene are completely lost in low-resolution images. To overcome this, many machine learning approaches have been proposed aiming at training a model to recover the lost details in the new scenes. Such approaches include the recent successful effort in utilizing deep learning techniques to solve super resolution problem. As proven, data itself plays a significant role in the machine learning process especially deep learning approaches which are data hungry. Therefore, to solve the problem, the process of gathering data and its formation could be equally as vital as the machine learning technique used. Herein, we are proposing a new data acquisition technique for gathering real image data set which could be used as an input for super resolution, noise cancellation and quality enhancement techniques. We use a beam-splitter to capture the same scene by a low resolution camera and a high resolution camera. Since we also release the raw images, this large-scale dataset could be used for other tasks such as ISP generation. Unlike current small-scale dataset used for these tasks, our proposed dataset includes 11,421 pairs of low-resolution high-resolution images of diverse scenes. To our knowledge this is the most complete dataset for super resolution, ISP and image quality enhancement. The benchmarking result shows how the new dataset can be successfully used to significantly improve the quality of real-world image super resolution.\r"
  },
  "cvpr2020_w31_identityenhancedresidualimagedenoising": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Identity Enhanced Residual Image Denoising",
    "authors": [
      "Saeed Anwar",
      "Cong Phuoc Huynh",
      "Fatih Porikli"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Anwar_Identity_Enhanced_Residual_Image_Denoising_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Anwar_Identity_Enhanced_Residual_Image_Denoising_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We propose to learn a fully-convolutional network model that consists of a Chain of Identity Mapping Modules and residual on the residual architecture for image denoising. Our network structure possesses three distinctive features that are important for the noise removal task. Firstly, each unit employs identity mappings as the skip connections and receives pre-activated input to preserve the gradient magnitude propagated in both the forward and backward directions. Secondly, by utilizing dilated kernels for the convolution layers in the residual branch, each neuron in the last convolution layer of each module can observe the full receptive field of the first layer. Lastly, we employ the residual on the residual architecture to ease the propagation of the high-level information. Contrary to current state-of-the-art real denoising networks, we also present a straightforward and single-stage network for real image denoising. The proposed network produces remarkably higher numerical accuracy and better visual image quality than the classical state-of-the-art and CNN algorithms when being evaluated on the three conventional benchmark and three real-world datasets.\r"
  },
  "cvpr2020_w31_structurepreservingcompressivesensingmrireconstructionusinggenerativeadversarialnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Structure Preserving Compressive Sensing MRI Reconstruction Using Generative Adversarial Networks",
    "authors": [
      "Puneesh Deora",
      "Bhavya Vasudeva",
      "Saumik Bhattacharya",
      "Pyari Mohan Pradhan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Deora_Structure_Preserving_Compressive_Sensing_MRI_Reconstruction_Using_Generative_Adversarial_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Deora_Structure_Preserving_Compressive_Sensing_MRI_Reconstruction_Using_Generative_Adversarial_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Compressive sensing magnetic resonance imaging (CS-MRI) accelerates the acquisition of MR images by breaking the Nyquist sampling limit. In this work, a novel generative adversarial network (GAN) based framework for CS-MRI reconstruction is proposed. Leveraging a combination of patch-based discriminator and structural similarity index based loss, our model focuses on preserving high frequency content as well as fine textural details in the reconstructed image. Dense and residual connections have been incorporated in a U-net based generator architecture to allow easier transfer of information as well as variable network length. We show that our algorithm outperforms state-of-the-art methods in terms of quality of reconstruction and robustness to noise. Also, the reconstruction time, which is of the order of milliseconds, makes it highly suitable for real-time clinical use.\r"
  },
  "cvpr2020_w31_lidialightweightlearnedimagedenoisingwithinstanceadaptation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "LIDIA: Lightweight Learned Image Denoising With Instance Adaptation",
    "authors": [
      "Gregory Vaksman",
      "Michael Elad",
      "Peyman Milanfar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Vaksman_LIDIA_Lightweight_Learned_Image_Denoising_With_Instance_Adaptation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Vaksman_LIDIA_Lightweight_Learned_Image_Denoising_With_Instance_Adaptation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Image denoising is a well studied problem with an extensive activity that has spread over several decades. Leading classical denoising methods are typically designed to exploit the inner structure in images by modeling local overlapping patches, and operating in an unsupervised fashion. In contrast, newcomers to this arena are supervised and universal neural-network-based methods that bypass this modeling altogether, targeting the inference goal directly and globally, tending to be deep and parameter heavy. This work proposes a novel lightweight learnable architecture for image denoising, using a combination of supervised and unsupervised training of it, the first aiming for a universal denoiser and the second for an instance adaptation. Our architecture embeds in it concepts taken from classical methods, leveraging patch processing, non-local self-similarity, representation sparsity and a multiscale treatment. Our proposed universal denoiser achieves near state-of-the-art results, while using a small fraction of the typical number of parameters. In addition, we introduce and demonstrate two highly effective ways for further boosting the denoising performance, by adapting this universal network to the input image. The code reproducing the results of this paper is available at https://github.com/grishavak/LIDIA-denoiser.\r"
  },
  "cvpr2020_w31_skyoptimizationsemanticallyawareimageprocessingofskiesinlow-lightphotography": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Sky Optimization: Semantically Aware Image Processing of Skies in Low-Light Photography",
    "authors": [
      "Orly Liba",
      "Longqi Cai",
      "Yun-Ta Tsai",
      "Elad Eban",
      "Yair Movshovitz-Attias",
      "Yael Pritch",
      "Huizhong Chen",
      "Jonathan T. Barron"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Liba_Sky_Optimization_Semantically_Aware_Image_Processing_of_Skies_in_Low-Light_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Liba_Sky_Optimization_Semantically_Aware_Image_Processing_of_Skies_in_Low-Light_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The sky is a major component of the appearance of a photograph, and its color and tone can strongly influence the mood of a picture. In nighttime photography, the sky can also suffer from noise and color artifacts. For this reason, there is a strong desire to process the sky in isolation from the rest of the scene to achieve an optimal look. In this work, we propose an automated method, which can run as a part of a camera pipeline, for creating accurate sky alpha-masks and using them to improve the appearance of the sky. Our method performs end-to-end sky optimization in less than half a second per image on a mobile device. We introduce a method for creating an accurate sky-mask dataset that is based on partially annotated images that are inpainted and refined by our modified weighted guided filter. We use this dataset to train a neural network for semantic sky segmentation. Due to the compute and power constraints of mobile devices, sky segmentation is performed at a low image resolution. Our modified weighted guided filter is used for edge-aware upsampling to resize the alpha-mask to a higher resolution. With this detailed mask we automatically apply post-processing steps to the sky in isolation, such as automatic spatially varying white-balance, brightness adjustments, contrast enhancement, and noise reduction.\r"
  },
  "cvpr2020_w31_fastandflexibleimageblinddenoisingviacompetitionofexperts": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Fast and Flexible Image Blind Denoising via Competition of Experts",
    "authors": [
      "Shunta Maeda"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Maeda_Fast_and_Flexible_Image_Blind_Denoising_via_Competition_of_Experts_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Maeda_Fast_and_Flexible_Image_Blind_Denoising_via_Competition_of_Experts_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Fast and flexible processing are two essential requirements for a number of practical applications of image denoising. Current state-of-the-art methods, however, still require either high computational cost or limited scopes of the target. We introduce an efficient ensemble network trained via a competition of expert networks, as an application for image blind denoising. We realize automatic division of unlabeled noisy datasets into clusters respectively optimized to enhance denoising performance. The architecture is scalable, can be extended to deal with diverse noise sources/levels without increasing the computation time. Taking advantage of this method, we save up to approximately 90% of computational cost without sacrifice of the denoising performance compared to single network models with identical architectures. We also compare the proposed method with several existing algorithms and observe significant outperformance over the standard denoising algorithm BM3D in terms of computational efficiency.\r"
  },
  "cvpr2020_w31_fabsoftenfacebeautificationviadynamicskinsmoothing,guidedfeathering,andtexturerestoration": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "FabSoften: Face Beautification via Dynamic Skin Smoothing, Guided Feathering, and Texture Restoration",
    "authors": [
      "Sudha Velusamy",
      "Rishubh Parihar",
      "Raviprasad Kini",
      "Aniket Rege"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Velusamy_FabSoften_Face_Beautification_via_Dynamic_Skin_Smoothing_Guided_Feathering_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Velusamy_FabSoften_Face_Beautification_via_Dynamic_Skin_Smoothing_Guided_Feathering_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Face retouching is a widespread application in modern smartphone cameras with its high business value evidenced by its broad user base. We propose a real-time face softening approach that smooths blemishes in the facial skin region, followed by a wavelet band manipulation to restore the underlying skin texture, which produces a highly appealing `beautified' face that retains its natural appearance. Softening is carried out by an attribute-aware dynamic smoothing filter that is guided by facial attributes, including the number of blemishes and coarseness of facial skin texture. The proposed solution is robust to wide variations in lighting conditions, skin nonuniformities, blemishes, the presence of facial accessories, and delicate hair-like regions. The method includes an explicit facial hair preservation module to preserve their delicate texture while smoothing blemishes. We perform a qualitative comparison of our proposed face softening approach with numerous state-of-the-art techniques and commercial products. We demonstrate the power of our method in producing beautified faces at a minimal performance cost, which enables smooth execution on low-power devices like smartphones.\r"
  },
  "cvpr2020_w31_physicallyplausiblespectralreconstructionfromrgbimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Physically Plausible Spectral Reconstruction From RGB Images",
    "authors": [
      "Yi-Tun Lin",
      "Graham D. Finlayson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Lin_Physically_Plausible_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Lin_Physically_Plausible_Spectral_Reconstruction_From_RGB_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recently Convolutional Neural Networks (CNN) have been used to reconstruct hyperspectral information from RGB images, and this spectral reconstruction problem (SR) can often be solved with good (low) error. However, little attention has been paid on whether these models' behavior can adhere to physics. We show that the leading CNN method introduces unexpected 'colorimetric errors', which means the recovered spectra do not reproduce ground-truth RGBs, and sometimes this discrepancy can be large. The problem is further compounded by exposure change. Indeed, most CNN models over-fit to fixed exposure and we demonstrate that this can result in poor performance when exposure varies. In this paper we show how CNN learning can be extended so that the physical plausibility of SR is enforced. Remarkably, our physically plausible CNN solutions advance both spectral and colorimetric performance of the original network, while the application of data augmentation trades off the network performance for model stability against varying exposure.\r"
  },
  "cvpr2020_w31_semanticpixeldistancesforimageediting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Semantic Pixel Distances for Image Editing",
    "authors": [
      "Josh Myers-Dean",
      "Scott Wehrwein"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Myers-Dean_Semantic_Pixel_Distances_for_Image_Editing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Myers-Dean_Semantic_Pixel_Distances_for_Image_Editing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Many image editing techniques make processing decisions based on measures of similarity between pairs of pixels. Traditionally, pixel similarity is measured using a simple L2 distance on RGB or luminance values. In this work, we explore a richer notion of similarity based on feature embeddings learned by convolutional neural networks. We propose to measure pixel similarity by combining distance in a semantically-meaningful feature embedding with traditional color difference. Using semantic features from the penultimate layer of an off-the-shelf semantic segmentation model, we evaluate our distance measure in two image editing applications. A user study shows that incorporating semantic distances into content-aware resizing via seam carving produces improved results. Off-the-shelf semantic features are found to have mixed effectiveness in content-based range masking, suggesting that training better general-purpose pixel embeddings presents a promising future direction for creating semantically-meaningful feature spaces that can be used in a variety of applications.\r"
  },
  "cvpr2020_w31_replacingmobilecameraispwithasingledeeplearningmodel": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "Replacing Mobile Camera ISP With a Single Deep Learning Model",
    "authors": [
      "Andrey Ignatov",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Ignatov_Replacing_Mobile_Camera_ISP_With_a_Single_Deep_Learning_Model_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Ignatov_Replacing_Mobile_Camera_ISP_With_a_Single_Deep_Learning_Model_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " As the popularity of mobile photography is growing constantly, lots of efforts are being invested now into building complex hand-crafted camera ISP solutions. In this work, we demonstrate that even the most sophisticated ISP pipelines can be replaced with a single end-to-end deep learning model trained without any prior knowledge about the sensor and optics used in a particular device. For this, we present PyNET, a novel pyramidal CNN architecture designed for fine-grained image restoration that implicitly learns to perform all ISP steps such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The model is trained to convert RAW Bayer data obtained directly from mobile camera sensor into photos captured with a professional high-end DSLR camera, making the solution independent of any particular mobile ISP implementation. To validate the proposed approach on the real data, we collected a large-scale dataset consisting of 10 thousand full-resolution RAW-RGB image pairs captured in the wild with the Huawei P20 cameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The experiments demonstrate that the proposed solution can easily get to the level of the embedded P20's ISP pipeline that, unlike our approach, is combining the data from two (RGB + B/W) camera sensors. The dataset, pre-trained models and codes used in this paper are available on the project website.\r"
  },
  "cvpr2020_w31_l2uweaframeworkfortheefficientenhancementoflow-lightunderwaterimagesusinglocalcontrastandmulti-scalefusion": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w31",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - New Trends in Image Restoration and Enhancement",
    "title": "L2UWE: A Framework for the Efficient Enhancement of Low-Light Underwater Images Using Local Contrast and Multi-Scale Fusion",
    "authors": [
      "Tunai Porto Marques",
      "Alexandra Branzan Albu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w31/Marques_L2UWE_A_Framework_for_the_Efficient_Enhancement_of_Low-Light_Underwater_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w31/Marques_L2UWE_A_Framework_for_the_Efficient_Enhancement_of_Low-Light_Underwater_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Images captured underwater often suffer from suboptimal illumination settings that can hide important visual features, reducing their quality. We present a novel single image low-light underwater image enhancer, L^2UWE, that builds on our observation that an efficient model of atmospheric lighting can be derived from local contrast information. We create two distinct models and generate two enhanced images from them: one that highlights finer details, the other focused on darkness removal. A multi-scale fusion process is employed to combine these images while emphasizing regions of higher luminance, saliency and local contrast. We demonstrate the performance of L^2UWE by using seven metrics to test it against seven state-of-the-art enhancement methods specific to underwater and low-light scenes.\r"
  },
  "cvpr2020_w56_cparrcategory-basedproposalanalysisforreferringrelationships": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "CPARR: Category-Based Proposal Analysis for Referring Relationships",
    "authors": [
      "Chuanzi He",
      "Haidong Zhu",
      "Jiyang Gao",
      "Kan Chen",
      "Ram Nevatia"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/He_CPARR_Category-Based_Proposal_Analysis_for_Referring_Relationships_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/He_CPARR_Category-Based_Proposal_Analysis_for_Referring_Relationships_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The task of referring relationships is to localize subject and object entities in an image satisfying a relationship query, which is given in the form of . This requires simultaneous localization of the subject and object entities in a specified relationship. We introduce a simple yet effective proposal-based method for referring relationships. Different from the existing methods such as SSAS, our method can generate a high-resolution result while reducing its complexity and ambiguity. Our method is composed of two modules: a category-based proposal generation module to select the proposals related to the entities and a predicate analysis module to score the compatibility of pairs of selected proposals. We show state-of-the-art performance on the referring relationship task on two public datasets: Visual Relationship Detection and Visual Genome.\r"
  },
  "cvpr2020_w56_improvedactivespeakerdetectionbasedonopticalflow": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Improved Active Speaker Detection Based on Optical Flow",
    "authors": [
      "Chong Huang",
      "Kazuhito Koishida"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Huang_Improved_Active_Speaker_Detection_Based_on_Optical_Flow_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Huang_Improved_Active_Speaker_Detection_Based_on_Optical_Flow_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Active speaker detection refers to the task of inferring which (if any) of the visible people in a video is/are speaking. Existing methods based on audiovisual fusion are often confused by factors such as non-speaking facial motions, varied illumination, and low-resolution recording. To address these problems, we propose a robust active speaker detection model by incorporating the dense optical flow to strengthen the visual representation of the facial motion. These audio and visual features are processed by a two-stream embedding network, and the embeddings are fed into a prediction network for the binary speaking/non-speaking classification. To improve the learning efficiency of the entire network, we design a multi-task learning strategy to train the network. The proposed method is evaluated on the most challenging audiovisual speaker detection benchmark, the AVA-ActiveSpeaker dataset. The results demonstrate that optical flow can improve the performance of neural networks when combined with raw pixels and audio signal. It is also shown that our method consistently outperforms the state-of-the-art method in terms of both the area under the receiver operating characteristic curve (+4.4%) and the balanced accuracy (+5.28%).\r"
  },
  "cvpr2020_w56_interactivevideoretrievalwithdialog": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Interactive Video Retrieval With Dialog",
    "authors": [
      "Sho Maeoki",
      "Kohei Uehara",
      "Tatsuya Harada"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Maeoki_Interactive_Video_Retrieval_With_Dialog_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Maeoki_Interactive_Video_Retrieval_With_Dialog_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In the contemporary world, recording videos can be done quickly and easily. The quantity and availability of videos have continued to increase, therefore, an effective video retrieval method has also become important. To retrieve a target video from a large collection of videos, a video retrieval system needs to obtain appropriate queries from a user. Given a sentence query, there are many similar videos related to the query. The video retrieval system requires more information in addition to the sentence to distinguish the target video from others. If the system actively collects more information on the target video, we can perform video retrieval effectively. Thus, we propose a system to retrieve videos by asking questions about the content of the videos, and leveraging the user's responses to the questions and the dialog history. Additionally, we confirmed the usefulness of the proposed system through experiments using the dataset called AVSD which includes videos and dialogs about the videos.\r"
  },
  "cvpr2020_w56_self-supervisedobjectdetectionandretrievalusingunlabeledvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Self-Supervised Object Detection and Retrieval Using Unlabeled Videos",
    "authors": [
      "Elad Amrani",
      "Rami Ben-Ari",
      "Inbar Shapira",
      "Tal Hakim",
      "Alex Bronstein"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Amrani_Self-Supervised_Object_Detection_and_Retrieval_Using_Unlabeled_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Amrani_Self-Supervised_Object_Detection_and_Retrieval_Using_Unlabeled_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Learning an object detection or retrieval system requires a large data set with manual annotations. Such data are expensive and time-consuming to create and therefore difficult to obtain on a large scale. In this work, we propose using the natural correlation in narrations and the visual presence of objects in video to learn an object detector and retriever without any manual labeling involved. We pose the problem as weakly supervised learning with noisy labels, and propose a novel object detection and retrieval paradigm under these constraints. We handle the background rejection by using contrastive samples and confront the high level of label noise with a new clustering score. Our evaluation is based on a set of ten objects with manual ground truth annotation in almost 5000 frames extracted from instructional videos from the web. We demonstrate superior results compared to state-of-the-art weakly-supervised approaches and report a strongly-labeled upper bound as well. While the focus of the paper is object detection and retrieval, the proposed methodology can be applied to a broader range of noisy weakly-supervised problems.\r"
  },
  "cvpr2020_w56_qualityandrelevancemetricsforselectionofmultimodalpretrainingdata": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Quality and Relevance Metrics for Selection of Multimodal Pretraining Data",
    "authors": [
      "Roshan Rao",
      "Sudha Rao",
      "Elnaz Nouri",
      "Debadeepta Dey",
      "Asli Celikyilmaz",
      "Bill Dolan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Rao_Quality_and_Relevance_Metrics_for_Selection_of_Multimodal_Pretraining_Data_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Rao_Quality_and_Relevance_Metrics_for_Selection_of_Multimodal_Pretraining_Data_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Self-supervised pretraining has become a strong force in both language and vision tasks. Current efforts to improve the effects of pretraining focus on improving network architecture or defining new tasks to extract representations from the data. We focus on a third axis, the data itself, to quantify and measure how different sources and quality of data can affect the learned representations. As pretraining datasets grow larger and larger, the cost of pretraining will continue to increase. This issue is especially acute for visuolingusitic data, as the cost of storage and processing for image and video data will rise quickly. We therefore examine four visuolinguistic datasets (three preexisting datasets and one collected by us) for their utility as pretraining datasets. We define metrics for dataset quality and relevance, propose a method for subsampling large corpuses for the data most relevant to a set of downstream multimodal vision and language tasks of interest, and show that this method increases performance across the board for all downstream tasks.\r"
  },
  "cvpr2020_w56_multi-modaldensevideocaptioning": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Multi-Modal Dense Video Captioning",
    "authors": [
      "Vladimir Iashin",
      "Esa Rahtu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Dense video captioning is a task of localizing interesting events from an untrimmed video and producing textual description (captions) for each localized event. Most of the previous works in dense video captioning are solely based on visual information and completely ignore the audio track. However, audio, and speech, in particular, are vital cues for a human observer in understanding an environment. In this paper, we present a new dense video captioning approach that is able to utilize any number of modalities for event description. Specifically, we show how audio and speech modalities may improve a dense video captioning model. We apply automatic speech recognition (ASR) system to obtain a temporally aligned textual description of the speech (similar to subtitles) and treat it as a separate input alongside video frames and the corresponding audio track. We formulate the captioning task as a machine translation problem and utilize recently proposed Transformer architecture to convert multi-modal input data into textual descriptions. We demonstrate the performance of our model on ActivityNet Captions dataset. The ablation studies indicate a considerable contribution from audio and speech components suggesting that these modalities contain substantial complementary information to video frames. Furthermore, we provide an in-depth analysis of the ActivityNet Caption results by leveraging the category tags obtained from original YouTube videos. Code is publicly available: github.com/v-iashin/MDVC.\r"
  },
  "cvpr2020_w56_cross-modalvariationalalignmentoflatentspaces": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Cross-Modal Variational Alignment of Latent Spaces",
    "authors": [
      "Thomas Theodoridis",
      "Theocharis Chatzis",
      "Vassilios Solachidis",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Theodoridis_Cross-Modal_Variational_Alignment_of_Latent_Spaces_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Theodoridis_Cross-Modal_Variational_Alignment_of_Latent_Spaces_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a novel cross-modal variational alignment method in order to process and relate information across different modalities. The proposed approach consists of two variational autoencoder (VAE) networks which generate and model the latent space of each modality. The first network is a multi-modal variational autoencoder that maps directly one modality to the other, while the second one is a single-modal variational autoencoder. In order to associate the two spaces, we apply variational alignment, which acts as a translation mechanism that projects the latent space of the first VAE onto the one of the single-modal VAE through an intermediate distribution. Experimental results on four well-known datasets, covering two different application domains (food image analysis and 3D hand pose estimation), show the generality of the proposed method and its superiority against a number of state-of-the-art approaches.\r"
  },
  "cvpr2020_w56_exploringphrasegroundingwithouttrainingcontextualisationandextensiontotext-basedimageretrieval": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Exploring Phrase Grounding Without Training: Contextualisation and Extension to Text-Based Image Retrieval",
    "authors": [
      "Letitia Parcalabescu",
      "Anette Frank"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/Parcalabescu_Exploring_Phrase_Grounding_Without_Training_Contextualisation_and_Extension_to_Text-Based_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/Parcalabescu_Exploring_Phrase_Grounding_Without_Training_Contextualisation_and_Extension_to_Text-Based_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Grounding phrases in images links the visual and the textual modalities and is useful for many image understanding and multimodal tasks. All known models heavily rely on annotated data and complex trainable systems to perform phrase grounding -- except for a recent work [38] that proposes a system requiring no training nor aligned data, yet is able to compete with (weakly) supervised systems on popular phrase grounding datasets. We explore and expand the upper bound of such a system, by contextualising both the image and language representation with structured representations. We show that our extensions benefit the model and establish a harder, but fairer baseline for (weakly) supervised models. We also perform a stress-test to assess the further applicability of such a system for creating a sentence-retrieval system requiring no training nor annotated data. We show that such models have a difficult start and a long way to go and that more research is needed.\r"
  },
  "cvpr2020_w56_classification-awaresemi-superviseddomainadaptation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w56",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Multimodal Learning",
    "title": "Classification-Aware Semi-Supervised Domain Adaptation",
    "authors": [
      "Gewen He",
      "Xiaofeng Liu",
      "Fangfang Fan",
      "Jane You"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w56/He_Classification-Aware_Semi-Supervised_Domain_Adaptation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w56/He_Classification-Aware_Semi-Supervised_Domain_Adaptation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep neural networks are usually data-starved, but manually annotation can be costly in many specific tasks. For instance, the emotion recognition from the audio. However, there is a large amount of public available labeled image-based facial expression recognition datasets. How could these images help for the audio emotion recognition with limited labeled data according to their inherent correlations can be a meaningful and challenging task. In this paper, we propose a semi-supervised adversarial network that allows the knowledge transfer from the labeled videos to the heterogeneous labeled audio domain hence enhancing the audio emotion recognition performance. Specifically, face image samples are translated to the spectrograms class-wisely. To harness the translated samples in a sparsely distributed area and construct a tighter decision boundary, we propose to precisely estimate the density on feature space and incorporate the reliable low-density sample with an annealing scheme. Moreover, the unlabeled audios are collected with the high-density path in a graph representation. As a possible \"\"recognition via generation\"\" framework, we empirically demonstrated its effectiveness on several audio emotional recognition benchmarks. We also demonstrated its generality on recent large-scaled semi-supervised domain adaptation tasks.\r"
  },
  "cvpr2020_w51_focuslongertoseebetterrecursivelyrefinedattentionforfine-grainedimageclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w51",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Focus Longer to See Better: Recursively Refined Attention for Fine-Grained Image Classification",
    "authors": [
      "Prateek Shroff",
      "Tianlong Chen",
      "Yunchao Wei",
      "Zhangyang Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w51/Shroff_Focus_Longer_to_See_Better_Recursively_Refined_Attention_for_Fine-Grained_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w51/Shroff_Focus_Longer_to_See_Better_Recursively_Refined_Attention_for_Fine-Grained_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep Neural Network has shown great strides in the coarse-grained image classification task. It was in part due to its strong ability to extract discriminative feature representations from the images. However, the marginal visual difference between different classes in fine-grained images makes this very task harder. In this paper, we tried to focus on these marginal differences to extract more representative features. Similar to human vision, our network repetitively focuses on parts of images to spot small discriminative parts among the classes. Moreover, we show through interpretability techniques how our network focus changes from coarse to fine details. Through our experiments, we also show that a simple attention model can aggregate (weighted) these finer details to focus on the most dominant discriminative part of the image. Our network uses only image-level labels and does not need bounding box/part annotation information. Further, the simplicity of our network makes it an easy plug-n-play module. Apart from providing interpretability, our network boosts the performance (up to 2%) when compared to its baseline counterparts.\r"
  },
  "cvpr2020_w51_color-constraineddehazingmodel": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w51",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "Color-Constrained Dehazing Model",
    "authors": [
      "Shengdong Zhang",
      "Yue Wu",
      "Yuanjie Zhao",
      "Zuomin Cheng",
      "Wenqi Ren"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w51/Zhang_Color-Constrained_Dehazing_Model_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w51/Zhang_Color-Constrained_Dehazing_Model_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we address the insufficiency of the popular atmospheric scattering model (ASM) used in the image dehazing problem. Unlike ASM assumes the global uniform atmospheric light and attenuation coefficients and thus often introduce unrealistic color after dehazing, we propose a novel dehazing model by relaxing the global uniform atmospheric assumption to local with additional color constraints to ensure more appealing and realistic dehazed results. More precisely, we make the modeling process as an optimization problem, whose cost function is composed of color constraint, local smooth of transmission map and atmospheric light. Consequently, we are able to generate more realistic dehazed images comparing to ASM, implying that deep neural networks trained with these samples could effectively learn how to dehaze images of complicated cases, especially when the global atmospheric assumption fails. Our extensive experimental studies also confirm that the proposed dehazing model outperforms the state-of-the-art methods by a noticeable margin on all three public benchmarks including HazeRD, RESIDE, and O-HAZE in terms of SSIM and PSNR.\r"
  },
  "cvpr2020_w51_vdflowjointlearningforopticalflowandvideodeblurring": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w51",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "VDFlow: Joint Learning for Optical Flow and Video Deblurring",
    "authors": [
      "Yanyang Yan",
      "Qingbo Wu",
      "Bo Xu",
      "Jingang Zhang",
      "Wenqi Ren"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w51/Yan_VDFlow_Joint_Learning_for_Optical_Flow_and_Video_Deblurring_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w51/Yan_VDFlow_Joint_Learning_for_Optical_Flow_and_Video_Deblurring_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Video deblurring is a challenging task as the blur in videos is caused by the combination of camera motion, object moving and depth variation. Recent deep neural networks improve the performance of video deblurring by using the concatenated neighboring frames to estimate the latent images directly. In this paper, we propose a united end-to-end network, called VDFlow, for both optical flow estimation and video deblurring simultaneously. The VDFlow contains two branches where feature representations are bi-directional propagated. The deblurring branch employs an encoder-decoder style network while the optical flow branch is based on the FlowNet network. The optical flow is no longer a tool for alignment but serves as an information carrier of motion trajectories, which helps to restore the latent sharp frames. Extensive experiments demonstrate that the proposed method performs favorably against the state-of-the-art video deblurring approaches on challenging blurry videos and improves the performance of optical flow estimation as well.\r"
  },
  "cvpr2020_w51_apointlightsourceinterferenceremovalmethodforimagedehazing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w51",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Bridging the Gap Between Computational Photography and Visual Recognition",
    "title": "A Point Light Source Interference Removal Method for Image Dehazing",
    "authors": [
      "Yanyang Yan",
      "Shengdong Zhang",
      "Mingye Ju",
      "Wenqi Ren",
      "Rui Wang",
      "Yuanfang Guo"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w51/Yan_A_Point_Light_Source_Interference_Removal_Method_for_Image_Dehazing_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w51/Yan_A_Point_Light_Source_Interference_Removal_Method_for_Image_Dehazing_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Single image haze removal has been a challenging problem and the performance of the most existing dehazing methods is degraded when point light sources exist in the hazy image. In this paper, we propose a point light source interference removal method (PLiSIR) to reduce the interferences when estimating the atmospheric light. According to our observation, the pixel intensity around the point light sources can be modeled approximately by Gaussian distribution. The locations of the interfered pixels are obtained reasonably regardless of the specific number of light sources. A binary masking map is then created for distinguishing whether the pixel is affected by light sources and thus PLiSIR can be adopted to dehazing algorithms by removing the interfered pixels, during the estimation of the atmospheric light. To demonstrate how to apply PLiSIR to different algorithms, we select the dark channel prior dehazing method (DCP) and the color attenuation prior dehazing method (CAP) as two carrier methods and introduce the adaptations accordingly. Experimental results indicate that the PLiSIR can assist DCP and CAP to better estimate the atmospheric light, and thus generate better dehazing results compared to the original DCP and CAP methods. Moreover, PLiSIR also helps DCP and CAP to simplify the parameter adjustment process of the guided filter. At last, we compare our modified DCP approach (which we refer to PLiSIR-DCP) with the state-of-the-art nighttime dehazing algorithm to present an approach which is suitable for both daytime and nighttime haze removal.\r"
  },
  "cvpr2020_w50_representations,metricsandstatisticsforshapeanalysisofelasticgraphs": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Representations, Metrics and Statistics for Shape Analysis of Elastic Graphs",
    "authors": [
      "Xiaoyang Guo",
      "Anuj Srivastava"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Guo_Representations_Metrics_and_Statistics_for_Shape_Analysis_of_Elastic_Graphs_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Guo_Representations_Metrics_and_Statistics_for_Shape_Analysis_of_Elastic_Graphs_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Past approaches for statistical shape analysis of objects have focused mainly on objects within the same topological classes, e.g. , scalar functions, Euclidean curves, or surfaces, etc. For objects that differ in more complex ways, the current literature offers only topological methods. This paper introduces a far-reaching geometric approach for analyzing shapes of graphical objects, such as road networks, blood vessels, brain fiber tracts, etc. It represents such objects, exhibiting differences in both geometries and topologies, as graphs made of curves with arbitrary shapes (edges) and connected at arbitrary junctions (nodes). To perform statistical analyses, one needs mathematical representations, metrics and other geometrical tools, such as geodesics, means, and covariances. This paper utilizes a quotient structure to develop efficient algorithms for computing these quantities, leading to useful statistical tools, including principal component analysis and analytical statistical testing and modeling of graphical shapes. The efficacy of this framework is demonstrated using various simulated as well as the real data from neurons and brain arterial networks.\r"
  },
  "cvpr2020_w50_pi-netadeeplearningapproachtoextracttopologicalpersistenceimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "PI-Net: A Deep Learning Approach to Extract Topological Persistence Images",
    "authors": [
      "Anirudh Som",
      "Hongjun Choi",
      "Karthikeyan Natesan Ramamurthy",
      "Matthew P. Buman",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Som_PI-Net_A_Deep_Learning_Approach_to_Extract_Topological_Persistence_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Som_PI-Net_A_Deep_Learning_Approach_to_Extract_Topological_Persistence_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Topological features such as persistence diagrams and their functional approximations like persistence images (PIs) have been showing substantial promise for machine learning and computer vision applications. This is greatly attributed to the robustness topological representations provide against different types of physical nuisance variables seen in real-world data, such as view-point, illumination, and more. However, key bottlenecks to their large scale adoption are computational expenditure and difficulty incorporating them in a differentiable architecture. We take an important step in this paper to mitigate these bottlenecks by proposing a novel one-step approach to generate PIs directly from the input data. We design two separate convolutional neural network architectures, one designed to take in multi-variate time series signals as input and another that accepts multi-channel images as input. We call these networks Signal PI-Net and Image PI-Net respectively. To the best of our knowledge, we are the first to propose the use of deep learning for computing topological features directly from data. We explore the use of the proposed PI-Net architectures on two applications: human activity recognition using tri-axial accelerometer sensor data and image classification. We demonstrate the ease of fusion of PIs in supervised deep learning architectures and speed up of several orders of magnitude for extracting PIs from data. Our code is available at https://github.com/anirudhsom/PI-Net.\r"
  },
  "cvpr2020_w50_hierarchicalimageclassificationusingentailmentconeembeddings": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Hierarchical Image Classification Using Entailment Cone Embeddings",
    "authors": [
      "Ankit Dhall",
      "Anastasia Makarova",
      "Octavian Ganea",
      "Dario Pavllo",
      "Michael Greeff",
      "Andreas Krause"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Dhall_Hierarchical_Image_Classification_Using_Entailment_Cone_Embeddings_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Image classification has been studied extensively but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. In this work, we present a set of methods to leverage information about the semantic hierarchy induced by class labels. We first inject label-hierarchy knowledge to an arbitrary CNN-based classifier and empirically show that the availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embedding-based models governed by both Euclidean and hyperbolic geometry, prevalent in natural language and tailor them to hierarchical image classification. We empirically validate all the models on the hierarchical ETHEC dataset.\r"
  },
  "cvpr2020_w50_amc-lossangularmargincontrastivelossforimprovedexplainabilityinimageclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "AMC-Loss: Angular Margin Contrastive Loss for Improved Explainability in Image Classification",
    "authors": [
      "Hongjun Choi",
      "Anirudh Som",
      "Pavan Turaga"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Choi_AMC-Loss_Angular_Margin_Contrastive_Loss_for_Improved_Explainability_in_Image_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Choi_AMC-Loss_Angular_Margin_Contrastive_Loss_for_Improved_Explainability_in_Image_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep-learning architectures for classification problems involve the cross-entropy loss sometimes assisted with auxiliary loss functions like center loss, contrastive loss and triplet loss. These auxiliary loss functions facilitate better discrimination between the different classes of interest. However, recent studies hint at the fact that these loss functions do not take into account the intrinsic angular distribution exhibited by the low-level and high-level feature representations. This results in less compactness between samples from the same class and unclear boundary separations between data clusters of different classes. In this paper, we address this issue by proposing the use of geometric constraints, rooted in Riemannian geometry. Specifically, we propose Angular Margin Contrastive Loss (AMC-Loss), a new loss function to be used along with the traditional cross-entropy loss. The AMC-Loss employs the discriminative angular distance metric that is equivalent to geodesic distance on a hypersphere manifold such that it can serve a clear geometric interpretation. We demonstrate the effectiveness of AMC-Loss by providing quantitative and qualitative results. We find that although the proposed geometrically constrained loss-function improves quantitative results modestly, it has a qualitatively surprisingly beneficial effect on increasing the interpretability of deep-net decisions as seen by the visual explanations generated by techniques such as the Grad-CAM. Our code is available at https://github.com/hchoi71/AMC-Loss.\r"
  },
  "cvpr2020_w50_smoothsummariesofpersistencediagramsandtextureclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Smooth Summaries of Persistence Diagrams and Texture Classification",
    "authors": [
      "Yu-Min Chung",
      "Michael Hull",
      "Austin Lawson"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Chung_Smooth_Summaries_of_Persistence_Diagrams_and_Texture_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Chung_Smooth_Summaries_of_Persistence_Diagrams_and_Texture_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Topological data analysis (TDA) is a rising field in the intersection of mathematics, statistics, and computer science/data science. Persistent homology is one of the most commonly used tools in TDA, in part because it can be easily visualized in the form of a persistence diagram. However, performing machine learning algorithms directly on persistence diagrams is a challenging task, and so a number of summaries have been proposed which transform persistence diagrams into vectors or functions. Many of these summaries fall into the persistence curve framework developed by Chung and Lawson. We extend this framework and introduce new class of smooth persistence curves which we call Gaussian persistence curves. We investigate the statistical properties of Gaussian persistence curves and apply them to texture datasets: UIUCTex and KTH. Our classification results on these texture datasets outperform the current state-of-arts methods in TDA.\r"
  },
  "cvpr2020_w50_gromov-wassersteinaveraginginariemannianframework": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Gromov-Wasserstein Averaging in a Riemannian Framework",
    "authors": [
      "Samir Chowdhury",
      "Tom Needham"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Chowdhury_Gromov-Wasserstein_Averaging_in_a_Riemannian_Framework_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Chowdhury_Gromov-Wasserstein_Averaging_in_a_Riemannian_Framework_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We introduce a theoretical framework for performing statistical tasks - including, but not limited to, averaging and principal component analysis - on the space of (possibly asymmetric) matrices with arbitrary entries and sizes. This is carried out under the lens of the Gromov-Wasserstein (GW) distance, and our methods translate the Riemannian framework of GW distances developed by Sturm into practical, implementable tools for network data analysis. Our methods are illustrated on datasets of letter graphs, asymmetric stochastic blockmodel networks, and planar shapes viewed as metric spaces. On the theoretical front, we supplement the work of Sturm by producing additional results on the tangent structure of this \"space of spaces\", as well as on the gradient flow of the Frechet functional on this space.\r"
  },
  "cvpr2020_w50_theweightedeulercurvetransformforshapeandimageanalysis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "The Weighted Euler Curve Transform for Shape and Image Analysis",
    "authors": [
      "Qitong Jiang",
      "Sebastian Kurtek",
      "Tom Needham"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Jiang_The_Weighted_Euler_Curve_Transform_for_Shape_and_Image_Analysis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Jiang_The_Weighted_Euler_Curve_Transform_for_Shape_and_Image_Analysis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The Euler Curve Transform (ECT) of Turner et al. is a complete invariant of an embedded simplicial complex, which is amenable to statistical analysis. We generalize the ECT to provide a similarly convenient representation for weighted simplicial complexes, objects which arise naturally, for example, in certain medical imaging applications. We leverage work of Ghrist et al. on Euler integral calculus to prove that this invariant - dubbed the Weighted Euler Curve Transform (WECT) - is also complete. We explain how to transform a segmented region of interest in a grayscale image into a weighted simplicial complex and then into a WECT representation. This WECT representation is applied to study Glioblastoma Multiforme brain tumor shape and texture data. We show that the WECT representation is effective at clustering tumors based on qualitative shape and texture features and that this clustering correlates with patient survival time.\r"
  },
  "cvpr2020_w50_aninterfacebetweengrassmannmanifoldsandvectorspaces": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "An Interface Between Grassmann Manifolds and Vector Spaces",
    "authors": [
      "Lincon S. Souza",
      "Naoya Sogi",
      "Bernardo B. Gatto",
      "Takumi Kobayashi",
      "Kazuhiro Fukui"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Souza_An_Interface_Between_Grassmann_Manifolds_and_Vector_Spaces_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Souza_An_Interface_Between_Grassmann_Manifolds_and_Vector_Spaces_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a method to map data from a Grassmann manifold to a vector space while maximizing discrimination capability for classification. Subspaces are a practical and robust representation for image set recognition. However, as they exist on a Grassmann manifold, machine learning tools constructed on Euclidean geometry cannot be promptly utilized. Recently, methods to construct end-to-end learnable models for subspaces are starting to be explored, but they require multiple matrix decompositions and can be hard to compute and extend. Therefore we introduce a layer to map Grassmann manifold-valued data to vector space, in such a way that it can be seamlessly used as a layer along with other powerful tools defined on Euclidean space. The key idea of our method is to formulate the manifold logarithmic map (log) as a learnable model, where we seek to learn a tangency point that minimizes a loss function with respect to the data. The log effectively transforms a manifold point into a tangent vector. This log model can be learned with Riemannian stochastic gradient descent on the target manifold. We demonstrate the effectiveness of our proposed method on the applications of hand shape recognition, face identification and facial emotion recognition.\r"
  },
  "cvpr2020_w50_simplifyingtransformationsforafamilyofelasticmetricsonthespaceofsurfaces": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Simplifying Transformations for a Family of Elastic Metrics on the Space of Surfaces",
    "authors": [
      "Zhe Su",
      "Martin Bauer",
      "Eric Klassen",
      "Kyle Gallivan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Su_Simplifying_Transformations_for_a_Family_of_Elastic_Metrics_on_the_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Su_Simplifying_Transformations_for_a_Family_of_Elastic_Metrics_on_the_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We define a new representation for immersed surfaces in Rn by combining the SRNF and the induced surface metric. Using the L2 metric on the space of SRNFs and the DeWitt metric on the space of surface metrics, we obtain a 3-parameter family of metrics that corresponds to the family of \"elastic metrics\" proposed by Jermyn et al. on the space of immersed surfaces. Similar to the original SRNF representation this new representation results in an extrinsic distance function on the space of immersed surfaces that is easy to compute as it is given by an explicit formula. In addition to avoiding the degeneracy of the SRNF it allows for a data-driven choice of the parameters of the metric, while still providing for fast and accurate registration of surfaces.\r"
  },
  "cvpr2020_w50_metriclearningwitha-basedscalarproductforimage-setrecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Metric Learning With A-Based Scalar Product for Image-Set Recognition",
    "authors": [
      "Naoya Sogi",
      "Lincon S. Souza",
      "Bernardo B. Gatto",
      "Kazuhiro Fukui"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Sogi_Metric_Learning_With_A-Based_Scalar_Product_for_Image-Set_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Sogi_Metric_Learning_With_A-Based_Scalar_Product_for_Image-Set_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a metric learning method for image set recognition using subspace representation. The subspace representation is effective for image set recognition where each image set is compactly represented by a subspace in a high dimensional vector space. In this framework, the similarity between two given image sets is measured by the canonical angles between the two corresponding subspaces. Many types of methods utilizing the concept of canonical angles have been developed and studied extensively. However, there still remains large potential in improving the ability to measure canonical angles. Our key idea is to learn a general scalar product space (metric space) that produces more valid canonical angles between two subspaces. To realize this idea, we first introduce an A-based scalar product instead of the standard scalar product, where A is a symmetric positive definite matrix and the canonical angles between two subspaces are measured through the A-based scalar product. We learn a discriminative metric space by optimizing metric A in terms of the Fisher ratio from local Fisher discriminant analysis. Besides, we introduce a mechanism to automatically reduce the dimension of the metric space by imposing a low-rank constraint on metric A. The effectiveness of the proposed methods is validated through extensive classification experiments on three real-world datasets.\r"
  },
  "cvpr2020_w50_ageometricconvneton3dshapemanifoldforgaitrecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Geometric ConvNet on 3D Shape Manifold for Gait Recognition",
    "authors": [
      "Nadia Hosni",
      "Boulbaba Ben Amor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Hosni_A_Geometric_ConvNet_on_3D_Shape_Manifold_for_Gait_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Hosni_A_Geometric_ConvNet_on_3D_Shape_Manifold_for_Gait_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this work we propose a geometric deep convolutional auto-encoder (DCAE) for the purpose of gait recognition by analyzing time-varying 3D skeletal data. Sequences are viewed as time-parameterized trajectories on the Kendall shape space S, results of modding out shape-preserving transformations (scaling, translations and rotations). The accommodation of ConvNet architectures to properly approximate manifold-valued trajectories on the underlying non-linear space S is a must. Thus, we make use of geometric steps prior to the encoding-decoding scheme. That is, shape trajectories are first log-mapped to tangent spaces attached to the shape space at a time-varying average trajectory u, then, obtained vectors are transported to a common tangent space Tu(0)(S) at the starting point of u. Without applying any prior temporal alignment (e.g. Dynamic Time Warping) or modeling (e.g. HMM, RNN), the transported trajectories are then fed to a convolutional auto-encoder to build subject-specific latent spaces. The proposed approach was tested on two publicly available datasets. Our approach outperforms existing approaches on CMU gait dataset, while performances on UPCV K2 are comparable to existing approaches. We demonstrate that combining geometric invariance (i.e. Kendall's representation) with our data-driven ConvNet model is suitable to alleviate spatial and temporal variability, respectively.\r"
  },
  "cvpr2020_w50_agenericunfoldingalgorithmformanifoldsestimatedbylocallinearapproximations": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "A Generic Unfolding Algorithm for Manifolds Estimated by Local Linear Approximations",
    "authors": [
      "Jonas Nordhaug Myhre",
      "Matineh Shaker",
      "Mustafa Devrim Kaba",
      "Robert Jenssen",
      "Deniz Erdogmus"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Myhre_A_Generic_Unfolding_Algorithm_for_Manifolds_Estimated_by_Local_Linear_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Myhre_A_Generic_Unfolding_Algorithm_for_Manifolds_Estimated_by_Local_Linear_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The individual stages of most popular manifold learning algorithms are complicated by overlapping ideas -- often consisting of a mix of learning how to embed, unfold and reduce the dimension of the manifold at the same time. Furthermore, the effect each step has on the final result is in many cases not clear. Research in both machine learning and mathematical communities has focused on the steps involved in manifold embedding and estimation, and sample sizes and performance bounds related to these operations have been explored. However, the problem of unwrapping or unfolding manifolds has received relatively little attention despite being an integral part of manifold learning in general. In this work, we present a new generic algorithm for unfolding manifolds that have been estimated by local linear approximations. Our algorithm is a combination of ideas from principal curves and density ridge estimation and tools from classical differential geometry. Numerical experiments on both real and synthetic data sets illustrates the merit of our proposed algorithm.\r"
  },
  "cvpr2020_w50_persistenthomology-basedprojectionpursuit": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Persistent Homology-Based Projection Pursuit",
    "authors": [
      "Oleg Kachan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Kachan_Persistent_Homology-Based_Projection_Pursuit_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Kachan_Persistent_Homology-Based_Projection_Pursuit_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Dimensionality reduction problem is stated as finding a mapping from the original to the low-dimensional space while preserving some relevant properties of the data. We formulate topology-preserving dimensionality reduction as finding the optimal orthogonal projection to the lower-dimensional subspace which minimizes discrepancy between persistent diagrams of the original data and the projection. This generalizes the classic projection pursuit algorithm which was originally designed to preserve the number of clusters, i.e. the 0-order topological invariant of the data. Our approach further allows to preserve k-th order invariants within the principled framework. We further pose the resulting optimization problem as the Riemannian optimization problem which allows for a natural and efficient solution.\r"
  },
  "cvpr2020_w50_curvatureasignatureforactionrecognitioninvideosequences": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Curvature: A Signature for Action Recognition in Video Sequences",
    "authors": [
      "He Chen",
      "Gregory S. Chirikjian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Chen_Curvature_A_Signature_for_Action_Recognition_in_Video_Sequences_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Chen_Curvature_A_Signature_for_Action_Recognition_in_Video_Sequences_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, a novel signature of human action recognition, namely the curvature of a video sequence, is introduced. In this way, the distribution of sequential data is modeled, which enables few-shot learning. Instead of depending on recognizing features within images, our algorithm views actions as sequences on the universal time scale across a whole sequence of images. The video sequence, viewed as a curve in pixel space, is aligned by reparameterization using the arclength of the curve in pixel space. Once such curvatures are obtained, statistical indexes are extracted and fed into a learning-based classifier. Overall, our method is simple but powerful. Preliminary experimental results show that our method is effective and achieves state-of-the-art performance in video-based human action recognition.\r"
  },
  "cvpr2020_w50_coarse-to-finehamiltoniandynamicsofhierarchicalflowsincomputationalanatomy": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Coarse-to-Fine Hamiltonian Dynamics of Hierarchical Flows in Computational Anatomy",
    "authors": [
      "Michael I. Miller",
      "Daniel J. Tward",
      "Alain Trouve"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Miller_Coarse-to-Fine_Hamiltonian_Dynamics_of_Hierarchical_Flows_in_Computational_Anatomy_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Miller_Coarse-to-Fine_Hamiltonian_Dynamics_of_Hierarchical_Flows_in_Computational_Anatomy_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present here the Hamiltonian control equations for hierarchical diffeomorphic flows of particles. We define the controls to be a series of multi-scale vector fields, each with their own reproducing kernel Hilbert space norm. The hierarchical control is connected across scale through successive refinements that refine as they ascend the hierarchy with commensurately higher bandwidth Green's kernels. Interestingly the geodesic equations do not separate, with fine scale motions determined by all of the particle information simultaneously, from coarse to fine. Additionally, the hierarchical conservation law is derived, defining the geodesics and demonstrating the constancy of the Hamiltonian. We show results on one simulated example and one example from histological images of an Alzheimer's disease brain. We introduce the varifold action to transport the weights of micro-scale particles for mapping to sub millimeter scale cortical folds.\r"
  },
  "cvpr2020_w50_infinitesimaldriftdiffeomorphometrymodelsforpopulationshapeanalysis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Infinitesimal Drift Diffeomorphometry Models for Population Shape Analysis",
    "authors": [
      "Brian C. Lee",
      "Daniel J. Tward",
      "Zhiyi Hu",
      "Alain Trouve",
      "Michael I. Miller"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Lee_Infinitesimal_Drift_Diffeomorphometry_Models_for_Population_Shape_Analysis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Lee_Infinitesimal_Drift_Diffeomorphometry_Models_for_Population_Shape_Analysis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Describing longitudinal morphometric differences between populations and individuals is a critical task in computational anatomy. In the context of the random orbit model of computational anatomy, this often implies study of the variation of individual shape trajectories associated to some mean field, as well as longitudinal morphological differences as encoded by similar subjects from representative populations. In this paper, we present a new method for computing the deviation of individual subjects from models of flow. We demonstrate estimation of the infinitesimal drift representing the mean flow of a population and its entrance into the Eulerian vector field controlling that flow. Each individual is studied longitudinally by modeling another associated individual drift which acts as the personalized control of the flow. We provide an augmentation of the classic LDDMM equations to generate \"biased geodesics\" for trajectory shooting algorithms, allowing for direct computation of the individual's deviation under the influence of a mean drift. Our new model is inspired by diffusion models from stochastic processes in which the personalized control is a non-stochastic term representing the additive Brownian component on top of the infinitesimal drift representing the population. We present results of our model on entorhinal cortical surfaces extracted from a patient population of the Alzheimer's Disease Neuroimaging Initiative.\r"
  },
  "cvpr2020_w50_deeplow-ranksubspaceclustering": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Deep Low-Rank Subspace Clustering",
    "authors": [
      "Mohsen Kheirandishfard",
      "Fariba Zohrizadeh",
      "Farhad Kamangar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Kheirandishfard_Deep_Low-Rank_Subspace_Clustering_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Kheirandishfard_Deep_Low-Rank_Subspace_Clustering_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper is concerned with developing a novel approach to tackle the problem of subspace clustering. The approach introduces a convolutional autoencoder-based architecture to generate low-rank representations (LRR) of input data which are proven to be very suitable for subspace clustering. We propose to insert a fully-connected linear layer and its transpose between the encoder and decoder to implicitly impose a rank constraint on the learned representations. We train this architecture by minimizing a standard deep subspace clustering loss function and then recover underlying subspaces by applying a variant of spectral clustering technique. Extensive experiments on benchmark datasets demonstrate that the proposed model can not only achieve very competitive clustering results using a relatively small network architecture but also can maintain its high level of performance across a wide range of LRRs. This implies that the model can be appropriately combined with the state-of-the-art subspace clustering architectures to produce more accurate results.\r"
  },
  "cvpr2020_w50_deeplearningofwarpingfunctionsforshapeanalysis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w50",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Differential Geometry in Computer Vision and Machine Learning",
    "title": "Deep Learning of Warping Functions for Shape Analysis",
    "authors": [
      "Elvis Nunez",
      "Shantanu H. Joshi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w50/Nunez_Deep_Learning_of_Warping_Functions_for_Shape_Analysis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w50/Nunez_Deep_Learning_of_Warping_Functions_for_Shape_Analysis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Rate-invariant or reparameterization-invariant matching between functions and shapes of curves, respectively, is an important problem in computer vision and medical imaging. Often, the computational cost of matching using approaches such as dynamic time warping or dynamic programming is prohibitive for large datasets. Here, we propose a deep neural-network-based approach for learning the warping functions from training data consisting of a large number of optimal matches, and use it to predict optimal diffeomorphic warping functions. Results show prediction performance on a synthetic dataset of bump functions and two-dimensional curves from the ETH-80 dataset as well as a significant reduction in computational cost.\r"
  },
  "cvpr2020_w35_viewpoint-awarechannel-wiseattentivenetworkforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Viewpoint-Aware Channel-Wise Attentive Network for Vehicle Re-Identification",
    "authors": [
      "Tsai-Shien Chen",
      "Man-Yu Lee",
      "Chih-Ting Liu",
      "Shao-Yi Chien"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Chen_Viewpoint-Aware_Channel-Wise_Attentive_Network_for_Vehicle_Re-Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Chen_Viewpoint-Aware_Channel-Wise_Attentive_Network_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification (re-ID) matches images of the same vehicle across different cameras. It is fundamentally challenging because the dramatically different appearance caused by different viewpoints would make the framework fail to match two vehicles of the same identity. Most existing works solved the problem by extracting viewpoint-aware feature via spatial attention mechanism, which, yet, usually suffers from noisy generated attention map or otherwise requires expensive keypoint labels to improve the quality. In this work, we propose Viewpoint-aware Channel-wise Attention Mechanism (VCAM) by observing the attention mechanism from a different aspect. Our VCAM enables the feature learning framework channel-wisely reweighing the importance of each feature maps according to the \"viewpoint\" of input vehicle. Extensive experiments validate the effectiveness of the proposed method and show that we perform favorably against state-of-the-arts methods on the public VeRi-776 dataset and obtain promising results on the 2020 AI City Challenge. We also conduct other experiments to demonstrate the interpretability of how our VCAM practically assists the learning framework.\r"
  },
  "cvpr2020_w35_city-scalemulti-cameravehicletrackingbysemanticattributeparsingandcross-cameratrackletmatching": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "City-Scale Multi-Camera Vehicle Tracking by Semantic Attribute Parsing and Cross-Camera Tracklet Matching",
    "authors": [
      "Yuhang He",
      "Jie Han",
      "Wentao Yu",
      "Xiaopeng Hong",
      "Xing Wei",
      "Yihong Gong"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/He_City-Scale_Multi-Camera_Vehicle_Tracking_by_Semantic_Attribute_Parsing_and_Cross-Camera_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/He_City-Scale_Multi-Camera_Vehicle_Tracking_by_Semantic_Attribute_Parsing_and_Cross-Camera_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper focuses on the Multi-Target Multi-Camera Tracking (MTMCT) task in a city-scale multi-camera network. As the trajectory of each target is naturally split into multiple sub-trajectories (namely local tracklets) in different cameras, the key issue of MTMCT is how to match local tracklets belonging to the same target across different cameras. To this end, we propose an efficient two-step MTMCT approach to robustly track vehicles in a camera network. It first generates all local tracklets and then matches the ones belonging to the same target across different cameras. More specifically, in the local tracklet generation phase, we follow the tracking-by-detection paradigm and link the detections to local tracklets by graph clustering. In the cross-camera tracklet matching phase, we first develop a spatial-temporal attention mechanism to produce robust tracklet representations. We then prune false matching candidates by traffic topology reasoning and match tracklets across cameras using the recently proposed TRACklet-to-Target Assignment (TRACTA) algorithm. The proposed method is evaluated on the City-Scale Multi-Camera Vehicle Tracking task at the 2020 AI City Challenge and achieves the second-best results.\r"
  },
  "cvpr2020_w35_avehiclecountsbyclassframeworkusingdistinguishedregionstrackingatmultipleintersections": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "A Vehicle Counts by Class Framework Using Distinguished Regions Tracking at Multiple Intersections",
    "authors": [
      "Nam Bui",
      "Hongsuk Yi",
      "Jiho Cho"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Bui_A_Vehicle_Counts_by_Class_Framework_Using_Distinguished_Regions_Tracking_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Bui_A_Vehicle_Counts_by_Class_Framework_Using_Distinguished_Regions_Tracking_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Turning movement counting plays an important step for traffic analysis at complex areas (e.g. intersections). Specifically, accurate and detailed traffic flow information enables the traffic control system to be more efficient and valuable. Recently, with the successful development of Deep Learning for vehicle detection and tracking, the current research focuses on video-based traffic analysis which is regarded as an emergent approach to monitoring vehicle movements. In this study, we present a comprehensive vehicle counting framework by integrating state-of-the-art techniques of object detection and tracking such as Yolo and DeepSort. Furthermore, in order to improve the vehicle counting problem, we propose a distinguished region tracking approach for the vehicle trajectory monitoring, which is able to work well with various scenarios, especially in complex areas with complicated movements. Regarding the experiment, the proposed framework is evaluated on the CVPR AI City Challenge 2020 dataset. Accordingly, our method is able to achieve around 85% of the accuracy which places to the top 10 of the leaderboard in Track 1 of the Challenge.\r"
  },
  "cvpr2020_w35_dualembeddingexpansionforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Dual Embedding Expansion for Vehicle Re-Identification",
    "authors": [
      "Clint Sebastian",
      "Raffaele Imbriaco",
      "Egor Bondarev",
      "Peter H. N. de With"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Sebastian_Dual_Embedding_Expansion_for_Vehicle_Re-Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Sebastian_Dual_Embedding_Expansion_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification plays a crucial role in the management of transportation infrastructure and traffic flow. However, this is a challenging task due to the large view-point variations in appearance, environmental and instance-related factors. Modern systems deploy CNNs to produce unique representations from the images of each vehicle instance. Most work focuses on leveraging new losses and network architectures to improve the descriptiveness of these representations. In contrast, our work concentrates on re-ranking and embedding expansion techniques. We propose an efficient approach for combining the outputs of multiple models at various scales while exploiting tracklet and neighbor information, called dual embedding expansion (DEx). Additionally, a comparative study of several common image retrieval techniques is presented in the context of vehicle re-ID. Our system yields competitive performance in the 2020 NVIDIA AI City Challenge with promising results. We demonstrate that DEx when combined with other re-ranking techniques, can produce an even larger gain without any additional attribute labels or manual supervision.\r"
  },
  "cvpr2020_w35_multi-domainlearningandidentityminingforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Multi-Domain Learning and Identity Mining for Vehicle Re-Identification",
    "authors": [
      "Shuting He",
      "Hao Luo",
      "Weihua Chen",
      "Miao Zhang",
      "Yuqi Zhang",
      "Fan Wang",
      "Hao Li",
      "Wei Jiang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/He_Multi-Domain_Learning_and_Identity_Mining_for_Vehicle_Re-Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/He_Multi-Domain_Learning_and_Identity_Mining_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper introduces our solution for the Trcak2 in AI City Challenge 2020 (AICITY20). The Track2 is a vehicle re-identification (ReID) task with both the real-world data and synthetic data. Our solution is based on a strong baseline with bag of tricks (BoT-BS) proposed in person ReID. At first, we propose a multi-domain learning method to joint the real-world and synthetic data to train the model. Then, we propose the Identity Mining method to automatically generate pseudo labels for a part of the testing data, which is better than the k-means clustering. The tracklet-level re-ranking strategy with weighted features is also used to post-process the results. Finally, with multiple-model ensemble, our method achieves 0.7322 in the mAP score which yields third place in the competition.\r"
  },
  "cvpr2020_w35_furthernon-localandchannelattentionnetworksforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Further Non-Local and Channel Attention Networks for Vehicle Re-Identification",
    "authors": [
      "Kai Liu",
      "Zheng Xu",
      "Zhaohui Hou",
      "Zhicheng Zhao",
      "Fei Su"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Liu_Further_Non-Local_and_Channel_Attention_Networks_for_Vehicle_Re-Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Liu_Further_Non-Local_and_Channel_Attention_Networks_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification remains challenging due to large intra-class difference and small inter-class variance. To address this problem, in AICity Vehicle Re-ID task 2020, we propose a two-branch adaptive attention network--Further Non-local and Channel attention (FNC) to improve feature representation and discrimination. Specifically, inspired by two-stream theory of visual cortex, based on Non-local and channel relation, a two-branch FNC network is constructed to capture multiple useful information. Second, an effective attention fusion method is proposed to sufficiently model the effects from spatial and channel attention. The experimental results show that our algorithm achieves 66.25%/Rank-1 and 53.54%/mAP in 2020 AICity Challenge Vehicle Re-ID task without using extra data, annotation and other auxiliary information, which demonstrate the effectiveness of the proposed FNC network.\r"
  },
  "cvpr2020_w35_multi-granularitytrackingwithmodularlizedcomponentsforunsupervisedvehiclesanomalydetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Multi-Granularity Tracking With Modularlized Components for Unsupervised Vehicles Anomaly Detection",
    "authors": [
      "Yingying Li",
      "Jie Wu",
      "Xue Bai",
      "Xipeng Yang",
      "Xiao Tan",
      "Guanbin Li",
      "Shilei Wen",
      "Hongwu Zhang",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Li_Multi-Granularity_Tracking_With_Modularlized_Components_for_Unsupervised_Vehicles_Anomaly_Detection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Li_Multi-Granularity_Tracking_With_Modularlized_Components_for_Unsupervised_Vehicles_Anomaly_Detection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Anomaly detection on road traffic is a fundamental computer vision task and plays a critical role in video structure analysis and urban traffic analysis. Although it has attracted intense attention in recent years, it remains a very challenging problem due to the complexity of the traffic scene, the dense chaos of traffic flow and the lack of fine-grained abnormal labeled data. In this paper, we propose a multi-granularity tracking approach with modularized components to analyze traffic anomaly detection. The modularized framework consists of a detection module, a background modeling module, a mask extraction module, and a multi-granularity tracking algorithm. Concretely, a box-level tracking branch and a pixel-level tracking branch is employed respectively to make abnormal predictions. Each tracking branch helps to capture abnormal abstractions at different granularity levels and provide rich and complementary information for the concept learning of abnormal behaviors. Finally, a novel fusion and backtracking optimization is further performed to refine the abnormal predictions. The experimental results reveal that our framework is superior in the Track4 test set of the NVIDIA AI CITY 2020 CHALLENGE, which ranked first in this competition, with a 98.5% F1-score and 4.8737 root mean square error.\r"
  },
  "cvpr2020_w35_electricityanefficientmulti-cameravehicletrackingsystemforintelligentcity": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "ELECTRICITY: An Efficient Multi-Camera Vehicle Tracking System for Intelligent City",
    "authors": [
      "Yijun Qian",
      "Lijun Yu",
      "Wenhe Liu",
      "Alexander G. Hauptmann"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Qian_ELECTRICITY_An_Efficient_Multi-Camera_Vehicle_Tracking_System_for_Intelligent_City_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Qian_ELECTRICITY_An_Efficient_Multi-Camera_Vehicle_Tracking_System_for_Intelligent_City_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " City-scale multi-camera vehicle tracking is an important task in the intelligent city and traffic management. It is quite challenging with large scale variance, frequent occlusion and appearance variance caused by viewing perspective difference. In this paper, we propose ELECTRICITY, an efficient multi-camera vehicle tracking system with aggregation loss and fast multi-target cross-camera tracking strategy. The proposed system contains four main modules. Firstly, we extract tracklets under a single camera view through object detection and multi-object tracking modules which shared the detection features. After that, we match the generated tracklets through a multi-camera re-identification module. Finally, we eliminate isolated tracklets and synchronize tracking ids according to the re-identification results. The proposed system wins the first place in the City-Scale Multi-Camera Vehicle Tracking of AI City 2020 Challenge (Track 3) with a score of 0.4585.\r"
  },
  "cvpr2020_w35_vehiclere-identificationbasedoncomplementaryfeatures": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Vehicle Re-Identification Based on Complementary Features",
    "authors": [
      "Cunyuan Gao",
      "Yi Hu",
      "Yi Zhang",
      "Rui Yao",
      "Yong Zhou",
      "Jiaqi Zhao"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Gao_Vehicle_Re-Identification_Based_on_Complementary_Features_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Gao_Vehicle_Re-Identification_Based_on_Complementary_Features_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this work, we present our solution to the vehicle re-identification (vehicle Re-ID) track in AI City Challenge 2020 (AIC2020). The purpose of vehicle Re-ID is to retrieve the same vehicle appeared across multiple cameras, and it could make a great contribution to the Intelligent Traffic System(ITS) and smart city. Due to the vehicle's orientation, lighting and inter-class similarity, it is difficult to achieve robust and discriminative representation feature. For the vehicle Re-ID track in AIC2020, our method is to fuse features extracted from different networks in order to take advantages of these networks and achieve complementary features. For each single model, several methods such as multi-loss, filter grafting, semi-supervised are used to increase the representation ability as better as possible. Top performance in City-Scale Multi-Camera Vehicle Re-Identification demonstrated the advantage of our methods, we got 5-th place in the vehicle Re-ID track of AIC2020.\r"
  },
  "cvpr2020_w35_towardsreal-timetrafficmovementcountandtrajectoryreconstructionusingvirtualtrafficlanes": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Towards Real-Time Traffic Movement Count and Trajectory Reconstruction Using Virtual Traffic Lanes",
    "authors": [
      "Awad Abdelhalim",
      "Montasir Abbas"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Abdelhalim_Towards_Real-Time_Traffic_Movement_Count_and_Trajectory_Reconstruction_Using_Virtual_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Abdelhalim_Towards_Real-Time_Traffic_Movement_Count_and_Trajectory_Reconstruction_Using_Virtual_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we discuss our framework and observations for AI City Challenge Track 1: Vehicle Counts by Class at Multiple Intersections. The framework we propose utilizes creating virtual traffic lanes for the movements of interest. Using a Python Graphical User Interface (GUI), the entry polygons for the movements of interest are identified. This leads to labeling the trajectories for the vehicles that have been first detected entering the region of interest via those entry polygons. Those vehicles, forming what we refer to as \"virtual traffic lanes\" inside the region of interest, are then used as identifiers for other vehicles detected further downstream using a nearest neighbors search. The framework we propose can run as an additional layer to any multi-object tracker with minimal additional computation. Our results and evaluation for the challenge track indicate the high potential of our proposed framework and showcase the momentous value of incorporating domain knowledge in computer-vision applications.\r"
  },
  "cvpr2020_w35_zero-viruszero-shotvehiclerouteunderstandingsystemforintelligenttransportation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Zero-VIRUS: Zero-Shot VehIcle Route Understanding System for Intelligent Transportation",
    "authors": [
      "Lijun Yu",
      "Qianyu Feng",
      "Yijun Qian",
      "Wenhe Liu",
      "Alexander G. Hauptmann"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Yu_Zero-VIRUS_Zero-Shot_VehIcle_Route_Understanding_System_for_Intelligent_Transportation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Yu_Zero-VIRUS_Zero-Shot_VehIcle_Route_Understanding_System_for_Intelligent_Transportation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Nowadays, understanding the traffic statistics in real city-scale camera networks takes an important place in the intelligent transportation field. Recently, vehicle route understanding brings a new challenge to the area. It aims to measure the traffic density by identifying the route of each vehicle in traffic cameras. This year, the AI City Challenge holds a competition with real-world traffic data on vehicle route understanding, which requires both efficiency and effectiveness. In this work, we propose Zero-VIRUS, a Zero-shot VehIcle Route Understanding System, which requires no annotation for vehicle tracklets and is applicable for the changeable real-world traffic scenarios. It adopts a novel 2D field modeling of pre-defined routes to estimate the proximity and completeness of each track. The proposed system has achieved third place on Dataset A in stage 1 of the competition (Track 1: Vehicle Counts by Class at Multiple Intersections) against world-wide participants on both effectiveness and efficiency, with a record of the top place on 50% of the test set.\r"
  },
  "cvpr2020_w35_determiningvehicleturncountsatmultipleintersectionsbyseparatedvehicleclassesusingcnns": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Determining Vehicle Turn Counts at Multiple Intersections by Separated Vehicle Classes Using CNNs",
    "authors": [
      "Jan Folenta",
      "Jakub Spanhel",
      "Vojtech Bartl",
      "Adam Herout"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Folenta_Determining_Vehicle_Turn_Counts_at_Multiple_Intersections_by_Separated_Vehicle_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Folenta_Determining_Vehicle_Turn_Counts_at_Multiple_Intersections_by_Separated_Vehicle_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In our submission to the NVIDIA AI City Challenge 2020, we address the problem of counting vehicles by their class at multiple intersections. Our solution is based on counting by tracking principle using convolutional neural networks in detection and tracking steps of the proposed method. We have achieved 6th place on the dataset part \"A\" of Track 1 with score S1 Total = 0.8829, (mwRMSE = 4.3616, S1 Effectiveness = 0.9094, S1 Efficiency = 0.8212). The proposed solution was placed at sixth place in the overall ranking on dataset part A.\r"
  },
  "cvpr2020_w35_goingbeyondrealdataarobustvisualrepresentationforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Going Beyond Real Data: A Robust Visual Representation for Vehicle Re-Identification",
    "authors": [
      "Zhedong Zheng",
      "Minyue Jiang",
      "Zhigang Wang",
      "Jian Wang",
      "Zechen Bai",
      "Xuanmeng Zhang",
      "Xin Yu",
      "Xiao Tan",
      "Yi Yang",
      "Shilei Wen",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Zheng_Going_Beyond_Real_Data_A_Robust_Visual_Representation_for_Vehicle_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Zheng_Going_Beyond_Real_Data_A_Robust_Visual_Representation_for_Vehicle_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this report, we present the Baidu-UTS submission to the AICity Challenge in CVPR 2020. This is the winning solution to the vehicle re-identification (re-id) track. We focus on developing a robust vehicle re-id system for real-world scenarios. In particular, we aim to fully leverage the merits of the synthetic data while arming with real images to learn a robust representation for vehicles in different views and illumination conditions. By comprehensively investigating and evaluating various data augmentation approaches and popular strong baselines, we analyze the bottleneck restrict- ing the vehicle re-id performance. Based on our analysis, we therefore design a vehicle re-id method with better data augmentation, training and post-processing strategies. Our proposed method has achieved the 1st place out of 41 teams, yielding 84.13% mAP on the private test set. We hope that our practice could shed light on using synthetic and real data effectively in training deep re-id networks and pave the way for real-world vehicle re-id systems.\r"
  },
  "cvpr2020_w35_countorcountwithoutbellsandwhistles": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Countor: Count Without Bells and Whistles",
    "authors": [
      "Andres Ospina",
      "Felipe Torres"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Ospina_Countor_Count_Without_Bells_and_Whistles_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Ospina_Countor_Count_Without_Bells_and_Whistles_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The effectiveness of an Intelligent transportation system (ITS) relies on the understanding of the vehicles behaviour. Different approaches are proposed to extract the attributes of the vehicles as Re-Identification (ReID) or multi-target single camera tracking (MTSC). The analysis of those attributes leads to the behavioural tasks as multi-target multi-camera tracking (MTMC) and Turn-counts (Count vehicles that go through a predefined path). In this work, we propose a novel approach to Turn-counts which uses a MTSC and a proposed path classifier. The proposed method is evaluated on CVPR AI City Challenge 2020. Our algorithm achieves second place in Turn-counts with a score of 0.9346.\r"
  },
  "cvpr2020_w35_voc-reidvehiclere-identificationbasedonvehicle-orientation-camera": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "VOC-ReID: Vehicle Re-Identification Based on Vehicle-Orientation-Camera",
    "authors": [
      "Xiangyu Zhu",
      "Zhenbo Luo",
      "Pei Fu",
      "Xiang Ji"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Zhu_VOC-ReID_Vehicle_Re-Identification_Based_on_Vehicle-Orientation-Camera_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Zhu_VOC-ReID_Vehicle_Re-Identification_Based_on_Vehicle-Orientation-Camera_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification is a challenging task due to high intra-class variances and small inter-class variances. In this work, we focus on the failure cases caused by similar background and shape. They pose serve bias on similarity, making it easier to neglect fine-grained information. To reduce the bias, we propose an approach named VOC-ReID, taking the triplet vehicle-orientation-camera as a whole and reforming background/shape similarity as camera/orientation re-identification. At first, we train models for vehicle, orientation and camera re-identification respectively. Then we use orientation and camera similarity as penalty to get final similarity. Besides, we propose a high performance baseline boosted by bag of tricks and weakly supervised data augmentation. Our algorithm achieves the second place in vehicle re-identification at the NVIDIA AI City Challenge 2020.\r"
  },
  "cvpr2020_w35_vehiclere-identificationinmulti-camerascenariosbasedonensemblingdeeplearningfeatures": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Vehicle Re-Identification in Multi-Camera Scenarios Based on Ensembling Deep Learning Features",
    "authors": [
      "Paula Moral",
      "Alvaro Garcia-Martin",
      "Jose M. Martinez"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Moral_Vehicle_Re-Identification_in_Multi-Camera_Scenarios_Based_on_Ensembling_Deep_Learning_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Moral_Vehicle_Re-Identification_in_Multi-Camera_Scenarios_Based_on_Ensembling_Deep_Learning_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification (ReID) across multiple cameras is one of the principal issues in Intelligent Transportation System (ITS). The main challenge that vehicle ReID presents is the large intra-class and small inter-class variability of vehicles appearance, followed by illumination changes, different viewpoints and scales, lack of labelled data and camera resolution. To address these problems, we present a vehicle ReID system that combines different ReID models, including appearance and orientation deep learning features. Additionally, for results refinement re-ranking and a post-processing step taking into account the vehicle trajectory information provided by the CityFlow-ReID dataset are applied.\r"
  },
  "cvpr2020_w35_fractionaldatadistillationmodelforanomalydetectionintrafficvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Fractional Data Distillation Model for Anomaly Detection in Traffic Videos",
    "authors": [
      "Linu Shine",
      "Vaishnav M A",
      "Jiji C.V."
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Shine_Fractional_Data_Distillation_Model_for_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Timely automatic detection of anomalies like road accidents forms the key to any intelligent traffic monitoring system. In this paper, we propose a novel Fractional Data Distillation model for segregating traffic anomaly videos from a test dataset, with a precise estimation of the start time of the anomalous event. The model follows a similar approach to that of the typical fractional distillation procedure, where the compounds are separated by varying the temperature. Our model fractionally extracts the anomalous events depending on their nature as the detection process progresses. Here, we employ two anomaly extractors namely Normal and Zoom, of which former works on the normal scale of video and the latter works on the magnified scale on the videos missed by the former, to separate the anomalies. The backbone of this segregation is scanning the background frames using the YOLOv3 detector for spotting possible anomalies. These anomaly candidates are further filtered and compared with detection on the foreground for matching detections to estimate the start time of the anomalous event. Experimental validation on track 4 of 2020 AI City Challenge shows an s4 score of 0.5438, with an F1 score of 0.7018.\r"
  },
  "cvpr2020_w35_strdansynthetic-to-realdomainadaptationnetworkforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle Re-Identification",
    "authors": [
      "Sangrok Lee",
      "Eunsoo Park",
      "Hongsuk Yi",
      "Sang Hun Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Lee_StRDAN_Synthetic-to-Real_Domain_Adaptation_Network_for_Vehicle_Re-Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Lee_StRDAN_Synthetic-to-Real_Domain_Adaptation_Network_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification (Re-ID) aims to obtain the same vehicles from vehicle images. Vehicle Re-ID is challenging but important for analyzing and predicting traffic flow in the city. Deep learning methods have achieved huge progress in this task. However, requiring a large amount of data is a critical shortcoming. To tackle this problem, we explore the method that uses inexpensive synthetic data to improve performance. Inspired by domain adaptation and semi-supervised method, we propose joint and disjoint losses that fully utilize the synthetic data and their labels without extra cost. We evaluate our network on VeRi and CityFlow dataset with mean average precision (mAP) metric. The results show that our method outperforms the real-world data only baseline by up to 12.87% in CityFlow and 3.1% in VeRi\r"
  },
  "cvpr2020_w35_robustandfastvehicleturn-countsatintersectionsviaanintegratedsolutionfromdetection,trackingandtrajectorymodeling": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Robust and Fast Vehicle Turn-Counts at Intersections via an Integrated Solution From Detection, Tracking and Trajectory Modeling",
    "authors": [
      "Zhihui Wang",
      "Bing Bai",
      "Yujun Xie",
      "Tengfei Xing",
      "Bineng Zhong",
      "Qinqin Zhou",
      "Yiping Meng",
      "Bin Xu",
      "Zhichao Song",
      "Pengfei Xu",
      "Runbo Hu",
      "Hua Chai"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Wang_Robust_and_Fast_Vehicle_Turn-Counts_at_Intersections_via_an_Integrated_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Wang_Robust_and_Fast_Vehicle_Turn-Counts_at_Intersections_via_an_Integrated_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we address the problem of vehicle turn- counts by class at multiple intersections, which is greatly challenged by inaccurate detection and tracking results caused by heavy weather, occlusion, illumination variations, background clutter, etc. Therefore, the complexity of the problem calls for an integrated solution that robustly ex- tracts as much visual information as possible and efficiently combines it through sequential feedback cycles. We pro- pose such an algorithm, which effectively combines detection, background modeling, tracking, trajectory modeling and matching in a sequential manner. Firstly, to improve detection performances, we design a GMM like background modeling method to detect moving objects. Then, the pro- posed GMM like background modeling method is combined with an effective yet efficiency deep learning based detector to achieve high-quality vehicle detection. Based on the detection results, a simple yet effective multi-object tracking method is proposed to generate each vehicle's movement trajectory. Conditioned on each vehicle's trajectory, we then propose a trajectory modeling and matching schema which leverages the direction and speed of a local vehicle's trajectory to improve the robustness and accuracy of vehicle turn-counts. Our method is validated on the AICity Track1 dataset A, and has achieved 91.40% in effectiveness, 95.4% in efficiency, and 92.60% S1-score, respectively. The experimental results show that our method is not only effective and efficient, but also can achieve robust counting performance in real-world scenes.\r"
  },
  "cvpr2020_w35_itask-intelligenttrafficanalysissoftwarekit": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "iTASK - Intelligent Traffic Analysis Software Kit",
    "authors": [
      "Minh-Triet Tran",
      "Tam V. Nguyen",
      "Trung-Hieu Hoang",
      "Trung-Nghia Le",
      "Khac-Tuan Nguyen",
      "Dat-Thanh Dinh",
      "Thanh-An Nguyen",
      "Hai-Dang Nguyen",
      "Xuan-Nhat Hoang",
      "Trong-Tung Nguyen",
      "Viet-Khoa Vo-Ho",
      "Trong-Le Do",
      "Lam Nguyen",
      "Minh-Quan Le",
      "Hoang-Phuc Nguyen-Dinh",
      "Trong-Thang Pham",
      "Xuan-Vy Nguyen",
      "E-Ro Nguyen",
      "Quoc-Cuong Tran",
      "Hung Tran",
      "Hieu Dao",
      "Mai-Khiem Tran",
      "Quang-Thuc Nguyen",
      "Tien-Phat Nguyen",
      "The-Anh Vu-Le",
      "Gia-Han Diep",
      "Minh N. Do"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Tran_iTASK_-_Intelligent_Traffic_Analysis_Software_Kit_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Tran_iTASK_-_Intelligent_Traffic_Analysis_Software_Kit_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Traffic flow analysis is essential for intelligent transportation systems. In this paper, we introduce our Intelligent Traffic Analysis Software Kit (iTASK) to tackle three challenging problems: vehicle flow counting, vehicle re-identification, and abnormal event detection. For the first problem, we propose to real-time track vehicles moving along the desired direction in corresponding motion-of-interests (MOIs). For the second problem, we consider each vehicle as a document with multiple semantic words (i.e., vehicle attributes) and transform the given problem to classical document retrieval. For the last problem, we propose to forward and backward refine anomaly detection using GAN-based future prediction and backward tracking completely stalled vehicle or sudden-change direction, respectively. Experiments on the datasets of traffic flow analysis from AI City Challenge 2020 show our competitive results, namely, S1 score of 0.8297 for vehicle flow counting in Track 1, mAP score of 0.3882 for vehicle re-identification in Track 2, and S4 score of 0.9059 for anomaly detection in Track 4. All data and source code are publicly available on our project page.\r"
  },
  "cvpr2020_w35_robustmovement-specificvehiclecountingatcrowdedintersections": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Robust Movement-Specific Vehicle Counting at Crowded Intersections",
    "authors": [
      "Zhongji Liu",
      "Wei Zhang",
      "Xu Gao",
      "Hao Meng",
      "Xiao Tan",
      "Xiaoxing Zhu",
      "Zhan Xue",
      "Xiaoqing Ye",
      "Hongwu Zhang",
      "Shilei Wen",
      "Errui Ding"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Liu_Robust_Movement-Specific_Vehicle_Counting_at_Crowded_Intersections_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Liu_Robust_Movement-Specific_Vehicle_Counting_at_Crowded_Intersections_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " With the demands of intelligent traffic, vehicle counting has become a vital problem, which can be used to mitigate traffic congestion and elevate the efficiency of the traffic light. Traditional vehicle counting problems focus on counting vehicles in a single frame or consecutive frames. Nevertheless, they are not expected to count vehicles by movements of interest (MOI), which can be pre-defined by all possible states of vehicles, combining different lanes and directions. In this paper, we mainly focus on movement-specific vehicle counting problems. A detection-tracking-counting (DTC) framework is applied, which detects and tracks objects in the region of interest (ROI), then counts those tracked trajectories by movements. To be specific, we propose the detection augmentation method and the Mahalanobis distance smoothness method to improve the multi-object tracking performance. For vehicle counting, a shape-based movement assignment method is carefully designed to categorize each trajectory by movements. Experiments are conducted on both the AICity 2020 Track-1 Dataset and the Vehicle-Track Dataset, which is built in this paper. Experimental results show the effectiveness and efficiency of our method.\r"
  },
  "cvpr2020_w35_largescalevehiclere-identificationbyknowledgetransferfromsimulateddataandtemporalattention": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Large Scale Vehicle Re-Identification by Knowledge Transfer From Simulated Data and Temporal Attention",
    "authors": [
      "Viktor Eckstein",
      "Arne Schumann",
      "Andreas Specker"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Eckstein_Large_Scale_Vehicle_Re-Identification_by_Knowledge_Transfer_From_Simulated_Data_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Eckstein_Large_Scale_Vehicle_Re-Identification_by_Knowledge_Transfer_From_Simulated_Data_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Automated re-identification (re-id) of vehicles is the foundation of many traffic analysis applications across camera networks, e.g. vehicle tracking, counting, or traffic density and flow estimation. The re-id task is made difficult by variations in lighting, viewpoint, image quality and similar vehicle models and colors that can occur across the network. These influences can cause a high visual appearance variation for the same vehicle while different vehicle may look near identical under similar conditions. However, with a growing number of available datasets and well crafted deep learning models, much progress has been made. We address the vehicle re-id task by relying on well-proven design choices from the closely related person re-id literature. In addition to this, we focus on viewpoint and occlusions variation. The former is addressed by incorporating vehicle viewpoint classification results into our matching distance. The required viewpoint classifier is trained predominantly on simulated data and we show that it can be applied to real-world imagery with minimal domain adaptation. We address occlusion by relying on temporal attention scores, which emphasize video frames in which occlusions are minimal. Finally, we further boost re-id accuracy by applying video-based re-ranking and an ensemble of complementary models.\r"
  },
  "cvpr2020_w35_attribute-guidedfeatureextractionandaugmentationrobustlearningforvehiclere-identification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Attribute-Guided Feature Extraction and Augmentation Robust Learning for Vehicle Re-Identification",
    "authors": [
      "Chaoran Zhuge",
      "Yujie Peng",
      "Yadong Li",
      "Jiangbo Ai",
      "Junru Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Zhuge_Attribute-Guided_Feature_Extraction_and_Augmentation_Robust_Learning_for_Vehicle_Re-Identification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Zhuge_Attribute-Guided_Feature_Extraction_and_Augmentation_Robust_Learning_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification is one of the core technologies of intelligent transportation systems and smart cities, but large intra-class diversity and inter-class similarity poses great challenges for existing method. In this paper, we propose a multi-guided learning approach which utilizing the information of attributes and meanwhile introducing two novel random augments to improve the robustness during training. What's more, we propose an attribute constraint method and group re-ranking strategy to refine matching results. Our method achieves mAP of 66.83% and rank-1 accuracy 76.05% in the CVPR 2020 AI City Challenge.\r"
  },
  "cvpr2020_w35_aicitychallenge2020-computervisionforsmarttransportationapplications": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "AI City Challenge 2020 - Computer Vision for Smart Transportation Applications",
    "authors": [
      "Ming-Ching Chang",
      "Chen-Kuo Chiang",
      "Chun-Ming Tsai",
      "Yun-Kai Chang",
      "Hsuan-Lun Chiang",
      "Yu-An Wang",
      "Shih-Ya Chang",
      "Yun-Lun Li",
      "Ming-Shuin Tsai",
      "Hung-Yu Tseng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Chang_AI_City_Challenge_2020_-_Computer_Vision_for_Smart_Transportation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Chang_AI_City_Challenge_2020_-_Computer_Vision_for_Smart_Transportation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present methods developed in our participation of the AI City 2020 Challenge (AIC20) and report evaluation results in this contest. With the blooming of AI computer vision techniques, vehicle detection, tracking, identification, and counting all have advanced significantly. However, whether these technologies are ready for real-world smart transportation usage is still a open question. The goal of this work is to apply and integrate state-of-the-art techniques for solving the challenge problems under a standardized setup and evaluation. We participated all 4 AIC20 challenge tracks (T1 to T4). In T1 challenge, we perform vehicle counting by associating deep features extracted from Mask-RCNN detections and tracklets, followed by vehicle movement zone matching. In T2 challenge, we perform vehicle type and color classification and then rank matching vehicles using a PGAM re-id network. In T3 challenge, we proposed a new Multi-Camera Tracking Network (MTCN) that takes single-camera vehicle tracking as input, and performs multi-camera tracklet fusion and linking, by jointly optimizing the matching of vehicle appearance and physical features. In T4 challenge, we adopt a leading method based on perspective detection and spatial-temporal matrix discriminating, and improve it with background modeling for traffic anomaly detection. We achieved top-6 and top-4 performance for T3 and T4 challenges respectively in the AIC20 leaderboard.\r"
  },
  "cvpr2020_w35_towardsreal-timesystemsforvehiclere-identification,multi-cameratracking,andanomalydetection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Towards Real-Time Systems for Vehicle Re-Identification, Multi-Camera Tracking, and Anomaly Detection",
    "authors": [
      "Neehar Peri",
      "Pirazh Khorramshahi",
      "Sai Saketh Rambhatla",
      "Vineet Shenoy",
      "Saumya Rawat",
      "Jun-Cheng Chen",
      "Rama Chellappa"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Peri_Towards_Real-Time_Systems_for_Vehicle_Re-Identification_Multi-Camera_Tracking_and_Anomaly_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Peri_Towards_Real-Time_Systems_for_Vehicle_Re-Identification_Multi-Camera_Tracking_and_Anomaly_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Vehicle re-identification, multi-camera vehicle tracking, and anomaly detection are essential for city-scale intelligent transportation systems. Both vehicle re-id and multi-camera tracking are challenging due to variations in aspect-ratio, occlusion, and orientation. Robust re-id and tracking systems must consider small scale variations in a vehicle's appearance to accurately distinguish among vehicles of the same make, model, and color. Scalability is critical for multi-camera systems, as the number of objects in a scene is not known a-priori. Anomaly detection presents a unique challenge due to a dearth of annotations and varied video quality. In this paper, we address the task of vehicle re-id by introducing an unsupervised excitation layer to enhance representation learning. We propose a multi-camera tracking pipeline leveraging this re-id feature extractor to compute a distance matrix and perform clustering to obtain multi-camera vehicle trajectories. Lastly, we leverage background modeling techniques to localize anomalies such as stalled vehicles and collisions. We show the effectiveness of our proposed method on the NVIDIA AI City Challenge, where we obtain 7th place out of 41 teams for the task of vehicle re-id, with an mAP score of 66.68% and achieve state-of-the-art results on the Vehicle-ID dataset. We also obtain an IDF1 score of 12.45% on multi-camera vehicle tracking, and an S4 score of 29.52% for task of anomaly detection, ranking in the top 5 for both tracks.\r"
  },
  "cvpr2020_w35_fastunsupervisedanomalydetectionintrafficvideos": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "Fast Unsupervised Anomaly Detection in Traffic Videos",
    "authors": [
      "Keval Doshi",
      "Yasin Yilmaz"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Doshi_Fast_Unsupervised_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Doshi_Fast_Unsupervised_Anomaly_Detection_in_Traffic_Videos_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Anomaly detection in traffic videos has been recently gaining attention due to its importance in intelligent transportation systems. Due to several factors such as weather, viewpoint, lighting conditions, etc. affecting the video quality of a real time traffic feed, it still remains a challenging problem. Even though the performance of state-of-the-art methods on the available benchmark dataset has been competitive, they demand a massive amount of external training data combined with significant computational resources. In this paper, we propose a fast unsupervised anomaly detection system comprising of three modules: preprocessing module, candidate selection module and backtracking anomaly detection module. The preprocessing module outputs stationary objects detected in a video. Then, the candidate selection module removes the misclassified stationary objects using a nearest neighbor approach and then uses K-means clustering to identify potential anomalous regions. Finally, the backtracking anomaly detection algorithm computes a similarity statistic and decides on the onset time of the anomaly. Experimental results on the Track 4 test set of the NVIDIA AI CITY 2020 challenge show the efficacy of the proposed framework as we achieve an F1-score of 0.5926 along with 8.2386 root mean square error (RMSE) and are ranked second in the competition.\r"
  },
  "cvpr2020_w35_the4thaicitychallenge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w35",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - AI City Challenge",
    "title": "The 4th AI City Challenge",
    "authors": [
      "Milind Naphade",
      "Shuo Wang",
      "David C. Anastasiu",
      "Zheng Tang",
      "Ming-Ching Chang",
      "Xiaodong Yang",
      "Liang Zheng",
      "Anuj Sharma",
      "Rama Chellappa",
      "Pranamesh Chakraborty"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w35/Naphade_The_4th_AI_City_Challenge_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w35/Naphade_The_4th_AI_City_Challenge_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.\r"
  },
  "cvpr2020_w34_amethodfordetectingtextofarbitraryshapesinnaturalscenesthatimprovestextspotting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "A Method for Detecting Text of Arbitrary Shapes in Natural Scenes That Improves Text Spotting",
    "authors": [
      "Qitong Wang",
      "Yi Zheng",
      "Margrit Betke"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Wang_A_Method_for_Detecting_Text_of_Arbitrary_Shapes_in_Natural_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Wang_A_Method_for_Detecting_Text_of_Arbitrary_Shapes_in_Natural_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Understanding the meaning of text in images of natural scenes like highway signs or store front emblems is particularly challenging if the text is foreshortened in the image or the letters are artistically distorted. We introduce a pipeline-based text spotting framework that can both detect and recognize text in various fonts, shapes, and orientations in natural scene images with complicated backgrounds. The main contribution of our work is the text detection component, which we call UHT, short for UNet, Heatmap, and Textfill. UHT uses a UNet to compute heatmaps for candidate text regions and a textfill algorithm to produce tight polygonal boundaries around each word in the candidate text. Our method trains the UNet with groundtruth heatmaps that we obtain from text bounding polygons provided by groundtruth annotations. Our text spotting framework, called UHTA, combines UHT with the state-of-the-art text recognition system ASTER. Experiments on four challenging and public scene-text-detection datasets (Total-Text, SCUT-CTW1500, MSRA-TD500, and COCO-Text) show the effectiveness and generalization ability of UHT in detecting not only multilingual (potentially rotated) straight but also curved text in scripts of multiple languages. Our experimental results of UHTA on the Total-Text dataset show that UHTA outperforms four state-of-the-art text spotting frameworks by at least 9.1 percent points in the F-measure, which suggests that UHTA may be used as a complete text detection and recognition system in real applications.\r"
  },
  "cvpr2020_w34_textualvisualsemanticdatasetfortextspotting": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Textual Visual Semantic Dataset for Text Spotting",
    "authors": [
      "Ahmed Sabir",
      "Francesc Moreno-Noguer",
      "Lluis Padro"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Sabir_Textual_Visual_Semantic_Dataset_for_Text_Spotting_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Sabir_Textual_Visual_Semantic_Dataset_for_Text_Spotting_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Text Spotting in the wild consists of detecting and recognizing text appearing in images (e.g. signboards, traffic signals or brands in clothing or objects). This is a challenging problem due to the complexity of the context where texts appear (uneven backgrounds, shading, occlusions, perspective distortions, etc.). Only a few approaches try to exploit the relation between text and its surrounding environment to better recognize text in the scene. In this paper, we propose a visual context dataset for Text Spotting in the wild, where the publicly available dataset COCO-text [Veit et al. 2016] has been extended with information about the scene (such as objects and places appearing in the image) to enable researchers to include semantic relations between texts and scene in their Text Spotting systems, and to offer a common framework for such approaches. For each text in an image, we extract three kinds of context information: objects in the scene, image location label and a textual image description (caption). We use state-of-the-art out-of-the-box available tools to extract this additional information. Since this information has textual form, it can be used to leverage text similarity or semantic relation methods into Text Spotting systems, either as a post-processing or in an end-to-end training strategy. Our data is publicly available in https://git.io/JeZTb.\r"
  },
  "cvpr2020_w34_readrecursiveautoencodersfordocumentlayoutgeneration": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "READ: Recursive Autoencoders for Document Layout Generation",
    "authors": [
      "Akshay Gadi Patil",
      "Omri Ben-Eliezer",
      "Or Perel",
      "Hadar Averbuch-Elor"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Patil_READ_Recursive_Autoencoders_for_Document_Layout_Generation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Patil_READ_Recursive_Autoencoders_for_Document_Layout_Generation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.\r"
  },
  "cvpr2020_w34_onrecognizingtextsofarbitraryshapeswith2dself-attention": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "On Recognizing Texts of Arbitrary Shapes With 2D Self-Attention",
    "authors": [
      "Junyeop Lee",
      "Sungrae Park",
      "Jeonghun Baek",
      "Seong Joon Oh",
      "Seonghyeon Kim",
      "Hwalsuk Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Lee_On_Recognizing_Texts_of_Arbitrary_Shapes_With_2D_Self-Attention_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Lee_On_Recognizing_Texts_of_Arbitrary_Shapes_With_2D_Self-Attention_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Scene text recognition (STR) is the task of recognizing character sequences in natural scenes. While there have been great advances in STR methods, current methods which convert two-dimensional (2D) image to one-dimensional (1D) feature map still fail to recognize texts in arbitrary shapes, such as heavily curved, rotated or vertically aligned texts, which are abundant in daily life (e.g. restaurant signs, product labels, company logos, etc). This paper introduces an architecture to recognizing texts of arbitrary shapes, named Self-Attention Text Recognition Network (SATRN). SATRN utilizes the self-attention mechanism, which is originally proposed to capture the dependency between word tokens in a sentence, to describe 2D spatial dependencies of characters in a scene text image. Exploiting the full-graph propagation of self-attention, SATRN can recognize texts with arbitrary arrangements and large inter-character spacing. As a result, our model outperforms all existing STR models by a large margin of 4.5 pp on average in \"irregular text\" benchmarks and also achieved state-of-the-art performance in two \"regular text\" benchmarks. We provide empirical analyses that illustrate the inner mechanisms and the extent to which the model is applicable (e.g. rotated and multi-line text). We will open-source the code.\r"
  },
  "cvpr2020_w34_alargedatasetofhistoricaljapanesedocumentswithcomplexlayouts": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "A Large Dataset of Historical Japanese Documents With Complex Layouts",
    "authors": [
      "Zejiang Shen",
      "Kaixuan Zhang",
      "Melissa Dell"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Shen_A_Large_Dataset_of_Historical_Japanese_Documents_With_Complex_Layouts_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Shen_A_Large_Dataset_of_Historical_Japanese_Documents_With_Complex_Layouts_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning-based approaches for automatic document layout analysis and content extraction have the potential to unlock rich information trapped in historical documents on a large scale. One major hurdle is the lack of large datasets for training robust models. In particular, little training data exist for Asian languages. To this end, we present HJDataset, a Large Dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts. A semi-rule based method is developed to extract the layout elements, and the results are checked by human inspectors. The resulting large-scale dataset is used to provide baseline performance analyses for text region detection using state-of-the-art deep learning models. And we demonstrate the usefulness of the dataset on real-world document digitization tasks. The dataset is available at https://dell-research-harvard.github.io/HJDataset/.\r"
  },
  "cvpr2020_w34_anaccuratesegmentation-basedscenetextdetectorwithcontextattentionandrepulsivetextborder": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "An Accurate Segmentation-Based Scene Text Detector With Context Attention and Repulsive Text Border",
    "authors": [
      "Xi Liu",
      "Gaojing Zhou",
      "Rui Zhang",
      "Xiaolin Wei"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Liu_An_Accurate_Segmentation-Based_Scene_Text_Detector_With_Context_Attention_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Liu_An_Accurate_Segmentation-Based_Scene_Text_Detector_With_Context_Attention_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Scene text detection is one of the most challenging problems in computer vision and has attr!acted great interest. In general, scene text detection methods are divided into two categories: detection-based and segmentation-based methods. Recently, the segmentation-based methods are more and more popular due to their superior performances and the advantages of detecting arbitrary-shape texts. However, there still exist the following problems: (a) the misclassification of the unexpected texts, (b) the split of long text lines, (c) the failure of separating very close text instances. In this paper, we propose an accurate segmentation-based detector, which is equipped with context attention and repulsive text border. The context attention incorporates global channel attention, non-local self-attention and spatial attention to better exploit the global and local context, which can greatly increase the discriminative ability for pixels. Due to the enhancement of pixel-level features, false positives and the misdetections of long texts are reduced. Besides, for the purpose of solving very close text instance, a repulsive pixel link, which focuses on the relationships between pixels at the border, is proposed. Experiments on several standard benchmarks, including MSRA-TD500, ICDAR2015, ICDAR2017-MLT and CTW1500, validate the superiority of the proposed method.\r"
  },
  "cvpr2020_w34_illegibletexttoreadabletextanimage-to-imagetransformationusingconditionalslicedwassersteinadversarialnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Illegible Text to Readable Text: An Image-to-Image Transformation Using Conditional Sliced Wasserstein Adversarial Networks",
    "authors": [
      "Mostafa Karimi",
      "Gopalkrishna Veni",
      "Yen-Yun Yu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Karimi_Illegible_Text_to_Readable_Text_An_Image-to-Image_Transformation_Using_Conditional_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Karimi_Illegible_Text_to_Readable_Text_An_Image-to-Image_Transformation_Using_Conditional_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Automatic text recognition from ancient handwritten record images is an important problem in the genealogy domain. However, critical challenges such as varying noise conditions, vanishing texts, and variations in handwriting makes the recognition task difficult. We tackle this problem by developing a handwritten-to-machine-print conditional Generative Adversarial network (HW2MP-GAN) model that formulates handwritten recognition as a text-Image-to-text-Image translation problem where a given image, typically in an illegible form, is converted into another image, close to its machine-print form. The proposed model consists of three-components including a generator, and word-level and character-level discriminators. The model incorporates Sliced Wasserstein distance (SWD) and U-Net architectures in HW2MP-GAN for better quality image-to-image transformation. Our experiments reveal that HW2MP-GAN outperforms state-of-the-art baseline cGAN models by almost 30 in Frechet Handwritten Distance (FHD), 0.6 in average Levenshtein distance and 39% in word accuracy for image-to-image translation on IAM database. Further, HW2MP-GAN improves handwritten recognition word accuracy by 1.3% compared to baseline handwritten recognition models on IAM database.\r"
  },
  "cvpr2020_w34_opticalbraillerecognitionbasedonsemanticsegmentationnetworkwithauxiliarylearningstrategy": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Optical Braille Recognition Based on Semantic Segmentation Network With Auxiliary Learning Strategy",
    "authors": [
      "Renqiang Li",
      "Hong Liu",
      "Xiangdong Wang",
      "Jianxing Xu",
      "Yueliang Qian"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Li_Optical_Braille_Recognition_Based_on_Semantic_Segmentation_Network_With_Auxiliary_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Li_Optical_Braille_Recognition_Based_on_Semantic_Segmentation_Network_With_Auxiliary_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Optical Braille Recognition methods usually use many designed steps, such as image de-skewing, Braille dots detection, Braille cell grids construction and Braille character recognition, which are less robust for complex Braille scenes. This paper proposes an optimal semantic segmentation framework BraUNet to directly detect and recognize Braille characters in the whole original Braille images. BraUNet adds extra auxiliary learning strategy to UNet network, which uses long-range connections of feature maps between encoder and decoder to get more low-level features. And auxiliary learning strategy can combine multi-class Braille characters segmentation with Braille foreground extraction, which can improve the feature learning ability and the Braille segmentation performance. Then morphological post-processing is used on semantic segmentation results to get the final individual Braille character regions. Experimental results show the proposed framework is robust, effective and fast for Braille characters segmentation and recognition on both complex double sided Braille image dataset and handwritten Braille image dataset.\r"
  },
  "cvpr2020_w34_font-protonetprototypicalnetwork-basedfontidentificationofdocumentimagesinlowdataregime": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Font-ProtoNet: Prototypical Network-Based Font Identification of Document Images in Low Data Regime",
    "authors": [
      "Nikita Goel",
      "Monika Sharma",
      "Lovekesh Vig"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Goel_Font-ProtoNet_Prototypical_Network-Based_Font_Identification_of_Document_Images_in_Low_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Goel_Font-ProtoNet_Prototypical_Network-Based_Font_Identification_of_Document_Images_in_Low_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While optical character recognition has attracted considerable interest from researchers in recent times, automating font identification in printed / scanned documents is still not a well explored problem. With the increasing variety of fonts in the open community, identifying the different fonts used in a given document image can often provide important visual cues for document understanding. Font identification is a challenging task owing to smaller inter-class variations and limited availability of labeled image data for a large variety of font images. In the absence of the original true type format (ttf) files, even synthetic data generation is not possible. To this end, we propose to utilize recent few-shot learning techniques like prototypical networks for font identification in scanned / printed document images using character images from different fonts as input for scarce data scenarios and call the proposed method Font-ProtoNet. This approach uses an initial set of classes to learn an embedding and centroid representations (as class prototypes), that are used to classify novel samples based on euclidean distance. We demonstrate that Font-ProtoNet gives encouraging results by training prototypical networks in few-shot learning settings on a synthetic dataset of 200 font classes and using the trained network to identify fonts on a synthetic dataset of 100 novel font classes. We have also tested our approach on the real-world Adobe Visual Font Recognition (AdobeVFR) dataset and obtained 59.86% and 71.01% word-level accuracy of font identification using 1-shot and 5-shot i.e.,1 and 5 character images per font class, respectively.\r"
  },
  "cvpr2020_w34_informationextractionfromdocumentimagesviafca-basedtemplatedetectionandknowledgegraphruleinduction": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Information Extraction From Document Images via FCA-Based Template Detection and Knowledge Graph Rule Induction",
    "authors": [
      "Mouli Rastogi",
      "Syed Afshan Ali",
      "Mrinal Rawat",
      "Lovekesh Vig",
      "Puneet Agarwal",
      "Gautam Shroff",
      "Ashwin Srinivasan"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Rastogi_Information_Extraction_From_Document_Images_via_FCA-Based_Template_Detection_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Rastogi_Information_Extraction_From_Document_Images_via_FCA-Based_Template_Detection_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We view information extraction from document images as a complex problem that requires a combination of 1) state of the art deep learning vision models for detection of entities and primitive relations, 2) symbolic background knowledge that expresses prior information of spatial and semantic relationships, using the entities and primitive relations from the neural detectors, and 3) learning of symbolic extraction rules using one, or few examples of annotated document images. Several challenges arise in ensuring that this neuro-symbolic software stack works together seamlessly. These include vision-based challenges to ensure that the documents are \"seen\" at the appropriate level of detail to detect entities; symbolic representation challenges in identifying primitive relations between the entities identified by the vision system; learning-based challenges of identifying the appropriate level of symbolic abstraction for the retrieval rules, the need to identify background knowledge that is relevant to the documents being analyzed, and learning general symbolic rules in data-deficient domains. In this paper, we describe how we meet some of these challenges in the design of our document-reading platform. In particular we focus on use cases with multiple templates which additionally involves finding structurally similar images in large heterogeneous document image collections. An adaptive lattice based template allocation module was utilized for evaluating document similarity based on both textual content and document structure. A knowledge graph is used for capturing document structure and a relational rule learning system is employed on the knowledge graph for generating extraction rules. Experiments on a publicly shared data-set of 1400 trade finance documents demonstrates the viability of the proposed system.\r"
  },
  "cvpr2020_w34_anocrforclassicalindicdocumentscontainingarbitrarilylongwords": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "An OCR for Classical Indic Documents Containing Arbitrarily Long Words",
    "authors": [
      "Agam Dwivedi",
      "Rohit Saluja",
      "Ravi Kiran Sarvadevabhatla"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Dwivedi_An_OCR_for_Classical_Indic_Documents_Containing_Arbitrarily_Long_Words_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Dwivedi_An_OCR_for_Classical_Indic_Documents_Containing_Arbitrarily_Long_Words_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " OCR for printed classical Indic documents written in Sanskrit is a challenging research problem. It involves complexities such as image degradation, lack of datasets and long-length words. Due to these challenges, the word accuracy of available OCR systems, both academic and industrial, is not very high for such documents. To address these shortcomings, we develop a Sanskrit specific OCR system. We present an attention-based LSTM model for reading Sanskrit characters in line images. We introduce a dataset of Sanskrit document images annotated at line level. To augment real data and enable high performance for our OCR, we also generate synthetic data via curated font selection and rendering designed to incorporate crucial glyph substitution rules. Consequently, our OCR achieves a word error rate of 15.97% and a character error rate of 3.71% on challenging Indic document texts and outperforms strong baselines. Overall, our contributions set the stage for application of OCRs on large corpora of classic Sanskrit texts containing arbitrarily long and highly conjoined words.\r"
  },
  "cvpr2020_w34_visualandtextualdeepfeaturefusionfordocumentimageclassification": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Visual and Textual Deep Feature Fusion for Document Image Classification",
    "authors": [
      "Souhail Bakkali",
      "Zuheng Ming",
      "Mickael Coustaty",
      "Marcal Rusinol"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Bakkali_Visual_and_Textual_Deep_Feature_Fusion_for_Document_Image_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Bakkali_Visual_and_Textual_Deep_Feature_Fusion_for_Document_Image_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The topic of text document image classification has been explored extensively over the past few years. Most recent approaches handled this task by jointly learning the visual features of document images and their corresponding textual contents. Due to the various structures of document images, the extraction of semantic information from its textual content is beneficial for document image processing tasks such as document retrieval, information extraction, and text classification. In this work, a two-stream neural architecture is proposed to perform the document image classification task. We conduct an exhaustive investigation of nowadays widely used neural networks as well as word embedding procedures used as backbones, in order to extract both visual and textual features from document images. Moreover, a joint feature learning approach that combines image features and text embeddings is introduced as a late fusion methodology. Both the theoretical analysis and the experimental results demonstrate the superiority of our proposed joint feature learning method comparatively to the single modalities. This joint learning approach outperforms the state-of-the-art results with a classification accuracy of 97.05% on the large-scale RVL-CDIP dataset.\r"
  },
  "cvpr2020_w34_clevalcharacter-levelevaluationfortextdetectionandrecognitiontasks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "CLEval: Character-Level Evaluation for Text Detection and Recognition Tasks",
    "authors": [
      "Youngmin Baek",
      "Daehyun Nam",
      "Sungrae Park",
      "Junyeop Lee",
      "Seung Shin",
      "Jeonghun Baek",
      "Chae Young Lee",
      "Hwalsuk Lee"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Baek_CLEval_Character-Level_Evaluation_for_Text_Detection_and_Recognition_Tasks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Baek_CLEval_Character-Level_Evaluation_for_Text_Detection_and_Recognition_Tasks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Despite the recent success of text detection and recognition methods, existing evaluation metrics fail to provide a fair and reliable comparison among those methods. In addition, there exists no end-to-end evaluation metric that takes characteristics of OCR tasks into account. Previous end-to-end metric contains cascaded errors from the binary scoring process applied in both detection and recognition tasks. Ignoring partially correct results raises a gap between quantitative and qualitative analysis, and prevents fine-grained assessment. Based on the fact that character is a key element of text, we hereby propose a Character-Level Evaluation metric (CLEval). In CLEval, the instance matching process handles split and merge detection cases, and the scoring process conducts character-level evaluation. By aggregating character-level scores, the CLEval metric provides a fine-grained evaluation of end-to-end results composed of the detection and recognition as well as individual evaluations for each module from the end-performance perspective. We believe that our metrics can play a key role in developing and analyzing state-of-the-art text detection and recognition methods. The evaluation code is publicly available at https://github.com/clovaai/CLEval.\r"
  },
  "cvpr2020_w34_recognizinghandwrittenmathematicalexpressionsviapairedduallossattentionnetworkandprintedmathematicalexpressions": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Recognizing Handwritten Mathematical Expressions via Paired Dual Loss Attention Network and Printed Mathematical Expressions",
    "authors": [
      "Anh Duc Le"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Le_Recognizing_Handwritten_Mathematical_Expressions_via_Paired_Dual_Loss_Attention_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Le_Recognizing_Handwritten_Mathematical_Expressions_via_Paired_Dual_Loss_Attention_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging problem because of the complicated structure and uncommon math symbols contained in HMEs. Moreover, the lack of training data is a serious issue, especially for deep learning-based systems. In this paper, we proposed a dual loss attention model that utilizes the existing latex corpus to improve accuracy. The proposed dual loss attention has two losses, including decoder loss and context matching loss to learn semantic invariant features for the encoder and latex grammar for the decoder from handwritten and printed MEs. The results of experiments on the CROHME 2014 and 2016 databases demonstrate the superiority and effectiveness of our proposed model. These results are competitive compared to others reported in recent literature.\r"
  },
  "cvpr2020_w34_symbolspottingondigitalarchitecturalfloorplansusingadeeplearning-basedframework": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Symbol Spotting on Digital Architectural Floor Plans Using a Deep Learning-Based Framework",
    "authors": [
      "Alireza Rezvanifar",
      "Melissa Cote",
      "Alexandra Branzan Albu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Rezvanifar_Symbol_Spotting_on_Digital_Architectural_Floor_Plans_Using_a_Deep_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Rezvanifar_Symbol_Spotting_on_Digital_Architectural_Floor_Plans_Using_a_Deep_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This papers focuses on symbol spotting on real-world digital architectural floor plans with a deep learning (DL)-based framework. Traditional on-the-fly symbol spotting methods are unable to address the semantic challenge of graphical notation variability, i.e. low intra-class symbol similarity, an issue that is particularly important in architectural floor plan analysis. The presence of occlusion and clutter, characteristic of real-world plans, along with a varying graphical symbol complexity from almost trivial to highly complex, also pose challenges to existing spotting methods. In this paper, we address all of the above issues by leveraging recent advances in DL and adapting an object detection framework based on the You-Only-Look-Once (YOLO) architecture. We propose a training strategy based on tiles, avoiding many issues particular to DL-based object detection networks related to the relative small size of symbols compared to entire floor plans, aspect ratios, and data augmentation. Experiments on real-world floor plans demonstrate that our method successfully detects architectural symbols with low intra-class similarity and of variable graphical complexity, even in the presence of heavy occlusion and clutter. Additional experiments on the public SESYD dataset confirm that our proposed approach can deal with various degradation and noise levels and outperforms other symbol spotting methods.\r"
  },
  "cvpr2020_w34_visualparsingwithquery-drivenglobalgraphattention(qd-gga)preliminaryresultsforhandwrittenmathformularecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "Visual Parsing With Query-Driven Global Graph Attention (QD-GGA): Preliminary Results for Handwritten Math Formula Recognition",
    "authors": [
      "Mahshad Mahdavi",
      "Richard Zanibbi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Mahdavi_Visual_Parsing_With_Query-Driven_Global_Graph_Attention_QD-GGA_Preliminary_Results_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Mahdavi_Visual_Parsing_With_Query-Driven_Global_Graph_Attention_QD-GGA_Preliminary_Results_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We present a new visual parsing method based on convolutional neural networks for handwritten mathematical formulas. The Query-Driven Global Graph Attention (QDGGA) parsing model employs multi-task learning, and uses a single feature representation for locating, classifying, and relating symbols. First, a Line-Of-Sight (LOS) graph is computed over the handwritten strokes in a formula. Second, class distributions for LOS nodes and edges are obtained using query-specific feature filters (i.e., attention) in a single feed-forward pass. Finally, a Maximum Spanning Tree (MST) is extracted from the weighted graph. Our preliminary results show that this is a promising new approach for visual parsing of handwritten formulas. Our data and source code are publicly available.\r"
  },
  "cvpr2020_w34_cascadetabnetanapproachforendtoendtabledetectionandstructurerecognitionfromimage-baseddocuments": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w34",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Text and Documents in the Deep Learning Era",
    "title": "CascadeTabNet: An Approach for End to End Table Detection and Structure Recognition From Image-Based Documents",
    "authors": [
      "Devashish Prasad",
      "Ayan Gadpal",
      "Kshitij Kapadni",
      "Manish Visave",
      "Kavita Sultanpure"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w34/Prasad_CascadeTabNet_An_Approach_for_End_to_End_Table_Detection_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w34/Prasad_CascadeTabNet_An_Approach_for_End_to_End_Table_Detection_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " An automatic table recognition method for interpretation of tabular data in document images majorly involves solving two problems of table detection and table structure recognition. The prior work involved solving both problems independently using two separate approaches. More recent works signify the use of deep learning-based solutions while also attempting to design an end to end solution. In this paper, we present an improved deep learning-based end to end approach for solving both problems of table detection and structure recognition using a single Convolution Neural Network (CNN) model. We propose CascadeTabNet: a Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet) based model that detects the regions of tables and recognizes the structural body cells from the detected tables at the same time. We evaluate our results on ICDAR 2013, ICDAR 2019 and TableBank public datasets. We achieved 3rd rank in ICDAR 2019 post-competition results for table detection while attaining the best accuracy results for the ICDAR 2013 and TableBank dataset. We also attain the highest accuracy results on the ICDAR 2019 table structure recognition dataset. Additionally, we demonstrate effective transfer learning and image augmentation techniques that enable CNNs to achieve very accurate table detection results. Code and dataset has been made available at: https://github.com/DevashishPrasad/CascadeTabNet\r"
  },
  "cvpr2020_w48_onimprovingthegeneralizationoffacerecognitioninthepresenceofocclusions": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "On Improving the Generalization of Face Recognition in the Presence of Occlusions",
    "authors": [
      "Xiang Xu",
      "Nikolaos Sarafianos",
      "Ioannis A. Kakadiaris"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Xu_On_Improving_the_Generalization_of_Face_Recognition_in_the_Presence_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Xu_On_Improving_the_Generalization_of_Face_Recognition_in_the_Presence_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we address a key limitation of existing 2D face recognition methods: robustness to occlusions caused by visual attributes. To accomplish this task, we systematically analyze the impact of facial attributes on the performance of a state-of-the-art face recognition method and through extensive experimentation, quantitatively analyze the performance degradation under different types of occlusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach improves the generalization ability of the facial embedding generator by learning discriminative embeddings despite the presence of such occlusions. The contributions of our occlusion-aware approach are two-fold. First, an attention mechanism is proposed that extracts local identity-related features from the global feature representations. The local features are then aggregated with the global representations to form a single facial embedding. Second, a simple, yet effective, training strategy is introduced to balance the non-occluded and occluded facial images. Extensive experiments with comparisons to strong baselines demonstrate that OREO improves the generalization ability of face recognition under occlusions by 10.17% in a single-image-based setting and outperforms the baseline by approximately 2% in terms of rank-1 accuracy in an image-set-based scenario.\r"
  },
  "cvpr2020_w48_latentfingerprintimageenhancementbasedonprogressivegenerativeadversarialnetwork": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Latent Fingerprint Image Enhancement Based on Progressive Generative Adversarial Network",
    "authors": [
      "Xijie Huang",
      "Peng Qian",
      "Manhua Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Huang_Latent_Fingerprint_Image_Enhancement_Based_on_Progressive_Generative_Adversarial_Network_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Huang_Latent_Fingerprint_Image_Enhancement_Based_on_Progressive_Generative_Adversarial_Network_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Latent fingerprints, a kind of fingerprints which are captured from the finger skin impressions at the crime scene, have been adopted to identify suspected criminals for a long time. However, poor latent fingerprint image quality owing to unstructured overlapping patterns, unclear ridge structure, and various background noise has brought a challenge to the recognition of latent fingerprints. Therefore, image enhancement is a crucial step for more accurate fingerprint recognition. In this paper, a latent fingerprint enhancement method based on the progressive generative adversarial network (GAN) is proposed. The powerful GAN structure provides an efficient translation from latent fingerprint to high-quality fingerprint. Our method consists of two stages: Progressive Offline Training (POT) and Iterative Online Testing (IOT). Progressive training makes our model not only focus on the local features such as minutiae but also preserve structure feature such as the orientation field. We extensively evaluate our model on NIST SD27 latent fingerprint dataset. With the help of orientation estimation task and progressive training scheme, our model achieves better recognition accuracy.\r"
  },
  "cvpr2020_w48_domainagnosticfeaturelearningforimageandvideobasedfaceanti-spoofing": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Domain Agnostic Feature Learning for Image and Video Based Face Anti-Spoofing",
    "authors": [
      "Suman Saha",
      "Wenhao Xu",
      "Menelaos Kanakis",
      "Stamatios Georgoulis",
      "Yuhua Chen",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Saha_Domain_Agnostic_Feature_Learning_for_Image_and_Video_Based_Face_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Saha_Domain_Agnostic_Feature_Learning_for_Image_and_Video_Based_Face_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Nowadays, the increasingly growing number of mobile and computing devices has led to a demand for safer user authentication systems. Face anti-spoofing is a measure towards this direction for biometric user authentication, and in particular face recognition, that tries to prevent spoof attacks. The state-of-the-art anti-spoofing techniques leverage the ability of deep neural networks to learn discriminative features, based on cues from the training set images or video samples, in an effort to detect spoof attacks. However, due to the particular nature of the problem, i.e. large variability due to factors like different backgrounds, lighting conditions, camera resolutions, spoof materials, etc., these techniques typically fail to generalize to new samples. In this paper, we explicitly tackle this problem and propose a class-conditional domain discriminator module, that, coupled with a gradient reversal layer, tries to generate live and spoof features that are discriminative, but at the same time robust against the aforementioned variability factors. Extensive experimental analysis shows the effectiveness of the proposed method over existing image- and video-based anti-spoofing techniques, both in terms of numerical improvement as well as when visualizing the learned features.\r"
  },
  "cvpr2020_w48_triple-ganprogressivefaceagingwithtripletranslationloss": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Triple-GAN: Progressive Face Aging With Triple Translation Loss",
    "authors": [
      "Han Fang",
      "Weihong Deng",
      "Yaoyao Zhong",
      "Jiani Hu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Fang_Triple-GAN_Progressive_Face_Aging_With_Triple_Translation_Loss_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Fang_Triple-GAN_Progressive_Face_Aging_With_Triple_Translation_Loss_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Face aging is a challenging task which aims at rendering face for input with aging effects and preserving identity information. However, existing methods have split the long term into several independent groups and ignore the correlations of age growth. To better learn the progressive translation of age patterns, we propose a novel Triple Generative Adversarial Networks (Triple-GAN) to simulate face aging. Instead of formulating ages as independent groups, Triple-GAN adopts triple translation loss to model the strong interrelationship of age patterns among different age groups. And to further learn the target aging effect, multiple training pairs are offered to learn the convincing mappings between labels and patterns. The quantitative and qualitative experimental results on CACD, MORPH and CALFW show the superiority of Triple-GAN in identity preservation and age classification.\r"
  },
  "cvpr2020_w48_plasticsurgeryanobstaclefordeepfacerecognition?": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Plastic Surgery: An Obstacle for Deep Face Recognition?",
    "authors": [
      "Christian Rathgeb",
      "Didem Dogan",
      "Fabian Stockhardt",
      "Maria De Marsico",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Rathgeb_Plastic_Surgery_An_Obstacle_for_Deep_Face_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Rathgeb_Plastic_Surgery_An_Obstacle_for_Deep_Face_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The impacts of plastic surgery on face recognition systems have been investigated in the past decade by many researchers. Diverse well-known face recognition approaches, e.g. based on PCA or LBP, have been benchmarked mostly on the web-collected IIITD plastic surgery face database. Generally, significant performance drops were reported when comparing facial images taken before and after plastic surgeries. On the one side, some researchers reported problems with said plastic surgery database, i.e. the presence of low quality images. On the other side, the applied methods no longer reflect the state-of-the-art in face recognition. This calls for evaluating the impact of plastic surgery on state-of-the-art deep face recognition systems anew considering high quality imagery of most relevant plastic surgeries. This work introduces the new Hochschule Darmstadt (HDA) plastic surgery database of facial images taken before and after surgery. This database vastly complies with the quality requirements defined by the International Civil Aviation Organization (ICAO) for electronic travel documents and comprises face images of the five most frequently applied facial plastic surgeries. The HDA plastic surgery database, the IIITD plastic surgery database, and a non-surgery database, i.e. ICAO-compliant subsets of the FRGCv2 and FERET datasets, are used for comparative verification and identification evaluations which are conducted using the commercial Cognitec FaceVACS system and the open-source ArcFace system. The obtained results suggest that the impact of plastic surgery on deep face recognition systems is less significant than that observed for previously benchmarked methods.\r"
  },
  "cvpr2020_w48_offlinesignatureverificationonreal-worlddocuments": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Offline Signature Verification on Real-World Documents",
    "authors": [
      "Deniz Engin",
      "Alperen Kantarci",
      "Secil Arslan",
      "Hazim Kemel Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Engin_Offline_Signature_Verification_on_Real-World_Documents_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Engin_Offline_Signature_Verification_on_Real-World_Documents_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Research on offline signature verification has explored a large variety of methods on multiple signature datasets, which are collected under controlled conditions. However, these datasets may not fully reflect the characteristics of the signatures in some practical use cases. Real-world signatures extracted from the formal documents may contain different types of occlusions, for example, stamps, company seals, ruling lines, and signature boxes. Moreover, they may have very high intra-class variations, where even genuine signatures resemble forgeries. In this paper, we address a real-world writer independent offline signature verification problem, in which, a bank's customers' transaction request documents that contain their occluded signatures are compared with their clean reference signatures. Our proposed method consists of two main components, a stamp cleaning method based on CycleGAN and signature representation based on CNNs. We extensively evaluate different verification setups, fine-tuning strategies, and signature representation approaches to have a thorough analysis of the problem. Moreover, we conduct a human evaluation to show the challenging nature of the problem. We run experiments both on our custom dataset, as well as on the publicly available Tobacco-800 dataset. The experimental results validate the difficulty of offline signature verification on real-world documents. However, by employing the stamp cleaning process, we improve the signature verification performance significantly.\r"
  },
  "cvpr2020_w48_fehashfullentropyhashforfacetemplateprotection": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "FEHash: Full Entropy Hash for Face Template Protection",
    "authors": [
      "Thao M. Dang",
      "Lam Tran",
      "Thuc D. Nguyen",
      "Deokjai Choi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Dang_FEHash_Full_Entropy_Hash_for_Face_Template_Protection_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Dang_FEHash_Full_Entropy_Hash_for_Face_Template_Protection_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we present a hashing function for the application of face template protection, which improves the correctness of existing algorithms while maintaining the security simultaneously. The novel architecture constructed based on four components: a self-defined concept called padding people, Random Fourier Features, Support Vector Machine, and Locality Sensitive Hashing. The proposed method is trained, with one-shot and multi-shot enrollment, to encode the user's biometric data to a predefined output with high probability. The predefined hashing output is cryptographically hashed and stored as a secure face template. Predesigning outputs ensures the strict requirements of biometric cryptosystems, namely, randomness and unlinkability. We prove that our method reaches the REQ-WBP (Weak Biometric Privacy) security level, which implies irreversibility. The efficacy of our approach is evaluated on the widely used CMU-PIE, FEI, and FERET databases; our matching performances achieve 100% genuine acceptance rate at 0% false acceptance rate for all three databases and enrollment types. To our knowledge, our matching results outperform most of state-of-the-art results.\r"
  },
  "cvpr2020_w48_defendingblackboxfacialrecognitionclassifiersagainstadversarialattacks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Defending Black Box Facial Recognition Classifiers Against Adversarial Attacks",
    "authors": [
      "Rajkumar Theagarajan",
      "Bir Bhanu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Theagarajan_Defending_Black_Box_Facial_Recognition_Classifiers_Against_Adversarial_Attacks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Theagarajan_Defending_Black_Box_Facial_Recognition_Classifiers_Against_Adversarial_Attacks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Defending adversarial attacks is a critical step towards reliable deployment of deep learning empowered solutions for biometrics verification. Current approaches for defending Black box models use the classification accuracy of the Black box as a performance metric for validating their defense. However, classification accuracy by itself is not a reliable metric to determine if the resulting image is \"adversarial-free\". This is a serious problem for online biometrics verification applications where the ground-truth of the incoming image is not known and hence we cannot compute the accuracy of the classifier or know if the image is \"adversarial-free\" or not. This paper proposes a novel framework for defending Black box systems from adversarial attacks using an ensemble of iterative adversarial image purifiers whose performance is continuously validated in a loop using Bayesian uncertainties. The proposed approach is (i) model agnostic, (ii) can convert single step black box defenses into an iterative defense and (iii) has the ability to reject adversarial examples. This paper uses facial recognition as a test case for validating the defense and experimental results on the MS-Celeb dataset show that the proposed approach can consistently detect adversarial examples and purify/reject them against a variety of adversarial attacks with different ranges of perturbations.\r"
  },
  "cvpr2020_w48_adversariallightprojectionattacksonfacerecognitionsystemsafeasibilitystudy": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study",
    "authors": [
      "Dinh-Luan Nguyen",
      "Sunpreet S. Arora",
      "Yuhang Wu",
      "Hao Yang"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Nguyen_Adversarial_Light_Projection_Attacks_on_Face_Recognition_Systems_A_Feasibility_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Nguyen_Adversarial_Light_Projection_Attacks_on_Face_Recognition_Systems_A_Feasibility_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Deep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using a one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary's face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.\r"
  },
  "cvpr2020_w48_seamlesspaymentsystemusingfaceandlow-energybluetooth": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Seamless Payment System Using Face and Low-Energy Bluetooth",
    "authors": [
      "Yeongnam Chae",
      "Kelvin Cheng",
      "Pankaj Wasnik",
      "Bjorn Stenger"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Chae_Seamless_Payment_System_Using_Face_and_Low-Energy_Bluetooth_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Chae_Seamless_Payment_System_Using_Face_and_Low-Energy_Bluetooth_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " This paper introduces a multi-modal authentication approach for payments using a face image and the low energy Bluetooth (BLE) signal of a user's device. Devices of registered users transmit temporally changing one-time identifiers. During a payment request, the query face image is only matched with database entries corresponding to nearby users, thereby significantly reducing the complexity of the task. For cases in which a user does not carry their device, the system includes a fallback mechanism to PIN-based two-factor authentication. A classifier on depth data input is used to reduce vulnerability to presentation attacks. We conducted a user study of different payment methods and demonstrated our system at a public event with 951 users.\r"
  },
  "cvpr2020_w48_seeingredppgbiometricsusingsmartphonecameras": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Seeing Red: PPG Biometrics Using Smartphone Cameras",
    "authors": [
      "Giulio Lovisotto",
      "Henry Turner",
      "Simon Eberz",
      "Ivan Martinovic"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Lovisotto_Seeing_Red_PPG_Biometrics_Using_Smartphone_Cameras_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Lovisotto_Seeing_Red_PPG_Biometrics_Using_Smartphone_Cameras_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " In this paper, we propose a system that enables photoplethysmogram (PPG)-based authentication by using a smartphone camera. PPG signals are obtained by recording a video from the camera as users are resting their finger on top of the camera lens. The signals can be extracted based on subtle changes in the video that are due to changes in the light reflection properties of the skin as the blood flows through the finger. We collect a dataset of PPG measurements from a set of 15 users over the course of 6-11 sessions per user using an iPhone X for the measurements. We design an authentication pipeline that leverages the uniqueness of each individual's cardiovascular system, identifying a set of distinctive features from each heartbeat. We conduct a set of experiments to evaluate the recognition performance of the PPG biometric trait, including cross-session scenarios which have been disregarded in previous work. We found that when aggregating sufficient samples for the decision we achieve an EER as low as 8%, but that the performance greatly decreases in the cross-session scenario, with an average EER of 20%.\r"
  },
  "cvpr2020_w48_qualityguidedsketch-to-photoimagesynthesis": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Quality Guided Sketch-to-Photo Image Synthesis",
    "authors": [
      "Uche Osahor",
      "Hadi Kazemi",
      "Ali Dabouei",
      "Nasser Nasrabadi"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Osahor_Quality_Guided_Sketch-to-Photo_Image_Synthesis_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Osahor_Quality_Guided_Sketch-to-Photo_Image_Synthesis_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Facial sketches drawn by artists are widely used for visual identification applications and mostly by law enforcement agencies, but the quality of these sketches depend on the ability of the artist to clearly replicate all the key facial features that could aid in capturing the true identity of a subject. Recent works have attempted to synthesize these sketches into plausible visual images to improve visual recognition and identification. However, synthesizing photo-realistic images from sketches proves to be an even more challenging task, especially for sensitive applications such as suspect identification. In this work, we propose a novel approach that adopts a generative adversarial network that synthesizes a single sketch into multiple synthetic images with unique attributes like hair color, sex, etc. We incorporate a hybrid discriminator which performs attribute classification of multiple target attributes, a quality guided encoder that minimizes the perceptual dissimilarity of the latent space embedding of the synthesized and real image at different layers in the network and an identity preserving network that maintains the identity of the synthesised image throughout the training process. Our approach is aimed at improving the visual appeal of the synthesised images while incorporating multiple attribute assignment to the generator without compromising the identity of the synthesised image. We synthesised sketches using XDOG filter for the CelebA, WVU Multi-modal and CelebA-HQ datasets and from an auxiliary generator trained on sketches from CUHK, IIT-D and FERET datasets. Our results are impressive compared to current state of the art.\r"
  },
  "cvpr2020_w48_ana-contrariobiometricfusionapproach": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "An A-Contrario Biometric Fusion Approach",
    "authors": [
      "Luis Di Martino",
      "Javier Preciozzi",
      "Rafael Grompone von Gioi",
      "Guillermo Garella",
      "Alicia Fernandez",
      "Federico Lecumberry"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Di_Martino_An_A-Contrario_Biometric_Fusion_Approach_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Di_Martino_An_A-Contrario_Biometric_Fusion_Approach_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Fusion is a key component in many biometric systems: it is one of the most widely used techniques to improve their accuracy. Each time we need to combine the output of systems that use different biometric traits, or different samples of the same biometric trait, or even different algorithms, we need to define a fusion strategy. Independently of the fusion method used, there is always a decision step, in which it is decided if the traits being compared correspond to the same individual or not. In this work, we present a statistical decision criterion based on the a-contrario framework, which has already proven to be useful in biometric applications. The proposed method and its theoretical background is described in detail, and its application to biometric fusion is illustrated with simulated and real data.\r"
  },
  "cvpr2020_w48_class-balancedtrainingfordeepfacerecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Class-Balanced Training for Deep Face Recognition",
    "authors": [
      "Yaobin Zhang",
      "Weihong Deng"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Zhang_Class-Balanced_Training_for_Deep_Face_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Zhang_Class-Balanced_Training_for_Deep_Face_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The performance of deep face recognition depends heavily on the training data. Recently, larger and larger datasets have been developed for the training of deep models. However, most face recognition training sets suffer from the class imbalance problem, and most studies ignore the benefit of optimizing dataset structures. In this paper, we study how class-balanced training can promote face recognition performance. A medium-scale face recognition training set BUPT-CBFace is built by exploring the optimal data structure from massive data. This publicly available dataset is characterized by the uniformly distributed sample size per class, as well as the balance between the number of classes and the number of samples in one class. Experimental results show that deep models trained with BUPT-CBFace can not only achieve comparable results to larger-scale datasets such as MS-Celeb-1M but also alleviate the problem of recognition bias.\r"
  },
  "cvpr2020_w48_acomprehensivestudyonlossfunctionsforcross-factorfacerecognition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "A Comprehensive Study on Loss Functions for Cross-Factor Face Recognition",
    "authors": [
      "Gee-Sern Jison Hsu",
      "Hung-Yi Wu",
      "Moi Hoon Yap"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Hsu_A_Comprehensive_Study_on_Loss_Functions_for_Cross-Factor_Face_Recognition_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Hsu_A_Comprehensive_Study_on_Loss_Functions_for_Cross-Factor_Face_Recognition_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " A significant progress has been made to face recognition in recent years. The progress includes the advancement of the deep learning solutions and the availability of more challenging databases. As the performance on previous benchmark databases, such as MPIE and LFW, saturates, more challenging databases are emerging and keep driving the development of face recognition technology. The loss function considered in a deep face recognition network plays a critical role for the performance. To better evaluate the state-of-the-art loss functions, we define four challenging factors, including pose, age, occlusion and resolution with specific databases and conduct an extensive experimental study on the latest loss functions. We select the IARPA Janus Benchmark-B (IJB-B) and IARPA Janus Benchmark-C (IJB-C) for pose, the FG-Net Aging Database (FG-Net) for age, the AR Face Database (AR Face) for occlusion, and the Surveillance Cameras Face Database (SCface) for low resolution. The loss functions include the Center Loss, the Marginal Loss, the SphereFace, the CosFace and the ArcFace. Although for most factors, the ArcFace outperforms others. However, the best performance against low-resolution is achieved by the SphereFace. Another attractive finding of this study is that the cross-age performance is the lowest among the four factors with a clear margin. This highlight possible directions for future research.\r"
  },
  "cvpr2020_w48_foldelectrocardiogramintoafingerprint": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "Fold Electrocardiogram Into a Fingerprint",
    "authors": [
      "Po-Ya Hsu",
      "Po-Han Hsu",
      "Hsin-Li Liu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Hsu_Fold_Electrocardiogram_Into_a_Fingerprint_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Hsu_Fold_Electrocardiogram_Into_a_Fingerprint_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Electrocardiogram (ECG) has become a popular biometric to study since it is highly secured against spoofing attack. In this study, we address the issues of hard-required ECG data and neglected causality in performing ECG identity matching tasks. First, we propose an ECG image generation algorithm that is able to handle any specified number of ECG heartbeats. Such an algorithm uses detected R-peaks as folding points and projects ECG data onto a two-dimensional image, which overcomes the challenge of hardly-required fixed length and truncated ECG. Second, we perform across-session testing. We construct the ECG identification models by using the past ECG data and evaluate their performance on future ECG data. Furthermore, we develop a voting strategy that is able to detect anomaly ECG heartbeats. Our novel ECG image generation approach shows to be a competitive ECG biometric model by leveraging transfer learning method. Such method has been evaluated on MIT-DB and ECG-ID datasets. We observe satisfiable results of the proposed models in both datasets: 100% on the MIT-DB and 94.4% on ECG-ID. More importantly, our method is available to generate satisfying results by using a single ECG beat to conduct identity matching task: 100% on the MIT-DB and 91.7% on ECG-ID. In addition, qualitative analysis presents the perceptual uniqueness of ECG between individuals. We believe that the proposed ECG biometric system is promising to identify humans with short ECG sequence.\r"
  },
  "cvpr2020_w48_whenpersonre-identificationmeetschangingclothes": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w48",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Biometrics",
    "title": "When Person Re-Identification Meets Changing Clothes",
    "authors": [
      "Fangbin Wan",
      "Yang Wu",
      "Xuelin Qian",
      "Yixiong Chen",
      "Yanwei Fu"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w48/Wan_When_Person_Re-Identification_Meets_Changing_Clothes_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w48/Wan_When_Person_Re-Identification_Meets_Changing_Clothes_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Person re-identification (ReID) is now an active research topic for AI-based video surveillance applications such as specific person search, but the practical issue that the target person(s) may change clothes (clothes inconsistency problem) has been overlooked for long. For the first time, this paper systematically studies this problem. We first overcome the difficulty of lack of suitable dataset, by collecting a small yet representative real dataset for testing whilst building a large realistic synthetic dataset for training and deeper studies. Facilitated by our new datasets, we are able to conduct various interesting new experiments for studying the influence of clothes inconsistency. We find that changing clothes makes ReID a much harder problem in the sense of bringing difficulties to learning effective representations and also challenges the generalization ability of previous ReID models to identify persons with unseen (new) clothes. Representative existing ReID models are adopted to show informative results on such a challenging setting, and we also provide some preliminary efforts on improving the robustness of existing models on handling the clothes inconsistency issue in the data. We believe that this study can be inspiring and helpful for encouraging more researches in this direction. The dataset is available on the project website: https://wanfb.github.io/dataset.html.\r"
  },
  "cvpr2020_w57_analyzingu-netrobustnessforsinglecellnucleussegmentationfromphasecontrastimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Analyzing U-Net Robustness for Single Cell Nucleus Segmentation From Phase Contrast Images",
    "authors": [
      "Chenyi Ling",
      "Michael Majurski",
      "Michael Halter",
      "Jeffrey Stinson",
      "Anne Plant",
      "Joe Chalfoun"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Ling_Analyzing_U-Net_Robustness_for_Single_Cell_Nucleus_Segmentation_From_Phase_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Ling_Analyzing_U-Net_Robustness_for_Single_Cell_Nucleus_Segmentation_From_Phase_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " We quantify the robustness of the semantic segmentation model U-Net, applied to single cell nuclei detection, with respect to the following factors: (1) automated vs manual training annotations, (2) quantity of training data, and (3) microscope image focus. The difficulty of obtaining sufficient volumes of accurate manually annotated training data to create an accurate Convolutional Neural Networks (CNN) model is overcome by the temporary use of fluorescent labels to automate the creation of training datasets using traditional image processing algorithms. The accuracy measurement is computed with respect to manually annotated masks which were also created to evaluate the effectiveness of using automated training set generation via the fluorescent images. The metric to compute the accuracy is the false positive/negative rate of cell nuclei detection. The goal is to maximize the true positive rate while minimizing the false positive rate. We found that automated segmentation of fluorescently labeled nuclei provides viable training data without the need for manual segmentation. A training dataset size of four large stitched images with medium cell density was enough to reach a true positive rate above 88 % and a false positive rate below 20%.\r"
  },
  "cvpr2020_w57_celeganserautomatedanalysisofnematodemorphologyandage": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Celeganser: Automated Analysis of Nematode Morphology and Age",
    "authors": [
      "Linfeng Wang",
      "Shu Kong",
      "Zachary Pincus",
      "Charless Fowlkes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Wang_Celeganser_Automated_Analysis_of_Nematode_Morphology_and_Age_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Wang_Celeganser_Automated_Analysis_of_Nematode_Morphology_and_Age_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The nematode Caenorhabditis elegans (C. elegans) serves as an important model organism in a wide variety of biological studies. In this paper we introduce a pipeline for automated analysis of C. elegans imagery for the purpose of studying life-span, health-span and the underlying genetic determinants of aging. Our system detects and segments the worm, and predicts body coordinates at each pixel location inside the worm. These coordinates provides dense correspondence across individual animals to allow for meaningful comparative analysis. We show that a model pre-trained to perform body-coordinate regression extracts rich features that can be used to predict the age of individual worms with high accuracy. This lays the ground for future research in quantifying the relation between organs' physiologic and biochemical state, and individual life/health-span.\r"
  },
  "cvpr2020_w57_estimationoforientationandcameraparametersfromcryo-electronmicroscopyimageswithvariationalautoencodersandgenerativeadversarialnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Estimation of Orientation and Camera Parameters From Cryo-Electron Microscopy Images With Variational Autoencoders and Generative Adversarial Networks",
    "authors": [
      "Nina Miolane",
      "Frederic Poitevin",
      "Yee-Ting Li",
      "Susan Holmes"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Miolane_Estimation_of_Orientation_and_Camera_Parameters_From_Cryo-Electron_Microscopy_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Miolane_Estimation_of_Orientation_and_Camera_Parameters_From_Cryo-Electron_Microscopy_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Cryo-electron microscopy (cryo-EM) is capable of producing reconstructed 3D images of biomolecules at near-atomic resolution. However, raw cryo-EM images are only highly corrupted - noisy and band-pass filtered - 2D projections of the target 3D biomolecules. Reconstructing the 3D molecular shape requires the estimation of the orientation of the biomolecule that has produced the given 2D image, and the estimation of camera parameters to correct for intensity defects. Current techniques performing these tasks are often computationally expensive, while the dataset sizes keep growing. There is a need for next-generation algorithms that preserve accuracy while improving speed and scalability. In this paper, we combine variational autoencoders (VAEs) and generative adversarial networks (GANs) to learn a low-dimensional latent representation of cryo-EM images. This analysis leads us to design an estimation method for orientation and camera parameters of single-particle cryo-EM images, which opens the door to faster cryo-EM biomolecule reconstruction.\r"
  },
  "cvpr2020_w57_wishefficient3dbiologicalshapeclassificationthroughwillmoreflowandsphericalharmonicsdecomposition": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "WISH: Efficient 3D Biological Shape Classification Through Willmore Flow and Spherical Harmonics Decomposition",
    "authors": [
      "Marco Agus",
      "Enrico Gobbetti",
      "Giovanni Pintore",
      "Corrado Cali",
      "Jens Schneider"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Agus_WISH_Efficient_3D_Biological_Shape_Classification_Through_Willmore_Flow_and_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Agus_WISH_Efficient_3D_Biological_Shape_Classification_Through_Willmore_Flow_and_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Shape analysis of cell nuclei, enabled by the recent advances in nano-scale digital imaging and reconstruction methods, is emerging as a very important tool to understand low-level biological processes. Current analysis techniques, however, are performed on 2D slices or assume very simple 3D shape approximations, limiting their discrimination capabilities. In this work, we introduce a compact rotation-invariant frequency-based representation of genus-0 3D shapes represented by manifold triangle meshes, that we apply to cell nuclei envelopes reconstructed from electron micrographs. The representation is robustly obtained through Spherical Harmonics coefficients over a spherical parameterization of the input mesh obtained through Willmore flow. Our results show how our method significantly improves the state-of-the-art in the classification of nuclear envelopes of rodent brain samples. Moreover, while our method is motivated by the analysis of specific biological shapes, the framework is of general use for the compact frequency encoding of any genus-0 surface.\r"
  },
  "cvpr2020_w57_feedbacku-netforcellimagesegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Feedback U-Net for Cell Image Segmentation",
    "authors": [
      "Eisuke Shibuya",
      "Kazuhiro Hotta"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Shibuya_Feedback_U-Net_for_Cell_Image_Segmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Shibuya_Feedback_U-Net_for_Cell_Image_Segmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Human brain is a layered structure, and performs not only a feedforward process from a lower layer to an upper layer but also a feedback process from an upper layer to a lower layer. This layer is a collection of neurons, and neural network is a mathematical model of the function of neurons. Although neural network imitates the human brain, everyone uses only feedforward process from the lower layer to the upper layer, and feedback process from the upper layer to the lower layer is not used. Therefore, in this paper we propose Feedback U-Net using Convolutional LSTM, which is segmentation method using Convolutional LSTM and feedback process. The output of U-net gave feedback to the input. By using Convolutional LSTM, the feature of the second lap by feedback is extracted based on the feature acquired in the first lap. On both of the Drosophila cell image and Mouse cell image datasets, our method outperformed conventional U-Net which uses only feedforward process.\r"
  },
  "cvpr2020_w57_multi-objectgraph-basedsegmentationwithnon-overlappingsurfaces": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Multi-Object Graph-Based Segmentation With Non-Overlapping Surfaces",
    "authors": [
      "Patrick M. Jensen",
      "Anders B. Dahl",
      "Vedrana A. Dahl"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Jensen_Multi-Object_Graph-Based_Segmentation_With_Non-Overlapping_Surfaces_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Jensen_Multi-Object_Graph-Based_Segmentation_With_Non-Overlapping_Surfaces_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " For 3D images, segmentation via fitting surface meshes to object boundaries provides an efficient way to handle large images and enforce geometric prior knowledge. Furthermore, fitting such meshes with graph cuts has proven to be a versatile and robust framework. However, when segmenting multiple distinct objects in one image, current methods do not allow the natural constraint that objects should not overlap. In this paper, we present an extension to graph cut based methods which can provide a globally optimal segmentation of thousands of objects while guaranteeing no overlap. Our method works by separating objects with planes whose positions are determined as part of the graph cut. To demonstrate the general applicability of our method, we apply it to several 3D microscopy data sets from both biology and materials science. Our results show both quantitative and qualitative improvements.\r"
  },
  "cvpr2020_w57_self-supervisedfeatureextractionfor3daxonsegmentation": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Self-Supervised Feature Extraction for 3D Axon Segmentation",
    "authors": [
      "Tzofi Klinghoffer",
      "Peter Morales",
      "Young-Gyun Park",
      "Nicholas Evans",
      "Kwanghun Chung",
      "Laura J. Brattain"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Klinghoffer_Self-Supervised_Feature_Extraction_for_3D_Axon_Segmentation_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Klinghoffer_Self-Supervised_Feature_Extraction_for_3D_Axon_Segmentation_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Existing learning-based methods to automatically trace axons in 3D brain imagery often rely on manually annotated segmentation labels. Labeling is a labor-intensive process and is not scalable to whole-brain analysis, which is needed for improved understanding of brain function. We propose a self-supervised auxiliary task that utilizes the tube-like structure of axons to build a feature extractor from unlabeled data. The proposed auxiliary task constrains a 3D convolutional neural network (CNN) to predict the order of permuted slices in an input 3D volume. By solving this task, the 3D CNN is able to learn features without ground-truth labels that are useful for downstream segmentation with the 3D U-Net model. To the best of our knowledge, our model is the first to perform automated segmentation of axons imaged at subcellular resolution with the SHIELD technique. We demonstrate improved segmentation performance over the 3D U-Net model on both the SHIELD PVGPe dataset and the BigNeuron Project, single neuron Janelia dataset.\r"
  },
  "cvpr2020_w57_detectionandclassificationofpollengrainmicroscopeimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Detection and Classification of Pollen Grain Microscope Images",
    "authors": [
      "Sebastiano Battiato",
      "Alessandro Ortis",
      "Francesca Trenta",
      "Lorenzo Ascari",
      "Mara Politi",
      "Consolata Siniscalco"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Battiato_Detection_and_Classification_of_Pollen_Grain_Microscope_Images_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Battiato_Detection_and_Classification_of_Pollen_Grain_Microscope_Images_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " With the spread of technology in several fields, there is an increasing demand to automate specialized tasks that usually require human involvement in order to maximize efficiency and reduce processing time. Pollen identification and classification is a proper example to be treated in the Palynology field, which has been an expensive qualitative process, involving observation and discrimination of features by highly qualified experts. Although it is the most accurate and useful method, it is a time-consuming process that slowed down the research progress. In this paper, we present a dataset composed of more than 13.000 objects, identified by an appropriate segmentation pipeline applied on aerobiological samples. Besides, we present the results obtained from the classification of these objects by taking advantage of several Machine Learning techniques, discussing which approaches have produced the most satisfactory results, and outlining the challenges we had to face to accomplish the task.\r"
  },
  "cvpr2020_w57_ctmccelltrackingwithmitosisdetectiondatasetchallenge": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "CTMC: Cell Tracking With Mitosis Detection Dataset Challenge",
    "authors": [
      "Samreen Anjum",
      "Danna Gurari"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Anjum_CTMC_Cell_Tracking_With_Mitosis_Detection_Dataset_Challenge_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Anjum_CTMC_Cell_Tracking_With_Mitosis_Detection_Dataset_Challenge_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While significant developments have been made in cell tracking algorithms, current datasets are still limited in size and diversity, especially for data-hungry generalized deep learning models. We introduce a new larger and more diverse cell tracking dataset in terms of number of sequences, length of sequences, and cell lines, accompanied with a public evaluation server and leaderboard to accelerate progress on this new challenging dataset. Our benchmarking of four top performing tracking algorithms highlights new challenges and opportunities to improve the state-of-the-art in cell tracking.\r"
  },
  "cvpr2020_w57_aweb-basedintelligenceplatformfordiagnosisofmalariainthickbloodsmearimagesacaseforadevelopingcountry": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "A Web-Based Intelligence Platform for Diagnosis of Malaria in Thick Blood Smear Images: A Case for a Developing Country",
    "authors": [
      "Rose Nakasi",
      "Jeremy Francis Tusubira",
      "Aminah Zawedde",
      "Ali Mansourian",
      "Ernest Mwebaze"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Nakasi_A_Web-Based_Intelligence_Platform_for_Diagnosis_of_Malaria_in_Thick_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Nakasi_A_Web-Based_Intelligence_Platform_for_Diagnosis_of_Malaria_in_Thick_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Malaria is a public health problem which affects developing countries world-wide. Inadequate skilled lab technicians in remote areas of developing countries result in untimely diagnosis of malaria parasites making it hard for effective control of the disease in highly endemic areas. The development of remote systems that can provide fast, accurate and timely diagnosis is thus a necessary innovation. With availability of internet, mobile phones and computers, rapid dissemination and timely reporting of medical image analytics is possible. This study aimed at developing and implementing an automated web-based Malaria diagnostic system for thick blood smear images under light microscopy to identify parasites. We implement an image processing algorithm based on a pre-trained model of Faster Convolutional Neural Network (Faster R-CNN) and then integrate it with web-based technology to allow easy and convenient online identification of parasites by medical practitioners. Experiments carried out on the online system with test images showed that the system could identify pathogens with a mean average precision of 0.9306. The system holds the potential to improve the efficiency and accuracy in malaria diagnosis, especially in remote areas of developing countries that lack adequate skilled labor.\r"
  },
  "cvpr2020_w57_atopologicalnomenclaturefor3dshapeanalysisinconnectomics": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "A Topological Nomenclature for 3D Shape Analysis in Connectomics",
    "authors": [
      "Abhimanyu Talwar",
      "Zudi Lin",
      "Donglai Wei",
      "Yuesong Wu",
      "Bowen Zheng",
      "Jinglin Zhao",
      "Won-Dong Jang",
      "Xueying Wang",
      "Jeff Lichtman",
      "Hanspeter Pfister"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Talwar_A_Topological_Nomenclature_for_3D_Shape_Analysis_in_Connectomics_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Talwar_A_Topological_Nomenclature_for_3D_Shape_Analysis_in_Connectomics_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " One of the essential tasks in connectomics is the morphology analysis of neurons and organelles like mitochondria to shed light on their biological properties. However, these biological objects often have tangled parts or complex branching patterns, which make it hard to abstract, categorize, and manipulate their morphology. In this paper, we develop a novel topological nomenclature system to name these objects like the appellation for chemical compounds to promote neuroscience analysis based on their skeletal structures. We first convert the volumetric representation into the topology-preserving reduced graph to untangle the objects. Next, we develop nomenclature rules for pyramidal neurons and mitochondria from the reduced graph and finally learn the feature embedding for shape manipulation. In ablation studies, we quantitatively show that graphs generated by our proposed method align with the perception of experts. On 3D shape retrieval and decomposition tasks, we qualitatively demonstrate that the encoded topological nomenclature features achieve better results than state-of-the-art shape descriptors. To advance neuroscience, we will release a 3D segmentation dataset of mitochondria and pyramidal neurons reconstructed from a 100um cube electron microscopy volume with their reduced graph and topological nomenclature annotations. Code is publicly available at https://github.com/donglaiw/ibexHelper.\r"
  },
  "cvpr2020_w57_representationlearningofhistopathologyimagesusinggraphneuralnetworks": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Representation Learning of Histopathology Images Using Graph Neural Networks",
    "authors": [
      "Mohammed Adnan",
      "Shivam Kalra",
      "Hamid R. Tizhoosh"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Adnan_Representation_Learning_of_Histopathology_Images_Using_Graph_Neural_Networks_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Adnan_Representation_Learning_of_Histopathology_Images_Using_Graph_Neural_Networks_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Representation learning for Whole Slide Images (WSIs) is pivotal in developing image-based systems to achieve higher precision in diagnostic pathology. We propose a two-stage framework for WSI representation learning. We sample relevant patches using a color-based method and use graph neural networks to learn relations among sampled patches to aggregate the image information into a single vector representation. We introduce attention via graph pooling to automatically infer patches with higher relevance. We demonstrate the performance of our approach for discriminating two sub-types of lung cancers, Lung Adenocarcinoma (LUAD) & Lung Squamous Cell Carcinoma (LUSC). We collected 1,026 lung cancer WSIs with the 40x magnification from The Cancer Genome Atlas (TCGA) dataset, the largest public repository of histopathology images and achieved state-of-the-art accuracy of 88.8 % and AUC of 0.89 on lung cancer sub-type classification by extracting features from a pre-trained DenseNet.\r"
  },
  "cvpr2020_w57_atopologicalencodingconvolutionalneuralnetworkforsegmentationof3dmultiphotonimagesofbrainvasculatureusingpersistenthomology": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "A Topological Encoding Convolutional Neural Network for Segmentation of 3D Multiphoton Images of Brain Vasculature Using Persistent Homology",
    "authors": [
      "Mohammad Haft-Javaherian",
      "Martin Villiger",
      "Chris B. Schaffer",
      "Nozomi Nishimura",
      "Polina Golland",
      "Brett E. Bouma"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Haft-Javaherian_A_Topological_Encoding_Convolutional_Neural_Network_for_Segmentation_of_3D_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Haft-Javaherian_A_Topological_Encoding_Convolutional_Neural_Network_for_Segmentation_of_3D_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " The clinical evidence suggests that cognitive disorders are associated with vasculature dysfunction and decreased blood flow in the brain. Hence, a functional understanding of the linkage between brain functionality and the vascular network is essential. However, methods to systematically and quantitatively describe and compare structures as complex as brain blood vessels are lacking. 3D imaging modalities such as multiphoton microscopy enables researchers to capture the network of brain vasculature with high spatial resolutions. Nonetheless, image processing and inference are some of the bottlenecks for biomedical research involving imaging, and any advancement in this area impacts many research groups. Here, we propose a topological encoding convolutional neural network based on persistent homology to segment 3D multiphoton images of brain vasculature. We demonstrate that our model outperforms state-of-the-art models in terms of the Dice coefficient and it is comparable in terms of other metrics such as sensitivity. Additionally, the topological characteristics of our model's segmentation results mimic manual ground truth. Our code and model are open source at https://github.com/mhaft/DeepVess.\r"
  },
  "cvpr2020_w57_rapidtrainingdatacreationbysynthesizingmedicalimagesforclassificationandlocalization": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Rapid Training Data Creation by Synthesizing Medical Images for Classification and Localization",
    "authors": [
      "Abhishek Kushwaha",
      "Sarthak Gupta",
      "Anish Bhanushali",
      "Tathagato Rai Dastidar"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Kushwaha_Rapid_Training_Data_Creation_by_Synthesizing_Medical_Images_for_Classification_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Kushwaha_Rapid_Training_Data_Creation_by_Synthesizing_Medical_Images_for_Classification_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " While the use of artificial intelligence (AI) for medical image analysis is gaining wide acceptance, the expertise, time and cost required to generate annotated data in the medical field are significantly high, due to limited availability of both data and expert annotation. Strongly supervised object localization models require data that is exhaustively annotated, meaning all objects of interest in an image are identified. This is difficult to achieve and verify for medical images. We present a method for the transformation of real data to train any Deep Neural Network to solve the above problems. We show the efficacy of this approach on both a weakly supervised localization model and a strongly supervised localization model. For the weakly supervised model, we show that the localization accuracy increases significantly using the generated data. For the strongly supervised model, this approach overcomes the need for exhaustive annotation on real images. In the latter model, we show that the accuracy, when trained with generated images, closely parallels the accuracy when trained with exhaustively annotated real images. The results are demonstrated on images of human urine samples obtained using microscopy.\r"
  },
  "cvpr2020_w57_content-basedpropagationofusermarkingsforinteractivesegmentationofpatternedimages": {
    "conf_id": "CVPR2020",
    "conf_sub_id": "w57",
    "is_workshop": true,
    "conf_name": "CVPR2020_workshops - Computer Vision for Microscopy Image Analysis",
    "title": "Content-Based Propagation of User Markings for Interactive Segmentation of Patterned Images",
    "authors": [
      "Vedrana A. Dahl",
      "Monica J. Emerson",
      "Camilla H. Trinderup",
      "Anders B. Dahl"
    ],
    "page_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/html/w57/Dahl_Content-Based_Propagation_of_User_Markings_for_Interactive_Segmentation_of_Patterned_CVPRW_2020_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/CVPR2020_workshops/../content_CVPRW_2020/papers/w57/Dahl_Content-Based_Propagation_of_User_Markings_for_Interactive_Segmentation_of_Patterned_CVPRW_2020_paper.pdf",
    "published": "2020-06",
    "summary": " Efficient and easy segmentation of images and volumes is of great practical importance. Segmentation problems that motivate our approach originate from microscopy imaging commonly used in materials science, medicine, and biology. We formulate image segmentation as a probabilistic pixel classification problem, and we apply segmentation as a step towards characterising image content. Our method allows the user to define structures of interest by interactively marking a subset of pixels. Thanks to the real-time feedback, the user can place new markings strategically, depending on the current outcome. The final pixel classification may be obtained from a very modest user input. An important ingredient of our method is a graph that encodes image content. This graph is built in an unsupervised manner during initialisation and is based on clustering of image features. Since we combine a limited amount of user-labelled data with the clustering information obtained from the unlabelled parts of the image, our method fits in the general framework of semi-supervised learning. We demonstrate how this can be a very efficient approach to segmentation through pixel classification.\r"
  }
}