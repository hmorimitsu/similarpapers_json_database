{
  "wacv2022_vaq_diliedeepinternallearningforimageenhancement": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "DILIE: Deep Internal Learning for Image Enhancement",
    "authors": [
      "Indra Deep Mastan",
      "Shanmuganathan Raman",
      "Prajwal Singh"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Mastan_DILIE_Deep_Internal_Learning_for_Image_Enhancement_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Mastan_DILIE_Deep_Internal_Learning_for_Image_Enhancement_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "We consider the generic deep image enhancement problem where an input image is transformed into a perceptually better-looking image. The methods mostly fall into two categories: training with prior examples methods and training with no-prior examples methods. Recently, Deep Internal Learning solutions to image enhancement in training with no-prior examples setup are gaining attention. We perform image enhancement using a deep internal learning framework. Our Deep Internal Learning for Image Enhancement framework (DILIE) enhances content features and style features and preserves semantics in the enhanced image. To validate the results, we use structure similarity and perceptual error, which is efficient in measuring the unrealistic deformation present in the images. We show that DILIE framework outputs good quality images for hazy and noisy image enhancement tasks.",
    "code_link": "https://github.com/clovaai/WCT2"
  },
  "wacv2022_vaq_improvededvrmodelforrobustandefficientvideosuper-resolution": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Improved EDVR Model for Robust and Efficient Video Super-Resolution",
    "authors": [
      "Yulin Huang",
      "Junying Chen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Huang_Improved_EDVR_Model_for_Robust_and_Efficient_Video_Super-Resolution_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Huang_Improved_EDVR_Model_for_Robust_and_Efficient_Video_Super-Resolution_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Computer vision technologies are increasingly commonly used in daily life, and video super-resolution is gradually drawing more attention in the computer vision community. In this work, we propose an improved EDVR model to tackle the robustness and efficiency problems of the original EDVR model in video super-resolution. First, to handle the blurring situations and emphasize the effective features, we devise a preprocessing module consisting of rigid convolution sub-modules and feature enhancement sub-modules, which are flexible and effective. Second, we devise a temporal 3D convolutional fusion module, which can extract information in image frames more accurately and rapidly. Third, to better utilize the information in feature maps, we design a new reconstruction block by introducing a new channel attention approach. Moreover, we use multiple programmatic methods to accelerate the model training and inference process, making the model useful for practical applications.",
    "code_link": ""
  },
  "wacv2022_vaq_utilizingnetworkfeaturestodetecterroneousinputs": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Utilizing Network Features To Detect Erroneous Inputs",
    "authors": [
      "Matt Gorbett",
      "Nathaniel Blanchard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Gorbett_Utilizing_Network_Features_To_Detect_Erroneous_Inputs_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Gorbett_Utilizing_Network_Features_To_Detect_Erroneous_Inputs_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Neural networks are vulnerable to a wide range of erroneous inputs such as corrupted, out-of-distribution, misclassified, and adversarial examples. Previously, separate solutions have been proposed for each of these faulty data types, however, in this work we show that a collective set of inputs with variegated data quality issues can be jointly identified with a single model. Specifically, we train a linear SVM classifier to detect four types of erroneous data using the hidden and softmax feature vectors of pre-trained neural networks. Our results indicate that these faulty data types generally exhibit linearly separable activation properties from correctly processed examples. We are able to identify erroneous inputs with an AUROC of 0.973 on CIFAR10, 0.957 on Tiny ImageNet, and 0.941 on ImageNet. We experimentally validate our findings across a diverse range of datasets, domains, and pre-trained models.",
    "code_link": ""
  },
  "wacv2022_vaq_taskadaptivenetworkforimagerestorationwithcombineddegradationfactors": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Task Adaptive Network for Image Restoration With Combined Degradation Factors",
    "authors": [
      "Jingyuan Zhou",
      "Chaktou Leong",
      "Minyi Lin",
      "Wantong Liao",
      "Congduan Li"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Zhou_Task_Adaptive_Network_for_Image_Restoration_With_Combined_Degradation_Factors_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Zhou_Task_Adaptive_Network_for_Image_Restoration_With_Combined_Degradation_Factors_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Existing methods have achieved excellent performance on image restoration, but most of them are designed for one type of degradation. However, the weather is complex in the real world. So networks designed for single tasks are usually difficult to apply. Therefore, we propose a task-adaptive attention module to enable the network to restore images with multiple degradation factors. The task-adaptive attention module mainly includes three parts: Task-Adaptive sub-network, Task Channel Attention, and Task Operation Attention. To evaluate the model, we construct a mixed degradation factors dataset that combines three degradation factors of rain, haze, and raindrop. The experimental results show that our method not only better restores images with mixed degradation factors, but also show competitive results compared to the state-of-the-art models of each task.",
    "code_link": ""
  },
  "wacv2022_vaq_imagequalityassessmentusingsyntheticimages": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Image Quality Assessment Using Synthetic Images",
    "authors": [
      "Pavan C. Madhusudana",
      "Neil Birkbeck",
      "Yilin Wang",
      "Balu Adsumilli",
      "Alan C. Bovik"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Madhusudana_Image_Quality_Assessment_Using_Synthetic_Images_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Madhusudana_Image_Quality_Assessment_Using_Synthetic_Images_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Training deep models using contrastive learning has achieved impressive performances on various computer vision tasks. Since training is done in a self-supervised manner on unlabeled data, contrastive learning is an attractive candidate for applications for which large labeled datasets are hard/expensive to obtain. In this work we investigate the outcomes of using contrastive learning on synthetically generated images for the Image Quality Assessment (IQA) problem. The training data consists of computer generated images corrupted with predetermined distortion types. Predicting distortion type and degree is used as an auxiliary task to learn image quality features. The learned representations are then used to predict quality in a No-Reference (NR) setting on real-world images. We show through extensive experiments that this model achieves comparable performance to state-of-the-art NR image quality models when evaluated on real images afflicted with synthetic distortions, even without using any real images during training. Our results indicate that training with synthetically generated images can also lead to effective, and perceptually relevant representations.",
    "code_link": "https://github.com/pavancm/CONTRIQUE_syn"
  },
  "wacv2022_vaq_faceqvecvectorqualityassessmentforfacebiometricsbasedonisocompliance": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "FaceQvec: Vector Quality Assessment for Face Biometrics Based on ISO Compliance",
    "authors": [
      "Javier Hernandez-Ortega",
      "Julian Fierrez",
      "Luis F. Gomez",
      "Aythami Morales",
      "Jose Luis Gonzalez-de-Suso",
      "Francisco Zamora-Martinez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Hernandez-Ortega_FaceQvec_Vector_Quality_Assessment_for_Face_Biometrics_Based_on_ISO_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Hernandez-Ortega_FaceQvec_Vector_Quality_Assessment_for_Face_Biometrics_Based_on_ISO_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper we develop FaceQvec, a software component for estimating the conformity of facial images with each of the points contemplated in the ISO/IEC 19794-5, a quality standard that defines general quality guidelines for face images that would make them acceptable or unacceptable for use in official documents such as passports or ID cards. This type of tool for quality assessment can help to improve the accuracy of face recognition, as well as to identify which factors are affecting the quality of a given face image and to take actions to eliminate or reduce those factors, e.g., with postprocessing techniques or re-acquisition of the image. FaceQvec consists of the automation of 25 individual tests related to different points contemplated in the aforementioned standard, as well as other characteristics of the images that have been considered to be related to facial quality. We first include the results of the quality tests evaluated on a development dataset captured under realistic conditions. We used those results to adjust the decision threshold of each test. Then we checked again their accuracy on a evaluation database that contains new face images not seen during development. The evaluation results demonstrate the accuracy of the individual tests for checking compliance with ISO/IEC 19794-5. FaceQvec is available online.",
    "code_link": ""
  },
  "wacv2022_vaq_sapnetsegmentation-awareprogressivenetworkforperceptualcontrastivederaining": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining",
    "authors": [
      "Shen Zheng",
      "Changjie Lu",
      "Yuxiong Wu",
      "Gaurav Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Zheng_SAPNet_Segmentation-Aware_Progressive_Network_for_Perceptual_Contrastive_Deraining_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Zheng_SAPNet_Segmentation-Aware_Progressive_Network_for_Perceptual_Contrastive_Deraining_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Deep learning algorithms have recently achieved promising deraining performances on both the natural and synthetic rainy datasets. As an essential low-level pre-processing stage, a deraining network should clear the rain streaks and preserve the fine semantic details. However, most existing methods only consider low-level image restoration. That limits their performances at high-level tasks requiring precise semantic information. To address this issue, in this paper, we present a segmentation-aware progressive network (SAPNet) based upon contrastive learning for single image deraining. We start our method with a lightweight derain network formed with progressive dilated units (PDU). The PDU can significantly expand the receptive field and characterize multi-scale rain streaks without the heavy computation on multi-scale images. A fundamental aspect of this work is an unsupervised background segmentation (UBS) network initialized with ImageNet and Gaussian weights. The UBS can faithfully preserve an image's semantic information and improve the generalization ability to unseen photos. Furthermore, we introduce a perceptual contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL) to regulate model learning. By exploiting the rainy image and groundtruth as the negative and the positive sample in the VGG-16 latent space, we bridge the fine semantic details between the derained image and the groundtruth in a fully constrained manner. Comprehensive experiments on synthetic and real-world rainy images show our model surpasses top-performing methods and aids object detection and semantic segmentation with considerable efficacy. A Pytorch Implementation is available at https://github.com/ShenZheng2000/SAPNet-for-image-deraining.",
    "code_link": "https://github.com/ShenZheng2000/SAPNetfor-image-deraining"
  },
  "wacv2022_vaq_uncertaintyquantificationusingvariationalinferenceforbiomedicalimagesegmentation": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Uncertainty Quantification Using Variational Inference for Biomedical Image Segmentation",
    "authors": [
      "Abhinav Sagar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Sagar_Uncertainty_Quantification_Using_Variational_Inference_for_Biomedical_Image_Segmentation_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Sagar_Uncertainty_Quantification_Using_Variational_Inference_for_Biomedical_Image_Segmentation_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Deep learning motivated by convolutional neural networks has been highly successful in a range of medical imaging problems like image classification, image segmentation, image synthesis etc. However for validation and interpretability, not only do we need the predictions made by the model but also how confident it is while making those predictions. This is important in safety critical applications for the people to accept it. In this work, we used an encoder decoder architecture based on variational inference techniques for segmenting brain tumour images. We evaluate our work on the publicly available BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over Union (IOU) as the evaluation metrics. Our model is able to segment brain tumours while taking into account both aleatoric uncertainty and epistemic uncertainty in a principled bayesian manner.",
    "code_link": ""
  },
  "wacv2022_vaq_rainganunsupervisedraindropremovalviadecompositionandcomposition": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "RainGAN: Unsupervised Raindrop Removal via Decomposition and Composition",
    "authors": [
      "Xu Yan",
      "Yuan Ren Loke"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Yan_RainGAN_Unsupervised_Raindrop_Removal_via_Decomposition_and_Composition_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Yan_RainGAN_Unsupervised_Raindrop_Removal_via_Decomposition_and_Composition_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Adherent raindrops on windshield or camera lens may distort and occlude vision, causing issues for downstream machine vision perception. Most of the existing raindrop removal methods focus on learning the mapping from a raindrop image to its clean content by the paired raindrop-clean images. However, the paired real-world data is difficult to collect in practice. This paper presents a novel framework for raindrop removal that eliminates the need for paired training samples. Based on the assumption that a raindrop image is a composition of a clean image and raindrop style, the proposed framework decomposes a raindrop image into a clean content image and a raindrop-style latent code. Inversely, it composes a clean content image and a raindrop style code to a raindrop image for data augmentation. The proposed framework introduces a domain-invariant residual block to facilitate the identity mapping for the clean portion of the raindrop image. Extensive experiments on real-world raindrop datasets show that our network can achieve superior performance in raindrop removal to other unpaired image-to-image translation methods, even with comparable performance with state-of-the-art methods that require paired data.",
    "code_link": ""
  },
  "wacv2022_vaq_depthcompletionauto-encoder": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Depth Completion Auto-Encoder",
    "authors": [
      "Kaiyue Lu",
      "Nick Barnes",
      "Saeed Anwar",
      "Liang Zheng"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Lu_Depth_Completion_Auto-Encoder_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Lu_Depth_Completion_Auto-Encoder_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "This paper proposes a new usage of integrating RGB image features for unsupervised depth completion. Instead of resorting to the image as input like existing works, we propose to employ the image to guide the learning process. Specifically, we regard dense depth as a reconstructed result of the sparse input, and formulate our model as an auto-encoder. To reduce structure inconsistency resulting from sparse depth, we employ the image to guide latent features by penalizing their difference in the training process. The image guidance loss enables our model to acquire more dense and structural cues that are beneficial to producing more accurate and consistent depth values. For inference, our model only takes sparse depth as input and no image is required. Our paradigm is new and pushes unsupervised depth completion further than existing works that require the image at test time. On the KITTI Depth Completion Benchmark, we validate its effectiveness through extensive experiments and achieve promising performance compared with other unsupervised works. The proposed method is also applicable to indoor scenes such as NYUv2.",
    "code_link": ""
  },
  "wacv2022_vaq_ano-referencemodelfordetectingaudioartifactsusingpretrainedaudioneuralnetworks": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "A No-Reference Model for Detecting Audio Artifacts Using Pretrained Audio Neural Networks",
    "authors": [
      "David Higham",
      "Ayush Bagla",
      "Veneta Haralampieva"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Higham_A_No-Reference_Model_for_Detecting_Audio_Artifacts_Using_Pretrained_Audio_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Higham_A_No-Reference_Model_for_Detecting_Audio_Artifacts_Using_Pretrained_Audio_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "This work presents a No-Reference model to detect audio artifacts in video. The model, based upon a Pretrained Audio Neural Network, classifies a 1 second audio segment as either: No Defect, Audio Hum, Audio Hiss, Audio Distortion or Audio Clicks. The model achieves a balanced accuracy of 0.986 on our proprietary simulated dataset.",
    "code_link": ""
  },
  "wacv2022_vaq_subjectivequalityassessmentofuser-generatedcontentgamingvideos": {
    "conf_id": "WACV2022",
    "conf_sub_id": "VAQ",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Video/Audio Quality in Computer Vision",
    "title": "Subjective Quality Assessment of User-Generated Content Gaming Videos",
    "authors": [
      "Xiangxu Yu",
      "Zhengzhong Tu",
      "Zhenqiang Ying",
      "Alan C. Bovik",
      "Neil Birkbeck",
      "Yilin Wang",
      "Balu Adsumilli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/html/Yu_Subjective_Quality_Assessment_of_User-Generated_Content_Gaming_Videos_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Yu_Subjective_Quality_Assessment_of_User-Generated_Content_Gaming_Videos_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Benefited from the rapid development of the digital game industry, the growing popularity of online user-generated content (UGC) videos for games has accelerated the development of perceptual video quality assessment (VQA) models specifically for gaming videos. As a novel UGC type, gaming videos are recorded by gamers and uploaded to major streaming media platforms such as YouTube and Twitch, and have been extremely popular among the audience. However, there is little work on VQA research related to gaming videos and understanding their characteristics. In order to promote the development of the gaming VQA model, we created a new UGC gaming video VQA resource, named LIVE-YouTube Gaming video quality (LIVE-YT-Gaming) database, composed of 600 authentic UGC gaming videos and 18,600 subjective quality ratings collected from an online subjective study. We also compared and analyzed several state-of-the-art (SOTA) VQA models on the new database. To support work in this field, the new database will be publicly available through the link: https://live.ece.utexas.edu/research/LIVE-YT-Gaming/index.html.",
    "code_link": ""
  },
  "wacv2022_hadcv_modellingambiguousassignmentsformulti-persontrackingincrowds": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "Modelling Ambiguous Assignments for Multi-Person Tracking in Crowds",
    "authors": [
      "Daniel Stadler",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Stadler_Modelling_Ambiguous_Assignments_for_Multi-Person_Tracking_in_Crowds_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Stadler_Modelling_Ambiguous_Assignments_for_Multi-Person_Tracking_in_Crowds_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Multi-person tracking is often solved with a tracking-by-detection approach that matches all tracks and detections simultaneously based on a distance matrix. In crowded scenes, ambiguous situations with similar track-detection distances occur, which leads to wrong assignments. To mitigate this problem, we propose a new association method that separately treats such difficult situations by modelling ambiguous assignments based on the differences in the distance matrix. Depending on the number of tracks and detections, for which the assignment task is determined ambiguous, different strategies to resolve these ambiguous situations are proposed. To further enhance the performance of our tracking framework, we introduce a camera motion-aware interpolation technique and make an adaptation to the motion model, which improves identity preservation. The effectiveness of our approach is demonstrated through extensive ablative experiments with different detection models. Moreover, the superiority w.r.t. other trackers is shown on the challenging MOT17 and MOT20 datasets, where state-of-the-art results are obtained.",
    "code_link": ""
  },
  "wacv2022_hadcv_win-failactionrecognition": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "Win-Fail Action Recognition",
    "authors": [
      "Paritosh Parmar",
      "Brendan Morris"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Parmar_Win-Fail_Action_Recognition_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Parmar_Win-Fail_Action_Recognition_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Current video/action understanding systems have demonstrated impressive performance on large recognition tasks. However, they might be limiting themselves to learning to recognize spatiotemporal patterns, rather than attempting to thoroughly understand the actions. To spur progress in the direction of a more comprehensive understanding of videos, we introduce the task of win-fail action recognition--differentiating between successful and failed attempts at various activities. We introduce a first of its kind paired win-fail action understanding dataset with samples from the following domains: \"General Stunts\", \"Internet Wins-Fails\", \"Trick Shots\", & \"Party Games\". Unlike existing action recognition datasets, intra-class variation is high making the task challenging, yet feasible. Using a battery of experiments, including a novel video retrieval test, we systematically analyze the characteristics of our win-fail task/dataset, and determine its suitability to serve as a video understanding problem benchmark. While current prototypical action recognition methods work well on our task/dataset, they still leave a large gap to achieve high performance. We hope to motivate more work towards the true understanding of actions/videos. Dataset will be available from: https://github.com/ParitoshParmar/Win-Fail-Action-Recognition.",
    "code_link": ""
  },
  "wacv2022_hadcv_gabriellav2towardsbettergeneralizationinsurveillancevideosforactiondetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "GabriellaV2: Towards Better Generalization in Surveillance Videos for Action Detection",
    "authors": [
      "Ishan Dave",
      "Zacchaeus Scheffer",
      "Akash Kumar",
      "Sarah Shiraz",
      "Yogesh Singh Rawat",
      "Mubarak Shah"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Dave_GabriellaV2_Towards_Better_Generalization_in_Surveillance_Videos_for_Action_Detection_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Activity detection has wide-reaching applications in video surveillance, sports, and behavior analysis. The existing literature in activity detection has mainly focused on benchmarks like AVA, AVA-Kinetics, UCF101-24, and JHMDB-21. However, these datasets fail to address all issues of real-world surveillance camera videos like untrimmed nature, tiny actor bounding boxes, multi-label nature of the actions, etc. In this work, we propose a real-time, online, action detection system which can generalize robustly on any unknown facility surveillance videos. Our real-time system mainly consists of tracklet generation, tracklet activity classification, and prediction refinement using the proposed post-processing algorithm. We tackle the challenging nature of action classification problem in various aspects like handling the class-imbalance training using PLM method and learning multi-label action correlations using LSEP loss. In order to improve the computational efficiency of the system, we utilize knowledge distillation. Our approach gets state-of-the-art performance on ActEV-SDL UF-full dataset and second place in TRECVID 2021 ActEV challenge. Project Webpage: www.crcv.ucf.edu/research/projects/gabriellav2/",
    "code_link": ""
  },
  "wacv2022_hadcv_argus++robustreal-timeactivitydetectionforunconstrainedvideostreamswithoverlappingcubeproposals": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "Argus++: Robust Real-Time Activity Detection for Unconstrained Video Streams With Overlapping Cube Proposals",
    "authors": [
      "Lijun Yu",
      "Yijun Qian",
      "Wenhe Liu",
      "Alexander G. Hauptmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Yu_Argus_Robust_Real-Time_Activity_Detection_for_Unconstrained_Video_Streams_With_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Yu_Argus_Robust_Real-Time_Activity_Detection_for_Unconstrained_Video_Streams_With_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Activity detection is one of the attractive computer vision tasks to exploit the video streams captured by widely installed cameras. Although achieving impressive performance, conventional activity detection algorithms are usually designed under certain constraints, such as using trimmed and/or object-centered video clips as inputs. Therefore, they failed to deal with the multi-scale multi-instance cases in real-world unconstrained video streams, which are untrimmed and have large field-of-views. Real-time requirements for streaming analysis also mark brute force expansion of them unfeasible. To overcome these issues, we propose Argus++, a robust real-time activity detection system for analyzing unconstrained video streams. The design of Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept of activity proposals to ensure coverage and completeness of activity detection through over-sampling. The overall system is optimized for real-time processing on standalone consumer-level hardware. Extensive experiments on different surveillance and driving scenarios demonstrated its superior performance in a series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021, NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.",
    "code_link": ""
  },
  "wacv2022_hadcv_pp-humansegconnectivity-awareportraitsegmentationwithalarge-scaleteleconferencingvideodataset": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "PP-HumanSeg: Connectivity-Aware Portrait Segmentation With a Large-Scale Teleconferencing Video Dataset",
    "authors": [
      "Lutao Chu",
      "Yi Liu",
      "Zewu Wu",
      "Shiyu Tang",
      "Guowei Chen",
      "Yuying Hao",
      "Juncai Peng",
      "Zhiliang Yu",
      "Zeyu Chen",
      "Baohua Lai",
      "Haoyi Xiong"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Chu_PP-HumanSeg_Connectivity-Aware_Portrait_Segmentation_With_a_Large-Scale_Teleconferencing_Video_Dataset_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Chu_PP-HumanSeg_Connectivity-Aware_Portrait_Segmentation_With_a_Large-Scale_Teleconferencing_Video_Dataset_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "As the COVID-19 pandemic rampages across the world, the demands of video conferencing surge. To this end, real-time portrait segmentation becomes a popular feature to replace backgrounds of conferencing participants. While feature-rich datasets, models and algorithms have been offered for segmentation that extract body postures from life scenes, portrait segmentation has yet not been well covered in a video conferencing context. To facilitate the progress in this field, we introduce an open-source solution named PP-HumanSeg. This work is the first to construct a large-scale video portrait dataset that contains 291 videos from 23 conference scenes with 14K fine-labeled frames and extensions to multi-camera teleconferencing. Furthermore, we propose a novel Self-supervised Connectivity-aware Learning (SCL) for semantic segmentation, which introduces a self-supervised connectivity-aware loss to improve the quality of segmentation results from the perspective of connectivity. And we propose an ultra-lightweight model with SCL for practical portrait segmentation, which achieves the best trade-off between IoU and the speed of inference. Extensive evaluations on our dataset demonstrate the superiority of SCL and our model. The source code is available at https://github.com/PaddlePaddle/PaddleSeg.",
    "code_link": ""
  },
  "wacv2022_hadcv_trmtemporalrelocationmoduleforvideorecognition": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "TRM: Temporal Relocation Module for Video Recognition",
    "authors": [
      "Yijun Qian",
      "Guoliang Kang",
      "Lijun Yu",
      "Wenhe Liu",
      "Alexander G. Hauptmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Qian_TRM_Temporal_Relocation_Module_for_Video_Recognition_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Qian_TRM_Temporal_Relocation_Module_for_Video_Recognition_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "One of the key differences between video and image understanding lies in how to model the temporal information. Due to the limit of convolution kernel size, most previous methods try to model long-term temporal information via sequentially stacked convolution layers. Such conventional manner doesn't explicitly differentiate regions/pixels with various temporal receptive requirements and may suffer from temporal information distortion. In this paper, we propose a novel Temporal Relocation Module (TRM), which can capture the long-term temporal dependence in a spatial-aware manner adaptively. Specifically, it relocates the spatial features along the temporal dimension, through which an adaptive temporal receptive field is aligned to each pixel spatial-wisely. As the relocation is performed within the global temporal interval of input video, TRM can potentially model the long-term temporal information with an equivalent receptive field of the entire video. Experiment results on three representative video recognition benchmarks demonstrate TRM outperforms previous state-of-the-arts noticeably and verifies the effectiveness of our method.",
    "code_link": ""
  },
  "wacv2022_hadcv_videoactionre-localizationusingspatio-temporalcorrelation": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "Video Action Re-Localization Using Spatio-Temporal Correlation",
    "authors": [
      "Akshaya Ramaswamy",
      "Karthik Seemakurthy",
      "Jayavardhana Gubbi",
      "Balamuralidhar P"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Ramaswamy_Video_Action_Re-Localization_Using_Spatio-Temporal_Correlation_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Ramaswamy_Video_Action_Re-Localization_Using_Spatio-Temporal_Correlation_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Video re-localization plays an important role in locating the moments of interest in a long videos, and is critical for a variety of applications such as surveillance video monitoring and retrieving similar archived videos for further comparison and analysis. Current re-localization approaches compute a feature vector using a video query for each video frame, and explore various feature matching techniques. These features do not capture information from varying temporal windows, and the dimension reduction to a vector leads to loss of spatio-temporal context. For efficient feature comparison and matching among thousands of videos, we design a Siamese Spatio-Temporal network comprising Convolution Neural Network and Long Short-term Memory blocks (CNN-LSTM) for feature extraction, followed by a correlation layer for spatio-temporal feature matching. We extract video features at varying temporal scales, and localize one or more segments in the reference video that semantically match the query clip. Our approach is evaluated on two benchmark datasets: AVAv2.1- Search and ActivityNet-Search. We show an improvement of over 12% in the mean average precision compared to existing approaches. We perform ablation experiments and show that the modular architecture and the holistic feature extraction expands the scope of this work to multiple video search applications.",
    "code_link": ""
  },
  "wacv2022_hadcv_signpose-basedtransformerforword-levelsignlanguagerecognition": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "Sign Pose-Based Transformer for Word-Level Sign Language Recognition",
    "authors": [
      "Maty\u00e1\u0161 Boh\u00e1\u010dek",
      "Marek Hr\u00faz"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Bohacek_Sign_Pose-Based_Transformer_for_Word-Level_Sign_Language_Recognition_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Bohacek_Sign_Pose-Based_Transformer_for_Word-Level_Sign_Language_Recognition_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper we present a system for word-level sign language recognition based on the Transformer model. We aim at a solution with low computational cost, since we see great potential in the usage of such recognition system on handheld devices. We base the recognition on the estimation of the pose of the human body in the form of 2D landmark locations. We introduce a robust pose normalization scheme which takes the signing space in considerationand processes the hand poses in a separate local coordinate system, independent on the body pose. We show experimentally the significant impact of this normalization on the accuracy of our proposed system. We introduce several augmentations of the body pose that further improve the accuracy, including a novel sequential joint rotation augmentation. With all the systems in place, we achieve state of theart top-1 results on the WLASL and LSA64 datasets. For WLASL, we are able to successfully recognize 63.18% of sign recordings in the 100-gloss subset, which is a relative improvement of 5% from the prior state of the art. For the 300-gloss subset, we achieve recognition rate of 43.78% which is a relative improvement of 3.8%. With the LSA64 dataset, we report test recognition accuracy of 100%.",
    "code_link": ""
  },
  "wacv2022_hadcv_actor-centrictubeletsforreal-timeactivitydetectioninextendedvideos": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "Actor-Centric Tubelets for Real-Time Activity Detection in Extended Videos",
    "authors": [
      "Effrosyni Mavroudi",
      "Prashast Bindal",
      "Ren\u00e9 Vidal"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Mavroudi_Actor-Centric_Tubelets_for_Real-Time_Activity_Detection_in_Extended_Videos_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Mavroudi_Actor-Centric_Tubelets_for_Real-Time_Activity_Detection_in_Extended_Videos_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "We address the problem of detecting human and vehicle activities in long, untrimmed surveillance videos that capture a large field of view. Most existing activity detection approaches are designed for recognizing atomic human actions performed in the foreground. Therefore, they are not suitable for detecting activities in extended videos, which contain multiple actors performing co-occurring, complex activities with extreme spatio-temporal scale variation. In this paper, we propose a modular, actor-centric framework for real-time activity detection in extended videos. In particular, we decompose an extended video into a collection of smaller actor-centric tubelets of interest. Each tubelet is a video sub-volume associated with an actor and includes adaptive visual context for recognizing the actor's activities. Once these tubelets are extracted via an object-detection-based approach, we are able to detect activities in each tubelet by focusing on the actor situated in its foreground. To accurately detect the activities of a tubelet's actor we take into account the interactions with other detected actors and objects within the tubelet. We encode such interactions with a dynamic visual spatio-temporal graph and process it with a Graph Neural Network that yields context-aware actor representations. We validate our activity detection framework on the MEVA (Multiview Extended Video with Activities) dataset and the ActEV 2021 Sequestered Data Leaderboard and demonstrate its effectiveness in terms of speed and performance.",
    "code_link": "https://github.com/usnistgov/ActEV_Scorer"
  },
  "wacv2022_hadcv_fromleaderboardtooperationsdivatransitionexperiences": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HADCV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Human Activity Detection in Multi-Camera, Continuous, Long-Duration Video",
    "title": "From Leaderboard to Operations: DIVA Transition Experiences",
    "authors": [
      "Bharadwaj Ravichandran",
      "Roderic Collins",
      "Keith Fieldhouse",
      "Kellie Corona",
      "Anthony Hoogs"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/html/Ravichandran_From_Leaderboard_to_Operations_DIVA_Transition_Experiences_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HADCV/papers/Ravichandran_From_Leaderboard_to_Operations_DIVA_Transition_Experiences_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "The IARPA Deep Intermodal Video Analytics (DIVA) program has sponsored the development of systems that detect and recognize activities in security video. During the period from September 2017 to March 2021, the development and evaluation of these systems was focused on optimizing accuracy, embodied in quantified metrics, against a large but relatively static corpus of video collected and annotated by the program. This focus was aided by various software engineering decisions collaboratively reached by the program performers and Test & Evaluation (T&E) team, which established a common software framework enabling ongoing quantitative evaluation via software submissions to a leaderboard. While continuing to support the leaderboard, in March 2021 the program began efforts, still in progress, to transition capabilities developed on DIVA from the research environment to operational evaluation and deployment. As an operational system is a different use case than a research environment, it is not surprising that design decisions favoring the former will not always align with the latter. This paper discusses our work to transition DIVA systems into an operational setting, particularly identifying and resolving conflicts between the evaluation framework and operational requirements. We describe transition efforts to date, propose future work, and conclude with lessons learned from the overall transition effort.",
    "code_link": ""
  },
  "wacv2022_dnow_autoqathequestionisnotonlywhat,butalsowhere": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Auto QA: The Question Is Not Only What, but Also Where",
    "authors": [
      "Sumit Kumar",
      "Badri N. Patro",
      "Vinay P. Namboodiri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Kumar_Auto_QA_The_Question_Is_Not_Only_What_but_Also_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Kumar_Auto_QA_The_Question_Is_Not_Only_What_but_Also_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Visual Question Answering can be a functionally relevant task if purposed as such. In this paper, we aim to investigate and evaluate its efficacy in terms of localization-based question answering. We do this specifically in the context of autonomous driving where this functionality is important. To achieve our aim, we provide a new dataset, Auto-QA. Our new dataset is built over the Argoverse dataset and provides a truly multi-modal setting with seven views per frame and point-cloud LIDAR data being available for answering a localization-based question. We contribute localized attention adaptations of most popular VQA baselines and evaluate them on this task. We also provide joint point-cloud and image-based baselines that perform well on this task. An additional evaluation that we perform is to analyse whether the attention module is accurate or not for the image-based VQA baselines. To summarize, through this work we thoroughly analyze the localization abilities through visual question answering for autonomous driving and provide a new benchmark task for the same. Our best joint baseline model achieves a useful 74.8% accuracy on this task. We release our dataset and source code for our baseline modules in the following webpage: \\url https://temporaryprojectpage.github.io/AUTO-QA/",
    "code_link": ""
  },
  "wacv2022_dnow_towardsunsupervisedonlinedomainadaptationforsemanticsegmentation": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Towards Unsupervised Online Domain Adaptation for Semantic Segmentation",
    "authors": [
      "Yevhen Kuznietsov",
      "Marc Proesmans",
      "Luc Van Gool"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Kuznietsov_Towards_Unsupervised_Online_Domain_Adaptation_for_Semantic_Segmentation_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Kuznietsov_Towards_Unsupervised_Online_Domain_Adaptation_for_Semantic_Segmentation_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In recent years, there has been significant progress in overcoming the negative effects of domain shift in semantic segmentation. Yet, existing unsupervised domain adaptation methods operate in an offline fashion, which imposes multiple restrictions on their deployment in real world scenarios. In this paper, we introduce a problem of online domain adaptation for semantic segmentation, which involves producing predictions for and, at the same time, continuously adapting a model to new frames of target domain videos. To tackle this problem, we propose a novel method which utilizes unsupervised structure-from-motion cues as the primary source of domain adaptation. By optimizing online the representation shared between depth and semantics networks, our geometry-guided algorithm achieves semantic segmentation performance comparable to state-of-the-art offline methods, without using target domain training data whatsoever.",
    "code_link": ""
  },
  "wacv2022_dnow_vquadvideoquestionansweringdiagnosticdataset": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "VQuAD: Video Question Answering Diagnostic Dataset",
    "authors": [
      "Vivek Gupta",
      "Badri N. Patro",
      "Hemant Parihar",
      "Vinay P. Namboodiri"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Gupta_VQuAD_Video_Question_Answering_Diagnostic_Dataset_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Gupta_VQuAD_Video_Question_Answering_Diagnostic_Dataset_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper, we investigate the task of Video-based Question Answering. We provide a diagnostic dataset that can be used to evaluate the extent of the reasoning abilities of various methods for solving this task. Previous datasets proposed for this task do not have this ability. Our dataset is large scale (around 1.3 million questions jointly for train and test) and evaluates both the spatial and temporal properties and the relationship between various objects for these properties. We evaluate the state-of-the-art language model (BERT) as a baseline to understand the extent of correlation based on language features alone. Other existing networks are then used to combine video features along with language features for solving this task. Unfortunately, we observe that the currently prevalent systems do not perform significantly better than the language baseline. We hypothesize that this is due to our efforts in ensuring that no obvious biases exist in this dataset and the dataset is balanced. To make progress, the learning techniques need to obtain an ability to reason, going beyond basic correlation of biases. This is an interesting and significant challenge provided through our work.",
    "code_link": ""
  },
  "wacv2022_dnow_reconstructivetrainingforreal-worldrobustnessinimageclassification": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Reconstructive Training for Real-World Robustness in Image Classification",
    "authors": [
      "David Patrick",
      "Michael Geyer",
      "Richard Tran",
      "Amanda Fernandez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Patrick_Reconstructive_Training_for_Real-World_Robustness_in_Image_Classification_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Patrick_Reconstructive_Training_for_Real-World_Robustness_in_Image_Classification_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In order to generalize to real-world data, computer vision models need to be robust to corruptions which maynot generally be available in the traditional benchmarkdatasets. Real world data is diverse and can vary over time - sensors may become damaged, environments may change, or users may provide malicious inputs. While substantial research has focused separately on processing specific image distortions or on defending against types of adversarial attack, some real-world applications will require vision models to generalize to corruptions, while additionally maintaining image quality. We propose a simple training strategy to leverage image reconstruction, with similarities to a GAN training process, to reduce image data corruptions while maintaining the visual integrity of the image. Our approach is demonstrated on several corruptions for the task of image classification, and compared with established approaches, with qualitative and quantitative improvements. Code available at: https://github.com/UTSA-VAIL/ReconstructiveTraining",
    "code_link": ""
  },
  "wacv2022_dnow_inductivebiasesforlowdatavqaadataaugmentationapproach": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Inductive Biases for Low Data VQA: A Data Augmentation Approach",
    "authors": [
      "Narjes Askarian",
      "Ehsan Abbasnejad",
      "Ingrid Zukerman",
      "Wray Buntine",
      "Gholamreza Haffari"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Askarian_Inductive_Biases_for_Low_Data_VQA_A_Data_Augmentation_Approach_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Askarian_Inductive_Biases_for_Low_Data_VQA_A_Data_Augmentation_Approach_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Visual question answering (VQA) is the problem of understanding rich image contexts and answering complex natural language questions about them. VQA models have recently achieved remarkable results when training on large-scale labeled datasets. However, annotating large amounts of data is not feasible in many domains. In this paper, we address the problem of VQA in a low-labeled data regime, which is under-explored in the literature. We take a data augmentation approach to enlarge the initial small labeled data in order to inject proper inductive biases into the VQA model. We encode the additional inductive biases in the questions by producing new ones taking advantage of the image annotations. Our results show up to 34% accuracy improvements compared to the baselines trained on only the initial labeled data.",
    "code_link": ""
  },
  "wacv2022_dnow_unsupervisedbatchnormadaptation(ubna)adomainadaptationmethodforsemanticsegmentationwithoutusingsourcedomainrepresentations": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for Semantic Segmentation Without Using Source Domain Representations",
    "authors": [
      "Marvin Klingner",
      "Jan-Aike Term\u00f6hlen",
      "Jacob Ritterbach",
      "Tim Fingscheidt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Klingner_Unsupervised_BatchNorm_Adaptation_UBNA_A_Domain_Adaptation_Method_for_Semantic_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Klingner_Unsupervised_BatchNorm_Adaptation_UBNA_A_Domain_Adaptation_Method_for_Semantic_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper we present a solution to the task of \"unsupervised domain adaptation (UDA) of a given pre-trained semantic segmentation model without relying on any source domain representations\". Previous UDA approaches for semantic segmentation either employed simultaneous training of the model in the source and target domains, or they relied on an additional network, replaying source domain knowledge to the model during adaptation. In contrast, we present our novel Unsupervised BatchNorm Adaptation (UBNA) method, which adapts a given pre-trained model to an unseen target domain without using---beyond the existing model parameters from pre-training---any source domain representations (neither data, nor networks) and which can also be applied in an online setting or using just a few unlabeled images from the target domain in a few-shot manner. Specifically, we partially adapt the normalization layer statistics to the target domain using an exponentially decaying momentum factor, thereby mixing the statistics from both domains. By evaluation on standard UDA benchmarks for semantic segmentation we show that this is superior to a model without adaptation and to baseline approaches using statistics from the target domain only. Compared to standard UDA approaches we report a trade-off between performance and usage of source domain representations.",
    "code_link": "https://github.com/ifnspaml/UBNA"
  },
  "wacv2022_dnow_uncertaintyawareproposalsegmentationforunknownobjectdetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Uncertainty Aware Proposal Segmentation for Unknown Object Detection",
    "authors": [
      "Yimeng Li",
      "Jana Ko\u0161eck\u00e1"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Li_Uncertainty_Aware_Proposal_Segmentation_for_Unknown_Object_Detection_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Li_Uncertainty_Aware_Proposal_Segmentation_for_Unknown_Object_Detection_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Recent efforts in deploying Deep Neural Networks for object detection in real world applications, such as autonomous driving, assume that all relevant object classes have been observed during training. Quantifying the performance of these models in settings when the test data is not represented in the training set has mostly focused on pixel-level uncertainty estimation techniques of models trained for semantic segmentation. This paper proposes to exploit additional predictions of semantic segmentation models and quantifying its confidences, followed by classification of object hypotheses as known vs. unknown, out of distribution objects. We use object proposals generated by Region Proposal Network (RPN) and adapt distance aware uncertainty estimation of semantic segmentation using Radial Basis Functions Networks (RBFN) for class agnostic object mask prediction. The augmented object proposals are then used to train a classifier for known vs. unknown objects categories. Experimental results demonstrate that the proposed method achieves parallel performance to state of the art methods for unknown object detection and can also be used effectively for reducing object detectors' false positive rate. Our method is well suited for applications where prediction of non-object background categories obtained by semantic segmentation is reliable.",
    "code_link": ""
  },
  "wacv2022_dnow_attentionguidedcosinemargintoovercomeclass-imbalanceinfew-shotroadobjectdetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DNOW",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Dealing With the Novelty in Open Worlds",
    "title": "Attention Guided Cosine Margin To Overcome Class-Imbalance in Few-Shot Road Object Detection",
    "authors": [
      "Ashutosh Agarwal",
      "Anay Majee",
      "Anbumani Subramanian",
      "Chetan Arora"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/html/Agarwal_Attention_Guided_Cosine_Margin_To_Overcome_Class-Imbalance_in_Few-Shot_Road_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Agarwal_Attention_Guided_Cosine_Margin_To_Overcome_Class-Imbalance_in_Few-Shot_Road_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Few-Shot Object Detectors (FSOD) are tasked to localize and classify objects in an image given only a few data samples. Recent trends in FSOD research show the adoption of metric and meta-learning techniques, which are prone to catastrophic forgetting and class confusion. To overcome these pitfalls in metric learning based FSOD techniques, we introduce an Attention Guided Cosine Margin (AGCM) that facilitates the creation of tighter and well separated class-specific feature clusters in the classification head of the object detector. The Attentive Proposal Fusion (APF) module introduced in AGCM minimizes catastrophic forgetting by reducing the intra-class variance among co-occurring classes. At the same time, the Cosine Margin penalty in AGCM increases the angular margin between confusing classes to overcome the challenge of class confusion between already learned (base) and newly added (novel) classes. We conduct our experiments on the India Driving Dataset (IDD), which presents a real-world class-imbalanced setting alongside popular FSOD benchmark PASCAL-VOC. Our method outperforms existing approaches by up to 6.4 mAP points on the IDD-OS and up to 2.0 mAP points on the IDD-10 splits for the 10-shot setting. On the PASCAL-VOC dataset, we outperform existing approaches by up to 4.9 mAP points.",
    "code_link": ""
  },
  "wacv2022_map-a_apersonalizedbenchmarkforfaceanti-spoofing": {
    "conf_id": "WACV2022",
    "conf_sub_id": "MAP-A",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Manipulation, Adversarial and Presentation Attacks in Biometrics",
    "title": "A Personalized Benchmark for Face Anti-Spoofing",
    "authors": [
      "Davide Belli",
      "Debasmit Das",
      "Bence Major",
      "Fatih Porikli"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Belli_A_Personalized_Benchmark_for_Face_Anti-Spoofing_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/papers/Belli_A_Personalized_Benchmark_for_Face_Anti-Spoofing_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Thanks to their ease-of-use and effectiveness, face authentication systems are nowadays ubiquitous in electronic devices to control access to protected data. However, the widespread adoption of such systems comes with security and reliability issues. This is because spoofs of face images can be easily fabricated to deceive the recognition systems. Hence, there is a need to integrate the user identification system with a robust face anti-spoofing element, which has the goal to detect whether a queried face image is a spoof or live. Most contemporary face anti-spoofing systems only rely on the query image to accept or reject tentative access. In real-world scenarios, however, face authentication systems often have an initial enrollment step where a few live images of the user are recorded and stored for identification purposes. In this paper, we present a complementary approach to augment existing face anti-spoofing benchmarks to account for enrollment images associated with each query image. We apply this strategy on two recently introduced datasets: CelebA-Spoof and SiW. We showcase how existing anti-spoofing models can be easily personalized using the subject's enrollment data, and we evaluate the effectiveness of the enhanced methods on the newly proposed datasets splits CelebA-Spoof-Enroll and SiW-Enroll.",
    "code_link": ""
  },
  "wacv2022_map-a_powerfulphysicaladversarialexamplesagainstpracticalfacerecognitionsystems": {
    "conf_id": "WACV2022",
    "conf_sub_id": "MAP-A",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Manipulation, Adversarial and Presentation Attacks in Biometrics",
    "title": "Powerful Physical Adversarial Examples Against Practical Face Recognition Systems",
    "authors": [
      "Inderjeet Singh",
      "Toshinori Araki",
      "Kazuya Kakizaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Singh_Powerful_Physical_Adversarial_Examples_Against_Practical_Face_Recognition_Systems_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/papers/Singh_Powerful_Physical_Adversarial_Examples_Against_Practical_Face_Recognition_Systems_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "It is well-known that the most existing machine learning (ML)-based safety-critical applications are vulnerable to carefully crafted input instances called adversarial examples (AXs). An adversary can conveniently attack these target systems from digital as well as physical worlds. This paper aims to the generation of robust physical AXs against face recognition systems. We present a novel smoothness loss function and a patch-noise combo attack for realizing powerful physical AXs. The smoothness loss interjects the concept of delayed constraints during the attack generation process, thereby causing better handling of optimization complexity and smoother AXs for the physical domain. The patch-noise combo attack combines patch noise and imperceptibly small noises from different distributions to generate powerful registration-based physical AXs. An extensive experimental analysis found that our smoothness loss results in robust and more transferable digital and physical AXs than the conventional techniques. Notably, our smoothness loss results in a 1.17 and 1.97 times better mean attack success rate (ASR) in physical white-box and black-box attacks, respectively. Our patch-noise combo attack furthers the performance gains and results in 2.39 and 4.74 times higher mean ASR than conventional technique in physical world white-box and black-box attacks, respectively.",
    "code_link": ""
  },
  "wacv2022_map-a_saliency-guidedtexturedcontactlens-awareirisrecognition": {
    "conf_id": "WACV2022",
    "conf_sub_id": "MAP-A",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Manipulation, Adversarial and Presentation Attacks in Biometrics",
    "title": "Saliency-Guided Textured Contact Lens-Aware Iris Recognition",
    "authors": [
      "Lucas Parzianello",
      "Adam Czajka"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Parzianello_Saliency-Guided_Textured_Contact_Lens-Aware_Iris_Recognition_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/papers/Parzianello_Saliency-Guided_Textured_Contact_Lens-Aware_Iris_Recognition_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Iris recognition requires an adequate level of the iris texture being visible to perform a reliable matching. In case when a textured contact lens covers the iris, a false non-match is reported or a presentation attack is detected. There are, however, scenarios in which one wants to maximize the probability of a correct match despite the iris texture being being partially or mostly obscured, for instance when a non-cooperative subject conceals their identity by purposely wearing textured contact lenses. This paper proposes an iris recognition method designed to detect and match portions of live iris tissue still visible when a person wears textured contact lenses. The proposed method includes (a) a convolutional neural network-based segmenter detecting partial live iris patterns, and (b) a Siamese network-based feature extraction model, trained in a novel way with images having non-iris information removed by blurring, to guide the network towards salient live iris features. Experiments matching pairs of iris images in which the iris is not wearing a lens in one image and is wearing a textured contact lens in the other, show a lower EER=10.6% for the proposed algorithm, compared to state-of-the-art iris code-based iris recognition (EER=33.6%). The source codes of the method are offered along with the paper.",
    "code_link": ""
  },
  "wacv2022_map-a_morphdetectionenhancedbystructuredgroupsparsity": {
    "conf_id": "WACV2022",
    "conf_sub_id": "MAP-A",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Manipulation, Adversarial and Presentation Attacks in Biometrics",
    "title": "Morph Detection Enhanced by Structured Group Sparsity",
    "authors": [
      "Poorya Aghdaie",
      "Baaria Chaudhary",
      "Sobhan Soleymani",
      "Jeremy Dawson",
      "Nasser M. Nasrabadi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Aghdaie_Morph_Detection_Enhanced_by_Structured_Group_Sparsity_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/papers/Aghdaie_Morph_Detection_Enhanced_by_Structured_Group_Sparsity_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper, we consider the challenge of face morphing attacks, which substantially undermine the integrity of face recognition systems such as those adopted for use in border protection agencies. Morph detection can be formulated as extracting fine-grained representations, where local discriminative features are harnessed for learning a hypothesis. To acquire discriminative features at different granularity as well as a decoupled spectral information, we leverage wavelet domain analysis to gain insight into the spatial-frequency content of a morphed face. As such, instead of using images in the RGB domain, we decompose every image into its wavelet sub-bands using 2D wavelet decomposition and a deep supervised feature selection scheme is employed to find the most discriminative wavelet sub-bands of input images. To this end, we train a Deep Neural Network (DNN) morph detector using the decomposed wavelet sub-bands of the morphed and bona fide images. In the training phase, our structured group sparsity-constrained DNN picks the most discriminative wavelet sub-bands out of all the sub-bands, with which we retrain our DNN, resulting in a precise detection of morphed images when inference is achieved on a probe image. The efficacy of our deep morph detector which is enhanced by structured group lasso is validated through experiments on three facial morph image databases, i.e., VISAPP17, LMA, and MorGAN.",
    "code_link": ""
  },
  "wacv2022_map-a_otb-morphone-timebiometricsviamorphingappliedtofacetemplates": {
    "conf_id": "WACV2022",
    "conf_sub_id": "MAP-A",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Manipulation, Adversarial and Presentation Attacks in Biometrics",
    "title": "OTB-Morph: One-Time Biometrics via Morphing Applied To Face Templates",
    "authors": [
      "Mahdi Ghafourian",
      "Julian Fierrez",
      "Ruben Vera-Rodriguez",
      "Ignacio Serna",
      "Aythami Morales"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Ghafourian_OTB-Morph_One-Time_Biometrics_via_Morphing_Applied_To_Face_Templates_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/papers/Ghafourian_OTB-Morph_One-Time_Biometrics_via_Morphing_Applied_To_Face_Templates_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Cancelable biometrics refers to a group of techniques in which the biometric inputs are transformed intentionally using a key before processing or storage. This transformation is repeatable enabling subsequent biometric comparisons. This paper introduces a new scheme for cancelable biometrics aimed at protecting the templates against potential attacks, applicable to any biometric-based recognition system. Our proposed scheme is based on time-varying keys obtained from morphing random biometric information. An experimental implementation of the proposed scheme is given for face biometrics. The results confirm that the proposed approach is able to withstand against leakage attacks while improving the recognition performance.",
    "code_link": ""
  },
  "wacv2022_map-a_synthesizingfaceimagesfrommatchscores": {
    "conf_id": "WACV2022",
    "conf_sub_id": "MAP-A",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Manipulation, Adversarial and Presentation Attacks in Biometrics",
    "title": "Synthesizing Face Images From Match Scores",
    "authors": [
      "Thomas Swearingen",
      "Arun Ross"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Swearingen_Synthesizing_Face_Images_From_Match_Scores_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/MAP-A/papers/Swearingen_Synthesizing_Face_Images_From_Match_Scores_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper, we consider the problem of generating a face image based on its match scores with other face images. Such an exercise is not only useful in understanding the relationship between face images, but it can also be used to understand the degree of privacy associated with match scores. We address the problem using two approaches. The first mixes face images to deduce the appearance of a missing face image and the second uses a convolutional autoencoder to further enhance the mixed face image. Experiments suggest the potential of the proposed approaches in generating a missing face image in a database by utilizing its relationship with other images in the database.",
    "code_link": "https://github.com/deepinsight/insightface"
  },
  "wacv2022_xai4b_semanticnetworkinterpretation": {
    "conf_id": "WACV2022",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Semantic Network Interpretation",
    "authors": [
      "Pei Guo",
      "Ryan Farrell"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/html/Guo_Semantic_Network_Interpretation_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/papers/Guo_Semantic_Network_Interpretation_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Network interpretation as an effort to reveal the features learned by a network remains largely visualization-based. In this paper, our goal is to tackle semantic network interpretation at both filter and decision level. For filter-level interpretation, we represent the concepts a filter encodes with a probability distribution of visual attributes. The decision-level interpretation is achieved by textual summarization that generates an explanatory sentence containing clues behind a network's decision. A Bayesian inference algorithm is proposed to automatically associate filters and network decisions with visual attributes. Human study confirms that the semantic interpretation is a beneficial alternative or complement to visualization methods. We demonstrate the crucial role that semantic network interpretation can play in understanding a network's failure patterns. More importantly, semantic network interpretation enables a better understanding of the correlation between a model's performance and its distribution metrics like filter selectivity and concept sparseness.",
    "code_link": ""
  },
  "wacv2022_xai4b_interpretabledeeplearning-basedforensicirissegmentationandrecognition": {
    "conf_id": "WACV2022",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Interpretable Deep Learning-Based Forensic Iris Segmentation and Recognition",
    "authors": [
      "Andrey Kuehlkamp",
      "Aidan Boyd",
      "Adam Czajka",
      "Kevin Bowyer",
      "Patrick Flynn",
      "Dennis Chute",
      "Eric Benjamin"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/html/Kuehlkamp_Interpretable_Deep_Learning-Based_Forensic_Iris_Segmentation_and_Recognition_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/papers/Kuehlkamp_Interpretable_Deep_Learning-Based_Forensic_Iris_Segmentation_and_Recognition_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Iris recognition of living individuals is a mature biometric modality, that has been adopted globally from governmental ID programs, border crossing, voter registration and de-duplication, to unlocking mobile phones. On the other hand, the possibility of recognizing deceased subjects with their iris patterns has emerged recently. In this paper, we present an end-to-end deep learning-based method for postmortem iris segmentation and recognition with a special visualization technique intended to support forensic human examiners in their efforts. The proposed postmortem iris segmentation approach outperforms the state of the art and -- in addition to iris annulus, as in case of classical iris segmentation methods -- detects abnormal regions caused by eye decomposition processes, such as furrows or irregular specular highlights present on the drying and wrinkling cornea. The method was trained and validated with data acquired from 171 cadavers, kept in mortuary conditions, and tested on subject-disjoint data acquired from 259 deceased subjects. To our knowledge, this is the largest corpus of data used in post-mortem iris recognition research to date. The source codes of the proposed method are offered with the paper. The test data will be available through the National Archive of Criminal Justice Data (NACJD) archives.",
    "code_link": "https://github.com/akuehlka/xai4b"
  },
  "wacv2022_xai4b_myopemodels-arefacepresentationattackdetectionmodelsshort-sighted?": {
    "conf_id": "WACV2022",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Myope Models - Are Face Presentation Attack Detection Models Short-Sighted?",
    "authors": [
      "Pedro C. Neto",
      "Ana F. Sequeira",
      "Jaime S. Cardoso"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/html/Neto_Myope_Models_-_Are_Face_Presentation_Attack_Detection_Models_Short-Sighted_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/papers/Neto_Myope_Models_-_Are_Face_Presentation_Attack_Detection_Models_Short-Sighted_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Presentation attacks are recurrent threats to biometric systems, where impostors attempt to bypass these systems. Humans often use background information as contextual cues for their visual system. Yet, regarding face-based systems, the background is often discarded, since face presentation attack detection (PAD) models are mostly trained with face crops. This work presents a comparative study of face PAD models (including multi-task learning, adversarial training and dynamic frame selection) in two settings: with and without crops. The results show that the performance is consistently better when the background is present in the images. The proposed multi-task methodology beats the state-of-the-art results on the ROSE-Youtu dataset by a large margin with an equal error rate of 0.2%. Furthermore, we analyze the models' predictions with Grad-CAM++ with the aim to investigate to what extent the models focus on background elements that are known to be useful for human inspection. From this analysis we can conclude that the background cues are not relevant across all the attacks. Thus, showing the capability of the model to leverage the background information only when necessary.",
    "code_link": ""
  },
  "wacv2022_xai4b_explainabilityoftheimplicationsofsupervisedandunsupervisedfaceimagequalityestimationsthroughactivationmapvariationanalysesinfacerecognitionmodels": {
    "conf_id": "WACV2022",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Explainability of the Implications of Supervised and Unsupervised Face Image Quality Estimations Through Activation Map Variation Analyses in Face Recognition Models",
    "authors": [
      "Biying Fu",
      "Naser Damer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/html/Fu_Explainability_of_the_Implications_of_Supervised_and_Unsupervised_Face_Image_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/papers/Fu_Explainability_of_the_Implications_of_Supervised_and_Unsupervised_Face_Image_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "It is challenging to derive explainability for unsupervised or statistical-based face image quality assessment (FIQA) methods. In this work, we propose a novel set of explainability tools to derive reasoning for different FIQA decisions and their face recognition (FR) performance implications. We avoid limiting the deployment of our tools to certain FIQA methods by basing our analyses on the behavior of FR models when processing samples with different FIQA decisions. This leads to explainability tools that can be applied for any FIQA method with any CNN-based FR solution using activation mapping to exhibit the network's activation derived from the face embedding. To avoid the low discrimination between the general spatial activation mapping of low and high-quality images in FR models, we build our explainability tools in a higher derivative space by analyzing the variation of the FR activation maps of image sets with different quality decisions. We demonstrate our tools and analyze the findings on four FIQA methods, by presenting inter and intra-FIQA method analyses. Our proposed tools and the analyses based on them point out, among other conclusions, that high-quality images typically cause consistent low activation on the areas outside of the central face region, while low-quality images, despite general low activation, have high variations of activation in such areas. Our explainability tools also extend to analyzing single images where we show that low-quality images tend to have an FR model spatial activation that strongly differs from what is expected from a high-quality image where this difference also tends to appear more in areas outside of the central face region and does correspond to issues like extreme poses and facial occlusions. The implementation of the proposed tools is accessible here.",
    "code_link": ""
  },
  "wacv2022_xai4b_skeleton-basedtypingstylelearningforpersonidentification": {
    "conf_id": "WACV2022",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Skeleton-Based Typing Style Learning for Person Identification",
    "authors": [
      "Lior Gelberg",
      "David Mendlovic",
      "Dan Raviv"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/html/Gelberg_Skeleton-Based_Typing_Style_Learning_for_Person_Identification_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/papers/Gelberg_Skeleton-Based_Typing_Style_Learning_for_Person_Identification_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "We present a novel approach for person identification based on typing-style, using a novel architecture constructed of adaptive non-local spatio-temporal graph convolutional network. Since type style dynamics convey meaningful information that can be useful for person identification, we extract the joints positions and then learn their movements' dynamics. Our non-local approach increases our model's robustness to noisy input data while analyzing joints locations instead of RGB data provides remarkable robustness to alternating environmental conditions, e.g., lighting, noise, etc. We further present two new datasets for typing style based person identification task and extensive evaluation that displays our model's superior discriminative and generalization abilities, when compared with state-of-the-art skeleton-based models.",
    "code_link": ""
  },
  "wacv2022_xai4b_supervisedcontrastivelearningforgeneralizableandexplainabledeepfakesdetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "XAI4B",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Explainable & Interpretable Artificial Intelligence for Biometrics",
    "title": "Supervised Contrastive Learning for Generalizable and Explainable DeepFakes Detection",
    "authors": [
      "Ying Xu",
      "Kiran Raja",
      "Marius Pedersen"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/html/Xu_Supervised_Contrastive_Learning_for_Generalizable_and_Explainable_DeepFakes_Detection_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/XAI4B/papers/Xu_Supervised_Contrastive_Learning_for_Generalizable_and_Explainable_DeepFakes_Detection_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "DeepFakes detection approaches have to be agnostic across generation type, quality, and appearance to provide a generalizable DeepFakes detector. Limited generalizability will hinder wide-scale deployment of detectors if they cannot handle unseen attacks in an open set scenario. We propose a generalizable detection model that can detect novel and unknown/unseen DeepFakes using a supervised contrastive (SupCon) loss. As DeepFakes can resemble the original image/video to a greater extent in terms of appearance and it becomes challenging to secern them, we propose to exploit the contrasts in the representation space to learn a generalizable detector. We further investigate the features learnt from our proposed approach for explainability. The analysis for explainability of the models advocates the need for fusion and motivated by this, we fuse the scores from the proposed SupCon model and the Xception network to exploit the variability from different architectures. The proposed model consistently performs better compared to the single model on both known data and unknown attacks consistently in a seen data setting and an unseen data setting, with generalizability and explainability as a basis. We obtain the highest accuracy of 78.74% using proposed SupCon model and an accuracy of 83.99% with proposed fusion in a true open-set evaluation scenario where the test class is unknown at the training phase. The paper also aligns with reproducible research by making the code available.",
    "code_link": "https://github.com/xuyingzhongguo/deepfake"
  },
  "wacv2022_dvpb_analysisofmanualandautomatedskintoneassignments": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DVPB",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Demographic Variations in Performance of Biometrics and Related Technology",
    "title": "Analysis of Manual and Automated Skin Tone Assignments",
    "authors": [
      "K. S. Krishnapriya",
      "Gabriella Pangelinan",
      "Michael C. King",
      "Kevin W. Bowyer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DVPB/html/Krishnapriya_Analysis_of_Manual_and_Automated_Skin_Tone_Assignments_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DVPB/papers/Krishnapriya_Analysis_of_Manual_and_Automated_Skin_Tone_Assignments_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "The Fitzpatrick scale is a standard tool in dermatology to classify skin types for melanin and sensitivity to sun exposure. After an in-person interview, the dermatologist would classify the person's skin type on a six-valued, light-to-dark scale. Various face image analysis researchers have recently categorized skin tone in face images on a six-valued, light-to-dark scale in order to look into questions of bias and accuracy related to skin tone. Categorization of skin tone on the basis of images rather than personal interview is not, on that basis alone, strictly speaking, on the Fitzpatrick scale. While the manual assignment of face images on a six-point, light-to-dark scale has been used by various researchers studying bias in face image analysis, to date there has been no study on the consistency and reliability of observers assigning skin type from an image. We analyze a set of manual skin type assignments from multiple observers viewing the same image set and find that there are inconsistencies between human raters. We then develop an algorithm for automated skin type assignments, which could be used in place of manual assignment by observers. Such an algorithm would allow for provision of skin tone annotations on large quantities of images beyond what could be accomplished by manual raters. To our knowledge, this is the first work to: (a) examine the consistency of manual skin tone ratings across observers, (b) document that there is substantial variation in the rating of the same image by different observers even when exemplar images are given for guidance and all images are color-corrected, and (c) compare manual versus automated skin tone ratings. We release the automated skin tone rating implementation so that other researchers may reproduce and extend the results in this paper.",
    "code_link": ""
  },
  "wacv2022_dvpb_similaritiesinafricanethnicfacesfromthebiometricrecognitionviewpoint": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DVPB",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Demographic Variations in Performance of Biometrics and Related Technology",
    "title": "Similarities in African Ethnic Faces From the Biometric Recognition Viewpoint",
    "authors": [
      "Ogechukwu Iloanusi",
      "Patrick J. Flynn",
      "Patrick Tinsley"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DVPB/html/Iloanusi_Similarities_in_African_Ethnic_Faces_From_the_Biometric_Recognition_Viewpoint_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DVPB/papers/Iloanusi_Similarities_in_African_Ethnic_Faces_From_the_Biometric_Recognition_Viewpoint_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Face pose, illumination, and facial expressions are known factors that affect face recognition performance and have been studied at length in the literature. The impacts of demographic factors such as gender, race, and age on performance have also been studied, with increasing interest recently in the context of algorithmic bias concerns. This work is a study of face recognition performance using a database of faces of Nigerian subjects drawn from 28 different ethnicities. There are documented differences in facial anthropometric characteristics between some Nigerian ethnicities, and this study was intended to establish initial results regarding the impact of these inter-ethnic differences on face recognition performance. A comparison to performance on a database of Caucasian/Asian face images is made. Our study analyses how 28 African ethnicities affect face identification performance metrics by focusing on the genuine and impostor scores' distributions. Our analysis shows that face identification performance is not remarkably influenced by varying ethnicities within the African race though there are significant differences in relation to the Caucasian/Asian set.",
    "code_link": ""
  },
  "wacv2022_dvpb_algorithmicfairnessinfacemorphingattackdetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "DVPB",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Demographic Variations in Performance of Biometrics and Related Technology",
    "title": "Algorithmic Fairness in Face Morphing Attack Detection",
    "authors": [
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/DVPB/html/Ramachandra_Algorithmic_Fairness_in_Face_Morphing_Attack_Detection_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/DVPB/papers/Ramachandra_Algorithmic_Fairness_in_Face_Morphing_Attack_Detection_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Face morphing attacks can compromise Face Recognition System (FRS) by exploiting their vulnerability. Face Morphing Attack Detection (MAD) techniques have been developed in recent past to deter such attacks and mitigate risks from morphing attacks. MAD algorithms, as any other algorithms should treat the images of subjects from different ethnic origins in an equal manner and provide non-discriminatory results. While the promising MAD algorithms are tested for robustness, there is no study comprehensively bench-marking their behaviour against various ethnicities. In this paper, we study and present a comprehensive analysis of algorithmic fairness of the existing Single image-based Morph Attack Detection (S-MAD) algorithms. We attempt to better understand the influence of ethnic bias on MAD algorithms and to this extent, we study the performance of MAD algorithms on a newly created dataset consisting of four different ethnic groups. With Extensive experiments using six different S-MAD techniques, we first present benchmark of detection performance and then measure the quantitative value of the algorithmic fairness for each of them using Fairness Discrepancy Rate (FDR). The results indicate the lack of fairness on all six different S-MAD methods when trained and tested on different ethnic groups suggesting the need for better MAD approaches to mitigate the algorithmic bias.",
    "code_link": ""
  },
  "wacv2022_rws_videorepresentationlearningthroughpredictionforonlineobjectdetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Video Representation Learning Through Prediction for Online Object Detection",
    "authors": [
      "Masato Fujitake",
      "Akihiro Sugimoto"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Fujitake_Video_Representation_Learning_Through_Prediction_for_Online_Object_Detection_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Fujitake_Video_Representation_Learning_Through_Prediction_for_Online_Object_Detection_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "We present a video representation learning framework for real-time video object detection. Our approach is based on the interesting observation that a powerful prior knowledge of video context helps to improve object recognition, and it can be acquired via learning video representations through stochastic video prediction. Our proposed framework utilizes the stochastic video prediction into object detection so that we first acquire a prior knowledge of videos to have video representations and then adjust them to object detection to improve the accuracy. We validate our proposed method on ImageNet VID and VisDrone-VID2019 datasets to demonstrate the effectiveness of video representation learning via future video prediction. In particular, our extensive experiments on ImageNet VID show that our approach achieves 73.1% mAP at 54 fps with ResNet-50 on commercial GPUs.",
    "code_link": ""
  },
  "wacv2022_rws_ontheimportanceofappearanceandinteractionfeaturerepresentationsforpersonre-identification": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "On the Importance of Appearance and Interaction Feature Representations for Person Re-Identification",
    "authors": [
      "Richard Blythman",
      "Andrea Zunino",
      "Christopher Murray",
      "Vittorio Murino"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Blythman_On_the_Importance_of_Appearance_and_Interaction_Feature_Representations_for_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Blythman_On_the_Importance_of_Appearance_and_Interaction_Feature_Representations_for_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In recent person re-identification (Re-ID) approaches, combining global and local appearance-based features has been shown to increase performance effectively. These types of models are often characterized by multiple branches that act as experts for specific local regions or global high-level semantic features. We argue that attention mechanisms can be useful for multi-branch Re-ID models by creating more robust representations based on the interaction of informative image features. In this paper, we investigate this idea and propose a novel multi-branch architecture with experts that learn distinct representations based on (i) the global image appearance and (ii) the interaction between features. Unlike former methods with local experts acting on partitions that are fixed a-priori, our feature interaction expert uses a novel attention-based pooling to automatically extract semantically-rich and discriminative features from different regions of a person image. Compared with existing attention-based algorithms, our method maintains the feature interaction information separately in order to discriminate between identities. Our approach achieves state-of-the-art performance across three popular benchmarks - CUHK03, Market1501 and MSMT17. Furthermore, saliency visualizations show that appearance and interaction experts learn complementary representations that attend to multiple discriminant regions, leading to improved classification ability.",
    "code_link": ""
  },
  "wacv2022_rws_learningfromsyntheticvehicles": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Learning From Synthetic Vehicles",
    "authors": [
      "Tae Soo Kim",
      "Bohoon Shim",
      "Michael Peven",
      "Weichao Qiu",
      "Alan Yuille",
      "Gregory D. Hager"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Kim_Learning_From_Synthetic_Vehicles_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Kim_Learning_From_Synthetic_Vehicles_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper, we release the Simulated Articulated VEhicles Dataset (SAVED) which contains images of synthetic vehicles with moveable vehicle parts. SAVED consists of images that are much more relevant for vehicle-related pattern-recognition tasks than other popular pretraining datasets such as ImageNet. Compared to a model initialized with ImageNet weights, we show that a model pretrained using SAVED leads to much better performance when recognizing vehicle parts and orientation directly from an image. We also find that a multi-task pretraining approach using fine-grained geometric signals available in SAVED leads to significant improvements in performance. By pretraining on SAVED instead of ImageNet, we reduce the error rate of one of the state of the art vehicle orientation estimators by 51.2% when tested on real images. We release SAVED and instructions on its usage here (https://taesoo-kim.github.io/)",
    "code_link": ""
  },
  "wacv2022_rws_class-awareobjectcounting": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Class-Aware Object Counting",
    "authors": [
      "Andreas Michel",
      "Wolfgang Gross",
      "Fabian Schenkel",
      "Wolfgang Middelmann"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Michel_Class-Aware_Object_Counting_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Michel_Class-Aware_Object_Counting_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Estimating the correct number of objects in a given natural scene is a common challenge in computer vision. Natural scenes usually contain multiple object categories and varying object densities. Detection-based algorithms are well suited for class-aware object counting and low object counts. However, they underperform with high or varying numbers of objects. To address this challenge, we propose an end-to-end approach to enhance an existing detection-based method with a multi-class density estimation branch. The results of both branches are fed into a successive count-estimation network, which estimates object counts for each category. Although these numbers do not contain any localization information, they can be used as a valuable indicator for verifying the exactness of the object detector results and improve its counting performance. In order to demonstrate the effectiveness, we evaluate our method on common object detection datasets.",
    "code_link": ""
  },
  "wacv2022_rws_fastandlightweightonlinepersonsearchforlarge-scalesurveillancesystems": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Fast and Lightweight Online Person Search for Large-Scale Surveillance Systems",
    "authors": [
      "Andreas Specker",
      "Lennart Moritz",
      "Mickael Cormier",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Specker_Fast_and_Lightweight_Online_Person_Search_for_Large-Scale_Surveillance_Systems_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Specker_Fast_and_Lightweight_Online_Person_Search_for_Large-Scale_Surveillance_Systems_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "The demand for methods for video analysis in the field of surveillance technology is rapidly growing due to the increasing amount of surveillance footage available. Intelligent methods for surveillance software offer numerous possibilities to support police investigations and crime prevention. This includes the integration of video processing pipelines for tasks such as detection of graffiti, suspicious luggage, or intruders. Another important surveillance task is the semi-automated search for specific persons-of-interest within a camera network. In this work, we identify the major obstacles for the development of person search systems as the real-time processing capability on affordable hardware and the performance gap of person detection and re-identification methods on unseen target domain data. In addition, we demonstrate the current potential of intelligent online person search by developing a real-world, large-scale surveillance system. An extensive evaluation is provided for person detection, tracking, and re-identification components on affordable hardware setups, for which the whole system achieves real-time processing up to 76 FPS.",
    "code_link": ""
  },
  "wacv2022_rws_smallorfaraway?exploitingdeepsuper-resolutionandaltitudedataforaerialanimalsurveillance": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for Aerial Animal Surveillance",
    "authors": [
      "Mowen Xue",
      "Theo Greenslade",
      "Majid Mirmehdi",
      "Tilo Burghardt"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Xue_Small_or_Far_Away_Exploiting_Deep_Super-Resolution_and_Altitude_Data_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Xue_Small_or_Far_Away_Exploiting_Deep_Super-Resolution_and_Altitude_Data_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Visuals captured by high-flying aerial drones are increasingly used to assess biodiversity and animal population dynamics around the globe. Yet, challenging acquisition scenarios and tiny animal depictions in airborne imagery, despite ultra- high resolution cameras, have so far been limiting factors for applying computer vision detectors successfully with high confidence. In this paper, we address the problem for the first time by combining deep object detectors with super-resolution techniques and altitude data. In particular, we show that the integration of a holistic attention network based super-resolution approach and a custom-built altitude data exploitation network into standard recognition pipelines can considerably increase the detection efficacy in real-world settings. We evaluate the system on two public, large aerial-capture animal datasets, SAVMAP and AED. We find that the proposed approach can consistently improve over ablated baselines and the state-of-the-art performance for both datasets. In addition, we provide a systematic analysis of the relationship between animal resolution and detection performance. We conclude that super-resolution and altitude knowledge exploitation techniques can significantly increase benchmarks across settings and, thus, should be used routinely when detecting minutely resolved animals in aerial imagery.",
    "code_link": ""
  },
  "wacv2022_rws_wherearewewithhumanposeestimationinreal-worldsurveillance?": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Where Are We With Human Pose Estimation in Real-World Surveillance?",
    "authors": [
      "Mickael Cormier",
      "Aris Clepe",
      "Andreas Specker",
      "J\u00fcrgen Beyerer"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Cormier_Where_Are_We_With_Human_Pose_Estimation_in_Real-World_Surveillance_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Cormier_Where_Are_We_With_Human_Pose_Estimation_in_Real-World_Surveillance_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "The rapidly increasing number of surveillance cameras offers a variety of opportunities for intelligent video analytics to improve public safety. Among many others, the automatic recognition of suspicious and violent behavior poses a key task. To preserve personal privacy, prevent ethnic bias, and reduce complexity, most approaches first extract the pose or skeleton of persons and subsequently perform activity recognition. However, current literature mainly focuses on research datasets and does not consider real-world challenges and requirements of human pose estimation. We close this gap by analyzing these challenges, such as inadequate data and the need for real-time processing, and proposing a framework for human pose estimation in uncontrolled crowded surveillance scenarios. Our system integrates mitigation measures as well as a tracking component to incorporate temporal information. Finally, we provide a detailed quantitative and qualitative analysis on both a scientific and a real-world dataset to highlight improvements and remaining obstacles towards robust real-world human pose estimation in uncooperative scenarios.",
    "code_link": ""
  },
  "wacv2022_rws_cloth-changingpersonre-identificationwithself-attention": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Cloth-Changing Person Re-Identification With Self-Attention",
    "authors": [
      "Vaibhav Bansal",
      "Gian Luca Foresti",
      "Niki Martinel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Bansal_Cloth-Changing_Person_Re-Identification_With_Self-Attention_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Bansal_Cloth-Changing_Person_Re-Identification_With_Self-Attention_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "The basic assumption in the standard person re-identification (ReID) problem is that the clothing of the target person IDs would remain constant over long periods. This assumption creates errors during real-world implementations. In addition, most of the methods that handle ReID use CNN-based networks and have found limited success because CNNs can exploit only local dependencies and suffer the loss of information due to the use of downsampling operations. In this paper, we focus on a more challenging, realistic scenario of long-term cloth-changing ReID (CC-ReID). We aim to learn robust and unique feature representations that are invariant to clothing changes to address the CC-ReID problem. To overcome the limitations faced by CNNs, we propose a Vision-transformer-based framework. We also propose to intuitively exploit the unique soft-biometric-based discriminative information such as gait features and pair them with ViT feature representation for allowing the model to generate long-range structural and contextual relationships that are crucial for re-identification task in the long-term scenario. To evaluate the proposed approach, we perform experiments on two recent CC-ReID datasets, PRCC and LTCC. The experimental results show that the proposed approach achieves state-of-the-art results on the CC-ReID task.",
    "code_link": ""
  },
  "wacv2022_rws_event-drivenre-idanewbenchmarkandmethodtowardsprivacy-preservingpersonre-identification": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Event-Driven Re-Id: A New Benchmark and Method Towards Privacy-Preserving Person Re-Identification",
    "authors": [
      "Shafiq Ahmad",
      "Gianluca Scarpellini",
      "Pietro Morerio",
      "Alessio Del Bue"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Ahmad_Event-Driven_Re-Id_A_New_Benchmark_and_Method_Towards_Privacy-Preserving_Person_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Ahmad_Event-Driven_Re-Id_A_New_Benchmark_and_Method_Towards_Privacy-Preserving_Person_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "The large-scale use of surveillance cameras in public spaces raised severe concerns about an individual privacy breach. Introducing privacy and security in video surveillance systems, primarily in person re-identification (re-id), is quite challenging. Event cameras are novel sensors, which only respond to brightness changes in the scene. This characteristic makes event-based vision sensors viable for privacy-preserving in video surveillance. Integrating privacy into the person re-id; this work investigates the possibility of performing person re-id with the event-camera network for the first time. We transform the asynchronous events stream generated by an event camera into synchronous image-like representations to leverage deep learning models and then evaluate how complex the re-id problem is with this new sensor modality. Interestingly, such event-based representations contain meaningful spatial details which are very similar to standard edges and contours. We use two different representations, image-like representation and their transformation to polar coordinates (which carry more distinct edge patterns). Finally, we train a person re-id model on such images to demonstrate the feasibility of performing event-driven re-id. We evaluate the performance of our approach and produce baseline results on two synthetic datasets (generated from publicly available datasets, SAIVT and DukeMTMC-reid).",
    "code_link": ""
  },
  "wacv2022_rws_diordistillobservationstorepresentationsformulti-objecttrackingandsegmentation": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "DIOR: DIstill Observations to Representations for Multi-Object Tracking and Segmentation",
    "authors": [
      "Jiarui Cai",
      "Yizhou Wang",
      "Hung-Min Hsu",
      "Haotian Zhang",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Cai_DIOR_DIstill_Observations_to_Representations_for_Multi-Object_Tracking_and_Segmentation_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Cai_DIOR_DIstill_Observations_to_Representations_for_Multi-Object_Tracking_and_Segmentation_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Multi-object tracking (MOT) has long been a crucial topic in the field of autonomous driving and security monitoring. With the saturation of the bounding-box-based MOT algorithms in recent years, a new task to track objects with instance segmentation, called multi-object tracking and segmentation (MOTS), provides a finer level of scene understanding and introduces potential improvements in tracking accuracy. In this paper, we introduce a video-based MOTS framework, named DIstill Observations to Representations (DIOR). A feature distiller is designed to extract and balance the comprehensive object representations: 1) the temporal distiller aggregates context information for consistency of features and smoothness of prediction longitudinally; 2) the spatial distiller on the target of interest within each bounding box removes ambiguity and irrelevance of background in the learned features. The subsequent tracking steps start with Hungarian matching based on feature similarity and masks continuity, which is efficient and straightforward. In addition, we propose short-term retrieval (STR) and long-term re-identification (re-ID) modules to avoid missing associations due to failures in detection or possible occlusion. Our method achieves state-of-the-art performance in both MOTS20 and KITTI-MOTS benchmarks.",
    "code_link": ""
  },
  "wacv2022_rws_fightdetectionfromstillimagesinthewild": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Fight Detection From Still Images in the Wild",
    "authors": [
      "\u015eeymanur Akt\u0131",
      "Ferda Ofli",
      "Muhammad Imran",
      "Hazim Kemal Ekenel"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Akti_Fight_Detection_From_Still_Images_in_the_Wild_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Akti_Fight_Detection_From_Still_Images_in_the_Wild_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Detecting fights from still images shared on social media is an important task required to limit the distribution of violent scenes in order to prevent their negative effects. For this reason, in this study, we address the problem of fight detection from still images collected from the web and social media. We explore how well one can detect fights from just a single still image. We also propose a new dataset, named Social Media Fight Images (SMFI), comprising real-world images of fight actions. Results of the extensive experiments on the proposed dataset show that fight actions can be recognized successfully from still images. That is, even without exploiting the temporal information, it is possible to detect fights with high accuracy by utilizing appearance only. We also perform cross-dataset experiments to evaluate the representation capacity of the collected dataset. These experiments indicate that, as in the other computer vision problems, there exists a dataset bias for the fight recognition problem. Although the methods achieve close to 100% accuracy when trained and tested on the same fight dataset, the cross-dataset accuracies are significantly lower, i.e., around 70% when more representative datasets are used for training. SMFI dataset is found to be one of the two most representative datasets among the utilized five fight datasets.",
    "code_link": "https://github.com/sayibet/SMFI"
  },
  "wacv2022_rws_semantic-guidedzero-shotlearningforlow-lightimage/videoenhancement": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement",
    "authors": [
      "Shen Zheng",
      "Gaurav Gupta"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Zheng_Semantic-Guided_Zero-Shot_Learning_for_Low-Light_ImageVideo_Enhancement_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Zheng_Semantic-Guided_Zero-Shot_Learning_for_Low-Light_ImageVideo_Enhancement_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Low-light images challenge both human perceptions and computer vision algorithms. It is crucial to make algorithms robust to enlighten low-light images for computational photography and computer vision applications such as real-time detection and segmentation. This paper proposes a semantic-guided zero-shot low-light enhancement network (SGZ) which is trained in the absence of paired images, unpaired datasets, and segmentation annotation. Firstly, we design an enhancement factor extraction network using depthwise separable convolution for an efficient estimate of the pixel-wise light deficiency of an low-light image. Secondly, we propose a recurrent image enhancement network to progressively enhance the low-light image with affordable model size. Finally, we introduce an unsupervised semantic segmentation network for preserving the semantic information during intensive enhancement. Extensive experiments on benchmark datasets and a low-light video demonstrate that our model outperforms the previous state-of-the-art. We further discuss the benefits of the proposed method for low-light detection and segmentation. Code is available at https://github.com/ShenZheng2000/Semantic-Guided-Low-Light-Image-Enhancement.",
    "code_link": "https://github.com/ShenZheng2000/SemanticGuided-Low-Light-Image-Enhancement"
  },
  "wacv2022_rws_multipleobjecttrackingandforecastingjointlypredictingcurrentandfutureobjectlocations": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Multiple Object Tracking and Forecasting: Jointly Predicting Current and Future Object Locations",
    "authors": [
      "Oluwafunmilola Kesa",
      "Olly Styles",
      "Victor Sanchez"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Kesa_Multiple_Object_Tracking_and_Forecasting_Jointly_Predicting_Current_and_Future_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Kesa_Multiple_Object_Tracking_and_Forecasting_Jointly_Predicting_Current_and_Future_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "This paper introduces a joint learning architecture (JLA) for multiple object tracking (MOT) and multiple object forecasting (MOF) in which the goal is to predict tracked objects' current and future locations simultaneously. MOF is a recent formulation of trajectory forecasting where the full object bounding boxes are predicted rather than trajectories alone. Existing works separate multiple object tracking and multiple object forecasting. Such an approach can propagate errors in tracking to forecasting. We propose a joint learning architecture for multiple object tracking and forecasting (MOTF). Our approach reduces the chances of propagating tracking errors to the forecasting module. In addition, we show, through a new data association step, that forecasting predictions can be used for tracking objects during occlusion. We adapt an existing MOT method to simultaneously predict current and future object locations and confirm that JLA benefits both the MOT and MOF tasks.",
    "code_link": ""
  },
  "wacv2022_rws_semanticsegmentationguidedreal-worldsuper-resolution": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Semantic Segmentation Guided Real-World Super-Resolution",
    "authors": [
      "Andreas Aakerberg",
      "Anders S. Johansen",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Aakerberg_Semantic_Segmentation_Guided_Real-World_Super-Resolution_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Aakerberg_Semantic_Segmentation_Guided_Real-World_Super-Resolution_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Real-world single image Super-Resolution (SR) aims to enhance the resolution and reconstruct High-Resolution (HR) details of real Low-Resolution (LR) images. This is different from the traditional SR setting, where the LR images are synthetically created, typically with bicubic downsampling. As the degradation process for real-world LR images are highly complex, SR of such images is much more challenging. Recent promising approaches to solve the Real-World Super-Resolution (RWSR) problem include the use of domain adaptation to create realistic training pairs, and self-learning based methods which learn an image specific SR model at test time. However, as domain adaptation is an inherently challenging problem in itself, SR models based solely on this approach are limited by the domain gap. In contrast, while self-learning based methods remove the need for paired training data by utilizing internal information in the LR image, these methods come with the cost of slow prediction times. This paper proposes a novel framework, Semantic Segmentation Guided Real-World Super-Resolution (SSG-RWSR), which uses an auxiliary semantic segmentation network to guide the SR learning. This results in noise-free reconstructions with accurate object boundaries, and enables training on real LR images. The latter allows our SR network to adapt to the image specific degradations, without Ground-Truth (GT) reference images. We support the guidance with domain adaptation to faithfully reconstruct realistic textures, and ensure color consistency. We evaluate our proposed method on two public available datasets, and present State-of -the-Art results in terms of perceptual image quality on both real and synthesized LR images.",
    "code_link": ""
  },
  "wacv2022_rws_improvingpersonre-identificationwithtemporalconstraints": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Improving Person Re-Identification With Temporal Constraints",
    "authors": [
      "Julia Dietlmeier",
      "Feiyan Hu",
      "Frances Ryan",
      "Noel E. O'Connor",
      "Kevin McGuinness"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Dietlmeier_Improving_Person_Re-Identification_With_Temporal_Constraints_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Dietlmeier_Improving_Person_Re-Identification_With_Temporal_Constraints_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this paper we introduce an image-based person re-identification dataset collected across five non-overlapping camera views in the large and busy airport in Dublin, Ireland. Unlike all publicly available image-based datasets, our dataset contains timestamp information in addition to frame number, and camera and person IDs. Also our dataset has been fully anonymized to comply with modern data privacy regulations. We apply state-of-the-art person re-identification models to our dataset and show that by leveraging the available timestamp information we are able to achieve a significant gain of 37.43% in mAP and a gain of 30.22% in Rank1 accuracy. We also propose a Bayesian temporal re-ranking post-processing step, which further adds a 10.03% gain in mAP and 9.95% gain in Rank1 accuracy metrics. This work on combining visual and temporal information is not possible on other image-based person re-identification datasets. We believe that the proposed new dataset will enable further development of person re-identification research for challenging real-world applications.",
    "code_link": ""
  },
  "wacv2022_rws_multi-targetmulti-cameratrackingofvehiclesbygraphauto-encoderandself-supervisedcameralinkmodel": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Multi-Target Multi-Camera Tracking of Vehicles by Graph Auto-Encoder and Self-Supervised Camera Link Model",
    "authors": [
      "Hung-Min Hsu",
      "Yizhou Wang",
      "Jiarui Cai",
      "Jenq-Neng Hwang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Hsu_Multi-Target_Multi-Camera_Tracking_of_Vehicles_by_Graph_Auto-Encoder_and_Self-Supervised_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Hsu_Multi-Target_Multi-Camera_Tracking_of_Vehicles_by_Graph_Auto-Encoder_and_Self-Supervised_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Multi-Target Multi-Camera Tracking (MTMCT) of vehicles is a challenging task in smart city related applications. The main challenge of MTMCT is how to accurately match the single-camera trajectories generated from different cameras and establish a complete global cross-camera trajectory for each target, i.e., the multi-camera trajectory matching problem. In this paper, we propose a novel framework to solve this problem using the self-supervised trajectory-based camera link model (CLM) with both appearance and topological features systematically extracted from a graph auto-encoder (GAE) network. Unlike most related works that represent the spatio-temporal relationships of multiple cameras with the laborious human-annotated CLM, we introduce a self-supervised CLM (SCLM) generation method that extracts the crucial multi-camera relationships among the vehicle trajectories passing through different cameras robustly and automatically. Moreover, we apply a GAE to encode topological information and appearance features to generate the topological embeddings. According to our experimental results, the proposed method achieves a new state-of-the-art on both CityFlow 2019 and CityFlow 2020 benchmarks with IDF1 of 77.21% and 55.56%, respectively.",
    "code_link": ""
  },
  "wacv2022_rws_real-timebanglalicenseplaterecognitionsystemforlowresourcevideo-basedapplications": {
    "conf_id": "WACV2022",
    "conf_sub_id": "RWS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Real-World Surveillance: Applications and Challenges",
    "title": "Real-Time Bangla License Plate Recognition System for Low Resource Video-Based Applications",
    "authors": [
      "Alif Ashrafee",
      "Akib Mohammed Khan",
      "Mohammad Sabik Irbaz",
      "MD Abdullah Al Nasim"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/html/Ashrafee_Real-Time_Bangla_License_Plate_Recognition_System_for_Low_Resource_Video-Based_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Ashrafee_Real-Time_Bangla_License_Plate_Recognition_System_for_Low_Resource_Video-Based_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Automatic License Plate Recognition systems aim to provide a solution for detecting, localizing, and recognizing license plate characters from vehicles appearing in video frames. However, deploying such systems in the real world requires real-time performance in low-resource environments. In our paper, we propose a two-stage detection pipeline paired with Vision API that provides real-time inference speed along with consistently accurate detection and recognition performance. We used a haar-cascade classifier as a filter on top of our backbone MobileNet SSDv2 detection model. This reduces inference time by only focusing on high confidence detections and using them for recognition. We also impose a temporal frame separation strategy to distinguish between multiple vehicle license plates in the same clip. Furthermore, there are no publicly available Bangla license plate datasets, for which we created an image dataset and a video dataset containing license plates in the wild. We trained our models on the image dataset and achieved an AP(0.5) score of 86% and tested our pipeline on the video dataset and observed reasonable detection and recognition performance (82.7% detection rate, and 60.8% OCR F1 score) with real-time processing speed (27.2 frames per second).",
    "code_link": ""
  },
  "wacv2022_hpiv_aa3dnetattentionaugmentedrealtime3dobjectdetection": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HPIV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Hazard Perception in Intelligent Vehicles",
    "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection",
    "authors": [
      "Abhinav Sagar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/html/Sagar_AA3DNet_Attention_Augmented_Real_Time_3D_Object_Detection_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/papers/Sagar_AA3DNet_Attention_Augmented_Real_Time_3D_Object_Detection_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird's Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at > 30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",
    "code_link": ""
  },
  "wacv2022_hpiv_moreorless(mol)defendingagainstmultipleperturbationattacksondeepneuralnetworksthroughmodelensembleandcompression": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HPIV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Hazard Perception in Intelligent Vehicles",
    "title": "More or Less (MoL): Defending Against Multiple Perturbation Attacks on Deep Neural Networks Through Model Ensemble and Compression",
    "authors": [
      "Hao Cheng",
      "Kaidi Xu",
      "Zhengang Li",
      "Pu Zhao",
      "Chenan Wang",
      "Xue Lin",
      "Bhavya Kailkhura",
      "Ryan Goldhahn"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/html/Cheng_More_or_Less_MoL_Defending_Against_Multiple_Perturbation_Attacks_on_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/papers/Cheng_More_or_Less_MoL_Defending_Against_Multiple_Perturbation_Attacks_on_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Deep neural networks (DNNs) have been adopted in many application domains due to their superior performance. However, their susceptibility under test-time adversarial perturbations and out-of-distribution shifts has attracted extensive research efforts. The adversarial training provides an effective defense method withstanding evolving attacking methods. However, DNNs obtained by adversarial training are usually robust to only a single type of adversarial perturbation that they are trained with. To tackle this problem, improvements have been made to incorporate multiple perturbation types into adversarial training process, but with limited flexibility in terms of perturbation types. This work investigates the design problem of deep learning (DL) systems robust to multiple perturbation attacks. To maximize flexibility, we adopt the model ensemble approach, where an ensemble of expert models dealing with various perturbation types are integrated through a trainable aggregator module. Expert models are obtained in parallel through adversarial training, targeting at respective perturbation types. Then, the aggregator module is (adversarially) trained together with fine-tuning of expert models, addressing the obfuscated gradients issue in adversarial robustness. Furthermore, in order to practically implement the robust ensemble model onto edge devices, the model compression approach is leveraged to reduce the ensemble model size. By exploring the most suitable model compression scheme, we significantly reduce the overall model size without compromising robustness. Proposed More or Less (MoL) defense outperforms state-of-the-art defenses against multiple perturbations.",
    "code_link": ""
  },
  "wacv2022_hpiv_onsalience-sensitivesignclassificationinautonomousvehiclepathplanningexperimentalexplorationswithanoveldataset": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HPIV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Hazard Perception in Intelligent Vehicles",
    "title": "On Salience-Sensitive Sign Classification in Autonomous Vehicle Path Planning: Experimental Explorations With a Novel Dataset",
    "authors": [
      "Ross Greer",
      "Jason Isa",
      "Nachiket Deo",
      "Akshay Rangesh",
      "Mohan M. Trivedi"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/html/Greer_On_Salience-Sensitive_Sign_Classification_in_Autonomous_Vehicle_Path_Planning_Experimental_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/papers/Greer_On_Salience-Sensitive_Sign_Classification_in_Autonomous_Vehicle_Path_Planning_Experimental_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Safe path planning in autonomous driving is a complex task due to the interplay of static scene elements and uncertain surrounding agents. While all static scene elements are a source of information, there is asymmetric importance to the information available to the ego vehicle. We present a dataset with a novel feature, sign salience, defined to indicate whether a sign is distinctly informative to the goals of the ego vehicle with regards to traffic regulations. Using convolutional networks on cropped signs, in tandem with experimental augmentation by road type, image coordinates, and planned maneuver, we predict the sign salience property with 76% accuracy, finding the best improvement using information on vehicle maneuver with sign images.",
    "code_link": ""
  },
  "wacv2022_hpiv_monoculardepthestimationusingmultiscaleneuralnetworkandfeaturefusion": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HPIV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Hazard Perception in Intelligent Vehicles",
    "title": "Monocular Depth Estimation Using Multi Scale Neural Network and Feature Fusion",
    "authors": [
      "Abhinav Sagar"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/html/Sagar_Monocular_Depth_Estimation_Using_Multi_Scale_Neural_Network_and_Feature_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/papers/Sagar_Monocular_Depth_Estimation_Using_Multi_Scale_Neural_Network_and_Feature_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Depth estimation from monocular images is a challenging problem in computer vision. In this paper, we tackle this problem using a novel network architecture using multi scale feature fusion. Our network uses two different blocks, first which uses different filter sizes for convolution and merges all the individual feature maps. The second block uses dilated convolutions in place of fully connected layers thus reducing computations and increasing the receptive field. We present a new loss function for training the network which uses a depth regression term, SSIM loss term and a multinomial logistic loss term combined. We train and test our network on Make 3D dataset, NYU Depth V2 dataset and Kitti dataset using standard evaluation metrics for depth estimation comprised of RMSE loss and SILog loss. Our network outperforms previous state of the art methods with lesser parameters.",
    "code_link": ""
  },
  "wacv2022_hpiv_robust3dobjectdetectionformovingobjectsbasedonpointpillars": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HPIV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Hazard Perception in Intelligent Vehicles",
    "title": "Robust 3D Object Detection for Moving Objects Based on PointPillars",
    "authors": [
      "Ryota Nakamura",
      "Shuichi Enokida"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/html/Nakamura_Robust_3D_Object_Detection_for_Moving_Objects_Based_on_PointPillars_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/papers/Nakamura_Robust_3D_Object_Detection_for_Moving_Objects_Based_on_PointPillars_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Deep learning techniques have been applied success-fully to detecting objects from video images, and the use of three-dimensional (3D) point clouds obtained from light detection and ranging (LiDAR) with VoxelNet and other techniques have previously been proposed for use in highly accurate object detection methods that are robust against lighting changes. However, while object detection from video images with deep learning has been observed to be continuous and stable, there are times when a few continuous frames suddenly go undetected, thereby resulting in a phenomenon known as a momentary missed detection.Extending the methodology discussed in a previous paper that examined the cause of these momentary missed detection in object detection in 3D point clouds with VoxelNet, this study proposes a robust network for detecting moving objects while considering the cause of similar momentary missed detection in PointPillars, which is an encoder developed based on VoxelNet.",
    "code_link": ""
  },
  "wacv2022_hpiv_weakly-supervisedfreespaceestimationthroughstochasticco-teaching": {
    "conf_id": "WACV2022",
    "conf_sub_id": "HPIV",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Hazard Perception in Intelligent Vehicles",
    "title": "Weakly-Supervised Free Space Estimation Through Stochastic Co-Teaching",
    "authors": [
      "Fran\u00e7ois Robinet",
      "Claudia Parera",
      "Christian Hundt",
      "Rapha\u00ebl Frank"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/html/Robinet_Weakly-Supervised_Free_Space_Estimation_Through_Stochastic_Co-Teaching_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/HPIV/papers/Robinet_Weakly-Supervised_Free_Space_Estimation_Through_Stochastic_Co-Teaching_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Free space estimation is an important problem for autonomous robot navigation. Traditional camera-based approaches train a segmentation model using an annotated dataset. The training data needs to capture the wide variety of environments and weather conditions encountered at runtime, making the annotation cost prohibitively high. In this work, we propose a novel approach for obtaining free space estimates from images taken with a single road-facing camera. We rely on a technique that generates weak free space labels without any supervision, which are then used as ground truth to train a segmentation model for free space estimation. Our work differs from prior attempts by explicitly taking label noise into account through the use of Co-Teaching. Since Co-Teaching has traditionally been investigated in classification tasks, we adapt it for segmentation and examine how its parameters affect performances in our experiments. In addition, we propose Stochastic Co-Teaching, which is a novel method to select clean samples that leads to enhanced results. We achieve an IoU of 82.6%, a Precision of 90.9%, and a Recall of 90.3%. Our best model reaches 87% of the IoU, 93% of the Precision, and 93% of the Recall of the equivalent fully-supervised baseline while using no human annotations. To the best of our knowledge, this work is the first to use Co-Teaching to train a free space segmentation model under explicit label noise. Our implementation and trained models are freely available online.",
    "code_link": "https://github.com/qubvel/segmentation_models"
  },
  "wacv2022_cv4ws_refiningopenposewithanewsportsdatasetforrobust2dposeestimation": {
    "conf_id": "WACV2022",
    "conf_sub_id": "CV4WS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Computer Vision for Winter Sports",
    "title": "Refining OpenPose With a New Sports Dataset for Robust 2D Pose Estimation",
    "authors": [
      "Takumi Kitamura",
      "Hitoshi Teshima",
      "Diego Thomas",
      "Hiroshi Kawasaki"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/html/Kitamura_Refining_OpenPose_With_a_New_Sports_Dataset_for_Robust_2D_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/papers/Kitamura_Refining_OpenPose_With_a_New_Sports_Dataset_for_Robust_2D_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "3D marker-less motion capture can be achieved by triangulating estimated multi-views 2D poses. However, when the 2D pose estimation fails, the 3D motion capture also fails. This is particularly challenging for sports performance of athletes, which have extreme poses. In extreme poses (like having the head down) state-of-the-art 2D pose estimator such as OpenPose do not work at all. In this paper, we propose a new method to improve the training of 2D pose estimators for extreme poses by leveraging a new sports dataset and our proposed data augmentation strategy. Our results show significant improvements over previous methods for 2D pose estimation of athletes performing acrobatic moves, while keeping state-of-the-art performance on standard datasets.",
    "code_link": "https://github.com/tensorboy/pytorch"
  },
  "wacv2022_cv4ws_ape-vathleteperformanceevaluationusingvideo": {
    "conf_id": "WACV2022",
    "conf_sub_id": "CV4WS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Computer Vision for Winter Sports",
    "title": "APE-V: Athlete Performance Evaluation Using Video",
    "authors": [
      "Chaitanya Roygaga",
      "Dhruva Patil",
      "Michael Boyle",
      "William Pickard",
      "Raoul Reiser",
      "Aparna Bharati",
      "Nathaniel Blanchard"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/html/Roygaga_APE-V_Athlete_Performance_Evaluation_Using_Video_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/papers/Roygaga_APE-V_Athlete_Performance_Evaluation_Using_Video_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Athletes typically undergo regular evaluations by trainers and coaches to assess performance and injury risk. One of the most popular movements to examine in athletes needing lower extremity strength and power is the vertical jump. Specifically, maximal effort countermovement and drop jumps performed on bilateral force plates provide a wealth of metrics. However, the expense of the equipment and expertise needed to interpret the results limits their use. Computer vision techniques applied to videos of such movements are a less expensive alternative for extracting complementary metrics. Blanchard et al. collected a dataset of 89 athletes performing these movements and showcased how OpenPose could be applied to the data. However, athlete error calls into question 46.2% of movements --- in these cases, an expert assessor would have the athlete redo the movement to eliminate the error. Here, we augmented Blanchard et al. with expert labels of error and established benchmark performance on automatic error identification. In total, 14 different types of errors were identified by trained annotators. Our benchmark models identified errors with an F1 score of 0.710 and a Kappa of 0.457 (Kappa measures accuracy over chance). All code and augmented labels can be found at https://blanchard-lab.github.io/apev.github.io/.",
    "code_link": ""
  },
  "wacv2022_cv4ws_detectingarbitraryintermediatekeypointsforhumanposeestimationwithvisiontransformers": {
    "conf_id": "WACV2022",
    "conf_sub_id": "CV4WS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Computer Vision for Winter Sports",
    "title": "Detecting Arbitrary Intermediate Keypoints for Human Pose Estimation With Vision Transformers",
    "authors": [
      "Katja Ludwig",
      "Philipp Harzig",
      "Rainer Lienhart"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/html/Ludwig_Detecting_Arbitrary_Intermediate_Keypoints_for_Human_Pose_Estimation_With_Vision_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/papers/Ludwig_Detecting_Arbitrary_Intermediate_Keypoints_for_Human_Pose_Estimation_With_Vision_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Most human pose estimation datasets have a fixed set of keypoints. Hence, trained models are only capable of detecting these defined points. Adding new points to the dataset requires a full retraining of the model. We present a model based on the Vision Transformer architecture that can detect these fixed points and arbitrary intermediate points without any computational overhead during inference time. Furthermore, independently detected intermediate keypoints can improve analyses derived from the keypoints such as the calculation of body angles. Our approach is based on TokenPose and replaces the fixed keypoint tokens with an embedding of human readable keypoint vectors to keypoint tokens. For ski jumpers, who benefit from intermediate detections especially of their skis, this model achieves the same performance as TokenPose on the fixed keypoints and can detect any intermediate keypoint directly.",
    "code_link": ""
  },
  "wacv2022_cv4ws_video-basedskijumpstylescoringfromposetrajectory": {
    "conf_id": "WACV2022",
    "conf_sub_id": "CV4WS",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Computer Vision for Winter Sports",
    "title": "Video-Based Ski Jump Style Scoring From Pose Trajectory",
    "authors": [
      "Dejan \u0160tepec",
      "Danijel Sko\u010daj"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/html/Stepec_Video-Based_Ski_Jump_Style_Scoring_From_Pose_Trajectory_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/CV4WS/papers/Stepec_Video-Based_Ski_Jump_Style_Scoring_From_Pose_Trajectory_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Ski jumping is one of the oldest winter sports and takes also part in the Winter Olympics from the very start in 1924. One of the components of the final score, which is used for ranking the competitors, is the style score, given by five judges. The goal of this work was to develop a prototype for automatic style scoring from videos. As the main source of information, the proposed approach uses the detected locations of the ski jumper body parts and his skis to capture a full-body movement through the entire ski jump. We extended a method for human pose estimation from images to detect also the tips and the tails of the skies and adapted it to the domain of ski jumping. We proposed a method to utilize the detected trajectories along with the scores given by real judges to build a model for predicting the style scores. The experimental results obtained on the data that we had available show that the proposed computer-vision-based system for automatic style scoring achieves an error comparable to the error of real judges.",
    "code_link": ""
  },
  "wacv2022_waci_generalizingimagingthroughscatteringmediawithuncertaintyestimates": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "Generalizing Imaging Through Scattering Media With Uncertainty Estimates",
    "authors": [
      "Jared M. Cochrane",
      "Matthew Beveridge",
      "Iddo Drori"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Cochrane_Generalizing_Imaging_Through_Scattering_Media_With_Uncertainty_Estimates_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Cochrane_Generalizing_Imaging_Through_Scattering_Media_With_Uncertainty_Estimates_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Imaging through scattering media is challenging: object features are hidden under highly-scattered photons. Conventional methods that characterize scattering properties, such as the media input-output transmission matrix, are susceptible to environmental disturbance that is not ideal for many imaging scenarios, especially in biomedical imaging. Learning from examples is ideal for imaging in highly scattered regimes because it is adaptable and accurate even when the microstructures of the scattering media change. In current approaches, network output on unseen scattering media contain artifacts that inhibit meaningful object recognition. We present a network architecture that is able to generate high quality images over a range of different scattering media and image sizes with minimal artifacts. Our network learns the statistical information within highly scattered speckle intensity patterns. This allows us to compute an accurate mapping from different speckle patterns to their corresponding objects given scattering media with varying microstructures. Our network demonstrates superior performance compared to similar models, especially when trained on a single scattering medium and then tested on unseen scattering media. We estimate the uncertainty of our approach and use the available data efficiently, increasing the generalizability of predicting objects from unseen scattering media with multiple different diffusers.",
    "code_link": ""
  },
  "wacv2022_waci_hyperspectralimagesuper-resolutioninarbitraryinput-outputbandsettings": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings",
    "authors": [
      "Zhongyang Zhang",
      "Zhiyang Xu",
      "Zia Ahmed",
      "Asif Salekin",
      "Tauhidur Rahman"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Zhang_Hyperspectral_Image_Super-Resolution_in_Arbitrary_Input-Output_Band_Settings_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Zhang_Hyperspectral_Image_Super-Resolution_in_Arbitrary_Input-Output_Band_Settings_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Hyperspectral image (HSI) with narrow spectral bands can capture rich spectral information, but it sacrifices its spatial resolution in the process. Many machine-learning-based HSI super-resolution (SR) algorithms have been proposed recently. However, one of the fundamental limitations of these approaches is that they are highly dependent on image and camera settings and can only learn to map an input HSI with one specific setting to an output HSI with another. However, different cameras capture images with different spectral response functions and bands numbers due to the diversity of HSI cameras. Consequently, the existing machine-learning-based approaches fail to learn to super-resolve HSIs for a wide variety of input-output band settings. We propose a single Meta-Learning-Based Super-Resolution (MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate SR HSIs with an arbitrary number of output bands' peak wavelengths. We leverage NTIRE2020 and ICVL datasets to train and validate the performance of the MLSR model. The results show that the single proposed model can successfully generate super-resolved HSI bands at arbitrary input-output band settings. The results are better or at least comparable to baselines that are separately trained on a specific input-output band setting.",
    "code_link": ""
  },
  "wacv2022_waci_stp-netspatio-temporalpolarizationnetworkforactionrecognitionusingpolarimetricvideos": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "STP-Net: Spatio-Temporal Polarization Network for Action Recognition Using Polarimetric Videos",
    "authors": [
      "R. Krishna Kanth",
      "Akshaya Ramaswamy",
      "A. Anil Kumar",
      "Jayavardhana Gubbi",
      "Balamuralidhar P"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Kanth_STP-Net_Spatio-Temporal_Polarization_Network_for_Action_Recognition_Using_Polarimetric_Videos_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Kanth_STP-Net_Spatio-Temporal_Polarization_Network_for_Action_Recognition_Using_Polarimetric_Videos_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Deep learning has brought tremendous progress in computer vision and natural language processing, and is used in multiple non-critical applications. A major bottleneck for its use in many other areas is the black box nature of these algorithms, resulting in a lack of explainability in their decisions. One of the key problems identified is the confounding effect, which causes confusion between the desired causes and other irrelevant factors affecting an outcome. This is more pronounced in the spatio-temporal case, such as the bias on the static background in the classification of a video. A way to handle this is by making use of sensors that capture additional scene properties, to mitigate spurious associations. In this work, we integrate the polarimetric videos with deep learning and evaluate it on the popular action recognition problem. We construct a dataset of polarimetric videos for fine-grained actions and study the effect of various parameters, extracted from the polarimetric video frames, as inputs to a deep network. Using these observations, we design a spatio-temporal polarization network (STP-Net) to effectively extract polarimetric features. This is evaluated on the recent HumanAct12 dataset for human activity recognition. Extensive evaluation clearly shows that the polarimetric modality is able to localize the correct action regions, leading to better generalizability.",
    "code_link": ""
  },
  "wacv2022_waci_jointmulti-scaletonemappinganddenoisingforhdrimageenhancement": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "Joint Multi-Scale Tone Mapping and Denoising for HDR Image Enhancement",
    "authors": [
      "Litao Hu",
      "Huaijin Chen",
      "Jan P. Allebach"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Hu_Joint_Multi-Scale_Tone_Mapping_and_Denoising_for_HDR_Image_Enhancement_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Hu_Joint_Multi-Scale_Tone_Mapping_and_Denoising_for_HDR_Image_Enhancement_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "An image processing unit (IPU), or image signal processor (ISP) for high dynamic range (HDR) imaging usually consists of demosaicing, white balancing, lens shading correction, color correction, denoising, and tone-mapping. Besides noise from the imaging sensors, almost every step in the ISP introduces or amplifies noise in different ways, and denoising operators are designed to reduce the noise from these sources. Designed for dynamic range compressing, tone-mapping operators in an ISP can significantly amplify the noise level, especially for images captured in low-light conditions, making denoising very difficult. Therefore, we propose a joint multi-scale denoising and tone-mapping framework that is designed with both operations in mind for HDR images. Our joint network is trained in an end-to-end format that optimizes both operators together, to prevent the tone-mapping operator from overwhelming the denoising operator. Our model outperforms existing HDR denoising and tone-mapping operators both quantitatively and qualitatively on most of our benchmarking datasets.",
    "code_link": ""
  },
  "wacv2022_waci_temporallyconsistentrelightingforportraitvideos": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "Temporally Consistent Relighting for Portrait Videos",
    "authors": [
      "Sreenithy Chandran",
      "Yannick Hold-Geoffroy",
      "Kalyan Sunkavalli",
      "Zhixin Shu",
      "Suren Jayasuriya"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Chandran_Temporally_Consistent_Relighting_for_Portrait_Videos_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Chandran_Temporally_Consistent_Relighting_for_Portrait_Videos_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Ensuring ideal lighting when recording videos of people can be a daunting task requiring a controlled environment and expensive equipment. Methods were recently proposed to perform portrait relighting for still images, enabling after-the-fact lighting enhancement. However, naively applying these methods on each frame independently yields videos plagued with flickering artifacts. In this work, we propose the first method to perform temporally consistent video portrait relighting. To achieve this, our method optimizes end-to-end both desired lighting and temporal consistency jointly. We do not require ground truth lighting annotations during training, allowing us to take advantage of the large corpus of portrait videos already available on the internet. We demonstrate that our method outperforms previous work in balancing accurate relighting and temporal consistency on a number of real-world portrait videos.",
    "code_link": ""
  },
  "wacv2022_waci_feature-alignnetworkwithknowledgedistillationforefficientdenoising": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "Feature-Align Network With Knowledge Distillation for Efficient Denoising",
    "authors": [
      "Lucas D. Young",
      "Fitsum A. Reda",
      "Rakesh Ranjan",
      "Jon Morton",
      "Jun Hu",
      "Yazhu Ling",
      "Xiaoyu Xiang",
      "David Liu",
      "Vikas Chandra"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Young_Feature-Align_Network_With_Knowledge_Distillation_for_Efficient_Denoising_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Young_Feature-Align_Network_With_Knowledge_Distillation_for_Efficient_Denoising_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "We propose an efficient neural network for RAW image denoising. Although neural network-based denoising has been extensively studied for image restoration, little attention has been given to efficient denoising for compute limited and power sensitive devices, such as smartphones and wearables. In this paper, we present a novel architecture and a suite of training techniques for high quality denoising in mobile devices. Our work is distinguished by three main contributions. (1) The Feature-Align layer that modulates the activations of an encoder-decoder architecture with the input noisy images. The auto modulation layer enforces attention to spatially varying noise that tends to be \"washed away\" by successive application of convolutions and non-linearity. (2) A novel Feature Matching Loss that allows knowledge distillation from large denoising networks in the form of a perceptual content loss. (3) Empirical analysis of our efficient model trained to specialize on different noise subranges. This opens an additional avenue for model size reduction by sacrificing memory for compute. Extensive experimental validation shows that our efficient model produces high quality denoising results that compete with state-of-the-art large networks, while using significantly fewer parameters and MACs. On the Darmstadt Noise Dataset benchmark, we achieve a PSNR of 48.28dB, while using 263 times fewer MACs and 17.6 times fewer parameters than the state-of-the-art network, which achieves 49.12dB.",
    "code_link": ""
  },
  "wacv2022_waci_idea-netadaptivedualself-attentionnetworkforsingleimagedenoising": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "IDEA-Net: Adaptive Dual Self-Attention Network for Single Image Denoising",
    "authors": [
      "Zheming Zuo",
      "Xinyu Chen",
      "Han Xu",
      "Jie Li",
      "Wenjuan Liao",
      "Zhi-Xin Yang",
      "Shizheng Wang"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Zuo_IDEA-Net_Adaptive_Dual_Self-Attention_Network_for_Single_Image_Denoising_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Zuo_IDEA-Net_Adaptive_Dual_Self-Attention_Network_for_Single_Image_Denoising_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Image denoising is a challenging task due to possible data bias and prediction variance. Existing approaches usually suffer from high computational cost. In this work, we propose an unsupervised image denoiser, dubbed as adaptIve Dual sElf-Attention Network (IDEA-Net), to handle these challenges. IDEA-Net benefits from a generatively learned image-wise dual self-attention region where the denoising process is enforced. Besides, IDEA-Net is not only robust to possible data bias but also helpful to reduce the prediction variance by applying a simplified encoder-decoder with Poisson dropout operations on a single noisy image merely. The proposed IDEA-Net demonstrated the outperformance on four benchmark datasets compared with other single-image-based learning and non-learning image denoisers. IDEA-Net also shows an appropriate choice to remove real-world noise in low-light and noisy scenes, which in turn, contribute to more accurate dark face detection. The source code is available at https://github.com/zhemingzuo/IDEA-Net.",
    "code_link": ""
  },
  "wacv2022_waci_multi-viewmotionsynthesisviaapplyingrotateddual-pixelblurkernels": {
    "conf_id": "WACV2022",
    "conf_sub_id": "WACI",
    "is_workshop": true,
    "conf_name": "WACV2022_workshops - Applications of Computational Imaging",
    "title": "Multi-View Motion Synthesis via Applying Rotated Dual-Pixel Blur Kernels",
    "authors": [
      "Abdullah Abuolaim",
      "Mahmoud Afifi",
      "Michael S. Brown"
    ],
    "page_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/html/Abuolaim_Multi-View_Motion_Synthesis_via_Applying_Rotated_Dual-Pixel_Blur_Kernels_WACVW_2022_paper.html",
    "pdf_url": "http://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Abuolaim_Multi-View_Motion_Synthesis_via_Applying_Rotated_Dual-Pixel_Blur_Kernels_WACVW_2022_paper.pdf",
    "published": "2022-01",
    "summary": "Portrait mode is widely available on smartphone cameras to provide an enhanced photographic experience. One of the primary effects applied to images captured in portrait mode is a synthetic shallow depth of field (DoF). The synthetic DoF (or bokeh effect) selectively blurs regions in the image to emulate the effect of using a large lens with a wide aperture. In addition, many applications now incorporate a new image motion attribute (NIMAT) to emulate background motion, where the motion is correlated with estimated depth at each pixel. In this work, we follow the trend of rendering the NIMAT effect by introducing a modification on the blur synthesis procedure in portrait mode. In particular, our modification enables a high-quality synthesis of multi-view bokeh from a single image by applying rotated blurring kernels. Given the synthesized multiple views, we can generate aesthetically realistic image motion similar to the NIMAT effect. We validate our approach qualitatively compared to the original NIMAT effect and other similar image motions, like Facebook 3D image. Our image motion demonstrates a smooth image view transition with fewer artifacts around the object boundary.",
    "code_link": ""
  }
}